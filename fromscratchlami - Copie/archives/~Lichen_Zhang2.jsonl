{"id": "povPXX2mbp", "cdate": 1672531200000, "mdate": 1695957837706, "content": {"title": "An Online and Unified Algorithm for Projection Matrix Vector Multiplication with Application to Empirical Risk Minimization", "abstract": "Online matrix vector multiplication is a fundamental step and bottleneck in many machine learning algorithms. It is defined as follows: given a matrix at the pre-processing phase, at each iteration..."}}
{"id": "m7_IUwkeBxY", "cdate": 1672531200000, "mdate": 1695957837721, "content": {"title": "A Nearly-Optimal Bound for Fast Regression with \u2113\u221e Guarantee", "abstract": "Given a matrix $A\\in \\mathbb{R}^{n\\times d}$ and a vector $b\\in \\mathbb{R}^n$, we consider the regression problem with $\\ell_\\infty$ guarantees: finding a vector $x\u2019\\in \\mathbb{R}^d$ such that $||x..."}}
{"id": "jtJwR0GLUP8", "cdate": 1672531200000, "mdate": 1683235214475, "content": {"title": "Convex Minimization with Integer Minima in \u00d5(n4) Time", "abstract": "Given a convex function $f$ on $\\mathbb{R}^n$ with an integer minimizer, we show how to find an exact minimizer of $f$ using $O(n^2 \\log n)$ calls to a separation oracle and $O(n^4 \\log n)$ time. The previous best polynomial time algorithm for this problem given in [Jiang, SODA 2021, JACM 2022] achieves $\\widetilde{O}(n^2)$ oracle complexity. However, the overall runtime of Jiang's algorithm is at least $\\widetilde{\\Omega}(n^8)$, due to expensive sub-routines such as the Lenstra-Lenstra-Lov\\'asz (LLL) algorithm [Lenstra, Lenstra, Lov\\'asz, Math. Ann. 1982] and random walk based cutting plane method [Bertsimas, Vempala, JACM 2004]. Our significant speedup is obtained by a nontrivial combination of a faster version of the LLL algorithm due to [Neumaier, Stehl\\'e, ISSAC 2016] that gives similar guarantees, the volumetric center cutting plane method (CPM) by [Vaidya, FOCS 1989] and its fast implementation given in [Jiang, Lee, Song, Wong, STOC 2020].   For the special case of submodular function minimization (SFM), our result implies a strongly polynomial time algorithm for this problem using $O(n^3 \\log n)$ calls to an evaluation oracle and $O(n^4 \\log n)$ additional arithmetic operations. Both the oracle complexity and the number of arithmetic operations of our more general algorithm are better than the previous best-known runtime algorithms for this specific problem given in [Lee, Sidford, Wong, FOCS 2015] and [Dadush, V\\'egh, Zambelli, SODA 2018, MOR 2021]."}}
{"id": "Z_OlyF2T2hl", "cdate": 1672531200000, "mdate": 1695957837722, "content": {"title": "Streaming Semidefinite Programs: O(\u221an) Passes, Small Space and Fast Runtime", "abstract": "We study the problem of solving semidefinite programs (SDP) in the streaming model. Specifically, $m$ constraint matrices and a target matrix $C$, all of size $n\\times n$ together with a vector $b\\in \\mathbb{R}^m$ are streamed to us one-by-one. The goal is to find a matrix $X\\in \\mathbb{R}^{n\\times n}$ such that $\\langle C, X\\rangle$ is maximized, subject to $\\langle A_i, X\\rangle=b_i$ for all $i\\in [m]$ and $X\\succeq 0$. Previous algorithmic studies of SDP primarily focus on \\emph{time-efficiency}, and all of them require a prohibitively large $\\Omega(mn^2)$ space in order to store \\emph{all the constraints}. Such space consumption is necessary for fast algorithms as it is the size of the input. In this work, we design an interior point method (IPM) that uses $\\widetilde O(m^2+n^2)$ space, which is strictly sublinear in the regime $n\\gg m$. Our algorithm takes $O(\\sqrt n\\log(1/\\epsilon))$ passes, which is standard for IPM. Moreover, when $m$ is much smaller than $n$, our algorithm also matches the time complexity of the state-of-the-art SDP solvers. To achieve such a sublinear space bound, we design a novel sketching method that enables one to compute a spectral approximation to the Hessian matrix in $O(m^2)$ space. To the best of our knowledge, this is the first method that successfully applies sketching technique to improve SDP algorithm in terms of space (also time)."}}
{"id": "Wj9TPPbU22", "cdate": 1672531200000, "mdate": 1695957837716, "content": {"title": "Space-Efficient Interior Point Method, with Applications to Linear Programming and Maximum Weight Bipartite Matching", "abstract": "We study the problem of solving linear program in the streaming model. Given a constraint matrix A \u2208 \u211d^{m\u00d7n} and vectors b \u2208 \u211d^m, c \u2208 \u211d^n, we develop a space-efficient interior point method that optimizes solely on the dual program. To this end, we obtain efficient algorithms for various different problems: - For general linear programs, we can solve them in \u00d5(\u221an log(1/\u03b5)) passes and \u00d5(n\u00b2) space for an \u03b5-approximate solution. To the best of our knowledge, this is the most efficient LP solver in streaming with no polynomial dependence on m for both space and passes. - For bipartite graphs, we can solve the minimum vertex cover and maximum weight matching problem in \u00d5(\u221am) passes and \u00d5(n) space. In addition to our space-efficient IPM, we also give algorithms for solving SDD systems and isolation lemma in \u00d5(n) spaces, which are the cornerstones for our graph results."}}
{"id": "SFyO0Li-V9", "cdate": 1672531200000, "mdate": 1695957837720, "content": {"title": "Efficient Alternating Minimization with Applications to Weighted Low Rank Approximation", "abstract": "Weighted low rank approximation is a fundamental problem in numerical linear algebra, and it has many applications in machine learning. Given a matrix $M \\in \\mathbb{R}^{n \\times n}$, a weight matrix $W \\in \\mathbb{R}_{\\geq 0}^{n \\times n}$, a parameter $k$, the goal is to output two matrices $U, V \\in \\mathbb{R}^{n \\times k}$ such that $\\| W \\circ (M - U V^\\top) \\|_F$ is minimized, where $\\circ$ denotes the Hadamard product. Such a problem is known to be NP-hard and even hard to approximate assuming Exponential Time Hypothesis [GG11, RSW16]. Meanwhile, alternating minimization is a good heuristic solution for approximating weighted low rank approximation. The work [LLR16] shows that, under mild assumptions, alternating minimization does provide provable guarantees. In this work, we develop an efficient and robust framework for alternating minimization. For weighted low rank approximation, this improves the runtime of [LLR16] from $n^2 k^2$ to $n^2k$. At the heart of our work framework is a high-accuracy multiple response regression solver together with a robust analysis of alternating minimization."}}
{"id": "Qyrg9JlzME", "cdate": 1672531200000, "mdate": 1695957837758, "content": {"title": "A Nearly-Linear Time Algorithm for Structured Support Vector Machines", "abstract": "Quadratic programming is a fundamental problem in the field of convex optimization. Many practical tasks can be formulated as quadratic programming, for example, the support vector machine (SVM). Linear SVM is one of the most popular tools over the last three decades in machine learning before deep learning method dominating. In general, a quadratic program has input size $\\Theta(n^2)$ (where $n$ is the number of variables), thus takes $\\Omega(n^2)$ time to solve. Nevertheless, quadratic programs coming from SVMs has input size $O(n)$, allowing the possibility of designing nearly-linear time algorithms. Two important classes of SVMs are programs admitting low-rank kernel factorizations and low-treewidth programs. Low-treewidth convex optimization has gained increasing interest in the past few years (e.g.~linear programming [Dong, Lee and Ye 2021] and semidefinite programming [Gu and Song 2022]). Therefore, an important open question is whether there exist nearly-linear time algorithms for quadratic programs with these nice structures. In this work, we provide the first nearly-linear time algorithm for solving quadratic programming with low-rank factorization or low-treewidth, and a small number of linear constraints. Our results imply nearly-linear time algorithms for low-treewidth or low-rank SVMs."}}
{"id": "F4LYdsyudG-", "cdate": 1672531200000, "mdate": 1681596352348, "content": {"title": "Low Rank Matrix Completion via Robust Alternating Minimization in Nearly Linear Time", "abstract": ""}}
{"id": "DUnlHpMK4x", "cdate": 1672531200000, "mdate": 1695957837716, "content": {"title": "Efficient Algorithm for Solving Hyperbolic Programs", "abstract": "Hyperbolic polynomials is a class of real-roots polynomials that has wide range of applications in theoretical computer science. Each hyperbolic polynomial also induces a hyperbolic cone that is of particular interest in optimization due to its generality, as by choosing the polynomial properly, one can easily recover the classic optimization problems such as linear programming and semidefinite programming. In this work, we develop efficient algorithms for hyperbolic programming, the problem in each one wants to minimize a linear objective, under a system of linear constraints and the solution must be in the hyperbolic cone induced by the hyperbolic polynomial. Our algorithm is an instance of interior point method (IPM) that, instead of following the central path, it follows the central Swath, which is a generalization of central path. To implement the IPM efficiently, we utilize a relaxation of the hyperbolic program to a quadratic program, coupled with the first four moments of the hyperbolic eigenvalues that are crucial to update the optimization direction. We further show that, given an evaluation oracle of the polynomial, our algorithm only requires $O(n^2d^{2.5})$ oracle calls, where $n$ is the number of variables and $d$ is the degree of the polynomial, with extra $O((n+m)^3 d^{0.5})$ arithmetic operations, where $m$ is the number of constraints."}}
{"id": "C3cDeMBJJiJ", "cdate": 1672531200000, "mdate": 1695957837717, "content": {"title": "Solving Attention Kernel Regression Problem via Pre-conditioner", "abstract": "Large language models have shown impressive performance in many tasks. One of the major features from the computation perspective is computing the attention matrix. Previous works [Zandieh, Han, Daliri, and Karba 2023, Alman and Song 2023] have formally studied the possibility and impossibility of approximating the attention matrix. In this work, we define and study a new problem which is called the attention kernel regression problem. We show how to solve the attention kernel regression in the input sparsity time of the data matrix."}}
