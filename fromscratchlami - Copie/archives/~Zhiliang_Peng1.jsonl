{"id": "2xpcrm49t7T", "cdate": 1690848000000, "mdate": 1693841109782, "content": {"title": "Conformer: Local Features Coupling Global Representations for Recognition and Detection", "abstract": "With convolution operations, Convolutional Neural Networks (CNNs) are good at extracting local features but experience difficulty to capture global representations. With cascaded self-attention modules, vision transformers can capture long-distance feature dependencies but unfortunately deteriorate local feature details. In this paper, we propose a hybrid network structure, termed Conformer, to take both advantages of convolution operations and self-attention mechanisms for enhanced representation learning. Conformer roots in feature coupling of CNN local features and transformer global representations under different resolutions in an interactive fashion. Conformer adopts a dual structure so that local details and global dependencies are retained to the maximum extent. We also propose a Conformer-based detector (ConformerDet), which learns to predict and refine object proposals, by performing region-level feature coupling in an augmented cross-attention fashion. Experiments on ImageNet and MS COCO datasets validate Conformer's superiority for visual recognition and object detection, demonstrating its potential to be a general backbone network."}}
{"id": "on9sP7K1LMm", "cdate": 1672531200000, "mdate": 1693841109781, "content": {"title": "Kosmos-2: Grounding Multimodal Large Language Models to the World", "abstract": "We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2."}}
{"id": "nvn2tJgfvUB", "cdate": 1672531200000, "mdate": 1693841109782, "content": {"title": "Magneto: A Foundation Transformer", "abstract": "A big convergence of model architectures across language, vision, speech, and multimodal is emerging. However, under the same name \u201dTransformers\u201d, the above areas use different implementations for ..."}}
{"id": "jF4DuihVFU", "cdate": 1672531200000, "mdate": 1693841109784, "content": {"title": "Magneto: A Foundation Transformer", "abstract": "A big convergence of model architectures across language, vision, speech, and multimodal is emerging. However, under the same name \u201dTransformers\u201d, the above areas use different implementations for ..."}}
{"id": "OUucKCztCe", "cdate": 1672531200000, "mdate": 1681636855575, "content": {"title": "Generic-to-Specific Distillation of Masked Autoencoders", "abstract": ""}}
{"id": "GRnPGuXUzxM", "cdate": 1672531200000, "mdate": 1693841109765, "content": {"title": "Kosmos-2: Grounding Multimodal Large Language Models to the World", "abstract": "We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2."}}
{"id": "6TIZy6Uv_8r", "cdate": 1672531200000, "mdate": 1693841109783, "content": {"title": "A Unified View of Masked Image Modeling", "abstract": "Masked image modeling has demonstrated great potential to eliminate the label-hungry problem of training large-scale vision Transformers, achieving impressive performance on various downstream tasks. In this work, we propose a unified view of masked image modeling after revisiting existing methods. Under the unified view, we introduce a simple yet effective method, termed as MaskDistill, which reconstructs normalized semantic features from teacher models at the masked positions, conditioning on corrupted input images. Experimental results on image classification and semantic segmentation show that MaskDistill achieves comparable or superior performance than state-of-the-art methods. When using the huge vision Transformer and pretraining 300 epochs, MaskDistill obtains 88.3% fine-tuning top-1 accuracy on ImageNet-1k (224 size) and 58.8 semantic segmentation mIoU metric on ADE20k (512 size). Code is enclosed in the supplementary materials."}}
{"id": "U1cPSPskTR", "cdate": 1665626344525, "mdate": null, "content": {"title": "Conformer: Local Features Coupling Global Representations for Visual Recognition", "abstract": "Within Convolutional Neural Network (CNN), the convolution operations are good at extracting local features but experience difficulty to capture global representations. Within visual transformer, the cascaded self-attention modules can capture long-distance feature dependencies but unfortunately deteriorate local feature details. In this paper, we propose a hybrid network structure, termed Conformer,\nto take advantage of convolutional operations and selfattention mechanisms for enhanced representation learning. Conformer roots in the Feature Coupling Unit (FCU), which fuses local features and global representations under different resolutions in an interactive fashion. Conformer adopts a concurrent structure so that local features and global representations are retained to the maximum extent. Experiments show that Conformer, under the comparable parameter complexity, outperforms the visual transformer (DeiT-B) by 2.3% on ImageNet. On MSCOCO, it outperforms ResNet-101 by 3.7% and 3.6% mAPs for object detection and instance segmentation, respectively, demonstrating\nthe great potential to be a general backbone network."}}
{"id": "VB75Pi89p7", "cdate": 1663849978686, "mdate": null, "content": {"title": "BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers", "abstract": "Masked image modeling (MIM) has demonstrated impressive results in self-supervised representation learning by recovering corrupted image patches. However, most existing studies operate on low-level image pixels, which hinders the exploitation of high-level semantics for representation models. In this work, we propose to use a semantic-rich visual tokenizer as the reconstruction target for masked prediction, providing a systematic way to promote MIM from pixel-level to semantic-level. Specifically, we propose vector-quantized knowledge distillation to train the tokenizer, which discretizes a continuous semantic space to compact codes. We then pretrain vision Transformers by predicting the original visual tokens for the masked image patches. Furthermore, we introduce a patch aggregation strategy which associates discrete image patches to enhance global semantic representation. Experiments on image classification and semantic segmentation show that BEiT v2 outperforms all compared MIM methods. On ImageNet-1K (224 size), the base-size BEiT v2 achieves $85.5\\%$ top-1 accuracy for fine-tuning and $80.1\\%$ top-1 accuracy for linear probing. The large-size BEiT v2 obtains $87.3\\%$ top-1 accuracy for ImageNet-1K (224 size) fine-tuning, and $56.7\\%$ mIoU on ADE20K for semantic segmentation. The code can be found in the supplementary materials."}}
{"id": "ygoDyywchyV", "cdate": 1640995200000, "mdate": 1667482160708, "content": {"title": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks", "abstract": "A big convergence of language, vision, and multimodal pretraining is emerging. In this work, we introduce a general-purpose multimodal foundation model BEiT-3, which achieves state-of-the-art transfer performance on both vision and vision-language tasks. Specifically, we advance the big convergence from three aspects: backbone architecture, pretraining task, and model scaling up. We introduce Multiway Transformers for general-purpose modeling, where the modular architecture enables both deep fusion and modality-specific encoding. Based on the shared backbone, we perform masked \"language\" modeling on images (Imglish), texts (English), and image-text pairs (\"parallel sentences\") in a unified manner. Experimental results show that BEiT-3 obtains state-of-the-art performance on object detection (COCO), semantic segmentation (ADE20K), image classification (ImageNet), visual reasoning (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO)."}}
