{"id": "U_MhWQ7vECt", "cdate": 1676827067364, "mdate": null, "content": {"title": "Stochastic Generative Flow Networks", "abstract": "Generative Flow Networks (or GFlowNets for short) are a family of probabilistic agents that learn to sample complex combinatorial structures through the lens of ``inference as control''. They have shown great potential in generating high-quality and diverse candidates from a given energy landscape. However, existing GFlowNets can be applied only to deterministic environments, and fail in more general tasks with stochastic dynamics, which can limit their applicability. To overcome this challenge, this paper introduces Stochastic GFlowNets, a new algorithm that extends GFlowNets to stochastic environments. By decomposing state transitions into two steps, Stochastic GFlowNets isolate environmental stochasticity and learn a dynamics model to capture it. Extensive experimental results demonstrate that Stochastic GFlowNets offer significant advantages over standard GFlowNets as well as MCMC- and RL-based approaches, on a variety of standard benchmarks with stochastic dynamics."}}
{"id": "_aff9_iVY-", "cdate": 1672531200000, "mdate": 1681650306251, "content": {"title": "Better Training of GFlowNets with Local Credit and Incomplete Trajectories", "abstract": ""}}
{"id": "XqBIzC0usYM", "cdate": 1672531200000, "mdate": 1681650306064, "content": {"title": "Stochastic Generative Flow Networks", "abstract": ""}}
{"id": "FV2UiY5w6Ca", "cdate": 1672531200000, "mdate": 1681650305661, "content": {"title": "Distributional GFlowNets with Quantile Flows", "abstract": ""}}
{"id": "DJEEqoAq7to", "cdate": 1663850140451, "mdate": null, "content": {"title": "RLx2: Training a Sparse Deep Reinforcement Learning Model from Scratch", "abstract": "Training deep reinforcement learning (DRL) models usually requires high computation costs. Therefore, compressing DRL models possesses immense potential for training acceleration and model deployment. However, existing methods that generate small models mainly adopt the knowledge distillation-based approach by iteratively training a dense network. As a result, the training process still demands massive computing resources. Indeed, sparse training from scratch in DRL has not been well explored and is particularly challenging due to non-stationarity in bootstrap training. In this work, we propose a novel sparse DRL training framework, \u201cthe Rigged Reinforcement Learning Lottery\u201d (RLx2), which builds upon gradient-based topology evolution and is capable of training a sparse DRL model based entirely on a sparse network. Specifically, RLx2 introduces a novel multi-step TD target mechanism with a dynamic-capacity replay buffer to achieve robust value learning and efficient topology exploration in sparse models. It also reaches state-of-the-art sparse training performance in several tasks, showing $7.5\\times$-$20\\times$ model compression with less than $3\\%$ performance degradation and up to $20\\times$ and $50\\times$ FLOPs reduction for training and inference, respectively."}}
{"id": "urF_CBK5XC0", "cdate": 1663849888573, "mdate": null, "content": {"title": "Generative Augmented Flow Networks", "abstract": "The Generative Flow Network is a probabilistic framework where an agent learns a stochastic policy for object generation, such that the probability of generating an object is proportional to a given reward function. Its effectiveness has been shown in discovering high-quality and diverse solutions, compared to reward-maximizing reinforcement learning-based methods. Nonetheless, GFlowNets only learn from rewards of the terminal states, which can limit its applicability. Indeed, intermediate rewards play a critical role in learning, for example from intrinsic motivation to provide intermediate feedback even in particularly challenging sparse reward tasks. Inspired by this, we propose Generative Augmented Flow Networks (GAFlowNets), a novel learning framework to incorporate intermediate rewards into GFlowNets. We specify intermediate rewards by intrinsic motivation to tackle the exploration problem in sparse reward environments. GAFlowNets can leverage edge-based and state-based intrinsic rewards in a joint way to improve exploration. Based on extensive experiments on the GridWorld task, we demonstrate the effectiveness and efficiency of GAFlowNet in terms of convergence, performance, and diversity of solutions. We further show that GAFlowNet is scalable to a more complex and large-scale molecule generation domain, where it achieves consistent and significant performance improvement."}}
{"id": "8LE06pFhqsW", "cdate": 1652737339156, "mdate": null, "content": {"title": "E-MAPP: Efficient Multi-Agent Reinforcement Learning with Parallel Program Guidance", "abstract": "A critical challenge in multi-agent reinforcement learning(MARL) is for multiple agents to efficiently accomplish complex, long-horizon tasks. The agents often have difficulties in cooperating on common goals, dividing complex tasks, and planning through several stages to make progress. We propose to address these challenges by guiding agents with programs designed for parallelization, since programs as a representation contain rich structural and semantic information, and are widely used as abstractions for long-horizon tasks. \nSpecifically, we introduce Efficient Multi-Agent Reinforcement Learning with Parallel Program Guidance(E-MAPP), a novel framework that leverages parallel programs to guide multiple agents to efficiently accomplish goals that require planning over $10+$ stages. \nE-MAPP integrates the structural information from a parallel program, promotes the cooperative behaviors grounded in program semantics, and improves the time efficiency via a task allocator. We conduct extensive experiments on a series of challenging, long-horizon cooperative tasks in the Overcooked environment. Results show that E-MAPP outperforms strong baselines in terms of the completion rate, time efficiency, and zero-shot generalization ability by a large margin."}}
{"id": "iFLexyO1dk", "cdate": 1640995200000, "mdate": 1681650305930, "content": {"title": "Plan Better Amid Conservatism: Offline Multi-Agent Reinforcement Learning with Actor Rectification", "abstract": ""}}
{"id": "iCLfioWkpc", "cdate": 1640995200000, "mdate": 1681490137050, "content": {"title": "E-MAPP: Efficient Multi-Agent Reinforcement Learning with Parallel Program Guidance", "abstract": ""}}
{"id": "fo4yKdB0qe", "cdate": 1640995200000, "mdate": 1681650305685, "content": {"title": "RLx2: Training a Sparse Deep Reinforcement Learning Model from Scratch", "abstract": ""}}
