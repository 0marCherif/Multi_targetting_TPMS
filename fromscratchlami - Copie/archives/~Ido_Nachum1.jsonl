{"id": "pcCBcVUFyY", "cdate": 1640995200000, "mdate": 1682254866695, "content": {"title": "A Johnson-Lindenstrauss Framework for Randomly Initialized CNNs", "abstract": "How does the geometric representation of a dataset change after the application of each randomly initialized layer of a neural network? The celebrated Johnson-Lindenstrauss lemma answers this question for linear fully-connected neural networks (FNNs), stating that the geometry is essentially preserved. For FNNs with the ReLU activation, the angle between two input contracts according to a known mapping. The question for non-linear convolutional neural networks (CNNs) becomes much more intricate. To answer this question, we introduce a geometric framework. For linear CNNs, we show that the Johnson--Lindenstrauss lemma continues to hold, namely, that the angle between two inputs is preserved. For CNNs with ReLU activation, on the other hand, the behavior is richer: The angle between the outputs contracts, where the level of contraction depends on the nature of the inputs. In particular, after one layer, the geometry of natural images is essentially preserved, whereas for Gaussian correlated inputs, CNNs exhibit the same contracting behavior as FNNs with ReLU activation."}}
{"id": "f-1CR7mx138", "cdate": 1640995200000, "mdate": 1683880814832, "content": {"title": "Finite Littlestone Dimension Implies Finite Information Complexity", "abstract": "We prove that every online learnable class of functions of Littlestone dimension d admits a learning algorithm with finite information complexity. Towards this end, we use the notion of a globally stable algorithm. Generally, the information complexity of such a globally stable algorithm is large yet finite, roughly exponential in d. We also show there is room for improvement; for a canonical online learnable class, indicator functions of affine subspaces of dimension d, the information complexity can be upper bounded logarithmically in d."}}
{"id": "YX0lrvdPQc", "cdate": 1632875616347, "mdate": null, "content": {"title": "A Johnson-Lindenstrauss Framework for Randomly Initialized CNNs", "abstract": "How does the geometric representation of a dataset change after the application of each randomly initialized layer of a neural network? The celebrated Johnson-Lindenstrauss lemma answers this question for linear fully-connected neural networks (FNNs), stating that the geometry is essentially preserved. For FNNs with the ReLU activation, the angle between two input contracts according to a known mapping. The question for non-linear convolutional neural networks (CNNs) becomes much more intricate. To answer this question, we introduce a geometric framework. For linear CNNs, we show that the Johnson--Lindenstrauss lemma continues to hold, namely, that the angle between two inputs is preserved. For CNNs with ReLU activation, on the other hand, the behavior is richer: The angle between the outputs contracts, where the level of contraction depends on the nature of the inputs. In particular, after one layer, the geometry of natural images is essentially preserved, whereas for Gaussian correlated inputs, CNNs exhibit the same contracting behavior as FNNs with ReLU activation. "}}
{"id": "oSaxCWkqXP", "cdate": 1609459200000, "mdate": 1682254866705, "content": {"title": "A Johnson-Lindenstrauss Framework for Randomly Initialized CNNs", "abstract": "How does the geometric representation of a dataset change after the application of each randomly initialized layer of a neural network? The celebrated Johnson--Lindenstrauss lemma answers this question for linear fully-connected neural networks (FNNs), stating that the geometry is essentially preserved. For FNNs with the ReLU activation, the angle between two inputs contracts according to a known mapping. The question for non-linear convolutional neural networks (CNNs) becomes much more intricate. To answer this question, we introduce a geometric framework. For linear CNNs, we show that the Johnson--Lindenstrauss lemma continues to hold, namely, that the angle between two inputs is preserved. For CNNs with ReLU activation, on the other hand, the behavior is richer: The angle between the outputs contracts, where the level of contraction depends on the nature of the inputs. In particular, after one layer, the geometry of natural images is essentially preserved, whereas for Gaussian correlated inputs, CNNs exhibit the same contracting behavior as FNNs with ReLU activation."}}
{"id": "LazZtYdkJp", "cdate": 1609459200000, "mdate": 1682254866703, "content": {"title": "Almost-Reed-Muller Codes Achieve Constant Rates for Random Errors", "abstract": "This paper considers \u201c <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\delta $ </tex-math></inline-formula> -almost Reed\u2013Muller codes\u201d, i.e., linear codes spanned by evaluations of all but a <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\delta $ </tex-math></inline-formula> fraction of monomials of degree at most <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$d$ </tex-math></inline-formula> . It is shown that for any <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\delta &gt; 0$ </tex-math></inline-formula> and any <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\varepsilon &gt;0$ </tex-math></inline-formula> , there exists a family of <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\delta $ </tex-math></inline-formula> -almost Reed\u2013Muller codes of constant rate that correct <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$1/2- \\varepsilon $ </tex-math></inline-formula> fraction of random errors with high probability. For exact Reed\u2013Muller codes, the analogous result is not known and represents a weaker version of the longstanding conjecture that Reed\u2013Muller codes achieve capacity for random errors (Abbe-Shpilka-Wigderson STOC \u201915). Our proof is based on the recent polarization result for Reed\u2013Muller codes, combined with a combinatorial approach to establishing inequalities between the Reed\u2013Muller code entropies."}}
{"id": "08zVClflkh", "cdate": 1609459200000, "mdate": 1682254866711, "content": {"title": "Regularization by Misclassification in ReLU Neural Networks", "abstract": "We study the implicit bias of ReLU neural networks trained by a variant of SGD where at each step, the label is changed with probability $p$ to a random label (label smoothing being a close variant of this procedure). Our experiments demonstrate that label noise propels the network to a sparse solution in the following sense: for a typical input, a small fraction of neurons are active, and the firing pattern of the hidden layers is sparser. In fact, for some instances, an appropriate amount of label noise does not only sparsify the network but further reduces the test error. We then turn to the theoretical analysis of such sparsification mechanisms, focusing on the extremal case of $p=1$. We show that in this case, the network withers as anticipated from experiments, but surprisingly, in different ways that depend on the learning rate and the presence of bias, with either weights vanishing or neurons ceasing to fire."}}
{"id": "jwFK-rtbv-Q", "cdate": 1577836800000, "mdate": 1682254866767, "content": {"title": "Almost-Reed-Muller Codes Achieve Constant Rates for Random Errors", "abstract": "This paper considers '$\\delta$-almost Reed-Muller codes', i.e., linear codes spanned by evaluations of all but a $\\delta$ fraction of monomials of degree at most $d$. It is shown that for any $\\delta > 0$ and any $\\varepsilon>0$, there exists a family of $\\delta$-almost Reed-Muller codes of constant rate that correct $1/2-\\varepsilon$ fraction of random errors with high probability. For exact Reed-Muller codes, the analogous result is not known and represents a weaker version of the longstanding conjecture that Reed-Muller codes achieve capacity for random errors (Abbe-Shpilka-Wigderson STOC '15). Our approach is based on the recent polarization result for Reed-Muller codes, combined with a combinatorial approach to establishing inequalities between the Reed-Muller code entropies."}}
{"id": "XkidbVNLXl", "cdate": 1577836800000, "mdate": 1683880814823, "content": {"title": "On Symmetry and Initialization for Neural Networks", "abstract": "This work provides an additional step in the theoretical understanding of neural networks. We consider neural networks with one hidden layer and show that when learning symmetric functions, one can choose initial conditions so that standard SGD training efficiently produces generalization guarantees. We empirically verify this and show that this does not hold when the initial conditions are chosen at random. The proof of convergence investigates the interaction between the two layers of the network. Our results highlight the importance of using symmetry in the design of neural networks."}}
{"id": "BT-d3W71WFs", "cdate": 1577836800000, "mdate": null, "content": {"title": "On the Perceptron's Compression", "abstract": "We study and provide exposition to several phenomena that are related to the perceptron\u2019s compression. One theme concerns modifications of the perceptron algorithm that yield better guarantees on the margin of the hyperplane it outputs. These modifications can be useful in training neural networks as well, and we demonstrate them with some experimental data. In a second theme, we deduce conclusions from the perceptron\u2019s compression in various contexts."}}
{"id": "Skeh-xBYDH", "cdate": 1569439747655, "mdate": null, "content": {"title": "On Symmetry and Initialization for Neural Networks", "abstract": "This work provides an additional step in the theoretical understanding of neural networks. We consider neural networks with one hidden layer and show that when learning symmetric functions, one can choose initial conditions so that standard SGD training efficiently produces generalization guarantees. We empirically verify this and show that this does not hold when the initial conditions are chosen at random. The proof of convergence investigates the interaction between the two layers of the network. Our results highlight the importance of using symmetry in the design of neural networks."}}
