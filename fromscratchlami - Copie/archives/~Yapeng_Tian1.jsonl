{"id": "Oaf30RP-bBi", "cdate": 1668768506968, "mdate": 1668768506968, "content": {"title": "Transformer-empowered Multi-scale Contextual Matching and Aggregation for Multi-contrast MRI Super-resolution", "abstract": "Magnetic resonance imaging (MRI) can present multi- contrast images of the same anatomical structures, enabling multi-contrast super-resolution (SR) techniques. Compared with SR reconstruction using a single-contrast, multi- contrast SR reconstruction is promising to yield SR images with higher quality by leveraging diverse yet complementary information embedded in different imaging modalities. However, existing methods still have two shortcomings: (1) they neglect that the multi-contrast features at different scales contain different anatomical details and hence lack effective mechanisms to match and fuse these features for better reconstruction; and (2) they are still deficient in capturing long-range dependencies, which are essential for the regions with complicated anatomical structures. We propose a novel network to comprehensively address these problems by developing a set of innovative Transformer- empowered multi-scale contextual matching and aggregation techniques; we call it McMRSR. Firstly, we tame trans- formers to model long-range dependencies in both reference and target images. Then, a new multi-scale contextual matching method is proposed to capture corresponding contexts from reference features at different scales. Further- more, we introduce a multi-scale aggregation mechanism to gradually and interactively aggregate multi-scale matched features for reconstructing the target SR MR image. Extensive experiments demonstrate that our network outperforms state-of-the-art approaches and has great potential to be applied in clinical practice."}}
{"id": "h8T5dZWTZ-Z", "cdate": 1663849973964, "mdate": null, "content": {"title": "Basic Binary Convolution Unit for Binarized Image Restoration Network", "abstract": "Lighter and faster image restoration (IR) models are crucial for the deployment on resource-limited devices. Binary neural network (BNN), one of the most promising model compression methods, can dramatically reduce the computations and parameters of full-precision convolutional neural networks (CNN). However, there are different properties between BNN and full-precision CNN, and we can hardly use the experience of designing CNN to develop BNN. In this study, we reconsider components in binary convolution, such as residual connection, BatchNorm, activation function, and structure, for IR tasks. We conduct systematic analyses to explain each component's role in binary convolution and discuss the pitfalls. Specifically, we find that residual connection can reduce the information loss caused by binarization; BatchNorm can solve the value range gap between residual connection and binary convolution; The position of the activation function dramatically affects the performance of BNN. Based on our findings and analyses, we design a simple yet efficient basic binary convolution unit (BBCU). Furthermore, we divide IR networks into four parts and specially design variants of BBCU for each part to explore the benefit of binarizing these parts. We conduct experiments on different IR tasks, and our BBCU significantly outperforms other BNNs and lightweight models, which shows that BBCU can serve as a basic unit for binarized IR networks. All codes and models will be released."}}
{"id": "Fg3mYW8owg", "cdate": 1663849864732, "mdate": null, "content": {"title": "Knowledge Distillation based Degradation Estimation for Blind Super-Resolution", "abstract": "Blind image super-resolution (Blind-SR) aims to recover a high-resolution (HR) image from its corresponding low-resolution (LR) input image with unknown degradations. Most of the existing works design an explicit degradation estimator for each degradation to guide SR. However, it is infeasible to provide concrete labels of multiple degradation combinations (\\eg, blur, noise, jpeg compression) to supervise the degradation estimator training. In addition, these special designs for certain degradation, such as blur, impedes the models from being generalized to handle different degradations. To this end, it is necessary to design an implicit degradation estimator that can extract discriminative degradation representation for all degradations without relying on the supervision of degradation ground-truth. In this paper, we propose a Knowledge Distillation based Blind-SR network (KDSR). It consists of a knowledge distillation based implicit degradation estimator network (KD-IDE) and an efficient SR network. To learn the KDSR model, we first train a teacher network: KD-IDE$_{T}$. It takes paired HR and LR patches as inputs and is optimized with the SR network jointly. Then, we further train a student network KD-IDE$_{S}$, which only takes LR images as input and learns to extract the same implicit degradation representation (IDR) as KD-IDE$_{T}$. In addition, to fully use extracted IDR, we design a simple, strong, and efficient IDR based dynamic convolution residual block (IDR-DCRB) to build an SR network. We conduct extensive experiments under classic and real-world degradation settings. The results show that KDSR achieves SOTA performance and can generalize to various degradation processes. The source codes and pre-trained models will be released."}}
{"id": "k8nG8lWMMjn", "cdate": 1663849822366, "mdate": null, "content": {"title": "Semantic Grouping Network for Audio Source Separation", "abstract": "Audio source separation is a typical and challenging problem that aims to separate individual sources from a mixture of audios. Recently, audio-visual separation approaches take advantage of the natural synchronization between the two modalities to boost separation performance. They extracted high-level semantics from visual inputs as the guidance to help disentangle sound representation for individual sources. Can we directly learn to disentangle the individual semantics from the sound itself? The dilemma is that multiple sound sources are mixed together in the original space. To tackle the difficulty, in this paper, we present a novel Semantic Grouping Network, termed as SGN, that can directly disentangle sound representations and extract high-level semantic information for each source from input audio mixture. Specifically, SGN aggregates category-wise source features through learnable class tokens of sounds. Then, the aggregated semantic features can be used as the guidance to separate the corresponding audio sources from the mixture. The proposed audio source separation framework is simple, flexible, and scalable. Comparing to the existing sound separation methods, our new framework can support audio separation from a flexible number of sources and is capable of generalizing to handle sound sources from different domains. We conducted extensive experiments on both music-only and universal sound separation benchmarks: MUSIC and FUSS. The results demonstrate that our SGN significantly outperforms previous audio-only methods and audio-visual models without utilizing additional visual cues."}}
{"id": "vgqhdIycB7", "cdate": 1661229682011, "mdate": 1661229682011, "content": {"title": "Learning to Answer Questions in Dynamic Audio-Visual Scenarios", "abstract": "In this paper, we focus on the Audio-Visual Question Answering (AVQA) task, which aims to answer questions\nregarding different visual objects, sounds, and their associations in videos. The problem requires comprehensive multimodal understanding and spatio-temporal reasoning over audio-visual scenes. To benchmark this task and facilitate our study, we introduce a large-scale MUSICAVQA dataset, which contains more than 45K questionanswer pairs covering 33 different question templates spanning over different modalities and question types. We develop several baselines and introduce a spatio-temporal grounded audio-visual network for the AVQA problem. Our results demonstrate that AVQA benefits from multisensory perception and our model outperforms recent A-, V-, and\nAVQA approaches. We believe that our built dataset has the potential to serve as testbed for evaluating and promoting progress in audio-visual scene understanding and spatio-temporal reasoning. Code and dataset: http://gewulab.github.io/MUSIC-AVQA/"}}
{"id": "kel5y90xuTf", "cdate": 1661229546746, "mdate": 1661229546746, "content": {"title": "Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video Parsing", "abstract": "In this paper, we introduce a new problem, named audiovisual video parsing, which aims to parse a video into temporal event\nsegments and label them as either audible, visible, or both. Such a problem is essential for a complete understanding of the scene depicted inside a video. To facilitate exploration, we collect a Look, Listen, and Parse (LLP) dataset to investigate audio-visual video parsing in a weakly-supervised manner. This task can be naturally formulated as a Multimodal Multiple Instance Learning (MMIL) problem. Concretely, we propose a novel hybrid attention network to explore unimodal and cross-modal temporal contexts simultaneously. We develop an attentive MMIL pooling method to adaptively explore useful audio and visual content from different temporal extent and modalities. Furthermore, we discover and mitigate modality bias and noisy label issues with an individual-guided learning mechanism\nand label smoothing technique, respectively. Experimental results show that the challenging audio-visual video parsing can be achieved even with only video-level weak labels. Our proposed framework can effectively leverage unimodal and cross-modal temporal contexts and alleviate modality bias and noisy labels problems"}}
{"id": "zfo2LqFEVY", "cdate": 1652737301350, "mdate": null, "content": {"title": "Multi-modal Grouping Network for Weakly-Supervised Audio-Visual Video Parsing", "abstract": "The audio-visual video parsing task aims to parse a video into modality- and category-aware temporal segments. Previous work mainly focuses on weakly-supervised approaches, which learn from video-level event labels. During training, they do not know which modality perceives and meanwhile which temporal segment contains the video event. Since there is no explicit grouping in the existing frameworks, the modality and temporal uncertainties make these methods suffer from false predictions. For instance, segments in the same category could be predicted in different event classes. Learning compact and discriminative multi-modal subspaces is essential for mitigating the issue. To this end, in this paper, we propose a novel Multi-modal Grouping Network, namely MGN, for explicitly semantic-aware grouping. Specifically, MGN aggregates event-aware unimodal features through unimodal grouping in terms of learnable categorical embedding tokens. Furthermore, it leverages the cross-modal grouping for modality-aware prediction to match the video-level target. Our simple framework achieves improving results against previous baselines on weakly-supervised audio-visual video parsing. In addition, our MGN is much more lightweight, using only 47.2% of the parameters of baselines (17 MB vs. 36 MB). Code is available at https://github.com/stoneMo/MGN."}}
{"id": "r_ueBdINfXq", "cdate": 1648670317143, "mdate": 1648670317143, "content": {"title": "TDAN: Temporally-Deformable Alignment Network for Video Super-Resolution", "abstract": "Video super-resolution (VSR) aims to restore a photorealistic high-resolution (HR) video frame from both its\ncorresponding low-resolution (LR) frame (reference frame)\nand multiple neighboring frames (supporting frames). Due\nto varying motion of cameras or objects, the reference\nframe and each support frame are not aligned. Therefore,\ntemporal alignment is a challenging yet important problem for VSR. Previous VSR methods usually utilize optical flow between the reference frame and each supporting\nframe to warp the supporting frame for temporal alignment. However, both inaccurate flow and the image-level\nwarping strategy will lead to artifacts in the warped supporting frames. To overcome the limitation, we propose a\ntemporally-deformable alignment network (TDAN) to adaptively align the reference frame and each supporting frame\nat the feature level without computing optical flow. The\nTDAN uses features from both the reference frame and each\nsupporting frame to dynamically predict offsets of sampling convolution kernels. By using the corresponding kernels, TDAN transforms supporting frames to align with the\nreference frame. To predict the HR video frame, a reconstruction network taking aligned frames and the reference frame is utilized. Experimental results demonstrate\nthat the TDAN is capable of alleviating occlusions and artifacts for temporal alignment and the TDAN-based VSR\nmodel outperforms several recent state-of-the-art VSR networks with a comparable or even much smaller model size."}}
{"id": "H0xgffLVfX5", "cdate": 1648670217688, "mdate": 1648670217688, "content": {"title": "Cyclic Co-Learning of Sounding Object Visual Grounding and Sound Separation", "abstract": "There are rich synchronized audio and visual events in\nour daily life. Inside the events, audio scenes are associated\nwith the corresponding visual objects; meanwhile, sounding\nobjects can indicate and help to separate their individual\nsounds in the audio track. Based on this observation, in this\npaper, we propose a cyclic co-learning (CCoL) paradigm\nthat can jointly learn sounding object visual grounding and\naudio-visual sound separation in a unified framework. Concretely, we can leverage grounded object-sound relations\nto improve the results of sound separation. Meanwhile,\nbenefiting from discriminative information from separated\nsounds, we improve training example sampling for sounding object grounding, which builds a co-learning cycle for\nthe two tasks and makes them mutually beneficial. Extensive experiments show that the proposed framework outperforms the compared recent approaches on both tasks, and\nthey can benefit from each other with our cyclic co-learning."}}
{"id": "rKbZc6rNf7c", "cdate": 1648670146381, "mdate": 1648670146381, "content": {"title": "Can audio-visual integration strengthen robustness under multimodal attacks?", "abstract": "In this paper, we propose to make a systematic study\non machines\u2019 multisensory perception under attacks. We\nuse the audio-visual event recognition task against multimodal adversarial attacks as a proxy to investigate the robustness of audio-visual learning. We attack audio, visual,\nand both modalities to explore whether audio-visual integration still strengthens perception and how different fusion\nmechanisms affect the robustness of audio-visual models.\nFor interpreting the multimodal interactions under attacks,\nwe learn a weakly-supervised sound source visual localization model to localize sounding regions in videos. To mitigate multimodal attacks, we propose an audio-visual defense approach based on an audio-visual dissimilarity constraint and external feature memory banks. Extensive experiments demonstrate that audio-visual models are susceptible to multimodal adversarial attacks; audio-visual\nintegration could decrease the model robustness rather\nthan strengthen under multimodal attacks; even a weakly supervised sound source visual localization model can be\nsuccessfully fooled; our defense method can improve the invulnerability of audio-visual networks without significantly\nsacrificing clean model performance."}}
