{"id": "xvLWypz8p8", "cdate": 1652737364760, "mdate": null, "content": {"title": "On Margins and Generalisation for Voting Classifiers", "abstract": "We study the generalisation properties of majority voting on finite ensembles of classifiers, proving margin-based generalisation bounds via the PAC-Bayes theory. These provide state-of-the-art guarantees on a number of classification tasks. Our central results leverage the Dirichlet posteriors studied recently by Zantedeschi et al. (2021) for training voting classifiers; in contrast to that work our bounds apply to non-randomised votes via the use of margins. Our contributions add perspective to the debate on the ``margins theory'' proposed by Schapire et al. (1998) for the generalisation of ensemble classifiers."}}
{"id": "qxvlOypu7db", "cdate": 1640995200000, "mdate": 1684343222844, "content": {"title": "On Margins and Generalisation for Voting Classifiers", "abstract": "We study the generalisation properties of majority voting on finite ensembles of classifiers, proving margin-based generalisation bounds via the PAC-Bayes theory. These provide state-of-the-art guarantees on a number of classification tasks. Our central results leverage the Dirichlet posteriors studied recently by Zantedeschi et al. (2021) for training voting classifiers; in contrast to that work our bounds apply to non-randomised votes via the use of margins. Our contributions add perspective to the debate on the ``margins theory'' proposed by Schapire et al. (1998) for the generalisation of ensemble classifiers."}}
{"id": "o0kxq-RmNJ6", "cdate": 1640995200000, "mdate": 1682334411397, "content": {"title": "Tighter PAC-Bayes Generalisation Bounds by Leveraging Example Difficulty", "abstract": "We introduce a modified version of the excess risk, which can be used to obtain tighter, fast-rate PAC-Bayesian generalisation bounds. This modified excess risk leverages information about the relative hardness of data examples to reduce the variance of its empirical counterpart, tightening the bound. We combine this with a new bound for $[-1, 1]$-valued (and potentially non-independent) signed losses, which is more favourable when they empirically have low variance around $0$. The primary new technical tool is a novel result for sequences of interdependent random vectors which may be of independent interest. We empirically evaluate these new bounds on a number of real-world datasets."}}
{"id": "m62cKi-lYVS", "cdate": 1640995200000, "mdate": 1682334411127, "content": {"title": "On Margins and Derandomisation in PAC-Bayes", "abstract": "We give a general recipe for derandomising PAC-Bayesian bounds using margins, with the critical ingredient being that our randomised predictions concentrate around some value. The tools we develop straightforwardly lead to margin bounds for various classifiers, including linear prediction\u2014a class that includes boosting and the support vector machine\u2014single-hidden-layer neural networks with an unusual erf activation function, and deep ReLU networks. Further we extend to partially-derandomised predictors where only some of the randomness of our estimators is removed, letting us extend bounds to cases where the concentration properties of our estimators are otherwise poor."}}
{"id": "S-EWYcO5RZq", "cdate": 1640995200000, "mdate": 1647384720612, "content": {"title": "Non-Vacuous Generalisation Bounds for Shallow Neural Networks", "abstract": "We focus on a specific class of shallow neural networks with a single hidden layer, namely those with $L_2$-normalised data and either a sigmoid-shaped Gaussian error function (\"erf\") activation or a Gaussian Error Linear Unit (GELU) activation. For these networks, we derive new generalisation bounds through the PAC-Bayesian theory; unlike most existing such bounds they apply to neural networks with deterministic rather than randomised parameters. Our bounds are empirically non-vacuous when the network is trained with vanilla stochastic gradient descent on MNIST and Fashion-MNIST."}}
{"id": "PZSfC3b0e7g", "cdate": 1640995200000, "mdate": 1682334411061, "content": {"title": "On Margins and Generalisation for Voting Classifiers", "abstract": "We study the generalisation properties of majority voting on finite ensembles of classifiers, proving margin-based generalisation bounds via the PAC-Bayes theory. These provide state-of-the-art guarantees on a number of classification tasks. Our central results leverage the Dirichlet posteriors studied recently by Zantedeschi et al. [2021] for training voting classifiers; in contrast to that work our bounds apply to non-randomised votes via the use of margins. Our contributions add perspective to the debate on the \"margins theory\" proposed by Schapire et al. [1998] for the generalisation of ensemble classifiers."}}
{"id": "GzBN0Be99Qp", "cdate": 1640995200000, "mdate": 1684343222843, "content": {"title": "A Note on the Efficient Evaluation of PAC-Bayes Bounds", "abstract": "When utilising PAC-Bayes theory for risk certification, it is usually necessary to estimate and bound the Gibbs risk of the PAC-Bayes posterior. Many works in the literature employ a method for this which requires a large number of passes of the dataset, incurring high computational cost. This manuscript presents a very general alternative which makes computational savings on the order of the dataset size."}}
{"id": "2bqwSVmuVk", "cdate": 1640995200000, "mdate": 1682334411236, "content": {"title": "Non-Vacuous Generalisation Bounds for Shallow Neural Networks", "abstract": "We focus on a specific class of shallow neural networks with a single hidden layer, namely those with $L_2$-normalised data and either a sigmoid-shaped Gaussian error function (\u201cerf\u201d) activation or..."}}
{"id": "S3MZtqd5A-5", "cdate": 1609459200000, "mdate": 1647384720697, "content": {"title": "Differentiable PAC-Bayes Objectives with Partially Aggregated Neural Networks", "abstract": "We make two related contributions motivated by the challenge of training stochastic neural networks, particularly in a PAC\u2013Bayesian setting: (1) we show how averaging over an ensemble of stochastic neural networks enables a new class of partially-aggregated estimators, proving that these lead to unbiased lower-variance output and gradient estimators; (2) we reformulate a PAC\u2013Bayesian bound for signed-output networks to derive in combination with the above a directly optimisable, differentiable objective and a generalisation guarantee, without using a surrogate loss or loosening the bound. We show empirically that this leads to competitive generalisation guarantees and compares favourably to other methods for training such networks. Finally, we note that the above leads to a simpler PAC\u2013Bayesian training scheme for sign-activation networks than previous work."}}
{"id": "BfGVtc_qA-c", "cdate": 1609459200000, "mdate": 1647384720730, "content": {"title": "On Margins and Derandomisation in PAC-Bayes", "abstract": "We give a general recipe for derandomising PAC-Bayesian bounds using margins, with the critical ingredient being that our randomised predictions concentrate around some value. The tools we develop straightforwardly lead to margin bounds for various classifiers, including linear prediction -- a class that includes boosting and the support vector machine -- single-hidden-layer neural networks with an unusual \\(\\erf\\) activation function, and deep ReLU networks. Further, we extend to partially-derandomised predictors where only some of the randomness is removed, letting us extend bounds to cases where the concentration properties of our predictors are otherwise poor."}}
