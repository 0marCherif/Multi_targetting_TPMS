{"id": "w6LsItagj3", "cdate": 1640995200000, "mdate": 1681733005873, "content": {"title": "Learning Sparse Fixed-Structure Gaussian Bayesian Networks", "abstract": "Gaussian Bayesian networks are widely used to model causal interactions among continuous variables. In this work, we study the problem of learning a fixed-structure Gaussian Bayesian network up to a bounded error in total variation distance. We analyze the commonly used node-wise least squares regression LeastSquares and prove that it has the near-optimal sample complexity. We also study a couple of new algorithms for the problem: BatchAvgLeastSquares takes the average of several batches of least squares solutions at each node, so that one can interpolate between the batch size and the number of batches. We show that BatchAvgLeastSquares also has near-optimal sample complexity. CauchyEst takes the median of solutions to several batches of linear systems at each node. We show that the algorithm specialized to polytrees, CauchyEstTree, has near-optimal sample complexity. Experimentally, we show that for uncontaminated, realizable data, the LeastSquares algorithm performs best, but in the presence of contamination or DAG misspecification, CauchyEst/CauchyEstTree and BatchAvgLeastSquares respectively perform better."}}
{"id": "9p9aZPjMZu", "cdate": 1640995200000, "mdate": 1681733005817, "content": {"title": "Perfect Matchings and Quantum Physics: Progress on Krenn's Conjecture", "abstract": "In 2017, Krenn reported that certain problems related to the perfect matchings and colourings of graphs emerge out of studying the constructability of general quantum states using modern photonic technologies. He realized that if we can prove that the \\emph{weighted matching index} of a graph, a parameter defined in terms of perfect matchings and colourings of the graph is at most 2, that could lead to exciting insights on the potential of resources of quantum inference. Motivated by this, he conjectured that the {weighted matching index} of any graph is at most 2. The first result on this conjecture was by Bogdanov, who proved that the \\emph{(unweighted) matching index} of graphs (non-isomorphic to $K_4$) is at most 2, thus classifying graphs non-isomorphic to $K_4$ into Type 0, Type 1 and Type 2. By definition, the weighted matching index of Type 0 graphs is 0. We give a structural characterization for Type 2 graphs, using which we settle Krenn's conjecture for Type 2 graphs. Using this characterization, we provide a simple $O(|V||E|)$ time algorithm to find the unweighted matching index of any graph. In view of our work, Krenn's conjecture remains to be proved only for Type 1 graphs. We give upper bounds for the weighted matching index in terms of connectivity parameters for such graphs. Using these bounds, for a slightly simplified version, we settle Krenn's conjecture for the class of graphs with vertex connectivity at most 2 and the class of graphs with maximum degree at most 4. Krenn has been publicizing his conjecture in various ways since 2017. He has even declared a reward for a resolution of his conjecture. We hope that this article will popularize the problem among computer scientists."}}
{"id": "vvPeb1JL1rp", "cdate": 1609459200000, "mdate": 1681733005817, "content": {"title": "Generalizations of Length Limited Huffman Coding for Hierarchical Memory Settings", "abstract": "In this paper, we study the problem of designing prefix-free encoding schemes having minimum average code length that can be decoded efficiently under a decode cost model that captures memory hierarchy induced cost functions. We also study a special case of this problem that is closely related to the length limited Huffman coding (LLHC) problem; we call this the soft-length limited Huffman coding problem. In this version, there is a penalty associated with each of the n characters of the alphabet whose encodings exceed a specified bound D(\u2264 n) where the penalty increases linearly with the length of the encoding beyond D. The goal of the problem is to find a prefix-free encoding having minimum average code length and total penalty within a pre-specified bound P. This generalizes the LLHC problem. We present an algorithm to solve this problem that runs in time O(nD). We study a further generalization in which the penalty function and the objective function can both be arbitrary monotonically non-decreasing functions of the codeword length. We provide dynamic programming based exact and PTAS algorithms for this setting."}}
{"id": "aGWHGgSf8-E", "cdate": 1609459200000, "mdate": 1681733005811, "content": {"title": "Efficient algorithms for decode efficient prefix codes", "abstract": "The cost of decompressing (decoding) data can be prohibitive for certain real-time applications. In many scenarios, it is acceptable to sacrifice (to some extent) on compression in the interest of fast decoding. We study anovel problem of finding a prefix tree having the best decode time under the constraint that the code length does not exceed a certain threshold for a natural class of memory access cost functions that use blocking (also referred to as lookup tables). We present exact and approximation algorithms for this problem that are based on dynamic programming and capitalize on interesting structures of the optimal solutions. The full version of this paper is available at [1]"}}
{"id": "1g0gPV8P5pb", "cdate": 1609459200000, "mdate": 1681733005825, "content": {"title": "Learning Sparse Fixed-Structure Gaussian Bayesian Networks", "abstract": "Gaussian Bayesian networks (a.k.a. linear Gaussian structural equation models) are widely used to model causal interactions among continuous variables. In this work, we study the problem of learning a fixed-structure Gaussian Bayesian network up to a bounded error in total variation distance. We analyze the commonly used node-wise least squares regression (LeastSquares) and prove that it has a near-optimal sample complexity. We also study a couple of new algorithms for the problem:   - BatchAvgLeastSquares takes the average of several batches of least squares solutions at each node, so that one can interpolate between the batch size and the number of batches. We show that BatchAvgLeastSquares also has near-optimal sample complexity.   - CauchyEst takes the median of solutions to several batches of linear systems at each node. We show that the algorithm specialized to polytrees, CauchyEstTree, has near-optimal sample complexity.   Experimentally, we show that for uncontaminated, realizable data, the LeastSquares algorithm performs best, but in the presence of contamination or DAG misspecification, CauchyEst/CauchyEstTree and BatchAvgLeastSquares respectively perform better."}}
{"id": "c_Mj0TtnbhI", "cdate": 1577836800000, "mdate": 1681733006034, "content": {"title": "Huffman Coding Based Encoding Techniques for Fast Distributed Deep Learning", "abstract": "Distributed stochastic algorithms, equipped with gradient compression techniques, such as codebook quantization, are becoming increasingly popular and considered state-of-the-art in training large deep neural network (DNN) models. However, communicating the quantized gradients in a network requires efficient encoding techniques. For this, practitioners generally use Elias encoding-based techniques without considering their computational overhead or data-volume. In this paper, based on Huffman coding, we propose several lossless encoding techniques that exploit different characteristics of the quantized gradients during distributed DNN training. Then, we show their effectiveness on 5 different DNN models across three different data-sets, and compare them with classic state-of-the-art Elias-based encoding techniques. Our results show that the proposed Huffman-based encoders (i.e., RLH, SH, and SHS) can reduce the encoded data-volume by up to 5.1\u00d7, 4.32\u00d7, and 3.8\u00d7, respectively, compared to the Elias-based encoders."}}
{"id": "aShjETPnpP-", "cdate": 1577836800000, "mdate": 1681733006099, "content": {"title": "Decode-Efficient Prefix Codes for Hierarchical Memory Models", "abstract": "The cost of uncompressing (decoding) data can be prohibitive in certain real-time applications, for example when predicting using compressed deep learning models. In many scenarios, it is acceptable to sacrifice to some extent on compression in the interest of fast decoding. In this work, we are interested in finding the prefix tree having the best decode time under the constraint that the code length does not exceed a certain threshold for a natural class of algorithms under the hierarchical memory model. We present an efficient optimal algorithm for this problem based on a dynamic program that capitalizes on an interesting structure of the optimal solution."}}
{"id": "Db8Px8k1KhU", "cdate": 1577836800000, "mdate": 1681733005803, "content": {"title": "Decode efficient prefix codes", "abstract": "In this paper, we study the problem of designing prefix-free encoding schemes having minimum average code length that can be decoded efficiently under a decode cost model that captures memory hierarchy induced cost functions. We also study a special case of this problem that is closely related to the length limited Huffman coding (LLHC) problem; we call this the {\\em soft-length limited Huffman coding} problem. In this version, there is a penalty associated with each of the $n$ characters of the alphabet whose encodings exceed a specified bound $D$($\\leq n$), where the penalty increases linearly with the length of the encoding beyond $D$. The goal of the problem is to find a prefix-free encoding having minimum average code length and total penalty within a pre-specified bound ${\\cal P}$. This generalizes the LLHC problem. We present an algorithm to solve this problem that runs in time $O( nD )$. We study a further generalization in which the penalty function and the objective function can both be arbitrary monotonically non-decreasing functions of the codeword length. We provide dynamic programming based exact and PTAS algorithms for this setting."}}
