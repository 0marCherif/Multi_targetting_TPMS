{"id": "El13DHXzJf", "cdate": 1672531200000, "mdate": 1679289384986, "content": {"title": "TAMUNA: Accelerated Federated Learning with Local Training and Partial Participation", "abstract": ""}}
{"id": "mejxBCu9EXc", "cdate": 1664731444610, "mdate": null, "content": {"title": "RandProx: Primal-Dual Optimization Algorithms with Randomized Proximal Updates", "abstract": "Proximal splitting algorithms are well suited for large-scale nonsmooth optimization problems. We propose a primal-dual algorithm, in which the dual update is randomized, with the proximity operator of one of the function replaced by a stochastic oracle. A nonsmooth variance-reduction technique is implemented so that the algorithm finds an exact minimizer of the general problem. We derive linear convergence results in presence of strong convexity. Several existing randomized algorithms, like Point-SAGA, are recovered as particular cases. Randomness helps getting faster algorithms; this has long been known for stochastic-gradient-type algorithms, and our work shows that this fully applies in the more general primal-dual setting as well."}}
{"id": "cB4N3G5udUS", "cdate": 1663850541501, "mdate": null, "content": {"title": "RandProx: Primal-Dual Optimization Algorithms with Randomized Proximal Updates", "abstract": "Proximal splitting algorithms are well suited to solving large-scale nonsmooth optimization problems, in particular those arising in machine learning. We propose a new primal\u2013dual algorithm, in which the dual update is randomized; equivalently, the proximity operator of one of the function in the problem is replaced by a stochastic oracle. For instance, some randomly chosen dual variables, instead of all, are updated at each iteration. Or, the proximity operator of a function is called with some small probability only. A nonsmooth variance-reduction technique is implemented so that the algorithm finds an exact minimizer of the general problem involving smooth and nonsmooth functions, possibly composed with linear operators. We derive linear convergence results in presence of strong convexity; these results are new even in the deterministic case, when our algorithms reverts to the recently proposed Primal\u2013Dual Davis\u2013Yin algorithm. Some randomized algorithms of the literature are also recovered as particular cases (e.g., Point-SAGA). But our randomization technique is general and encompasses many unbiased mechanisms beyond sampling and probabilistic updates, including compression. Since the convergence speed depends on the slowest among the primal and dual contraction mechanisms, the iteration complexity might remain the same when randomness is used. On the other hand, the computation complexity can be significantly reduced. Overall, randomness helps getting faster algorithms. This has long been known for stochastic-gradient-type algorithms, and our work shows that this fully applies in the more general primal\u2013dual setting as well."}}
{"id": "PeJO709WUup", "cdate": 1652737500407, "mdate": null, "content": {"title": "EF-BV: A Unified Theory of Error Feedback and Variance Reduction Mechanisms for Biased and Unbiased Compression in Distributed Optimization", "abstract": "In distributed or federated optimization and learning, communication between the different computing units is often the bottleneck and gradient compression is widely used to reduce the number of bits sent within each communication round of iterative methods. There are two classes of compression operators and separate algorithms making use of them. In the case of unbiased random compressors with bounded variance (e.g., rand-k), the DIANA algorithm of Mishchenko et al. (2019), which implements a variance reduction technique for handling the variance introduced by compression, is the current state of the art. In the case of biased and contractive compressors (e.g., top-k), the EF21 algorithm of Richt\u00e1rik et al. (2021), which instead implements an error-feedback mechanism, is the current state of the art. These two classes of compression schemes and algorithms are distinct, with different analyses and proof techniques. In this paper, we unify them into a single framework and propose a new algorithm, recovering DIANA and EF21 as particular cases. Our general approach works with a new, larger class of compressors, which has two parameters, the bias and the variance, and includes unbiased and biased compressors as particular cases. This allows us to inherit the best of the two worlds: like EF21 and unlike DIANA, biased compressors, like top-k, whose good performance in practice is recognized, can be used. And like DIANA and unlike EF21, independent randomness at the compressors allows to mitigate the effects of compression, with the convergence rate improving when the number of parallel workers is large. This is the first time that an algorithm with all these features is proposed. We prove its linear convergence under certain conditions. Our approach takes a step towards better understanding of two so-far distinct worlds of communication-efficient distributed learning."}}
{"id": "-RQQ5cbXVPR", "cdate": 1652457812966, "mdate": 1652457812966, "content": {"title": "From Local SGD to Local Fixed-Point Methods for Federated Learning", "abstract": "Most algorithms for solving optimization prob- lems or finding saddle points of convex\u2013concave functions are fixed-point algorithms. In this work we consider the generic problem of finding a fixed point of an average of operators, or an approxima- tion thereof, in a distributed setting. Our work is motivated by the needs of federated learning. In this context, each local operator models the com- putations done locally on a mobile device. We investigate two strategies to achieve such a con- sensus: one based on a fixed number of local steps, and the other based on randomized computations. In both cases, the goal is to limit communication of the locally-computed variables, which is often the bottleneck in distributed frameworks. We per- form convergence analysis of both methods and conduct a number of experiments highlighting the benefits of our approach."}}
{"id": "w2xra4Zttf", "cdate": 1640995200000, "mdate": 1679289384540, "content": {"title": "Provably Doubly Accelerated Federated Learning: The First Theoretically Successful Combination of Local Training and Compressed Communication", "abstract": ""}}
{"id": "jlRdL8DxJjU", "cdate": 1640995200000, "mdate": 1679289385631, "content": {"title": "An Optimal Algorithm for Strongly Convex Minimization under Affine Constraints", "abstract": ""}}
{"id": "VwblaK-N6tU", "cdate": 1640995200000, "mdate": 1679289384553, "content": {"title": "Dualize, Split, Randomize: Toward Fast Nonsmooth Optimization Algorithms", "abstract": ""}}
{"id": "UAv9wLmOTC", "cdate": 1640995200000, "mdate": 1667933194997, "content": {"title": "EF-BV: A Unified Theory of Error Feedback and Variance Reduction Mechanisms for Biased and Unbiased Compression in Distributed Optimization", "abstract": "In distributed or federated optimization and learning, communication between the different computing units is often the bottleneck and gradient compression is widely used to reduce the number of bits sent within each communication round of iterative methods. There are two classes of compression operators and separate algorithms making use of them. In the case of unbiased random compressors with bounded variance (e.g., rand-k), the DIANA algorithm of Mishchenko et al. (2019), which implements a variance reduction technique for handling the variance introduced by compression, is the current state of the art. In the case of biased and contractive compressors (e.g., top-k), the EF21 algorithm of Richt\\'arik et al. (2021), which instead implements an error-feedback mechanism, is the current state of the art. These two classes of compression schemes and algorithms are distinct, with different analyses and proof techniques. In this paper, we unify them into a single framework and propose a new algorithm, recovering DIANA and EF21 as particular cases. Our general approach works with a new, larger class of compressors, which has two parameters, the bias and the variance, and includes unbiased and biased compressors as particular cases. This allows us to inherit the best of the two worlds: like EF21 and unlike DIANA, biased compressors, like top-k, whose good performance in practice is recognized, can be used. And like DIANA and unlike EF21, independent randomness at the compressors allows to mitigate the effects of compression, with the convergence rate improving when the number of parallel workers is large. This is the first time that an algorithm with all these features is proposed. We prove its linear convergence under certain conditions. Our approach takes a step towards better understanding of two so-far distinct worlds of communication-efficient distributed learning."}}
{"id": "PaSk9etJee", "cdate": 1640995200000, "mdate": 1679289384961, "content": {"title": "Tikhonov Regularization of Circle-Valued Signals", "abstract": ""}}
