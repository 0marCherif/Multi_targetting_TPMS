{"id": "dg1nMCNmaX0", "cdate": 1672531200000, "mdate": 1692735812858, "content": {"title": "Online Bidding Algorithms for Return-on-Spend Constrained Advertisers\u2731", "abstract": "We study online auto-bidding algorithms for a single advertiser maximizing value under the Return-on-Spend (RoS) constraint, quantifying performance in terms of regret relative to the optimal offline solution that knows all queries a priori. We contribute a simple online algorithm that achieves near-optimal regret in expectation while always respecting the RoS constraint when the input queries are i.i.d. samples from some distribution. Integrating our results with \u00a0[9] achieves near-optimal regret under both RoS and fixed budget constraints. Our algorithm uses the primal-dual framework with online mirror descent (OMD) for the dual updates, and the analysis utilizes new insights into the gradient structure."}}
{"id": "8xccCiF9JQ6", "cdate": 1652737823066, "mdate": null, "content": {"title": "A Fast Scale-Invariant Algorithm for Non-negative Least Squares with Non-negative Data", "abstract": " Nonnegative (linear) least square problems are a fundamental class of problems that is well-studied in statistical learning and for which solvers have been implemented in many of the standard programming languages used within the machine learning community. The existing off-the-shelf solvers view the non-negativity constraint in these problems as an obstacle and, compared to unconstrained least squares, perform additional effort to address it. However, in many of the typical applications, the data itself is nonnegative as well, and we show that the nonnegativity in this case makes the problem easier. In particular, while the worst-case dimension-independent oracle complexity of unconstrained least squares problems necessarily scales with one of the data matrix constants (typically the spectral norm) and these problems are solved to additive error, we show that nonnegative least squares problems with nonnegative data are solvable to  multiplicative error and with complexity that is independent of any matrix constants. The algorithm we introduce is accelerated and based on a primal-dual perspective. We further show how to provably obtain linear convergence using adaptive restart coupled with our method and demonstrate its effectiveness on large-scale data via numerical experiments. "}}
{"id": "X4WAq7JQHbA", "cdate": 1652737746528, "mdate": null, "content": {"title": "Decomposable Non-Smooth Convex Optimization with Nearly-Linear Gradient Oracle Complexity", "abstract": "Many fundamental problems in machine learning can be formulated by the convex program \n\\[ \\min_{\\theta\\in \\mathbb{R}^d}\\ \\sum_{i=1}^{n}f_{i}(\\theta), \\]\nwhere each $f_i$ is a convex, Lipschitz function supported on a subset of $d_i$ coordinates of $\\theta$. One common approach to this problem, exemplified by stochastic gradient descent, involves sampling one $f_i$ term at every iteration to make progress. This approach crucially relies on a notion of uniformity across the $f_i$'s, formally captured by their condition number. In this work, we give an algorithm that minimizes the above convex formulation to $\\epsilon$-accuracy in $\\widetilde{O}(\\sum_{i=1}^n d_i \\log (1 /\\epsilon))$ gradient computations, with no assumptions on the condition number.  The previous best algorithm independent of the condition number is the standard cutting plane method, which requires $O(nd \\log (1/\\epsilon))$ gradient computations. As a corollary, we improve upon the evaluation oracle complexity for decomposable submodular minimization by [Axiotis, Karczmarz, Mukherjee, Sankowski and Vladu, ICML 2021]. Our main technical contribution is an adaptive procedure to select an $f_i$ term at every iteration via a novel combination of cutting-plane and interior-point methods.\n"}}
{"id": "slKVqAflN5", "cdate": 1652737742065, "mdate": null, "content": {"title": "A gradient sampling method with complexity guarantees for Lipschitz functions in high and low dimensions", "abstract": "Zhang et al. (ICML 2020) introduced a novel modification of Goldstein's classical subgradient method, with an efficiency guarantee of $O(\\varepsilon^{-4})$ for minimizing Lipschitz functions. Their work, however, makes use of an oracle that is not efficiently implementable. In this paper, we obtain the same efficiency guarantee with a standard subgradient oracle, thus making our algorithm efficiently implementable. Our resulting method works on any Lipschitz function whose value and gradient can be evaluated at points of differentiability. We additionally present a new cutting plane algorithm that achieves an efficiency of  $O(d\\varepsilon^{-2}\\log S)$ for the class of $S$-smooth (and possibly non-convex) functions in low dimensions. Strikingly, this $\\epsilon$-dependence matches the lower bounds for the convex setting. "}}
{"id": "jtDWK8hiJJt", "cdate": 1640995200000, "mdate": 1652976694970, "content": {"title": "Computing Lewis Weights to High Precision", "abstract": "We present an algorithm for computing approximate \u2113p Lewis weights to high precision. Given a full-rank A \u220a \u211dm \u00d7 n with m \u2265 n and a scalar p > 2, our algorithm computes \u220a-approximate \u2113p Lewis weights of A in \u00d5p(log(1/\u220a)) iterations; the cost of each iteration is linear in the input size plus the cost of computing the leverage scores of DA for diagonal D \u220a \u211dm \u00d7 m. Prior to our work, such a computational complexity was known only for p \u220a (0,4) [CP15], and combined with this result, our work yields the first polylogarithmic-depth polynomial-work algorithm for the problem of computing \u2113p Lewis weights to high precision for all constant p > 0. An important consequence of this result is also the first polylogarithmic-depth polynomial-work algorithm for computing a nearly optimal self-concordant barrier for a polytope."}}
{"id": "gOZOJ0yDEZL", "cdate": 1640995200000, "mdate": 1692735812857, "content": {"title": "Decomposable Non-Smooth Convex Optimization with Nearly-Linear Gradient Oracle Complexity", "abstract": "Many fundamental problems in machine learning can be formulated by the convex program \\[ \\min_{\\theta\\in \\mathbb{R}^d}\\ \\sum_{i=1}^{n}f_{i}(\\theta), \\]where each $f_i$ is a convex, Lipschitz function supported on a subset of $d_i$ coordinates of $\\theta$. One common approach to this problem, exemplified by stochastic gradient descent, involves sampling one $f_i$ term at every iteration to make progress. This approach crucially relies on a notion of uniformity across the $f_i$'s, formally captured by their condition number. In this work, we give an algorithm that minimizes the above convex formulation to $\\epsilon$-accuracy in $\\widetilde{O}(\\sum_{i=1}^n d_i \\log (1 /\\epsilon))$ gradient computations, with no assumptions on the condition number. The previous best algorithm independent of the condition number is the standard cutting plane method, which requires $O(nd \\log (1/\\epsilon))$ gradient computations. As a corollary, we improve upon the evaluation oracle complexity for decomposable submodular minimization by [Axiotis, Karczmarz, Mukherjee, Sankowski and Vladu, ICML 2021]. Our main technical contribution is an adaptive procedure to select an $f_i$ term at every iteration via a novel combination of cutting-plane and interior-point methods."}}
{"id": "erLb90nWCb", "cdate": 1640995200000, "mdate": 1692735812859, "content": {"title": "A Fast Scale-Invariant Algorithm for Non-negative Least Squares with Non-negative Data", "abstract": "Nonnegative (linear) least square problems are a fundamental class of problems that is well-studied in statistical learning and for which solvers have been implemented in many of the standard programming languages used within the machine learning community. The existing off-the-shelf solvers view the non-negativity constraint in these problems as an obstacle and, compared to unconstrained least squares, perform additional effort to address it. However, in many of the typical applications, the data itself is nonnegative as well, and we show that the nonnegativity in this case makes the problem easier. In particular, while the worst-case dimension-independent oracle complexity of unconstrained least squares problems necessarily scales with one of the data matrix constants (typically the spectral norm) and these problems are solved to additive error, we show that nonnegative least squares problems with nonnegative data are solvable to multiplicative error and with complexity that is independent of any matrix constants. The algorithm we introduce is accelerated and based on a primal-dual perspective. We further show how to provably obtain linear convergence using adaptive restart coupled with our method and demonstrate its effectiveness on large-scale data via numerical experiments."}}
{"id": "3ene6vGluFa", "cdate": 1640995200000, "mdate": 1692735812858, "content": {"title": "A gradient sampling method with complexity guarantees for Lipschitz functions in high and low dimensions", "abstract": "Zhang et al. (ICML 2020) introduced a novel modification of Goldstein's classical subgradient method, with an efficiency guarantee of $O(\\varepsilon^{-4})$ for minimizing Lipschitz functions. Their work, however, makes use of an oracle that is not efficiently implementable. In this paper, we obtain the same efficiency guarantee with a standard subgradient oracle, thus making our algorithm efficiently implementable. Our resulting method works on any Lipschitz function whose value and gradient can be evaluated at points of differentiability. We additionally present a new cutting plane algorithm that achieves an efficiency of $O(d\\varepsilon^{-2}\\log S)$ for the class of $S$-smooth (and possibly non-convex) functions in low dimensions. Strikingly, this $\\epsilon$-dependence matches the lower bounds for the convex setting."}}
{"id": "vgbpVFLdC1l", "cdate": 1577836800000, "mdate": null, "content": {"title": "A Faster Interior Point Method for Semidefinite Programming", "abstract": "Semidefinite programs (SDPs) are a fundamental class of optimization problems with important recent applications in approximation algorithms, quantum complexity, robust learning, algorithmic rounding, and adversarial deep learning. This paper presents a faster interior point method to solve generic SDPs with variable size $n \\times n$ and $m$ constraints in time \\begin{align*} \\widetilde{O}(\\sqrt{n}( mn^2 + m^\\omega + n^\\omega) \\log(1 / \\epsilon) ), \\end{align*} where $\\omega$ is the exponent of matrix multiplication and $\\epsilon$ is the relative accuracy. In the predominant case of $m \\geq n$, our runtime outperforms that of the previous fastest SDP solver, which is based on the cutting plane method of Jiang, Lee, Song, and Wong [JLSW20].   Our algorithm's runtime can be naturally interpreted as follows: $\\widetilde{O}(\\sqrt{n} \\log (1/\\epsilon))$ is the number of iterations needed for our interior point method, $mn^2$ is the input size, and $m^\\omega + n^\\omega$ is the time to invert the Hessian and slack matrix in each iteration. These constitute natural barriers to further improving the runtime of interior point methods for solving generic SDPs."}}
{"id": "ng82SThHUS", "cdate": 1577836800000, "mdate": 1683235214382, "content": {"title": "A Faster Interior Point Method for Semidefinite Programming", "abstract": "Semidefinite programs (SDPs) are a fundamental class of optimization problems with important recent applications in approximation algorithms, quantum complexity, robust learning, algorithmic rounding, and adversarial deep learning. This paper presents a faster interior point method to solve generic SDPs with variable size <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$n \\times n$</tex> and m constraints in time \\begin{equation*} \\tilde{O}(\\sqrt{n}(mn^{2}+m^{\\omega}+n^{\\omega})\\log(1/\\epsilon)), \\end{equation*} where <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\omega$</tex> is the exponent of matrix multiplication and <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\epsilon$</tex> is the relative accuracy. In the predominant case of <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$m\\geq n$</tex> , our runtime outperforms that of the previous fastest SDP solver, which is based on the cutting plane method [JLSW20]. Our algorithm's runtime can be naturally interpreted as follows: <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$O(\\sqrt{n}\\log(1/\\epsilon))$</tex> is the number of iterations needed for our interior point method, <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$mn^{2}$</tex> is the input size, and <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$m^{\\omega}+n^{\\omega}$</tex> is the time to invert the Hessian and slack matrix in each iteration. These constitute natural barriers to further improving the runtime of interior point methods for solving generic SDPs."}}
