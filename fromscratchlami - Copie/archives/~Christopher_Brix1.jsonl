{"id": "Vaut_ukFjU", "cdate": 1609459200000, "mdate": 1668207017796, "content": {"title": "Two-Way Neural Machine Translation: A Proof of Concept for Bidirectional Translation Modeling Using a Two-Dimensional Grid", "abstract": ""}}
{"id": "qc4T8sVUI7", "cdate": 1577836800000, "mdate": 1668207017802, "content": {"title": "Two-Way Neural Machine Translation: A Proof of Concept for Bidirectional Translation Modeling using a Two-Dimensional Grid", "abstract": "Neural translation models have proven to be effective in capturing sufficient information from a source sentence and generating a high-quality target sentence. However, it is not easy to get the best effect for bidirectional translation, i.e., both source-to-target and target-to-source translation using a single model. If we exclude some pioneering attempts, such as multilingual systems, all other bidirectional translation approaches are required to train two individual models. This paper proposes to build a single end-to-end bidirectional translation model using a two-dimensional grid, where the left-to-right decoding generates source-to-target, and the bottom-to-up decoding creates target-to-source output. Instead of training two models independently, our approach encourages a single network to jointly learn to translate in both directions. Experiments on the WMT 2018 German$\\leftrightarrow$English and Turkish$\\leftrightarrow$English translation tasks show that the proposed model is capable of generating a good translation quality and has sufficient potential to direct the research."}}
{"id": "VDSo1ayjC9v", "cdate": 1577836800000, "mdate": 1668207017810, "content": {"title": "Debona: Decoupled Boundary Network Analysis for Tighter Bounds and Faster Adversarial Robustness Proofs", "abstract": "Neural networks are commonly used in safety-critical real-world applications. Unfortunately, the predicted output is often highly sensitive to small, and possibly imperceptible, changes to the input data. Proving that either no such adversarial examples exist, or providing a concrete instance, is therefore crucial to ensure safe applications. As enumerating and testing all potential adversarial examples is computationally infeasible, verification techniques have been developed to provide mathematically sound proofs of their absence using overestimations of the network activations. We propose an improved technique for computing tight upper and lower bounds of these node values, based on increased flexibility gained by computing both bounds independently of each other. Furthermore, we gain an additional improvement by re-implementing part of the original state-of-the-art software \"Neurify\", leading to a faster analysis. Combined, these adaptations reduce the necessary runtime by up to 94%, and allow a successful search for networks and inputs that were previously too complex. We provide proofs for tight upper and lower bounds on max-pooling layers in convolutional networks. To ensure widespread usability, we open source our implementation \"Debona\", featuring both the implementation specific enhancements as well as the refined boundary computation for faster and more exact~results."}}
{"id": "RAjWi0s8zo_", "cdate": 1577836800000, "mdate": 1668207017805, "content": {"title": "Successfully Applying the Stabilized Lottery Ticket Hypothesis to the Transformer Architecture", "abstract": ""}}
{"id": "HJ-WvzG_ZH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Towards Two-Dimensional Sequence to Sequence Model in Neural Machine Translation", "abstract": "This work investigates an alternative model for neural machine translation (NMT) and proposes a novel architecture, where we employ a multi-dimensional long short-term memory (MDLSTM) for translation modeling. In the state-of-the-art methods, source and target sentences are treated as one-dimensional sequences over time, while we view translation as a two-dimensional (2D) mapping using an MDLSTM layer to define the correspondence between source and target words. We extend beyond the current sequence to sequence backbone NMT models to a 2D structure in which the source and target sentences are aligned with each other in a 2D grid. Our proposed topology shows consistent improvements over attention-based sequence to sequence model on two WMT 2017 tasks, German\\leftrightarrow$English."}}
{"id": "NPL25_PXvz2", "cdate": 1483228800000, "mdate": 1668207017827, "content": {"title": "Empirical Investigation of Optimization Algorithms in Neural Machine Translation", "abstract": ""}}
