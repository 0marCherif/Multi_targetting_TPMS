{"id": "9cTEQWMo1BF", "cdate": 1686324855207, "mdate": null, "content": {"title": "LabelFormer: Object Trajectory Re\ufb01nement for Offboard Perception from LiDAR Point Clouds", "abstract": "A major bottleneck to scaling-up training of self-driving perception systems are the human annotations required for supervision. A promising alternative is to leverage \u201cauto-labelling\u201d offboard perception models that are trained to automatically generate annotations from raw LiDAR point clouds at a fraction of the cost. Auto-labels are most commonly generated via a two-stage approach \u2013 first objects are detected and tracked over time, and then each object trajectory is passed to a learned refinement model to improve accuracy. Since existing refinement models are overly complex and lack advanced temporal reasoning capabilities, in this work we propose LabelFormer, a simple, efficient, and effective trajectory-level refinement approach. Our approach first encodes each frame\u2019s observations separately, then exploits self-attention to reason about the trajectory with full temporal context, and finally decodes the re\ufb01ned object size and per-frame poses. Evaluation on both urban and highway datasets demonstrates that LabelFormer outperforms existing works by a large margin. Finally, we show that training on a dataset augmented with auto-labels generated by our method leads to improved downstream detection performance compared to existing methods. Please visit the project website for details https://waabi.ai/labelformer/."}}
{"id": "TFbwV6I0VLg", "cdate": 1663850026315, "mdate": null, "content": {"title": "SlotFormer: Unsupervised Visual Dynamics Simulation with Object-Centric Models", "abstract": "Understanding dynamics from visual observations is a challenging problem that requires disentangling individual objects from the scene and learning their interactions. While recent object-centric models can successfully decompose a scene into objects, modeling their dynamics effectively still remains a challenge. We address this problem by introducing SlotFormer -- a Transformer-based autoregressive model operating on learned object-centric representations. Given a video clip, our approach reasons over object features to model spatio-temporal relationships and predicts accurate future object states. In this paper, we successfully apply SlotFormer to perform video prediction on datasets with complex object interactions. Moreover, the unsupervised SlotFormer's dynamics model can be used to improve the performance on supervised downstream tasks, such as Visual Question Answering (VQA), and goal-conditioned planning. Compared to past works on dynamics modeling, our method achieves significantly better long-term synthesis of object dynamics, while retaining high quality visual generation. Besides, SlotFormer enables VQA models to reason about the future without object-level labels, even outperforming counterparts that use ground-truth annotations. Finally, we show its ability to serve as a world model for model-based planning, which is competitive with methods designed specifically for such tasks."}}
{"id": "TTeMp6953v4", "cdate": 1654886253015, "mdate": null, "content": {"title": "SlotFormer: Long-Term Dynamic Modeling in Object-Centric Models", "abstract": "Understanding dynamics from visual observations is a challenging problem that requires disentangling individual objects from the scene and learning their interactions. While recent object-centric models can successfully decompose a scene into objects, modeling their dynamics effectively still remains a challenge. We address this problem by introducing SlotFormer - a Transformer-based autoregressive\nmodel operating on learned object-centric representations. Given a video clip, our approach performs dynamic reasoning over object features to model spatial-temporal object relationships and generate realistic future frames. In this paper, we successfully apply SlotFormer to the problem of consistent long-term dynamic modeling in object-centric models. We compare SlotFormer to image-based video prediction models and object-centric dynamic models on two synthetic video datasets consisting of complex object interactions. Our method generates videos of high quality as measured by conventional video prediction metrics, while achieving significantly better long-term synthesis of object dynamics."}}
{"id": "xzU2TEWhwm", "cdate": 1640995200000, "mdate": 1682362804753, "content": {"title": "SAGE: Saliency-Guided Mixup with Optimal Rearrangements", "abstract": ""}}
{"id": "b58C89EPjv", "cdate": 1640995200000, "mdate": 1668189403565, "content": {"title": "Graph2Vid: Flow graph to Video Grounding for Weakly-supervised Multi-Step Localization", "abstract": ""}}
{"id": "b4qlmy2G7q-", "cdate": 1640995200000, "mdate": 1668189403717, "content": {"title": "Flow Graph to Video Grounding for Weakly-Supervised Multi-step Localization", "abstract": ""}}
{"id": "I3Cjf2Wd6-", "cdate": 1640995200000, "mdate": 1667337328210, "content": {"title": "SlotFormer: Unsupervised Visual Dynamics Simulation with Object-Centric Models", "abstract": "Understanding dynamics from visual observations is a challenging problem that requires disentangling individual objects from the scene and learning their interactions. While recent object-centric models can successfully decompose a scene into objects, modeling their dynamics effectively still remains a challenge. We address this problem by introducing SlotFormer -- a Transformer-based autoregressive model operating on learned object-centric representations. Given a video clip, our approach reasons over object features to model spatio-temporal relationships and predicts accurate future object states. In this paper, we successfully apply SlotFormer to perform video prediction on datasets with complex object interactions. Moreover, the unsupervised SlotFormer's dynamics model can be used to improve the performance on supervised downstream tasks, such as Visual Question Answering (VQA), and goal-conditioned planning. Compared to past works on dynamics modeling, our method achieves significantly better long-term synthesis of object dynamics, while retaining high quality visual generation. Besides, SlotFormer enables VQA models to reason about the future without object-level labels, even outperforming counterparts that use ground-truth annotations. Finally, we show its ability to serve as a world model for model-based planning, which is competitive with methods designed specifically for such tasks."}}
{"id": "HOPwmZ82r96", "cdate": 1640995200000, "mdate": 1668622278903, "content": {"title": "P3IV: Probabilistic Procedure Planning from Instructional Videos with Weak Supervision", "abstract": ""}}
{"id": "A_Aeb-XLozL", "cdate": 1621629747047, "mdate": null, "content": {"title": "Drop-DTW: Aligning Common Signal Between Sequences While Dropping Outliers", "abstract": "In this work, we consider the problem of sequence-to-sequence alignment for signals containing outliers.  Assuming the absence of outliers, the standard Dynamic Time Warping (DTW) algorithm efficiently computes the optimal alignment between two (generally) variable-length sequences.  While DTW is robust to temporal shifts and dilations of the signal, it fails to align sequences in a meaningful way in the presence of outliers that can be arbitrarily interspersed in the sequences.  To address this problem, we introduce Drop-DTW, a novel algorithm that aligns the common signal between the sequences while automatically dropping the outlier elements from the matching.  The entire procedure is implemented as a single dynamic program that is efficient and fully differentiable.  In our experiments, we show that Drop-DTW is a robust similarity measure for sequence retrieval and demonstrate its effectiveness as a training loss on diverse applications. With Drop-DTW, we address temporal step localization on instructional videos, representation learning from noisy videos, and cross-modal representation learning for audio-visual retrieval and localization. In all applications, we take a weakly- or unsupervised approach and demonstrate state-of-the-art results under these settings."}}
{"id": "NYE5uSg3PYB", "cdate": 1609459200000, "mdate": 1667989633421, "content": {"title": "On the Importance of Visual Context for Data Augmentation in Scene Understanding", "abstract": "Performing data augmentation for learning deep neural networks is known to be important for training visual recognition systems. By artificially increasing the number of training examples, it helps reducing overfitting and improves generalization. While simple image transformations can already improve predictive performance in most vision tasks, larger gains can be obtained by leveraging task-specific prior knowledge. In this work, we consider object detection, semantic and instance segmentation and augment the training images by blending objects in existing scenes, using instance segmentation annotations. We observe that randomly pasting objects on images hurts the performance, unless the object is placed in the right context. To resolve this issue, we propose an explicit context model by using a convolutional neural network, which predicts whether an image region is suitable for placing a given object or not. In our experiments, we show that our approach is able to improve object detection, semantic and instance segmentation on the PASCAL VOC12 and COCO datasets, with significant gains in a limited annotation scenario, i.e., when only one category is annotated. We also show that the method is not limited to datasets that come with expensive pixel-wise instance annotations and can be used when only bounding boxes are available, by employing weakly-supervised learning for instance masks approximation."}}
