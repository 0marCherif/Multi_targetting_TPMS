{"id": "OMBaY1XuE3", "cdate": 1664824312541, "mdate": 1664824312541, "content": {"title": " High-performance Evolutionary Algorithms for Online Neuron Control", "abstract": "Recently, optimization has become an emerging tool for neuroscientists to study neural code. In the visual system, neurons respond to images with graded and noisy responses. Image patterns eliciting highest responses are diagnostic of the coding content of the neuron. To find these patterns, we have used black-box optimizers to search a 4096d image space, leading to the evolution of images that maximize neuronal responses. Although genetic algorithm (GA) has been commonly used, there haven't been any systematic investigations to reveal the best performing optimizer or the underlying principles necessary to improve them.\nHere, we conducted a large scale in silico benchmark of optimizers for activation maximization and found that Covariance Matrix Adaptation (CMA) excelled in its achieved activation. We compared CMA against GA and found that CMA surpassed the maximal activation of GA by 66% in silico and 44% in vivo. We analyzed the structure of Evolution trajectories and found that the key to success was not covariance matrix adaptation, but local search towards informative dimensions and an effective step size decay. Guided by these principles and the geometry of the image manifold, we developed SphereCMA optimizer which competed well against CMA, proving the validity of the identified principles. Code available at this https URL"}}
{"id": "6UvnOdlbIbY", "cdate": 1664194166986, "mdate": null, "content": {"title": "On the Level Sets and Invariance of Neural Tuning Landscapes", "abstract": "Visual representations can be defined as the activations of neuronal populations in response to images. The activation of a neuron as a function over all image space has been described as a \"tuning landscape\". As a function over a high-dimensional space, what is the structure of this landscape? In this study, we characterize tuning landscapes through the lens of level sets and Morse theory. A recent study measured the in vivo two-dimensional tuning maps of neurons in different brain regions. Here, we developed a statistically reliable signature for these maps based on the change of topology in level sets. We found this topological signature changed progressively throughout the cortical hierarchy, with similar trends found for units in convolutional neural networks (CNNs). Further, we analyzed the geometry of level sets on the tuning landscapes of CNN units. We advanced the hypothesis that higher-order units can be locally regarded as isotropic radial basis functions, but not globally. This shows the power of level sets as a conceptual tool to understand neuronal activations over image space. "}}
{"id": "Rpazl253IHb", "cdate": 1634055191891, "mdate": null, "content": {"title": "On the use of Cortical Magnification and Saccades as Biological Proxies for Data Augmentation", "abstract": "Self-supervised learning is a strong way to learn useful representations from the bulk of natural data. It's suggested to be responsible for building the visual representation in humans, but the specific objective and algorithm are unknown. Currently, most self-supervised methods encourage the system to learn an invariant representation of different transformations of the same image in contrast to those of other images. However, such transformations are generally non-biologically plausible, and often consist of contrived perceptual schemes such as random cropping and color jittering. In this paper, we attempt to reconfigure these augmentations to be more biologically or perceptually plausible while still conferring the same benefits for encouraging a good representation. Critically, we find that random cropping can be substituted by cortical magnification, and saccade-like sampling of the image could also assist the representation learning. The feasibility of these transformations suggests a potential way that biological visual systems could implement self-supervision. Further, they break the widely accepted spatially-uniform processing assumption used in many computer vision algorithms, suggesting a role for spatially-adaptive computation in humans and machines alike."}}
{"id": "I913Sp8V9MX", "cdate": 1624900542683, "mdate": 1624900542683, "content": {"title": "What constitutes understanding of ventral pathway function?", "abstract": "Scientific question: How can we use computational models to decipher and/or represent neuronal\ntuning properties, and standardize descriptions for comparisons across stimuli/tasks/species?\n\nIntroduction. At the onset of visual neuroscience, first there was light, and then came explanations.\nWorking to stimulate neurons in primary visual cortex (V1) in 1958, Hubel and Wiesel projected white light onto\ncat retinas using a modified ophthalmoscope and a slide projector. They had glass- and brass slides with\ndrawings and cutouts, using them to shape light into simple geometric patterns. Among their many findings,\nthey established V1 neurons showed higher activity to specifically placed line segments \u2013 lines optimized in\ntheir location, length/width, color, and their rotation. The simplicity of these stimuli allowed for straightforward\ninterpretations, specifically that V1 neurons signal contour orientation1.\n\nObservations vs. interpretations. There were five components that made these experiments\ncanonical, and have been included in most subsequent studies of visual neuroscience:\n1) A physical stimulus (e.g., light patterns on a projection screen/computer monitor).\n2) A generative method for producing the physical stimuli (e.g., lines drawn manually on slides,\nvariables for a computer graphics library, vectors in a generative adversarial network).\n3) An experimenter-labeled stimulus space (e.g., orientation, categories) with a metric to order/cluster\nthe physical stimuli (e.g., angular distance, perceptual similarity).\n4) Neuronal activity associated with each stimulus (e.g., spike rates).\n5) A potential mechanism suggesting how those tuning functions could arise from earlier inputs (e.g.,\nspatially aligned projections from neurons in the midbrain [lateral geniculate nucleus]).\nThe first and fourth components are observables (we refer to these as pixels and spikes). The second\nand third components are fundamentally entangled with the experimenter\u2019s theories and interpretations (we\nrefer to these as methods and spaces). The linchpin observation is that in this experimental design, the\nrelationship between pixels and spikes is causal, but the relationship between spaces and spikes is\ncorrelational. Theoretically, there can be alternative explanations implicit in any given stimulus space which\nalso affect neuronal activity \u2014 in an experiment, the subject\u2019s brain only has access to the physical stimuli, not\nto the meaning attached to it."}}
{"id": "GH7QRzUDdXG", "cdate": 1601308161054, "mdate": null, "content": {"title": "A Geometric Analysis of Deep Generative Image Models and Its Applications", "abstract": "Generative adversarial networks (GANs) have emerged as a powerful unsupervised method to model the statistical patterns of real-world data sets, such as natural images. These networks are trained to map random inputs in their latent space to new samples representative of the learned data. However, the structure of the latent space is hard to intuit due to its high dimensionality and the non-linearity of the generator, which limits the usefulness of the models. Understanding the latent space requires a way to identify input codes for existing real-world images (inversion), and a way to identify directions with known image transformations (interpretability). Here, we use a geometric framework to address both issues simultaneously. We develop an architecture-agnostic method to compute the Riemannian metric of the image manifold created by GANs. The eigen-decomposition of the metric isolates axes that account for different levels of image variability. An empirical analysis of several pretrained GANs shows that image variation around each position is concentrated along surprisingly few major axes (the space is highly anisotropic) and the directions that create this large variation are similar at different positions in the space (the space is homogeneous). We show that many of the top eigenvectors correspond to interpretable transforms in the image space, with a substantial part of eigenspace corresponding to minor transforms which could be compressed out. This geometric understanding unifies key previous results related to GAN interpretability. We show that the use of this metric allows for more efficient optimization in the latent space (e.g. GAN inversion) and facilitates unsupervised discovery of interpretable axes. Our results illustrate that defining the geometry of the GAN image manifold can serve as a general framework for understanding GANs. "}}
