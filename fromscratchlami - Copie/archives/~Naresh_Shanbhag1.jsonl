{"id": "lDpkCp6FgZ", "cdate": 1672531200000, "mdate": 1681726387585, "content": {"title": "On the Robustness of Randomized Ensembles to Adversarial Perturbations", "abstract": "Randomized ensemble classifiers (RECs), where one classifier is randomly selected during inference, have emerged as an attractive alternative to traditional ensembling methods for realizing adversarially robust classifiers with limited compute requirements. However, recent works have shown that existing methods for constructing RECs are more vulnerable than initially claimed, casting major doubts on their efficacy and prompting fundamental questions such as: \"When are RECs useful?\", \"What are their limits?\", and \"How do we train them?\". In this work, we first demystify RECs as we derive fundamental results regarding their theoretical limits, necessary and sufficient conditions for them to be useful, and more. Leveraging this new understanding, we propose a new boosting algorithm (BARRE) for training robust RECs, and empirically demonstrate its effectiveness at defending against strong $\\ell_\\infty$ norm-bounded adversaries across various network architectures and datasets. Our code can be found at https://github.com/hsndbk4/BARRE."}}
{"id": "k3VANp85b4S", "cdate": 1663850251006, "mdate": null, "content": {"title": "On the Robustness of Randomized Ensembles to Adversarial Perturbations", "abstract": "Randomized ensemble classifiers (RECs), where one classifier is randomly selected during inference, have emerged as an attractive alternative to traditional ensembling methods for realizing adversarially robust classifiers with limited compute requirements. However, recent works have shown that existing methods for constructing RECs are more vulnerable than initially claimed, casting major doubts on their efficacy and prompting fundamental questions such as: \"When are RECs useful?\", \"What are their limits?\", and \"How do we train them?\". In this work, we first demystify RECs as we derive fundamental results regarding their theoretical limits, necessary and sufficient conditions for them to be useful, and more. Leveraging this new understanding, we propose a new boosting\nalgorithm (BARRE) for training robust RECs, and empirically demonstrate its effectiveness at defending against strong $\\ell_\\infty$ norm-bounded adversaries across various network architectures and datasets. Our code is submitted as part of the supplementary material, and will be publicly released on GitHub"}}
{"id": "va3CqkEkIBy", "cdate": 1640995200000, "mdate": 1681726384885, "content": {"title": "Adversarial Vulnerability of Randomized Ensembles", "abstract": "Despite the tremendous success of deep neural networks across various tasks, their vulnerability to imperceptible adversarial perturbations has hindered their deployment in the real world. Recently..."}}
{"id": "sze_MppYdkD", "cdate": 1640995200000, "mdate": 1664639447166, "content": {"title": "Fundamental Limits on Energy-Delay-Accuracy of In-Memory Architectures in Inference Applications", "abstract": "This article obtains fundamental limits on the computational precision of in-memory computing architectures (IMCs). An IMC noise model and associated signal-to-noise ratio (SNR) metrics are defined and their interrelationships analyzed to show that the accuracy of IMCs is fundamentally limited by the compute SNR ( <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">${\\mathrm {SNR}}_{a}$ </tex-math></inline-formula> ) of its analog core, and that activation, weight, and output (ADC) precision needs to be assigned appropriately for the final output SNR ( <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">${\\mathrm {SNR}}_{T}$ </tex-math></inline-formula> ) to approach <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">${\\mathrm {SNR}}_{a}$ </tex-math></inline-formula> . The minimum precision criterion (MPC) is proposed to minimize the analog-to-digital converter (ADC) precision and hence its overhead. Three in-memory compute models\u2014charge summing (QS), current summing (IS), and charge redistribution (QR)\u2014are shown to underlie most known IMCs. Noise, energy, and delay expressions for the compute models are developed and employed to derive expressions for the SNR, ADC precision, energy, and latency of IMCs. The compute SNR expressions are validated via Monte Carlo simulations in a 65 nm CMOS process. For a 512 row SRAM array, it is shown that: 1) IMCs have an upper bound on their maximum achievable <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">${\\mathrm {SNR}}_{a}$ </tex-math></inline-formula> due to constraints on energy, area and voltage swing, and this upper bound reduces with technology scaling for QS-based architectures; 2) MPC enables <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">${\\mathrm {SNR}}_{T}$ </tex-math></inline-formula> to approach <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">${\\mathrm {SNR}}_{a}$ </tex-math></inline-formula> to be realized with minimal ADC precision; and 3) QS-based (QR-based) architectures are preferred for low (high) compute SNR scenarios."}}
{"id": "_8Nd9_kWOa", "cdate": 1640995200000, "mdate": 1681726387588, "content": {"title": "Adversarial Vulnerability of Randomized Ensembles", "abstract": "Despite the tremendous success of deep neural networks across various tasks, their vulnerability to imperceptible adversarial perturbations has hindered their deployment in the real world. Recently, works on randomized ensembles have empirically demonstrated significant improvements in adversarial robustness over standard adversarially trained (AT) models with minimal computational overhead, making them a promising solution for safety-critical resource-constrained applications. However, this impressive performance raises the question: Are these robustness gains provided by randomized ensembles real? In this work we address this question both theoretically and empirically. We first establish theoretically that commonly employed robustness evaluation methods such as adaptive PGD provide a false sense of security in this setting. Subsequently, we propose a theoretically-sound and efficient adversarial attack algorithm (ARC) capable of compromising random ensembles even in cases where adaptive PGD fails to do so. We conduct comprehensive experiments across a variety of network architectures, training schemes, datasets, and norms to support our claims, and empirically establish that randomized ensembles are in fact more vulnerable to $\\ell_p$-bounded adversarial perturbations than even standard AT models. Our code can be found at https://github.com/hsndbk4/ARC."}}
{"id": "Vwa16sxq4O", "cdate": 1640995200000, "mdate": 1681726387138, "content": {"title": "IMPQ: Reduced Complexity Neural Networks Via Granular Precision Assignment", "abstract": "The demand for the deployment of deep neural networks (DNN) on resource-constrained Edge platforms is ever increasing. Today\u2019s DNN accelerators support mixed-precision computations to enable reduction of computational and storage costs but require networks with precision at variable granularity, i.e., network, layer or kernel level. However, the problem of granular precision assignment is challenging due to an exponentially large search space and efficient methods for such precision assignment are lacking. To address this problem, we introduce the iterative mixed-precision quantization (IMPQ) framework to allocate precision at variable granularity. IMPQ employs a sensitivity metric to order the weight/activation groups in terms of the likelihood of misclassifying input samples due to its quantization noise. It iteratively reduces the precision of the weights and activations of a pretrained full-precision network starting with the least sensitive group. Compared to state-of-the-art methods, IMPQ reduces computational costs by 2\u00d7 -to-2.5\u00d7 for compact networks such as MobileNet-V1 on ImageNet with no accuracy loss. Our experiments reveal that kernel-wise granular precision assignment provides 1.7\u00d7 higher compression than layer-wise assignment."}}
{"id": "56dOsMBBAKp", "cdate": 1640995200000, "mdate": 1681726387600, "content": {"title": "Fundamental Limits on the Computational Accuracy of Resistive Crossbar-based In-memory Architectures", "abstract": "In-memory computing (IMC) architectures exhibit an intrinsic trade-off between computational accuracy and energy efficiency. This paper determines the fundamental limits on the compute SNR of MRAM-, ReRAM-, and FeFET-based crossbars by employing statistical signal and noise models. For a specific dot-product dimension N, the maximum compute SNR (SNR <inf xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">max</inf> ) is shown to occur at an optimum value of sensing resistance $R_{s}^{*}$ where clipping and quantization noise contributions from the analog-to-digital converter (ADC) are balanced out. SNR <inf xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">max</inf> can be further improved by choosing devices with higher resistive contrast R <inf xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">off</inf> /R <inf xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">on</inf> , e.g., FeFET, but only until it attains a value in the range 12-15. Beyond this point, mismatch in the input digital-to-analog converters (DACs) and bitcell variations begin to dominate the compute SNR. Finally, by mapping a ResNet20 (CIFAR-10) network onto resistive crossbars, it is shown that the array-level compute SNR maximizing circuit parameters also maximizes the network-level accuracy."}}
{"id": "4W-faueA2dP", "cdate": 1640995200000, "mdate": 1681726388023, "content": {"title": "Coordinated Science Laboratory 70th Anniversary Symposium: The Future of Computing", "abstract": "In 2021, the Coordinated Science Laboratory CSL, an Interdisciplinary Research Unit at the University of Illinois Urbana-Champaign, hosted the Future of Computing Symposium to celebrate its 70th anniversary. CSL's research covers the full computing stack, computing's impact on society and the resulting need for social responsibility. In this white paper, we summarize the major technological points, insights, and directions that speakers brought forward during the Future of Computing Symposium. Participants discussed topics related to new computing paradigms, technologies, algorithms, behaviors, and research challenges to be expected in the future. The symposium focused on new computing paradigms that are going beyond traditional computing and the research needed to support their realization. These needs included stressing security and privacy, the end to end human cyber physical systems and with them the analysis of the end to end artificial intelligence needs. Furthermore, advances that enable immersive environments for users, the boundaries between humans and machines will blur and become seamless. Particular integration challenges were made clear in the final discussion on the integration of autonomous driving, robo taxis, pedestrians, and future cities. Innovative approaches were outlined to motivate the next generation of researchers to work on these challenges. The discussion brought out the importance of considering not just individual research areas, but innovations at the intersections between computing research efforts and relevant application domains, such as health care, transportation, energy systems, and manufacturing."}}
{"id": "0cV3wZ2uTcW", "cdate": 1640995200000, "mdate": 1681726387822, "content": {"title": "Comprehending In-memory Computing Trends via Proper Benchmarking", "abstract": "Since its inception in 2014 [1], the modern version of in-memory computing (IMC) has become an active area of research in integrated circuit design globally for realizing artificial intelligence and machine learning workloads. Since 2018, > 40 IMC-related papers have been published in top circuit design conferences demonstrating significant reductions (>20X) in energy over their digital counterparts especially at the bank-level. Today, bank-level IMC designs have matured but it is not clear what the limiting factors are. This lack of clarity is due to multiple reasons including: 1) the conceptual complexity of IMCs due to its full-stack (devices-to-systems) nature, 2) the presence of a fundamental energy-efficiency vs. compute SNR trade-off due to its analog computations, and 3) the statistical nature of machine learning workloads. The absence of a rigorous benchmarking methodology for IMCs - a problem facing machine learning ICs in general [2] - further obfuscates the underlying trade-offs. As a result, it has become difficult to evaluate the novelty of IMC-related ideas being proposed and therefore gauge the true progress in this exciting field."}}
{"id": "2-0wS9-cSL", "cdate": 1621630282531, "mdate": null, "content": {"title": "Robustifying $\\ell_\\infty$  Adversarial Training to the Union of Perturbation Models", "abstract": "Classical adversarial training (AT) frameworks are designed to achieve high adversarial accuracy against a single attack type, typically $\\ell_\\infty$ norm-bounded perturbations. Recent extensions in AT have focused on defending against the union of multiple perturbation models but this benefit is obtained at the expense of a significant (up to $10\\times$) increase in training complexity over single-attack $\\ell_\\infty$ AT. In this work, we expand the capabilities of widely popular $\\ell_\\infty$ single-attack AT frameworks to providing robustness to the union of ($\\ell_\\infty, \\ell_2, \\ell_1$) perturbations while preserving their training efficiency. Our technique, referred to as Shaped Noise Augmented Processing (SNAP), exploits a well-established byproduct of single-attack AT frameworks -- the reduction in the curvature of the decision boundary of networks. SNAP prepends a given deep net with a shaped noise augmentation layer whose distribution is learned along with network parameters using any standard single-attack AT. As a result, SNAP enhances adversarial accuracy of ResNet-18 on CIFAR-10 against the union of ($\\ell_\\infty, \\ell_2, \\ell_1$) perturbation models by 14%-to-20% for four state-of-the-art (SOTA) single-attack $\\ell_\\infty$ AT frameworks, and, for the first time, establishes a benchmark for ResNet-50 and ResNet-101 on ImageNet."}}
