{"id": "fqYHI2E5v0W", "cdate": 1686191618361, "mdate": 1686191618361, "content": {"title": "Accelerating dataset distillation via model augmentation", "abstract": "Dataset Distillation (DD), a newly emerging field, aims at generating much smaller but efficient synthetic training datasets from large ones. Existing DD methods based on gradient matching achieve leading performance; however, they are extremely computationally intensive as they require continuously optimizing a dataset among thousands of randomly initialized models. In this paper, we assume that training the synthetic data with diverse models leads to better generalization performance. Thus we propose two model augmentation techniques, i.e. using early-stage models and parameter perturbation to learn an informative synthetic set with significantly reduced training cost. Extensive experiments demonstrate that our method achieves up to 20\u00d7 speedup and comparable performance on par with state-of-the-art methods."}}
{"id": "ZBMpG7fWwOP", "cdate": 1663850493534, "mdate": null, "content": {"title": "Game Theoretic Mixed Experts for Combinational Adversarial Machine Learning", "abstract": "Recent advances in adversarial machine learning have shown that defenses considered to be robust are actually susceptible to adversarial attacks which are specifically tailored to target their weaknesses. These defenses include Barrage of Random Transforms (BaRT), Friendly Adversarial Training (FAT), Trash is Treasure (TiT) and ensemble models made up of Vision Transformers (ViTs), Big Transfer models and Spiking Neural Networks (SNNs). It remains an open question, however, as to whether the adversarial examples designed to target one defense will be similarly misclassified by another defense. In this paper, we provide the first adversarial defense transferability study, as well as a game theoretic framework for ensemble adversarial attacks and defenses. Our framework is called Game theoretic Mixed Experts (GaME) and is designed to find the Mixed-Nash strategy for an attacker that can employ compositional adversarial attacks. We show that this framework creates an ensemble of defenses with greater robustness than a combinational defense with a uniform or random probability distribution. Overall, our framework and analyses advance the field of adversarial machine learning by yielding new insights into compositional attack and defense formulations."}}
{"id": "6EFe8msk28O", "cdate": 1640995200000, "mdate": 1642352351113, "content": {"title": "Graph-Based Shape Analysis for Heterogeneous Geometric Datasets: Similarity, Retrieval and Substructure Matching", "abstract": "Highlights \u2022 We developed graph analysis tools processing on MDBD for shape analyzing tasks. \u2022 We designed a light DNN with comparable classification accuracy of other leading classifiers. \u2022 We proposed a graph kernel based on MDBD with linear time complexity. \u2022 We established a substructure matching method with graph segmentationation and kernel. Abstract Practically all existing shape analysis and processing algorithms have been developed for specific geometric representations of 3D models. However, the product development process always involves a large number of often incompatible geometric representations tailored to specific computational tasks that take place during this process. Consequently, a substantial effort has been expended to develop robust geometric data translation and conversion algorithms, but the existing methods have well known limitations. The Maximal Disjoint Ball Decomposition (MDBD) was recently defined as a unique and stable geometric construction and used to define universal shape descriptors based on the contact graph associated with MDBD. In this paper, we demonstrate that by applying graph analysis tools to MDBD in conjunction with graph convolutional neural networks and graph kernels, one can effectively develop methods to perform similarity, retrieval and substructure matching from geometric models regardless of their native geometric representation. We show that our representation-agnostic approach achieves comparable performance with state-of-the-art geometric processing methods on standard yet heterogeneous benchmark datasets while supporting all valid geometric representations."}}
{"id": "zfTEVx0dgqf", "cdate": 1609459200000, "mdate": 1642352351347, "content": {"title": "Enabling Super-Fast Deep Learning on Tiny Energy-Harvesting IoT Devices", "abstract": "Energy harvesting (EH) IoT devices that operate intermittently without batteries, coupled with advances in deep neural networks (DNNs), have opened up new opportunities for enabling sustainable smart applications. Nevertheless, implementing those computation and memory-intensive intelligent algorithms on EH devices is extremely difficult due to the challenges of limited resources and intermittent power supply that causes frequent failures. To address those challenges, this paper proposes a methodology that enables fast deep learning with low-energy accelerators for tiny energy harvesting devices. We first propose $RAD$, a resource-aware structured DNN training framework, which employs block circulant matrix and structured pruning to achieve high compression for leveraging the advantage of various vector operation accelerators. A DNN implementation method, $ACE$, is then proposed that employs low-energy accelerators to profit maximum performance with small energy consumption. Finally, we further design $FLEX$, the system support for intermittent computation in energy harvesting situations. Experimental results from three different DNN models demonstrate that $RAD$, $ACE$, and $FLEX$ can enable fast and correct inference on energy harvesting devices with up to 4.26X runtime reduction, up to 7.7X energy reduction with higher accuracy over the state-of-the-art."}}
{"id": "zd5YOqf9Tyd", "cdate": 1609459200000, "mdate": 1623609802089, "content": {"title": "TAG: Transformer Attack from Gradient", "abstract": "Although federated learning has increasingly gained attention in terms of effectively utilizing local devices for data privacy enhancement, recent studies show that publicly shared gradients in the training process can reveal the private training images (gradient leakage) to a third-party in computer vision. We have, however, no systematic understanding of the gradient leakage mechanism on the Transformer based language models. In this paper, as the first attempt, we formulate the gradient attack problem on the Transformer-based language models and propose a gradient attack algorithm, TAG, to reconstruct the local training data. We develop a set of metrics to evaluate the effectiveness of the proposed attack algorithm quantitatively. Experimental results on Transformer, TinyBERT$_{4}$, TinyBERT$_{6}$, BERT$_{BASE}$, and BERT$_{LARGE}$ using GLUE benchmark show that TAG works well on more weight distributions in reconstructing training data and achieves 1.5$\\times$ recover rate and 2.5$\\times$ ROUGE-2 over prior methods without the need of ground truth label. TAG can obtain up to 90$\\%$ data by attacking gradients in CoLA dataset. In addition, TAG has a stronger adversary on large models, small dictionary size, and small input length. We hope the proposed TAG will shed some light on the privacy leakage problem in Transformer-based NLP models."}}
{"id": "zBlBPXfCbU", "cdate": 1609459200000, "mdate": 1636877081849, "content": {"title": "Enabling Retrain-free Deep Neural Network Pruning Using Surrogate Lagrangian Relaxation", "abstract": "Network pruning is a widely used technique to reduce computation cost and model size for deep neural networks. However, the typical three-stage pipeline, i.e., training, pruning and retraining (fine-tuning) significantly increases the overall training trails. In this paper, we develop a systematic weight-pruning optimization approach based on Surrogate Lagrangian relaxation (SLR), which is tailored to overcome difficulties caused by the discrete nature of the weight-pruning problem while ensuring fast convergence. We further accelerate the convergence of the SLR by using quadratic penalties. Model parameters obtained by SLR during the training phase are much closer to their optimal values as compared to those obtained by other state-of-the-art methods. We evaluate the proposed method on image classification tasks using CIFAR-10 and ImageNet, as well as object detection tasks using COCO 2014 and Ultra-Fast-Lane-Detection using TuSimple lane detection dataset. Experimental results demonstrate that our SLR-based weight-pruning optimization approach achieves higher compression rate than state-of-the-arts under the same accuracy requirement. It also achieves a high model accuracy even at the hard-pruning stage without retraining (reduces the traditional three-stage pruning to two-stage). Given a limited budget of retraining epochs, our approach quickly recovers the model accuracy."}}
{"id": "qeYiZ35Z0ok", "cdate": 1609459200000, "mdate": 1636877081558, "content": {"title": "Dancing along Battery: Enabling Transformer with Run-time Reconfigurability on Mobile Devices", "abstract": "A pruning-based AutoML framework for run-time reconfigurability, namely RT <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">3</sup> , is proposed in this work. This enables Transformer-based large Natural Language Processing (NLP) models to be efficiently executed on resource-constrained mobile devices and reconfigured (i.e., switching models for dynamic hardware conditions) at run-time. Such reconfigurability is the key to save energy for battery-powered mobile devices, which widely use dynamic voltage and frequency scaling (DVFS) technique for hardware reconfiguration to prolong battery life. In this work, we creatively explore a hybrid block-structured pruning (BP) and pattern pruning (PP) for Transformer-based models and first attempt to combine hardware and software reconfiguration to maximally save energy for battery-powered mobile devices. Specifically, RT <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">3</sup> integrates two-level optimizations: First, it utilizes an efficient BP as the first-step compression for resource-constrained mobile devices; then, RT <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">3</sup> heuristically generates a shrunken search space based on the first level optimization and searches multiple pattern sets with diverse sparsity for PP via reinforcement learning to support lightweight software reconfiguration, which corresponds to available frequency levels of DVFS (i.e., hardware reconfiguration). At run-time, RT <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">3</sup> can switch the lightweight pattern sets within 45ms to guarantee the required real-time constraint at different frequency levels. Results further show that RT <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">3</sup> can prolong battery life over $ 4\\times$ improvement with less than 1% accuracy loss for Transformer and 1.5% score decrease for DistilBERT."}}
{"id": "qELdYpfUK6T", "cdate": 1609459200000, "mdate": 1642352351345, "content": {"title": "An End-to-end Multi-task Object Detection using Embedded GPU in Autonomous Driving", "abstract": "Autonomous driving has gained popularity due to its high reliability compared to human drivers. Autonomous vehicles combine variety of sensors to perceive their surroundings, and use deep learning (DL) to extract complicated information from the sensing data. However, there are several challenges: Many DL models have explosive model sizes, and therefore not only time consuming but also power consuming when implementing on embedded systems on vehicles, further degrading the battery life-cycle. The current on-board AI treats lane detection and car location separately. In this paper, we propose an end-to-end multi-task environment detection framework. We fuse the 3D point cloud object detection model and lane detection model, with model compression technique applied. As on-board sensors forward information to the multi-task network, it not only parallel two detection tasks to extract combination information, but also reduces entire running time of the DL model. Experiments show by adding the model compression technique, the running speed of multi-task model improves more than 2\u00d7. Also, running time of lane detection model on Nvidia Jetson TX2 is almost 6\u00d7 less comparing with running on CPU, which shows reasonableness of using embedded AI computing device on autonomous vehicle. Index Terms-3D Object Detection, LIDAR Point Cloud, Lane Detection, Weight Pruning, Embedded Computing Device."}}
{"id": "pyWctuTq-S_", "cdate": 1609459200000, "mdate": 1642352351347, "content": {"title": "Design, Sensing, and Control of a Novel UAV Platform for Aerial Drilling and Screwing", "abstract": "Hole drilling and bolt screwing are frequently performed tasks in construction, decoration, and maintenance. Traditionally, sending human workers to perform these tasks in hard-to-reach locations is both dangerous and costly. In this letter, we present an aerial manipulation platform that allows a human user to remotely conduct omnidirectional drilling and screwing. The design of the platform features a quadrotor UAV with each pair of rotors independently tilted by a servo, forming an \u201cH\u201d configuration, on which a 1-DOF manipulator carrying a motorized drill or screw driver is mounted. With such a design, the end-effector can face any direction on the longitudinal plane and exert a big enough contact force for drilling and screwing without the need of changing the vehicle body's orientation. Compared to previous UAVs that can only drill holes vertically into the ground, the proposed design also allows horizonal drilling/screwing into a wall or a cliff, making it suitable for a vast range of real applications. Based on the dynamic equations of the system, a dual-level control law is proposed. The low-level attitude controller uses an adaptive robust control (ARC) to accurately regulate the attitude angles in the presence of force/torque uncertainties that may occur during the drilling and screwing process, while a selective impedance controller is implemented at high level to indirectly control the contact force commanded by the user. In addition, a vision-based real-time target identification and tracking method integrating a YOLO v3 real-time object detector with feature tracking, and morphological operations is developed to identify and track the target point for drilling and screwing specified by the user. Various in-lab experiments on a self-made prototype demonstrate the feasibility and effectiveness of the proposed approach for aerial drilling and screwing."}}
{"id": "n-tbl9P9yhY", "cdate": 1609459200000, "mdate": 1642352351346, "content": {"title": "Accelerating Transformer-based Deep Learning Models on FPGAs using Column Balanced Block Pruning", "abstract": "Although Transformer-based language representations achieve state-of-the-art accuracy on various natural language processing (NLP) tasks, the large model size has been challenging the resource constrained computing platforms. Weight pruning, as a popular and effective technique in reducing the number of weight parameters and accelerating the Transformer, has been investigated on GPUs. However, the Transformer acceleration using weight pruning on field-programmable gate array (FPGAs) remains unexplored. This paper investigates the column balanced block-wise pruning on Transformer and designs an FPGA acceleration engine to customize the balanced blockwise matrix multiplication. We implement the Transformer model with proper hardware scheduling, and the experiments show that the Transformer inference on FPGA achieves 10.35 ms latency with the batch size of 32, which is 10.96 * speed up comparing to CPU platform and 2.08 * speed up comparing to GPU platform."}}
