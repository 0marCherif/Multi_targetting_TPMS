{"id": "xdCgimSZZsv", "cdate": 1672531200000, "mdate": 1703319289177, "content": {"title": "CROP: Towards Distributional-Shift Robust Reinforcement Learning Using Compact Reshaped Observation Processing", "abstract": "The safe application of reinforcement learning (RL) requires generalization from limited training data to unseen scenarios. Yet, fulfilling tasks under changing circumstances is a key challenge in RL. Current state-of-the-art approaches for generalization apply data augmentation techniques to increase the diversity of training data. Even though this prevents overfitting to the training environment(s), it hinders policy optimization. Crafting a suitable observation, only containing crucial information, has been shown to be a challenging task itself. To improve data efficiency and generalization capabilities, we propose Compact Reshaped Observation Processing (CROP) to reduce the state information used for policy optimization. By providing only relevant information, overfitting to a specific training layout is precluded and generalization to unseen environments is improved. We formulate three CROPs that can be applied to fully observable observation- and action-spaces and provide methodical foundation. We empirically show the improvements of CROP in a distributionally shifted safety gridworld. We furthermore provide benchmark comparisons to full observability and data-augmentation in two different-sized procedurally generated mazes."}}
{"id": "wNr7tn3H_Rz", "cdate": 1672531200000, "mdate": 1699271187946, "content": {"title": "Attention-Based Recurrence for Multi-Agent Reinforcement Learning under Stochastic Partial Observability", "abstract": "Stochastic partial observability poses a major challenge for decentralized coordination in multi-agent reinforcement learning but is largely neglected in state-of-the-art research due to a strong f..."}}
{"id": "knuG56tD2Yy", "cdate": 1672531200000, "mdate": 1683832153065, "content": {"title": "Learning to Participate Through Trading of Reward Shares", "abstract": ""}}
{"id": "aX7Sv75uUJY", "cdate": 1672531200000, "mdate": 1674926976037, "content": {"title": "DIRECT: Learning from Sparse and Shifting Rewards using Discriminative Reward Co-Training", "abstract": "We propose discriminative reward co-training (DIRECT) as an extension to deep reinforcement learning algorithms. Building upon the concept of self-imitation learning (SIL), we introduce an imitation buffer to store beneficial trajectories generated by the policy determined by their return. A discriminator network is trained concurrently to the policy to distinguish between trajectories generated by the current policy and beneficial trajectories generated by previous policies. The discriminator's verdict is used to construct a reward signal for optimizing the policy. By interpolating prior experience, DIRECT is able to act as a surrogate, steering policy optimization towards more valuable regions of the reward landscape thus learning an optimal policy. Our results show that DIRECT outperforms state-of-the-art algorithms in sparse- and shifting-reward environments being able to provide a surrogate reward to the policy and direct the optimization towards valuable areas."}}
{"id": "SisiNo_A_u", "cdate": 1672531200000, "mdate": 1703319289152, "content": {"title": "Towards Transfer Learning for Large-Scale Image Classification Using Annealing-Based Quantum Boltzmann Machines", "abstract": "Quantum Transfer Learning (QTL) recently gained popularity as a hybrid quantum-classical approach for image classification tasks by efficiently combining the feature extraction capabilities of large Convolutional Neural Networks with the potential benefits of Quantum Machine Learning (QML). Existing approaches, however, only utilize gate-based Variational Quantum Circuits for the quantum part of these procedures. In this work we present an approach to employ Quantum Annealing (QA) in QTL-based image classification. Specifically, we propose using annealing-based Quantum Boltzmann Machines as part of a hybrid quantum-classical pipeline to learn the classification of real-world, large-scale data such as medical images through supervised training. We demonstrate our approach by applying it to the three-class COVID-CT-MD dataset, a collection of lung Computed Tomography (CT) scan slices. Using Simulated Annealing as a stand-in for actual QA, we compare our method to classical transfer learning, using a neural network of the same order of magnitude, to display its improved classification performance. We find that our approach consistently outperforms its classical baseline in terms of test accuracy and AUC-ROC-Score and needs less training epochs to do this."}}
{"id": "RbPV9o62bOM", "cdate": 1672531200000, "mdate": 1703319289157, "content": {"title": "Hybrid Quantum Machine Learning Assisted Classification of COVID-19 from Computed Tomography Scans", "abstract": "Practical quantum computing (QC) is still in its in-fancy and problems considered are usually fairly small, especially in quantum machine learning when compared to its classical counterpart. Image processing applications in particular require models that are able to handle a large amount of features, and while classical approaches can easily tackle this, it is a major challenge and a cause for harsh restrictions in contemporary QC. In this paper, we apply a hybrid quantum machine learning approach to a practically relevant problem with real world-data. That is, we apply hybrid quantum transfer learning to an image processing task in the field of medical image processing. More specifically, we classify large CT-scans of the lung into COVID-19, CAP, or Normal. We discuss quantum image embedding as well as hybrid quantum machine learning and evaluate several approaches to quantum transfer learning with various quantum circuits and embedding techniques."}}
{"id": "Onl45w-ZVG", "cdate": 1672531200000, "mdate": 1683832153072, "content": {"title": "CROP: Towards Distributional-Shift Robust Reinforcement Learning using Compact Reshaped Observation Processing", "abstract": "The safe application of reinforcement learning (RL) requires generalization from limited training data to unseen scenarios. Yet, fulfilling tasks under changing circumstances is a key challenge in RL. Current state-of-the-art approaches for generalization apply data augmentation techniques to increase the diversity of training data. Even though this prevents overfitting to the training environment(s), it hinders policy optimization. Crafting a suitable observation, only containing crucial information, has been shown to be a challenging task itself. To improve data efficiency and generalization capabilities, we propose Compact Reshaped Observation Processing (CROP) to reduce the state information used for policy optimization. By providing only relevant information, overfitting to a specific training layout is precluded and generalization to unseen environments is improved. We formulate three CROPs that can be applied to fully observable observation- and action-spaces and provide methodical foundation. We empirically show the improvements of CROP in a distributionally shifted safety gridworld. We furthermore provide benchmark comparisons to full observability and data-augmentation in two different-sized procedurally generated mazes."}}
{"id": "-oUOTz4woqD", "cdate": 1672531200000, "mdate": 1683832153064, "content": {"title": "SEQUENT: Towards Traceable Quantum Machine Learning Using Sequential Quantum Enhanced Training", "abstract": ""}}
{"id": "lVUxPw9B7kG", "cdate": 1640995200000, "mdate": 1683832153067, "content": {"title": "Capturing Dependencies Within Machine Learning via a Formal Process Model", "abstract": "The development of Machine Learning (ML) models is more than just a special case of software development (SD): ML models acquire properties and fulfill requirements even without direct human interaction in a seemingly uncontrollable manner. Nonetheless, the underlying processes can be described in a formal way. We define a comprehensive SD process model for ML that encompasses most tasks and artifacts described in the literature in a consistent way. In addition to the production of the necessary artifacts, we also focus on generating and validating fitting descriptions in the form of specifications. We stress the importance of further evolving the ML model throughout its life-cycle even after initial training and testing. Thus, we provide various interaction points with standard SD processes in which ML often is an encapsulated task. Further, our SD process model allows to formulate ML as a (meta-) optimization problem. If automated rigorously, it can be used to realize self-adaptive autonomous systems. Finally, our SD process model features a description of time that allows to reason about the progress within ML development processes. This might lead to further applications of formal methods within the field of ML."}}
{"id": "AkhuAcKg8Xj", "cdate": 1640995200000, "mdate": 1674926976100, "content": {"title": "Emergent Cooperation from Mutual Acknowledgment Exchange", "abstract": ""}}
