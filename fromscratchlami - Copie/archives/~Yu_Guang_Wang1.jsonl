{"id": "VlYup7V1_Ht", "cdate": 1664046168840, "mdate": null, "content": {"title": "ACMP: Allen-Cahn Message Passing with Attractive and Repulsive Forces for Graph Neural Networks", "abstract": "Neural message passing is a basic feature extraction unit for graph-structured data considering neighboring node features in network propagation from one layer to the next. We model such process by an interacting particle system with attractive and repulsive forces and Allen-Cahn force arising in the modeling of phase transition. The dynamics of the system is a reaction-diffusion process which can separate particles without blowing up. This induces an Allen-Cahn message passing (ACMP) for graph neural networks where the numerical iteration for the particle system solution constitutes the message passing propagation. ACMP which has a simple implementation with a neural ODE solver can propel the network depth up to one hundred of layers with theoretically proven strictly positive lower bound of the Dirichlet energy. It thus provides a deep model of GNNs circumventing the common GNN problem of oversmoothing. GNNs with ACMP achieve state of the art performance for real-world node classification tasks on both homophilic and heterophilic datasets."}}
{"id": "IWoHx6bY4Zm", "cdate": 1663850300118, "mdate": null, "content": {"title": "Lightweight Equivariant Graph Representation Learning for Protein Engineering", "abstract": "This work tackles the issue of directed evolution in computational protein design that makes accurate predictions of the function of a protein mutant. We design a lightweight pre-training graph neural network model for multi-task protein representation learning from its 3D structure. Rather than reconstructing and optimizing the protein structure, the trained model recovers the amino acid types and key properties of the central residues from a given noisy three-dimensional local environment. On the prediction task for the higher-order mutants, where many amino acid sites of the protein are mutated, the proposed training strategy achieves remarkably higher performance by 20% improvement at the cost of requiring less than 1% of computational resources that are required by popular transformer-based state-of-the-art deep learning models for protein design."}}
{"id": "4fZc_79Lrqs", "cdate": 1663850042406, "mdate": null, "content": {"title": "ACMP: Allen-Cahn Message Passing with Attractive and Repulsive Forces for Graph Neural Networks", "abstract": "Neural message passing is a basic feature extraction unit for graph-structured data considering neighboring node features in network propagation from one layer to the next. We model such process by an interacting particle system with attractive and repulsive forces and the Allen-Cahn force arising in the modeling of phase transition. The dynamics of the system is a reaction-diffusion process which can separate particles without blowing up. This induces an Allen-Cahn message passing (ACMP) for graph neural networks where the numerical iteration for the particle system solution constitutes the message passing propagation. ACMP which has a simple implementation with a neural ODE solver can propel the network depth up to one hundred of layers with theoretically proven strictly positive lower bound of the Dirichlet energy. It thus provides a deep model of GNNs circumventing the common GNN problem of oversmoothing. GNNs with ACMP achieve state of the art performance for real-world node classification tasks on both homophilic and heterophilic datasets. Codes are available at https://github.com/ykiiiiii/ACMP"}}
{"id": "eKc91g8K2nm", "cdate": 1663624350380, "mdate": 1663624350380, "content": {"title": "Oversquashing in GNNs through the lens of information contraction and graph expansion", "abstract": "The quality of signal propagation in message-passing graph neural networks (GNNs) strongly influences their expressivity as has been observed in recent works. In particular, for prediction tasks relying on long-range interactions, recursive aggregation of node features can lead to an undesired phenomenon called \"oversquashing\". We present a framework for analyzing oversquashing based on information contraction. Our analysis is guided by a model of reliable computation due to von Neumann that lends a new insight into oversquashing as signal quenching in noisy computation graphs. Building on this, we propose a graph rewiring algorithm aimed at alleviating oversquashing. Our algorithm employs a random local edge flip primitive motivated by an expander graph construction. We compare the spectral expansion properties of our algorithm with that of an existing curvature-based non-local rewiring strategy. Synthetic experiments show that while our algorithm in general has a slower rate of expansion, it is overall computationally cheaper, preserves the node degrees exactly and never disconnects the graph."}}
{"id": "kQsniwmGgF5", "cdate": 1662812634802, "mdate": null, "content": {"title": "Well-conditioned Spectral Transforms for Dynamic Graph Representation", "abstract": "This work establishes a fully-spectral framework to capture informative long-range temporal interactions in a dynamic system. We connect the spectral transform to the low-rank self-attention mechanisms and investigate its energy-balancing effect and computational efficiency. Based on the observations, we leverage the adaptive power method SVD and global graph framelet convolution to encode time-dependent features and graph structure for continuous-time dynamic graph representation learning. The former serves as an efficient high-order linear self-attention with determined propagation rules, and the latter establishes scalable and transferable geometric characterization for property prediction. Empirically, the proposed model learns well-conditioned hidden representations on a variety of online learning tasks, and it achieves top performance with a reduced number of learnable parameters and faster propagation speed."}}
{"id": "cWHjTGwxK-N", "cdate": 1650995656941, "mdate": 1650995656941, "content": {"title": "Riemann localisation on the sphere", "abstract": "This paper first shows that the Riemann localisation property holds for the Fourier-Laplace series partial sum for sufficiently smooth functions on the two-dimensional sphere, but does not hold for spheres of higher dimension. By Riemann localisation on the sphere \ud835\udd4a\ud835\udc51\u2282\u211d\ud835\udc51+1, \ud835\udc51\u22652, we mean that for a suitable subset X of \ud835\udd43\ud835\udc5d(\ud835\udd4a\ud835\udc51), 1\u2264\ud835\udc5d\u2264\u221e, the \ud835\udd43\ud835\udc5d-norm of the Fourier local convolution of \ud835\udc53\u2208\ud835\udc4b converges to zero as the degree goes to infinity. The Fourier local convolution of f at \ud835\udc31\u2208\ud835\udd4a\ud835\udc51 is the Fourier convolution with a modified version of f obtained by replacing values of f by zero on a neighbourhood of \ud835\udc31. The failure of Riemann localisation for \ud835\udc51>2 can be overcome by considering a filtered version: we prove that for a sphere of any dimension and sufficiently smooth filter the corresponding local convolution always has the Riemann localisation property. Key tools are asymptotic estimates of the Fourier and filtered kernels."}}
{"id": "Bo1QJGsVgTh", "cdate": 1650995519476, "mdate": 1650995519476, "content": {"title": "Tight framelets and fast framelet filter bank transforms on manifolds", "abstract": "Tight framelets on a smooth and compact Riemannian manifold M provide a tool of multiresolution analysis for data from geosciences, astrophysics, medical sciences, etc. This work investigates the construction, characterizations, and applications of tight framelets on such a manifold M. Characterizations of the tightness of a sequence of framelet systems for L_2(M) in both the continuous and semi-discrete settings are provided. Tight framelets associated with framelet filter banks on M can then be easily designed and fast framelet filter bank transforms on M are shown to be realizable with nearly linear computational complexity. Explicit construction of tight framelets on the sphere S^2 as well as numerical examples are given."}}
{"id": "VSHxoWNhvqi", "cdate": 1650995248928, "mdate": 1650995248928, "content": {"title": "Fully discrete needlet approximation on the sphere", "abstract": "Spherical needlets are highly localized radial polynomials on the sphere , , with centers at the nodes of a suitable cubature rule. The original semidiscrete spherical needlet approximation of Narcowich, Petrushev and Ward is not computable, in that the needlet coefficients depend on inner product integrals. In this work we approximate these integrals by a second quadrature rule with an appropriate degree of precision, to construct a fully discrete needlet approximation. We prove that the resulting approximation is equivalent to filtered hyperinterpolation, that is to a filtered Fourier\u2013Laplace series partial sum with inner products replaced by appropriate cubature sums. It follows that the -error of discrete needlet approximation of order J for  and  has for a function f in the Sobolev space  the optimal rate of convergence in the sense of optimal recovery, namely . Moreover, this is achieved with a filter function that is of smoothness class , in contrast to the usually assumed . A numerical experiment for a class of functions in known Sobolev smoothness classes gives  errors for the fully discrete needlet approximation that are almost identical to those for the original semidiscrete needlet approximation. Another experiment uses needlets over the whole sphere for the lower levels together with high-level needlets with centers restricted to a local region. The resulting errors are reduced in the local region away from the boundary, indicating that local refinement in special regions is a promising strategy."}}
{"id": "cV8peb4G0gD", "cdate": 1650995005319, "mdate": 1650995005319, "content": {"title": "A new probe of Gaussianity and isotropy with application to cosmic microwave background maps", "abstract": "We introduce a new mathematical tool (a direction-dependent probe) to analyze the randomness of purported isotropic Gaussian random  fields on the sphere. If the  field is isotropic and Gaussian then the probe coefficients for a given direction should be realizations of uncorrelated scalar Gaussian random variables. To study the randomness of a field, we use the autocorrelation of the sequence of probe coefficients (which are just the Fourier coefficients a_{l,0} if the z-axis is taken in the probe direction). We introduce a particular function on the sphere (called the AC discrepancy) that accentuates the departure from Gaussianity and isotropy. We apply the probe to assess the full-sky cosmic microwave background (CMB) temperature maps produced by the Planck collaboration (PR2 2015 and PR3 2018), with special attention to the inpainted maps. We find that for some of the maps, there are many directions for which the departures are significant, especially near the galactic plane. We also look briefly at the noninpainted Planck maps, for which the computed AC discrepancy maps have a very different character, with features that are global rather than local."}}
{"id": "PgXwF1_Zuo", "cdate": 1650994370374, "mdate": 1650994370374, "content": {"title": "Grassmann graph pooling", "abstract": "Geometric deep learning that employs the geometric and topological features of data has attracted increasing attention in deep neural networks. Learning the intrinsic structure property of data is a crucial step for dimensionality reduction and effective feature extraction. This paper develops Grassmann graph embedding, which combines graph convolutions to capture the main components within graphs' hidden representations. Each set of featured graph nodes is mapped to a point on a Grassmann matrix manifold through Singular Value Decomposition, which is then embedded into a symmetric matrix space that approximates denoised second-order feature information. The view of treating nodes as a set could inspire many potential applications. In particular, we propose Grassmann (global graph) pooling that can connect with any graph convolution for graph neural networks. The Grassmann pooling achieves state-of-the-art performance on a variety of graph prediction benchmarks."}}
