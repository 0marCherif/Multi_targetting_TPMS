{"id": "UqiiZ9Xdlrw", "cdate": 1665251223265, "mdate": null, "content": {"title": "Distributional deep Q-learning with CVaR regression", "abstract": "Reinforcement learning (RL) allows an agent interacting sequentially with an environment to maximize its long-term return, in expectation. In distributional RL (DRL), the agent is also interested in the probability distribution of the return, not just its expected value. This so-called distributional perspective of RL has led to new algorithms with improved empirical performance. In this paper, we recall the atomic DRL (ADRL) framework based on atomic distributions projected via the Wasserstein-2 metric. Then, we derive two new deep ADRL algorithms, namely SAD-Q-learning and MAD-Q-learning (both for the control task). Numerical experiments on various environments compare our approach against existing deep (distributional) RL methods."}}
{"id": "BhEWBvS7Bgc", "cdate": 1609459200000, "mdate": 1645716829246, "content": {"title": "Robustness and risk management via distributional dynamic programming", "abstract": "In dynamic programming (DP) and reinforcement learning (RL), an agent learns to act optimally in terms of expected long-term return by sequentially interacting with its environment modeled by a Markov decision process (MDP). More generally in distributional reinforcement learning (DRL), the focus is on the whole distribution of the return, not just its expectation. Although DRL-based methods produced state-of-the-art performance in RL with function approximation, they involve additional quantities (compared to the non-distributional setting) that are still not well understood. As a first contribution, we introduce a new class of distributional operators, together with a practical DP algorithm for policy evaluation, that come with a robust MDP interpretation. Indeed, our approach reformulates through an augmented state space where each state is split into a worst-case substate and a best-case substate, whose values are maximized by safe and risky policies respectively. Finally, we derive distributional operators and DP algorithms solving a new control task: How to distinguish safe from risky optimal actions in order to break ties in the space of optimal policies?"}}
{"id": "WboSO9hS0np", "cdate": 1577836800000, "mdate": null, "content": {"title": "Weighted Empirical Risk Minimization: Sample Selection Bias Correction based on Importance Sampling", "abstract": "We consider statistical learning problems, when the distribution $P'$ of the training observations $Z'_1,\\; \\ldots,\\; Z'_n$ differs from the distribution $P$ involved in the risk one seeks to minimize (referred to as the test distribution) but is still defined on the same measurable space as $P$ and dominates it. In the unrealistic case where the likelihood ratio $\\Phi(z)=dP/dP'(z)$ is known, one may straightforwardly extends the Empirical Risk Minimization (ERM) approach to this specific transfer learning setup using the same idea as that behind Importance Sampling, by minimizing a weighted version of the empirical risk functional computed from the 'biased' training data $Z'_i$ with weights $\\Phi(Z'_i)$. Although the importance function $\\Phi(z)$ is generally unknown in practice, we show that, in various situations frequently encountered in practice, it takes a simple form and can be directly estimated from the $Z'_i$'s and some auxiliary information on the statistical population $P$. By means of linearization techniques, we then prove that the generalization capacity of the approach aforementioned is preserved when plugging the resulting estimates of the $\\Phi(Z'_i)$'s into the weighted empirical risk. Beyond these theoretical guarantees, numerical results provide strong empirical evidence of the relevance of the approach promoted in this article."}}
{"id": "9ihXU1nBUAK", "cdate": 1577836800000, "mdate": null, "content": {"title": "Weighted Emprirical Risk Minimization: Transfer Learning based on Importance Sampling", "abstract": ""}}
{"id": "Bye2uJHYwr", "cdate": 1569439603690, "mdate": null, "content": {"title": "Weighted Empirical Risk Minimization: Transfer Learning based on Importance Sampling", "abstract": "We consider statistical learning problems, when the distribution $P'$ of the training observations $Z'_1,\\; \\ldots,\\; Z'_n$ differs from the distribution $P$ involved in the risk one seeks to minimize (referred to as the \\textit{test distribution}) but is still defined on the same measurable space as $P$ and dominates it. In the unrealistic case where the likelihood ratio $\\Phi(z)=dP/dP'(z)$ is known, one may straightforwardly extends the Empirical Risk Minimization (ERM) approach to this specific \\textit{transfer learning} setup using the same idea as that behind Importance Sampling, by minimizing a weighted version of the empirical risk functional computed from the 'biased' training data $Z'_i$ with weights $\\Phi(Z'_i)$. Although the \\textit{importance function} $\\Phi(z)$ is generally unknown in practice, we show that, in various situations frequently encountered in practice, it takes a simple form and can be directly estimated from the $Z'_i$'s and some auxiliary information on the statistical population $P$. By means of linearization techniques, we then prove that the generalization capacity of the approach aforementioned is preserved when plugging the resulting estimates of the $\\Phi(Z'_i)$'s into the  weighted empirical risk. Beyond these theoretical guarantees, numerical results provide strong empirical evidence of the relevance of the approach promoted in this article."}}
{"id": "bpjsx300KgD", "cdate": 1546300800000, "mdate": null, "content": {"title": "Dimensionality Reduction and (Bucket) Ranking: a Mass Transportation Approach", "abstract": "Whereas most dimensionality reduction techniques (\\textit{e.g.} PCA, ICA, NMF) for multivariate data essentially rely on linear algebra to a certain extent, summarizing ranking data, viewed as real..."}}
{"id": "xoCl_bC8yLV", "cdate": 1514764800000, "mdate": null, "content": {"title": "Profitable Bandits", "abstract": "Originally motivated by default risk management applications, this paper investigates a novel problem, referred to as the profitable bandit problem here. At each step, an agent chooses a subset of the K possible actions. For each action chosen, she then receives the sum of a random number of rewards. Her objective is to maximize her cumulated earnings. We adapt and study three well-known strategies in this purpose, that were proved to be most efficient in other settings: kl-UCB, Bayes-UCB and Thompson Sampling. For each of them, we prove a finite time regret bound which, together with a lower bound we obtain as well, establishes asymptotic optimality. Our goal is also to compare these three strategies from a theoretical and empirical perspective both at the same time. We give simple, self-contained proofs that emphasize their similarities, as well as their differences. While both Bayesian strategies are automatically adapted to the geometry of information, the numerical experiments carried out show a slight advantage for Thompson Sampling in practice."}}
{"id": "dgRQ6QEVAdo", "cdate": 1514764800000, "mdate": null, "content": {"title": "Dimensionality Reduction and (Bucket) Ranking: a Mass Transportation Approach", "abstract": "Whereas most dimensionality reduction techniques (e.g. PCA, ICA, NMF) for multivariate data essentially rely on linear algebra to a certain extent, summarizing ranking data, viewed as realizations of a random permutation $\\Sigma$ on a set of items indexed by $i\\in \\{1,\\ldots,\\; n\\}$, is a great statistical challenge, due to the absence of vector space structure for the set of permutations $\\mathfrak{S}_n$. It is the goal of this article to develop an original framework for possibly reducing the number of parameters required to describe the distribution of a statistical population composed of rankings/permutations, on the premise that the collection of items under study can be partitioned into subsets/buckets, such that, with high probability, items in a certain bucket are either all ranked higher or else all ranked lower than items in another bucket. In this context, $\\Sigma$'s distribution can be hopefully represented in a sparse manner by a bucket distribution, i.e. a bucket ordering plus the ranking distributions within each bucket. More precisely, we introduce a dedicated distortion measure, based on a mass transportation metric, in order to quantify the accuracy of such representations. The performance of buckets minimizing an empirical version of the distortion is investigated through a rate bound analysis. Complexity penalization techniques are also considered to select the shape of a bucket order with minimum expected distortion. Beyond theoretical concepts and results, numerical experiments on real ranking data are displayed in order to provide empirical evidence of the relevance of the approach promoted."}}
{"id": "DVzHYLFM2F_", "cdate": 1514764800000, "mdate": null, "content": {"title": "Profitable Bandits", "abstract": "Originally motivated by default risk management applications, this paper investigates a novel problem, referred to as the \\emph{profitable bandit problem} here. At each step, an agent chooses a sub..."}}
{"id": "ZbILS45psm6", "cdate": 1483228800000, "mdate": null, "content": {"title": "Max K-armed bandit: On the ExtremeHunter algorithm and beyond", "abstract": "This paper is devoted to the study of the max K-armed bandit problem, which consists in sequentially allocating resources in order to detect extreme values. Our contribution is twofold. We first significantly refine the analysis of the ExtremeHunter algorithm carried out in Carpentier and Valko (2014), and next propose an alternative approach, showing that, remarkably, Extreme Bandits can be reduced to a classical version of the bandit problem to a certain extent. Beyond the formal analysis, these two approaches are compared through numerical experiments."}}
