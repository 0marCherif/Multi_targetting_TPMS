{"id": "kAUgrkn4LsY", "cdate": 1672531200000, "mdate": 1682318723862, "content": {"title": "Transformer-Based Cross-Modal Recipe Embeddings with Large Batch Training", "abstract": "Cross-modal recipe retrieval aims to exploit the relationships and accomplish mutual retrieval between recipe images and texts, which is clear for human but arduous to formulate. Although many previous works endeavored to solve this problem, most works did not efficiently exploit the cross-modal information among recipe data. In this paper, we present a frustratingly straightforward cross-modal recipe retrieval framework, Transformer-based Network for Large Batch Training\u00a0(TNLBT) achieving high performance on both recipe retrieval and image generation tasks, which is designed to efficiently exploit the rich cross-modal information. In our proposed framework, Transformer-based encoders are applied for both image and text encoding for cross-modal embedding learning. We also adopt several loss functions like self-supervised learning loss on recipe text to encourage the model to further promote the cross-modal embedding learning. Since contrastive learning could benefit from a larger batch size according to the recent literature on self-supervised learning, we adopt a large batch size during training and have validated its effectiveness. The experimental results showed that TNLBT significantly outperformed the current state-of-the-art frameworks in both cross-modal recipe retrieval and image generation tasks on the benchmark Recipe1M by a huge margin. We also found that CLIP-ViT performs better than ViT-B as the image encoder backbone. This is the first work which confirmed the effectiveness of large batch training on cross-modal recipe embedding learning."}}
{"id": "eGH4jmFSJFh", "cdate": 1672531200000, "mdate": 1682318723885, "content": {"title": "Virtual Try-On Considering Temporal Consistency for Videoconferencing", "abstract": "Virtual fitting, in which a person\u2019s image is changed to an arbitrary clothing image, is expected to be applied to shopping sites and videoconferencing. In real-time virtual fitting, image-based methods using a knowledge distillation technique can generate high-quality fitting images by inputting only the image of arbitrary clothing and a person without requiring the additional data like pose information. However, there are few studies that perform fast virtual fitting from arbitrary clothing images stably with real person images for situations such as videoconferencing considering temporal consistency. Therefore, the purpose of this demo is to perform robust virtual fitting with temporal consistency for videoconferencing. First, we created a virtual fitting system and verified how effective the existing fast image fitting method is for webcam video. The results showed that the existing methods do not adjust the dataset and do not consider temporal consistency, and thus are unstable for input images similar to videoconferencing. Therefore, we propose to train a model that adjusts the dataset to be similar to a videoconference and to add temporal consistency loss. Qualitative evaluation of the proposed model confirms that the model exhibits less flicker than the baseline. Figure\u00a01 shows an example usage of our try-on system which is running on Zoom."}}
{"id": "GgMh-AW5d4", "cdate": 1668590089196, "mdate": 1668590089196, "content": {"title": "Distinct Class-Specific Saliency Maps for Weakly Supervised Semantic Segmentation", "abstract": "In this paper, we deal with a weakly supervised semantic segmentation problem where only training images with image-level labels are available. We propose a weakly supervised semantic segmentation method which is based on CNN-based class-specific saliency maps and fully-connected CRF. To obtain distinct class-specific saliency maps which can be used as unary potentials of CRF, we propose a novel method to estimate class saliency maps which improves the method proposed by Simonyan et al. (2014) significantly by the following improvements: (1) using CNN derivatives with respect to feature maps of the intermediate convolutional layers with up-sampling instead of an input image; (2) subtracting the saliency maps of the other classes from the saliency maps of the target class to differentiate target objects from other objects; (3) aggregating multiple-scale class saliency maps to compensate lower resolution of the feature maps. After obtaining distinct class saliency maps, we apply fully-connected CRF by using the class maps as unary potentials. By the experiments, we show that the proposed method has outperformed state-of-the-art results with the PASCAL VOC 2012 dataset under the weakly-supervised setting."}}
{"id": "FH3Mwjb_H8B", "cdate": 1663849909190, "mdate": null, "content": {"title": "Focusing on what to decode and what to train: Efficient Training with HOI Split Decoders and Split Target Guided DeNoising", "abstract": "Recent one-stage transformer-based methods achieve notable gains in the Human-object Interaction Detection (HOI) task by leveraging the detection of DETR. However, the current methods redirect the detection target of the object decoder, and the box target is not explicitly separated from the query embeddings, which leads to long and hard training. Furthermore, matching the predicted HOI instances with the ground-truth is more challenging than object detection, simply adapting training strategies from the object detection makes the training more difficult. To clear the ambiguity between human and object detection, we propose a novel one-stage framework (SOV), which consists of a subject decoder, an object decoder, and a well-designed verb decoder. Three split decoders with explicitly defined box queries share the prediction burden and accelerate the training convergence. To further improve the training efficiency, we propose a novel Split Target Guided (STG) DeNoising strategy, which leverages learnable object label embeddings and verb label embeddings to guide the training. In addition, for the prediction part, the label-specific information is directly fed into the decoders by initializing the query embeddings from the learnable label embeddings. Extensive experiments show that our method (SOV-STG) achieves 3$\\times$ fewer training epochs and 4.68\\% higher accuracy than the state-of-the-art method."}}
{"id": "tZ6jrT7f73v", "cdate": 1652993282569, "mdate": 1652993282569, "content": {"title": "Cross-Modal Recipe Embeddings by Disentangling Recipe Contents and Dish Styles", "abstract": "Nowadays, cooking recipe sharing sites on the Web are widely used,\nand play a major role in everyday home cooking. Since cooking\nrecipes consist of dish photos and recipe texts, cross-modal recipe\nsearch is being actively explored. To enable cross-modal search,\nboth food image features and cooking text recipe features are embedded into the same shared space in general. However, in most of\nthe existing studies, a one-to-one correspondence between a recipe\ntext and a dish image in the embedding space is assumed, although\nan unlimited number of photos with different serving styles and\ndifferent plates can be associated with the same recipe.\nIn this paper, we propose a RDE-GAN (Recipe Disentangled\nEmbedding GAN) which separates food image information into a\nrecipe image feature and a non-recipe shape feature. In addition,\nwe generate a food image by integrating both the recipe embedding\nand a shape feature. Since the proposed embedding is free from\nserving and plate styles which are unrelated to cooking recipes,\nthe experimental results showed that it outperformed the existing\nmethods on cross-modal recipe search. We also confirmed that only\neither shape or recipe elements can be changed at the time of food\nimage generation."}}
{"id": "zBjR16ZToKq", "cdate": 1640995200000, "mdate": 1667956240375, "content": {"title": "Unseen Food Segmentation", "abstract": ""}}
{"id": "tq9tGFULyAq", "cdate": 1640995200000, "mdate": 1667956240703, "content": {"title": "Real Scale Hungry Networks: Real Scale 3D Reconstruction of a Dish and a Plate using Implicit Function and a Single RGB-D Image", "abstract": "The management of dietary calorie content using information technology has become an essential topic in the multimedia field of research in recent years. Therefore, many researchers and companies are conducting research and developing applications. Many methods for estimating the calorie content of a food use image recognition. However, these methods have a problem. They cannot consider the 3D heights and depths of the food because they only consider the food as a 2D object, even though the actual meal is 3D. To solve this problem, we would like to utilize 3D reconstruction techniques based on deep learning, developed in recent years, but most of these methods reconstruct the normalized objects. Being normalized means that the actual size is unknown, making it difficult to use them for estimating calories and nutritional value. In this paper, we propose a method using an implicit function representation that reconstructs the 3D shapes of a dish and plate as they are in real scale, using an RGB-D image and camera parameters."}}
{"id": "ngPtoEcYAH", "cdate": 1640995200000, "mdate": 1667956240406, "content": {"title": "DepthGrillCam: A Mobile Application for Real-time Eating Action Recording Using RGB-D Images", "abstract": "An automatic meal recording is one of typical applications of image recognition technology. In fact, some mobile apps on meal recording have been released so far. Most of the apps assume that a user takes a meal photo before start eating. However, this approach is not appropriate for the meals in which foods are served while taking meals such as food buffets, shared large plates and hot pots. In this study, we propose a mobile meal recording system that estimates food calories during eating in the real-time way by eating action recognition with RGB-D images obtained by a front-mounted depth sensor on a smartphone. % which is used for face recognition. In the experiments with the mobile app implemented for an iPhone, in the situation of eating grilled meat, the proposed system improved the accuracy of calorie estimation by up to 28% and recognized the correct meal category with 6.67 times higher accuracy in eating action recognition compared to the baseline system."}}
{"id": "ixSOP6-MZdw", "cdate": 1640995200000, "mdate": 1667956240597, "content": {"title": "Transformer-based Cross-Modal Recipe Embeddings with Large Batch Training", "abstract": "In this paper, we present a cross-modal recipe retrieval framework, Transformer-based Network for Large Batch Training (TNLBT), which is inspired by ACME~(Adversarial Cross-Modal Embedding) and H-T~(Hierarchical Transformer). TNLBT aims to accomplish retrieval tasks while generating images from recipe embeddings. We apply the Hierarchical Transformer-based recipe text encoder, the Vision Transformer~(ViT)-based recipe image encoder, and an adversarial network architecture to enable better cross-modal embedding learning for recipe texts and images. In addition, we use self-supervised learning to exploit the rich information in the recipe texts having no corresponding images. Since contrastive learning could benefit from a larger batch size according to the recent literature on self-supervised learning, we adopt a large batch size during training and have validated its effectiveness. In the experiments, the proposed framework significantly outperformed the current state-of-the-art frameworks in both cross-modal recipe retrieval and image generation tasks on the benchmark Recipe1M. This is the first work which confirmed the effectiveness of large batch training on cross-modal recipe embeddings."}}
{"id": "hx7msL7fxgL", "cdate": 1640995200000, "mdate": 1667956240333, "content": {"title": "StyleGAN-based CLIP-guided Image Shape Manipulation", "abstract": "In this paper, we propose a text-guided image manipulation method which focuses on editing shape attribute using text description. We combine an image generation model, StyleGAN2, and image-text matching model, CLIP, and we have achieved the goal of image shape attribute manipulation by modifying the parameters of the pretrained StyleGAN2 generator. Qualitative and quantitative evaluations are conducted to demonstrate the effectiveness of the proposed method."}}
