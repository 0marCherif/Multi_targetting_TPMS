{"id": "yNvSDthi12", "cdate": 1672531200000, "mdate": 1695973008965, "content": {"title": "Going Beyond Linear Mode Connectivity: The Layerwise Linear Feature Connectivity", "abstract": "Recent work has revealed many intriguing empirical phenomena in neural network training, despite the poorly understood and highly complex loss landscapes and training dynamics. One of these phenomena, Linear Mode Connectivity (LMC), has gained considerable attention due to the intriguing observation that different solutions can be connected by a linear path in the parameter space while maintaining near-constant training and test losses. In this work, we introduce a stronger notion of linear connectivity, Layerwise Linear Feature Connectivity (LLFC), which says that the feature maps of every layer in different trained networks are also linearly connected. We provide comprehensive empirical evidence for LLFC across a wide range of settings, demonstrating that whenever two trained networks satisfy LMC (via either spawning or permutation methods), they also satisfy LLFC in nearly all the layers. Furthermore, we delve deeper into the underlying factors contributing to LLFC, which reveal new insights into the spawning and permutation approaches. The study of LLFC transcends and advances our understanding of LMC by adopting a feature-learning perspective."}}
{"id": "tMsLnEztbgH", "cdate": 1672531200000, "mdate": 1695973008947, "content": {"title": "Can We Faithfully Represent Absence States to Compute Shapley Values on a DNN?", "abstract": ""}}
{"id": "Qf40Bu2sBQ", "cdate": 1672531200000, "mdate": 1695973008963, "content": {"title": "Defects of Convolutional Decoder Networks in Frequency Representation", "abstract": "In this paper, we prove the representation defects of a cascaded convolutional decoder network, considering the capacity of representing different frequency components of an input sample. We conduc..."}}
{"id": "0cm8HroIxJV", "cdate": 1663849947281, "mdate": null, "content": {"title": "Explaining Representation Bottlenecks of Convolutional Decoder Networks", "abstract": "In this paper, we prove representation bottlenecks of a cascaded convolutional decoder network, considering the capacity of representing different frequency components of an input sample. We conduct the discrete Fourier transform on each channel of the feature map in an intermediate layer of the decoder network. Then, we introduce the rule of the forward propagation of such intermediate-layer spectrum maps, which is equivalent to the forward propagation of feature maps through a convolutional layer. Based on this, we find that each frequency component in the spectrum map is forward propagated independently with other frequency components. Furthermore, we prove two bottlenecks in representing feature spectrums. First, we prove that the convolution operation, the zero-padding operation, and a set of other settings all make a convolutional decoder network more likely to weaken high-frequency components. Second, we prove that the upsampling operation generates a feature spectrum, in which strong signals repetitively appears at certain frequencies. We will release all codes when this paper is accepted."}}
{"id": "lMPJP3nRGtJ", "cdate": 1663849947161, "mdate": null, "content": {"title": "Batch Normalization Is Blind to the First and Second Derivatives of the Loss w.r.t. Features", "abstract": "We prove that when we do the Taylor series expansion of the loss function, the BN operation will block the influence of the first-order term and most influence of the second-order term of the loss. This is a potential defect of the BN operation. We also find that such a problem is caused by the standardization phase of the BN operation. We believe that the proof of the blindness of a deep model is of significant value to avoiding systemic collapses of a deep model, although such a blindness does not always makes significant damages in all applications. Experiments show that the BN operation significantly affects feature representations in specific tasks."}}
{"id": "YV8tP7bW6Kt", "cdate": 1663849944064, "mdate": null, "content": {"title": "Can We Faithfully Represent Absence States to Compute Shapley Values on a DNN?", "abstract": "Masking some input variables of a deep neural network (DNN) and computing output changes on the masked input sample represent a typical way to compute attributions of input variables in the sample. People usually mask an input variable using its baseline value. However, there is no theory to examine whether baseline value faithfully represents the absence of an input variable, i.e., removing all signals from the input variable. Fortunately, recent studies (Ren et al., 2023a; Deng et al., 2022a) show that the inference score of a DNN can be strictly disentangled into a set of causal patterns (or concepts) encoded by the DNN. Therefore, we propose to use causal patterns to examine the faithfulness of baseline values. More crucially, it is proven that causal patterns can be explained as the elementary rationale of the Shapley value. Furthermore, we propose a method to learn optimal baseline values, and experimental results have demonstrated its effectiveness."}}
{"id": "guDx6PeSPOZ", "cdate": 1640995200000, "mdate": 1683889285444, "content": {"title": "Batch Normalization Is Blind to the First and Second Derivatives of the Loss", "abstract": "In this paper, we prove the effects of the BN operation on the back-propagation of the first and second derivatives of the loss. When we do the Taylor series expansion of the loss function, we prove that the BN operation will block the influence of the first-order term and most influence of the second-order term of the loss. We also find that such a problem is caused by the standardization phase of the BN operation. Experimental results have verified our theoretical conclusions, and we have found that the BN operation significantly affects feature representations in specific tasks, where losses of different samples share similar analytic formulas."}}
{"id": "ZV3PZXrRDQ", "cdate": 1632875564167, "mdate": null, "content": {"title": "Towards a Game-Theoretic View of Baseline Values in the Shapley Value", "abstract": "This paper aims to formulate the problem of estimating optimal baseline values, which are used to compute the Shapley value in game theory. In the computation of Shapley values, people usually set an input variable to its baseline value to represent the absence of this variable. However, there are no studies on how to ensure that baseline values represent the absence states of variables without bringing in additional information, which ensures the trustworthiness of the Shapley value. To this end, previous studies usually determine baseline values in an empirical manner, which are not reliable. Therefore, we revisit the feature representation of a deep model in game theory, and formulate the absence state of an input variable. From the perspective of game-theoretic interaction, we learn the optimal baseline value of each input variable. Experimental results have demonstrated the effectiveness of our method. The code will be released when the paper is accepted."}}
{"id": "fMaIxda5Y6K", "cdate": 1621629779540, "mdate": null, "content": {"title": "Towards a Unified Game-Theoretic View of Adversarial Perturbations and Robustness", "abstract": "This paper provides a unified view to explain different adversarial attacks and defense methods, i.e. the view of multi-order interactions between input variables of DNNs. Based on the multi-order interaction, we discover that adversarial attacks mainly affect high-order interactions to fool the DNN. Furthermore, we find that the robustness of adversarially trained DNNs comes from category-specific low-order interactions. Our findings provide a potential method to unify adversarial perturbations and robustness, which can explain the existing robustness-boosting methods in a principle way. Besides, our findings also make a revision of previous inaccurate understanding of the shape bias of adversarially learned features. Our code is available online at https://github.com/Jie-Ren/A-Unified-Game-Theoretic-Interpretation-of-Adversarial-Robustness."}}
{"id": "OlUDQWhg3aO", "cdate": 1609459200000, "mdate": 1648702819160, "content": {"title": "Learning Baseline Values for Shapley Values", "abstract": "This paper aims to formulate the problem of estimating the optimal baseline values for the Shapley value in game theory. The Shapley value measures the attribution of each input variable of a complex model, which is computed as the marginal benefit from the presence of this variable w.r.t.its absence under different contexts. To this end, people usually set the input variable to its baseline value to represent the absence of this variable (i.e.the no-signal state of this variable). Previous studies usually determine the baseline values in an empirical manner, which hurts the trustworthiness of the Shapley value. In this paper, we revisit the feature representation of a deep model from the perspective of game theory, and define the multi-variate interaction patterns of input variables to define the no-signal state of an input variable. Based on the multi-variate interaction, we learn the optimal baseline value of each input variable. Experimental results have demonstrated the effectiveness of our method."}}
