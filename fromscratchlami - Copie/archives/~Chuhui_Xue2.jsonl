{"id": "nUmCcZ5RKF", "cdate": 1663849825475, "mdate": null, "content": {"title": "IS SYNTHETIC DATA FROM GENERATIVE MODELS READY FOR IMAGE RECOGNITION?", "abstract": "Recent text-to-image generation models have shown promising results in generating high-fidelity photo-realistic images. Though the results are astonishing to human eyes, how applicable these generated images are for recognition tasks remains under-explored. In this work, we extensively study whether and how synthetic images generated from state-of-the-art text-to-image generation models can be used for image recognition tasks, and focus on two perspectives: synthetic data for improving classification models in the data-scare settings (i.e. zero-shot and few-shot), and synthetic data for large-scale model pre-training for transfer learning. We showcase the powerfulness and shortcomings of synthetic data from existing generative models, and propose strategies for better applying synthetic data for recognition tasks. Code: https://github.com/CVMI-Lab/SyntheticData. "}}
{"id": "n2EEbUzETI", "cdate": 1632875456212, "mdate": null, "content": {"title": "Contextual Text Detection", "abstract": "Most existing scene text detectors focus on the detection of characters or words which capture partial textual messages only in most cases due to the missing of contextual information. For a better understanding of text in scenes, it is more desired to detect contextual text blocks which consist of one or multiple integral text units (e.g., characters, words, or phrases) in a specific order, delivering certain independent and complete textual messages. This paper presents Contextual Text Detection, a new setup that detects contextual text blocks for better understanding of texts in scenes. We formulate the new setup by a dual detection task that first detects integral text units and then groups them into a contextual text block. Specifically, we design a novel scene text grouping technique which treats each integral text unit as a token and groups multiple integral tokens belonging to the same contextual text block into an ordered token sequence. To facilitate the future research, we create two new datasets SCUT-CTW-Context and ReCTS-Context where each contextual text block is well annotated by an ordered sequence of integral text units. In addition, we introduce three evaluation metrics that measure contextual text detection in local accuracy, continuity, and global accuracy, respectively. Extensive experiments show that the proposed method detects contextual text blocks effectively. This development including codes, datasets and annotation tools will be published at http://xxxxxxx."}}
{"id": "rkbby5-u-S", "cdate": 1514764800000, "mdate": null, "content": {"title": "Verisimilar Image Synthesis for Accurate Detection and Recognition of Texts in Scenes", "abstract": "The requirement of large amounts of annotated images has become one grand challenge while training deep neural network models for various visual detection and recognition tasks. This paper presents a novel image synthesis technique that aims to generate a large amount of annotated scene text images for training accurate and robust scene text detection and recognition models. The proposed technique consists of three innovative designs. First, it realizes \u201csemantic coherent\u201d synthesis by embedding texts at semantically sensible regions within the background image, where the semantic coherence is achieved by leveraging the semantic annotations of objects and image regions that have been created in the prior semantic segmentation research. Second, it exploits visual saliency to determine the embedding locations within each semantic sensible region, which coincides with the fact that texts are often placed around homogeneous regions for better visibility in scenes. Third, it designs an adaptive text appearance model that determines the color and brightness of embedded texts by learning from the feature of real scene text images adaptively. The proposed technique has been evaluated over five public datasets and the experiments show its superior performance in training accurate and robust scene text detection and recognition models."}}
{"id": "ByWPy5-OWB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Accurate Scene Text Detection Through Border Semantics Awareness and Bootstrapping", "abstract": "This paper presents a scene text detection technique that exploits bootstrapping and text border semantics for accurate localization of texts in scenes. A novel bootstrapping technique is designed which samples multiple \u2018subsections\u2019 of a word or text line and accordingly relieves the constraint of limited training data effectively. At the same time, the repeated sampling of text \u2018subsections\u2019 improves the consistency of the predicted text feature maps which is critical in predicting a single complete instead of multiple broken boxes for long words or text lines. In addition, a semantics-aware text border detection technique is designed which produces four types of text border segments for each scene text. With semantics-aware text borders, scene texts can be localized more accurately by regressing text pixels around the ends of words or text lines instead of all text pixels which often leads to inaccurate localization while dealing with long words or text lines. Extensive experiments demonstrate the effectiveness of the proposed techniques, and superior performance is obtained over several public datasets, e.g. 80.1 f-score for the MSRA-TD500, 67.1 f-score for the ICDAR2017-RCTW, etc."}}
