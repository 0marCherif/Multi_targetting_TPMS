{"id": "g6e-VkDe_rJ", "cdate": 1676827079152, "mdate": null, "content": {"title": "Efficient Privacy-Preserving Stochastic Nonconvex Optimization", "abstract": "While many solutions for privacy-preserving convex empirical risk minimization (ERM) have been developed, privacy-preserving nonconvex ERM remains a challenge. We study nonconvex ERM, which takes the form of minimizing a finite-sum of nonconvex loss functions over a training set. We propose a new differentially private stochastic gradient descent algorithm for nonconvex ERM that achieves strong privacy guarantees efficiently, and provide a tight analysis of its privacy and utility guarantees, as well as its gradient complexity. Our algorithm reduces gradient complexity while matching the best-known utility guarantee. Our experiments on benchmark nonconvex ERM problems demonstrate superior performance in terms of both training cost and utility gains compared with previous differentially private methods using the same privacy budgets."}}
{"id": "KKfjOEvDwQ", "cdate": 1664731452750, "mdate": null, "content": {"title": "Distributed Online and Bandit Convex Optimization", "abstract": "We study the problems of distributed online and bandit convex optimization against an adaptive adversary. Our goal is to minimize the average regret on M machines working in parallel over T rounds that can communicate R times intermittently. Assuming the underlying cost functions are convex, our results show collaboration is not beneficial if the machines have access to the first-order gradient information at the queried points. We show that in this setting, simple non-collaborative algorithms are min-max optimal, as opposed to the case for stochastic functions, where each machine samples the cost functions from a fixed distribution. Next, we consider the more challenging setting of federated optimization with bandit (zeroth-order) feedback, where the machines can only access values of the cost functions at the queried points. The key finding here is to identify the high-dimensional regime where collaboration is beneficial and may even lead to a linear speedup in the number of machines. Our results are the first attempts towards bridging the gap between distributed online optimization against stochastic and adaptive adversaries."}}
{"id": "SNElc7QmMDe", "cdate": 1652737602689, "mdate": null, "content": {"title": "Towards Optimal Communication Complexity in Distributed Non-Convex Optimization", "abstract": "We study the problem of distributed stochastic non-convex optimization with intermittent communication. We consider the full participation setting where $M$ machines work in parallel over $R$ communication rounds and the partial participation setting where $M$ machines are sampled independently every round from some meta-distribution over machines. We propose and analyze a new algorithm that improves existing methods by requiring fewer and lighter variance reduction operations. We also present lower bounds, showing our algorithm is either $\\textit{optimal}$ or $\\textit{almost optimal}$ in most settings. Numerical experiments demonstrate the superior performance of our algorithm."}}
{"id": "a68yJLSKY-P", "cdate": 1632875720432, "mdate": null, "content": {"title": "Adaptive Differentially Private Empirical Risk Minimization ", "abstract": "We propose an adaptive (stochastic) gradient perturbation method for differentially private  empirical risk minimization. At each iteration, the random noise added to the gradient is optimally adapted to the stepsize; we name this process adaptive differentially private (ADP) learning.  Given the same privacy budget, we prove that the ADP method considerably improves the utility guarantee compared to the standard differentially private method in which vanilla random noise is added. Our method is particularly useful for gradient-based algorithms with non-constant learning rate, including variants of AdaGrad (Duchi et al., 2011). We provide extensive numerical experiments to demonstrate the effectiveness of the proposed adaptive differentially private algorithm."}}
{"id": "oCULH4gtmbN", "cdate": 1609459200000, "mdate": 1681490497069, "content": {"title": "Adaptive Differentially Private Empirical Risk Minimization", "abstract": ""}}
{"id": "cjIP9sJwPT4", "cdate": 1609459200000, "mdate": 1632867915159, "content": {"title": "Variance-reduced First-order Meta-learning for Natural Language Processing Tasks", "abstract": "Lingxiao Wang, Kevin Huang, Tengyu Ma, Quanquan Gu, Jing Huang. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021."}}
{"id": "QdTngwUvHfI", "cdate": 1609459200000, "mdate": 1658520989551, "content": {"title": "Revisiting Membership Inference Under Realistic Assumptions", "abstract": ""}}
{"id": "KCO3kUDpQZy", "cdate": 1609459200000, "mdate": 1681490496932, "content": {"title": "Towards Efficient and Effective Privacy-Preserving Machine Learning", "abstract": ""}}
{"id": "IkYAx-XOKgn", "cdate": 1596118184102, "mdate": null, "content": {"title": "A Unified Variance Reduction-Based Framework for Nonconvex Low-Rank Matrix Recovery", "abstract": "We propose a generic framework based on a new stochastic variance-reduced gradient descent algorithm for accelerating nonconvex low-rank matrix recovery. Starting from an appropriate initial estimator, our proposed algorithm performs projected gradient descent based on a novel semi-stochastic gradient specifically designed for low-rank matrix recovery. Based upon the mild restricted strong convexity and smoothness conditions, we derive a projected notion of the restricted Lipschitz continuous gradient property, and prove that our algorithm enjoys linear convergence rate to the unknown low-rank matrix with an improved computational complexity. Moreover, our algorithm can be employed to both noiseless and noisy observations, where the (near) optimal sample complexity and statistical rate can be attained respectively. We further illustrate the superiority of our generic framework through several specific examples, both theoretically and experimentally."}}
{"id": "Inr9m_BpQE3", "cdate": 1596118028507, "mdate": null, "content": {"title": "A Primal-Dual Analysis of Global Optimality in Nonconvex Low-Rank Matrix Recovery", "abstract": "We propose a primal-dual based framework for analyzing the global optimality of nonconvex lowrank matrix recovery. Our analysis are based on the restricted strongly convex and smooth conditions, which can be verified for a broad family of loss functions. In addition, our analytic framework can directly handle the widely-used incoherence constraints through the lens of duality. We illustrate the applicability of the proposed framework to matrix completion and one-bit matrix completion, and prove that all these problems have no spurious local minima. Our results not only improve the sample complexity required for characterizing the global optimality of matrix completion, but also resolve an open problem in Ge et al. (2017) regarding one-bit matrix completion. Numerical experiments show that primaldual based algorithm can successfully recover the global optimum for various low-rank problems."}}
