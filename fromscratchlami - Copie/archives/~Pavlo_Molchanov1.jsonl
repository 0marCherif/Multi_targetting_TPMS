{"id": "mKNAOg7CLX", "cdate": 1663850394872, "mdate": null, "content": {"title": "Towards Dynamic Sparsification by Iterative Prune-Grow LookAheads", "abstract": "Model sparsification is a process of removing redundant connections in a neural network, making it more compact and faster. Most pruning methods start with a dense pretrained model, which is computationally intensive to train. Other pruning approaches perform compression at initialization which saves training time, however, at the cost of final accuracy as an unreliable architecture can be selected given weak feature representation. In this work, we re-formulate network sparsification as an exploitation-exploration process during initial training to enable dynamic learning of network sparsification. The exploitation phase assumes architecture stability and trains it to maximize accuracy. Whereas the exploration phase challenges the current architecture with a novel $\\textit{LookAhead}$ step that reactivates pruned parameters, quickly updates them together with existing ones, and reconfigures the sparse architecture with a pruning-growing paradigm. We demonstrate that $\\textit{LookAhead}$ methodology can effectively and efficiently oversee both architecture and performance during training, enabling early pruning with a capability of future recovery to correct previous poor pruning selections. Extensive results on ImageNet and CIFAR datasets show consistent improvements over the prior art by large margins, for varying networks towards both structured and unstructured sparsity. For example, our method surpasses recent work by $+1.3\\%$ top-1 accuracy at the same compression ratio for ResNet50-ImageNet unstructured sparsity. Moreover, our structured sparsity results also improve upon the previous best hardware-aware pruning method by $+0.8\\%$ top-1 accuracy for MobileNet-ImageNet sparsification, offering $+134$ in hardware FPS(im/s), while halving the training cost."}}
{"id": "f7VHa2mwDEq", "cdate": 1663850006341, "mdate": null, "content": {"title": "Heterogeneous Continual Learning", "abstract": "We propose a novel framework and a solution to tackle the continual learning (CL) problem with progressive evolution of neural networks. Most CL methods focus on adapting a single network to a new task/class by modifying its weights. However, with rapid progress in architecture design, the problem of adapting existing solutions to novel architectures becomes relevant. For the first time, we propose Heterogeneous Continual Learning (HCL) to address this problem, where a wide range of evolving network architectures emerge continually together with novel data/tasks. As a solution, we build on top of the distillation family of techniques and modify it to a new setting where a weaker model takes the role of a teacher; meanwhile, a new stronger architecture acts as a student. Furthermore, we consider a setup of limited access to previous data and propose Quick Deep Inversion (QDI) to recover prior task visual features to support knowledge transfer. QDI significantly reduces computational costs compared to previous solutions and improves overall performance. In summary, we propose a new setup for CL with a modified knowledge distillation paradigm and design a quick data inversion method to enhance distillation. Our evaluation of various benchmarks shows that the proposed method can successfully progress over various networks while outperforming state-of-the-art methods with a 2x improvement on accuracy."}}
{"id": "HZJje06x6IO", "cdate": 1663849937817, "mdate": null, "content": {"title": "Global Context Vision Transformers", "abstract": "We propose global context vision transformer (GC ViT), a novel architecture that enhances parameter and compute utilization for computer vision tasks. The core of the novel model are  global context self-attention modules, joint with standard local self-attention, to effectively yet efficiently model both long and short-range spatial interactions, as an alternative to complex operations such as an attention masks or local windows shifting. While the local self-attention modules are responsible for modeling short-range information, the global query tokens are shared across all global self-attention modules to interact with local key and values. In addition, we address the lack of inductive bias in ViTs and improve the modeling of inter-channel dependencies by proposing a novel downsampler which leverages a parameter-efficient fused inverted residual block. The proposed GC ViT achieves new state-of-the-art performance across image classification, object detection and semantic segmentation tasks. On ImageNet-1K dataset for classification, the tiny, small and base variants of GC ViT with 28M, 51M and 90M parameters achieve 83.4%, 83.9% and 84.4% Top-1 accuracy, respectively, surpassing comparably-sized prior art such as CNN-based ConvNeXt and ViT-based Swin Transformer. Pre-trained GC ViT backbones in downstream tasks of object detection, instance segmentation, and semantic segmentation on MS COCO and ADE20K datasets outperform prior work consistently, sometimes by large margins."}}
{"id": "cUOR-_VsavA", "cdate": 1652737852946, "mdate": null, "content": {"title": "Structural Pruning via Latency-Saliency Knapsack", "abstract": "Structural pruning can simplify network architecture and improve inference speed. We propose Hardware-Aware Latency Pruning (HALP) that formulates structural pruning as a global resource allocation optimization problem, aiming at maximizing the accuracy while constraining latency under a predefined budget on targeting device. For filter importance ranking, HALP leverages latency lookup table to track latency reduction potential and global saliency score to gauge accuracy drop. Both metrics can be evaluated very efficiently during pruning, allowing us to reformulate global structural pruning under a reward maximization problem given target constraint. This makes the problem solvable via our augmented knapsack solver, enabling HALP to surpass prior work in pruning efficacy and accuracy-efficiency trade-off. We examine HALP on both classification and detection tasks, over varying networks, on ImageNet and VOC datasets, on different platforms. In particular, for ResNet-50/-101 pruning on ImageNet, HALP improves network throughput by $1.60\\times$/$1.90\\times$ with $+0.3\\%$/$-0.2\\%$ top-1 accuracy changes, respectively. For SSD pruning on VOC, HALP improves throughput by $1.94\\times$ with only a $0.56$ mAP drop. HALP consistently outperforms prior art, sometimes by large margins. Project page at \\url{https://halp-neurips.github.io/}."}}
{"id": "RmzNH3A1cWc", "cdate": 1632875697303, "mdate": null, "content": {"title": "Hardware-Aware Network Transformation", "abstract": "In this paper, we tackle the problem of network acceleration by proposing hardware-aware network transformation (HANT), an approach that builds on neural architecture search techniques and teacher-student distillation. HANT consists of two phases: in the first phase, it trains many alternative operations for every layer of the teacher network using layer-wise feature map distillation.  In the second phase, it solves the combinatorial selection of efficient operations using a novel constrained integer linear optimization approach. In extensive experiments, we show that HANT can successfully accelerate three different families of network architectures (EfficientNetsV1, EfficientNetsV2 and ResNests), over two different target hardware platforms with minimal loss of accuracy. For example, HANT accelerates EfficientNetsV1-B6 by 3.6 with <0.4% drop in top-1 accuracy on ImageNet. When comparing the same latency level, HANT can accelerate EfficientNetV1-B4 to the same latency as EfficientNetV1-B1 while achieving 3% higher accuracy. We also show that applying HANT to EfficientNetV1 results in the automated discovery of the same (qualitative) architecture modifications later incorporated in EfficientNetV2. Finally, HANT\u2019s efficient search allows us to examine a large pool of 197 operations per layer, resulting in new insights into the accuracy-latency tradeoffs for different operations."}}
{"id": "LzBBxCg-xpa", "cdate": 1632875531835, "mdate": null, "content": {"title": "NViT: Vision Transformer Compression and Parameter Redistribution", "abstract": "Transformers yield state-of-the-art results across many tasks. However, they still impose huge computational costs during inference. We apply global, structural pruning with latency-aware regularization on all parameters of the Vision Transformer (ViT) model for latency reduction. Furthermore, we analyze the pruned architectures and find interesting regularities in the final weight structure. Our discovered insights lead to a new architecture called NViT (Novel ViT), with a redistribution of where parameters are used. This architecture utilizes parameters more efficiently and enables control of the latency-accuracy trade-off. On ImageNet-1K, we prune the DEIT-Base (Touvron et al., 2021) model to a 2.6$\\times$ FLOPs reduction, 5.1$\\times$ parameter reduction, and 1.9$\\times$ run-time speedup with only 0.07% loss in accuracy. We achieve more than 1% accuracy gain when compressing the base model to the throughput of the Small/Tiny variants. NViT gains 0.1-1.1% accuracy over the hand-designed DEIT family when trained from scratch, while being faster."}}
{"id": "jgAl403zfau", "cdate": 1632875471727, "mdate": null, "content": {"title": "HALP: Hardware-Aware Latency Pruning", "abstract": "Structural pruning can simplify network architecture and improve the inference speed. We propose Hardware-Aware Latency Pruning (HALP) that formulates structural pruning as a global resource allocation optimization problem, aiming at maximizing the accuracy while constraining latency under a predefined budget. For filter importance ranking, HALP leverages latency lookup table to track latency reduction potential and global saliency score to gauge on accuracy drop. Both metrics can be evaluated very efficiently during pruning, allowing us to reformulate global structural pruning under a reward maximization problem given target constraint. This makes the problem solvable via our augmented knapsack solver, enabling HALP to surpass prior work in pruning efficacy and accuracy-efficiency trade-off. We examine HALP on both classification and detection tasks, over varying networks, on ImageNet1K and VOC datasets. In particular for ResNet-50/-101 pruning on ImageNet1K, HALP improves network speed by $1.60\\times$/$1.90\\times$ with $+0.3\\%$/$-0.2\\%$ top-1 accuracy changes, respectively. For SSD pruning on VOC, HALP improves throughput by $1.94\\times$ with only a $0.56$ mAP drop. HALP consistently outperforms prior art, sometimes by large margins."}}
{"id": "b1lMDqXeYNF", "cdate": 1620733631563, "mdate": null, "content": {"title": "Optimal Quantization using Scaled Codebook ", "abstract": "We study the problem of quantizingNsorted, scalar datapoints with a fixed codebook containing K entries that are allowed to be rescaled.  The problem is defined as finding the optimal scaling factor \u03b1 and the datapoint assignments into  the \u03b1-scaled  codebook  to  minimize  the  squared  error between original and quantized points.  Previously, the globally optimal algorithms for this problem were derived only for certain codebooks (binary and ternary) or under the assumption of certain distributions (Gaussian,  Laplacian).  By studying the properties of the optimal quantizer, we derive an O(NKlogK) algorithm that is guaranteed to  find  the  optimal  quantization  parameters  for  any  fixed codebook regardless of data distribution.  We apply our algorithm to synthetic and real-world neural network quantization problems and demonstrate the effectiveness of ourapproach."}}
{"id": "G_xm7S5Bqe", "cdate": 1581936611168, "mdate": null, "content": {"title": "Improving Landmark Localization with Semi-Supervised Learning", "abstract": "We present two techniques to improve landmark localization in images from partially annotated datasets. Our\nprimary goal is to leverage the common situation where precise landmark locations are only provided for a small data\nsubset, but where class labels for classification or regression tasks related to the landmarks are more abundantly\navailable. First, we propose the framework of sequential\nmultitasking and explore it here through an architecture for\nlandmark localization where training with class labels acts\nas an auxiliary signal to guide the landmark localization on\nunlabeled data. A key aspect of our approach is that errors\ncan be backpropagated through a complete landmark localization model. Second, we propose and explore an unsupervised learning technique for landmark localization based on\nhaving a model predict equivariant landmarks with respect\nto transformations applied to the image. We show that these\ntechniques, improve landmark prediction considerably and\ncan learn effective detectors even when only a small fraction of the dataset has landmark labels. We present results\non two toy datasets and four real datasets, with hands and\nfaces, and report new state-of-the-art on two datasets in the\nwild, e.g. with only 5% of labeled images we outperform\nprevious state-of-the-art trained on the AFLW dataset."}}
{"id": "SyeDZGzH3S", "cdate": 1574408703010, "mdate": null, "content": {"title": "Few-Shot Adaptive Gaze Estimation", "abstract": "Inter-personal anatomical differences limit the accuracy of person-independent gaze estimation networks. Yet there is a need to lower gaze errors further to enable applications requiring higher quality. Further gains can be achieved by personalizing gaze networks, ideally with few calibration samples. However, over-parameterized neural networks are not amenable to learning from few examples as they can quickly over-fit. We embrace these challenges and propose a novel framework for Few-shot Adaptive GaZE Estimation (FAZE) for learning person-specific gaze networks with very few (less than or equal to 9) calibration samples. FAZE learns a rotation-aware latent representation of gaze via a disentangling encoder-decoder architecture along with a highly adaptable gaze estimator trained using meta-learning. It is capable of adapting to any new person to yield significant performance gains with as few as 3 samples, yielding state-of-the-art performance of 3.18 degrees on GazeCapture, a 19% improvement over prior art. We open-source our code at https://github.com/NVlabs/few_shot_gaze."}}
