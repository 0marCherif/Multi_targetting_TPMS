{"id": "QOpAjC3k-mA", "cdate": 1577836800000, "mdate": null, "content": {"title": "On Tractable Representations of Binary Neural Networks.", "abstract": "We consider the compilation of a binary neural network's decision function into tractable representations such as Ordered Binary Decision Diagrams (OBDDs) and Sentential Decision Diagrams (SDDs). Obtaining this function as an OBDD/SDD facilitates the explanation and formal verification of a neural network's behavior. First, we consider the task of verifying the robustness of a neural network, and show how we can compute the expected robustness of a neural network, given an OBDD/SDD representation of it. Next, we consider a more efficient approach for compiling neural networks, based on a pseudo-polynomial time algorithm for compiling a neuron. We then provide a case study in a handwritten digits dataset, highlighting how two neural networks trained from the same dataset can have very high accuracies, yet have very different levels of robustness. Finally, in experiments, we show that it is feasible to obtain compact representations of neural networks as SDDs."}}
{"id": "tuHZDEPjyRm", "cdate": 1546300800000, "mdate": null, "content": {"title": "Structured Bayesian Networks: From Inference to Learning with Routes.", "abstract": "Structured Bayesian networks (SBNs) are a recently proposed class of probabilistic graphical models which integrate background knowledge in two forms: conditional independence constraints and Boolean domain constraints. In this paper, we propose the first exact inference algorithm for SBNs, based on compiling a given SBN to a Probabilistic Sentential Decision Diagram (PSDD). We further identify a tractable subclass of SBNs, which have PSDDs of polynomial size. These SBNs yield a tractable model of route distributions, whose structure can be learned from GPS data, using a simple algorithm that we propose. Empirically, we demonstrate the utility of our inference algorithm, showing that it can be an order-ofmagnitude more efficient than more traditional approaches to exact inference. We demonstrate the utility of our learning algorithm, showing that it can learn more accurate models and classifiers from GPS data."}}
{"id": "tIQPheVvFp0", "cdate": 1546300800000, "mdate": null, "content": {"title": "Compiling Bayesian Network Classifiers into Decision Graphs.", "abstract": "We propose an algorithm for compiling Bayesian network classifiers into decision graphs that mimic the input and output behavior of the classifiers. In particular, we compile Bayesian network classifiers into ordered decision graphs, which are tractable and can be exponentially smaller in size than decision trees. This tractability facilitates reasoning about the behavior of Bayesian network classifiers, including the explanation of decisions they make. Our compilation algorithm comes with guarantees on the time of compilation and the size of compiled decision graphs. We apply our compilation algorithm to classifiers from the literature and discuss some case studies in which we show how to automatically explain their decisions and verify properties of their behavior."}}
{"id": "dxBkpt5tnf", "cdate": 1546300800000, "mdate": null, "content": {"title": "Verifying Binarized Neural Networks by Angluin-Style Learning.", "abstract": "We consider the problem of verifying the behavior of binarized neural networks on some input region. We propose an Angluin-style learning algorithm to compile a neural network on a given region into an Ordered Binary Decision Diagram (OBDD), using a SAT solver as an equivalence oracle. The OBDD allows us to efficiently answer a range of verification queries, including counting, computing the probability of counterexamples, and identifying common characteristics of counterexamples. We also present experimental results on verifying binarized neural networks that recognize images of handwritten digits."}}
{"id": "KTvrdRCr8VH", "cdate": 1546300800000, "mdate": null, "content": {"title": "On the relative expressiveness of Bayesian and neural networks.", "abstract": "A neural network computes a function. A central property of neural networks is that they are \u201cuniversal approximators:\u201d for a given continuous function, there exists a neural network that can approximate it arbitrarily well, given enough neurons (and some additional assumptions). In contrast, a Bayesian network is a model, but each of its queries can be viewed as computing a function. In this paper, we identify some key distinctions between the functions computed by neural networks and those by marginal Bayesian network queries, showing that the former are more expressive than the latter. Moreover, we propose a simple augmentation to Bayesian networks (a testing operator), which enables their marginal queries to become \u201cuniversal approximators.\u201d Previous article in issue Next article in issue"}}
{"id": "HyblF3Wd-S", "cdate": 1546300800000, "mdate": null, "content": {"title": "Conditional Independence in Testing Bayesian Networks", "abstract": "Testing Bayesian Networks (TBNs) were introduced recently to represent a set of distributions, one of which is selected based on the given evidence and used for reasoning. TBNs are more expressive ..."}}
{"id": "wxyqc7jFnaH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Formal Verification of Bayesian Network Classifiers.", "abstract": "A new approach was recently proposed for {\\em explaining} the decisions made by Bayesian network classifiers. This approach is based on first compiling a given classifier (i.e., its decision functi..."}}
{"id": "_Y2KjEKHzQQ", "cdate": 1514764800000, "mdate": null, "content": {"title": "On the Relative Expressiveness of Bayesian and Neural Networks.", "abstract": "A neural network computes a function. A central property of neural networks is that they are \"universal approximators:\" for a given continuous function, there exists a neural network that can approximate it arbitrarily well, given enough neurons (and some additional assumptions). In contrast, a Bayesian network is a model, but each of its queries can be viewed as computing a function. In this paper, we identify some key distinctions between the functions computed by neural networks and those by marginal Bayesian network queries, showing that the former are more expressive than the latter. Moreover, we propose a simple augmentation to Bayesian networks (a testing operator), which enables their marginal queries to become \"universal approximators.\""}}
{"id": "_B186bjAgtr", "cdate": 1514764800000, "mdate": null, "content": {"title": "On the Relative Expressiveness of Bayesian and Neural Networks.", "abstract": "A neural network computes a function. A central property of neural networks is that they are \u201cuniversal approximators:\u201d for a given continuous function, there exists a neural network that can approximate it arbitrarily well, given enough neurons (and some additional assumptions). In contrast, a Bayesian network is a model, but each of its queries can be viewed as computing a function. In this paper, we identify some key distinctions between the functions computed by neural networks and those by Bayesian network queries, showing that the former are more expressive than the latter. Moreover, we propose a simple augmentation to Bayesian networks (a testing operator), which enables their queries to become \u201cuniversal approximators\u201d as well."}}
{"id": "UlXF2NFe6M", "cdate": 1514764800000, "mdate": null, "content": {"title": "On pruning with the MDL Score.", "abstract": "Highlights \u2022 We develop techniques for pruning the search space of Bayesian networks, when enumerating the k-best structures from data. \u2022 We identify pruning techniques that apply to score-based structure learning in general, and specific ones for MDL scores. \u2022 Empirically, we show our techniques can allow state-of-the-art methods to scale to larger datasets with more variables. Abstract The space of Bayesian network structures is prohibitively large and hence numerous techniques have been developed to prune this search space, but without eliminating the optimal structure. Such techniques are critical for structure learning to scale to larger datasets with more variables. Prior works exploited properties of the MDL score to prune away large regions of the search space that can be safely ignored by optimal structure learning algorithms. In this paper, we propose new techniques for pruning regions of the search space that can be safely ignored by algorithms that enumerate the k -best Bayesian network structures. Empirically, these techniques allow a state-of-the-art structure enumeration algorithm to scale to datasets with significantly more variables. Previous article in issue Next article in issue"}}
