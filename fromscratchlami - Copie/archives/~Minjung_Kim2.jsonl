{"id": "X2HpTuEyrpA", "cdate": 1672531200000, "mdate": 1698631425421, "content": {"title": "EP2P-Loc: End-to-End 3D Point to 2D Pixel Localization for Large-Scale Visual Localization", "abstract": "Visual localization is the task of estimating a 6-DoF camera pose of a query image within a provided 3D reference map. Thanks to recent advances in various 3D sensors, 3D point clouds are becoming a more accurate and affordable option for building the reference map, but research to match the points of 3D point clouds with pixels in 2D images for visual localization remains challenging. Existing approaches that jointly learn 2D-3D feature matching suffer from low inliers due to representational differences between the two modalities, and the methods that bypass this problem into classification have an issue of poor refinement. In this work, we propose EP2P-Loc, a novel large-scale visual localization method that mitigates such appearance discrepancy and enables end-to-end training for pose estimation. To increase the number of inliers, we propose a simple algorithm to remove invisible 3D points in the image, and find all 2D-3D correspondences without keypoint detection. To reduce memory usage and search complexity, we take a coarse-to-fine approach where we extract patch-level features from 2D images, then perform 2D patch classification on each 3D point, and obtain the exact corresponding 2D pixel coordinates through positional encoding. Finally, for the first time in this task, we employ a differentiable PnP for end-to-end training. In the experiments on newly curated large-scale indoor and outdoor benchmarks based on 2D-3D-S and KITTI, we show that our method achieves the state-of-the-art performance compared to existing visual localization and image-to-point cloud registration methods."}}
{"id": "_xfpfXvYfN", "cdate": 1640995200000, "mdate": 1681717309023, "content": {"title": "Indoor/Outdoor Transition Recognition Based on Door Detection", "abstract": "An autonomous navigating mobile robot uses various sensors for localization, such as GPS, laser scanner, IMU, odometer, and camera. These sensors use different weights and parameters according to the location, whether the robot is in indoors or outdoors. For both indoor and outdoor (IO) seamless navigation, the robot needs to know where the robot is, either indoors or outdoors, and when the IO transition has been made. This way the robot can switch the appropriate weights and parameters at the right time.This paper describes the method of identifying the moment when the indoor and outdoor transition is made. In order to recognize the time of IO transition, door detection with YOLOv5 is used. As the robot approaches the door, the detected bounding box of the door shows changes in size. This feature is used for IO transition recognition. The algorithm is tested with two scenarios and it shows that this method is simple and applicable to real world and the result is reliable to the detection failure."}}
{"id": "Js3Fj4AuxQ", "cdate": 1609459200000, "mdate": 1681652243138, "content": {"title": "Drop-Bottleneck: Learning Discrete Compressed Representation for Noise-Robust Exploration", "abstract": ""}}
{"id": "Dxzaa2caNyG", "cdate": 1609459200000, "mdate": 1681652243239, "content": {"title": "Drop-Bottleneck: Learning Discrete Compressed Representation for Noise-Robust Exploration", "abstract": ""}}
{"id": "1rxHOBjeDUW", "cdate": 1601308172208, "mdate": null, "content": {"title": "Drop-Bottleneck: Learning Discrete Compressed Representation for Noise-Robust Exploration", "abstract": "We propose a novel information bottleneck (IB) method named Drop-Bottleneck, which discretely drops features that are irrelevant to the target variable. Drop-Bottleneck not only enjoys a simple and tractable compression objective but also additionally provides a deterministic compressed representation of the input variable, which is useful for inference tasks that require consistent representation. Moreover, it can jointly learn a feature extractor and select features considering each feature dimension's relevance to the target task, which is unattainable by most neural network-based IB methods. We propose an exploration method based on Drop-Bottleneck for reinforcement learning tasks. In a multitude of noisy and reward sparse maze navigation tasks in VizDoom (Kempka et al., 2016) and DMLab (Beattie et al., 2016), our exploration method achieves state-of-the-art performance. As a new IB framework, we demonstrate that Drop-Bottleneck outperforms Variational Information Bottleneck (VIB) (Alemi et al., 2017) in multiple aspects including adversarial robustness and dimensionality reduction."}}
{"id": "TBkOzEcQYCB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Memorization Precedes Generation: Learning Unsupervised GANs with Memory Networks", "abstract": "We propose an approach to address two issues that commonly occur during training of unsupervised GANs. First, since GANs use only a continuous latent distribution to embed multiple classes or clusters of data, they often do not correctly handle the structural discontinuity between disparate classes in a latent space. Second, discriminators of GANs easily forget about past generated samples by generators, incurring instability during adversarial training. We argue that these two infamous problems of unsupervised GAN training can be largely alleviated by a learnable memory network to which both generators and discriminators can access. Generators can effectively learn representation of training samples to understand underlying cluster distributions of data, which ease the structure discontinuity problem. At the same time, discriminators can better memorize clusters of previously generated samples, which mitigate the forgetting problem. We propose a novel end-to-end GAN model named memoryGAN, which involves a memory network that is unsupervisedly trainable and integrable to many existing GAN models. With evaluations on multiple datasets such as Fashion-MNIST, CelebA, CIFAR10, and Chairs, we show that our model is probabilistically interpretable, and generates realistic image samples of high visual fidelity. The memoryGAN also achieves the state-of-the-art inception scores over unsupervised GAN models on the CIFAR10 dataset, without any optimization tricks and weaker divergences."}}
