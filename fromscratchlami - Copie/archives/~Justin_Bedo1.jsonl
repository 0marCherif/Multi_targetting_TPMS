{"id": "I2Vf0BdvQ0r", "cdate": 1600384453817, "mdate": null, "content": {"title": "Multivariate Spearman\u2019s\u03c1for Aggregating Ranks Using Copulas", "abstract": "We study the problem of rank aggregation:  given a set of ranked lists, we want to form aconsensus ranking.  Furthermore, we consider the case of extreme lists:  i.e., only the rank ofthe best or worst elements are known.  We impute missing ranks and generalise Spearman\u2019s\u03c1to extreme ranks.  Our main contribution is the derivation of a non-parametric estimatorfor rank aggregation based on multivariate extensions of Spearman\u2019s\u03c1, which measurescorrelation between a set of ranked lists.  Multivariate Spearman\u2019s\u03c1is defined using copulas,and we show that the geometric mean of normalised ranks maximises multivariate correlation.Motivated by this, we propose a weighted geometric mean approach for learning to rankwhich has a closed form least squares solution.  When only the best (top-k) or worst (bottom-k) elements of a ranked list are known, we impute the missing ranks by the average value,allowing us to apply Spearman\u2019s\u03c1.  We discuss an optimistic and pessimistic imputation ofmissing values, which respectively maximise and minimise correlation, and show its effecton aggregating university rankings.  Finally, we demonstrate good performance on the rankaggregation benchmarks MQ2007 and MQ2008."}}
{"id": "aM8UPc-hm4O", "cdate": 1600384340363, "mdate": null, "content": {"title": "The Poisson Margin Test for Normalization-FreeSignificance Analysis of NGS Data", "abstract": "\nThe Poisson Margin Test for Normalization-FreeSignificance Analysis of NGS DataADAM KOWALCZYK, JUSTIN BEDO, THOMAS CONWAY, and BRYAN BERESFORD-SMITHABSTRACTThe current methods for the determination of the statistical significance of peaks and re-gions in next generation sequencing (NGS) data require an explicit normalization step tocompensate for (global or local) imbalances in the sizes of sequenced and mapped libraries.There are no canonical methods for performing such compensations; hence, a number ofdifferent procedures serving this goal in different ways can be found in the literature.Unfortunately, the normalization has a significant impact on the final results. Differentmethods yield very different numbers of detected \u2018\u2018significant peaks\u2019\u2019 even in the simplestscenario of ChIP-Seq experiments that compare the enrichment in a single sample relativeto a matching control. This becomes an even more acute issue in the more general case of thecomparison of multiple samples, where a number of arbitrary design choices will be re-quired in the data analysis stage, each option resulting in possibly (significantly) differentoutcomes. In this article, we investigate a principled statistical procedure that eliminates theneed for a normalization step. We outline its basic properties, in particular the scaling upondepth of sequencing. For the sake of illustration and comparison, we report the results of re-analyzing a ChIP-Seq experiment for transcription factor binding site detection. In order toquantify the differences between outcomes, we use a novel method based on the accuracy ofin silicoprediction by support vector machine (SVM) models trained on part of the genomeand tested on the remainder. See Kowalczyk et al. (2009) for supplementary material."}}
{"id": "yDOqCwuvUb2", "cdate": 1600384148321, "mdate": null, "content": {"title": "BioShake: a Haskell EDSL forbioinformatics workflows", "abstract": "Typical bioinformatics analyses comprise of long running computational workflows.An important part of reproducible research is the management and execution ofthese workflows to allow robust execution and to minimise errors. BioShake is anembedded domain specific language in Haskell for specifying and executing computa-tional workflows for bioinformatics that significantly reduces the possibility of errorsoccurring. Unlike other workflow frameworks, BioShake raises many properties tothe type level allowing the correctness of a workflow to be statically checked duringcompilation, catching errors before any lengthy execution process. BioShake buildson the Shake build tool to provide robust dependency tracking, parallel execution,reporting, and resumption capabilities. Finally, BioShake abstracts execution so thatjobs can either be executed directly or submitted to a cluster. BioShake is available athttp://github.com/PapenfussLab/bioshake."}}
{"id": "Y3Pd7CliFw5", "cdate": 1600384029013, "mdate": null, "content": {"title": "Information theoretic alignment freevariant calling", "abstract": "While traditional methods for calling variants across whole genome sequence data relyon alignment to an appropriate reference sequence, alternative techniques are neededwhen a suitable reference does not exist. We present a novel alignment and assemblyfree variant calling method based on information theoretic principles designed to detectvariants have strong statistical evidence for their ability to segregate samples in a givendataset. Our method uses the context surrounding a particular nucleotide to definevariants. Given a set of reads, we model the probability of observing a given nucleotideconditioned on the surrounding prefix and suffixes of lengthkas a multinomialdistribution. We then estimate which of these contexts are stable intra-sample andvarying inter-sample using a statistic based on the Kullback\u2013Leibler divergence.The utility of the variant calling method was evaluated through analysis of a pair ofbacterial datasets and a mouse dataset. We found that our variants are highly informa-tive for supervised learning tasks with performance similar to standard reference basedcalls and another reference free method (DiscoSNP++). Comparisons against referencebased calls showed our method was able to capture very similar population structure onthe bacterial dataset. The algorithm\u2019s focus on discriminatory variants makes it suitablefor many common analysis tasks for organisms that are too diverse to be mapped backto a single reference sequence."}}
{"id": "xqUZ6bq2ifv", "cdate": 1514764800000, "mdate": null, "content": {"title": "PPFA: Privacy Preserving Fog-Enabled Aggregation in Smart Grid", "abstract": "For constrained end devices in Internet of Things, such as smart meters (SMs), data transmission is an energy-consuming operation. To address this problem, we propose an efficient and privacy-preserving aggregation system with the aid of Fog computing architecture, named PPFA, which enables the intermediate Fog nodes to periodically collect data from nearby SMs and accurately derive aggregate statistics as the fine-grained Fog level aggregation. The Cloud/utility supplier computes overall aggregate statistics by aggregating Fog level aggregation. To minimize the privacy leakage and mitigate the utility loss, we use more efficient and concentrated Gaussian mechanism to distribute noise generation among parties, thus offering provable differential privacy guarantees of the aggregate statistic on both Fog level and Cloud level. In addition, to ensure aggregator obliviousness and system robustness, we put forward a two-layer encryption scheme: the first layer applies OTP to encrypt individual noisy measurement to achieve aggregator obliviousness, while the second layer uses public-key cryptography for authentication purpose. Our scheme is simple, efficient, and practical, it requires only one round of data exchange among a SM, its connected Fog node and the Cloud if there are no node failures, otherwise, one extra round is needed between a meter, its connected Fog node, and the trusted third party."}}
{"id": "uZmERWfze4", "cdate": 1451606400000, "mdate": 1632897616735, "content": {"title": "Privately Matching k-mers", "abstract": ""}}
{"id": "JQMGQ_EHMER", "cdate": 1388534400000, "mdate": 1632897616728, "content": {"title": "Deep Belief Networks and Biomedical Text Categorisation", "abstract": "Antonio Jimeno Yepes, Andrew MacKinlay, Justin Bedo, Rahil Garvani, Qiang Chen. Proceedings of the Australasian Language Technology Association Workshop 2014. 2014."}}
{"id": "T_lsLyd4vO", "cdate": 1293840000000, "mdate": 1632897616897, "content": {"title": "Genome annotation test with validation on transcription start site and ChIP-Seq for Pol-II binding data", "abstract": "Many ChIP-Seq experiments are aimed at developing gold standards for determining the locations of various genomic features such as transcription start or transcription factor binding sites on the whole genome. Many such pioneering experiments lack rigorous testing methods and adequate \u2018gold standard\u2019 annotations to compare against as they themselves are the most reliable source of empirical data available. To overcome this problem, we propose a self-consistency test whereby a dataset is tested against itself. It relies on a supervised machine learning style protocol for in silico annotation of a genome and accuracy estimation to guarantee, at least, self-consistency."}}
{"id": "37R3jizOFq", "cdate": 1199145600000, "mdate": 1632897616733, "content": {"title": "Microarray Design Using the Hilbert-Schmidt Independence Criterion", "abstract": "This paper explores the design problem of selecting a small subset of clones from a large pool for creation of a microarray plate. A new kernel based unsupervised feature selection method using the Hilbert\u2013Schmidt independence criterion (hsic) is presented and evaluated on three microarray datasets: the Alon colon cancer dataset, the van\u00a0\u2019t\u00a0Veer breast cancer dataset, and a multiclass cancer of unknown primary dataset. The experiments show that subsets selected by the hsic resulted in equivalent or better performance than supervised feature selection, with the added benefit that the subsets are not target specific."}}
{"id": "S1-s8n-uZB", "cdate": 1167609600000, "mdate": null, "content": {"title": "Supervised feature selection via dependence estimation", "abstract": "We introduce a framework for filtering features that employs the Hilbert-Schmidt Independence Criterion (HSIC) as a measure of dependence between the features and the labels. The key idea is that good features should maximise such dependence. Feature selection for various supervised learning problems (including classification and regression) is unified under this framework, and the solutions can be approximated using a backward-elimination algorithm. We demonstrate the usefulness of our method on both artificial and real world datasets."}}
