{"id": "OaVXkPgSgxt", "cdate": 1640995200000, "mdate": 1664712207647, "content": {"title": "Light-weight Spatio-Temporal Graphs for Segmentation and Ejection Fraction Prediction in Cardiac Ultrasound", "abstract": "Accurate and consistent predictions of echocardiography parameters are important for cardiovascular diagnosis and treatment. In particular, segmentations of the left ventricle can be used to derive ventricular volume, ejection fraction (EF) and other relevant measurements. In this paper we propose a new automated method called EchoGraphs for predicting ejection fraction and segmenting the left ventricle by detecting anatomical keypoints. Models for direct coordinate regression based on Graph Convolutional Networks (GCNs) are used to detect the keypoints. GCNs can learn to represent the cardiac shape based on local appearance of each keypoint, as well as global spatial and temporal structures of all keypoints combined. We evaluate our EchoGraphs model on the EchoNet benchmark dataset. Compared to semantic segmentation, GCNs show accurate segmentation and improvements in robustness and inference run-time. EF is computed simultaneously to segmentations and our method also obtains state-of-the-art ejection fraction estimation. Source code is available online: https://github.com/guybenyosef/EchoGraphs ."}}
{"id": "Dtf_25grFe-", "cdate": 1621618987227, "mdate": null, "content": {"title": "Minimal Images in Deep Neural Networks: Fragile Object Recognition in Natural Images", "abstract": "The human ability to recognize objects is impaired when the object is not shown\nin full. \"Minimal images\" are the smallest regions of an image that remain recog\u0002nizable for humans. Ullman et al. (2016) show that a slight modification of the\nlocation and size of the visible region of the minimal image produces a sharp drop\nin human recognition accuracy. In this paper, we demonstrate that such drops in\naccuracy due to changes of the visible region are a common phenomenon between\nhumans and existing state-of-the-art deep neural networks (DNNs), and are much\nmore prominent in DNNs. We found many cases where DNNs classified one region\ncorrectly and the other incorrectly, though they only differed by one row or column\nof pixels, and were often bigger than the average human minimal image size. We\nshow that this phenomenon is independent from previous works that have reported\nlack of invariance to minor modifications in object location in DNNs. Our results\nthus reveal a new failure mode of DNNs that also affects humans to a much lesser\ndegree. They expose how fragile DNN recognition ability is for natural images\neven without adversarial patterns being introduced. Bringing the robustness of\nDNNs in natural images to the human level remains an open challenge for the\ncommunity.\n"}}
{"id": "1FIKwBvuftY", "cdate": 1618762269877, "mdate": null, "content": {"title": "Image interpretation above and below the object level", "abstract": "Computational models of vision have advanced in recent years at a rapid rate, rivalling in some areas human-level performance. Much of the progress to date has focused on analysing the visual scene at the object level\u2014the recognition and localization of objects in the scene. Human understanding of images reaches a richer and deeper image understanding both \u2018below' the object level, such as identifying and localizing object parts and sub-parts, as well as \u2018above\u2019 the object level, such as identifying object relations, and agents with their actions and interactions. In both cases, understanding depends on recovering meaningful structures in the image, and their components, properties and inter-relations, a process referred here as \u2018image interpretation'. In this paper, we describe recent directions, based on human and computer vision studies, towards human-like image interpretation, beyond the reach of current schemes, both below the object level, as well as some aspects of image interpretation at the level of meaningful configurations beyond the recognition of individual objects, and in particular, interactions between two people in close contact. In both cases the recognition process depends on the detailed interpretation of so-called \u2018minimal images\u2019, and at both levels recognition depends on combining \u2018bottom-up' processing, proceeding from low to higher levels of a processing hierarchy, together with \u2018top-down' processing, proceeding from high to lower levels stages of visual analysis."}}
{"id": "sgJ8gqvriDU", "cdate": 1618762095378, "mdate": null, "content": {"title": "Minimal videos: Trade-off between spatial and temporal information in human and machine vision", "abstract": "Objects and their parts can be visually recognized from purely spatial or purely temporal information but the mechanisms integrating space and time are poorly understood. Here we show that visual recognition of objects and actions can be achieved by efficiently combining spatial and motion cues in configurations where each source on its own is insufficient for recognition. This analysis is obtained by identifying minimal videos: these are short and tiny video clips in which objects, parts, and actions can be reliably recognized, but any reduction in either space or time makes them unrecognizable. Human recognition in minimal videos is invariably accompanied by full interpretation of the internal components of the video. State-of-the-art deep convolutional networks for dynamic recognition cannot replicate human behavior in these configurations. The gap between human and machine vision demonstrated here is due to critical mechanisms for full spatiotemporal interpretation that are lacking in current computational models."}}
{"id": "5mElm0TICvm", "cdate": 1618761650088, "mdate": null, "content": {"title": "Full Interpretation Of Minimal Images", "abstract": "The goal in this work is to model the process of \u2018full interpretation\u2019 of object images, which is the ability to identify and localize all semantic features and parts that are recognized by human observers. The task is approached by dividing the interpretation of the complete object to the interpretation of multiple reduced but interpretable local regions. In such reduced regions, interpretation is simpler, since the number of semantic components is small, and the variability of possible configurations is low.\n\nWe model the interpretation process by identifying primitive components and relations that play a useful role in local interpretation by humans. To identify useful components and relations used in the interpretation process, we consider the interpretation of \u2018minimal configurations\u2019: these are reduced local regions, which are minimal in the sense that further reduction renders them unrecognizable and uninterpretable. We show that such minimal interpretable images have useful properties, which we use to identify informative features and relations used for full interpretation. We describe our interpretation model, and show results of detailed interpretations of minimal configurations, produced automatically by the model. Finally, we discuss possible extensions and implications of full interpretation to difficult visual tasks, such as recognizing social interactions, which are beyond the scope of current models of visual recognition."}}
{"id": "zbbGCUQQAeF", "cdate": 1609459200000, "mdate": 1652016884892, "content": {"title": "PETS2021: Through-foliage detection and tracking challenge and evaluation", "abstract": "This paper presents the outcomes of the PETS2021 challenge held in conjunction with AVSS2021 and sponsored by the EU FOLDOUT project. The challenge comprises the publication of a novel video surveillance dataset on through-foliage detection, the defined challenges addressing person detection and tracking in fragmented occlusion scenarios, and quantitative and qualitative performance evaluation of challenge results submitted by six worldwide participants. The results show that while several detection and tracking methods achieve overall good results, through-foliage detection and tracking remains a challenging task for surveillance systems especially as it serves as the input to behaviour (threat) recognition."}}
{"id": "qHW7v3woxb2", "cdate": 1609459200000, "mdate": 1638993337567, "content": {"title": "A model for full local image interpretation", "abstract": "We describe a computational model of humans' ability to provide a detailed interpretation of components in a scene. Humans can identify in an image meaningful components almost everywhere, and identifying these components is an essential part of the visual process, and of understanding the surrounding scene and its potential meaning to the viewer. Detailed interpretation is beyond the scope of current models of visual recognition. Our model suggests that this is a fundamental limitation, related to the fact that existing models rely on feed-forward but limited top-down processing. In our model, a first recognition stage leads to the initial activation of class candidates, which is incomplete and with limited accuracy. This stage then triggers the application of class-specific interpretation and validation processes, which recover richer and more accurate interpretation of the visible scene. We discuss implications of the model for visual interpretation by humans and by computer vision models."}}
{"id": "kxWcVbmmVBg", "cdate": 1609459200000, "mdate": 1652016884858, "content": {"title": "Person Re-ID Testbed with Multi-Modal Sensors", "abstract": "Person Re-ID is a challenging problem and is gaining more attention due to demands in security, intelligent system and other applications. Most person Re-ID works are vision-based, such as image, video, or broadly speaking, face recognition-based techniques. Recently, several multi-modal person Re-ID datasets were released, including RGB+IR, RGB+text, RGB+WiFi, which shows the potential of the multi-modal sensor-based person Re-ID approach. However, there are several common issues in public datasets, such as short time duration, lack of appearance change, and limited activities, resulting in un-robust models. For example, vision-based Re-ID models are sensitive to appearance change. In this work, a person Re-ID testbed with multi-modal sensors is created, allowing the collection of sensing modalities including RGB, IR, depth, WiFi, radar, and audio. This novel dataset will cover normal daily office activities with large time span over multi-seasons. Initial analytic results are obtained for evaluating different person Re-ID models, based on small datasets collected in this testbed."}}
{"id": "HzbskfehlhS", "cdate": 1609459200000, "mdate": null, "content": {"title": "Parallel mesh reconstruction streams for pose estimation of interacting hands", "abstract": "We present a new multi-stream 3D mesh reconstruction network (MSMR-Net) for hand pose estimation from a single RGB image. Our model consists of an image encoder followed by a mesh-convolution decoder composed of connected graph convolution layers. In contrast to previous models that form a single mesh decoding path, our decoder network incorporates multiple cross-resolution trajectories that are executed in parallel. Thus, global and local information are shared to form rich decoding representations at minor additional parameter cost compared to the single trajectory network. We demonstrate the effectiveness of our method in hand-hand and hand-object interaction scenarios at various levels of interaction. To evaluate the former scenario, we propose a method to generate RGB images of closely interacting hands. Moreoever, we suggest a metric to quantify the degree of interaction and show that close hand interactions are particularly challenging. Experimental results show that the MSMR-Net outperforms existing algorithms on the hand-object FreiHAND dataset as well as on our own hand-hand dataset."}}
{"id": "1C7jiP_LF7e", "cdate": 1609459200000, "mdate": null, "content": {"title": "What can human minimal videos tell us about dynamic recognition models?", "abstract": "In human vision objects and their parts can be visually recognized from purely spatial or purely temporal information but the mechanisms integrating space and time are poorly understood. Here we show that human visual recognition of objects and actions can be achieved by efficiently combining spatial and motion cues in configurations where each source on its own is insufficient for recognition. This analysis is obtained by identifying minimal videos: these are short and tiny video clips in which objects, parts, and actions can be reliably recognized, but any reduction in either space or time makes them unrecognizable. State-of-the-art deep networks for dynamic visual recognition cannot replicate human behavior in these configurations. This gap between humans and machines points to critical mechanisms in human dynamic vision that are lacking in current models."}}
