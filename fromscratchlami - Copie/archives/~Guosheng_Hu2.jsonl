{"id": "i4N1NJug-z", "cdate": 1668097356985, "mdate": 1668097356985, "content": {"title": "Reducing Distributional Uncertainty by Mutual Information Maximisation and Transferable Feature Learning", "abstract": "Distributional uncertainty exists broadly in many real-worldapplications, one of which in the form of domain discrepancy. Yet in theexisting literature, the mathematical definition of it is missing. In thispaper, we propose to formulate the distributional uncertainty both be-tween the source(s) and target domain(s) and within each domain usingmutual information. Further, to reduce distributional uncertainty (e.g.domain discrepancy), we (1) maximise the mutual information betweensource and target domains and (2) propose a transferable feature learningscheme, balancing two complementary and discriminative feature learn-ing processes (general texture learning and self-supervised transferableshape learning) according to the uncertainty. We conduct extensive ex-periments  on  both  domain  adaption  and  domain  generalisation  usingchallenging common benchmarks: Office-Home and DomainNet. Resultsshow the great effectiveness of the proposed method and its superiorityover the state-of-the-art methods."}}
{"id": "IrjF5o79h-I", "cdate": 1668097260875, "mdate": 1668097260875, "content": {"title": "Training noise-robust deep neural networks via meta-learning", "abstract": "Label noise may significantly degrade the performance of Deep Neural Networks (DNNs). To train noise-robust DNNs, Loss correction (LC) approaches have been introduced. LC approaches assume the noisy labels are corrupted from clean (ground-truth) labels by an unknown noise transition matrix T. The backbone DNNs and T can be trained separately, where T is approximated with prior knowledge. For example, T is constructed by stacking the maximum or mean predic-tions of the samples from each class. In this work, we pro-pose a new loss correction approach, named as Meta Loss Correction (MLC), to directly learn T from data via the meta-learning framework. The MLC is model-agnostic and learns T from data rather than heuristically approximates it using prior knowledge. Extensive evaluations are conducted on computer vision (MNIST, CIFAR-10, CIFAR-100, Cloth-ing1M) and natural language processing (Twitter) datasets. The experimental results show that MLC achieves very com-petitive performance against state-of-the-art approaches."}}
{"id": "v1Vrqp0iAyf", "cdate": 1640995200000, "mdate": 1668592634645, "content": {"title": "Efficient One-Stage Video Object Detection by Exploiting Temporal Consistency", "abstract": "Recently, one-stage detectors have achieved competitive accuracy and faster speed compared with traditional two-stage detectors on image data. However, in the field of video object detection (VOD), most existing VOD methods are still based on two-stage detectors. Moreover, directly adapting existing VOD methods to one-stage detectors introduces unaffordable computational costs. In this paper, we first analyse the computational bottlenecks of using one-stage detectors for VOD. Based on the analysis, we present a simple yet efficient framework to address the computational bottlenecks and achieve efficient one-stage VOD by exploiting the temporal consistency in video frames. Specifically, our method consists of a location prior network to filter out background regions and a size prior network to skip unnecessary computations on low-level feature maps for specific frames. We test our method on various modern one-stage detectors and conduct extensive experiments on the ImageNet VID dataset. Excellent experimental results demonstrate the superior effectiveness, efficiency, and compatibility of our method. The code is available at https://github.com/guanxiongsun/EOVOD ."}}
{"id": "tYdcMqI_vJ", "cdate": 1640995200000, "mdate": 1668592634829, "content": {"title": "TDViT: Temporal Dilated Video Transformer for Dense Video Tasks", "abstract": "Deep video models, for example, 3D CNNs or video transformers, have achieved promising performance on sparse video tasks, i.e., predicting one result per video. However, challenges arise when adapting existing deep video models to dense video tasks, i.e., predicting one result per frame. Specifically, these models are expensive for deployment, less effective when handling redundant frames and difficult to capture long-range temporal correlations. To overcome these issues, we propose a Temporal Dilated Video Transformer (TDViT) that consists of carefully-designed temporal dilated transformer blocks (TDTB). TDTB can efficiently extract spatiotemporal representations and effectively alleviate the negative effect of temporal redundancy. Furthermore, by using hierarchical TDTBs, our approach obtains an exponentially expanded temporal receptive field and therefore can model long-range dynamics. Extensive experiments are conducted on two different dense video benchmarks, i.e., ImageNet VID for video object detection and YouTube VIS for video instance segmentation. Excellent experimental results demonstrate the superior efficiency, effectiveness, and compatibility of our method. The code is available at https://github.com/guanxiongsun/TDViT ."}}
{"id": "pAEMRcY_Pn", "cdate": 1640995200000, "mdate": 1668592634627, "content": {"title": "Multi-Definition Video Deepfake Detection via Semantics Reduction and Cross-Domain Training", "abstract": "The recent development of Deepfake videos directly threatens our information security and personal privacy. Although lots of previous works have made much progress on the Deepfake detection, we empirically find that the existing approaches do not perform well on the low definition (LD) and crossdefinition (high and low) videos. To address this problem, in this paper, we follow two motivations: (1) high-level semantics reduction and (2) cross-domain training. For (1), we propose the Facial Structure Destruction and Adversarial Jigsaw Loss to reduce our model to learn high-level semantics and focus on learning low-level discriminative information; For (2), we propose a domain generalization method based on adversarial learning. We conduct extensive experiments on the FaceForensics++ dataset. Results show the great effectiveness of our method and we also achieve very competitive performance against state-of-the-art methods."}}
{"id": "Uj63GNOXt7", "cdate": 1640995200000, "mdate": 1668592634616, "content": {"title": "RCANet: Row-Column Attention Network for Semantic Segmentation", "abstract": "Establishing high-order interactions among pixels and object parts is one of the most fundamental problems in semantic segmentation. The recent proposals are based on non-local methods which utilize the self-attention mechanism to capture the long-range correlations. However, non-local methods could be very expensive, both theoretically and experimentally. Moreover, non-local methods are typically designed to address spatial correlations rather than feature correlations across channels. In this work, we propose a Row-Column Attention Network (RCANet) to encode globally contextual information. It consists of a row-wise intra-channel attention module and a column-wise intra-channel attention module, followed by a cross-channel interaction module. We conduct experiments on two datasets: Cityscapes and ADE20K. The results show that our method is comparable to the state-of-the-art methods for semantic segmentation."}}
{"id": "8pRrjJgV8A", "cdate": 1640995200000, "mdate": 1668592634821, "content": {"title": "Conv-Adapter: Exploring Parameter Efficient Transfer Learning for ConvNets", "abstract": "While parameter efficient tuning (PET) methods have shown great potential with transformer architecture on Natural Language Processing (NLP) tasks, their effectiveness is still under-studied with large-scale ConvNets on Computer Vision (CV) tasks. This paper proposes Conv-Adapter, a PET module designed for ConvNets. Conv-Adapter is light-weight, domain-transferable, and architecture-agnostic with generalized performance on different tasks. When transferring on downstream tasks, Conv-Adapter learns tasks-specific feature modulation to the intermediate representations of backbone while keeping the pre-trained parameters frozen. By introducing only a tiny amount of learnable parameters, e.g., only 3.5% full fine-tuning parameters of ResNet50, Conv-Adapter outperforms previous PET baseline methods and achieves comparable or surpasses the performance of full fine-tuning on 23 classification tasks of various domains. It also presents superior performance on few-shot classifications, with an average margin of 3.39%. Beyond classification, Conv-Adapter can generalize to detection and segmentation tasks with more than 50% reduction of parameters but comparable performance to the traditional full fine-tuning."}}
{"id": "2MeObl9n_0", "cdate": 1640995200000, "mdate": 1668592634834, "content": {"title": "MetaMixUp: Learning Adaptive Interpolation Policy of MixUp With Metalearning", "abstract": "MixUp is an effective data augmentation method to regularize deep neural networks via random linear interpolations between pairs of samples and their labels. It plays an important role in model regularization, semisupervised learning (SSL), and domain adaption. However, despite its empirical success, its deficiency of randomly mixing samples has poorly been studied. Since deep networks are capable of memorizing the entire data set, the corrupted samples generated by vanilla MixUp with a badly chosen interpolation policy will degrade the performance of networks. To overcome overfitting to corrupted samples, inspired by metalearning (learning to learn), we propose a novel technique of learning to a mixup in this work, namely, MetaMixUp. Unlike the vanilla MixUp that samples interpolation policy from a predefined distribution, this article introduces a metalearning-based online optimization approach to dynamically learn the interpolation policy in a data-adaptive way (learning to learn better). The validation set performance via metalearning captures the noisy degree, which provides optimal directions for interpolation policy learning. Furthermore, we adapt our method for pseudolabel-based SSL along with a refined pseudolabeling strategy. In our experiments, our method achieves better performance than vanilla MixUp and its variants under SL configuration. In particular, extensive experiments show that our MetaMixUp adapted SSL greatly outperforms MixUp and many state-of-the-art methods on CIFAR-10 and SVHN benchmarks under the SSL configuration."}}
{"id": "1RL1EKO20Xd", "cdate": 1640995200000, "mdate": 1668592634633, "content": {"title": "Self-attention neural architecture search for semantic image segmentation", "abstract": ""}}
{"id": "1FLX9Mu2rx", "cdate": 1640995200000, "mdate": 1668592634656, "content": {"title": "Boosting Active Learning via Improving Test Performance", "abstract": "Central to active learning (AL) is what data should be selected for annotation. Existing works attempt to select highly uncertain or informative data for annotation. Nevertheless, it remains unclear how selected data impacts the test performance of the task model used in AL. In this work, we explore such an impact by theoretically proving that selecting unlabeled data of higher gradient norm leads to a lower upper-bound of test loss, resulting in a better test performance. However, due to the lack of label information, directly computing gradient norm for unlabeled data is infeasible. To address this challenge, we propose two schemes, namely expected-gradnorm and entropy-gradnorm. The former computes the gradient norm by constructing an expected empirical loss while the latter constructs an unsupervised loss with entropy. Furthermore, we integrate the two schemes in a universal AL framework. We evaluate our method on classical image classification and semantic segmentation tasks. To demonstrate its competency in domain applications and its robustness to noise, we also validate our method on a cellular imaging analysis task, namely cryo-Electron Tomography subtomogram classification. Results demonstrate that our method achieves superior performance against the state of the art. We refer readers to https://arxiv.org/pdf/2112.05683.pdf for the full version of this paper which includes the appendix and source code link."}}
