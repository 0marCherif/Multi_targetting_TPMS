{"id": "nIWJvC0zEn", "cdate": 1672531200000, "mdate": 1693616658219, "content": {"title": "HK-LegiCoST: Leveraging Non-Verbatim Transcripts for Speech Translation", "abstract": "We introduce HK-LegiCoST, a new three-way parallel corpus of Cantonese-English translations, containing 600+ hours of Cantonese audio, its standard traditional Chinese transcript, and English translation, segmented and aligned at the sentence level. We describe the notable challenges in corpus preparation: segmentation, alignment of long audio recordings, and sentence-level alignment with non-verbatim transcripts. Such transcripts make the corpus suitable for speech translation research when there are significant differences between the spoken and written forms of the source language. Due to its large size, we are able to demonstrate competitive speech translation baselines on HK-LegiCoST and extend them to promising cross-corpus results on the FLEURS Cantonese subset. These results deliver insights into speech recognition and translation research in languages for which non-verbatim or ``noisy'' transcription is common due to various factors, including vernacular and dialectal speech."}}
{"id": "YCl5JVzj5ss", "cdate": 1672531200000, "mdate": 1680550772882, "content": {"title": "PHONEix: Acoustic Feature Processing Strategy for Enhanced Singing Pronunciation with Phoneme Distribution Predictor", "abstract": ""}}
{"id": "McZ3uSawkQ", "cdate": 1672531200000, "mdate": 1693616658212, "content": {"title": "Bypass Temporal Classification: Weakly Supervised Automatic Speech Recognition with Imperfect Transcripts", "abstract": "This paper presents a novel algorithm for building an automatic speech recognition (ASR) model with imperfect training data. Imperfectly transcribed speech is a prevalent issue in human-annotated speech corpora, which degrades the performance of ASR models. To address this problem, we propose Bypass Temporal Classification (BTC) as an expansion of the Connectionist Temporal Classification (CTC) criterion. BTC explicitly encodes the uncertainties associated with transcripts during training. This is accomplished by enhancing the flexibility of the training graph, which is implemented as a weighted finite-state transducer (WFST) composition. The proposed algorithm improves the robustness and accuracy of ASR systems, particularly when working with imprecisely transcribed speech corpora. Our implementation will be open-sourced."}}
{"id": "5HmyEnURSs", "cdate": 1672531200000, "mdate": 1696559631480, "content": {"title": "Alternative Pseudo-Labeling for Semi-Supervised Automatic Speech Recognition", "abstract": "When labeled data is insufficient, pseudo-labeling based semi-supervised learning can significantly improve the performance of automatic speech recognition. However, pseudo-labels are often noisy, containing numerous incorrect tokens. Taking noisy labels as ground-truth results in suboptimal performance. Previous works attempted to mitigate this issue by either filtering out the nosiest pseudo-labels or improving the overall quality of pseudo-labels. While these methods are effective to some extent, it is unrealistic to entirely eliminate incorrect tokens in pseudo-labels. In this work, we propose a novel framework named alternative pseudo-labeling to tackle the issue of noisy pseudo-labels from the perspective of the training objective. The framework comprises several components. Firstly, a generalized CTC loss function is introduced to handle noisy pseudo-labels by accepting alternative tokens in the positions of incorrect tokens. Applying this loss function in pseudo-labeling requires detecting incorrect tokens in pseudo-labels. In this work, we adopt a confidence-based error detection method that identifies incorrect tokens by comparing their confidence scores with a given threshold, thus necessitating the confidence score to be discriminative. Hence, the second proposed technique is the contrastive CTC loss function that widens the confidence gap between the correctly and incorrectly predicted tokens, thereby improving the error detection ability. Additionally, obtaining satisfactory performance with confidence-based error detection typically requires extensive threshold tuning. To save the pain of manual tuning, we propose an automatic thresholding method that uses labeled data as a proxy for determining the threshold. Experiments demonstrate that alternative pseudo-labeling outperforms existing pseudo-labeling approaches on various datasets."}}
{"id": "E_EQzfSETi", "cdate": 1640995200000, "mdate": 1680550772881, "content": {"title": "EURO: ESPnet Unsupervised ASR Open-source Toolkit", "abstract": ""}}
{"id": "D5I0twLQ9x", "cdate": 1640995200000, "mdate": 1680550772882, "content": {"title": "Bridging Speech and Textual Pre-trained Models with Unsupervised ASR", "abstract": ""}}
{"id": "gwlL0shHK7W", "cdate": 1609459200000, "mdate": 1661382802038, "content": {"title": "Decoupling recognition and transcription in Mandarin ASR", "abstract": "Much of the recent literature on automatic speech recognition (ASR) is taking an end-to-end approach. Unlike English where the writing system is closely related to sound, Chinese characters (Hanzi) represent meaning, not sound. We propose factoring audio -> Hanzi into two sub-tasks: (1) audio -> Pinyin and (2) Pinyin -> Hanzi, where Pinyin is a system of phonetic transcription of standard Chinese. Factoring the audio -> Hanzi task in this way achieves 3.9% CER (character error rate) on the Aishell-1 corpus, the best result reported on this dataset so far."}}
{"id": "FK5Tg5f4HKy", "cdate": 1609459200000, "mdate": 1661382802057, "content": {"title": "Decoupling Recognition and Transcription in Mandarin ASR", "abstract": "Much of the recent literature on automatic speech recognition (ASR) is taking an end-to-end approach. Unlike English where the writing system is closely related to sound, Chinese characters (Hanzi) represent meaning, not sound. We propose factoring audio &#x2192; Hanzi into two sub-tasks: (1) audio &#x2192; Pinyin and (2) Pinyin &#x2192; Hanzi, where Pinyin is a system of phonetic transcription of standard Chinese. Factoring the audio &#x2192; Hanzi task in this way achieves 3.9&#x0025; CER (character error rate) on the Aishell-1 corpus, the best result reported on this dataset so far."}}
{"id": "6We1GybaQHx", "cdate": 1609459200000, "mdate": 1648672964456, "content": {"title": "Training Hybrid Models on Noisy Transliterated Transcripts for Code-Switched Speech Recognition", "abstract": "In this paper, we describe the JHU-GoVivace submission for subtask 2 (code-switching task) of the Multilingual and Code-switching ASR challenges for low resource Indian languages. We built a hybrid HMM-DNN system with several improvements over the provided baseline in terms of lexical, language, and acoustic modeling. For lexical modeling, we investigate using unified pronunciations and phonesets derived from the baseline lexicon and publicly available Wikipron lexicons in Bengali and Hindi to expand the pronunciation lexicons. We explore several neural network architectures, along with supervised pretraining and multilingual training for acoustic modeling. We also describe how we used large externally crawled web text for language modeling. Since the challenge data contain artefacts such as misalignments, various data cleanup methods are explored, including acoustic-driven pronunciation learning to help discover Indian-accented pronunciations for English words as well as transcribed punctuation. As a result of these efforts, our best systems achieve transliterated WERs of 19.5% and 23.2% on the non-duplicated development sets for Hindi-English and Bengali-English, respectively."}}
{"id": "hfP_1qVtAz", "cdate": 1514764800000, "mdate": 1637198869058, "content": {"title": "A Pruned Rnnlm Lattice-Rescoring Algorithm for Automatic Speech Recognition", "abstract": "Lattice-rescoring is a common approach to take advantage of recurrent neural language models in ASR, where a word-lattice is generated from 1st-pass decoding and the lattice is then rescored with a neural model, and an <i xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">n</i> -gram approximation method is usually adopted to limit the search space. In this work, we describe a pruned lattice-rescoring algorithm for ASR, improving the n-gram approximation method. The pruned algorithm further limits the search space and uses heuristic search to pick better histories when expanding the lattice. Experiments show that the proposed algorithm achieves better ASR accuracies while running much faster than the standard algorithm. In particular, it brings a 4x speedup for lattice-rescoring with 4-gram approximation while giving better recognition accuracies than the standard algorithm."}}
