{"id": "U-B3LYRoFB0Q", "cdate": 1632862123484, "mdate": 1632862123484, "content": {"title": "Accelerated Inexact First-Order Methods for Solving Nonconvex Composite Optimization Problems", "abstract": "This thesis focuses on developing and analyzing accelerated and inexact first-order methods for solving or finding stationary points of various nonconvex composite optimization (NCO) problems. Our main tools mainly come from variational and convex analysis, and our key results are in the form of iteration complexity bounds and how these bounds compare to other ones in the literature."}}
{"id": "JgvT5wjEbc4d", "cdate": 1632862060602, "mdate": null, "content": {"title": "FISTA and Extensions - Review and New Insights", "abstract": "The purpose of this technical report is to review the main properties of an accelerated composite gradient (ACG) method commonly referred to as the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA). In addition, we state a version of FISTA for solving both convex and strongly convex composite minimization problems and derive its iteration complexities to generate iterates satisfying various stopping criteria, including one which arises in the course of solving other composite optimization problems via inexact proximal point schemes. This report also discusses different reformulations of the convex version of FISTA and how they relate to other formulations in the literature."}}
{"id": "8fv-wVtW4Js", "cdate": 1632861974785, "mdate": 1632861974785, "content": {"title": "Iteration-complexity of a proximal augmented Lagrangian method for solving nonconvex composite optimization problems with nonlinear convex constraints", "abstract": "This paper proposes and analyzes a proximal augmented Lagrangian (NL-IAPIAL) method for solving smooth nonconvex composite optimization problems with nonlinear K-convex constraints, i.e., the constraints are convex with respect to the order given by a closed convex cone K. Each NL-IAPIAL iteration consists of inexactly solving a proximal augmented Lagrangian subproblem by an accelerated composite gradient (ACG) method followed by a Lagrange multiplier update. Under some mild assumptions, it is shown that NL-IAPIAL generates an approximate stationary solution of the constrained problem in O(log(1/\u03c1)/\u03c1^3) inner iterations, where \u03c1 > 0 is a given tolerance. Numerical experiments are also given to illustrate the computational efficiency of the proposed method."}}
{"id": "i9qI756K9Dg", "cdate": 1632861858416, "mdate": 1632861858416, "content": {"title": "Accelerated inexact composite gradient methods for nonconvex spectral optimization problems", "abstract": "This paper presents two inexact composite gradient methods, one inner accelerated and another doubly accelerated, for solving a class of nonconvex spectral composite optimization problems. More specifically, the objective function for these problems is of the form f1 + f2 + h, where f1 and f2 are differentiable nonconvex matrix functions with Lipschitz continuous gradients, h is a proper closed convex matrix function, and both f2 and h can be expressed as functions that operate on the singular values of their inputs. The methods essentially use an accelerated composite gradient method to solve a sequence of proximal subproblems involving the linear approximation of f1 and the singular value functions underlying f2 and h. Unlike other composite gradient-based methods, the proposed methods take advantage of both the composite and spectral structure underlying the objective function in order to efficiently generate their solutions. Numerical experiments are presented to demonstrate the practicality of these methods on a set of real-world and randomly generated spectral optimization problems."}}
{"id": "7UusUtb_l4Tr", "cdate": 1632861739138, "mdate": 1632861739138, "content": {"title": "Iteration-complexity of an inner accelerated inexact proximal augmented Lagrangian method based on the classical Lagrangian function and a full Lagrange multiplier update", "abstract": "This paper establishes the iteration-complexity of an inner accelerated inexact proximal augmented Lagrangian (IAPIAL) method for solving linearly constrained smooth nonconvex composite optimization problems which is based on the classical Lagrangian function and, most importantly, performs a full Lagrangian multiplier update, i.e., no shrinking factor is incorporated on it. More specifically, each IAPIAL iteration consists of inexactly solving a proximal augmented Lagrangian subproblem by an accelerated composite gradient (ACG) method followed by a full Lagrange multiplier update. Under the assumption that the domain of the composite function is bounded and the problem has a Slater point, it is shown that IAPIAL generates an approximate stationary solution in at most O(log(1/p)/p^3) ACG iterations, where p > 0 is the tolerance for both stationarity and feasibility. Finally, the above bound is derived without assuming that the initial point is feasible."}}
{"id": "_YMaqGDU-2l", "cdate": 1632861655980, "mdate": 1632861655980, "content": {"title": "Rankmax: An adaptive projection alternative to the softmax function", "abstract": "Several machine learning models involve mapping a score vector to a probability vector. Usually, this is done by projecting the score vector onto a probability simplex, and such projections are often characterized as Lipschitz continuous approximations of the argmax function, whose Lipschitz constant is controlled by a parameter that is similar to a softmax temperature. The aforementioned parameter has been observed to affect the quality of these models and is typically either treated as a constant or decayed over time. In this work, we propose a method that adapts this parameter to individual training examples. The resulting method exhibits desirable properties, such as sparsity of its support and numerically efficient implementation, and we find that it significantly outperforms competing non-adaptive projection methods. In our analysis, we also derive the general solution of (Bregman) projections onto the (n, k)-simplex, a result which may be of independent interest."}}
{"id": "GnCs5d-2dH", "cdate": 1632861516927, "mdate": 1632861516927, "content": {"title": "An efficient adaptive accelerated inexact proximal point method for solving linearly constrained nonconvex composite problems", "abstract": "This paper proposes an efficient adaptive variant of a quadratic penalty accelerated inexact proximal point (QP-AIPP) method proposed earlier by the authors. Both the QP-AIPP method and its variant solve linearly set constrained nonconvex composite optimization problems using a quadratic penalty approach where the generated penalized subproblems are solved by a variant of the underlying AIPP method. The variant, in turn, solves a given penalized subproblem by generating a sequence of proximal subproblems which are then solved by an accelerated composite gradient algorithm. The main difference between AIPP and its variant is that the proximal subproblems in the former are always convex while the ones in the latter are not necessarily convex due to the fact that their prox parameters are chosen as aggressively as possible so as to improve efficiency. The possibly nonconvex proximal subproblems generated by the AIPP variant are also tentatively solved by a novel adaptive accelerated composite gradient algorithm based on the validity of some key convergence inequalities. As a result, the variant generates a sequence of proximal subproblems where the stepsizes are adaptively changed according to the responses obtained from the calls to the accelerated composite gradient algorithm. Finally, numerical results are given to demonstrate the efficiency of the proposed AIPP and QP-AIPP variants."}}
{"id": "WBhPPUILp4H", "cdate": 1632861462072, "mdate": 1632861462072, "content": {"title": "An accelerated inexact proximal point method for solving nonconvex-concave min-max problems", "abstract": "This paper presents smoothing schemes for obtaining approximate stationary points of unconstrained or linearly-constrained composite nonconvex-concave min-max (and hence nonsmooth) problems by applying well-known algorithms to composite smooth approximations of the original problems. More specifically, in the unconstrained (resp. constrained) case, approximate stationary points of the original problem are obtained by applying, to its composite smooth approximation, an accelerated inexact proximal point (resp. quadratic penalty) method presented in a previous paper by the authors. Iteration complexity bounds for both smoothing schemes are also established. Finally, numerical results are given to demonstrate the efficiency of the unconstrained smoothing scheme."}}
{"id": "xsGxOgtQu2O", "cdate": 1632861391538, "mdate": 1632861391538, "content": {"title": "Complexity of a quadratic penalty accelerated inexact proximal point method for solving linearly constrained nonconvex composite programs", "abstract": "This paper analyzes the iteration complexity of a quadratic penalty accelerated inexact proximal point method for solving linearly constrained nonconvex composite programs. More specifically, the objective function is of the form f + h, where f is a differentiable function whose gradient is Lipschitz continuous and h is a closed convex function with possibly unbounded domain. The method, basically, consists of applying an accelerated inexact proximal point method for approximately solving a sequence of quadratic penalized subproblems associated with the linearly constrained problem. Each subproblem of the proximal point method is in turn approximately solved by an accelerated composite gradient (ACG) method. It is shown that the proposed scheme generates a \u03c1-approximate stationary point in at most O(\u03c1\u22123) ACG iterations. Finally, numerical results showing the efficiency of the proposed method are also given."}}
{"id": "rkluJ2R9KQ", "cdate": 1538087903798, "mdate": null, "content": {"title": "A new dog learns old tricks:  RL finds classic optimization algorithms", "abstract": "This paper introduces a novel framework for learning algorithms to solve online combinatorial optimization problems. Towards this goal, we introduce a number of key ideas from traditional algorithms and complexity theory. First, we draw a new connection between primal-dual methods and reinforcement learning. Next, we introduce the concept of adversarial distributions (universal and high-entropy training sets), which are distributions that encourage the learner to find algorithms that work well in the worst case. We test our new ideas on a number of optimization problem such as the AdWords problem, the online knapsack problem, and the secretary problem. Our results indicate that the models have learned behaviours that are consistent with the traditional optimal algorithms for these problems."}}
