{"id": "UoIqUZfbbq", "cdate": 1696328529220, "mdate": 1696328529220, "content": {"title": "Weisfeiler and Leman go Hyperbolic: Learning Distance Preserving Node Representations", "abstract": "In recent years, graph neural networks (GNNs)\nhave emerged as a promising tool for solving\nmachine learning problems on graphs. Most\nGNNs are members of the family of message\npassing neural networks (MPNNs). There is a\nclose connection between these models and the\nWeisfeiler-Leman (WL) test of isomorphism, an\nalgorithm that can successfully test isomorphism\nfor a broad class of graphs. Recently, much research has focused on measuring the expressive\npower of GNNs. For instance, it has been shown\nthat standard MPNNs are at most as powerful as\nWL in terms of distinguishing non-isomorphic\ngraphs. However, these studies have largely ignored the distances between the representations\nof nodes/graphs which are of paramount importance for learning tasks. In this paper, we define\na distance function between nodes that is based\non the hierarchy produced by the WL algorithm\nand propose a model that learns representations\nwhich preserve those distances between nodes.\nSince the emerging hierarchy corresponds to a\ntree, to learn these representations, we capitalize\non recent advances in the field of hyperbolic neural networks. We empirically evaluate the proposed model on standard node and graph classification datasets where it achieves competitive\nperformance with state-of-the-art models."}}
{"id": "_BjtIlib8N9", "cdate": 1664248834668, "mdate": null, "content": {"title": "Structure-Aware Antibiotic Resistance Classification using Graph Neural Networks", "abstract": "Antibiotics are traditionally used to treat bacterial infections. However, bacteria can develop immunity to drugs, making them ineffective and thus posing a serious threat to global health. Identifying and classifying the genes responsible for this resistance is critical for the prevention, diagnosis, and treatment of infections as well as the understanding of its mechanisms. Previous methods developed for this purpose have mostly been sequence-based, relying on comparisons to existing databases or machine learning models trained on sequence features. However, genes with comparable functions may not always have similar sequences. As a result, in this paper, we develop a deep learning model that uses the protein structure as a complement to the sequence to classify novel ARGs (antibiotic resistant genes), which we expect to provide more useful information than the sequence alone. The proposed approach consists of two steps. First, we capitalize on the celebrated AlphaFold model to predict the 3D structure of a protein from its amino acid sequence. Then, we process the sequence using a transformers-based language model while we also apply a graph neural network to the graph extracted from the structure. We evaluate the proposed architecture on a standard benchmark dataset where it outperforms state-of-the-art methods. "}}
{"id": "hKE2p9tpNk", "cdate": 1640995200000, "mdate": 1651485046807, "content": {"title": "Graph Ordering Attention Networks", "abstract": "Graph Neural Networks (GNNs) have been successfully used in many problems involving graph-structured data, achieving state-of-the-art performance. GNNs typically employ a message-passing scheme, in which every node aggregates information from its neighbors using a permutation-invariant aggregation function. Standard well-examined choices such as the mean or sum aggregation functions have limited capabilities, as they are not able to capture interactions among neighbors. In this work, we formalize these interactions using an information-theoretic framework that notably includes synergistic information. Driven by this definition, we introduce the Graph Ordering Attention (GOAT) layer, a novel GNN component that captures interactions between nodes in a neighborhood. This is achieved by learning local node orderings via an attention mechanism and processing the ordered representations using a recurrent neural network aggregator. This design allows us to make use of a permutation-sensitive aggregator while maintaining the permutation-equivariance of the proposed GOAT layer. The GOAT model demonstrates its increased performance in modeling graph metrics that capture complex information, such as the betweenness centrality and the effective size of a node. In practical use-cases, its superior modeling capability is confirmed through its success in several real-world node classification benchmarks."}}
{"id": "wnu92uikf92", "cdate": 1609459200000, "mdate": 1648675259097, "content": {"title": "Operation Embeddings for Neural Architecture Search", "abstract": "Neural Architecture Search (NAS) has recently gained increased attention, as a class of approaches that automatically searches in an input space of network architectures. A crucial part of the NAS pipeline is the encoding of the architecture that consists of the applied computational blocks, namely the operations and the links between them. Most of the existing approaches either fail to capture the structural properties of the architectures or use hand-engineered vector to encode the operator information. In this paper, we propose the replacement of fixed operator encoding with learnable representations in the optimization process. This approach, which effectively captures the relations of different operations, leads to smoother and more accurate representations of the architectures and consequently to improved performance of the end task. Our extensive evaluation in ENAS benchmark demonstrates the effectiveness of the proposed operation embeddings to the generation of highly accurate models, achieving state-of-the-art performance. Finally, our method produces top-performing architectures that share similar operation and graph patterns, highlighting a strong correlation between the structural properties of the architecture and its performance."}}
{"id": "H9IYlen1wqX", "cdate": 1609459200000, "mdate": 1648675259117, "content": {"title": "Graph-based Neural Architecture Search with Operation Embeddings", "abstract": "Neural Architecture Search (NAS) has recently gained increased attention, as a class of approaches that automatically searches in an input space of network architectures. A crucial part of the NAS pipeline is the encoding of the architecture that consists of the applied computational blocks, namely the operations and the links between them. Most of the existing approaches either fail to capture the structural properties of the architectures or use hand-engineered vector to encode the operator information. In this paper, we propose the replacement of fixed operator encoding with learnable representations in the optimization process. This approach, which effectively captures the relations of different operations, leads to smoother and more accurate representations of the architectures and consequently to improved performance of the end task. Our extensive evaluation in ENAS benchmark demonstrates the effectiveness of the proposed operation embeddings to the generation of highly accurate models, achieving state-of-the-art performance. Finally, our method produces top-performing architectures that share similar operation and graph pat-terns, highlighting a strong correlation between the structural properties of the architecture and its performance."}}
