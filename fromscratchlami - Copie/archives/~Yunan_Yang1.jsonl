{"id": "z_p5PMsgtse", "cdate": 1672531200000, "mdate": 1682327819239, "content": {"title": "Learning Dynamical Systems From Invariant Measures", "abstract": "We extend the methodology in [Yang et al., 2023] to learn autonomous continuous-time dynamical systems from invariant measures. The highlight of our approach is to reformulate the inverse problem of learning ODEs or SDEs from data as a PDE-constrained optimization problem. This shift in perspective allows us to learn from slowly sampled inference trajectories and perform uncertainty quantification for the forecasted dynamics. Our approach also yields a forward model with better stability than direct trajectory simulation in certain situations. We present numerical results for the Van der Pol oscillator and the Lorenz-63 system, together with real-world applications to Hall-effect thruster dynamics and temperature prediction, to demonstrate the effectiveness of the proposed approach."}}
{"id": "j_rNCd_26T", "cdate": 1672531200000, "mdate": 1682327819249, "content": {"title": "Adaptive State-Dependent Diffusion for Derivative-Free Optimization", "abstract": "This paper develops and analyzes a stochastic derivative-free optimization strategy. A key feature is the state-dependent adaptive variance. We prove global convergence in probability with algebraic rate and give the quantitative results in numerical examples. A striking fact is that convergence is achieved without explicit information of the gradient and even without comparing different objective function values as in established methods such as the simplex method and simulated annealing. It can otherwise be compared to annealing with state-dependent temperature."}}
{"id": "W1oLDIn8i5", "cdate": 1672531200000, "mdate": 1682322995338, "content": {"title": "Neural Inverse Operators for Solving PDE Inverse Problems", "abstract": "A large class of inverse problems for PDEs are only well-defined as mappings from operators to functions. Existing operator learning frameworks map functions to functions and need to be modified to learn inverse maps from data. We propose a novel architecture termed Neural Inverse Operators (NIOs) to solve these PDE inverse problems. Motivated by the underlying mathematical structure, NIO is based on a suitable composition of DeepONets and FNOs to approximate mappings from operators to functions. A variety of experiments are presented to demonstrate that NIOs significantly outperform baselines and solve PDE inverse problems robustly, accurately and are several orders of magnitude faster than existing direct and PDE-constrained optimization methods."}}
{"id": "oLIZ2jGTiv", "cdate": 1663850175353, "mdate": null, "content": {"title": "Tuning Frequency Bias in Neural Network Training with Nonuniform Data", "abstract": "Small generalization errors of over-parameterized neural networks (NNs) can be partially explained by the frequency biasing phenomenon, where gradient-based algorithms minimize the low-frequency misfit before reducing the high-frequency residuals. Using the Neural Tangent Kernel (NTK), one can provide a theoretically rigorous analysis for training where data are drawn from constant or piecewise-constant probability densities. Since most training data sets are not drawn from such distributions, we use the NTK model and a data-dependent quadrature rule to theoretically quantify the frequency biasing of NN training given fully nonuniform data. By replacing the loss function with a carefully selected Sobolev norm, we can further amplify, dampen, counterbalance, or reverse the intrinsic frequency biasing in NN training."}}
{"id": "uPHi_UJwDC", "cdate": 1640995200000, "mdate": 1682327819115, "content": {"title": "A Generalized Weighted Optimization Method for Computational Learning and Inversion", "abstract": "The generalization capacity of various machine learning models exhibits different phenomena in the under- and over-parameterized regimes. In this paper, we focus on regression models such as feature regression and kernel regression and analyze a generalized weighted least-squares optimization method for computational learning and inversion with noisy data. The highlight of the proposed framework is that we allow weighting in both the parameter space and the data space. The weighting scheme encodes both a priori knowledge on the object to be learned and a strategy to weight the contribution of different data points in the loss function. Here, we characterize the impact of the weighting scheme on the generalization error of the learning method, where we derive explicit generalization errors for the random Fourier feature model in both the under- and over-parameterized regimes. For more general feature maps, error bounds are provided based on the singular values of the feature matrix. We demonstrate that appropriate weighting from prior knowledge can improve the generalization capability of the learned model."}}
{"id": "hs9IwEo51T", "cdate": 1640995200000, "mdate": 1682327819118, "content": {"title": "A Quadrature Perspective on Frequency Bias in Neural Network Training with Nonuniform Data", "abstract": "Small generalization errors of over-parameterized neural networks (NNs) can be partially explained by the frequency biasing phenomenon, where gradient-based algorithms minimize the low-frequency misfit before reducing the high-frequency residuals. Using the Neural Tangent Kernel (NTK), one can provide a theoretically rigorous analysis for training where data are drawn from constant or piecewise-constant probability densities. Since most training data sets are not drawn from such distributions, we use the NTK model and a data-dependent quadrature rule to theoretically quantify the frequency biasing of NN training given fully nonuniform data. By replacing the loss function with a carefully selected Sobolev norm, we can further amplify, dampen, counterbalance, or reverse the intrinsic frequency biasing in NN training."}}
{"id": "fuKzGpD4Zov", "cdate": 1640995200000, "mdate": 1682327819117, "content": {"title": "An Algebraically Converging Stochastic Gradient Descent Algorithm for Global Optimization", "abstract": "We propose a new gradient descent algorithm with added stochastic terms for finding the global optimizers of nonconvex optimization problems, referred to as ``AdaVar'' here. A key component in the algorithm is the adaptive tuning of the randomness based on the value of the objective function. In the language of simulated annealing, the temperature is state-dependent. With this, we prove the global convergence of the algorithm with an algebraic rate both in probability and in the parameter space. This is an improvement over the classical rate from using a simpler control of the noise term. The convergence proof is based on the actual discrete setup of the algorithm. We also present several numerical examples to demonstrate the efficiency and robustness of the algorithm for reasonably complex objective functions."}}
{"id": "ZYbA0KlPmd", "cdate": 1640995200000, "mdate": 1682327819125, "content": {"title": "Adjoint DSMC for Nonlinear Spatially-Homogeneous Boltzmann Equation With a General Collision Model", "abstract": "We derive an adjoint method for the Direct Simulation Monte Carlo (DSMC) method for the spatially homogeneous Boltzmann equation with a general collision law. This generalizes our previous results in [Caflisch, R., Silantyev, D. and Yang, Y., 2021. Journal of Computational Physics, 439, p.110404], which was restricted to the case of Maxwell molecules, for which the collision rate is constant. The main difficulty in generalizing the previous results is that a rejection sampling step is required in the DSMC algorithm in order to handle the variable collision rate. We find a new term corresponding to the so-called score function in the adjoint equation and a new adjoint Jacobian matrix capturing the dependence of the collision parameter on the velocities. The new formula works for a much more general class of collision models."}}
{"id": "Rap_VYrvxT", "cdate": 1640995200000, "mdate": 1682327819120, "content": {"title": "A Generalized Weighted Optimization Method for Computational Learning and Inversion", "abstract": "The generalization capacity of various machine learning models exhibits different phenomena in the under- and over-parameterized regimes. In this paper, we focus on regression models such as feature regression and kernel regression and analyze a generalized weighted least-squares optimization method for computational learning and inversion with noisy data. The highlight of the proposed framework is that we allow weighting in both the parameter space and the data space. The weighting scheme encodes both a priori knowledge on the object to be learned and a strategy to weight the contribution of different data points in the loss function. Here, we characterize the impact of the weighting scheme on the generalization error of the learning method, where we derive explicit generalization errors for the random Fourier feature model in both the under- and over-parameterized regimes. For more general feature maps, error bounds are provided based on the singular values of the feature matrix. We demonstrate that appropriate weighting from prior knowledge can improve the generalization capability of the learned model."}}
{"id": "NkEV05l0i0Y", "cdate": 1640995200000, "mdate": 1682327819085, "content": {"title": "Anderson acceleration based on the H-s Sobolev norm for contractive and noncontractive fixed-point operators", "abstract": ""}}
