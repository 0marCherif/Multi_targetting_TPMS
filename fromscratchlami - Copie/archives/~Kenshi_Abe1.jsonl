{"id": "vxxepl74B7P", "cdate": 1693718897155, "mdate": 1693718897155, "content": {"title": "Why Guided Dialog Policy Learning performs well? Understanding the role of adversarial learning and its alternative", "abstract": "Dialog policies, which determine a system's action based on the current state at each dialog turn, are crucial to the success of the dialog. In recent years, reinforcement learning (RL) has emerged as a promising option for dialog policy learning (DPL). In RL-based DPL, dialog policies are updated according to rewards. The manual construction of fine-grained rewards, such as state-action-based ones, to effectively guide the dialog policy is challenging in multi-domain task-oriented dialog scenarios with numerous state-action pair combinations. One way to estimate rewards from collected data is to train the reward estimator and dialog policy simultaneously using adversarial learning (AL). Although this method has demonstrated superior performance experimentally, it is fraught with the inherent problems of AL, such as mode collapse. This paper first identifies the role of AL in DPL through detailed analyses of the objective functions of dialog policy and reward estimator. Next, based on these analyses, we propose a method that eliminates AL from reward estimation and DPL while retaining its advantages. We evaluate our method using MultiWOZ, a multi-domain task-oriented dialog corpus."}}
{"id": "jNnsIDP_EXQ", "cdate": 1672531200000, "mdate": 1681654458072, "content": {"title": "Learning in Multi-Memory Games Triggers Complex Dynamics Diverging from Nash Equilibrium", "abstract": ""}}
{"id": "MEQnPpVqJv", "cdate": 1672531200000, "mdate": 1683794316568, "content": {"title": "Exploration of Unranked Items in Safe Online Learning to Re-Rank", "abstract": "Bandit algorithms for online learning to rank (OLTR) problems often aim to maximize long-term revenue by utilizing user feedback. From a practical point of view, however, such algorithms have a high risk of hurting user experience due to their aggressive exploration. Thus, there has been a rising demand for safe exploration in recent years. One approach to safe exploration is to gradually enhance the quality of an original ranking that is already guaranteed acceptable quality. In this paper, we propose a safe OLTR algorithm that efficiently exchanges one of the items in the current ranking with an item outside the ranking (i.e., an unranked item) to perform exploration. We select an unranked item optimistically to explore based on Kullback-Leibler upper confidence bounds (KL-UCB) and safely re-rank the items including the selected one. Through experiments, we demonstrate that the proposed algorithm improves long-term regret from baselines without any safety violation."}}
{"id": "StZVqP8sqe5", "cdate": 1646077538124, "mdate": null, "content": {"title": "Mutation-Driven Follow the Regularized Leader for Last-Iterate Convergence in Zero-Sum Games", "abstract": "In this study, we consider a variant of the Follow the Regularized Leader (FTRL) dynamics in two-player zero-sum games. FTRL is guaranteed to converge to a Nash equilibrium when time-averaging the strategies, while a lot of variants suffer from the issue of limit cycling behavior, i.e., lack the last-iterate convergence guarantee. To this end, we propose mutant FTRL (M-FTRL), an algorithm that introduces mutation for the perturbation of action probabilities. We then investigate the continuous-time dynamics of M-FTRL and provide the strong convergence guarantees toward stationary points that approximate Nash equilibria under full-information feedback. Furthermore, our simulation demonstrates that M-FTRL can enjoy faster convergence rates than FTRL and optimistic FTRL under full-information feedback and surprisingly exhibits clear convergence under bandit feedback."}}
{"id": "q9xe2ZQCWz0", "cdate": 1640995200000, "mdate": 1675060428388, "content": {"title": "Mutation-driven follow the regularized leader for last-iterate convergence in zero-sum games", "abstract": "In this study, we consider a variant of the Follow the Regularized Leader (FTRL) dynamics in two-player zero-sum games. FTRL is guaranteed to converge to a Nash equilibrium when time-averaging the ..."}}
{"id": "pAlAKc_XjwE", "cdate": 1640995200000, "mdate": 1675060428751, "content": {"title": "Last-Iterate Convergence with Full- and Noisy-Information Feedback in Two-Player Zero-Sum Games", "abstract": "This paper proposes Mutation-Driven Multiplicative Weights Update (M2WU) for learning an equilibrium in two-player zero-sum normal-form games and proves that it exhibits the last-iterate convergence property in both full and noisy feedback settings. In the former, players observe their exact gradient vectors of the utility functions. In the latter, they only observe the noisy gradient vectors. Even the celebrated Multiplicative Weights Update (MWU) and Optimistic MWU (OMWU) algorithms may not converge to a Nash equilibrium with noisy feedback. On the contrary, M2WU exhibits the last-iterate convergence to a stationary point near a Nash equilibrium in both feedback settings. We then prove that it converges to an exact Nash equilibrium by iteratively adapting the mutation term. We empirically confirm that M2WU outperforms MWU and OMWU in exploitability and convergence rates."}}
{"id": "Tpub8ZMKl5q", "cdate": 1640995200000, "mdate": 1675060428741, "content": {"title": "Thresholded Lasso Bandit", "abstract": "In this paper, we revisit the regret minimization problem in sparse stochastic contextual linear bandits, where feature vectors may be of large dimension $d$, but where the reward function depends ..."}}
{"id": "CvaaO7DAGA", "cdate": 1640995200000, "mdate": 1675060428289, "content": {"title": "Anytime Capacity Expansion in Medical Residency Match by Monte Carlo Tree Search", "abstract": "This paper considers the capacity expansion problem in two-sided matchings, where the policymaker is allowed to allocate some extra seats as well as the standard seats. In medical residency match, each hospital accepts a limited number of doctors. Such capacity constraints are typically given in advance. However, such exogenous constraints can compromise the welfare of the doctors; some popular hospitals inevitably dismiss some of their favorite doctors. Meanwhile, it is often the case that the hospitals are also benefited to accept a few extra doctors. To tackle the problem, we propose an anytime method that the upper confidence tree searches the space of capacity expansions, each of which has a resident-optimal stable assignment that the deferred acceptance method finds. Constructing a good search tree representation significantly boosts the performance of the proposed method. Our simulation shows that the proposed method identifies an almost optimal capacity expansion with a significantly smaller computational budget than exact methods based on mixed-integer programming."}}
{"id": "B7ivbmHGLJh", "cdate": 1640995200000, "mdate": 1675060428389, "content": {"title": "Policy Gradient Algorithms with Monte-Carlo Tree Search for Non-Markov Decision Processes", "abstract": "Policy gradient (PG) is a reinforcement learning (RL) approach that optimizes a parameterized policy model for an expected return using gradient ascent. Given a well-parameterized policy model, such as a neural network model, with appropriate initial parameters, the PG algorithms work well even when environment does not have the Markov property. Otherwise, they can be trapped on a plateau or suffer from peakiness effects. As another successful RL approach, algorithms based on Monte-Carlo Tree Search (MCTS), which include AlphaZero, have obtained groundbreaking results especially on the board game playing domain. They are also suitable to be applied to non-Markov decision processes. However, since the standard MCTS does not have the ability to learn state representation, the size of the tree-search space can be too large to search. In this work, we examine a mixture policy of PG and MCTS to complement each other's difficulties and take advantage of them. We derive conditions for asymptotic convergence with results of a two-timescale stochastic approximation and propose an algorithm that satisfies these conditions. The effectivity of the proposed methods is verified through numerical experiments on non-Markov decision processes."}}
{"id": "25ryXYCSBC", "cdate": 1640995200000, "mdate": 1668869143755, "content": {"title": "Fair Matrix Factorisation for Large-Scale Recommender Systems", "abstract": "Recommender systems are hedged with various requirements, such as ranking quality, optimisation efficiency, and item fairness. Item fairness is an emerging yet impending issue in practical systems. The notion of item fairness requires controlling the opportunity of items (e.g. the exposure) by considering the entire set of rankings recommended for users. However, the intrinsic nature of fairness destroys the separability of optimisation subproblems for users and items, which is an essential property of conventional scalable algorithms, such as implicit alternating least squares (iALS). Few fairness-aware methods are thus available for large-scale item recommendation. Because of the paucity of simple tools for practitioners, unfairness issues would be costly to solve or, at worst, would be abandoned. This study takes a step towards solving real-world unfairness issues by developing a simple and scalable collaborative filtering method for fairness-aware item recommendation. We built a method named fiADMM, which inherits the scalability of iALS and maintains a provable convergence guarantee."}}
