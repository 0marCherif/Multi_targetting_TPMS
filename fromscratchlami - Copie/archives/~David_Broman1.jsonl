{"id": "fjFPsW30if", "cdate": 1672531200000, "mdate": 1681795586191, "content": {"title": "Statically Resolvable Ambiguity", "abstract": "Traditionally, a grammar defining the syntax of a programming language is typically both context free and unambiguous. However, recent work suggests that an attractive alternative is to use ambiguous grammars,thus postponing the task of resolving the ambiguity to the end user. If all programs accepted by an ambiguous grammar can be rewritten unambiguously, then the parser for the grammar is said to be resolvably ambiguous. Guaranteeing resolvable ambiguity statically---for all programs---is hard, where previous work only solves it partially using techniques based on property-based testing. In this paper, we present the first efficient, practical, and proven correct solution to the statically resolvable ambiguity problem. Our approach introduces several key ideas, including splittable productions, operator sequences, and the concept of a grouper that works in tandem with a standard parser. We prove static resolvability using a Coq mechanization and demonstrate its efficiency and practical applicability by implementing and integrating resolvable ambiguity into an essential part of the standard OCaml parser."}}
{"id": "N_WN8JMqmrH", "cdate": 1672531200000, "mdate": 1681795586018, "content": {"title": "Suspension Analysis and Selective Continuation-Passing Style for Higher-Order Probabilistic Programming Languages", "abstract": "Probabilistic programming languages (PPLs) make encoding and automatically solving statistical inference problems relatively easy by separating models from the inference algorithm. A popular choice for solving inference problems is to use Monte Carlo inference algorithms. For higher-order functional PPLs, these inference algorithms rely on execution suspension to perform inference, most often enabled through a full continuation-passing style (CPS) transformation. However, standard CPS transformations for PPL compilers introduce significant overhead, a problem the community has generally overlooked. State-of-the-art solutions either perform complete CPS transformations with performance penalties due to unnecessary closure allocations or use efficient, but complex, low-level solutions that are often not available in high-level languages. In contrast to prior work, we develop a new approach that is both efficient and easy to implement using higher-order languages. Specifically, we design a novel static suspension analysis technique that determines the parts of a program that require suspension, given a particular inference algorithm. The analysis result allows selectively CPS transforming the program only where necessary. We formally prove the correctness of the suspension analysis and implement both the suspension analysis and selective CPS transformation in the Miking CorePPL compiler. We evaluate the implementation for a large number of Monte Carlo inference algorithms on real-world models from phylogenetics, epidemiology, and topic modeling. The evaluation results demonstrate significant improvements across all models and inference algorithms."}}
{"id": "-ESvA2kvJoa", "cdate": 1672531200000, "mdate": 1681795586035, "content": {"title": "Automatic Alignment in Higher-Order Probabilistic Programming Languages", "abstract": "Probabilistic Programming Languages (PPLs) allow users to encode statistical inference problems and automatically apply an inference algorithm to solve them. Popular inference algorithms for PPLs, such as sequential Monte Carlo (SMC) and Markov chain Monte Carlo (MCMC), are built around checkpoints -- relevant events for the inference algorithm during the execution of a probabilistic program. Deciding the location of checkpoints is, in current PPLs, not done optimally. To solve this problem, we present a static analysis technique that automatically determines checkpoints in programs, relieving PPL users of this task. The analysis identifies a set of checkpoints that execute in the same order in every program run -- they are aligned. We formalize alignment, prove the correctness of the analysis, and implement the analysis as part of the higher-order functional PPL Miking CorePPL. By utilizing the alignment analysis, we design two novel inference algorithm variants: aligned SMC and aligned lightweight MCMC. We show, through real-world experiments, that they significantly improve inference execution time and accuracy compared to standard PPL versions of SMC and MCMC."}}
{"id": "zu07rNMIfUk", "cdate": 1640995200000, "mdate": 1681795586092, "content": {"title": "Stochastic Approximation for Identification of Non-Linear Differential-Algebraic Equations with Process Disturbances", "abstract": "Differential-algebraic equations, commonly used to model physical systems, are the basis for many equation-based object-oriented modeling languages. When systems described by such equations are influenced by unknown process disturbances, estimating unknown parameters from experimental data becomes difficult. This is because of problems with the existence of well-defined solutions and the computational tractability of estimators. In this paper, we propose a way to minimize a cost function\u2014whose minimizer is a consistent estimator of the true parameters\u2014using stochastic gradient descent. This approach scales significantly better with the number of unknown parameters than other currently available methods for the same type of problem. The performance of the method is demonstrated through a simulation study with three unknown parameters. The experiments show a significantly reduced variance of the estimator, compared to an output error method neglecting the influence of process disturbances, as well as an ability to reduce the estimation bias of parameters that the output error method particularly struggles with."}}
{"id": "VJA2Y3-QNd8", "cdate": 1640995200000, "mdate": 1681795586391, "content": {"title": "Expression Acceleration: Seamless Parallelization of Typed High-Level Languages", "abstract": "Efficient parallelization of algorithms on general-purpose GPUs is today essential in many areas. However, it is a non-trivial task for software engineers to utilize GPUs to improve the performance of high-level programs in general. Although many domain-specific approaches are available for GPU acceleration, it is difficult to accelerate existing high-level programs without rewriting parts of the programs using low-level GPU code. In this paper, we propose a different approach, where expressions are marked for acceleration, and the compiler automatically infers which code needs to be accelerated. We call this approach expression acceleration. We design a compiler pipeline for the approach and show how to handle several challenges, including expression extraction, well-formedness, and compiling using multiple backends. The approach is designed and implemented within a statically-typed functional intermediate language and evaluated using three distinct non-trivial case studies."}}
{"id": "3mkBcWqwfY", "cdate": 1640995200000, "mdate": 1681795586035, "content": {"title": "Programming with Context-Sensitive Holes using Dependency-Aware Tuning", "abstract": "Developing efficient and maintainable software systems is both hard and time consuming. In particular, non-functional performance requirements involve many design and implementation decisions that can be difficult to take early during system development. Choices -- such as selection of data structures or where and how to parallelize code -- typically require extensive manual tuning that is both time consuming and error-prone. Although various auto-tuning approaches exist, they are either specialized for certain domains or require extensive code rewriting to work for different contexts in the code. In this paper, we introduce a new methodology for writing programs with holes, that is, decision variables explicitly stated in the program code that enable developers to postpone decisions during development. We introduce and evaluate two novel ideas: (i) context-sensitive holes that are expanded by the compiler into sets of decision variables for automatic tuning, and (ii) dependency-aware tuning, where static analysis reduces the search space by finding the set of decision variables that can be tuned independently of each other. We evaluate the two new concepts in a system called Miking, where we show how the general methodology can be used for automatic algorithm selection, data structure decisions, and parallelization choices."}}
{"id": "3cCt6JVN6k", "cdate": 1640995200000, "mdate": 1681795586169, "content": {"title": "Compiling Universal Probabilistic Programming Languages with Efficient Parallel Sequential Monte Carlo Inference", "abstract": "Probabilistic programming languages (PPLs) allow users to encode arbitrary inference problems, and PPL implementations provide general-purpose automatic inference for these problems. However, constructing inference implementations that are efficient enough is challenging for many real-world problems. Often, this is due to PPLs not fully exploiting available parallelization and optimization opportunities. For example, handling probabilistic checkpoints in PPLs through continuation-passing style transformations or non-preemptive multitasking\u2014as is done in many popular PPLs\u2014often disallows compilation to low-level languages required for high-performance platforms such as GPUs. To solve the checkpoint problem, we introduce the concept of PPL control-flow graphs (PCFGs)\u2014a simple and efficient approach to checkpoints in low-level languages. We use this approach to implement RootPPL: a low-level PPL built on CUDA and C++ with OpenMP, providing highly efficient and massively parallel SMC inference. We also introduce a general method of compiling universal high-level PPLs to PCFGs and illustrate its application when compiling Miking CorePPL\u2014a high-level universal PPL\u2014to RootPPL. The approach is the first to compile a universal PPL to GPUs with SMC inference. We evaluate RootPPL and the CorePPL compiler through a set of real-world experiments in the domains of phylogenetics and epidemiology, demonstrating up to 6 $$\\times $$ speedups over state-of-the-art PPLs implementing SMC inference."}}
{"id": "umE2xqRbl-", "cdate": 1609459200000, "mdate": 1681795586296, "content": {"title": "Interactive Programmatic Modeling", "abstract": "Modeling and computational analyses are fundamental activities within science and engineering. Analysis activities can take various forms, such as simulation of executable models, formal verification of model properties, or inference of hidden model variables. Traditionally, tools for modeling and analysis have similar workflows: (i) a user designs a textual or graphical model or the model is inferred from data, (ii) a tool performs computational analyses on the model, and (iii) a visualization tool displays the resulting data. This article identifies three inherent problems with the traditional approach: the recomputation problem, the variable inspection problem, and the model expressiveness problem. As a solution, we propose a conceptual framework called Interactive Programmatic Modeling. We formalize the interface of the framework and illustrate how it can be used in two different domains: equation-based modeling and probabilistic programming."}}
{"id": "ty8Yzf9Yd28", "cdate": 1609459200000, "mdate": 1681795586025, "content": {"title": "Correctness of Sequential Monte Carlo Inference for Probabilistic Programming Languages", "abstract": "Probabilistic programming is an approach to reasoning under uncertainty by encoding inference problems as programs. In order to solve these inference problems, probabilistic programming languages (PPLs) employ different inference algorithms, such as sequential Monte Carlo (SMC), Markov chain Monte Carlo (MCMC), or variational methods. Existing research on such algorithms mainly concerns their implementation and efficiency, rather than the correctness of the algorithms themselves when applied in the context of expressive PPLs. To remedy this, we give a correctness proof for SMC methods in the context of an expressive PPL calculus, representative of popular PPLs such as WebPPL, Anglican, and Birch. Previous work have studied correctness of MCMC using an operational semantics, and correctness of SMC and MCMC in a denotational setting without term recursion. However, for SMC inference\u2014one of the most commonly used algorithms in PPLs as of today\u2014no formal correctness proof exists in an operational setting. In particular, an open question is if the resample locations in a probabilistic program affects the correctness of SMC. We solve this fundamental problem, and make four novel contributions: (i) we extend an untyped PPL lambda calculus and operational semantics to include explicit resample terms, expressing synchronization points in SMC inference; (ii) we prove, for the first time, that subject to mild restrictions, any placement of the explicit resample terms is valid for a generic form of SMC inference; (iii) as a result of (ii), our calculus benefits from classic results from the SMC literature: a law of large numbers and an unbiased estimate of the model evidence; and (iv) we formalize the bootstrap particle filter for the calculus and discuss how our results can be further extended to other SMC algorithms."}}
{"id": "ktHO2Y6R7g", "cdate": 1609459200000, "mdate": 1681795586281, "content": {"title": "Identification of Non-Linear Differential-Algebraic Equation Models with Process Disturbances", "abstract": "Differential-algebraic equations (DAEs) arise naturally as a result of equation-based object-oriented modeling. In many cases, these models contain unknown parameters that have to be estimated using experimental data. However, often the system is subject to unknown disturbances which, if not taken into account in the estimation, can severely affect the model\u2019s accuracy. For non-linear state-space models, particle filter methods have been developed to tackle this issue. Unfortunately, applying such methods to non-linear DAEs requires a transformation into a state-space form, which is particularly difficult to obtain for models with process disturbances. In this paper, we propose a simulation-based prediction error method that can be used for non-linear DAEs where disturbances are modeled as continuous-time stochastic processes. To the authors\u2019 best knowledge, there are no general methods successfully dealing with parameter estimation for this type of model. One of the challenges in particle filtering methods are random variations in the minimized cost function due to the nature of the algorithm. In our approach, a similar phenomenon occurs and we explicitly consider how to sample the underlying continuous process to mitigate this problem. The method is illustrated numerically on a pendulum example; the simulation results suggest that the method delivers consistent estimates."}}
