{"id": "S1lJjVHeUH", "cdate": 1567802519288, "mdate": null, "content": {"title": "Convergence Guarantees for Adaptive Bayesian Quadrature Methods", "abstract": "Adaptive Bayesian quadrature (ABQ) is a powerful approach to numerical integration that empirically compares favorably with Monte Carlo integration on problems of medium dimensionality (where non-adaptive quadrature is not competitive). Its key ingredient is an acquisition function that changes as a function of  previously collected values of the integrand. While this adaptivity appears to be empirically powerful, it complicates analysis. Consequently, there are no theoretical guarantees so far for this class of methods. In this work, for a broad class of adaptive Bayesian quadrature methods, we prove consistency, deriving non-tight but informative convergence rates. To do so we introduce a new concept we call \\emph{weak adaptivity}. In guaranteeing consistency of ABQ, weak adaptivity is notionally similar to the ideas of \\emph{detailed balance} and \\emph{ergodicity} in Markov Chain Monte Carlo methods, which allow sufficient conditions for consistency of MCMC. Likewise, our results identify a large and flexible class of adaptive Bayesian quadrature rules as consistent, within which practitioners can develop empirically efficient methods."}}
{"id": "rkWyVn-dZH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Kernel Recursive ABC: Point Estimation with Intractable Likelihood", "abstract": "We propose a novel approach to parameter estimation for simulator-based statistical models with intractable likelihood. Our proposed method involves recursive application of kernel ABC and kernel h..."}}
{"id": "rkbjKObOZr", "cdate": 1451606400000, "mdate": null, "content": {"title": "Convergence guarantees for kernel-based quadrature rules in misspecified settings", "abstract": "Kernel-based quadrature rules are becoming important in machine learning and statistics, as they achieve super-$\u00a5sqrt{n}$ convergence rates in numerical integration, and thus provide alternatives to Monte Carlo integration in challenging settings where integrands are expensive to evaluate or where integrands are high dimensional. These rules are based on the assumption that the integrand has a certain degree of smoothness, which is expressed as that the integrand belongs to a certain reproducing kernel Hilbert space (RKHS). However, this assumption can be violated in practice (e.g., when the integrand is a black box function), and no general theory has been established for the convergence of kernel quadratures in such misspecified settings. Our contribution is in proving that kernel quadratures can be consistent even when the integrand does not belong to the assumed RKHS, i.e., when the integrand is less smooth than assumed. Specifically, we derive convergence rates that depend on the (unknown) lesser smoothness of the integrand, where the degree of smoothness is expressed via powers of RKHSs or via Sobolev spaces."}}
{"id": "r1EgJyWOZB", "cdate": 1388534400000, "mdate": null, "content": {"title": "Monte Carlo Filtering Using Kernel Embedding of Distributions", "abstract": "Recent advances of kernel methods have yielded a framework for representing probabilities using a reproducing kernel Hilbert space, called kernel embedding of distributions. In this paper, we propose a Monte Carlo filtering algorithm based on kernel embeddings. The proposed method is applied to state-space models where sampling from the transition model is possible, while the observation model is to be learned from training samples without assuming a parametric model. As a theoretical basis of the proposed method, we prove consistency of the Monte Carlo method combined with kernel embeddings. Experimental results on synthetic models and real vision-based robot localization confirm the effectiveness of the proposed approach."}}
