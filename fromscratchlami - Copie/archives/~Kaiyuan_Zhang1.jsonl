{"id": "uqFelbN-Fg", "cdate": 1672531200000, "mdate": 1695949141935, "content": {"title": "ImU: Physical Impersonating Attack for Face Recognition System with Natural Style Changes", "abstract": "This paper presents a novel physical impersonating attack against face recognition systems. It aims at generating consistent style changes across multiple pictures of the attacker under different conditions and poses. Additionally, the style changes are required to be physically realizable by make-up and can induce the intended misclassification. To achieve the goal, we develop novel techniques to embed multiple pictures of the same physical person to vectors in the StyleGAN\u2019s latent space, such that the embedded latent vectors have some implicit correlations to make the search for consistent style changes feasible. Our digital and physical evaluation results show our approach can allow an outsider attacker to successfully impersonate the insiders with consistent and natural changes."}}
{"id": "mzqPwZAKQg", "cdate": 1672531200000, "mdate": 1695949141930, "content": {"title": "FLIP: A Provable Defense Framework for Backdoor Mitigation in Federated Learning", "abstract": ""}}
{"id": "Iaol0huB5X", "cdate": 1672531200000, "mdate": 1695949141962, "content": {"title": "ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned Samples in NLP", "abstract": "Backdoor attacks have emerged as a prominent threat to natural language processing (NLP) models, where the presence of specific triggers in the input can lead poisoned models to misclassify these inputs to predetermined target classes. Current detection mechanisms are limited by their inability to address more covert backdoor strategies, such as style-based attacks. In this work, we propose an innovative test-time poisoned sample detection framework that hinges on the interpretability of model predictions, grounded in the semantic meaning of inputs. We contend that triggers (e.g., infrequent words) are not supposed to fundamentally alter the underlying semantic meanings of poisoned samples as they want to stay stealthy. Based on this observation, we hypothesize that while the model's predictions for paraphrased clean samples should remain stable, predictions for poisoned samples should revert to their true labels upon the mutations applied to triggers during the paraphrasing process. We employ ChatGPT, a state-of-the-art large language model, as our paraphraser and formulate the trigger-removal task as a prompt engineering problem. We adopt fuzzing, a technique commonly used for unearthing software vulnerabilities, to discover optimal paraphrase prompts that can effectively eliminate triggers while concurrently maintaining input semantics. Experiments on 4 types of backdoor attacks, including the subtle style backdoors, and 4 distinct datasets demonstrate that our approach surpasses baseline methods, including STRIP, RAP, and ONION, in precision and recall."}}
{"id": "E39mJ-6cwm", "cdate": 1672531200000, "mdate": 1681586542382, "content": {"title": "Detecting Backdoors in Pre-trained Encoders", "abstract": ""}}
{"id": "1L2LPjz2yqH", "cdate": 1672531200000, "mdate": 1680903477154, "content": {"title": "BEAGLE: Forensics of Deep Learning Backdoor Attack for Better Defense", "abstract": ""}}
{"id": "Xo2E217_M4n", "cdate": 1663850525684, "mdate": null, "content": {"title": "FLIP: A Provable Defense Framework for Backdoor Mitigation in Federated Learning", "abstract": "Federated Learning (FL) is a distributed learning paradigm that enables different parties to train a model together for high quality and strong privacy protection. In this scenario, individual participants may get compromised and perform backdoor attacks by poisoning the data (or gradients). Existing work on robust aggregation and certified FL robustness does not study how hardening benign clients can affect the global model (and the malicious clients). In this work, we theoretically analyze the connection among cross-entropy loss, attack success rate, and clean accuracy in this setting. Moreover, we propose a trigger reverse engineering based defense and show that our method can achieve robustness improvement with guarantee (i.e., reducing the attack success rate) without affecting benign accuracy. We conduct comprehensive experiments across different datasets and attack settings. Our results on nine competing SOTA defense methods show the empirical superiority of our method on both single-shot and continuous FL backdoor attacks. Code is available at https://github.com/KaiyuanZh/FLIP."}}
