{"id": "a9WVZyzqgn", "cdate": 1696336476493, "mdate": 1696336476493, "content": {"title": "Dynamic Hypergraph Structure Learning for Traffic Flow Forecasting", "abstract": "This paper studies the problem of traffic flow\nforecasting, which aims to predict future traffic conditions on\nthe basis of road networks and traffic conditions in the past.\nThe problem is typically solved by modeling complex spatiotemporal correlations in traffic data using spatio-temporal graph\nneural networks (GNNs). However, the performance of these\nmethods is still far from satisfactory since GNNs usually have\nlimited representation capacity when it comes to complex traffic\nnetworks. Graphs, by nature, fall short in capturing non-pairwise\nrelations. Even worse, existing methods follow the paradigm\nof message passing that aggregates neighborhood information\nlinearly, which fails to capture complicated spatio-temporal highorder interactions. To tackle these issues, in this paper, we\npropose a novel model named Dynamic Hypergraph Structure\nLearning (DyHSL) for traffic flow prediction. To learn nonpairwise relationships, our DyHSL extracts hypergraph structural information to model dynamics in the traffic networks, and\nupdates each node representation by aggregating messages from\nits associated hyperedges. Additionally, to capture high-order\nspatio-temporal relations in the road network, we introduce an\ninteractive graph convolution block, which further models the\nneighborhood interaction for each node. Finally, we integrate\nthese two views into a holistic multi-scale correlation extraction\nmodule, which conducts temporal pooling with different scales\nto model different temporal patterns. Extensive experiments\non four popular traffic benchmark datasets demonstrate the\neffectiveness of our proposed DyHSL compared with a broad\nrange of competing baselines."}}
{"id": "66lwEpqBZA", "cdate": 1668595437114, "mdate": 1668595437114, "content": {"title": "Rethinking IoU-based Optimization for Single-stage 3D Object Detection", "abstract": "Since Intersection-over-Union (IoU) based optimization maintains the consistency of the final IoU prediction metric and losses, it has been widely used in both regression and classification branches of single-stage 2D object detectors. Recently, several 3D object detection methods adopt IoU-based optimization and directly replace the 2D IoU with 3D IoU. However, such a direct computation in 3D is very costly due to the complex implementation and inefficient backward operations. Moreover, 3D IoU-based optimization is sub-optimal as it is sensitive to rotation and thus can cause training instability and detection performance deterioration. In this paper, we propose a novel Rotation-Decoupled IoU (RDIoU) method that can mitigate the rotation-sensitivity issue, and produce more efficient optimization objectives compared with 3D IoU during the training stage. Specifically, our RDIoU simplifies the complex interactions of regression parameters by decoupling the rotation variable as an independent term, yet preserving the geometry of 3D IoU. By incorporating RDIoU into both the regression and classification branches, the network is encouraged to learn more precise bounding boxes and concurrently overcome the misalignment issue between classification and regression. Extensive experiments on the benchmark KITTI and Waymo Open Dataset validate that our RDIoU method can bring substantial improvement for the single-stage 3D object detection."}}
{"id": "qZrAPlmkWhC", "cdate": 1668595201567, "mdate": 1668595201567, "content": {"title": "Improving 3d object detection with channel-wise transformer", "abstract": "Though 3D object detection from point clouds has achieved rapid progress in recent years, the lack of flexible and high-performance proposal refinement remains a great hurdle for existing state-of-the-art two-stage detectors. Previous works on refining 3D proposals have relied on human-designed components such as keypoints sampling, set abstraction and multi-scale feature fusion to produce powerful 3D object representations. Such methods, however, have limited ability to capture rich contextual dependencies among points. In this paper, we leverage the high-quality region proposal network and a Channel-wise Transformer architecture to constitute our two-stage 3D object detection framework (CT3D) with minimal hand-crafted design. The proposed CT3D simultaneously performs proposal-aware embedding and channel-wise context aggregation for the point features within each proposal. Specifically, CT3D uses proposal's keypoints for spatial contextual modelling and learns attention propagation in the encoding module, mapping the proposal to point embeddings. Next, a new channel-wise decoding module enriches the query-key interaction via channel-wise re-weighting to effectively merge multi-level contexts, which contributes to more accurate object predictions. Extensive experiments demonstrate that our CT3D method has superior performance and excellent scalability. Remarkably, CT3D achieves the AP of 81.77% in the moderate car category on the KITTI test 3D detection benchmark, outperforms state-of-the-art 3D detectors."}}
{"id": "I9VZRzTSDNA", "cdate": 1668566308246, "mdate": 1668566308246, "content": {"title": "SP-ViT: Learning 2D Spatial Priors for Vision Transformers", "abstract": "Transformers have shown great potential in image classification and established state-of-the-art results on the ImageNet benchmark. In contrast to CNNs which leverage the local correlation properties of image content, the spatial arrangement of an image is dissolved in transformers at the input level. As a result, vision transformers (ViT) are initially unbiased in learning spatial relationships from data, where nearby pixels have the same chance of interacting as far away pixels and complex relationships can be learned more easily. Yet, due to their large capacity, ViTs converge more slowly and are prone to overfitting in low-data regimes. To overcome this limitation, we propose\nSpatial Prior \u2013 enhanced Self-Attention (SP-SA), a novel variant of Self-Attention (SA) tailored for ViTs. Unlike convolutional inductive biases, which focus exclusively on hard-coded local regions, the proposed Spatial Priors are learned by the model itself and take a variety of complementary spatial relations into account. Experiments show that SP-SA consistently improves the performance of ViT models. We denote the resulting models SP-ViT. Taking a recently proposed vision transformer LV-ViT as an example, when equipped with SP-SA, the largest model achieves a record-equalling 86.1% Top-1 accuracy with nearly halved parameters (150M SP-ViT-L\u2191384 vs 271M CaiT-M-36\u2191384) among all ImageNet-1K models trained on 224x224 and fine-tuned on 384x384 resolution w/o extra data."}}
{"id": "HXmli9WIyq", "cdate": 1667812869506, "mdate": 1667812869506, "content": {"title": "CoHOZ: Contrastive Multimodal Prompt Tuning for Hierarchical Open-set Zero-shot Recognition", "abstract": "Practical image recognition often encounters samples whose labels either are totally unknown or belong to new classes outside the training set. The first problem refers to the open-set recognition (OSR), in which unknown classes are recognized as one with no more semantic information. While the latter is called zero-shot learning (ZSL), in which new classes are usually predefined. The existing literature mostly addresses these two problems separately. In this paper, we take the ambition for solving the combination of these two problems to fulfill semantically recognizing the unknown classes detected in OSR by zero-shot prediction. We propose the Contrastive multimodal prompt tuning for Hierarchical Open-set Zero-shot recognition (CoHOZ). Specifically, we firstly build a global and compatible hierarchical label tree with all downstream datasets aligned, which lays foundations for other modules. To detect unknown classes, we propose the contrastive continuous prompt tuning, which introduces additional negative classes from the fine level of the built hierarchy for prompt learning. To generate candidate classes for zero-shot prediction on the unknown data using prompt, we combine the built hierarchy to collect candidate classes from coarse to fine. In our experiments, when following the standard OSR protocol regarding all the unknown classes as a single class, CoHOZ achieves a new state-of-the-art performance both in unknown detection and open-set recognition. Few-shot tuning by the CoHOZ also shows competitive performance on them. In addition, the detailed semantic information of unknown classes are well explored, which has also been verified in experiments."}}
{"id": "V4CJqUUYG62", "cdate": 1667528062629, "mdate": 1667528062629, "content": {"title": "Category dictionary guided unsupervised domain adaptation for object detection", "abstract": "Unsupervised domain adaption (UDA) is a promising solution to enhance the generalization ability of a model from a source domain to a target domain without manually annotating labels for target data. Recent works in cross-domain object detection mostly resort to adversarial feature adaptation to match the marginal distributions of two domains. However, perfect feature alignment is hard to achieve and is likely to cause negative transfer due to the high complexity of object detection. In this paper, we propose a category dictionary guided (CDG) UDA model for cross-domain object detection, which learns category-specific dictionaries from the source domain to represent the candidate boxes in target domain. The representation residual can be used for not only pseudo label assignment but also quality (eg, IoU) estimation of the candidate box. A residual weighted self-training paradigm is then developed to implicitly align source and target domains for detection model training. Compared with decision boundary based classifiers such as softmax, the proposed CDG scheme can select more informative and reliable pseudo-boxes. Experimental results on benchmark datasets show that the proposed CDG significantly exceeds the state-of-the-arts in cross-domain object detection."}}
{"id": "0AS3LYWDUNB", "cdate": 1667527448990, "mdate": 1667527448990, "content": {"title": "Dynamic Anchor Feature Selection for Single-Shot Object Detection", "abstract": "The design of anchors is critical to the performance of one-stage detectors. Recently, the anchor refinement module (ARM) has been proposed to adjust the initialization of default anchors, providing the detector a better anchor reference. However, this module brings another problem: all pixels at a feature map have the same receptive field while the anchors associated with each pixel have different positions and sizes. This discordance may lead to a less effective detector. In this paper, we present a dynamic feature selection operation to select new pixels in a feature map for each refined anchor received from the ARM. The pixels are selected based on the new anchor position and size so that the receptive filed of these pixels can fit the anchor areas well, which makes the detector, especially the regression part, much easier to optimize. Furthermore, to enhance the representation ability of selected feature pixels, we design a bidirectional feature fusion module by combining features from early and deep layers. Extensive experiments on both PASCAL VOC and COCO demonstrate the effectiveness of our dynamic anchor feature selection (DAFS) operation. For the case of high IoU threshold, our DAFS can improve the mAP by a large margin."}}
{"id": "S9XA0GM5R0T", "cdate": 1664386903464, "mdate": 1664386903464, "content": {"title": "Cross-Domain Empirical Risk Minimization for Unbiased Long-tailed Classification", "abstract": "We address the overlooked unbiasedness in existing long-tailed classification methods: we find that their overall improvement is mostly attributed to the biased preference of\" tail\" over\" head\", as the test distribution is assumed to be balanced; however, when the test is as imbalanced as the long-tailed training data---let the test respect Zipf's law of nature---the\" tail\" bias is no longer beneficial overall because it hurts the\" head\" majorities. In this paper, we propose Cross-Domain Empirical Risk Minimization (xERM) for training an unbiased test-agnostic model to achieve strong performances on both test distributions, which empirically demonstrates that xERM fundamentally improves the classification by learning better feature representation rather than the\" head vs. tail\" game. Based on causality, we further theoretically explain why xERM achieves unbiasedness: the bias caused by the domain selection is removed by adjusting the empirical risks on the imbalanced domain and the balanced but unseen domain."}}
{"id": "PldynS56bN", "cdate": 1663849869295, "mdate": null, "content": {"title": "Contextual Convolutional Networks", "abstract": "This paper presents a new Convolutional Neural Network, named Contextual Convolutional Network, that capably serves as a general-purpose backbone for visual recognition. Most existing convolutional backbones follow the representation-to-classification paradigm, where representations of the input are firstly generated by category-agnostic convolutional operations, and then fed into classifiers for specific perceptual tasks (e.g., classification and segmentation). In this paper, we deviate from this classic paradigm and propose to augment potential category memberships as contextual priors in the convolution for contextualized representation learning. Specifically, top-k likely classes from the preceding stage are encoded as a contextual prior vector. Based on this vector and the preceding features, offsets for spatial sampling locations and kernel weights are generated to modulate the convolution operations. The new convolutions can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation without additional supervision. The qualities of Contextual Convolutional Networks make it compatible with a broad range of vision tasks and boost the state-of-the-art architecture ConvNeXt-Tiny by 1.8% on top-1 accuracy of ImageNet classification. The superiority of the proposed model reveals the potential of contextualized representation learning for vision tasks. Code is available at: \\url{https://github.com/liang4sx/contextual_cnn}.\n"}}
{"id": "3d6PLMQm5Uj", "cdate": 1658219034470, "mdate": null, "content": {"title": "Beyond a Video Frame Interpolator: A Space Decoupled Learning Approach to Continuous Image Transition", "abstract": "Video frame interpolation (VFI) aims to improve the temporal resolution of a video sequence. Most of the existing deep learning based VFI methods adopt off-the-shelf optical flow algorithms to estimate the bidirectional flows and interpolate the missing frames accordingly. Though having achieved a great success, these methods require much human experience to tune the bidirectional flows and often generate unpleasant results when the estimated flows are not accurate. In this work, we rethink the VFI problem and formulate it as a continuous image transition (CIT) task, whose key issue is to transition an image from one space to another space continuously. More specifically, we learn to implicitly decouple the images into a translatable flow space and a non-translatable feature space. The former depicts the translatable states between the given images, while the later aims to reconstruct the intermediate features that cannot be directly translated. In this way, we can easily perform image interpolation in the flow space and intermediate image synthesis in the feature space, obtaining a CIT model. The proposed space decoupled learning (SDL) approach is simple to implement, while it provides an effective framework to a variety of CIT problems beyond VFI, such as style transfer and image morphing. Our extensive experiments on a variety of CIT tasks demonstrate the superiority of SDL to existing methods. Codes will be made publicly available."}}
