{"id": "6nC7UNRe7K", "cdate": 1684308596630, "mdate": 1684308596630, "content": {"title": "The Matching Problem Has No Small Symmetric SDP.", "abstract": "Any symmetric SDP relaxation of the matching problem has exponential size."}}
{"id": "kFIg4UyH4M", "cdate": 1640995200000, "mdate": 1682705923926, "content": {"title": "N-Grammer: Augmenting Transformers with latent n-grams", "abstract": "Transformer models have recently emerged as one of the foundational models in natural language processing, and as a byproduct, there is significant recent interest and investment in scaling these models. However, the training and inference costs of these large Transformer language models are prohibitive, thus necessitating more research in identifying more efficient variants. In this work, we propose a simple yet effective modification to the Transformer architecture inspired by the literature in statistical language modeling, by augmenting the model with n-grams that are constructed from a discrete latent representation of the text sequence. We evaluate our model, the N-Grammer on language modeling on the C4 data-set as well as text classification on the SuperGLUE data-set, and find that it outperforms several strong baselines such as the Transformer and the Primer. We open-source our model for reproducibility purposes in Jax."}}
{"id": "pqTKt4HzTL", "cdate": 1609459200000, "mdate": 1681709466217, "content": {"title": "Hurdles to Progress in Long-form Question Answering", "abstract": ""}}
{"id": "lIpILiYdH7", "cdate": 1609459200000, "mdate": 1682705923927, "content": {"title": "Efficient Content-Based Sparse Attention with Routing Transformers", "abstract": "Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1"}}
{"id": "eCGBKERtUk", "cdate": 1609459200000, "mdate": null, "content": {"title": "Hurdles to Progress in Long-form Question Answering", "abstract": "The task of long-form question answering (LFQA) involves retrieving documents relevant to a given question and using them to generate a paragraph-length answer. While many models have recently been proposed for LFQA, we show in this paper that the task formulation raises fundamental challenges regarding evaluation and dataset creation that currently preclude meaningful modeling progress. To demonstrate these challenges, we first design a new system that relies on sparse attention and contrastive retriever learning to achieve state-of-the-art performance on the ELI5 LFQA dataset. While our system tops the public leaderboard, a detailed analysis reveals several troubling trends: (1) our system's generated answers are not actually grounded in the documents that it retrieves; (2) ELI5 contains significant train / validation overlap, as at least 81% of ELI5 validation questions occur in paraphrased form in the training set; (3) ROUGE-L is not an informative metric of generated answer quality and can be easily gamed; and (4) human evaluations used for other text generation tasks are unreliable for LFQA. We offer suggestions to mitigate each of these issues, which we hope will lead to more rigorous LFQA research and meaningful progress in the future."}}
{"id": "sumpynBK1Qd", "cdate": 1577836800000, "mdate": 1682705923932, "content": {"title": "Efficient Content-Based Sparse Attention with Routing Transformers", "abstract": "Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic compute and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: it combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to $O\\left(n^{1.5}d\\right)$ from $O\\left(n^2d\\right)$ for sequence length $n$ and hidden dimension $d$. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity) as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192."}}
{"id": "B1gjs6EtDr", "cdate": 1569439139242, "mdate": null, "content": {"title": "Efficient Content-Based Sparse Attention with Routing Transformers", "abstract": "Self-attention has recently been adopted for a wide range of sequence modeling\nproblems. Despite its effectiveness, self-attention suffers quadratic compute and\nmemory requirements with respect to sequence length. Successful approaches to\nreduce this complexity focused on attention to local sliding windows or a small\nset of locations independent of content. Our work proposes to learn dynamic\nsparse attention patterns that avoid allocating computation and memory to attend\nto content unrelated to the query of interest. This work builds upon two lines of\nresearch: it combines the modeling flexibility of prior work on content-based sparse\nattention with the efficiency gains from approaches based on local, temporal sparse\nattention. Our model, the Routing Transformer, endows self-attention with a sparse\nrouting module based on online k-means while reducing the overall complexity of\nattention to O(n^{1.5}d) from O(n^2d) for sequence length n and hidden dimension\nd. We show that our model outperforms comparable sparse attention models on\nlanguage modeling on Wikitext-103 (15.8 vs 18.3 perplexity) as well as on\nimage generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers.\nCode will be open-sourced on acceptance."}}
{"id": "qrjLs6OGCqW", "cdate": 1546300800000, "mdate": 1682705923969, "content": {"title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations."}}
{"id": "iFE_NKbaOy5", "cdate": 1546300800000, "mdate": 1682705923938, "content": {"title": "Unsupervised Paraphrasing without Translation", "abstract": "Paraphrasing exemplifies the ability to abstract semantic content from surface forms. Recent work on automatic paraphrasing is dominated by methods leveraging Machine Translation (MT) as an intermediate step. This contrasts with humans, who can paraphrase without being bilingual. This work proposes to learn paraphrasing models from an unlabeled monolingual corpus only. To that end, we propose a residual variant of vector-quantized variational auto-encoder. We compare with MT-based approaches on paraphrase identification, generation, and training augmentation. Monolingual paraphrasing outperforms unsupervised translation in all settings. Comparisons with supervised translation are more mixed: monolingual paraphrasing is interesting for identification and augmentation; supervised translation is superior for generation."}}
{"id": "Xai4AQVYCpK", "cdate": 1546300800000, "mdate": 1682705923967, "content": {"title": "Unsupervised Paraphrasing without Translation", "abstract": ""}}
