{"id": "UmaiVbwN1v", "cdate": 1652737700891, "mdate": null, "content": {"title": "A Win-win Deal: Towards Sparse and Robust Pre-trained Language Models", "abstract": "Despite the remarkable success of pre-trained language models (PLMs), they still face two challenges: First, large-scale PLMs are inefficient in terms of memory footprint and computation. Second, on the downstream tasks, PLMs tend to rely on the dataset bias and struggle to generalize to out-of-distribution (OOD) data. In response to the efficiency problem, recent studies show that dense PLMs can be replaced with sparse subnetworks without hurting the performance. Such subnetworks can be found in three scenarios: 1) the fine-tuned PLMs, 2) the raw PLMs and then fine-tuned in isolation, and even inside 3) PLMs without any parameter fine-tuning. However, these results are only obtained in the in-distribution (ID) setting. In this paper, we extend the study on PLMs subnetworks to the OOD setting, investigating whether sparsity and robustness to dataset bias can be achieved simultaneously. To this end, we conduct extensive experiments with the pre-trained BERT model on three natural language understanding (NLU) tasks. Our results demonstrate that \\textbf{sparse and robust subnetworks (SRNets) can consistently be found in BERT}, across the aforementioned three scenarios, using different training and compression methods. Furthermore, we explore the upper bound of SRNets using the OOD information and show that \\textbf{there exist sparse and almost unbiased BERT subnetworks}. Finally, we present 1) an analytical study that provides insights on how to promote the efficiency of SRNets searching process and 2) a solution to improve subnetworks' performance at high sparsity. The code is available at \\url{https://github.com/llyx97/sparse-and-robust-PLM}."}}
{"id": "woIL3cAMBQ", "cdate": 1609459200000, "mdate": 1636850933584, "content": {"title": "SOM-NCSCM : An Efficient Neural Chinese Sentence Compression Model Enhanced with Self-Organizing Map", "abstract": "Kangli Zi, Shi Wang, Yu Liu, Jicun Li, Yanan Cao, Cungen Cao. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021."}}
{"id": "sQql5QNATFq", "cdate": 1609459200000, "mdate": 1636009568496, "content": {"title": "Flexible Non-Autoregressive Extractive Summarization with Threshold: How to Extract a Non-Fixed Number of Summary Sentences", "abstract": "Sentence-level extractive summarization is a fundamental yet challenging task, and recent powerful approaches prefer to pick sentences sorted by the predicted probabilities until the length limit is reached, a.k.a. ``Top-K Strategy''. This length limit is fixed based on the validation set, resulting in the lack of flexibility. In this work, we propose a more flexible and accurate non-autoregressive method for single document extractive summarization, extracting a non-fixed number of summary sentences without the sorting step. We call our approach ThresSum as it picks sentences simultaneously and individually from the source document when the predicted probabilities exceed a threshold. During training, the model enhances sentence representation through iterative refinement and the intermediate latent variables receive some weak supervision with soft labels, which are generated progressively by adjusting the temperature with a knowledge distillation algorithm. Specifically, the temperature is initialized with high value and drops along with the iteration until a temperature of 1. Experimental results on CNN/DM and NYT datasets have demonstrated the effectiveness of ThresSum, which significantly outperforms BERTSUMEXT with a substantial improvement of 0.74 ROUGE-1 score on CNN/DM. Our source code will be available on Github."}}
{"id": "lkctg41Z_Sv", "cdate": 1609459200000, "mdate": 1634284450887, "content": {"title": "Multi-Granularity Heterogeneous Graph for Document-Level Relation Extraction", "abstract": "Reading text to extract relational facts has been a long-standing goal in natural language processing. It becomes especially challenging when the extraction scope is extended to document level, where multiple entities in a document generally exhibit complex intra- and inter-sentence relations. In this paper, we propose a novel Multi-granularity Heterogeneous Graph (MHG) to tackle this challenge. Specifically, we define four types of nodes with different granularities and eight types of edges based on heuristic rules, entrusting the MHG two major advantages. On the one hand, it connects any two entities with a short path in the graph to better handle the complex inter-sentence interactions between entities. On the other hand, it enables rich interactions among nodes with different granularities to promote accurate multi-hop reasoning. Experimental results on the largest document-level relation extraction dataset suggest that the proposed model achieves new state-of-the-art performance."}}
{"id": "gg_9vR2sVw-", "cdate": 1609459200000, "mdate": 1636009568496, "content": {"title": "Deep Differential Amplifier for Extractive Summarization", "abstract": "Ruipeng Jia, Yanan Cao, Fang Fang, Yuchen Zhou, Zheng Fang, Yanbing Liu, Shi Wang. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021."}}
{"id": "gLbbfp4kJL", "cdate": 1609459200000, "mdate": 1636850931802, "content": {"title": "TEBNER: Domain Specific Named Entity Recognition with Type Expanded Boundary-aware Network", "abstract": "Zheng Fang, Yanan Cao, Tai Li, Ruipeng Jia, Fang Fang, Yanmin Shang, Yuhai Lu. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021."}}
{"id": "V6YQngIXUQu", "cdate": 1609459200000, "mdate": 1636850925355, "content": {"title": "Incorporating Specific Knowledge into End-to-End Task-oriented Dialogue Systems", "abstract": "External knowledge is vital to many natural language processing tasks. However, current end-to-end dialogue systems often struggle to interface knowledge bases(KBs) with response smoothly and effectively. In this paper, we convert the raw knowledge into relation knowledge and integrated knowledge and then incorporate them into end-to-end task-oriented dialogue systems. The relation knowledge extracted from knowledge triples is combined with dialogue history, aiming to enhance semantic inputs and support better language understanding. Integrated knowledge involves entities and relations by graph attention, assisting the model in generating informative responses. The experimental results on three public dialogue datasets show that our model improves over the previous state-of-the-art models in sentence fluency and informativeness."}}
{"id": "FJfpDUln0OP", "cdate": 1609459200000, "mdate": 1636850928063, "content": {"title": "Fake News Detection with Heterogenous Deep Graph Convolutional Network", "abstract": "Fake news detection is a challenging problem due to its tremendous real-world political and social impacts. Previous works judged the authenticity of news mainly based on the content of a single news, which is generally not effective because the fake news is often written to mislead users by mimicking the true news. This paper innovatively utilizes the connection between multiple news, such as their relevance in time, content, topic and source, to detect fake news. We construct a heterogeneous graph with different types of nodes and edges, which is named as News Detection Graph (NDG), to integrate various information of multiple news. In order to learn deep representation of news nodes, we propose a Heterogenous Deep Convolutional Network (HDGCN) which utilizes a wider receptive field, a neighbor sampling strategy and a hierarchical attention mechanism. Extensive experiments carried on two real-world datasets demonstrated the effectiveness of our work in solving the fake news detection problem."}}
{"id": "4r9wV5Ux_i", "cdate": 1609459200000, "mdate": 1636850936478, "content": {"title": "How Does Knowledge Graph Embedding Extrapolate to Unseen Data: a Semantic Evidence View", "abstract": "Knowledge Graph Embedding (KGE) aims to learn representations for entities and relations. Most KGE models have gained great success, especially on extrapolation scenarios. Specifically, given an unseen triple (h, r, t), a trained model can still correctly predict t from (h, r, ?), or h from (?, r, t), such extrapolation ability is impressive. However, most existing KGE works focus on the design of delicate triple modeling function, which mainly tells us how to measure the plausibility of observed triples, but offers limited explanation of why the methods can extrapolate to unseen data, and what are the important factors to help KGE extrapolate. Therefore in this work, we attempt to study the KGE extrapolation of two problems: 1. How does KGE extrapolate to unseen data? 2. How to design the KGE model with better extrapolation ability? For the problem 1, we first discuss the impact factors for extrapolation and from relation, entity and triple level respectively, propose three Semantic Evidences (SEs), which can be observed from train set and provide important semantic information for extrapolation. Then we verify the effectiveness of SEs through extensive experiments on several typical KGE methods. For the problem 2, to make better use of the three levels of SE, we propose a novel GNN-based KGE model, called Semantic Evidence aware Graph Neural Network (SE-GNN). In SE-GNN, each level of SE is modeled explicitly by the corresponding neighbor pattern, and merged sufficiently by the multi-layer aggregation, which contributes to obtaining more extrapolative knowledge representation. Finally, through extensive experiments on FB15k-237 and WN18RR datasets, we show that SE-GNN achieves state-of-the-art performance on Knowledge Graph Completion task and performs a better extrapolation ability. Our code is available at https://github.com/renli1024/SE-GNN."}}
{"id": "oOwaDos_sS", "cdate": 1577836800000, "mdate": 1636850928280, "content": {"title": "HIN: Hierarchical Inference Network for Document-Level Relation Extraction", "abstract": "Document-level RE requires reading, inferring and aggregating over multiple sentences. From our point of view, it is necessary for document-level RE to take advantage of multi-granularity inference information: entity level, sentence level and document level. Thus, how to obtain and aggregate the inference information with different granularity is challenging for document-level RE, which has not been considered by previous work. In this paper, we propose a Hierarchical Inference Network (HIN) to make full use of the abundant information from entity level, sentence level and document level. Translation constraint and bilinear transformation are applied to target entity pair in multiple subspaces to get entity-level inference information. Next, we model the inference between entity-level information and sentence representation to achieve sentence-level inference information. Finally, a hierarchical aggregation approach is adopted to obtain the document-level inference information. In this way, our model can effectively aggregate inference information from these three different granularities. Experimental results show that our method achieves state-of-the-art performance on the large-scale DocRED dataset. We also demonstrate that using BERT representations can further substantially boost the performance."}}
