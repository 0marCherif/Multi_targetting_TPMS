{"id": "6_Lzr76yt_P", "cdate": 1678870963734, "mdate": 1678870963734, "content": {"title": "Reconstructing invariances of CT image denoising networks using invertible neural networks", "abstract": "Long lasting efforts have been made to reduce radiation dose and thus the potential radiation risk to the patient for CT acquisitions without severe deterioration of image quality. To this end, different reconstruction and noise reduction algorithms have been developed, many of which are based on iterative reconstruction techniques, incorporating prior knowledge in the image domain. Recently, deep learning-based methods have shown impressive performance, outperforming many of the previously proposed CT denoising approaches both visually and quantitatively. However, with most neural networks being black boxes they remain notoriously difficult to interpret and concerns about the robustness and safety of such denoising methods have been raised. In this work we want to lay the fundamentals for a post-hoc interpretation of existing CT denoising networks by reconstructing their invariances."}}
{"id": "O-xLLzUwx3", "cdate": 1668631574287, "mdate": null, "content": {"title": "Deep learning-based reconstruction of interventional tools and devices from four X-ray projections for tomographic interventional guidance", "abstract": "Purpose: Image guidance for minimally invasive interventions is usually performed by acquiring fluoroscopic images using a monoplanar or a biplanar C-arm system. However, the projective data provide only limited information about the spatial structure and position of interventional tools and devices such as stents, guide wires, or coils. In this work, we propose a deep learning-based pipeline for real-time tomographic (four-dimensional [4D]) interventional guidance at conventional dose levels.\nMethods: Our pipeline is comprised of two steps. In the first one, interventional tools are extracted from four cone-beam CT projections using a deep convolutional neural network. These projections are then Feldkamp reconstructed and fed into a second network, which is trained to segment the interventional tools and devices in this highly undersampled reconstruction. Both networks are trained using simulated CT data and evaluated on both simulated data and C-arm cone-beam CT measurements of stents, coils, and guide wires.\nResults: The pipeline is capable of reconstructing interventional tools from only four X-ray projections without the need for a patient prior. At an isotropic voxel size of 100\u00b5m, our methods achieve a precision/recall within a 100\u00b5m environment of the ground truth of 93%/98%, 90%/71%, and 93%/76% for guide wires, stents, and coils, respectively.\nConclusions: A deep learning-based approach for 4D interventional guidance is able to overcome the drawbacks of today's interventional guidance by providing full spatiotemporal (4D) information about the interventional tools at dose levels comparable to conventional fluoroscopy."}}
{"id": "FQzwanJok9", "cdate": 1609459200000, "mdate": 1668627777668, "content": {"title": "DiagViB-6: A Diagnostic Benchmark Suite for Vision Models in the Presence of Shortcut and Generalization Opportunities", "abstract": "Common deep neural networks (DNNs) for image classification have been shown to rely on shortcut opportunities (SO) in the form of predictive and easy-to-represent visual factors. This is known as shortcut learning and leads to impaired generalization. In this work, we show that common DNNs also suffer from shortcut learning when predicting only basic visual object factors of variation (FoV) such as shape, color, or texture. We argue that besides shortcut opportunities, generalization opportunities (GO) are also an inherent part of real-world vision data and arise from partial independence between predicted classes and FoVs. We also argue that it is necessary for DNNs to exploit GO to overcome shortcut learning. Our core contribution is to introduce the Diagnostic Vision Benchmark suite DiagViB-6, which includes datasets and metrics to study a network\u2019s shortcut vulnerability and generalization capability for six independent FoV. In particular, DiagViB-6 allows controlling the type and degree of SO and GO in a dataset. We benchmark a wide range of popular vision architectures and show that they can exploit GO only to a limited extent."}}
