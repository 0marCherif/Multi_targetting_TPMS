{"id": "20dw9d0qlP", "cdate": 1672655960204, "mdate": 1672655960204, "content": {"title": "Sampling-based inference for large linear models, with application to linearised Laplace", "abstract": "Large-scale linear models are ubiquitous throughout machine learning, with con- temporary application as surrogate models for neural network uncertainty quan- tification; that is, the linearised Laplace method. Alas, the computational cost associated with Bayesian linear models constrains this method\u2019s application to small networks, small output spaces and small datasets. We address this limitation by introducing a scalable sample-based Bayesian inference method for conjugate Gaussian multi-output linear models, together with a matching method for hy- perparameter (regularisation) selection. Furthermore, we use a classic feature normalisation method (the g-prior) to resolve a previously highlighted pathology of the linearised Laplace method. Together, these contributions allow us to perform linearised neural network inference with ResNet-18 on CIFAR100 (11M parame- ters, 100 output dimensions \u00d7 50k datapoints) and with a U-Net on a high-resolution\ntomographic reconstruction task (2M parameters, 251k output dimensions)."}}
{"id": "aoDyX6vSqsd", "cdate": 1663850290586, "mdate": null, "content": {"title": "Sampling-based inference for large linear models, with application to linearised Laplace", "abstract": "Large-scale linear models are ubiquitous throughout machine learning, with contemporary application as surrogate models for neural network uncertainty quantification; that is, the linearised Laplace method. Alas, the computational cost associated with Bayesian linear models constrains this method's application to small networks, small output spaces and small datasets. We address this limitation by introducing a scalable sample-based Bayesian inference method for conjugate Gaussian multi-output linear models, together with a matching method for hyperparameter (regularisation) selection. Furthermore, we use a classic feature normalisation method (the g-prior) to resolve a previously highlighted pathology of the linearised Laplace method. Together, these contributions allow us to perform linearised neural network inference with ResNet-18 on CIFAR100 (11M parameters, 100 output dimensions \u00d7 50k datapoints) and with a U-Net on a high-resolution tomographic reconstruction task (2M parameters, 251k output dimensions)."}}
{"id": "220Z65e1uc3", "cdate": 1663840876341, "mdate": 1663840876341, "content": {"title": "Adapting the Linearised Laplace Model Evidence for Modern Deep Learning", "abstract": "The linearised Laplace method for estimating model uncertainty has received renewed attention in the Bayesian deep learning community. The method provides reliable error bars and admits a closed form expression for the model evidence, allowing for scalable selection of model hyperparameters. In this work, we examine the assumptions behind this method, particularly in conjunction with model selection. We show that these interact poorly with some now-standard tools of deep learning\u2014stochastic approximation methods and normalisation layers\u2014and make recommendations for how to better adapt this classic method to the modern setting. We provide theoretical support for our recommendations and validate them empirically on MLPs, classic CNNs, residual networks with and without normalisation\nlayers, generative autoencoders and transformers."}}
{"id": "rGGnV4QtZXh", "cdate": 1640995200000, "mdate": 1681880806747, "content": {"title": "Adapting the Linearised Laplace Model Evidence for Modern Deep Learning", "abstract": "The linearised Laplace method for estimating model uncertainty has received renewed attention in the Bayesian deep learning community. The method provides reliable error bars and admits a closed-form expression for the model evidence, allowing for scalable selection of model hyperparameters. In this work, we examine the assumptions behind this method, particularly in conjunction with model selection. We show that these interact poorly with some now-standard tools of deep learning--stochastic approximation methods and normalisation layers--and make recommendations for how to better adapt this classic method to the modern setting. We provide theoretical support for our recommendations and validate them empirically on MLPs, classic CNNs, residual networks with and without normalisation layers, generative autoencoders and transformers."}}
{"id": "ftJpN02eV_", "cdate": 1640995200000, "mdate": 1681880806563, "content": {"title": "Adapting the Linearised Laplace Model Evidence for Modern Deep Learning", "abstract": "The linearised Laplace method for estimating model uncertainty has received renewed attention in the Bayesian deep learning community. The method provides reliable error bars and admits a closed-fo..."}}
{"id": "7qyVXSbkgqp", "cdate": 1640995200000, "mdate": 1681880806759, "content": {"title": "Sampling-based inference for large linear models, with application to linearised Laplace", "abstract": "Large-scale linear models are ubiquitous throughout machine learning, with contemporary application as surrogate models for neural network uncertainty quantification; that is, the linearised Laplace method. Alas, the computational cost associated with Bayesian linear models constrains this method's application to small networks, small output spaces and small datasets. We address this limitation by introducing a scalable sample-based Bayesian inference method for conjugate Gaussian multi-output linear models, together with a matching method for hyperparameter (regularisation) selection. Furthermore, we use a classic feature normalisation method (the g-prior) to resolve a previously highlighted pathology of the linearised Laplace method. Together, these contributions allow us to perform linearised neural network inference with ResNet-18 on CIFAR100 (11M parameters, 100 outputs x 50k datapoints), with ResNet-50 on Imagenet (50M parameters, 1000 outputs x 1.2M datapoints) and with a U-Net on a high-resolution tomographic reconstruction task (2M parameters, 251k output~dimensions)."}}
{"id": "uUH8x-h9zdB", "cdate": 1637576010106, "mdate": null, "content": {"title": "Linearised Laplace Inference in Networks with Normalisation Layers and the Neural g-Prior", "abstract": "We show that for neural networks (NN) with normalisation layers, i.e. batch norm, layer norm, or group norm, the Laplace model evidence does not approximate the volume of a posterior mode and is thus unsuitable for model selection. We instead propose to use the Laplace evidence of the linearized network, which is robust to the presence of these layers. We also identify heterogeneity in the scale of Jacobian entries corresponding to different weights. We ameliorate this issue by extending the scale-invariant g-prior to NNs. We demonstrate these methods on toy regression, and image classification with a CNN."}}
{"id": "UtNbGRwWeXM", "cdate": 1599591008700, "mdate": null, "content": {"title": "Successor Uncertainties: exploration and uncertainty in temporal difference learning", "abstract": "Posterior sampling for reinforcement learning (PSRL) is an effective method for balancing exploration and exploitation in reinforcement learning. Randomised value functions (RVF) can be viewed as a promising approach to scaling PSRL. However, we show that most contemporary algorithms combining RVF with neural network function approximation do not possess the properties which make PSRL effective, and provably fail in sparse reward problems. Moreover, we find that propagation of uncertainty, a property of PSRL previously thought important for exploration, does not preclude this failure. We use these insights to design Successor Uncertainties (SU), a cheap and easy to implement RVF algorithm that retains key properties of PSRL. SU is highly effective on hard tabular exploration benchmarks. Furthermore, on the Atari 2600 domain, it surpasses human performance on 38 of 49 games tested (achieving a median human normalised score of 2.09), and outperforms its closest RVF competitor, Bootstrapped DQN, on 36 of those."}}
{"id": "q3MDFa4NpP9", "cdate": 1577836800000, "mdate": null, "content": {"title": "Bandit optimisation of functions in the Mat\u00e9rn kernel RKHS", "abstract": "We consider the problem of optimising functions in the reproducing kernel Hilbert space (RKHS) of a Mat\\'ern kernel with smoothness parameter $\\nu$ over the domain $[0,1]^d$ under noisy bandit feedback. Our contribution, the $\\pi$-GP-UCB algorithm, is the first practical approach with guaranteed sublinear regret for all $\\nu>1$ and $d \\geq 1$. Empirical validation suggests better performance and drastically improved computational scalablity compared with its predecessor, Improved GP-UCB."}}
{"id": "HBHWZ0sVHxc", "cdate": 1577836800000, "mdate": 1645722569383, "content": {"title": "Bandit optimisation of functions in the Mat\u00e9rn kernel RKHS", "abstract": "We consider the problem of optimising functions in the reproducing kernel Hilbert space (RKHS) of a Mat\u00e9rn kernel with smoothness parameter $u$ over the domain $[0,1]^d$ under noisy bandit feedback..."}}
