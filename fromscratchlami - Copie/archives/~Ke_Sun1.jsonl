{"id": "rxrLt7rTlAr", "cdate": 1652737495520, "mdate": null, "content": {"title": "Fair Wrapping for Black-box Predictions", "abstract": "We introduce a new family of techniques to post-process (``wrap\") a black-box classifier in order to reduce its bias. Our technique builds on the recent analysis of improper loss functions whose optimization can correct any twist in prediction, unfairness being treated as a twist. In the post-processing, we learn a wrapper function which we define as an $\\alpha$-tree, which modifies the prediction. We provide two generic boosting algorithms to learn $\\alpha$-trees. We show that our modification has appealing properties in terms of composition of $\\alpha$-trees, generalization, interpretability, and KL divergence between modified and original predictions. We exemplify the use of our technique in three fairness notions: conditional value-at-risk, equality of opportunity, and statistical parity; and provide experiments on several readily available datasets."}}
{"id": "dKq2xanSAur", "cdate": 1640609270030, "mdate": 1640609270030, "content": {"title": "Unsupervised skeleton learning for manifold denoising", "abstract": "The representative samples can be pictured as the skeleton of a point cloud. We learn a discrete distribution defined over all samples, so that these skeleton points have large probabilities and the outliers have probabilities close to zero. The basic assumption is that any observation is generated from a nearby skeleton point. The learning objective is to minimize the communication cost from a random sample to its generation source. Experiments show that the learned distribution highlights a compact size of key positions. It is further applied to a denoising task as an indirect method of evaluation. The clustering structures of image datasets are best preserved among several methods investigated."}}
{"id": "JFudOSx9ieN", "cdate": 1640609058729, "mdate": 1640609058729, "content": {"title": "An Information Geometry of Statistical Manifold Learning", "abstract": "Manifold learning seeks low-dimensional repre-\nsentations of high-dimensional data. The main\ntactics have been exploring the geometry in\nan input data space and an output embedding\nspace. We develop a manifold learning the-\nory in a hypothesis space consisting of models.\nA model means a specific instance of a collec-\ntion of points, e.g., the input data collectively\nor the output embedding collectively. The semi-\nRiemannian metric of this hypothesis space is\nuniquely derived in closed form based on the in-\nformation geometry of probability distributions.\nThere, manifold learning is interpreted as a tra-\njectory of intermediate models. The volume of a\ncontinuous region reveals an amount of informa-\ntion. It can be measured to define model com-\nplexity and embedding quality. This provides\ndeep unified perspectives of manifold learning\ntheory."}}
{"id": "kF12dRKDkVJ", "cdate": 1640608962078, "mdate": 1640608962078, "content": {"title": "Space-Time Local Embeddings", "abstract": "Space-time is a profound concept in physics. This concept was shown to be\nuseful for dimensionality reduction. We present basic definitions with interest-\ning counter-intuitions. We give theoretical propositions to show that space-time\nis a more powerful representation than Euclidean space. We apply this concept\nto manifold learning for preserving local information. Empirical results on non-\nmetric datasets show that more information can be preserved in space-time"}}
{"id": "NiM9Q7Z95z", "cdate": 1621630124312, "mdate": null, "content": {"title": "Secure Quantized Training for Deep Learning", "abstract": "We have implemented training of neural networks in secure multi-party\ncomputation (MPC) using quantization commonly used in said setting. To\nthe best of our knowledge, we are the first to present training of\nMNIST purely implemented in MPC that comes within one percent of\naccuracy of training using plaintext computation.  We found that\ntraining with MPC is possible, but it takes more epochs and achieves a\nlower accuracy than the usual CPU/GPU computation.  More concretely,\nwe have trained a network with two convolution and two dense layers to\n98.5% accuracy in 150 epochs. This took a day in our MPC\nimplementation.\n"}}
{"id": "XGSQfOVxVp4", "cdate": 1621630089396, "mdate": null, "content": {"title": "On the Variance of the Fisher Information for Deep Learning", "abstract": "In the realm of deep learning, the Fisher information matrix (FIM) gives novel insights and useful tools to characterize the loss landscape, perform second-order optimization, and build geometric learning theories. The exact FIM is either unavailable in closed form or too expensive to compute. In practice, it is almost always estimated based on empirical samples.  We investigate two such estimators based on two equivalent representations of the FIM --- both unbiased and consistent. Their estimation quality is naturally gauged by their variance given in closed form. We analyze how the parametric structure of a deep neural network can affect the variance. The meaning of this variance measure and its upper bounds are then discussed in the context of deep learning."}}
{"id": "iLn-bhP-kKH", "cdate": 1621629769165, "mdate": null, "content": {"title": "Contrastive Laplacian Eigenmaps", "abstract": "Graph contrastive learning attracts/disperses node representations for similar/dissimilar node pairs under some notion of similarity. It may be combined with a low-dimensional embedding of nodes to preserve intrinsic and structural properties of a graph. In this paper, we extend the celebrated Laplacian Eigenmaps with contrastive learning, and call them COntrastive Laplacian EigenmapS (COLES). Starting from a GAN-inspired contrastive formulation, we show that the Jensen-Shannon divergence underlying many contrastive graph embedding models fails under disjoint positive and negative distributions, which may naturally emerge during sampling in the contrastive setting. In contrast, we demonstrate analytically that COLES essentially minimizes a surrogate of Wasserstein distance, which is known to cope well under disjoint distributions. Moreover, we show that the loss of COLES belongs to the family of so-called block-contrastive losses, previously shown to be superior compared to pair-wise losses typically used by contrastive methods. We show on popular benchmarks/backbones that COLES offers favourable accuracy/scalability compared to DeepWalk, GCN, Graph2Gauss, DGI and GRACE baselines."}}
{"id": "HyeNkD-KDS", "cdate": 1569425116005, "mdate": null, "content": {"title": "Fisher-Bures Adversary Graph Convolutional Networks", "abstract": "In a graph convolutional network, we assume that the graph G is generated wrt some observation noise. During learning, we make small random perturbations \u0394G of the graph and try to improve generalization. Based on quantum information geometry, \u0394G can be characterized by the eigendecomposition of the graph Laplacian matrix. We try to minimize the loss wrt the perturbed G+\u0394G while making \u0394G to be effective in terms of the Fisher information of the neural network. Our proposed model can consistently improve graph convolutional networks on semi-supervised node classification tasks with reasonable computational overhead. We present three different geometries on the manifold of graphs: the intrinsic geometry measures the information theoretic dynamics of a graph; the extrinsic geometry characterizes how such dynamics can affect externally a graph neural network; the embedding geometry is for measuring node embeddings. These new analytical tools are useful in developing a good understanding of graph neural networks and fostering new techniques. "}}
{"id": "r1xkIjA9tX", "cdate": 1538087751097, "mdate": null, "content": {"title": "q-Neurons: Neuron Activations based on Stochastic Jackson's Derivative Operators", "abstract": "We propose a new generic type of stochastic neurons, called $q$-neurons, that considers activation functions based on Jackson's $q$-derivatives, with stochastic parameters $q$. Our generalization of neural network architectures with $q$-neurons is shown to be both scalable and very easy to implement. We demonstrate experimentally consistently improved performances over state-of-the-art standard activation functions, both on training and testing loss functions.\n"}}
{"id": "S1WdAuW_-r", "cdate": 1514764800000, "mdate": null, "content": {"title": "Representation Learning of Compositional Data", "abstract": "We consider the problem of learning a low dimensional representation for compositional data. Compositional data consists of a collection of nonnegative data that sum to a constant value. Since the parts of the collection are statistically dependent, many standard tools cannot be directly applied. Instead, compositional data must be first transformed before analysis. Focusing on principal component analysis (PCA), we propose an approach that allows low dimensional representation learning directly from the original data. Our approach combines the benefits of the log-ratio transformation from compositional data analysis and exponential family PCA. A key tool in its derivation is a generalization of the scaled Bregman theorem, that relates the perspective transform of a Bregman divergence to the Bregman divergence of a perspective transform and a remainder conformal divergence. Our proposed approach includes a convenient surrogate (upper bound) loss of the exponential family PCA which has an easy to optimize form. We also derive the corresponding form for nonlinear autoencoders. Experiments on simulated data and microbiome data show the promise of our method."}}
