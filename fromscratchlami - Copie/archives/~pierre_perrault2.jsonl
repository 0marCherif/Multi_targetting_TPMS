{"id": "RQ8X_iK3HT5", "cdate": 1652737821523, "mdate": null, "content": {"title": "When Combinatorial Thompson Sampling meets Approximation Regret", "abstract": "We study the Combinatorial Thompson Sampling policy (CTS) for combinatorial multi-armed bandit problems (CMAB), within an approximation regret setting. Although CTS has attracted a lot of interest, it has a drawback that other usual CMAB policies do not have when considering non-exact oracles: for some oracles, CTS has a poor approximation regret (scaling linearly with the time horizon $T$) [Wang and Chen, 2018]. A study is then necessary to discriminate the oracles on which CTS could learn. This study was started by Kong et al. [2021]: they gave the first approximation regret analysis of CTS for the greedy oracle, obtaining an upper bound of order $\\mathcal{O}{\\left(\\log(T)/\\Delta^2\\right)}$, where $\\Delta$ is some minimal reward gap. In this paper, our objective is to push this study further than the simple case of the greedy oracle. We provide the first $\\mathcal{O}{\\left(\\log(T)/\\Delta\\right)}$ approximation regret upper bound for CTS, obtained under a specific condition on the approximation oracle, allowing a reduction to the exact oracle analysis. We thus term this condition Reduce2Exact, and observe that it is satisfied in many concrete examples. Moreover, it can be extended to the probabilistically triggered arms setting, thus capturing even more problems, such as online influence maximization."}}
{"id": "SyEYyjWd-H", "cdate": 1546300800000, "mdate": null, "content": {"title": "Exploiting structure of uncertainty for efficient matroid semi-bandits", "abstract": "We improve the efficiency of algorithms for stochastic combinatorial semi-bandits. In most interesting problems, state-of-the-art algorithms take advantage of structural properties of rewards, such..."}}
