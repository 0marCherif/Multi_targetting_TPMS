{"id": "vTg_6yjfv5y", "cdate": 1672531200000, "mdate": 1681649715576, "content": {"title": "CHAMPAGNE: Learning Real-world Conversation from Large-Scale Web Videos", "abstract": ""}}
{"id": "liJmfY0FaDo", "cdate": 1640995200000, "mdate": 1672802805522, "content": {"title": "Meet Your Favorite Character: Open-domain Chatbot Mimicking Fictional Characters with only a Few Utterances", "abstract": ""}}
{"id": "9dl7D0NMQR", "cdate": 1640995200000, "mdate": 1672802805541, "content": {"title": "Understanding and Improving the Exemplar-based Generation for Open-domain Conversation", "abstract": ""}}
{"id": "28g3LoAU0vm", "cdate": 1640995200000, "mdate": 1681649715583, "content": {"title": "Measuring and Improving Semantic Diversity of Dialogue Generation", "abstract": ""}}
{"id": "oiyLgez1f6o", "cdate": 1609459200000, "mdate": 1632669312394, "content": {"title": "Disentangling Label Distribution for Long-Tailed Visual Recognition", "abstract": "The current evaluation protocol of long-tailed visual recognition trains the classification model on the long-tailed source label distribution and evaluates its performance on the uniform target label distribution. Such protocol has questionable practicality since the target may also be long-tailed. Therefore, we formulate long-tailed visual recognition as a label shift problem where the target and source label distributions are different. One of the significant hurdles in dealing with the label shift problem is the entanglement between the source label distribution and the model prediction. In this paper, we focus on disentangling the source label distribution from the model prediction. We first introduce a simple but overlooked baseline method that matches the target label distribution by post-processing the model prediction trained by the cross-entropy loss and the Softmax function. Although this method surpasses state-of-the-art methods on benchmark datasets, it can be further improved by directly disentangling the source label distribution from the model prediction in the training phase. Thus, we propose a novel method, LAbel distribution DisEntangling (LADE) loss based on the optimal bound of Donsker-Varadhan representation. LADE achieves state-of-the-art performance on benchmark datasets such as CIFAR-100-LT, Places-LT, ImageNet-LT, and iNaturalist 2018. Moreover, LADE outperforms existing methods on various shifted target label distributions, showing the general adaptability of our proposed method."}}
{"id": "On60lBZTOlmi", "cdate": 1609459200000, "mdate": 1637028292668, "content": {"title": "Distilling the Knowledge of Large-scale Generative Models into Retrieval Models for Efficient Open-domain Conversation", "abstract": "Beomsu Kim, Seokjun Seo, Seungju Han, Enkhbayar Erdenee, Buru Chang. Findings of the Association for Computational Linguistics: EMNLP 2021. 2021."}}
{"id": "gK1c-E1eTrC", "cdate": 1577836800000, "mdate": 1637028292665, "content": {"title": "Attentron: Few-Shot Text-to-Speech Utilizing Attention-Based Variable-Length Embedding", "abstract": "On account of growing demands for personalization, the need for a so-called few-shot TTS system that clones speakers with only a few data is emerging. To address this issue, we propose Attentron, a few-shot TTS model that clones voices of speakers unseen during training. It introduces two special encoders, each serving different purposes. A fine-grained encoder extracts variable-length style information via an attention mechanism, and a coarse-grained encoder greatly stabilizes the speech synthesis, circumventing unintelligible gibberish even for synthesizing speech of unseen speakers. In addition, the model can scale out to an arbitrary number of reference audios to improve the quality of the synthesized speech. According to our experiments, including a human evaluation, the proposed model significantly outperforms state-of-the-art models when generating speech for unseen speakers in terms of speaker similarity and quality."}}
