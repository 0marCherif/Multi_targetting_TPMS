{"id": "jVRkuPEJinN", "cdate": 1672531200000, "mdate": 1680963694125, "content": {"title": "A Unified BEV Model for Joint Learning of 3D Local Features and Overlap Estimation", "abstract": ""}}
{"id": "FqRjz8ATWkM", "cdate": 1667455746982, "mdate": 1667455746982, "content": {"title": "ACDet: Attentive Cross-view Fusion for LiDAR-based 3D Object Detection", "abstract": "Recent works on 3D object detection take the range image\nas input, which have achieved comparable performance\nwith bird\u2019s eye view (BEV) based methods. Compared to\nBEV, range view provides dense and compact observations\nwhich allows for more popular feature encoders. To leverage\ncomplementary information of range view and BEV,\nwe present ACDet - a novel single-stage multi-view fusion\nmethod. Rather than fusing point-level features from range\nview and BEV at early stage, the key contribution is that we\nintroduce an attentive cross-view fusion module based on\ntransformer to fuse higher level features, and further adopt\na supervised foreground mask learned from BEV features to\nenhance the fused features. Notably, a geometric-attention\nkernel is proposed to enhance features extracted from range\nimage. Finally, we design an anchor-free detection head\nwith optimized label assignment strategy, and its performance\nexceeds the existing anchor-based and anchor-free\n3D detection heads by a large margin. We evaluate our\nACDet model extensively on the KITTI dataset and Waymo\nOpen Dataset (WOD). ACDet outperforms most of singlestage\nmodels on KITTI dataset in terms of multi-class 3D\nand BEV mean average precision. ACDet also outperforms\nboth range-view and multi-view fusion methods on WOD."}}
{"id": "FdxF_InA-v", "cdate": 1640995200000, "mdate": 1680963694118, "content": {"title": "ACDet: Attentive Cross-view Fusion for LiDAR-based 3D Object Detection", "abstract": ""}}
{"id": "quir9BFJnQ", "cdate": 1577836800000, "mdate": 1668073805874, "content": {"title": "LiDAR Inertial Odometry Aided Robust LiDAR Localization System in Changing City Scenes", "abstract": "Environmental fluctuations pose crucial challenges to a localization system in autonomous driving. We present a robust LiDAR localization system that maintains its kinematic estimation in changing urban scenarios by using a dead reckoning solution implemented through a LiDAR inertial odometry. Our localization framework jointly uses information from complementary modalities such as global matching and LiDAR inertial odometry to achieve accurate and smooth localization estimation. To improve the performance of the LiDAR odometry, we incorporate inertial and LiDAR intensity cues into an occupancy grid based LiDAR odometry to enhance frame-to-frame motion and matching estimation. Multi-resolution occupancy grid is implemented yielding a coarse-to-fine approach to balance the odometry's precision and computational requirement. To fuse both the odometry and global matching results, we formulate a MAP estimation problem in a pose graph fusion framework that can be efficiently solved. An effective environmental change detection method is proposed that allows us to know exactly when and what portion of the map requires an update. We comprehensively validate the effectiveness of the proposed approaches using both the Apollo-SouthBay dataset and our internal dataset. The results confirm that our efforts lead to a more robust and accurate localization system, especially in dynamically changing urban scenarios."}}
{"id": "jQRmNyzCiq", "cdate": 1577836800000, "mdate": 1668073805852, "content": {"title": "DA4AD: End-to-end Deep Attention Aware Features Aided Visual Localization for Autonomous Driving", "abstract": "We present a visual localization framework based on novel deep attention aware features for autonomous driving that achieves centimeter level localization accuracy. Conventional approaches to the visual localization problem rely on handcrafted features or human-made objects on the road. They are known to be either prone to unstable matching caused by severe appearance or lighting changes, or too scarce to deliver constant and robust localization results in challenging scenarios. In this work, we seek to exploit the deep attention mechanism to search for salient, distinctive and stable features that are good for long-term matching in the scene through a novel end-to-end deep neural network. Furthermore, our learned feature descriptors are demonstrated to be competent to establish robust matches and therefore successfully estimate the optimal camera poses with high precision. We comprehensively validate the effectiveness of our method using a freshly collected dataset with high-quality ground truth trajectories and hardware synchronization between sensors. Results demonstrate that our method achieves a competitive localization accuracy when compared to the LiDAR-based localization solutions under various challenging circumstances, leading to a potential low-cost localization solution for autonomous driving."}}
{"id": "284Fy1F20F", "cdate": 1577836800000, "mdate": 1668073805852, "content": {"title": "DA4AD: End-to-End Deep Attention-Based Visual Localization for Autonomous Driving", "abstract": "We present a visual localization framework based on novel deep attention aware features for autonomous driving that achieves centimeter level localization accuracy. Conventional approaches to the visual localization problem rely on handcrafted features or human-made objects on the road. They are known to be either prone to unstable matching caused by severe appearance or lighting changes, or too scarce to deliver constant and robust localization results in challenging scenarios. In this work, we seek to exploit the deep attention mechanism to search for salient, distinctive and stable features that are good for long-term matching in the scene through a novel end-to-end deep neural network. Furthermore, our learned feature descriptors are demonstrated to be competent to establish robust matches and therefore successfully estimate the optimal camera poses with high precision. We comprehensively validate the effectiveness of our method using a freshly collected dataset with high-quality ground truth trajectories and hardware synchronization between sensors. Results demonstrate that our method achieves a competitive localization accuracy when compared to the LiDAR-based localization solutions under various challenging circumstances, leading to a potential low-cost localization solution for autonomous driving."}}
{"id": "wI9JKyPjoZ", "cdate": 1546300800000, "mdate": 1668073805854, "content": {"title": "DeepICP: An End-to-End Deep Neural Network for 3D Point Cloud Registration", "abstract": "We present DeepICP - a novel end-to-end learning-based 3D point cloud registration framework that achieves comparable registration accuracy to prior state-of-the-art geometric methods. Different from other keypoint based methods where a RANSAC procedure is usually needed, we implement the use of various deep neural network structures to establish an end-to-end trainable network. Our keypoint detector is trained through this end-to-end structure and enables the system to avoid the inference of dynamic objects, leverages the help of sufficiently salient features on stationary objects, and as a result, achieves high robustness. Rather than searching the corresponding points among existing points, the key contribution is that we innovatively generate them based on learned matching probabilities among a group of candidates, which can boost the registration accuracy. Our loss function incorporates both the local similarity and the global geometric constraints to ensure all above network designs can converge towards the right direction. We comprehensively validate the effectiveness of our approach using both the KITTI dataset and the Apollo-SouthBay dataset. Results demonstrate that our method achieves comparable or better performance than the state-of-the-art geometry-based methods. Detailed ablation and visualization analysis are included to further illustrate the behavior and insights of our network. The low registration error and high robustness of our method makes it attractive for substantial applications relying on the point cloud registration task."}}
{"id": "O140DOjngT", "cdate": 1546300800000, "mdate": 1668073805868, "content": {"title": "DeepVCP: An End-to-End Deep Neural Network for Point Cloud Registration", "abstract": "We present DeepVCP - a novel end-to-end learning-based 3D point cloud registration framework that achieves comparable registration accuracy to prior state-of-the-art geometric methods. Different from other keypoint based methods where a RANSAC procedure is usually needed, we implement the use of various deep neural network structures to establish an end-to-end trainable network. Our keypoint detector is trained through this end-to-end structure and enables the system to avoid the interference of dynamic objects, leverages the help of sufficiently salient features on stationary objects, and as a result, achieves high robustness. Rather than searching the corresponding points among existing points, the key contribution is that we innovatively generate them based on learned matching probabilities among a group of candidates, which can boost the registration accuracy. We comprehensively validate the effectiveness of our approach using both the KITTI dataset and the Apollo-SouthBay dataset. Results demonstrate that our method achieves comparable registration accuracy and runtime efficiency to the state-of-the-art geometry-based methods, but with higher robustness to inaccurate initial poses. Detailed ablation and visualization analysis are included to further illustrate the behavior and insights of our network. The low registration error and high robustness of our method make it attractive to the substantial applications relying on the point cloud registration task."}}
{"id": "B7N3GQl_6r", "cdate": 1546300800000, "mdate": null, "content": {"title": "L3-Net: Towards Learning Based LiDAR Localization for Autonomous Driving.", "abstract": "We present L3-Net - a novel learning-based LiDAR localization system that achieves centimeter-level localization accuracy, comparable to prior state-of-the-art systems with hand-crafted pipelines. Rather than relying on these hand-crafted modules, we innovatively implement the use of various deep neural network structures to establish a learning-based approach. L3-Net learns local descriptors specifically optimized for matching in different real-world driving scenarios. 3D convolutions over a cost volume built in the solution space significantly boosts the localization accuracy. RNNs are demonstrated to be effective in modeling the vehicle's dynamics, yielding better temporal smoothness and accuracy. We comprehensively validate the effectiveness of our approach using freshly collected datasets. Multiple trials of repetitive data collection over the same road and areas make our dataset ideal for testing localization systems. The SunnyvaleBigLoop sequences, with a year's time interval between the collected mapping and testing data, made it quite challenging, but the low localization error of our method in these datasets demonstrates its maturity for real industrial implementation."}}
{"id": "3yX-oaYy9nd", "cdate": 1546300800000, "mdate": 1668073805851, "content": {"title": "L3-Net: Towards Learning Based LiDAR Localization for Autonomous Driving", "abstract": "We present L3-Net - a novel learning-based LiDAR localization system that achieves centimeter-level localization accuracy, comparable to prior state-of-the-art systems with hand-crafted pipelines. Rather than relying on these hand-crafted modules, we innovatively implement the use of various deep neural network structures to establish a learning-based approach. L3-Net learns local descriptors specifically optimized for matching in different real-world driving scenarios. 3D convolutions over a cost volume built in the solution space significantly boosts the localization accuracy. RNNs are demonstrated to be effective in modeling the vehicle's dynamics, yielding better temporal smoothness and accuracy. We comprehensively validate the effectiveness of our approach using freshly collected datasets. Multiple trials of repetitive data collection over the same road and areas make our dataset ideal for testing localization systems. The SunnyvaleBigLoop sequences, with a year's time interval between the collected mapping and testing data, made it quite challenging, but the low localization error of our method in these datasets demonstrates its maturity for real industrial implementation."}}
