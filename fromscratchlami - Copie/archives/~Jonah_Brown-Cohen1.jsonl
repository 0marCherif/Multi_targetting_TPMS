{"id": "6nC7UNRe7K", "cdate": 1684308596630, "mdate": 1684308596630, "content": {"title": "The Matching Problem Has No Small Symmetric SDP.", "abstract": "Any symmetric SDP relaxation of the matching problem has exponential size."}}
{"id": "QL85H5Mkip", "cdate": 1663850325535, "mdate": null, "content": {"title": "Uncovering Directions of Instability via Quadratic Approximation of Deep Neural Loss in Reinforcement Learning", "abstract": "Learning in MDPs with highly complex state representations is currently possible due to multiple advancements in reinforcement learning algorithm design. However, this incline in complexity, and furthermore the increase in the dimensions of the observation came at the cost of non-robustness that can be taken advantage of (i.e. moving along worst-case directions in the observation space). To solve this policy instability problem we propose a novel method to ascertain the presence of these non-robust directions via quadratic approximation of the deep neural policy loss. Our method provides a theoretical basis for the fundamental cut-off between stable observations and non-robust observations.  Furthermore, our technique is computationally efficient, and does not depend on the methods used to produce the worst-case directions. We conduct extensive experiments in the Arcade Learning Environment with several different non-robust alteration techniques. Most significantly, we demonstrate the effectiveness of our approach even in the setting where alterations are explicitly optimized to circumvent our proposed method."}}
{"id": "f7cWROZYSU", "cdate": 1632875753620, "mdate": null, "content": {"title": "Detecting Worst-case Corruptions via Loss Landscape Curvature in Deep Reinforcement Learning", "abstract": "The non-robustness of neural network policies to adversarial examples poses a challenge for deep reinforcement learning. One natural approach to mitigate the impact of adversarial examples is to develop methods to detect when a given input is adversarial. In this work we introduce a novel approach for detecting adversarial examples that is computationally efficient, is agnostic to the method used to generate adversarial examples, and theoretically well-motivated. Our method is based on a measure of the local curvature of the neural network policy, which we show differs between adversarial and clean examples. We empirically demonstrate the effectiveness of our method in the Atari environment against a large set of state-of-the-art algorithms for generating adversarial examples. Furthermore, we exhibit the effectiveness of our detection algorithm with the presence of multiple strong detection-aware adversaries. "}}
{"id": "U5Af9S_RcI0", "cdate": 1621630289708, "mdate": null, "content": {"title": "Faster Algorithms and Constant Lower Bounds for the Worst-Case Expected Error", "abstract": "The study of statistical estimation without distributional assumptions on data values, but with knowledge of data collection methods was recently introduced by Chen, Valiant and Valiant (NeurIPS 2020). In this framework, the goal is to design estimators that minimize the worst-case expected error. Here the expectation is over a known, randomized data collection process from some population, and the data values corresponding to each element of the population are assumed to be worst-case. Chen, Valiant and Valiant show that, when data values are $\\ell_{\\infty}$-normalized, there is a polynomial time algorithm to compute an estimator for the mean with worst-case expected error that is within a factor $\\frac{\\pi}{2}$ of the optimum within the natural class of semilinear estimators. However, this algorithm is based on optimizing a somewhat complex concave objective function over a constrained set of positive semidefinite matrices, and thus does not come with explicit runtime guarantees beyond being polynomial time in the input.\n\nIn this paper we design provably efficient algorithms for approximating the optimal semilinear estimator based on online convex optimization. In the setting where data values are $\\ell_{\\infty}$-normalized, our algorithm achieves a $\\frac{\\pi}{2}$-approximation by iteratively solving a sequence of standard SDPs. When data values are $\\ell_2$-normalized, our algorithm iteratively computes the top eigenvector of a sequence of matrices, and does not lose any multiplicative approximation factor. Further, using experiments in settings where sample membership is correlated with data values (e.g. \"importance sampling\" and \"snowball sampling\"), we show that our $\\ell_2$-normalized algorithm gives a similar advantage over standard estimators as the original $\\ell_{\\infty}$-normalized algorithm of Chen, Valiant and Valiant, but with much lower computational complexity. We complement these positive results by stating a simple combinatorial condition which, if satisfied by a data collection process, implies that any (not necessarily semilinear) estimator for the mean has constant worst-case expected error."}}
{"id": "xp1JOiurVf", "cdate": 1609459200000, "mdate": 1683995786612, "content": {"title": "Faster Algorithms and Constant Lower Bounds for the Worst-Case Expected Error", "abstract": "The study of statistical estimation without distributional assumptions on data values, but with knowledge of data collection methods was recently introduced by Chen, Valiant and Valiant (NeurIPS 2020). In this framework, the goal is to design estimators that minimize the worst-case expected error. Here the expectation is over a known, randomized data collection process from some population, and the data values corresponding to each element of the population are assumed to be worst-case. Chen, Valiant and Valiant show that, when data values are $\\ell_{\\infty}$-normalized, there is a polynomial time algorithm to compute an estimator for the mean with worst-case expected error that is within a factor $\\frac{\\pi}{2}$ of the optimum within the natural class of semilinear estimators. However, their algorithm is based on optimizing a somewhat complex concave objective function over a constrained set of positive semidefinite matrices, and thus does not come with explicit runtime guarantees beyond being polynomial time in the input. In this paper we design provably efficient algorithms for approximating the optimal semilinear estimator based on online convex optimization. In the setting where data values are $\\ell_{\\infty}$-normalized, our algorithm achieves a $\\frac{\\pi}{2}$-approximation by iteratively solving a sequence of standard SDPs. When data values are $\\ell_2$-normalized, our algorithm iteratively computes the top eigenvector of a sequence of matrices, and does not lose any multiplicative approximation factor. We complement these positive results by stating a simple combinatorial condition which, if satisfied by a data collection process, implies that any (not necessarily semilinear) estimator for the mean has constant worst-case expected error."}}
{"id": "qViS_JPyzR3", "cdate": 1609459200000, "mdate": 1683995786694, "content": {"title": "Faster Algorithms and Constant Lower Bounds for the Worst-Case Expected Error", "abstract": "The study of statistical estimation without distributional assumptions on data values, but with knowledge of data collection methods was recently introduced by Chen, Valiant and Valiant (NeurIPS 2020). In this framework, the goal is to design estimators that minimize the worst-case expected error. Here the expectation is over a known, randomized data collection process from some population, and the data values corresponding to each element of the population are assumed to be worst-case. Chen, Valiant and Valiant show that, when data values are $\\ell_{\\infty}$-normalized, there is a polynomial time algorithm to compute an estimator for the mean with worst-case expected error that is within a factor $\\frac{\\pi}{2}$ of the optimum within the natural class of semilinear estimators. However, this algorithm is based on optimizing a somewhat complex concave objective function over a constrained set of positive semidefinite matrices, and thus does not come with explicit runtime guarantees beyond being polynomial time in the input.In this paper we design provably efficient algorithms for approximating the optimal semilinear estimator based on online convex optimization. In the setting where data values are $\\ell_{\\infty}$-normalized, our algorithm achieves a $\\frac{\\pi}{2}$-approximation by iteratively solving a sequence of standard SDPs. When data values are $\\ell_2$-normalized, our algorithm iteratively computes the top eigenvector of a sequence of matrices, and does not lose any multiplicative approximation factor. Further, using experiments in settings where sample membership is correlated with data values (e.g. \"importance sampling\" and \"snowball sampling\"), we show that our $\\ell_2$-normalized algorithm gives a similar advantage over standard estimators as the original $\\ell_{\\infty}$-normalized algorithm of Chen, Valiant and Valiant, but with much lower computational complexity. We complement these positive results by stating a simple combinatorial condition which, if satisfied by a data collection process, implies that any (not necessarily semilinear) estimator for the mean has constant worst-case expected error."}}
{"id": "cG1VESvIogB", "cdate": 1609459200000, "mdate": 1683995786719, "content": {"title": "Optimal Inapproximability with Universal Factor Graphs", "abstract": ""}}
{"id": "6lH64Nixy8", "cdate": 1577836800000, "mdate": 1683995786780, "content": {"title": "Extended Formulation Lower Bounds for Refuting Random CSPs", "abstract": ""}}
{"id": "pyRTfHTMpHR", "cdate": 1546300800000, "mdate": 1683995786776, "content": {"title": "Optimal Inapproximability with Universal Factor Graphs", "abstract": ""}}
{"id": "KGHWT_CyFqw", "cdate": 1546300800000, "mdate": null, "content": {"title": "Formal Barriers to Longest-Chain Proof-of-Stake Protocols", "abstract": "The security of most existing cryptocurrencies is based on a concept called Proof-of-Work, in which users must solve a computationally hard cryptopuzzle to authorize transactions (\"one unit of computation, one vote''). This leads to enormous expenditure on hardware and electricity in order to collect the rewards associated with transaction authorization. Proof-of-Stake is an alternative concept that instead selects users to authorize transactions proportional to their wealth (\"one coin, one vote\"). Some aspects of the two paradigms are the same. For instance, obtaining voting power in Proof-of-Stake has a monetary cost just as in Proof-of-Work: a coin cannot be freely duplicated any more easily than a unit of computation. However some aspects are fundamentally different. In particular, exactly because Proof-of-Stake is wasteless, there is no inherent resource cost to deviating (commonly referred to as the \"Nothing-at-Stake'' problem). In contrast to prior work, we focus on incentive-driven deviations (any participant will deviate if doing so yields higher revenue) instead of adversarial corruption (an adversary may take over a significant fraction of the network, but the remaining players follow the protocol). The main results of this paper are several formal barriers to designing incentive-compatible proof-of-stake cryptocurrencies (that don't apply to proof-of-work)."}}
