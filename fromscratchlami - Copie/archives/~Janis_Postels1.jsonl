{"id": "IyBlj_JCtwQ", "cdate": 1684250514641, "mdate": 1684250514641, "content": {"title": "SHIFT: A Synthetic Driving Dataset for Continuous Multi-Task Domain Adaptation", "abstract": "Adapting to a continuously evolving environment is a\nsafety-critical challenge inevitably faced by all autonomous\ndriving systems. Existing image and video driving datasets,\nhowever, fall short of capturing the mutable nature of the\nreal world. In this paper, we introduce the largest multitask synthetic dataset for autonomous driving, SHIFT. It\npresents discrete and continuous shifts in cloudiness, rain\nand fog intensity, time of day, and vehicle and pedestrian\ndensity. Featuring a comprehensive sensor suite and annotations for several mainstream perception tasks, SHIFT\nallows investigating the degradation of a perception system performance at increasing levels of domain shift, fostering the development of continuous adaptation strategies\nto mitigate this problem and assess model robustness and\ngenerality. Our dataset and benchmark toolkit are publicly\navailable at www.vis.xyz/shift."}}
{"id": "MosSgoAdSmQ", "cdate": 1668678400597, "mdate": 1668678400597, "content": {"title": "SHIFT: A Synthetic Driving Dataset for Continuous Multi-Task Domain Adaptation", "abstract": "Adapting to a continuously evolving environment is a safety-critical challenge inevitably faced by all autonomous-driving systems. Existing image- and video-based driving datasets, however, fall short of capturing the mutable nature of the real world. In this paper, we introduce the largest multi-task synthetic dataset for autonomous driving, SHIFT. It presents discrete and continuous shifts in cloudiness, rain and fog intensity, time of day, and vehicle and pedestrian density. Featuring a comprehensive sensor suite and annotations for several mainstream perception tasks, SHIFT allows to investigate how a perception systems' performance degrades at increasing levels of domain shift, fostering the development of continuous adaptation strategies to mitigate this problem and assessing the robustness and generality of a model. Our dataset and benchmark toolkit are publicly available at www.vis.xyz/shift."}}
{"id": "CMO3p8AuBo", "cdate": 1667376791003, "mdate": 1667376791003, "content": {"title": "Variational transformer networks for layout generation", "abstract": "Generative models able to synthesize layouts of different kinds (eg documents, user interfaces or furniture arrangements) are a useful tool to aid design processes and as a first step in the generation of synthetic data, among other tasks. We exploit the properties of self-attention layers to capture high level relationships between elements in a layout, and use these as the building blocks of the well-known Variational Autoencoder (VAE) formulation. Our proposed Variational Transformer Network (VTN) is capable of learning margins, alignments and other global design rules without explicit supervision. Layouts sampled from our model have a high degree of resemblance to the training data, while demonstrating appealing diversity. In an extensive evaluation on publicly available benchmarks for different layout types VTNs achieve state-of-the-art diversity and perceptual quality. Additionally, we show the capabilities of this method as part of a document layout detection pipeline.\n"}}
{"id": "Rx5cLoJavoD", "cdate": 1640995200000, "mdate": 1667335881038, "content": {"title": "ManiFlow: Implicitly Representing Manifolds with Normalizing Flows", "abstract": "Normalizing Flows (NFs) are flexible explicit generative models that have been shown to accurately model complex real-world data distributions. However, their invertibility constraint imposes limitations on data distributions that reside on lower dimensional manifolds embedded in higher dimensional space. Practically, this shortcoming is often bypassed by adding noise to the data which impacts the quality of the generated samples. In contrast to prior work, we approach this problem by generating samples from the original data distribution given full knowledge about the perturbed distribution and the noise model. To this end, we establish that NFs trained on perturbed data implicitly represent the manifold in regions of maximum likelihood. Then, we propose an optimization objective that recovers the most likely point on the manifold given a sample from the perturbed distribution. Finally, we focus on 3D point clouds for which we utilize the explicit nature of NFs, i.e. surface normals extracted from the gradient of the log-likelihood and the log-likelihood itself, to apply Poisson surface reconstruction to refine generated point sets."}}
{"id": "LI0IjmvYVkR", "cdate": 1640995200000, "mdate": 1667335880973, "content": {"title": "SHIFT: A Synthetic Driving Dataset for Continuous Multi-Task Domain Adaptation", "abstract": "Adapting to a continuously evolving environment is a safety-critical challenge inevitably faced by all autonomous driving systems. Existing image and video driving datasets, however, fall short of capturing the mutable nature of the real world. In this paper, we introduce the largest multi-task synthetic dataset for autonomous driving, SHIFT. It presents discrete and continuous shifts in cloudiness, rain and fog intensity, time of day, and vehicle and pedestrian density. Featuring a comprehensive sensor suite and annotations for several mainstream perception tasks, SHIFT allows investigating the degradation of a perception system performance at increasing levels of domain shift, fostering the development of continuous adaptation strategies to mitigate this problem and assess model robustness and generality. Our dataset and benchmark toolkit are publicly available at www.vis.xyz/shift."}}
{"id": "LH6imPcG_v", "cdate": 1640995200000, "mdate": 1667335880978, "content": {"title": "SHIFT: A Synthetic Driving Dataset for Continuous Multi-Task Domain Adaptation", "abstract": "Adapting to a continuously evolving environment is a safety-critical challenge inevitably faced by all autonomous-driving systems. Existing image- and video-based driving datasets, however, fall short of capturing the mutable nature of the real world. In this paper, we introduce the largest multi-task synthetic dataset for autonomous driving, SHIFT. It presents discrete and continuous shifts in cloudiness, rain and fog intensity, time of day, and vehicle and pedestrian density. Featuring a comprehensive sensor suite and annotations for several mainstream perception tasks, SHIFT allows to investigate how a perception systems' performance degrades at increasing levels of domain shift, fostering the development of continuous adaptation strategies to mitigate this problem and assessing the robustness and generality of a model. Our dataset and benchmark toolkit are publicly available at www.vis.xyz/shift."}}
{"id": "GPttV-njC9r", "cdate": 1640995200000, "mdate": 1668072450766, "content": {"title": "Implicit Neural Representations for Image Compression", "abstract": "Implicit Neural Representations (INRs) gained attention as a novel and effective representation for various data types. Recently, prior work applied INRs to image compressing. Such compression algorithms are promising candidates as a general purpose approach for any coordinate-based data modality. However, in order to live up to this promise current INR-based compression algorithms need to improve their rate-distortion performance by a large margin. This work progresses on this problem. First, we propose meta-learned initializations for INR-based compression which improves rate-distortion performance. As a side effect it also leads to leads to faster convergence speed. Secondly, we introduce a simple yet highly effective change to the network architecture compared to prior work on INR-based compression. Namely, we combine SIREN networks with positional encodings which improves rate distortion performance. Our contributions to source compression with INRs vastly outperform prior work. We show that our INR-based compression algorithm, meta-learning combined with SIREN and positional encodings, outperforms JPEG2000 and Rate-Distortion Autoencoders on Kodak with 2x reduced dimensionality for the first time and closes the gap on full resolution images. To underline the generality of INR-based source compression, we further perform experiments on 3D shape compression where our method greatly outperforms Draco - a traditional compression algorithm."}}
{"id": "2sBgysqkNX", "cdate": 1640995200000, "mdate": 1667335880974, "content": {"title": "On the Practicality of Deterministic Epistemic Uncertainty", "abstract": "A set of novel approaches for estimating epistemic uncertainty in deep neural networks with a single forward pass has recently emerged as a valid alternative to Bayesian Neural Networks. On the pre..."}}
{"id": "W3-hiLnUYl", "cdate": 1632875702070, "mdate": null, "content": {"title": "On the Practicality of Deterministic Epistemic Uncertainty", "abstract": "A set of novel approaches for estimating epistemic uncertainty in deep neural networks with a single forward pass has recently emerged as a valid alternative to Bayesian Neural Networks. On the premise of informative representations, these deterministic uncertainty methods (DUMs) achieve strong performance on detecting out-of-distribution (OOD) data while adding negligible computational costs at inference time. However, it remains unclear whether DUMs are well calibrated and can seamlessly scale to real-world applications - both prerequisites for their practical deployment. To this end, we first provide a taxonomy of DUMs and evaluate their calibration under continuous distributional shifts. Then, we extend them to semantic segmentation. We find that, while DUMs scale to realistic vision tasks and perform well on OOD detection, the practicality of current methods is undermined by poor calibration under distributional shifts."}}
{"id": "ZDD2TbZn7X1", "cdate": 1612721259714, "mdate": null, "content": {"title": "The OOD Blind Spot of Unsupervised Anomaly Detection", "abstract": "Deep unsupervised generative models are regarded as a promising alternative to supervised counterparts in the field of MRI-based lesion detection. They denote a principled approach for detecting unseen types of anomalies without relying on large amounts of expensive ground truth annotations. To this end, deep generative models are trained exclusively on data from healthy patients and detect lesions as  Out-of-Distribution (OOD) data at test time (i.e. low likelihood). While this is a promising way of bypassing the need for costly annotations, this work demonstrates that it also renders this widely used unsupervised anomaly detection approach particularly vulnerable to non-lesion-based OOD data (e.g. data from different sensors). Since models are likely to be exposed to such OOD data in production, it is crucial to employ safety mechanisms to filter for such samples and run inference only on input for which the model is able to provide reliable results. We first show extensively that conventional, unsupervised anomaly detection mechanisms fail when being presented with true OOD data. Secondly, we apply prior knowledge to disentangle lesion-based OOD from their non-lesion-based counterparts."}}
