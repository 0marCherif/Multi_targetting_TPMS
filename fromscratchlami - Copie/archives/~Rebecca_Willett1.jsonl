{"id": "Z-hsb_8Eym", "cdate": 1694766393654, "mdate": 1694766393654, "content": {"title": "Deep Stochastic Mechanics", "abstract": "This paper introduces a novel deep-learning-based approach for numerical simulation of a time- evolving Schr \u0308odinger equation inspired by stochastic mechanics and generative diffusion models. Unlike existing approaches, which exhibit computational complexity that scales exponentially in the problem dimension, our method allows us to adapt to the latent low-dimensional structure of the wave function by sampling from the Markovian diffusion. Depending on the latent dimension, our method may have far lower computational complexity in higher dimensions. Moreover, we propose novel equations for stochastic quantum mechanics, resulting in linear computational com- plexity with respect to the number of dimensions. Numerical simulations verify our theoretical findings and show a significant advantage of our method compared to other deep-learning-based approaches used for quantum mechanics."}}
{"id": "RQzTcjDF84w", "cdate": 1684029827587, "mdate": 1684029827587, "content": {"title": "Beyond Ensemble Averages: Leveraging Climate Model Ensembles for Subseasonal Forecasting", "abstract": "Producing high-quality forecasts of key climate variables such as temperature and precipitation on subseasonal time scales has long been a gap in operational forecasting. Recent studies have shown promising results using machine learning (ML) models to advance subseasonal forecasting (SSF), but several open questions remain. First, several past approaches use the average of an ensemble of physics-based forecasts as an input feature of these models. However, ensemble forecasts contain information that can aid prediction beyond only the ensemble mean. Second, past methods have focused on average performance, whereas forecasts of extreme events are far more important for planning and mitigation purposes. Third, climate forecasts correspond to a spatially-varying collection of forecasts, and different methods account for spatial variability in the response differently. Trade-offs between different approaches may be mitigated with model stacking. This paper describes the application of a variety of ML methods used to predict monthly average precipitation and two meter temperature using physics-based predictions (ensemble forecasts) and observational data such as relative humidity, pressure at sea level, or geopotential height, two weeks in advance for the whole continental United States. Regression, quantile regression, and tercile classification tasks using linear models, random forests, convolutional neural networks, and stacked models are considered. The proposed models outperform common baselines such as historical averages (or quantiles) and ensemble averages (or quantiles). This paper further includes an investigation of feature importance, trade-offs between using the full ensemble or only the ensemble average, and different modes of accounting for spatial variability."}}
{"id": "6scShPCpdDu", "cdate": 1652737586836, "mdate": null, "content": {"title": "Embed and Emulate: Learning to estimate parameters of dynamical systems with uncertainty quantification", "abstract": "This paper explores learning emulators for parameter estimation with uncertainty estimation of high-dimensional dynamical systems. We assume access to a computationally complex simulator that inputs a candidate parameter and outputs a corresponding multi-channel time series. Our task is to accurately estimate a range of likely values of the underlying parameters. Standard iterative approaches necessitate running the simulator many times, which is computationally prohibitive. This paper describes a novel framework for learning feature embeddings of observed dynamics jointly with an emulator that can replace high-cost simulators. Leveraging a contrastive learning approach, our method exploits intrinsic data properties within and across parameter and trajectory domains. On a coupled 396-dimensional multiscale Lorenz 96 system, our method significantly outperforms a typical parameter estimation method based on predefined metrics and a classical numerical simulator, and with only 1.19% of the baseline's computation time. Ablation studies highlight the potential of explicitly designing learned emulators for parameter estimation by leveraging contrastive learning."}}
{"id": "a68yJLSKY-P", "cdate": 1632875720432, "mdate": null, "content": {"title": "Adaptive Differentially Private Empirical Risk Minimization ", "abstract": "We propose an adaptive (stochastic) gradient perturbation method for differentially private  empirical risk minimization. At each iteration, the random noise added to the gradient is optimally adapted to the stepsize; we name this process adaptive differentially private (ADP) learning.  Given the same privacy budget, we prove that the ADP method considerably improves the utility guarantee compared to the standard differentially private method in which vanilla random noise is added. Our method is particularly useful for gradient-based algorithms with non-constant learning rate, including variants of AdaGrad (Duchi et al., 2011). We provide extensive numerical experiments to demonstrate the effectiveness of the proposed adaptive differentially private algorithm."}}
{"id": "X_jSy6seRj", "cdate": 1621630074900, "mdate": null, "content": {"title": "Pure Exploration in Kernel and Neural Bandits", "abstract": "We study pure exploration in bandits, where the dimension of the feature representation can be much larger than the number of arms. To overcome the curse of dimensionality, we propose to adaptively embed the feature representation of each arm into a lower-dimensional space and carefully deal with the induced model misspecifications. Our approach is conceptually very different from existing works that can either only handle low-dimensional linear bandits or passively deal with model misspecifications. We showcase the application of our approach to two pure exploration settings that were previously under-studied: (1) the reward function belongs to a possibly infinite-dimensional Reproducing Kernel Hilbert Space, and (2) the reward function is nonlinear and can be approximated by neural networks. Our main results provide sample complexity guarantees that only depend on the effective dimension of the feature spaces in the kernel or neural representations. Extensive experiments conducted on both synthetic and real-world datasets demonstrate the efficacy of our methods."}}
{"id": "H1lNPxHKDH", "cdate": 1569439835712, "mdate": null, "content": {"title": "A Function Space View of Bounded Norm Infinite Width ReLU Nets: The Multivariate Case", "abstract": "We give a tight characterization of the (vectorized Euclidean) norm of weights required to realize a function $f:\\mathbb{R}\\rightarrow \\mathbb{R}^d$ as a single hidden-layer ReLU network with an unbounded number of units (infinite width), extending the univariate characterization of Savarese et al. (2019) to the multivariate case."}}
{"id": "SyxYnQ398H", "cdate": 1568486321045, "mdate": null, "content": {"title": "Learning to Solve Linear Inverse Problems in Imaging with Neumann Networks", "abstract": "Recent advances have illustrated that it is often possible to learn to solve linear inverse problems in imaging using training data that can outperform more traditional regularized least squares solutions. Along these lines, we present some extensions of the Neumann network, a recently introduced end-to-end learned architecture inspired by a truncated Neumann series expansion of the solution map to a regularized least squares problem. Here we summarize the Neumann network approach, and show that it has a form compatible with the optimal reconstruction function for a given inverse problem. We also investigate an extension of the Neumann network that incorporates a more sample efficient patch-based regularization approach."}}
{"id": "r1b_B3ZdZH", "cdate": 1546300800000, "mdate": null, "content": {"title": "Bilinear Bandits with Low-rank Structure", "abstract": "We introduce the bilinear bandit problem with low-rank structure in which an action takes the form of a pair of arms from two different entity types, and the reward is a bilinear function of the kn..."}}
{"id": "BXc354ldTr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Online Data Thinning via Multi-Subspace Tracking.", "abstract": "In an era of ubiquitous large-scale streaming data, the availability of data far exceeds the capacity of expert human analysts. In many settings, such data is either discarded or stored unprocessed in data centers. This paper proposes a method of online data thinning, in which large-scale streaming datasets are winnowed to preserve unique, anomalous, or salient elements for timely expert analysis. At the heart of this proposed approach is an online anomaly detection method based on dynamic, low-rank Gaussian mixture models. Specifically, the high-dimensional covariance matrices associated with the Gaussian components are associated with low-rank models. According to this model, most observations lie near a union of subspaces. The low-rank modeling mitigates the curse of dimensionality associated with anomaly detection for high-dimensional data, and recent advances in subspace clustering and subspace tracking allow the proposed method to adapt to dynamic environments. Furthermore, the proposed method allows subsampling, is robust to missing data, and uses a mini-batch online optimization approach. The resulting algorithms are scalable, efficient, and are capable of operating in real time. Experiments on wide-area motion imagery and e-mail databases illustrate the efficacy of the proposed approach."}}
{"id": "rJEN4ClOZH", "cdate": 1483228800000, "mdate": null, "content": {"title": "On Learning High Dimensional Structured Single Index Models", "abstract": "Single Index Models (SIMs) are simple yet flexible semi-parametric models for classification and regression, where response variables are modeled as a nonlinear, monotonic function of a linear combination of features. Estimation in this context requires learning both the feature weights and the nonlinear function that relates features to observations. While methods have been described to learn SIMs in the low dimensional regime, a method that can efficiently learn SIMs in high dimensions, and under general structural assumptions, has not been forthcoming. In this paper, we propose computationally efficient algorithms for SIM inference in high dimensions using atomic norm regularization. This general approach to imposing structure in high-dimensional modeling specializes to sparsity, group sparsity, and low-rank assumptions among others. We also provide a scalable, stochastic version of the method. Experiments show that the method we propose enjoys superior predictive performance when compared to generalized linear models such as logistic regression, on several real-world datasets."}}
