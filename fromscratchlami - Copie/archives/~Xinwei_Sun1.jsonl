{"id": "TxHONn9wmk", "cdate": 1696317837284, "mdate": 1696317837284, "content": {"title": "Controlling the False Discovery Rate in Transformational Sparsity: Split Knockoffs", "abstract": "Controlling the False Discovery Rate (FDR) in a variable selection procedure is critical for reproducible discoveries, which receives an extensive study in sparse linear models. However, in many scenarios, the sparsity constraint is not directly imposed on the parameters, but on a linear transformation of the parameters to be estimated. Examples can be found in total variations, wavelet transforms, fused LASSO, and trend filtering, etc. In this paper, we propose a data adaptive FDR control in this transformational sparsity setting, the Split Knockoff method. The proposed scheme exploits both variable and data splitting. By variable splitting, the linear transformation constraint is relaxed to its Euclidean proximity in a lifted parameter space, yielding an orthogonal design for improved power and orthogonal Split Knockoff copies. Moreover, by randomly splitting the data into two independent subsets, new knockoff statistics are generated with signs as independent Bernoulli random variables, enabling inverse supermartingale constructions for provable FDR control. Simulation experiments show that the proposed methodology achieves desired FDR and power. An application to Alzheimer's Disease study is provided that atrophy brain regions and their abnormal connections can be discovered based on a structural Magnetic Resonance Imaging dataset (ADNI)."}}
{"id": "L0ozmeWske", "cdate": 1696317428195, "mdate": 1696317428195, "content": {"title": "Exploring Structural Sparsity of Deep Networks via Inverse Scale Spaces", "abstract": "The great success of deep neural networks is built upon their over-parameterization, which smooths the optimization\nlandscape without degrading the generalization ability. Despite the benefits of over-parameterization, a huge amount of parameters\nmakes deep networks cumbersome in daily life applications. On the other hand, training neural networks without over-parameterization\nfaces many practical problems, e.g., being trapped in the local optimal. Though techniques such as pruning and distillation are\ndeveloped, they are expensive in fully training a dense network as backward selection methods; and there is still a void on\nsystematically exploring forward selection methods for learning structural sparsity in deep networks. To fill in this gap, this paper\nproposes a new approach based on differential inclusions of inverse scale spaces. Specifically, our method can generate a family of\nmodels from simple to complex ones along the dynamics via coupling a pair of parameters, such that over-parameterized deep models\nand their structural sparsity can be explored simultaneously. This kind of differential inclusion scheme has a simple discretization,\ndubbed Deep structure splitting Linearized Bregman Iteration (DessiLBI), whose global convergence in learning deep networks could\nbe established under the Kurdyka-\u00baojasiewicz framework. Particularly, we explore several applications of DessiLBI, including finding\nsparse structures of networks directly via the coupled structure parameter and growing networks from simple to complex ones\nprogressively. Experimental evidence shows that our method achieves comparable and even better performance than the competitive\noptimizers in exploring the sparse structure of several widely used backbones on the benchmark datasets. Remarkably, with early\nstopping, our method unveils \u201cwinning tickets\u201d in early epochs: the effective sparse network structures with comparable test accuracy to\nfully trained over-parameterized models, that are further transferable to similar alternative tasks. Furthermore, our method is able to\ngrow networks efficiently with adaptive filter configurations, demonstrating the good performance with much less computational cost.\nCodes and models can be downloaded at https://github.com/DessiLBI2020/DessiLBI ."}}
{"id": "S3l8gDboi-v", "cdate": 1672531200000, "mdate": 1680102154479, "content": {"title": "Knockoffs-SPR: Clean Sample Selection in Learning with Noisy Labels", "abstract": ""}}
{"id": "Rm50CYxbjB", "cdate": 1672531200000, "mdate": 1683769709459, "content": {"title": "Exploring Structural Sparsity of Deep Networks Via Inverse Scale Spaces", "abstract": "The great success of deep neural networks is built upon their over-parameterization, which smooths the optimization landscape without degrading the generalization ability. Despite the benefits of over-parameterization, a huge amount of parameters makes deep networks cumbersome in daily life applications. On the other hand, training neural networks without over-parameterization faces many practical problems, e.g., being trapped in the local optimal. Though techniques such as pruning and distillation are developed, they are expensive in fully training a dense network as backward selection methods; and there is still a void on systematically exploring forward selection methods for learning structural sparsity in deep networks. To fill in this gap, this paper proposes a new approach based on differential inclusions of inverse scale spaces. Specifically, our method can generate a family of models from simple to complex ones along the dynamics via coupling a pair of parameters, such that over-parameterized deep models and their structural sparsity can be explored simultaneously. This kind of differential inclusion scheme has a simple discretization, dubbed Deep structure splitting Linearized Bregman Iteration ( <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">DessiLBI</i> ), whose global convergence in learning deep networks could be established under the Kurdyka-\u0141ojasiewicz framework. Particularly, we explore several applications of DessiLBI, including finding sparse structures of networks directly via the coupled structure parameter and growing networks from simple to complex ones progressively. Experimental evidence shows that our method achieves comparable and even better performance than the competitive optimizers in exploring the sparse structure of several widely used backbones on the benchmark datasets. Remarkably, with early stopping, our method unveils \u201cwinning tickets\u201d in early epochs: the effective sparse network structures with comparable test accuracy to fully trained over-parameterized models, that are further transferable to similar alternative tasks. Furthermore, our method is able to grow networks efficiently with adaptive filter configurations, demonstrating the good performance with much less computational cost. Codes and models can be downloaded at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/DessiLBI2020/DessiLBI</uri> ."}}
{"id": "7D4qg-BsZJN", "cdate": 1672531200000, "mdate": 1684140521480, "content": {"title": "Causal Discovery with Unobserved Variables: A Proxy Variable Approach", "abstract": "Discovering causal relations from observational data is important. The existence of unobserved variables, such as latent confounders or mediators, can mislead the causal identification. To address this issue, proximal causal discovery methods proposed to adjust for the bias with the proxy of the unobserved variable. However, these methods presumed the data is discrete, which limits their real-world application. In this paper, we propose a proximal causal discovery method that can well handle the continuous variables. Our observation is that discretizing continuous variables can can lead to serious errors and comprise the power of the proxy. Therefore, to use proxy variables in the continuous case, the critical point is to control the discretization error. To this end, we identify mild regularity conditions on the conditional distributions, enabling us to control the discretization error to an infinitesimal level, as long as the proxy is discretized with sufficiently fine, finite bins. Based on this, we design a proxy-based hypothesis test for identifying causal relationships when unobserved variables are present. Our test is consistent, meaning it has ideal power when large samples are available. We demonstrate the effectiveness of our method using synthetic and real-world data."}}
{"id": "0YtAiXcO4Xu", "cdate": 1672531200000, "mdate": 1684140521808, "content": {"title": "Causal Discovery from Subsampled Time Series with Proxy Variables", "abstract": "Inferring causal structures from time series data is the central interest of many scientific inquiries. A major barrier to such inference is the problem of subsampling, i.e., the frequency of measurement is much lower than that of causal influence. To overcome this problem, numerous methods have been proposed, yet either was limited to the linear case or failed to achieve identifiability. In this paper, we propose a constraint-based algorithm that can identify the entire causal structure from subsampled time series, without any parametric constraint. Our observation is that the challenge of subsampling arises mainly from hidden variables at the unobserved time steps. Meanwhile, every hidden variable has an observed proxy, which is essentially itself at some observable time in the future, benefiting from the temporal structure. Based on these, we can leverage the proxies to remove the bias induced by the hidden variables and hence achieve identifiability. Following this intuition, we propose a proxy-based causal discovery algorithm. Our algorithm is nonparametric and can achieve full causal identification. Theoretical advantages are reflected in synthetic and real-world experiments."}}
{"id": "YP4QEmqh6Ia", "cdate": 1663850204705, "mdate": null, "content": {"title": "Which Invariance Should We Transfer? A Causal Minimax Learning Approach", "abstract": "A major barrier to deploy current machine learning models lies in their sensitivity to dataset shifts. To resolve this problem, most existing studies attempted to transfer stable information to unseen environments. Among these, graph-based methods causally decomposed the data generating process into stable and mutable mechanisms. By removing the effect of mutable generation, they identified a set of stable predictors. However, a key question regarding robustness remains: which subset of the whole stable information should the model transfer, in order to achieve optimal generalization ability? To answer this question, we provide a comprehensive minimax analysis that fully characterizes conditions for a subset to be optimal. Particularly in general cases, we propose to maximize over mutable mechanisms (i.e., the source of dataset shifts), which is provable to identify the worst-case risk over all environments. This ensures us to select the optimal subset with the minimal worst-case risk. To reduce computational costs, we propose to search over only equivalent classes in terms of worst-case risk, instead of over all subsets. In cases when the searching space is still large, we turn this subset selection problem into a sparse min-max optimization scheme, which enjoys the simplicity and efficiency of implementation. The utility of our methods is demonstrated on the diagnosis of Alzheimer's Disease and gene function prediction. "}}
{"id": "gUZWOE42l6Q", "cdate": 1663850053357, "mdate": null, "content": {"title": "Out-of-distribution Representation Learning for Time Series Classification", "abstract": "Time series classification is an important problem in the real world. Due to its non-stationary property that the distribution changes over time, it remains challenging to build models for generalization to unseen distributions. In this paper, we propose to view time series classification from the distribution perspective. We argue that the temporal complexity of a time series dataset could attribute to unknown latent distributions that need characterize. To this end, we propose DIVERSIFY for out-of-distribution (OOD) representation learning on dynamic distributions of times series. DIVERSIFY takes an iterative process: it first obtains the \u2018worst-case\u2019 latent distribution scenario via adversarial training, then reduces the gap between these latent distributions. We then show that such an algorithm is theoretically supported. Extensive experiments are conducted on seven datasets with different OOD settings across gesture recognition, speech commands recognition, wearable stress and affect detection, and sensor-based human activity recognition. Qualitative and quantitative results demonstrate that DIVERSIFY significantly outperforms other baselines and effectively characterizes the latent distributions. Code is available at https://github.com/microsoft/robustlearn."}}
{"id": "-HHJZlRpGb", "cdate": 1663849818636, "mdate": null, "content": {"title": "Learning Domain-Agnostic Representation for Disease Diagnosis", "abstract": "In clinical environments, image-based diagnosis is desired to achieve robustness on multi-center samples. Toward this goal, a natural way is to capture only clinically disease-related features. However, such disease-related features are often entangled with center-effect, disabling robust transferring to unseen centers/domains. To disentangle disease-related features, we first leverage structural causal modeling to explicitly model disease-related and center-effects that are provable to be disentangled from each other. Guided by this, we propose a novel Domain Agnostic Representation Model (DarMo) based on variational Auto-Encoder. To facilitate disentanglement, we design domain-agnostic and domain-aware encoders to respectively capture disease-related features and varied center-effects by incorporating a domain-aware batch normalization layer. Besides, we constrain the disease-related features to well predict the disease label as well as clinical attributes, by leveraging Graph Convolutional Network (GCN) into our decoder. The effectiveness and utility of our method are demonstrated by the superior performance over others on both public datasets and inhouse datasets."}}
{"id": "rrDVCQSt5tn", "cdate": 1640995200000, "mdate": 1681553193555, "content": {"title": "Out-of-Distribution Representation Learning for Time Series Classification", "abstract": ""}}
