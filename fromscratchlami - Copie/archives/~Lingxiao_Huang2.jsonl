{"id": "ed3N9dNRvf", "cdate": 1672531200000, "mdate": 1681653472330, "content": {"title": "Coresets for Clustering with General Assignment Constraints", "abstract": ""}}
{"id": "eBh2ZCpgAVJ", "cdate": 1672531200000, "mdate": 1681655972921, "content": {"title": "On Coresets for Clustering in Small Dimensional Euclidean Spaces", "abstract": "We consider the problem of constructing small coresets for $k$-Median in Euclidean spaces. Given a large set of data points $P\\subset \\mathbb{R}^d$, a coreset is a much smaller set $S\\subset \\mathbb{R}^d$, so that the $k$-Median costs of any $k$ centers w.r.t. $P$ and $S$ are close. Existing literature mainly focuses on the high-dimension case and there has been great success in obtaining dimension-independent bounds, whereas the case for small $d$ is largely unexplored. Considering many applications of Euclidean clustering algorithms are in small dimensions and the lack of systematic studies in the current literature, this paper investigates coresets for $k$-Median in small dimensions. For small $d$, a natural question is whether existing near-optimal dimension-independent bounds can be significantly improved. We provide affirmative answers to this question for a range of parameters. Moreover, new lower bound results are also proved, which are the highest for small $d$. In particular, we completely settle the coreset size bound for $1$-d $k$-Median (up to log factors). Interestingly, our results imply a strong separation between $1$-d $1$-Median and $1$-d $2$-Median. As far as we know, this is the first such separation between $k=1$ and $k=2$ in any dimension."}}
{"id": "aJsk6-5sbD", "cdate": 1672531200000, "mdate": 1681655973113, "content": {"title": "Improved Coresets for Clustering with Capacity and Fairness Constraints", "abstract": "We study coresets for clustering with capacity and fairness constraints. Our main result is a near-linear time algorithm to construct $\\tilde{O}(k^2\\varepsilon^{-2z-2})$-sized $\\varepsilon$-coresets for capacitated $(k,z)$-clustering which improves a recent $\\tilde{O}(k^3\\varepsilon^{-3z-2})$ bound by [BCAJ+22, HJLW23]. As a corollary, we also save a factor of $k \\varepsilon^{-z}$ on the coreset size for fair $(k,z)$-clustering compared to them. We fundamentally improve the hierarchical uniform sampling framework of [BCAJ+22] by adaptively selecting sample size on each ring instance, proportional to its clustering cost to an optimal solution. Our analysis relies on a key geometric observation that reduces the number of total ``effective centers\" from [BCAJ+22]'s $\\tilde{O}(k^2\\varepsilon^{-z})$ to merely $O(k\\log \\varepsilon^{-1})$ by being able to ``ignore'' all center points that are too far or too close to the ring center."}}
{"id": "Fr_OTF8ePi", "cdate": 1672531200000, "mdate": 1681653472319, "content": {"title": "The Power of Uniform Sampling for k-Median", "abstract": ""}}
{"id": "Nc1ZkRW8Vde", "cdate": 1663850258756, "mdate": null, "content": {"title": "Near-optimal Coresets for Robust Clustering", "abstract": "We consider robust clustering problems in $\\mathbb{R}^d$, specifically $k$-clustering problems (e.g., $k$-Median and $k$-Means) with $m$ \\emph{outliers}, where the cost for a given center set $C \\subset \\mathbb{R}^d$ aggregates the distances from $C$ to all but the furthest $m$ data points, instead of all points as in classical clustering. We focus on the $\\epsilon$-coreset for robust clustering, a small proxy of the dataset that preserves the clustering cost within $\\epsilon$-relative error for all center sets. Our main result is an $\\epsilon$-coreset of size $O(m + \\mathrm{poly}(k \\epsilon^{-1}))$ that can be constructed in near-linear time. This significantly improves previous results, which either suffers an exponential dependence on $(m + k)$ [Feldman and Schulman, SODA'12], or has a weaker bi-criteria guarantee [Huang et al., FOCS'18]. Furthermore, we show this dependence in $m$ is nearly-optimal, and the fact that it is isolated from other factors may be crucial for dealing with large number of outliers. We construct our coresets by adapting to the outlier setting a recent framework [Braverman et al., FOCS'22] which was designed for capacity-constrained clustering, overcoming a new challenge that the participating terms in the cost, particularly the excluded $m$ outlier points, are dependent on the center set $C$. We validate our coresets on various datasets, and we observe a superior size-accuracy tradeoff compared with popular baselines including uniform sampling and sensitivity sampling. We also achieve a significant speedup of existing approximation algorithms for robust clustering using our coresets."}}
{"id": "QjQibO3scV_", "cdate": 1663849852474, "mdate": null, "content": {"title": "Revocable Deep Reinforcement Learning with Affinity Regularization for Outlier-Robust Graph Matching", "abstract": "Graph matching (GM) has been a building block in various areas including computer vision and pattern recognition. Despite recent impressive progress, existing deep GM methods often have obvious difficulty in handling outliers, which are ubiquitous in practice. We propose a deep reinforcement learning based approach RGM, whose sequential node matching scheme naturally fits the strategy for selective inlier matching against outliers. A revocable action framework is devised to improve the agent's flexibility against the complex constrained GM. Moreover, we propose a quadratic approximation technique to regularize the affinity score, in the presence of outliers. As such, the agent can finish inlier matching timely when the affinity score stops growing, for which otherwise an additional parameter i.e. the number of inliers is needed to avoid matching outliers. In this paper, we focus on learning the back-end solver under the most general form of GM: the Lawler's QAP, whose input is the affinity matrix. Especially, our approach can also boost existing GM methods that use such input. Experiments on multiple real-world datasets demonstrate its performance regarding both accuracy and robustness."}}
{"id": "OlDEMIbCvTl", "cdate": 1652737454212, "mdate": null, "content": {"title": "Efficient Submodular Optimization under Noise: Local Search is Robust", "abstract": "The problem of monotone submodular maximization has been studied extensively due to its wide range of applications. However, there are cases where one can only access the objective function in a distorted or noisy form because of the uncertain nature or the errors involved in the evaluation. This paper considers the problem of constrained monotone submodular maximization with noisy oracles introduced by Hassidim and Singer (2017). For a cardinality constraint, we propose an algorithm achieving a near-optimal (1-1/e-O(epsilon))-approximation guarantee (for arbitrary epsilon > 0) with only a polynomial number of queries to the noisy value oracle, which improves the exponential query complexity of Singer and Hassidim (2018). For general matroid constraints, we show the first constant approximation algorithm in the presence of noise. Our main approaches are to design a novel local search framework that can handle the effect of noise and to construct certain smoothing surrogate functions for noise reduction."}}
{"id": "N0tKCpMhA2", "cdate": 1652737435764, "mdate": null, "content": {"title": "Coresets for Vertical Federated Learning: Regularized Linear Regression and $K$-Means Clustering", "abstract": "Vertical federated learning (VFL), where data features are stored in multiple parties distributively, is an important area in machine learning. However, the communication complexity for VFL is typically very high. In this paper, we propose a unified framework by constructing \\emph{coresets} in a distributed fashion for communication-efficient VFL. We study two important learning tasks in the VFL setting: regularized linear regression and $k$-means clustering, and apply our coreset framework to both problems. We theoretically show that using coresets can drastically alleviate the communication complexity, while nearly maintain the solution quality. Numerical experiments are conducted to corroborate our theoretical findings."}}
{"id": "uT7rOb8r7Th", "cdate": 1640995200000, "mdate": 1668095248131, "content": {"title": "M-Mix: Generating Hard Negatives via Multi-sample Mixing for Contrastive Learning", "abstract": "Negative pairs, especially hard negatives as combined with common negatives (easy to discriminate), are essential in contrastive learning, which plays a role of avoiding degenerate solutions in the sense of constant representation across different instances. Inspired by recent hard negative mining methods via pairwise mixup operation in vision, we propose M-Mix, which dynamically generates a sequence of hard negatives. Compared with previous methods, M-Mix mainly has three features: 1) adaptively choose samples to mix; 2) simultaneously mix multiple samples; 3) automatically assign different mixing weights to the selected samples. We evaluate our method on two image datasets (CIFAR-10, CIFAR-100), five node classification datasets (PPI, DBLP, Pubmed, etc), five graph classification datasets (IMDB, PTC_MR, etc), and two downstream combinatorial tasks (graph edit distance and node clustering). Results show that it achieves state-of-the-art performance under self-supervised settings. Code is available at: https://github.com/Sherrylone/m-mix."}}
{"id": "lNByZL7yWD", "cdate": 1640995200000, "mdate": 1681655972354, "content": {"title": "Coresets for Vertical Federated Learning: Regularized Linear Regression and K-Means Clustering", "abstract": "Vertical federated learning (VFL), where data features are stored in multiple parties distributively, is an important area in machine learning. However, the communication complexity for VFL is typically very high. In this paper, we propose a unified framework by constructing coresets in a distributed fashion for communication-efficient VFL. We study two important learning tasks in the VFL setting: regularized linear regression and $k$-means clustering, and apply our coreset framework to both problems. We theoretically show that using coresets can drastically alleviate the communication complexity, while nearly maintain the solution quality. Numerical experiments are conducted to corroborate our theoretical findings."}}
