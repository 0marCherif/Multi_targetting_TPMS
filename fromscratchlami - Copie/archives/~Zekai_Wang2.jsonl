{"id": "ibpE1ekkCP", "cdate": 1672531200000, "mdate": 1681891323368, "content": {"title": "Better Diffusion Models Further Improve Adversarial Training", "abstract": "It has been recognized that the data generated by the denoising diffusion probabilistic model (DDPM) improves adversarial training. After two years of rapid development in diffusion models, a question naturally arises: can better diffusion models further improve adversarial training? This paper gives an affirmative answer by employing the most recent diffusion model which has higher efficiency ($\\sim 20$ sampling steps) and image quality (lower FID score) compared with DDPM. Our adversarially trained models achieve state-of-the-art performance on RobustBench using only generated data (no external datasets). Under the $\\ell_\\infty$-norm threat model with $\\epsilon=8/255$, our models achieve $70.69\\%$ and $42.67\\%$ robust accuracy on CIFAR-10 and CIFAR-100, respectively, i.e. improving upon previous state-of-the-art models by $+4.58\\%$ and $+8.03\\%$. Under the $\\ell_2$-norm threat model with $\\epsilon=128/255$, our models achieve $84.86\\%$ on CIFAR-10 ($+4.44\\%$). These results also beat previous works that use external data. We also provide compelling results on the SVHN and TinyImageNet datasets. Our code is available at https://github.com/wzekai99/DM-Improves-AT."}}
{"id": "LqGA2JMLwBw", "cdate": 1652737513147, "mdate": null, "content": {"title": "On the Tradeoff Between Robustness and Fairness", "abstract": "Interestingly, recent experimental results [2, 26, 22] have identified a robust fairness phenomenon in adversarial training (AT), namely that a robust model well-trained by AT exhibits a remarkable disparity of standard accuracy and robust accuracy among different classes compared with natural training. However, the effect of different perturbation radii in AT on robust fairness has not been studied, and one natural question is raised: does a tradeoff exist between average robustness and robust fairness? Our extensive experimental results provide an affirmative answer to this question: with an increasing perturbation radius, stronger AT will lead to a larger class-wise disparity of robust accuracy. Theoretically, we analyze the class-wise performance of adversarially trained linear models with mixture Gaussian distribution. Our theoretical results support our observations. Moreover, our theory shows that  adversarial training easily leads to more serious robust fairness issue than natural training. Motivated by theoretical results, we propose a fairly adversarial training (FAT) method to mitigate the tradeoff between average robustness and robust fairness. Experimental results validate the effectiveness of our proposed method.\n"}}
{"id": "laWLYMYDEVe", "cdate": 1640995200000, "mdate": 1681585705282, "content": {"title": "MetaWeighting: Learning to Weight Tasks in Multi-Task Learning", "abstract": ""}}
{"id": "OQXhDnaOI1", "cdate": 1640995200000, "mdate": 1682322235671, "content": {"title": "Robustness Verification for Contrastive Learning", "abstract": "Contrastive adversarial training has successfully improved the robustness of contrastive learning (CL). However, the robustness metric used in these methods is linked to attack algorithms, image la..."}}
{"id": "BXpn6ryPpFh", "cdate": 1609459200000, "mdate": 1649848592166, "content": {"title": "BanditMTL: Bandit-based Multi-task Learning for Text Classification", "abstract": "Yuren Mao, Zekai Wang, Weiwei Liu, Xuemin Lin, Wenbin Hu. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021."}}
