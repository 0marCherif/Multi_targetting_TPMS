{"id": "uMlLT_xuiE", "cdate": 1677713812880, "mdate": null, "content": {"title": "Predicting Targets with Data from Non-Conforming Sources ", "abstract": "Machine learning applications to real-world settings are often tasked with making predictions on data generated by multiple sources. There are many methods for understanding when data is Out-Of-Distribution (OOD). A less explored area of importance is where OOD data can be considered In-Distribution (ID) when conditioned by its generating data source. Within this preliminary research, we focus on this issue and propose methods for building classification models capable of making predictions on data in which labels can depend on their source."}}
{"id": "VPX0ln_YoG", "cdate": 1663850571996, "mdate": null, "content": {"title": "Loss Adapted Plasticity: Learning From Data With Unreliable Sources", "abstract": "When data is streaming from multiple sources, conventional training methods update model weights often assuming the same level of reliability for each source; that is: a model does not consider data quality of a specific source during training. In many applications, sources can have varied levels of noise or corruption that can produce negative effects on the learning of a robust machine learning model. A key issue is that the quality of data or labels for individual sources is often not available to a model during training and could vary over time. A solution to this problem is to consider the mistakes made while training on data originating from sources and utilise this to create a perceived data quality for each source. This paper demonstrates a technique that can be applied to any gradient descent optimiser: Update model weights as a function of the perceived reliability of data sources within a wider data set. The algorithm controls the plasticity of a given model to weight updates based on the history of losses from individual data sources. We show that applying this technique can significantly improve model performance when trained on a mixture of reliable and unreliable data sources, and maintain performance when models are trained on data sources that are all considered reliable. "}}
{"id": "ofLwshMBL_H", "cdate": 1632875757416, "mdate": null, "content": {"title": "Continual Learning Using Task Conditional Neural Networks", "abstract": "Conventional deep learning models have limited capacity in learning multiple tasks sequentially. The issue of forgetting the previously learned tasks in continual learning is known as catastrophic forgetting or interference. When the input data or the goal of learning changes, a continual model will learn and adapt to the new status. However, the model will not remember or recognise any revisits to the previous states. This causes performance reduction and re-training curves in dealing with periodic or irregularly reoccurring changes in the data or goals. Dynamic approaches, which assign new neuron resources to the upcoming tasks, are introduced to address this issue. However, most of the dynamic methods need task information about the upcoming tasks during the inference phase to activate the corresponding neurons. To address this issue, we introduce Task Conditional Neural Network which allows the model to identify the task information automatically. The proposed model can continually learn and embed new tasks into the model without losing the information about previously learned tasks. We evaluate the proposed model combined with the mixture of experts approach on the MNIST and CIFAR100 datasets and show how it significantly improves the continual learning process without requiring task information in advance."}}
{"id": "u6ybkty-bL", "cdate": 1632875702609, "mdate": null, "content": {"title": "When Complexity Is Good: Do We Need Recurrent Deep Learning For Time Series Outlier Detection?", "abstract": "Outlier detection is a critical part of understanding a dataset and extracting results. Outlier detection is used in different domains for various reasons; including detecting stolen credit cards, spikes of energy usage, web attacks, or in-home activity monitoring. Within this paper, we look at when it is appropriate to apply recurrent deep learning methods for time series outlier detection versus non-recurrent methods. Recurrent deep learning methods have a larger capacity for learning complex representations in time series data. We apply these methods to various synthetic and real-world datasets, including a dataset containing information about the in-home movement of people living with dementia in a clinical study cross-referenced with their recorded unplanned hospital admissions and infection episodes. We also introduce two new outlier detection methods, that can be useful in detecting contextual outliers in time series data where complex temporal relationships and local variations in the time series are important."}}
{"id": "fpU10jwpPvw", "cdate": 1632875466420, "mdate": null, "content": {"title": "Folded Hamiltonian Monte Carlo for Bayesian Generative Adversarial Networks", "abstract": "Generative Adversarial Networks (GANs) can learn complex distributions over images, audio, and data that are difficult to model. We deploy a Bayesian formulation for unsupervised and semi-supervised GAN learning. We propose Folded Hamiltonian Monte Carlo (F-HMC) within this framework to marginalise the weights of the generators and discriminators. The resulting approach improves the performance by having suitable entropy in generated candidates for generator and discriminators' weights. Our proposed model efficiently approximates the high dimensional data due to its parallel composition, increases the accuracy of generated samples and generates interpretable and diverse candidate samples. We have presented the analytical formulation as well as the mathematical proof of the F-HMC. The performance of our model in terms of autocorrelation of generated samples on converging to a high dimensional multi-modal dataset exhibits the effectiveness of the proposed solution. Experimental results on high-dimensional synthetic multi-modal data and natural image benchmarks, including CIFAR-10, SVHN and ImageNet, show that F-HMC outperforms the state-of-the-art methods in terms of test error rates, runtimes per epoch, inception score and Frechet Inception Distance scores."}}
{"id": "SJEP5WZdbS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Spatio-Temporal Analysis for Smart City Data", "abstract": "The data gathered from smart cities can help citizens and city manager planners know where and when they should be aware of the repercussions regarding events happening in different parts of the city. Most of the smart city data analysis solutions are focused on the events and occurrences of the city as a whole, making it difficult to discern the exact place and time of the consequences of a particular event. We propose a novel method to model the events in a city in space and time. We apply our methodology for vehicular traffic data basing our models in (convolutional) neuronal networks."}}
{"id": "H1Ebez-dbS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Web Stream Processing Workshop Chairs' Welcome & Organization", "abstract": "Applications in different domains require reactive processing of massive, dynamically generated streams of data. This trend is increasingly visible also on the Web, where more and more streaming sources are becoming available. These originate from social networks, sensor networks, the Internet of Things (IoT) and many other technologies that use the Web as a platform for sharing data. This has resulted in new Web-centric efforts such as the Web of Things (WoT), which focuses on exposing and describing the IoT resources on the Web; or the Social Web which provides protocols, vocabularies, and APIs to facilitate access to social communications and interactions on the Web."}}
