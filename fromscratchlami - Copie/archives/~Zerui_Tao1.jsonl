{"id": "TloXPFDgPK", "cdate": 1640995200000, "mdate": 1681717829132, "content": {"title": "Permutation Search of Tensor Network Structures via Local Sampling", "abstract": "Recent works put much effort into tensor network structure search (TN-SS), aiming to select suitable tensor network (TN) structures, involving the TN-ranks, formats, and so on, for the decomposition or learning tasks. In this paper, we consider a practical variant of TN-SS, dubbed TN permutation search (TN-PS), in which we search for good mappings from tensor modes onto TN vertices (core tensors) for compact TN representations. We conduct a theoretical investigation of TN-PS and propose a practically-efficient algorithm to resolve the problem. Theoretically, we prove the counting and metric properties of search spaces of TN-PS, analyzing for the first time the impact of TN structures on these unique properties. Numerically, we propose a novel meta-heuristic algorithm, in which the searching is done by randomly sampling in a neighborhood established in our theory, and then recurrently updating the neighborhood until convergence. Numerical results demonstrate that the new algorithm can reduce the required model size of TNs in extensive benchmarks, implying the improvement in the expressive power of TNs. Furthermore, the computational cost for the new algorithm is significantly less than that in~\\cite{li2020evolutionary}."}}
{"id": "08FglarBVbH", "cdate": 1640995200000, "mdate": 1681717828338, "content": {"title": "Permutation Search of Tensor Network Structures via Local Sampling", "abstract": "Recent works put much effort into <em>tensor network structure search</em> (TN-SS), aiming to select suitable tensor network (TN) structures, involving the TN-ranks, formats, and so on, for the dec..."}}
{"id": "mAA1SfR_iun", "cdate": 1621629947036, "mdate": null, "content": {"title": "Graph-Constrained Structure Search for Tensor Network Representation", "abstract": "Recent works paid effort on the structure search issue for tensor network (TN) representation, of which the aim is to select the optimal network for TN contraction to fit a tensor. In practice, however, it is more inclined to solve its sub-problem: searching TN structures from candidates with a similar topology like a cycle or lattice. We name this problem the graph-constrained structure search, and it remains open to this date. In this work, we conduct a thorough investigation of this issue from both the theoretical and practical aspects. Theoretically, we prove that the TN structures are generally irregular under graph constraints yet can be universally embedded into a low-dimensional regular discrete space. Guided by the theoretical results, we propose a simple algorithm, which can encode the graph-constrained TN structures into fixed-length strings for practical purposes by a ` \u201crandom-key\" trick, and empirical results demonstrate the effectiveness and efficiency of the proposed coding method on extensive benchmark TN representation tasks."}}
{"id": "u-JT3OwONrn", "cdate": 1609459200000, "mdate": 1681717828876, "content": {"title": "Bayesian Latent Factor Model for Higher-order Data", "abstract": "Latent factor models are canonical tools to learn low-dimensional and linear embedding of original data. Traditional latent factor models are based on low-rank matrix factorization of covariance ma..."}}
{"id": "fNIKAKS_g7", "cdate": 1609459200000, "mdate": 1668221195318, "content": {"title": "Tensor Decomposition Via Core Tensor Networks", "abstract": "Tensor decomposition (TD) has shown promising performance in image completion and denoising. Existing methods always aim to decompose one tensor into latent factors or core tensors by optimizing a particular cost function based on a specific tensor model. These algorithms iteratively learn the optima from random initialization given any individual tensor, resulting in slow convergence and low efficiency. In this paper, we propose an efficient TD algorithm that aims to learn a global mapping from input tensors to latent core tensors, under the assumption that the mappings of multiple tensors might be shared or highly correlated. To this end, we train a deep neural network (DNN) to model the global mapping and then apply it to decompose a newly given tensor with high efficiency. Furthermore, the initial values of DNN are learned based on meta-learning methods. By leveraging the pretrained core tensor DNN, our proposed method enables us to perform TD efficiently and accurately. Experimental results demonstrate the significant improvements of our method over other TD methods in terms of speed and accuracy."}}
