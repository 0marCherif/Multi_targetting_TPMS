{"id": "DfuXPC1Jf6", "cdate": 1675088723341, "mdate": 1675088723341, "content": {"title": "Pareto models for discriminative multi-class linear dimensionality reduction", "abstract": "We address the class masking problem in multi-class linear discriminant analysis (LDA). In the multi-class setting, LDA does not maximize each pairwise distance between classes, but rather maximizes the sum of all pairwise distances. This results in serious overlaps between classes that are close to each other in the input space, and degrades classification performance. Our research proposes Pareto Discriminant Analysis (PARDA); an approach for multi-class discriminative analysis that builds over multi-objective optimizing models. PARDA decomposes the multi-class problem to a set of objective functions, each representing the distance between every pair of classes. Unlike existing LDA extensions that maximize the sum of all distances, PARDA maximizes each pairwise distance to maximally separate all class means, while minimizing the class overlap in the lower dimensional space. Experimental results on various data sets show consistent and promising performance of PARDA when compared with well-known multi-class LDA extensions."}}
{"id": "1uUjPmKrcUf", "cdate": 1675088600914, "mdate": 1675088600914, "content": {"title": "Local generalized quadratic distance metrics: application to the k-nearest neighbours classifier", "abstract": "Finding the set of nearest neighbours for a query point of interest appears in a variety of algorithms for machine learning and pattern recognition. Examples include k nearest neighbour classification, information retrieval, case-based reasoning, mani- fold learning, and nonlinear dimensionality reduction. In this work, we propose a new approach for determining a distance metric from the data for finding such neighbouring points. For a query point of interest, our approach learns a generalized quadratic distance (GQD) metric based on the statistical properties in a \u201csmall\u201d neighbourhood for the point of interest. The locally learned GQD metric captures information such as the density, curvature, and the intrinsic dimensionality for the points falling in this particular neighbourhood. Unfortunately, learning the GQD parameters under such a local learning mechanism is a challenging problem with a high computational overhead. To address these challenges, we estimate the GQD parameters using the minimum volume covering ellipsoid (MVCE) for a set of points. The advantage of the MVCE is two-fold. First, the MVCE together with the local learning approach approximate the functionality of a well known robust estimator for covariance matrices. Second, computing the MVCE is a convex optimization problem which, in addition to having a unique global solution, can be efficiently solved using a first order optimization algorithm. We validate our metric learning approach on a large variety of datasets and show that the proposed metric has promising results when compared with five algorithms from the literature for supervised metric learning."}}
{"id": "PXMxTio8DW_", "cdate": 1675088396166, "mdate": 1675088396166, "content": {"title": "Learning a Metric Space for Neighbourhood Topology Estimation: Application to Manifold Learning", "abstract": "Manifold learning algorithms rely on a neighbourhood graph to provide an estimate of the data\u2019s local topology. Unfortunately, current methods for estimating local topology assume local Euclidean geometry and locally uniform data density, which often leads to poor data embeddings. We address these shortcomings by proposing a framework that combines local learning with parametric density estimation for local topology estimation. Given a data set D \u2282 X , we first estimate a new metric space (X, dX ) that characterizes the varying sample density of X in X, then use (X,dX) as a new (pilot) input space for the graph construction step of the manifold learning process. The proposed framework results in significantly improved embeddings, which we demonstrated objectively by assessing clustering accuracy."}}
{"id": "qU1G_mlHwT", "cdate": 1675038797579, "mdate": 1675038797579, "content": {"title": "A GENERATIVE-DISCRIMINATIVE HYBRID FOR SEQUENTIAL DATA CLASSIFICATION", "abstract": "Classification of Sequential data using discriminative models such as SVMs is very hard due to the variable length of this type of data. On the other hand, generative models such as HMMs have become the standard tool for representing sequential data due to their efficiency. This paper proposes a general generative-discriminative framework that uses HMMs to map the variable length sequential data into a fixed size p-dimensional vector (likelihood score) that can be easily classified using any discriminative model. The preliminary experiments of the framework on the MNIST database for handwritten digits have achieved a better recognition rate of 98.02% than that of standard HMMs (94.19%)."}}
{"id": "ZHviwGslJzR", "cdate": 1675038589721, "mdate": 1675038589721, "content": {"title": "A Framework for Hypothesis Learning Over Sets of Vectors", "abstract": "Sets of vectors, or bags of features, are a common data representation in domains such as computer vision and speech recognition. However, learning a hypothesis (classification, clustering, etc.) over sets of vectors is usually hindered by their particular structure, in which each object in a data set is represented by a different number of vectors of fixed dimensionality. This nonuniform format of the input data requires the learning algorithm to implicitly handle this non-regular type of input, either by unifying the format of the input, or by extracting the necessary information out of it. \n\nIn this paper we propose an unsupervised learning frame- work for unifying the representation of sets of vectors. The framework defines a metric space over probability distributions representing the sets of vectors, followed by a spectral embedding step for these distributions. The spectral embed- ding step offers an implicit clustering for the data, combined with a reduction \u2013 by orders of magnitude \u2013 in the data\u2019s space complexity, resulting in significantly faster hypothesis learning over the sets of vectors. Moreover, it allows the framework to easily generalize to out-of-sample examples using the Nystrom formula. Although the framework is application independent, we test its validity in the context of human action recognition from video sequences. Besides the previously mentioned properties, the framework does in- deed show better performance than other approaches in the literature."}}
{"id": "m80el-qDUw2", "cdate": 1675038447876, "mdate": 1675038447876, "content": {"title": "Generalization in Unsupervised Learning", "abstract": "We are interested in the following questions. Given a finite dataset S, with neither labels nor side information, and an unsupervised learning algorithm A, can the generalization of A be assessed on S? Similarly, given two unsupervised learning algorithms, A1 and A2, for the same learning task, can one assess whether one will generalize \u201cbetter\u201d on future data drawn from the same source as S? In this paper, we develop a general approach to answering these questions in a reliable and efficient manner using mild assumptions on A. We first propose a generalization criterion for unsupervised learning that is analogous to prediction error in supervised learning. Then, we develop a computationally efficient procedure that realizes the generalization criterion on finite data sets, and propose and extension for comparing the generalization of two algorithms on the same data set. We validate the overall framework on algorithms for clustering and dimensionality reduction (linear and nonlinear)."}}
{"id": "BP6r08H4fl", "cdate": 1675038321106, "mdate": 1675038321106, "content": {"title": "On the structure of hidden Markov models", "abstract": "This paper investigates the effect of HMM structure on the performance of HMM-based classifiers. The investigation is based on the framework of graphical models, the diffusion of credits of HMMs and empirical experiments. Although some researchers have focused on determining the number of states, this study shows that the topology has a stronger influence on increasing the performance of HMM-based classifiers than the number of states."}}
{"id": "I77uAHZ2R0", "cdate": 1675038122458, "mdate": 1675038122458, "content": {"title": "A Note on Metric Properties for Some Divergence Measures: The Gaussian Case", "abstract": "Multivariate Gaussian densities are pervasive in pattern recognition and machine learning. A central operation that appears in most of these areas is to measure the difference between two multivariate Gaussians. Unfortunately, traditional measures based on the Kullback\u2013Leibler (KL) divergence and the Bhattacharyya distance do not satisfy all metric axioms necessary for many algorithms. In this paper we propose a modification for the KL divergence and the Bhattacharyya distance, for multivariate Gaussian densities, that transforms the two measures into distance metrics. Next, we show how these metric axioms impact the unfolding process of manifold learning algorithms. Finally, we illustrate the efficacy of the proposed metrics on two different manifold learning algorithms when used for motion clustering in video data. Our results show that, in this particular application, the new proposed metrics lead to boosts in performance (at least 7%) when compared to other divergence measures."}}
{"id": "4EoOeAMs3F", "cdate": 1675036952326, "mdate": 1675036952326, "content": {"title": "An Exponential Efron-Stein Inequality for Lq Stable Learning Rules", "abstract": "There is an accumulating evidence in the literature that stability of learning algorithms is a key characteristic that permits a learning algorithm to generalize. Despite various insightful results in this direction, there seems to be an overlooked dichotomy in the type of stability-based generalization bounds we have in the literature. On one hand, the literature seems to suggest that exponential generalization bounds for the estimated risk, which are optimal, can be only obtained through stringent, distribution independent and computationally intractable notions of stability such as uniform stability. On the other hand, it seems that weaker notions of stability such as hypothesis stability, although it is distribution dependent and more amenable to computation, can only yield polynomial generalization bounds for the estimated risk, which are suboptimal.\n\nIn this paper, we address the gap between these two regimes of results. In particular, the main question we address here is whether it is possible to derive exponential generalization bounds for the estimated risk using a notion of stability that is computationally tractable and distribution dependent, but weaker than uniform stability. Using recent advances in concentration inequalities, and using a notion of stability that is weaker than uniform stability but distribution dependent and amenable to computation, we derive an exponential tail bound for the concentration of the estimated risk of a hypothesis returned by a general learning rule, where the estimated risk is expressed in terms of either the resubstitution estimate (empirical error), or the deleted (or, leave-one-out) estimate. As an illustration we derive exponential tail bounds for ridge regression with unbounded responses, where we show how stability changes with the tail behaviour of the response variables."}}
{"id": "BXEb9ZzxupB", "cdate": 1546300800000, "mdate": null, "content": {"title": "An Exponential Tail Bound for the Deleted Estimate.", "abstract": "There is an accumulating evidence in the literature that stability of learning algorithms is a key characteristic that permits a learning algorithm to generalize. Despite various insightful results in this direction, there seems to be an overlooked dichotomy in the type of stability-based generalization bounds we have in the literature. On one hand, the literature seems to suggest that exponential generalization bounds for the estimated risk, which are optimal, can be only obtained through stringent, distribution independent and computationally intractable notions of stability such as uniform stability. On the other hand, it seems that weaker notions of stability such as hypothesis stability, although it is distribution dependent and more amenable to computation, can only yield polynomial generalization bounds for the estimated risk, which are suboptimal. In this paper, we address the gap between these two regimes of results. In particular, the main question we address here is whether it is possible to derive exponential generalization bounds for the estimated risk using a notion of stability that is computationally tractable and distribution dependent, but weaker than uniform stability. Using recent advances in concentration inequalities, and using a notion of stability that is weaker than uniform stability but distribution dependent and amenable to computation, we derive an exponential tail bound for the concentration of the estimated risk of a hypothesis returned by a general learning rule, where the estimated risk is expressed in terms of the deleted estimate. Interestingly, we note that our final bound has similarities to previous exponential generalization bounds for the deleted estimate, in particular, the result of Bousquet and Elisseeff (2002) for the regression case."}}
