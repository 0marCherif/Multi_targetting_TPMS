{"id": "0tpZgkAKVjB", "cdate": 1652737750538, "mdate": null, "content": {"title": "Luckiness in Multiscale Online Learning", "abstract": "Algorithms for full-information online learning are classically tuned to minimize their worst-case regret. Modern algorithms additionally provide tighter guarantees outside the adversarial regime, most notably in the form of constant pseudoregret bounds under statistical margin assumptions. We investigate the multiscale extension of the problem where the loss ranges of the experts are vastly different. Here, the regret with respect to each expert needs to scale with its range, instead of the maximum overall range. We develop new multiscale algorithms, tuning schemes and analysis techniques to show that worst-case robustness and adaptation to easy data can be combined at a negligible cost. We further develop an extension with optimism and apply it to solve multiscale two-player zero-sum games. We demonstrate experimentally the superior performance of our scale-adaptive algorithm and discuss the subtle relationship of our results to Freund's 2016 open problem.\n"}}
{"id": "4wVlNqBJXg", "cdate": 1621630303488, "mdate": null, "content": {"title": "Optimal Best-Arm Identification Methods for Tail-Risk Measures", "abstract": "Conditional value-at-risk (CVaR) and value-at-risk (VaR) are popular tail-risk measures in finance and insurance industries as well as in highly reliable, safety-critical uncertain environments where often the underlying probability distributions are heavy-tailed. We use the multi-armed bandit best-arm identification framework and consider the problem of identifying the arm from amongst finitely many that has the smallest CVaR, VaR, or weighted sum of CVaR and mean. The latter captures the risk-return trade-off common in finance. Our main contribution is an optimal $\\delta$-correct algorithm that acts on general arms, including heavy-tailed distributions, and matches the lower bound on the expected number of samples needed, asymptotically (as $ \\delta$ approaches $0$). The algorithm requires solving a non-convex optimization problem in the space of probability measures, that requires delicate analysis. En-route, we develop new non-asymptotic, anytime-valid, empirical-likelihood-based concentration inequalities for tail-risk measures. "}}
{"id": "DDoDN0BLLhb", "cdate": 1621630222697, "mdate": null, "content": {"title": "A/B/n Testing with Control in the Presence of Subpopulations", "abstract": "Motivated by A/B/n testing applications, we consider a finite set of distributions (called \\emph{arms}), one of which is treated as a \\emph{control}. We assume that the population is stratified into homogeneous subpopulations. At every time step, a subpopulation is sampled and an arm is chosen: the resulting observation is an independent draw from the arm conditioned on the subpopulation. The quality of each arm is assessed through a weighted combination of its subpopulation means. We propose a strategy for sequentially choosing one arm per time step so as to discover as fast as possible which arms, if any, have higher weighted expectation than the control. This strategy is shown to be asymptotically optimal in the following sense: if $\\tau_\\delta$ is the first time when the strategy ensures that it is able to output the correct answer with probability at least $1-\\delta$, then $\\mathbb{E}[\\tau_\\delta]$ grows linearly with $\\log(1/\\delta)$ at the exact optimal rate. This rate is identified in the paper in three different settings: (1) when the experimenter does not observe the subpopulation information, (2) when the subpopulation of each sample is observed but not chosen, and (3) when the experimenter can select the subpopulation from which each response is sampled. We illustrate the efficiency of the proposed strategy with numerical simulations on synthetic and real data collected from an A/B/n experiment."}}
{"id": "fK3qqHmMgI", "cdate": 1577836800000, "mdate": null, "content": {"title": "Lipschitz and Comparator-Norm Adaptivity in Online Learning", "abstract": "We study Online Convex Optimization in the unbounded setting where neither predictions nor gradient are constrained. The goal is to simultaneously adapt to both the sequence of gradients and the comparator. We first develop parameter-free and scale-free algorithms for a simplified setting with hints. We present two versions: the first adapts to the squared norms of both comparator and gradients separately using $O(d)$ time per round, the second adapts to their squared inner products (which measure variance only in the comparator direction) in time $O(d^3)$ per round. We then generalize two prior reductions to the unbounded setting; one to not need hints, and a second to deal with the range ratio problem (which already arises in prior work). We discuss their optimality in light of prior and new lower bounds. We apply our methods to obtain sharper regret bounds for scale-invariant online prediction with linear models."}}
{"id": "qAqXswdSxGr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Safe Testing", "abstract": "We develop the theory of hypothesis testing based on the E-value, a notion of evidence that, unlike the p-value, allows for effortlessly combining results from several studies in the common scenario where the decision to perform a new study may depend on previous outcomes. Tests based on E-values are safe, i.e. they preserve Type-I error guarantees, under such optional continuation. We define growth-rate optimality (GRO) as an analogue of power in an optional continuation context, and we show how to construct GRO E-variables for general testing problems with composite null and alternative, emphasizing models with nuisance parameters. GRO E-values take the form of Bayes factors with special priors. We illustrate the theory using several classic examples including a one-sample safe t-test (in which the right Haar prior turns out to be GRO) and the 2x2 contingency table (in which the GRO prior is different from standard priors). Sharing Fisherian, Neymanian and Jeffreys-Bayesian interpretations, E-values and the corresponding tests may provide a methodology acceptable to adherents of all three schools."}}
{"id": "J9xxi-m1Dxk", "cdate": 1546300800000, "mdate": null, "content": {"title": "Non-Asymptotic Pure Exploration by Solving Games", "abstract": "Pure exploration (aka active testing) is the fundamental task of sequentially gathering information to answer a query about a stochastic environment. Good algorithms make few mistakes and take few samples. Lower bounds (for multi-armed bandit models with arms in an exponential family) reveal that the sample complexity is determined by the solution to an optimisation problem. The existing state of the art algorithms achieve asymptotic optimality by solving a plug-in estimate of that optimisation problem at each step. We interpret the optimisation problem as an unknown game, and propose sampling rules based on iterative strategies to estimate and converge to its saddle point. We apply no-regret learners to obtain the first finite confidence guarantees that are adapted to the exponential family and which apply to any pure exploration query and bandit structure. Moreover, our algorithms only use a best response oracle instead of fully solving the optimisation problem."}}
{"id": "IASC0c-koCm", "cdate": 1546300800000, "mdate": null, "content": {"title": "Efficient Algorithms for Minimax Decisions Under Tree-Structured Incompleteness", "abstract": "When decisions must be based on incomplete (coarsened) observations and the coarsening mechanism is unknown, a minimax approach offers the best guarantees on the decision maker\u2019s expected loss. Recent work has derived mathematical conditions characterizing minimax optimal decisions, but also found that computing such decisions is a difficult problem in general. This problem is equivalent to that of maximizing a certain conditional entropy expression. In this work, we present a highly efficient algorithm for the case where the coarsening mechanism can be represented by a tree, whose vertices are outcomes and whose edges are coarse observations."}}
{"id": "BkNfd_-_bS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Sequential Test for the Lowest Mean: From Thompson to Murphy Sampling", "abstract": "Learning the minimum/maximum mean among a finite set of distributions is a fundamental sub-problem in planning, game tree search and reinforcement learning. We formalize this learning task as the problem of sequentially testing how the minimum mean among a finite set of distributions compares to a given threshold. We develop refined non-asymptotic lower bounds, which show that optimality mandates very different sampling behavior for a low vs high true minimum. We show that Thompson Sampling and the intuitive Lower Confidence Bounds policy each nail only one of these cases. We develop a novel approach that we call Murphy Sampling. Even though it entertains exclusively low true minima, we prove that MS is optimal for both possibilities. We then design advanced self-normalized deviation inequalities, fueling more aggressive stopping rules. We complement our theoretical guarantees by experiments showing that MS works best in practice."}}
{"id": "AOG29IrXsuU", "cdate": 1514764800000, "mdate": null, "content": {"title": "Mixture Martingales Revisited with Applications to Sequential Tests and Confidence Intervals", "abstract": "This paper presents new deviation inequalities that are valid uniformly in time under adaptive sampling in a multi-armed bandit model. The deviations are measured using the Kullback-Leibler divergence in a given one-dimensional exponential family, and may take into account several arms at a time. They are obtained by constructing for each arm a mixture martingale based on a hierarchical prior, and by multiplying those martingales. Our deviation inequalities allow us to analyze stopping rules based on generalized likelihood ratios for a large class of sequential identification problems, and to construct tight confidence intervals for some functions of the means of the arms."}}
{"id": "rJWgZOWdZB", "cdate": 1483228800000, "mdate": null, "content": {"title": "Random Permutation Online Isotonic Regression", "abstract": "We revisit isotonic regression on linear orders, the problem of fitting monotonic functions to best explain the data, in an online setting. It was previously shown that online isotonic regression is unlearnable in a fully adversarial model, which lead to its study in the fixed design model. Here, we instead develop the more practical random permutation model. We show that the regret is bounded above by the excess leave-one-out loss for which we develop efficient algorithms and matching lower bounds. We also analyze the class of simple and popular forward algorithms and recommend where to look for algorithms for online isotonic regression on partial orders."}}
