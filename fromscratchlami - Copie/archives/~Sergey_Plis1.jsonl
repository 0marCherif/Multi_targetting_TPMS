{"id": "LNL6uJBfZuP", "cdate": 1683663222081, "mdate": 1683663222081, "content": {"title": "Can recurrent models know more than we do?", "abstract": "Model interpretation is an active research area, aiming to unravel the black box of  deep learning models. One common approach, saliency, leverages the gradients of the model to produce a per-input map highlighting the features most important for a correct prediction. However, saliency faces challenges in recurrent models due to the \u201cvanishing saliency\u201d problem: gradients decay significantly towards earlier time steps. We alleviate this problem and improve the quality of saliency maps by augmenting recurrent models with an attention mechanism. We validate our methodology on synthetic data and compare these results to previous work. This synthetic experiment quantitatively validates that our methodology effectively captures the underlying signal of the input data. To show that our work is valid in a real-world setting, we apply it to functional magnetic resonance imaging (fMRI) data consisting of individuals with and without a diagnosis of schizophrenia. fMRI is notoriously complicated and a perfect candidate to show that our method works even for complex, high-dimensional data. Specifically, we use our methodology to find the relevant temporal information of the subjects and connect our findings to current and past research."}}
{"id": "wKSw2SDTcq", "cdate": 1680307200000, "mdate": 1682534784072, "content": {"title": "Federated Analysis in COINSTAC Reveals Functional Network Connectivity and Spectral Links to Smoking and Alcohol Consumption in Nearly 2,000 Adolescent Brains", "abstract": "With the growth of decentralized/federated analysis approaches in neuroimaging, the opportunities to study brain disorders using data from multiple sites has grown multi-fold. One such initiative is the Neuromark, a fully automated spatially constrained independent component analysis (ICA) that is used to link brain network abnormalities among different datasets, studies, and disorders while leveraging subject-specific networks. In this study, we implement the neuromark pipeline in COINSTAC, an open-source neuroimaging framework for collaborative/decentralized analysis. Decentralized exploratory analysis of nearly 2000 resting-state functional magnetic resonance imaging datasets collected at different sites across two cohorts and co-located in different countries was performed to study the resting brain functional network connectivity changes in adolescents who smoke and consume alcohol. Results showed hypoconnectivity across the majority of networks including sensory, default mode, and subcortical domains, more for alcohol than smoking, and decreased low frequency power. These findings suggest that global reduced synchronization is associated with both tobacco and alcohol use. This proof-of-concept work demonstrates the utility and incentives associated with large-scale decentralized collaborations spanning multiple sites."}}
{"id": "pgBdXIV8aFY", "cdate": 1672531200000, "mdate": 1682534784435, "content": {"title": "SalientGrads: Sparse Models for Communication Efficient and Data Aware Distributed Federated Training", "abstract": "Federated learning (FL) enables the training of a model leveraging decentralized data in client sites while preserving privacy by not collecting data. However, one of the significant challenges of FL is limited computation and low communication bandwidth in resource limited edge client nodes. To address this, several solutions have been proposed in recent times including transmitting sparse models and learning dynamic masks iteratively, among others. However, many of these methods rely on transmitting the model weights throughout the entire training process as they are based on ad-hoc or random pruning criteria. In this work, we propose Salient Grads, which simplifies the process of sparse training by choosing a data aware subnetwork before training, based on the model-parameter's saliency scores, which is calculated from the local client data. Moreover only highly sparse gradients are transmitted between the server and client models during the training process unlike most methods that rely on sharing the entire dense model in each round. We also demonstrate the efficacy of our method in a real world federated learning application and report improvement in wall-clock communication time."}}
{"id": "vfUSpAFyEC", "cdate": 1667393654042, "mdate": null, "content": {"title": "Causal Learning through Deliberate Undersampling", "abstract": "Domain scientists interested in causal mechanisms are usually limited by the frequency at which they can collect the measurements of social, physical, or biological systems. A common and plausible assumption is that higher measurement frequencies are the only way to gain more informative data about  the underlying dynamical causal structure. This assumption is a strong driver for designing new, faster instruments, but such instruments might not be feasible or even possible. In this paper, we show that this assumption is incorrect: there are situations in which we can gain additional information about the causal structure by measuring more \\emph{slowly} than our current instruments. We present an algorithm that uses graphs at multiple measurement timescales to infer underlying causal structure, and show that inclusion of structures at slower timescales can nonetheless reduce the size of the equivalence class of possible causal structures. We provide simulation data about the probability of cases in which deliberate undersampling yields a gain, as well as the size of this gain. "}}
{"id": "hnqkGVw_jtA", "cdate": 1664737656764, "mdate": null, "content": {"title": "Information Bottleneck for Multi-Task LSTMs", "abstract": "Neural networks, which have had a profound effect on how researchers study complex phenomena, do so through a complex, nonlinear mathematical structure which can be difficult to interpret or understand. This is especially true for recurrent models, as their dynamic structure can be difficult to measure and analyze. However, interpretability is a key factor in understanding certain problems such as text and language analysis. In this paper, we present a novel introspection method for LSTMs trained to solve complex language problems, such as sentiment analysis. Inspired by Information Bottleneck theory, our method uses a state-of-the-art information theoretic framework to visualize shared information around labels, features, and between layers. We apply our approach on simulated data, and real sentiment analysis datasets, providing novel, information-theoretic insights into internal model dynamics."}}
{"id": "_qBTi54t-we", "cdate": 1664300345080, "mdate": null, "content": {"title": "Reducing Causal Illusions through Deliberate Undersampling", "abstract": "Domain scientists interested in the causal mechanisms are usually limited by the frequency at which they can collect the measurements of social, physical, or biological systems.\nIt is a reasonable assumption that higher frequency is more informative of the causal structure.\nThis assumption is a strong driver for designing new, faster instruments.\nA task that is expensive and often impossible at the current state of technology.\nIn this work, we show that counter to the intuition it is possible for causal systems to improve the estimation of causal graphs from undersampled time-series by augmenting the measurements with those collected at a rate slower than currently available.\nWe present an algorithm able to take advantage of measurement time-scale graphs estimated from data at various sampling rates and lower the underdeterminacy of the system by reducing the equivalence size.\nWe investigate the probability of cases in which deliberate undersampling yields a gain and the size of this gain. "}}
{"id": "qjdr2QiBIWR", "cdate": 1664300344525, "mdate": null, "content": {"title": "GRACE-C: Generalized Rate Agnostic Causal Estimation via Constraints", "abstract": "Graphical structures estimated by causal learning algorithms from time series data can provide highly misleading causal information if the causal timescale of the generating process fails to match the measurement timescale of the data. Existing algorithms provide limited resources to respond to this challenge, and so researchers must either use models that they know are likely misleading, or else forego causal learning entirely. Existing methods face up-to-four distinct shortfalls, as they might a) require that the difference between causal and measurement timescales is known; b) only handle very small number of random variables when the timescale difference is unknown; c) only apply to pairs of variables (albeit with fewer assumptions about prior knowledge); or d) be unable to find a solution given statistical noise in the data. This paper addresses all four challenges. Our algorithm combines constraint programming with both theoretical insights into the problem structure and prior information about admissible causal interactions to achieve gains of multiple orders of magnitude in speed and informativeness. The resulting system scales to significantly larger sets of random variables (>100) without knowledge of the timescale difference while maintaining  theoretical guarantees. This method is also robust to edge misidentification and can use parametric connection strengths, while optionally finding the optimal among many possible solutions. "}}
{"id": "H73xwqPfW2f", "cdate": 1663850139376, "mdate": null, "content": {"title": "Multitask Reinforcement Learning by Optimizing Neural Pathways", "abstract": "Reinforcement learning (RL) algorithms have achieved great success in learning specific tasks, as evidenced by examples such as AlphaGo or fusion control. However, it is still difficult for an RL agent to learn how to solve multiple tasks. In this paper, we propose a novel multitask learning framework, in which multiple specialized pathways through a single network are trained simultaneously, with each pathway focusing on a single task. We show that this approach achieves competitive performance with existing multitask RL methods, while using only 5% of the number of neurons per task. We demonstrate empirically the success of our approach on several continuous control tasks, in both online and offline training."}}
{"id": "B_pCIsX8KL_", "cdate": 1663849880362, "mdate": null, "content": {"title": "GRACE-C: Generalized Rate Agnostic Causal Estimation via Constraints", "abstract": "Graphical structures estimated by causal learning algorithms from time series data can provide highly misleading causal information if the causal timescale of the generating process fails to match the measurement timescale of the data. Existing algorithms provide limited resources to respond to this challenge, and so researchers must either use models that they know are likely misleading, or else forego causal learning entirely. Existing methods face up-to-four distinct shortfalls, as they might a) require that the difference between causal and measurement timescales is known; b) only handle very small number of random variables when the timescale difference is unknown; c) only apply to pairs of variables (albeit with fewer assumptions about prior knowledge); or d) be unable to find a solution given statistical noise in the data. This paper aims to address these challenges. We present an approach that combines constraint programming with both theoretical insights into the problem structure and prior information about admissible causal interactions to achieve speed up of multiple orders of magnitude. The resulting system scales to significantly larger sets of random variables ($>100$) without knowledge of the timescale difference while maintaining  theoretical guarantees. This method is also robust to edge misidentification and can use parametric connection strengths, while optionally finding the optimal among many possible solutions."}}
{"id": "rtIeCBr8W-5", "cdate": 1646515526116, "mdate": null, "content": {"title": "Geometrically Guided Saliency Maps", "abstract": "Interpretability methods for deep neural networks mainly focus on modifying the rules of automatic differentiation or perturbing the input and observing the score drop to determine the most relevant features. Among them, gradient-based attribution methods, such as saliency maps, are arguably the most popular. Still, the produced saliency maps may often lack intelligibility. We address this problem based on recent discoveries in geometric properties of deep neural networks' loss landscape that reveal the existence of a multiplicity of local minima in the vicinity of a trained model's loss surface. We introduce two methods that leverage the geometry of the loss landscape to improve interpretability: 1) \"Geometrically Guided Integrated Gradients\", applying gradient ascent to each interpolation point of the linear path as a guide. 2) \"Geometric Ensemble Gradients\", generating ensemble saliency maps by sampling proximal iso-loss models. Compared to vanilla and integrated gradients, these methods significantly improve saliency maps in quantitative and visual terms. We verify our findings on MNIST and Imagenet datasets across convolutional, ResNet, and Inception V3 architectures."}}
