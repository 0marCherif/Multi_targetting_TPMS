{"id": "5R96mIU85IW", "cdate": 1663850390408, "mdate": null, "content": {"title": "Effectively using  public data in privacy preserving Machine learning", "abstract": "A key challenge towards differentially private machine learning is balancing the trade-off between privacy and utility. \nA recent line of work has demonstrated that leveraging  \\emph{public data samples} can enhance the utility of DP-trained models (for the same privacy guarantees). \nIn this work, we show that public data can be used to improve utility in DP models significantly more than shown in recent works.  \nTowards this end, we introduce a modified DP-SGD algorithm that leverages public data during its training process. \nOur technique uses public data in two complementary ways: (1) it uses generative models trained on public data to produce synthetic data that is effectively embedded in multiple steps of the training pipeline; (2) it uses a new gradient clipping mechanism  (required for achieving differential privacy) which changes the \\emph{origin} of gradient vectors using information inferred from available public and synthesized data. \nOur experimental results demonstrate the effectiveness of our approach in improving the state-of-the-art in differentially private machine learning across multiple datasets, network architectures, and application domains. \nNotably, we achieve a $75\\%$ accuracy on CIFAR10  when using only $2,000$ public images;  this is \\emph{significantly higher} than the  state-of-the-art which is $68\\%$  for DP-SGD with the privacy budget of $\\varepsilon=2,\\delta=10^{-5}$ (given the same number of public data points)."}}
{"id": "eKkpcdSA52W", "cdate": 1631893917417, "mdate": null, "content": {"title": "A Novel Self-Distillation Architecture to Defeat Membership Inference Attacks", "abstract": "Membership inference attacks are a key measure to evaluate privacy leakage in machine learning (ML) models, which aim to distinguish training members from non-members by exploiting differential behavior of the models on member and non-member inputs. We propose a new framework to train privacy-preserving models that induces similar behavior on member and non-member inputs to mitigate practical membership inference attacks. Our framework, called SELENA, has two major components. The first component and the core of our defense, called Split-AI, is a novel ensemble architecture for training. We prove that our Split-AI architecture defends against a large family of membership inference attacks, however, it is susceptible to new adaptive attacks. Therefore, we use a second component in our framework called Self-Distillation to protect against such stronger attacks, which (self-)distills the training dataset through our Split-AI ensemble and has no reliance on external public datasets. We perform extensive experiments on major benchmark datasets and the results show that our approach achieves a better trade-off between membership privacy and utility compared to previous defenses."}}
