{"id": "F_9w7Wl78IH", "cdate": 1652737867268, "mdate": null, "content": {"title": "The Impact of Task Underspecification in Evaluating Deep Reinforcement Learning", "abstract": "Evaluations of Deep Reinforcement Learning (DRL) methods are an integral part of scientific progress of the field. Beyond designing DRL methods for general intelligence, designing task-specific methods is becoming increasingly prominent for real-world applications. In these settings, the standard evaluation practice involves using a few instances of Markov Decision Processes (MDPs) to represent the task. However, many tasks induce a large family of MDPs owing to variations in the underlying environment, particularly in real-world contexts. For example, in traffic signal control, variations may stem from intersection geometries and traffic flow levels. The select MDP instances may thus inadvertently cause overfitting, lacking the statistical power to draw conclusions about the method's true performance across the family. In this article, we augment DRL evaluations to consider parameterized families of MDPs. We show that in comparison to evaluating DRL methods on select MDP instances, evaluating the MDP family often yields a substantially different relative ranking of methods, casting doubt on what methods should be considered state-of-the-art. We validate this phenomenon in standard control benchmarks and the real-world application of traffic signal control. At the same time, we show that accurately evaluating on an MDP family is nontrivial. Overall, this work identifies new challenges for empirical rigor in reinforcement learning, especially as the outcomes of DRL trickle into downstream decision-making."}}
{"id": "ptatMLkxX-", "cdate": 1640995200000, "mdate": 1667487061782, "content": {"title": "The Impact of Task Underspecification in Evaluating Deep Reinforcement Learning", "abstract": "Evaluations of Deep Reinforcement Learning (DRL) methods are an integral part of scientific progress of the field. Beyond designing DRL methods for general intelligence, designing task-specific methods is becoming increasingly prominent for real-world applications. In these settings, the standard evaluation practice involves using a few instances of Markov Decision Processes (MDPs) to represent the task. However, many tasks induce a large family of MDPs owing to variations in the underlying environment, particularly in real-world contexts. For example, in traffic signal control, variations may stem from intersection geometries and traffic flow levels. The select MDP instances may thus inadvertently cause overfitting, lacking the statistical power to draw conclusions about the method's true performance across the family. In this article, we augment DRL evaluations to consider parameterized families of MDPs. We show that in comparison to evaluating DRL methods on select MDP instances, evaluating the MDP family often yields a substantially different relative ranking of methods, casting doubt on what methods should be considered state-of-the-art. We validate this phenomenon in standard control benchmarks and the real-world application of traffic signal control. At the same time, we show that accurately evaluating on an MDP family is nontrivial. Overall, this work identifies new challenges for empirical rigor in reinforcement learning, especially as the outcomes of DRL trickle into downstream decision-making."}}
{"id": "UJrQ_DbHvp", "cdate": 1640995200000, "mdate": 1667487061808, "content": {"title": "The Braess Paradox in Dynamic Traffic", "abstract": "The Braess's Paradox (BP) is the observation that adding one or more roads to the existing road network will counter-intuitively increase traffic congestion and slow down the overall traffic flow. Previously, the existence of the BP is modeled using the static traffic assignment model, which solves for the user equilibrium subject to network flow conservation to find the equilibrium state and distributes all vehicles instantaneously. Such approach neglects the dynamic nature of real-world traffic, including vehicle behaviors and the interaction between vehicles and the infrastructure. As such, this article proposes a dynamic traffic network model and empirically validates the existence of the BP under dynamic traffic. In particular, we use microsimulation environment to study the impacts of an added path on a grid network. We explore how the network flow, vehicle travel time, and network capacity respond, as well as when the BP will occur."}}
{"id": "TsYCtHTCVKc", "cdate": 1640995200000, "mdate": 1667487061788, "content": {"title": "Learning Eco-Driving Strategies at Signalized Intersections", "abstract": "Signalized intersections in arterial roads result in persistent vehicle idling and excess accelerations, contributing to fuel consumption and CO2 emissions. There has thus been a line of work studying eco-driving control strategies to reduce fuel consumption and emission levels at intersections. However, methods to devise effective control strategies across a variety of traffic settings remain elusive. In this paper, we propose a reinforcement learning (RL) approach to learn effective eco-driving control strategies. We analyze the potential impact of a learned strategy on fuel consumption, CO2 emission, and travel time and compare with naturalistic driving and model-based baselines. We further demonstrate the generalizability of the learned policies under mixed traffic scenarios. Simulation results indicate that scenarios with 100% penetration of connected autonomous vehicles (CAV) may yield as high as 18% reduction in fuel consumption and 25% reduction in CO2 emission levels while even improving travel speed by 20%. Furthermore, results indicate that even 25% CAV penetration can bring at least 50% of the total fuel and emission reduction benefits."}}
{"id": "9KZEz9xssI", "cdate": 1640995200000, "mdate": 1667487061807, "content": {"title": "Learning Eco-Driving Strategies at Signalized Intersections", "abstract": "Signalized intersections in arterial roads result in persistent vehicle idling and excess accelerations, contributing to fuel consumption and CO <inf xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</inf> emissions. There has thus been a line of work studying eco-driving control strategies to reduce fuel consumption and emission levels at intersections. However, methods to devise effective control strategies across a variety of traffic settings remain elusive. In this paper, we propose a reinforcement learning (RL) approach to learn effective eco-driving control strategies. We analyze the potential impact of a learned strategy on fuel consumption, CO <inf xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</inf> emission, and travel time and compare with naturalistic driving and model-based baselines. We further demonstrate the generalizability of the learned policies under mixed traffic scenarios. Simulation results indicate that scenarios with 100% penetration of connected autonomous vehicles (CAV) may yield as high as 18% reduction in fuel consumption and 25% reduction in CO <inf xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</inf> emission levels while even improving travel speed by 20%. Furthermore, results indicate that even 25% CAV penetration can bring at least 50% of the total fuel and emission reduction benefits."}}
{"id": "pXhRVZtQhv", "cdate": 1609459200000, "mdate": 1667487061816, "content": {"title": "Fleet management for ride-pooling with meeting points at scale: a case study in the five boroughs of New York City", "abstract": "Introducing meeting points to ride-pooling (RP) services has been shown to increase the satisfaction level of both riders and service providers. Passengers may choose to walk to a meeting point for a cost reduction. Drivers may also get matched with more riders without making additional stops. There are economic benefits of using ride-pooling with meeting points (RPMP) compared to the traditional RP services. Many RPMP models have been proposed to better understand their benefits. However, most prior works study RPMP either with a restricted set of parameters or at a small scale due to the expensive computation involved. In this paper, we propose STaRS+, a scalable RPMP framework that is based on a comprehensive integer linear programming model. The high scalability of STaRS+ is achieved by utilizing a heuristic optimization strategy along with a novel shortest-path caching scheme. We applied our model to the NYC metro area to evaluate the scalability of the framework and demonstrate the importance of city-scale simulations. Our results show that city-scale simulations can reveal valuable insights for city planners that are not always visible at smaller scales. To the best of our knowledge, STaRS+ is the first study on the RPMP that can solve large-scale instances on the order of the entire NYC metro area."}}
{"id": "9HIJfSmYqRw", "cdate": 1609459200000, "mdate": 1667487061789, "content": {"title": "Mixed Autonomous Supervision in Traffic Signal Control", "abstract": "Traffic signal control is a critical component for ensuring smooth traffic flows in city corridors. To this end, deep reinforcement learning (RL) agents have recently been proposed. These learned black-box policies may out-perform manually tuned policies on average, but they remain imperfect and come at a cost of how easy they are to interpret and supervise. These challenges hinder their adoption in real-world systems. To address the first challenge, this paper devises naturally interpretable decision tree policies that imitate expert deep neural network policies. To address supervision, we define a new formalization called Mixed Autonomous Supervision (MAS), which concerns integrating an imperfect policy into an existing supervision system. We propose a two-part supervision model with online automated supervision and offline human supervision to implement MAS. We present a novel blind spot detection algorithm for decision tree policies to encourage the safe transfer of control to an automated fail-safe policy (online supervision) and an interactive dashboard DTLight for offline human supervision. We show the decision tree policies are just as performant as the RL policies, and the proposed supervision model has a significant benefit in scenarios derived from real traffic situations."}}
