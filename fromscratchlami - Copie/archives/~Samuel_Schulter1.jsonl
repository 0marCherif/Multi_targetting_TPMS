{"id": "21FNoED07B", "cdate": 1696382905153, "mdate": 1696382905153, "content": {"title": "Efficient Controllable Multi-Task Architectures", "abstract": "We aim to train a multi-task model such that users can\nadjust the desired compute budget and relative importance\nof task performances after deployment, without retraining.\nThis enables optimizing performance for dynamically varying user needs, without heavy computational overhead to\ntrain and save models for various scenarios. To this end,\nwe propose a multi-task model consisting of a shared encoder and task-specific decoders where both encoder and\ndecoder channel widths are slimmable. Our key idea is to\ncontrol the task importance by varying the capacities of\ntask-specific decoders, while controlling the total computational cost by jointly adjusting the encoder capacity. This\nimproves overall accuracy by allowing a stronger encoder\nfor a given budget, increases control over computational\ncost, and delivers high-quality slimmed sub-architectures\nbased on user\u2019s constraints. Our training strategy involves a\nnovel \u2018Configuration-Invariant Knowledge Distillation\u2019 loss\nthat enforces backbone representations to be invariant under\ndifferent runtime width configurations to enhance accuracy.\nFurther, we present a simple but effective search algorithm\nthat translates user constraints to runtime width configurations of both the shared encoder and task decoders, for\nsampling the sub-architectures. The key rule for the search\nalgorithm is to provide a larger computational budget to\nthe higher preferred task decoder, while searching a shared\nencoder configuration that enhances the overall MTL performance. Various experiments on three multi-task benchmarks\n(PASCALContext, NYUDv2, and CIFAR100-MTL) with diverse backbone architectures demonstrate the advantage of\nour approach. For example, our method shows a higher\ncontrollability by \u223c 33.5% in the NYUD-v2 dataset over\nprior methods, while incurring much less compute cost."}}
{"id": "ENqS6t_8gDQ", "cdate": 1667353613049, "mdate": 1667353613049, "content": {"title": "Shuffle and Attend: Video Domain Adaptation", "abstract": "We address the problem of domain adaptation in videos for the task of human action recognition. Inspired by image-based domain adaptation, we can perform video adaptation by aligning the features of frames or clips of source and target videos. However, equally aligning all clips is suboptimal as not all clips are informative for the task. As the first novelty, we propose an attention mechanism which focuses on more discriminative clips and directly optimizes for video-level (cf. clip-level) alignment. As the backgrounds are often very different between source and target, the source background-corrupted model adapts poorly to target domain videos. To alleviate this, as a second novelty, we propose to use the clip order prediction as an auxiliary task. The clip order prediction loss, when combined with domain adversarial loss, encourages learning of representations which focus on the humans and objects involved in the actions, rather than the uninformative and widely differing (between source and target) backgrounds. We empirically show that both components contribute positively towards adaptation performance. We report state-of-the-art performances on two out of three challenging public benchmarks, two based on the UCF and HMDB datasets, and one on Kinetics to NEC-Drone datasets. We also support the intuitions and the results with qualitative results.\n\n"}}
{"id": "VL3EYbwTG4V", "cdate": 1649719876205, "mdate": 1649719876205, "content": {"title": "Controllable Dynamic Multi-Task Architectures", "abstract": "Multi-task learning commonly encounters competition for resources among tasks, specifically when model capacity is limited. This challenge motivates models which allow control over the relative importance of tasks and total compute cost during inference time. In this work, we propose such a controllable multi-task network that dynamically adjusts its architecture and weights to match the desired task preference as well as the resource constraints. In contrast to the existing dynamic multi-task approaches that adjust only the weights within a fixed architecture, our approach affords the flexibility to dynamically control the total computational cost and match the user-preferred task importance better. We propose a disentangled training of two hypernetworks, by exploiting task affinity and a novel branching regularized loss, to take input preferences and accordingly predict tree-structured models with adapted weights. Experiments on three multi-task benchmarks, namely PASCAL-Context, NYU-v2, and CIFAR-100, show the efficacy of our approach."}}
{"id": "uXSYoLhh73", "cdate": 1640995200000, "mdate": 1668272750589, "content": {"title": "Single-Stream Multi-level Alignment for Vision-Language Pretraining", "abstract": "Self-supervised vision-language pretraining from pure images and text with a contrastive loss is effective, but ignores fine-grained alignment due to a dual-stream architecture that aligns image and text representations only on a global level. Earlier, supervised, non-contrastive methods were capable of finer-grained alignment, but required dense annotations that were not scalable. We propose a single stream architecture that aligns images and language at multiple levels: global, fine-grained patch-token, and conceptual/semantic, using two novel tasks: symmetric cross-modality reconstruction (XMM) and a pseudo-labeled key word prediction (PSL). In XMM, we mask input tokens from one modality and use cross-modal information to reconstruct the masked token, thus improving fine-grained alignment between the two modalities. In PSL, we use attention to select keywords in a caption, use a momentum encoder to recommend other important keywords that are missing from the caption but represented in the image, and then train the visual encoder to predict the presence of those keywords, helping it learn semantic concepts that are essential for grounding a textual token to an image region. We demonstrate competitive performance and improved data efficiency on image-text retrieval, grounding, visual question answering/reasoning against larger models and models trained on more data. Code and models available at zaidkhan.me/SIMLA ."}}
{"id": "QiuDFV7eNFp", "cdate": 1640995200000, "mdate": 1668272750408, "content": {"title": "Controllable Dynamic Multi-Task Architectures", "abstract": "Multi-task learning commonly encounters competition for resources among tasks, specifically when model capac-ity is limited. This challenge motivates models which al-low control over the relative importance of tasks and total compute cost during inference time. In this work, we pro-pose such a controllable multi-task network that dynami-cally adjusts its architecture and weights to match the de-sired task preference as well as the resource constraints. In contrast to the existing dynamic multi-task approaches that adjust only the weights within a fixed architecture, our approach affords the flexibility to dynamically control the total computational cost and match the user-preferred task importance better. We propose a disentangled training of two hype rnetwo rks, by exploiting task affinity and a novel branching regularized loss, to take input prefer-ences and accordingly predict tree-structured models with adapted weights. Experiments on three multi-task bench-marks, namely PASCAL-Context, NYU-v2, and CIFAR-100, show the efficacy of our approach. Project page is available at https://www.nec-labs.com/-mas/DYMU."}}
{"id": "Mis22EHOhxO", "cdate": 1640995200000, "mdate": 1668272750414, "content": {"title": "Exploiting Unlabeled Data with Vision and Language Models for Object Detection", "abstract": "Building robust and generic object detection frameworks requires scaling to larger label spaces and bigger training datasets. However, it is prohibitively costly to acquire annotations for thousands of categories at a large scale. We propose a novel method that leverages the rich semantics available in recent vision and language models to localize and classify objects in unlabeled images, effectively generating pseudo labels for object detection. Starting with a generic and class-agnostic region proposal mechanism, we use vision and language models to categorize each region of an image into any object category that is required for downstream tasks. We demonstrate the value of the generated pseudo labels in two specific tasks, open-vocabulary detection, where a model needs to generalize to unseen object categories, and semi-supervised object detection, where additional unlabeled images can be used to improve the model. Our empirical evaluation shows the effectiveness of the pseudo labels in both tasks, where we outperform competitive baselines and achieve a novel state-of-the-art for open-vocabulary object detection. Our code is available at https://github.com/xiaofeng94/VL-PLM ."}}
{"id": "F-avtTm-eG3", "cdate": 1640995200000, "mdate": 1668272750531, "content": {"title": "MM-TTA: Multi-Modal Test-Time Adaptation for 3D Semantic Segmentation", "abstract": "Test-time adaptation approaches have recently emerged as a practical solution for handling domain shift without access to the source domain data. In this paper, we propose and explore a new multi-modal extension of test-time adaptation for 3D semantic segmentation. We find that, directly applying existing methods usually results in performance instability at test time, because multi-modal input is not considered jointly. To design a framework that can take full advantage of multi-modality, where each modality provides regularized self-supervisory signals to other modalities, we propose two complementary modules within and across the modalities. First, Intra-modal Pseudo-label Generation (Intra-PG) is introduced to obtain reliable pseudo labels within each modality by aggregating information from two models that are both pre-trained on source data but updated with target data at different paces. Second, Inter-modal Pseudo-label Refinement (Inter-PR) adaptively selects more reliable pseudo labels from different modalities based on a proposed consistency scheme. Experiments demonstrate that our regularized pseudo labels produce stable self-learning signals in numerous multi-modal test-time adaptation scenarios for 3D semantic segmentation. Visit our project website at https://www.nec-labs.com/~mas/MM-TTA"}}
{"id": "4ghZRNl7cf", "cdate": 1640995200000, "mdate": 1668272750296, "content": {"title": "On Generalizing Beyond Domains in Cross-Domain Continual Learning", "abstract": "Humans have the ability to accumulate knowledge of new tasks in varying conditions, but deep neural networks of-ten suffer from catastrophic forgetting of previously learned knowledge after learning a new task. Many recent methods focus on preventing catastrophic forgetting under the assumption of train and test data following similar distributions. In this work, we consider a more realistic scenario of continual learning under domain shifts where the model must generalize its inference to an unseen domain. To this end, we encourage learning semantically meaningful features by equipping the classifier with class similarity metrics as learning parameters which are obtained through Mahalanobis similarity computations. Learning of the backbone representation along with these extra parameters is done seamlessly in an end-to-end manner. In addition, we propose an approach based on the exponential moving average of the parameters for better knowledge distillation. We demonstrate that, to a great extent, existing continual learning algorithms fail to handle the forgetting issue under multiple distributions, while our proposed approach learns new tasks under domain shift with accuracy boosts up to 10% on challenging datasets such as DomainNet and OfficeHome."}}
{"id": "iLxhNEyIFf5", "cdate": 1609459200000, "mdate": 1668272750400, "content": {"title": "Domain Adaptation for Semantic Segmentation via Patch-Wise Contrastive Learning", "abstract": "We introduce a novel approach to unsupervised and semi-supervised domain adaptation for semantic segmentation. Unlike many earlier methods that rely on adversarial learning for feature alignment, we leverage contrastive learning to bridge the domain gap by aligning the features of structurally similar label patches across domains. As a result, the networks are easier to train and deliver better performance. Our approach consistently outperforms state-of-the-art unsupervised and semi-supervised methods on two challenging domain adaptive segmentation tasks, particularly with a small number of target domain annotations. It can also be naturally extended to weakly-supervised domain adaptation, where only a minor drop in accuracy can save up to 75% of annotation cost."}}
{"id": "wTeDJ9rOKx", "cdate": 1577836800000, "mdate": 1668272750588, "content": {"title": "Domain Adaptive Semantic Segmentation Using Weak Labels", "abstract": "Learning semantic segmentation models requires a huge amount of pixel-wise labeling. However, labeled data may only be available abundantly in a domain different from the desired target domain, which only has minimal or no annotations. In this work, we propose a novel framework for domain adaptation in semantic segmentation with image-level weak labels in the target domain. The weak labels may be obtained based on a model prediction for unsupervised domain adaptation (UDA), or from a human annotator in a new weakly-supervised domain adaptation (WDA) paradigm for semantic segmentation. Using weak labels is both practical and useful, since (i) collecting image-level target annotations is comparably cheap in WDA and incurs no cost in UDA, and (ii) it opens the opportunity for category-wise domain alignment. Our framework uses weak labels to enable the interplay between feature alignment and pseudo-labeling, improving both in the process of domain adaptation. Specifically, we develop a weak-label classification module to enforce the network to attend to certain categories, and then use such training signals to guide the proposed category-wise alignment method. In experiments, we show considerable improvements with respect to the existing state-of-the-arts in UDA and present a new benchmark in the WDA setting. Project page is at http://www.nec-labs.com/~mas/WeakSegDA ."}}
