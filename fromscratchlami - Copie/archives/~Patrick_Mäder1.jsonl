{"id": "5R4rj3gxE-X", "cdate": 1667374868716, "mdate": 1667374868716, "content": {"title": "StickyPillars: Robust and Efficient Feature Matching on Point Clouds using Graph Neural Networks", "abstract": "Robust point cloud registration in real-time is an impor-\ntant prerequisite for many mapping and localization algo-\nrithms. Traditional methods like ICP tend to fail without\ngood initialization, insufficient overlap or in the presence of\ndynamic objects. Modern deep learning based registration\napproaches present much better results, but suffer from a\nheavy runtime. We overcome these drawbacks by introduc-\ning StickyPillars, a fast, accurate and extremely robust deep\nmiddle-end 3D feature matching method on point clouds.\nIt uses graph neural networks and performs context aggre-\ngation on sparse 3D key-points with the aid of transformer\nbased multi-head self and cross-attention. The network out-\nput is used as the cost for an optimal transport problem\nwhose solution yields the final matching probabilities. The\nsystem does not rely on hand crafted feature descriptors or\nheuristic matching strategies. We present state-of-art art ac-\ncuracy results on the registration problem demonstrated on\nthe KITTI dataset while being four times faster then leading\ndeep methods. Furthermore, we integrate our matching sys-\ntem into a LiDAR odometry pipeline yielding most accurate\nresults on the KITTI odometry dataset. Finally, we demon-\nstrate robustness on KITTI odometry. Our method remains\nstable in accuracy where state-of-the-art procedures fail on\nframe drops and higher speeds."}}
{"id": "zglnNwLN-V", "cdate": 1667374482899, "mdate": 1667374482899, "content": {"title": "StickyLocalization: Robust End-To-End Relocalization on Points Clouds using Graph Neural Networks", "abstract": "Relocalization inside pre-built maps provides a big benefit\nin the course of today\u2019s autonomous driving tasks where the\nmap can be considered as an additional sensor for refining\nthe estimated current pose of the vehicle. Due to potentially\nlarge drifts in the initial pose guess as well as maps con-\ntaining unfiltered dynamic and temporal static objects (e.g.\nparking cars), traditional methods like ICP tend to fail and\nshow high computation times. We propose a novel and fast\nrelocalization method for accurate pose estimation inside\na pre-built map based on 3D point clouds. The method is\nrobust against inaccurate initialization caused by low perfor-\nmance GPS systems and tolerates the presence of unfiltered\nobjects by specifically learning to extract significant features\nfrom current scans and adjacent map sections. More specif-\nically, we introduce a novel distance-based matching loss\nenabling us to simultaneously extract important information\nfrom raw point clouds and aggregating inner- and inter-\ncloud context by utilizing self- and cross-attention inside a\nGraph Neural Network. We evaluate StickyLocalization\u2019s\n(SL) performance through an extensive series of experiments\nusing two benchmark datasets in terms of Relocalization\non NuScenes and Loop Closing using KITTI\u2019s Odometry\ndataset. We found that SL outperforms state-of-the art point\ncloud registration and relocalization methods in terms of\ntransformation errors and runtime."}}
{"id": "6vSDzn-4FlW", "cdate": 1632875723126, "mdate": null, "content": {"title": "Synaptic Diversity in ANNs Can Facilitate Faster Learning", "abstract": "Various advancements in artificial neural networks (ANNs) are inspired by biological concepts, e.g., the artificial neuron, an efficient model of biological nerve cells demonstrating learning capabilities on large amounts of data. More recent inspirations with promising results are advanced regularization techniques, e.g., synaptic scaling, and backpropagation alternatives, e.g., Targetprop. While neurosciences continuously discover and better understand the mechanisms of biological neural networks (BNNs), new opportunities for a transfer of these concepts towards ANNs arise. However, only few concepts are readily applicable and improvements for ANNs are far from being guaranteed. In this paper, we focus on the inhomogeneous and dynamically changing structures of BNNs in contrast to mostly homogeneous and fix topologies of ANNs. More specifically, we transfer concepts of synaptic diversity, namely spontaneous synaptic remodeling, diversity in synaptic plasticity and multi-synaptic connectivity to ANNs. We observe ANNs enhanced by synaptic diversity concepts to learn faster, to predict with higher accuracy and to be more resilient to gradient inversion attacks. Our proposed methods are easily applicable to existing ANN topologies and are therefore supposed to stimulate an adaptation of and further research into these mechanisms."}}
{"id": "iPFvQwo9NJa", "cdate": 1609459200000, "mdate": null, "content": {"title": "OmniDet: Surround View Cameras Based Multi-Task Visual Perception Network for Autonomous Driving", "abstract": "Surround View fisheye cameras are commonly deployed in automated driving for 360 <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$^\\circ$</tex-math></inline-formula> near-field sensing around the vehicle. This work presents a multi-task visual perception network on unrectified fisheye images to enable the vehicle to sense its surrounding environment. It consists of six primary tasks necessary for an autonomous driving system: depth estimation, visual odometry, semantic segmentation, motion segmentation, object detection, and lens soiling detection. We demonstrate that the jointly trained model performs better than the respective single task versions. Our multi-task model has a shared encoder providing a significant computational advantage and has synergized decoders where tasks support each other. We propose a novel camera geometry based adaptation mechanism to encode the fisheye distortion model both at training and inference. This was crucial to enable training on the WoodScape dataset, comprised of data from different parts of the world collected by 12 different cameras mounted on three different cars with different intrinsics and viewpoints. Given that bounding boxes is not a good representation for distorted fisheye images, we also extend object detection to use a polygon with non-uniformly sampled vertices. We additionally evaluate our model on standard automotive datasets, namely KITTI and Cityscapes. We obtain the state-of-the-art results on KITTI for depth estimation and pose estimation tasks and competitive performance on the other tasks. We perform extensive ablation studies on various architecture choices and task weighting methodologies. A short video at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://youtu.be/xbSjZ5OfPes</uri> provides qualitative results."}}
{"id": "VOy9rNNTJQN", "cdate": 1609459200000, "mdate": null, "content": {"title": "BEVDetNet: Bird's Eye View LiDAR Point Cloud based Real-time 3D Object Detection for Autonomous Driving", "abstract": "3D object detection based on LiDAR point clouds is a crucial module in autonomous driving particularly for long range sensing. Most of the research is focused on achieving higher accuracy and these models are not optimized for deployment on embedded systems from the perspective of latency and power efficiency. For high speed driving scenarios, latency is a crucial parameter as it provides more time to react to dangerous situations. Typically a voxel or point-cloud based 3D convolution approach is utilized for this module. Firstly, they are inefficient on embedded platforms as they are not suitable for efficient parallelization. Secondly, they have a variable runtime due to level of sparsity of the scene which is against the determinism needed in a safety system. In this work, we aim to develop a very low latency algorithm with fixed runtime. We propose a novel semantic segmentation architecture as a single unified model for object center detection using key points, box predictions and orientation prediction using binned classification in a simpler Bird's Eye View (BEV) 2D representation. The proposed architecture can be trivially extended to include semantic segmentation classes like road without any additional computation. The proposed model has a latency of 4 ms on the embedded Nvidia Xavier platform. The model is 5X faster than other top accuracy models with a minimal accuracy degradation of 2% in Average Precision at IoU=0.5 on KITTI dataset."}}
{"id": "Ri7Cp0Q3keb", "cdate": 1609459200000, "mdate": null, "content": {"title": "OmniDet: Surround View Cameras based Multi-task Visual Perception Network for Autonomous Driving", "abstract": "Surround View fisheye cameras are commonly deployed in automated driving for 360\\deg{} near-field sensing around the vehicle. This work presents a multi-task visual perception network on unrectified fisheye images to enable the vehicle to sense its surrounding environment. It consists of six primary tasks necessary for an autonomous driving system: depth estimation, visual odometry, semantic segmentation, motion segmentation, object detection, and lens soiling detection. We demonstrate that the jointly trained model performs better than the respective single task versions. Our multi-task model has a shared encoder providing a significant computational advantage and has synergized decoders where tasks support each other. We propose a novel camera geometry based adaptation mechanism to encode the fisheye distortion model both at training and inference. This was crucial to enable training on the WoodScape dataset, comprised of data from different parts of the world collected by 12 different cameras mounted on three different cars with different intrinsics and viewpoints. Given that bounding boxes is not a good representation for distorted fisheye images, we also extend object detection to use a polygon with non-uniformly sampled vertices. We additionally evaluate our model on standard automotive datasets, namely KITTI and Cityscapes. We obtain the state-of-the-art results on KITTI for depth estimation and pose estimation tasks and competitive performance on the other tasks. We perform extensive ablation studies on various architecture choices and task weighting methodologies. A short video at https://youtu.be/xbSjZ5OfPes provides qualitative results."}}
{"id": "ETulUcOekOl", "cdate": 1609459200000, "mdate": null, "content": {"title": "SVDistNet: Self-Supervised Near-Field Distance Estimation on Surround View Fisheye Cameras", "abstract": "A 360{\\deg} perception of scene geometry is essential for automated driving, notably for parking and urban driving scenarios. Typically, it is achieved using surround-view fisheye cameras, focusing on the near-field area around the vehicle. The majority of current depth estimation approaches focus on employing just a single camera, which cannot be straightforwardly generalized to multiple cameras. The depth estimation model must be tested on a variety of cameras equipped to millions of cars with varying camera geometries. Even within a single car, intrinsics vary due to manufacturing tolerances. Deep learning models are sensitive to these changes, and it is practically infeasible to train and test on each camera variant. As a result, we present novel camera-geometry adaptive multi-scale convolutions which utilize the camera parameters as a conditional input, enabling the model to generalize to previously unseen fisheye cameras. Additionally, we improve the distance estimation by pairwise and patchwise vector-based self-attention encoder networks. We evaluate our approach on the Fisheye WoodScape surround-view dataset, significantly improving over previous approaches. We also show a generalization of our approach across different camera viewing angles and perform extensive experiments to support our contributions. To enable comparison with other approaches, we evaluate the front camera data on the KITTI dataset (pinhole camera images) and achieve state-of-the-art performance among self-supervised monocular methods. An overview video with qualitative results is provided at https://youtu.be/bmX0UcU9wtA. Baseline code and dataset will be made public."}}
{"id": "zqtuZCzujBH", "cdate": 1577836800000, "mdate": null, "content": {"title": "Flora Capture: a citizen science application for collecting structured plant observations", "abstract": "Background Digital plant images are becoming increasingly important. First, given a large number of images deep learning algorithms can be trained to automatically identify plants. Second, structured image-based observations provide information about plant morphological characteristics. Finally in the course of digitalization, digital plant collections receive more and more interest in schools and universities. Results We developed a freely available mobile application called Flora Capture allowing users to collect series of plant images from predefined perspectives. These images, together with accompanying metadata, are transferred to a central project server where each observation is reviewed and validated by a team of botanical experts. Currently, more than 4800 plant species, naturally occurring in the Central European region, are covered by the application. More than 200,000 images, depicting more than 1700 plant species, have been collected by thousands of users since the initial app release in 2016. Conclusion Flora Capture allows experts, laymen and citizen scientists to collect a digital herbarium and share structured multi-modal observations of plants. Collected images contribute, e.g., to the training of plant identification algorithms, but also suit educational purposes. Additionally, presence records collected with each observation allow contribute to verifiable records of plant occurrences across the world."}}
{"id": "YV0n_wB1eQb", "cdate": 1577836800000, "mdate": null, "content": {"title": "Request for comments: conversation patterns in issue tracking systems of open-source projects", "abstract": "Issue tracking systems play an important role in developing software systems. They provide a central place to store and maintain different development artifacts. Various studies are concerned with the contained bug reports, features, the relations among them and traces to the projects code base. However, an issue tracker can also be used as a communication channel between project contributors by attaching comments to issues. Less is known on how users actually utilize this functionality. In this paper, we study more than 270,000 comments from twelve open-source projects. We analyze to what extend comments are used and then study the structure occurring in threads of comments. Based on the order of comments and participating contributors, we identified three patterns of conversation: monolog, feedback, and collaboration. Our results show that most conversations are collaborations among two or more developers discussing the issue."}}
{"id": "MHVyhF4sHnl", "cdate": 1577836800000, "mdate": null, "content": {"title": "SpojitR: Intelligently Link Development Artifacts", "abstract": "Traceability has been acknowledged as an important part of the software development process and is considered relevant when performing tasks such as change impact and coverage analysis. With the growing popularity of issue tracking systems and version control systems developers began including the unique identifiers of issues to commit messages. The goal of this message tagging is to trace related artifacts and eventually establish project-wide traceability. However, the trace creation process is still performed manually and not free of errors, i. e. developers may forget to tag their commit with an issue id. The prototype spojitR is designed to assist developers in tagging commit messages and thus (semi-) automatically creating trace links between commits and an issue they are working on. When no tag is present in a commit message, spojitR offers the developer a short recommendation list of potential issue ids to tag the commit message. We evaluated our tool using an open-source project hosted by the Apache Software Foundation. The source code, a demonstration, and a video about spojitR is available online: https://github.com/SECSY-Group/spojitr."}}
