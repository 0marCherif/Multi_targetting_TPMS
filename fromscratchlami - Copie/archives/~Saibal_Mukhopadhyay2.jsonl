{"id": "drc4yTRyzL", "cdate": 1668761243459, "mdate": null, "content": {"title": "An Algorithm-Hardware Co-design Framework to Overcome Imperfections of Mixed-signal DNN Accelerators", "abstract": "In recent years, processing in memory (PIM) based mixed-signal designs have been proposed as energy- and area-efficient solutions with ultra-high throughput to accelerate DNN computations. However, PIM designs are sensitive to imperfections such as noise, weight and conductance variations that substantially degrade the DNN accuracy. To address this issue, we propose a novel algorithm-hardware co-design framework hereafter referred to as HybridAC that simultaneously avoids accuracy degradation due to imperfections, improves area utilization, and reduces data movement and energy dissipation. We derive a data-movement-aware weight selection method that does not require retraining to preserve its original performance. It computes a fraction of the results with a small number of variation-sensitive weights using a robust digital accelerator, while the main computation is performed in analog PIM units. This is the first work that not only provides a variation-robust architecture, but also improves the area, power, and energy of the existing designs considerably. HybridAC is adapted to leverage the preceding weight selection method by reducing ADC precision, peripheral circuitry, and hybrid quantization to optimize the design. Our comprehensive experiments show that, even in the presence of variation as high as 50%, HybridAC can reduce the accuracy degradation from 60 - 90% (without protection) to 1 - 2% for different DNNs across diverse datasets. In addition to providing more robust computation, compared to the ISAAC (SRE), HybridAC improves the execution time, energy, area, power, area-efficiency, and power-efficiency by 26%(14%), 52%(40%), 28%(28%), 57%(45%), 43%(5x), and 81%(3.9x), respectively"}}
{"id": "ZCStthyW-TD", "cdate": 1663850388980, "mdate": null, "content": {"title": "Associative Memory Augmented Asynchronous Spatiotemporal Representation Learning for Event-based Perception", "abstract": "We propose $\\textit{EventFormer}$, a computationally efficient event-based representation learning framework for asynchronously processing event camera data. EventFormer treats sparse input events as a spatially unordered set and models their spatial interactions using self-attention mechanism. An associative memory-augmented recurrent module is used to correlate with the stored representation computed from past events. A memory addressing mechanism is proposed to store and retrieve the latent states only $\\textit{where}$ these events occur and update them only $\\textit{when}$ they occur. The representation learning shift from input space to the latent memory space resulting in reduced computation cost for processing each event. We show that EventFormer achieves 0.5$\\%$ and 9$\\%$ better accuracy with 30000$\\times$ and 200$\\times$ less computation compared to the state-of-the-art dense and event-based method, respectively, on event-based object recognition datasets."}}
{"id": "QIRtAqoXwj", "cdate": 1663850357453, "mdate": null, "content": {"title": "Heterogeneous Neuronal and Synaptic Dynamics for Spike-Efficient Unsupervised Learning: Theory and Design Principles", "abstract": "This paper shows that the heterogeneity in neuronal and synaptic dynamics reduces the spiking activity of a Recurrent Spiking Neural Network (RSNN) while improving prediction performance, enabling spike-efficient (unsupervised) learning.\nWe analytically show that the diversity in neurons' integration/relaxation dynamics improves an RSNN's ability to learn more distinct input patterns (higher memory capacity), leading to improved classification and prediction performance. We further prove that heterogeneous Spike-Timing-Dependent-Plasticity (STDP) dynamics of synapses reduce spiking activity but preserve memory capacity. The analytical results motivate Heterogeneous RSNN design using Bayesian optimization to determine heterogeneity in neurons and synapses to improve $\\mathcal{E}$, defined as the ratio of spiking activity and memory capacity. The empirical results on time series classification and prediction tasks show that optimized HRSNN increases performance and reduces spiking activity compared to a homogeneous RSNN."}}
{"id": "mXPoBtnpMnuy", "cdate": 1663850253024, "mdate": null, "content": {"title": "Unsupervised 3D Object Learning through Neuron Activity aware Plasticity", "abstract": "We present an unsupervised deep learning model for 3D object classification. Conventional Hebbian learning, a well-known unsupervised model, suffers from loss of local features leading to reduced performance for tasks with complex geometric objects. We present a deep network with a novel Neuron Activity Aware (NeAW) Hebbian learning rule that dynamically switches the neurons to be governed by Hebbian learning or anti-Hebbian learning, depending on its activity. We analytically show that NeAW Hebbian learning relieves the bias in neuron activity, allowing more neurons to attend to the representation of the 3D objects. Empirical results show that the NeAW Hebbian learning outperforms other variants of Hebbian learning and shows higher accuracy over fully supervised models when training data is limited."}}
{"id": "bp-LJ4y_XC", "cdate": 1632875767646, "mdate": null, "content": {"title": "Sequence Approximation using Feedforward Spiking Neural Network for Spatiotemporal Learning: Theory and Optimization Methods", "abstract": "A dynamical system of spiking neurons with only feedforward connections can classify spatiotemporal patterns without recurrent connections. However, the theoretical construct of a feedforward spiking neural network (SNN) for approximating a temporal sequence remains unclear, making it challenging to optimize SNN architectures for learning complex spatiotemporal patterns. In this work, we establish a theoretical framework to understand and improve sequence approximation using a feedforward SNN. Our framework shows that a feedforward SNN with one neuron per layer and skip-layer connections can approximate the mapping function between any arbitrary pairs of input and output spike train on a compact domain. Moreover, we prove that heterogeneous neurons with varying dynamics and skip-layer connections improve sequence approximation using feedforward SNN. Consequently, we propose SNN architectures incorporating the preceding constructs that are trained using supervised backpropagation-through-time (BPTT) and unsupervised spiking-timing-dependent plasticity (STDP) algorithms for classification of spatiotemporal data. A dual-search-space Bayesian optimization method is developed to optimize architecture and parameters of the proposed SNN with heterogeneous neuron dynamics and skip-layer connections. "}}
{"id": "BJg1fgBYwH", "cdate": 1569439751504, "mdate": null, "content": {"title": "SAFE-DNN: A Deep Neural Network with Spike Assisted Feature Extraction for Noise Robust Inference", "abstract": "We present a Deep Neural Network with Spike Assisted Feature Extraction (SAFE-DNN) to improve robustness of classification under stochastic perturbation of inputs. The proposed network augments a DNN with unsupervised learning of low-level features using spiking neuron network (SNN) with Spike-Time-Dependent-Plasticity (STDP). The complete network learns to ignore local perturbation while performing global feature detection and classification. The experimental results on CIFAR-10 and ImageNet subset demonstrate improved noise robustness for multiple DNN architectures without sacrificing accuracy on clean images."}}
{"id": "HyRVBzap-", "cdate": 1518730190150, "mdate": null, "content": {"title": "Cascade Adversarial Machine Learning Regularized with a Unified Embedding", "abstract": "Injecting adversarial examples during training, known as adversarial training, can improve robustness against one-step attacks, but not for unknown iterative attacks. To address this challenge, we first show iteratively generated adversarial images easily transfer between networks trained with the same strategy. Inspired by this observation, we propose cascade adversarial training, which transfers the knowledge of the end results of adversarial training. We train a network from scratch by injecting iteratively generated adversarial images crafted from already defended networks in addition to one-step adversarial images from the network being trained. We also propose to utilize embedding space for both classification and low-level (pixel-level) similarity learning to ignore unknown pixel level perturbation. During training, we inject adversarial images without replacing their corresponding clean images and penalize the distance between the two embeddings (clean and adversarial). Experimental results show that cascade adversarial training together with our proposed low-level similarity learning efficiently enhances the robustness against iterative attacks, but at the expense of decreased robustness against one-step attacks. We show that combining those two techniques can also improve robustness under the worst case black box attack scenario."}}
