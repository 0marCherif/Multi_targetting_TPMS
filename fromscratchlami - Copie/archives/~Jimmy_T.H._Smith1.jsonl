{"id": "8vKKm3Q7QJ", "cdate": 1672092835386, "mdate": 1672092835386, "content": {"title": "Simplified State Space Layers for Sequence Modeling", "abstract": "Models using structured state space sequence (S4) layers have achieved state-of-the-art\nperformance on long-range sequence modeling tasks. An S4 layer combines linear state space\nmodels (SSMs), the HiPPO framework, and deep learning to achieve high performance. We\nbuild on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas\nan S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one\nmulti-input, multi-output SSM. We establish a connection between S5 and S4, and use this to\ndevelop the initialization and parameterization used by the S5 model. The result is a state space\nlayer that can leverage efficient and widely implemented parallel scans, allowing S5 to match\nthe computational efficiency of S4, while also achieving state-of-the-art performance on several\nlong-range sequence modeling tasks. S5 averages 87.4% on the long range arena benchmark, and\n98.5% on the most difficult Path-X task."}}
{"id": "Ai8Hw3AXqks", "cdate": 1663850188621, "mdate": null, "content": {"title": "Simplified State Space Layers for Sequence Modeling", "abstract": "Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer.  Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM.  We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S5 model.  The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks.  S5 averages $87.4\\%$ on the long range arena benchmark, and $98.5\\%$ on the most difficult Path-X task."}}
{"id": "LjLi240wM0o", "cdate": 1640995200000, "mdate": 1681661326544, "content": {"title": "Simplified State Space Layers for Sequence Modeling", "abstract": "Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages 87.4% on the long range arena benchmark, and 98.5% on the most difficult Path-X task."}}
{"id": "od-00q5T2vB", "cdate": 1621630288696, "mdate": null, "content": {"title": "Reverse engineering recurrent neural networks with Jacobian switching linear dynamical systems", "abstract": "Recurrent neural networks (RNNs) are powerful models for processing time-series data, but it remains challenging to understand how they function. Improving this understanding is of substantial interest to both the machine learning and neuroscience communities. The framework of reverse engineering a trained RNN by linearizing around its fixed points has provided insight, but the approach has significant challenges. These include difficulty choosing which fixed point to expand around when studying RNN dynamics and error accumulation when reconstructing the nonlinear dynamics with the linearized dynamics. We present a new model that overcomes these limitations by co-training an RNN with a novel switching linear dynamical system (SLDS) formulation. A first-order Taylor series expansion of the co-trained RNN and an auxiliary function trained to pick out the RNN's fixed points govern the SLDS dynamics. The results are a trained SLDS variant that closely approximates the RNN, an auxiliary function that can produce a fixed point for each point in state-space, and a trained nonlinear RNN whose dynamics have been regularized such that its first-order terms perform the computation, if possible. This model removes the post-training fixed point optimization and allows us to unambiguously study the learned dynamics of the SLDS at any point in state-space.  It also generalizes SLDS models to continuous manifolds of switching points while sharing parameters across switches. We validate the utility of the model on two synthetic tasks relevant to previous work reverse engineering RNNs. We then show that our model can be used as a drop-in in more complex architectures, such as LFADS, and apply this LFADS hybrid to analyze single-trial spiking activity from the motor system of a non-human primate."}}
{"id": "NunS5sT2eyV", "cdate": 1609459200000, "mdate": 1681661326502, "content": {"title": "Reverse engineering recurrent neural networks with Jacobian switching linear dynamical systems", "abstract": "Recurrent neural networks (RNNs) are powerful models for processing time-series data, but it remains challenging to understand how they function. Improving this understanding is of substantial interest to both the machine learning and neuroscience communities. The framework of reverse engineering a trained RNN by linearizing around its fixed points has provided insight, but the approach has significant challenges. These include difficulty choosing which fixed point to expand around when studying RNN dynamics and error accumulation when reconstructing the nonlinear dynamics with the linearized dynamics. We present a new model that overcomes these limitations by co-training an RNN with a novel switching linear dynamical system (SLDS) formulation. A first-order Taylor series expansion of the co-trained RNN and an auxiliary function trained to pick out the RNN's fixed points govern the SLDS dynamics. The results are a trained SLDS variant that closely approximates the RNN, an auxiliary function that can produce a fixed point for each point in state-space, and a trained nonlinear RNN whose dynamics have been regularized such that its first-order terms perform the computation, if possible. This model removes the post-training fixed point optimization and allows us to unambiguously study the learned dynamics of the SLDS at any point in state-space. It also generalizes SLDS models to continuous manifolds of switching points while sharing parameters across switches. We validate the utility of the model on two synthetic tasks relevant to previous work reverse engineering RNNs. We then show that our model can be used as a drop-in in more complex architectures, such as LFADS, and apply this LFADS hybrid to analyze single-trial spiking activity from the motor system of a non-human primate."}}
