{"id": "GbPfLS1AU6", "cdate": 1672531200000, "mdate": 1695953374956, "content": {"title": "Backpropagation through Combinatorial Algorithms: Identity with Projection Works", "abstract": ""}}
{"id": "JZMR727O29", "cdate": 1663850530516, "mdate": null, "content": {"title": "Backpropagation through Combinatorial Algorithms: Identity with Projection Works", "abstract": "Embedding discrete solvers as differentiable layers has given modern deep learning architectures combinatorial expressivity and discrete reasoning capabilities. The derivative of these solvers is zero or undefined, therefore a meaningful replacement is crucial for effective gradient-based learning. Prior works rely on smoothing the solver with input perturbations, relaxing the solver to continuous problems, or interpolating the loss landscape with techniques that typically require additional solver calls, introduce extra hyper-parameters, or compromise performance. We propose a principled approach to exploit the geometry of the discrete solution space to treat the solver as a negative identity on the backward pass and further provide a theoretical justification. Our experiments demonstrate that such a straightforward hyper-parameter-free approach is able to compete with previous more complex methods on numerous experiments such as backpropagation through discrete samplers, deep graph matching, and image retrieval. Furthermore, we substitute the previously proposed problem-specific and label-dependent margin with a generic regularization procedure that prevents cost collapse and increases robustness."}}
{"id": "HlNrasOGb4x", "cdate": 1640995200000, "mdate": 1682318576572, "content": {"title": "Gradient Backpropagation Through Combinatorial Algorithms: Identity with Projection Works", "abstract": "Embedding discrete solvers as differentiable layers has given modern deep learning architectures combinatorial expressivity and discrete reasoning capabilities. The derivative of these solvers is zero or undefined, therefore a meaningful replacement is crucial for effective gradient-based learning. Prior works rely on smoothing the solver with input perturbations, relaxing the solver to continuous problems, or interpolating the loss landscape with techniques that typically require additional solver calls, introduce extra hyper-parameters, or compromise performance. We propose a principled approach to exploit the geometry of the discrete solution space to treat the solver as a negative identity on the backward pass and further provide a theoretical justification. Our experiments demonstrate that such a straightforward hyper-parameter-free approach is able to compete with previous more complex methods on numerous experiments such as backpropagation through discrete samplers, deep graph matching, and image retrieval. Furthermore, we substitute the previously proposed problem-specific and label-dependent margin with a generic regularization procedure that prevents cost collapse and increases robustness."}}
{"id": "nI-MEn9hyXn", "cdate": 1609459200000, "mdate": 1633301379717, "content": {"title": "CombOptNet: Fit the Right NP-Hard Problem by Learning Integer Programming Constraints", "abstract": "Bridging logical and algorithmic reasoning with modern machine learning techniques is a fundamental challenge with potentially transformative impact. On the algorithmic side, many NP-hard problems ..."}}
{"id": "eLpqubBqgPK", "cdate": 1609459200000, "mdate": 1633301379766, "content": {"title": "CombOptNet: Fit the Right NP-Hard Problem by Learning Integer Programming Constraints", "abstract": "Bridging logical and algorithmic reasoning with modern machine learning techniques is a fundamental challenge with potentially transformative impact. On the algorithmic side, many NP-hard problems can be expressed as integer programs, in which the constraints play the role of their \"combinatorial specification.\" In this work, we aim to integrate integer programming solvers into neural network architectures as layers capable of learning both the cost terms and the constraints. The resulting end-to-end trainable architectures jointly extract features from raw data and solve a suitable (learned) combinatorial problem with state-of-the-art integer programming solvers. We demonstrate the potential of such layers with an extensive performance analysis on synthetic data and with a demonstration on a competitive computer vision keypoint matching benchmark."}}
{"id": "-3qCWheZhxU", "cdate": 1602926480159, "mdate": null, "content": {"title": "Fit The Right NP-Hard Problem: End-to-end Learning of Integer Programming Constraints", "abstract": "Bridging logical and algorithmic reasoning with modern machine learning\ntechniques is a fundamental challenge with potentially transformative impact.\nOn the algorithmic side, many NP-Hard problems can be expressed as integer\nprograms, in which the constraints play the role of their ``combinatorial\nspecification''. In this work, we aim to integrate integer programming solvers\ninto neural network architectures by providing loss functions for \\emph{both}\nthe objective and the constraints. The resulting end-to-end trainable\narchitectures have the power of jointly extracting features from raw data and\nof solving a suitable (learned) combinatorial problem with state-of-the-art\ninteger programming solvers. We experimentally validate our approach on\nartificial datasets created from random constraints, and on solving\n\\textsc{Knapsack} instances from their description in natural language."}}
{"id": "wtlgds8DNL", "cdate": 1577836800000, "mdate": 1633301379701, "content": {"title": "Deep Graph Matching via Blackbox Differentiation of Combinatorial Solvers", "abstract": "Building on recent progress at the intersection of combinatorial optimization and deep learning, we propose an end-to-end trainable architecture for deep graph matching that contains unmodified combinatorial solvers. Using the presence of heavily optimized combinatorial solvers together with some improvements in architecture design, we advance state-of-the-art on deep graph matching benchmarks for keypoint correspondence. In addition, we highlight the conceptual advantages of incorporating solvers into deep learning architectures, such as the possibility of post-processing with a strong multi-graph matching solver or the indifference to changes in the training setting. Finally, we propose two new challenging experimental setups."}}
{"id": "hNQBa-P4g3J", "cdate": 1577836800000, "mdate": 1633301379741, "content": {"title": "Deep Graph Matching via Blackbox Differentiation of Combinatorial Solvers", "abstract": "Building on recent progress at the intersection of combinatorial optimization and deep learning, we propose an end-to-end trainable architecture for deep graph matching that contains unmodified combinatorial solvers. Using the presence of heavily optimized combinatorial solvers together with some improvements in architecture design, we advance state-of-the-art on deep graph matching benchmarks for keypoint correspondence. In addition, we highlight the conceptual advantages of incorporating solvers into deep learning architectures, such as the possibility of post-processing with a strong multi-graph matching solver or the indifference to changes in the training setting. Finally, we propose two new challenging experimental setups. The code is available at https://github.com/martius-lab/blackbox-deep-graph-matching"}}
{"id": "Vrgliri4D1", "cdate": 1577836800000, "mdate": 1682318576981, "content": {"title": "Differentiation of Blackbox Combinatorial Solvers", "abstract": "Achieving fusion of deep learning with combinatorial algorithms promises transformative changes to artificial intelligence. One possible approach is to introduce combinatorial building blocks into neural networks. Such end-to-end architectures have the potential to tackle combinatorial problems on raw input data such as ensuring global consistency in multi-object tracking or route planning on maps in robotics. In this work, we present a method that implements an efficient backward pass through blackbox implementations of combinatorial solvers with linear objective functions. We provide both theoretical and experimental backing. In particular, we incorporate the Gurobi MIP solver, Blossom V algorithm, and Dijkstra's algorithm into architectures that extract suitable features from raw inputs for the traveling salesman problem, the min-cost perfect matching problem and the shortest path problem."}}
{"id": "Ol-O295z6B", "cdate": 1577836800000, "mdate": 1682318576877, "content": {"title": "Optimizing Rank-Based Metrics With Blackbox Differentiation", "abstract": "Rank-based metrics are some of the most widely used criteria for performance evaluation of computer vision models. Despite years of effort, direct optimization for these metrics remains a challenge due to their non-differentiable and non-decomposable nature. We present an efficient, theoretically sound, and general method for differentiating rank-based metrics with mini-batch gradient descent. In addition, we address optimization instability and sparsity of the supervision signal that both arise from using rank-based metrics as optimization targets. Resulting losses based on recall and Average Precision are applied to image retrieval and object detection tasks. We obtain performance that is competitive with state-of-the-art on standard image retrieval datasets and consistently improve performance of near state-of-the-art object detectors."}}
