{"id": "HajBMLCKPyq", "cdate": 1672531200000, "mdate": 1680222128732, "content": {"title": "Self Supervision Does Not Help Natural Language Supervision at Scale", "abstract": ""}}
{"id": "JAvY0KIM2kC", "cdate": 1609459200000, "mdate": null, "content": {"title": "Neural Parts: Learning Expressive 3D Shape Abstractions with Invertible Neural Networks", "abstract": "Impressive progress in 3D shape extraction led to representations that can capture object geometries with high fidelity. In parallel, primitive-based methods seek to represent objects as semantically consistent part arrangements. However, due to the simplicity of existing primitive representations, these methods fail to accurately reconstruct 3D shapes using a small number of primitives/parts. We address the trade-off between reconstruction quality and number of parts with Neural Parts, a novel 3D primitive representation that defines primitives using an Invertible Neural Network (INN) which implements homeomorphic mappings between a sphere and the target object. The INN allows us to compute the inverse mapping of the homeomorphism, which in turn, enables the efficient computation of both the implicit surface function of a primitive and its mesh, without any additional post-processing. Our model learns to parse 3D objects into semantically consistent part arrangements without any part-level supervision. Evaluations on ShapeNet, D-FAUST and FreiHAND demonstrate that our primitives can capture complex geometries and thus simultaneously achieve geometrically accurate as well as interpretable reconstructions using an order of magnitude fewer primitives than state-of-the-art shape abstraction methods."}}
{"id": "xF5TRmTfTAZ", "cdate": 1577836800000, "mdate": null, "content": {"title": "Fast Transformers with Clustered Attention", "abstract": "Transformers have been proven a successful model for a variety of tasks in sequence modeling. However, computing the attention matrix, which is their key component, has quadratic complexity with respect to the sequence length, thus making them prohibitively expensive for large sequences. To address this, we propose clustered attention, which instead of computing the attention for every query, groups queries into clusters and computes attention just for the centroids. To further improve this approximation, we use the computed clusters to identify the keys with the highest attention per query and compute the exact key/query dot products. This results in a model with linear complexity with respect to the sequence length for a fixed number of clusters. We evaluate our approach on two automatic speech recognition datasets and show that our model consistently outperforms vanilla transformers for a given computational budget. Finally, we demonstrate that our model can approximate arbitrarily complex attention distributions with a minimal number of clusters by approximating a pretrained BERT model on GLUE and SQuAD benchmarks with only 25 clusters and no loss in performance."}}
{"id": "TP037h-mwxg", "cdate": 1577836800000, "mdate": null, "content": {"title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention", "abstract": "Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences."}}
{"id": "SkWAZnWubH", "cdate": 1546300800000, "mdate": null, "content": {"title": "Processing Megapixel Images with Deep Attention-Sampling Models", "abstract": "Existing deep architectures cannot operate on very large signals such as megapixel images due to computational and memory constraints. To tackle this limitation, we propose a fully differentiable e..."}}
{"id": "SJWGK2-_WS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Not All Samples Are Created Equal: Deep Learning with Importance Sampling", "abstract": "Deep Neural Network training spends most of the computation on examples that are properly handled, and could be ignored. We propose to mitigate this phenomenon with a principled importance sampling..."}}
