{"id": "_RIAV79baLC", "cdate": 1681654007706, "mdate": 1681654007706, "content": {"title": "Faster Kernel Interpolation for Gaussian Processes", "abstract": "A key challenge in scaling Gaussian Process (GP) regression to massive datasets is that exact inference\nrequires computation with a dense n \u00d7 n kernel matrix, where n is the number of data points. Significant\nwork focuses on approximating the kernel matrix via interpolation using a smaller set of m \u201cinducing\npoints\u201d. Structured kernel interpolation (SKI) is among the most scalable methods: by placing inducing points\non a dense grid and using structured matrix algebra, SKI achieves per-iteration time of O(n + m log m)\nfor approximate inference. This linear scaling in n enables inference for very large data sets; however the\ncost is per-iteration, which remains a limitation for extremely large n. We show that the SKI per-iteration\ntime can be reduced to O(m log m) after a single O(n) time precomputation step by reframing SKI as\nsolving a natural Bayesian linear regression problem with a fixed set of m compact basis functions. With\nper-iteration complexity independent of the dataset size n for a fixed grid, our method scales to truly massive\ndata sets. We demonstrate speedups in practice for a wide range of m and n and apply the method to GP\ninference on a three-dimensional weather radar dataset with over 100 million points. Our code is available\nat https://github.com/ymohit/fkigp."}}
{"id": "ACThGJBOctg", "cdate": 1652737570277, "mdate": null, "content": {"title": "Kernel Interpolation with Sparse Grids", "abstract": "Structured kernel interpolation (SKI) accelerates Gaussian processes (GP) inference by interpolating the kernel covariance function using a dense grid of inducing points, whose corresponding kernel matrix is highly structured and thus amenable to fast linear algebra. Unfortunately, SKI scales poorly in the dimension of the input points, since the dense grid size grows exponentially with the dimension. To mitigate this issue, we propose the use of sparse grids within the SKI framework. These grids enable accurate interpolation, but with a number of points growing more slowly with dimension. We contribute a novel nearly linear time matrix-vector multiplication algorithm for the sparse grid kernel matrix. We also describe how sparse grids can be combined with an efficient interpolation scheme based on simplicial complexes. With these modifications, we demonstrate that SKI can be scaled to higher dimensions while maintaining accuracy, for both synthetic and real datasets. "}}
{"id": "HJeq43AqF7", "cdate": 1538087986316, "mdate": null, "content": {"title": "Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Auto-Encoders ", "abstract": "Syntax is a powerful abstraction for language understanding. Many downstream tasks require segmenting input text into meaningful constituent chunks (e.g., noun phrases or entities); more generally, models for learning semantic representations of text benefit from integrating syntax in the form of parse trees (e.g., tree-LSTMs). Supervised parsers have traditionally been used to obtain these trees, but lately interest has increased in unsupervised methods that induce syntactic representations directly from unlabeled text. To this end, we propose the deep inside-outside recursive autoencoder (DIORA), a fully-unsupervised method for discovering syntax that simultaneously learns representations for constituents within the induced tree. Unlike many prior approaches, DIORA does not rely on supervision from auxiliary downstream tasks and is thus not constrained to particular domains. Furthermore, competing approaches do not learn explicit phrase representations along with tree structures, which limits their applicability to phrase-based tasks. Extensive experiments on unsupervised parsing, segmentation, and phrase clustering demonstrate the efficacy of our method. DIORA achieves the state of the art in unsupervised parsing (46.9 F1) on the benchmark WSJ dataset."}}
