{"id": "XfpmehHGo2", "cdate": 1676827100184, "mdate": null, "content": {"title": "Optimistic Thompson Sampling-based Algorithms for Episodic Reinforcement Learning", "abstract": "We propose two  Thompson Sampling-like, model-based learning algorithms for episodic Markov decision processes (MDPs) with a finite time horizon. Our proposed algorithms are inspired by Optimistic Thompson Sampling (O-TS), empirically studied in Chapelle and Li [2011], May et al. [2012] for stochastic multi-armed bandits. The key idea for the original O-TS is to clip the posterior distribution in an optimistic way to ensure that the sampled models are always better than the empirical models. Both of our proposed algorithms are easy to implement and only need one posterior sample to construct an episode-dependent model. Our first algorithm, Optimistic Thompson Sampling for MDPs (O-TS-MDP), achieves a $\\widetilde{O} \\left(\\sqrt{AS^2H^4T} \\right)$ regret bound, where $S$ is the size of the state space, $A$ is the size of the action space, $H$ is the number of time-steps per episode and $T$ is the number of episodes. Our second algorithm, Optimistic Thompson Sampling plus for MDPs (O-TS-MDP$^+$),  achieves the (near)-optimal $\\widetilde{O} \\left(\\sqrt{ASH^3T} \\right)$ regret bound by taking a more aggressive clipping strategy.  Since O-TS was only empirically studied previously, we derive regret bounds of O-TS for stochastic bandits. In addition, we propose,  O-TS-Bandit$^+$, a randomized version of UCB1 [Auer et al., 2002], for stochastic bandits. Both O-TS and O-TS-Bandit$^+$ achieve the optimal $O\\left(\\frac{A\\ln(T)}{\\Delta} \\right)$ problem-dependent regret bound, where $\\Delta$ denotes the sub-optimality gap."}}
{"id": "LcL_O3rQUB-", "cdate": 1672531200000, "mdate": 1688657887608, "content": {"title": "FineDeb: A Debiasing Framework for Language Models", "abstract": "As language models are increasingly included in human-facing machine learning tools, bias against demographic subgroups has gained attention. We propose FineDeb, a two-phase debiasing framework for language models that starts with contextual debiasing of embeddings learned by pretrained language models. The model is then fine-tuned on a language modeling objective. Our results show that FineDeb offers stronger debiasing in comparison to other methods which often result in models as biased as the original language model. Our framework is generalizable for demographics with multiple classes, and we demonstrate its effectiveness through extensive experiments and comparisons with state of the art techniques. We release our code and data on GitHub."}}
{"id": "YvrAyFZq0ID", "cdate": 1663850487712, "mdate": null, "content": {"title": "Long Term Fairness via Performative Distributionally Robust Optimization", "abstract": "Fairness researchers in machine learning (ML) have coalesced around several fairness criteria which provide formal definitions of what it means for an ML model to be fair. However, these criteria have some serious limitations. We identify four key shortcomings of these formal fairness criteria and address them by extending performative prediction to include a distributionally robust objective.  Performative prediction is a recent framework developed to understand the effects of when deploying model influences the distribution on which it is making predictions.  We prove a convergence result for our proposed repeated distributionally robust optimization (RDRO).  We further verify our results empirically and develop experiments to demonstrate the impact of using RDRO on learning fair ML models."}}
{"id": "Bfzg8d8j9x5", "cdate": 1646077549530, "mdate": null, "content": {"title": "Near-Optimal Thompson Sampling-based Algorithms for Differentially Private Stochastic Bandits", "abstract": "We address differentially private stochastic bandits. We present two (near)-optimal  Thompson Sampling-based learning algorithms: DP-TS and Lazy-DP-TS. The core idea in achieving optimality  is  the principle of optimism in the face of uncertainty. We reshape the posterior distribution in an optimistic way as compared to the  non-private Thompson Sampling. Our DP-TS achieves a $\\sum\\limits_{j \\in \\mathcal{A}: \\Delta_j > 0} O \\left(\\frac{\\log(T)}{\\min \\left\\{\\epsilon, \\Delta_j \\right\\} )} \\log \\left(\\frac{\\log(T)}{\\epsilon \\cdot \\Delta_j} \\right) \\right)$ regret bound, where $\\mathcal{A}$ is the arm set, $\\Delta_j$ is the sub-optimality gap of a sub-optimal arm $j$, and $\\epsilon$ is the  privacy parameter. Our Lazy-DP-TS gets rid of the extra $\\log$ factor by using the idea of dropping observations. The regret of Lazy-DP-TS  is  $ \\sum\\limits_{j \\in \\mathcal{A}: \\Delta_j > 0} O \\left(\\frac{\\log(T)}{\\min \\left\\{\\epsilon, \\Delta_j \\right\\}} \\right)$, which matches the  regret lower bound. Additionally, we conduct experiments to compare the empirical performance of our proposed  algorithms with the existing optimal  algorithms for differentially private stochastic bandits."}}
{"id": "yvIhrSnrCf1", "cdate": 1640995200000, "mdate": 1688657887949, "content": {"title": "Near-optimal Thompson sampling-based algorithms for differentially private stochastic bandits", "abstract": "We address differentially private stochastic bandits. We present two (near)-optimal Thompson Sampling-based learning algorithms: DP-TS and Lazy-DP-TS. The core idea in achieving optimality is the principle of optimism in the face of uncertainty. We reshape the posterior distribution in an optimistic way as compared to the non-private Thompson Sampling. Our DP-TS achieves a $\\sum\\limits_{j \\in \\mathcal{A}: \\Delta_j > 0} O \\left(\\frac{\\log(T)}{\\min \\left\\{\\epsilon, \\Delta_j \\right\\} )} \\log \\left(\\frac{\\log(T)}{\\epsilon \\cdot \\Delta_j} \\right) \\right)$ regret bound, where $\\mathcal{A}$ is the arm set, $\\Delta_j$ is the sub-optimality gap of a sub-optimal arm $j$, and $\\epsilon$ is the privacy parameter. Our Lazy-DP-TS gets rid of the extra $\\log$ factor by using the idea of dropping observations. The regret of Lazy-DP-TS is $ \\sum\\limits_{j \\in \\mathcal{A}: \\Delta_j > 0} O \\left(\\frac{\\log(T)}{\\min \\left\\{\\epsilon, \\Delta_j \\right\\}} \\right)$, which matches the regret lower bound. Additionally, we conduct experiments to compare the empirical performance of our proposed algorithms with the existing optimal algorithms for differentially private stochastic bandits."}}
{"id": "ynLN_R6QCrr", "cdate": 1640995200000, "mdate": 1688657887802, "content": {"title": "Resonance in Weight Space: Covariate Shift Can Drive Divergence of SGD with Momentum", "abstract": "Most convergence guarantees for stochastic gradient descent with momentum (SGDm) rely on iid sampling. Yet, SGDm is often used outside this regime, in settings with temporally correlated input samples such as continual learning and reinforcement learning. Existing work has shown that SGDm with a decaying step-size can converge under Markovian temporal correlation. In this work, we show that SGDm under covariate shift with a fixed step-size can be unstable and diverge. In particular, we show SGDm under covariate shift is a parametric oscillator, and so can suffer from a phenomenon known as resonance. We approximate the learning system as a time varying system of ordinary differential equations, and leverage existing theory to characterize the system's divergence/convergence as resonant/nonresonant modes. The theoretical result is limited to the linear setting with periodic covariate shift, so we empirically supplement this result to show that resonance phenomena persist even under non-periodic covariate shift, nonlinear dynamics with neural networks, and optimizers other than SGDm."}}
{"id": "OUuCu5imBe", "cdate": 1640995200000, "mdate": 1688657887792, "content": {"title": "Resonance in Weight Space: Covariate Shift Can Drive Divergence of SGD with Momentum", "abstract": "Most convergence guarantees for stochastic gradient descent with momentum (SGDm) rely on iid sampling. Yet, SGDm is often used outside this regime, in settings with temporally correlated input samples such as continual learning and reinforcement learning. Existing work has shown that SGDm with a decaying step-size can converge under Markovian temporal correlation. In this work, we show that SGDm under covariate shift with a fixed step-size can be unstable and diverge. In particular, we show SGDm under covariate shift is a parametric oscillator, and so can suffer from a phenomenon known as resonance. We approximate the learning system as a time varying system of ordinary differential equations, and leverage existing theory to characterize the system's divergence/convergence as resonant/nonresonant modes. The theoretical result is limited to the linear setting with periodic covariate shift, so we empirically supplement this result to show that resonance phenomena persist even under non-periodic covariate shift, nonlinear dynamics with neural networks, and optimizers other than SGDm."}}
{"id": "AOCBfB-guk", "cdate": 1640995200000, "mdate": 1688657887802, "content": {"title": "Long Term Fairness for Minority Groups via Performative Distributionally Robust Optimization", "abstract": "Fairness researchers in machine learning (ML) have coalesced around several fairness criteria which provide formal definitions of what it means for an ML model to be fair. However, these criteria have some serious limitations. We identify four key shortcomings of these formal fairness criteria, and aim to help to address them by extending performative prediction to include a distributionally robust objective."}}
{"id": "5ECQL05ub0J", "cdate": 1632875764141, "mdate": null, "content": {"title": "Resonance in Weight Space: Covariate Shift Can Drive Divergence of SGD with Momentum", "abstract": "Most convergence guarantees for stochastic gradient descent with momentum (SGDm) rely on iid  sampling. Yet, SGDm is often used outside this regime, in settings with temporally correlated input samples such as continual learning and reinforcement learning. Existing work has shown that SGDm with a decaying step-size can converge under Markovian temporal correlation. In this work, we show that SGDm under covariate shift with a fixed step-size can be unstable and diverge. In particular, we show SGDm under covariate shift is a parametric oscillator, and so can suffer from a phenomenon known as resonance. We approximate the learning system as a time varying system of ordinary differential equations, and leverage existing theory to characterize the system's divergence/convergence as resonant/nonresonant modes. The theoretical result is limited to the linear setting with periodic covariate shift, so we empirically supplement this result to show that resonance phenomena persist even under non-periodic covariate shift, nonlinear dynamics with neural networks, and optimizers other than SGDm."}}
{"id": "fAxMXYIVtnP", "cdate": 1577836800000, "mdate": null, "content": {"title": "Multi Type Mean Field Reinforcement Learning", "abstract": "Mean field theory provides an effective way of scaling multiagent reinforcement learning algorithms to environments with many agents that can be abstracted by a virtual mean agent. In this paper, we extend mean field multiagent algorithms to multiple types. The types enable the relaxation of a core assumption in mean field reinforcement learning, which is that all agents in the environment are playing almost similar strategies and have the same goal. We conduct experiments on three different testbeds for the field of many agent reinforcement learning, based on the standard MAgents framework. We consider two different kinds of mean field environments: a) Games where agents belong to predefined types that are known a priori and b) Games where the type of each agent is unknown and therefore must be learned based on observations. We introduce new algorithms for each type of game and demonstrate their superior performance over state of the art algorithms that assume that all agents belong to the same type and other baseline algorithms in the MAgent framework."}}
