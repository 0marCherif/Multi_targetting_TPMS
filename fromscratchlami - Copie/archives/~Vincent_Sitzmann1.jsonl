{"id": "2derKAVsXH", "cdate": 1698871148238, "mdate": 1698871148238, "content": {"title": "Deep Medial Fields", "abstract": "Implicit representations of geometry, such as occupancy fields or signed\ndistance fields (SDF), have recently re-gained popularity in encoding 3D\nsolid shape in a functional form. In this work, we introduce medial fields:\na field function derived from the medial axis transform (MAT) that makes\navailable information about the underlying 3D geometry that is immediately\nuseful for a number of downstream tasks. In particular, the medial field\nencodes the local thickness of a 3D shape, and enables O(1) projection of a\nquery point onto the medial axis. To construct the medial field we require\nnothing but the SDF of the shape itself, thus allowing its straightforward\nincorporation in any application that relies on signed distance fields. Working in unison with the O(1) surface projection supported by the SDF, the\nmedial field opens the door for an entirely new set of efficient, shape-aware\noperations on implicit representations. We present three such applications,\nincluding a modification to sphere tracing that renders implicit representations with better convergence properties, a fast construction method for\nmemory-efficient rigid-body collision proxies, and an efficient approximation of ambient occlusion that remains stable with respect to viewpoint\nvariations."}}
{"id": "Gu7XtUSMoc", "cdate": 1695440963366, "mdate": 1695440963366, "content": {"title": "Kubric: A scalable dataset generator", "abstract": "Data is the driving force of machine learning, with the amount and quality of training data often being more important for the performance of a system than architecture and training details. But collecting, processing and annotating real data at scale is difficult, expensive, and frequently raises additional privacy, fairness and legal concerns. Synthetic data is a powerful tool with the potential to address these shortcomings: 1) it is cheap 2) supports rich ground-truth annotations 3) offers full control over data and 4) can circumvent or mitigate problems regarding bias, privacy and licensing. Unfortunately, software tools for effective data generation are less mature than those for architecture design and training, which leads to fragmented generation efforts. To address these problems we introduce Kubric, an open-source Python framework that interfaces with PyBullet and Blender to generate photo-realistic scenes, with rich annotations, and seamlessly scales to large jobs distributed over thousands of machines, and generating TBs of data. We demonstrate the effectiveness of Kubric by presenting a series of 11 different generated datasets for tasks ranging from studying 3D NeRF models to optical flow estimation. We release Kubric, the used assets, all of the generation code, as well as the rendered datasets for reuse and modification"}}
{"id": "jniceUY6PF", "cdate": 1668673862676, "mdate": 1668673862676, "content": {"title": "Seeing 3D Objects in a Single Image via Self-Supervised Static-Dynamic Disentanglement", "abstract": "Human perception reliably identifies movable and immovable parts of 3D scenes, and completes the 3D structure of objects and background from incomplete observations. We learn this skill not via labeled examples, but simply by observing objects move. In this work, we propose an approach that observes unlabeled multi-view videos at training time and learns to map a single image observation of a complex scene, such as a street with cars, to a 3D neural scene representation that is disentangled into movable and immovable parts while plausibly completing its 3D structure. We separately parameterize movable and immovable scene parts via 2D neural ground plans. These ground plans are 2D grids of features aligned with the ground plane that can be locally decoded into 3D neural radiance fields. Our model is trained self-supervised via neural rendering. We demonstrate that the structure inherent to our disentangled 3D representation enables a variety of downstream tasks in street-scale 3D scenes using simple heuristics, such as extraction of object-centric 3D representations, novel view synthesis, instance segmentation, and 3D bounding box prediction, highlighting its value as a backbone for data-efficient 3D scene understanding models. This disentanglement further enables scene editing via object manipulation such as deletion, insertion, and rigid-body motion"}}
{"id": "Pza24zf9FpS", "cdate": 1663849840364, "mdate": null, "content": {"title": "Neural Groundplans: Persistent Neural Scene Representations from a Single Image", "abstract": "We present a method to map 2D image observations of a scene to a persistent 3D scene representation, enabling novel view synthesis and disentangled representation of the movable and immovable components of the scene. Motivated by the bird\u2019s-eye-view (BEV) representation commonly used in vision and robotics, we propose conditional neural groundplans, ground-aligned 2D feature grids, as persistent and memory-efficient scene representations. Our method is trained self-supervised from unlabeled multi-view observations using differentiable rendering, and learns to complete geometry and appearance of occluded regions. In addition, we show that we can leverage multi-view videos at training time to learn to separately reconstruct static and movable components of the scene from a single image at test time. The ability to separately reconstruct movable objects enables a variety of downstream tasks using simple heuristics, such as extraction of object-centric 3D representations, novel view synthesis, instance-level segmentation, 3D bounding box prediction, and scene editing. This highlights the value of neural groundplans as a backbone for efficient 3D scene understanding models."}}
{"id": "IJNDyqdRF0m", "cdate": 1652737428625, "mdate": null, "content": {"title": "Decomposing NeRF for Editing via Feature Field Distillation", "abstract": "Emerging neural radiance fields (NeRF) are a promising scene representation for computer graphics, enabling high-quality 3D reconstruction and novel view synthesis from image observations.\nHowever, editing a scene represented by a NeRF is challenging, as the underlying connectionist representations such as MLPs or voxel grids are not object-centric or compositional.\nIn particular, it has been difficult to selectively edit specific regions or objects.\nIn this work, we tackle the problem of semantic scene decomposition of NeRFs to enable query-based local editing of the represented 3D scenes.\nWe propose to distill the knowledge of off-the-shelf, self-supervised 2D image feature extractors such as CLIP-LSeg or DINO into a 3D feature field optimized in parallel to the radiance field.\nGiven a user-specified query of various modalities such as text, an image patch, or a point-and-click selection, 3D feature fields semantically decompose 3D space without the need for re-training, and enables us to semantically select and edit regions in the radiance field.\nOur experiments validate that the distilled feature fields can transfer recent progress in 2D vision and language foundation models to 3D scene representations, enabling convincing 3D segmentation and selective editing of emerging neural graphics representations."}}
{"id": "uU61vknNP9", "cdate": 1640995200000, "mdate": 1663861068663, "content": {"title": "Kubric: A scalable dataset generator", "abstract": "Data is the driving force of machine learning, with the amount and quality of training data often being more important for the performance of a system than architecture and training details. But collecting, processing and annotating real data at scale is difficult, expensive, and frequently raises additional privacy, fairness and legal concerns. Synthetic data is a powerful tool with the potential to address these shortcomings: 1) it is cheap 2) supports rich ground-truth annotations 3) offers full control over data and 4) can circumvent or mitigate problems regarding bias, privacy and licensing. Unfortunately, software tools for effective data generation are less mature than those for architecture design and training, which leads to fragmented generation efforts. To address these problems we introduce Kubric, an open-source Python framework that interfaces with PyBullet and Blender to generate photo-realistic scenes, with rich annotations, and seamlessly scales to large jobs distributed over thousands of machines, and generating TBs of data. We demonstrate the effectiveness of Kubric by presenting a series of 13 different generated datasets for tasks ranging from studying 3D NeRF models to optical flow estimation. We release Kubric, the used assets, all of the generation code, as well as the rendered datasets for reuse and modification."}}
{"id": "iyG1jEKqwUG", "cdate": 1640995200000, "mdate": 1663861049202, "content": {"title": "Advances in Neural Rendering", "abstract": ""}}
{"id": "YwH3Ll8OktT", "cdate": 1640995200000, "mdate": 1663861068661, "content": {"title": "Decomposing NeRF for Editing via Feature Field Distillation", "abstract": "Emerging neural radiance fields (NeRF) are a promising scene representation for computer graphics, enabling high-quality 3D reconstruction and novel view synthesis from image observations. However, editing a scene represented by a NeRF is challenging, as the underlying connectionist representations such as MLPs or voxel grids are not object-centric or compositional. In particular, it has been difficult to selectively edit specific regions or objects. In this work, we tackle the problem of semantic scene decomposition of NeRFs to enable query-based local editing of the represented 3D scenes. We propose to distill the knowledge of off-the-shelf, self-supervised 2D image feature extractors such as CLIP-LSeg or DINO into a 3D feature field optimized in parallel to the radiance field. Given a user-specified query of various modalities such as text, an image patch, or a point-and-click selection, 3D feature fields semantically decompose 3D space without the need for re-training and enable us to semantically select and edit regions in the radiance field. Our experiments validate that the distilled feature fields (DFFs) can transfer recent progress in 2D vision and language foundation models to 3D scene representations, enabling convincing 3D segmentation and selective editing of emerging neural graphics representations."}}
{"id": "XPkO9z7AYa9", "cdate": 1640995200000, "mdate": 1663861049132, "content": {"title": "Neural Descriptor Fields: SE(3)-Equivariant Object Representations for Manipulation", "abstract": "We present Neural Descriptor Fields (NDFs), an object representation that encodes both points and relative poses between an object and a target (such as a robot gripper or a rack used for hanging) via category-level descriptors. We employ this representation for object manipulation, where given a task demonstration, we want to repeat the same task on a new object instance from the same category. We propose to achieve this objective by searching (via optimization) for the pose whose descriptor matches that observed in the demonstration. NDFs are conveniently trained in a self-supervised fashion via a 3D auto-encoding task that does not rely on expert-labeled keypoints. Further, NDFs are SE(3)-equivariant, guaranteeing performance that generalizes across all possible 3D object translations and rotations. We demonstrate learning of manipulation tasks from few (\u223c5-10) demonstrations both in simulation and on a real robot. Our performance generalizes across both object instances and 6-DoF object poses, and significantly outperforms a recent baseline that relies on 2D descriptors. Project website: https://yilundu.github.io/ndf/"}}
{"id": "ObDvnAapDP", "cdate": 1640995200000, "mdate": 1663861049264, "content": {"title": "Neural Fields in Visual Computing and Beyond", "abstract": ""}}
