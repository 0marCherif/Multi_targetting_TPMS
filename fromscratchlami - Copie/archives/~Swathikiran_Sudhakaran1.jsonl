{"id": "QgX15Mdi1E_", "cdate": 1621629722187, "mdate": null, "content": {"title": "Space-time Mixing Attention for Video Transformer", "abstract": "This paper is on video recognition using Transformers. Very recent attempts in this area have demonstrated promising results in terms of recognition accuracy, yet they have been also shown to induce, in many cases, significant computational overheads due to the additional modelling of the temporal information. In this work, we propose a Video Transformer model the complexity of which scales linearly with the number of frames in the video sequence and hence induces no overhead compared to an image-based Transformer model. To achieve this, our model makes two approximations to the full space-time attention used in Video Transformers: (a) It restricts time attention to a local temporal window and capitalizes on the Transformer's depth to obtain full temporal coverage of the video sequence. (b) It uses efficient space-time mixing to attend jointly spatial and temporal locations without inducing any additional cost on top of a spatial-only attention model. We also show how to integrate 2 very lightweight mechanisms for global temporal-only attention which provide additional accuracy improvements at minimal computational cost. We demonstrate that our model produces very high recognition accuracy on the most popular video recognition datasets while at the same time being significantly more efficient than other Video Transformer models."}}
{"id": "-4NyBA_u4G3", "cdate": 1609459200000, "mdate": null, "content": {"title": "Learning to Recognize Actions on Objects in Egocentric Video with Attention Dictionaries", "abstract": "We present EgoACO, a deep neural architecture for video action recognition that learns to pool action-context-object descriptors from frame level features by leveraging the verb-noun structure of action labels in egocentric video datasets. The core component of EgoACO is class activation pooling (CAP), a differentiable pooling operation that combines ideas from bilinear pooling for fine-grained recognition and from feature learning for discriminative localization. CAP uses self-attention with a dictionary of learnable weights to pool from the most relevant feature regions. Through CAP, EgoACO learns to decode object and scene context descriptors from video frame features. For temporal modeling in EgoACO, we design a recurrent version of class activation pooling termed Long Short-Term Attention (LSTA). LSTA extends convolutional gated LSTM with built-in spatial attention and a re-designed output gate. Action, object and context descriptors are fused by a multi-head prediction that accounts for the inter-dependencies between noun-verb-action structured labels in egocentric video datasets. EgoACO features built-in visual explanations, helping learning and interpretation. Results on the two largest egocentric action recognition datasets currently available, EPIC-KITCHENS and EGTEA, show that by explicitly decoding action-context-object descriptors, EgoACO achieves state-of-the-art recognition performance."}}
{"id": "o-9TZlmqnUp", "cdate": 1577836800000, "mdate": null, "content": {"title": "Gate-Shift Networks for Video Action Recognition", "abstract": "Deep 3D CNNs for video action recognition are designed to learn powerful representations in the joint spatio-temporal feature space. In practice however, because of the large number of parameters and computations involved, they may under-perform in the lack of sufficiently large datasets for training them at scale. In this paper we introduce spatial gating in spatial-temporal decomposition of 3D kernels. We implement this concept with Gate-Shift Module (GSM). GSM is lightweight and turns a 2D-CNN into a highly efficient spatio-temporal feature extractor. With GSM plugged in, a 2D-CNN learns to adaptively route features through time and combine them, at almost no additional parameters and computational overhead. We perform an extensive evaluation of the proposed module to study its effectiveness in video action recognition, achieving state-of-the-art results on Something Something-V1 and Diving48 datasets, and obtaining competitive results on EPIC-Kitchens with far less model complexity."}}
{"id": "A5OL-jFMiJu", "cdate": 1577836800000, "mdate": null, "content": {"title": "FBK-HUPBA Submission to the EPIC-Kitchens Action Recognition 2020 Challenge", "abstract": "In this report we describe the technical details of our submission to the EPIC-Kitchens Action Recognition 2020 Challenge. To participate in the challenge we deployed spatio-temporal feature extraction and aggregation models we have developed recently: Gate-Shift Module (GSM) [1] and EgoACO, an extension of Long Short-Term Attention (LSTA) [2]. We design an ensemble of GSM and EgoACO model families with different backbones and pre-training to generate the prediction scores. Our submission, visible on the public leaderboard with team name FBK-HUPBA, achieved a top-1 action recognition accuracy of 40.0% on S1 setting, and 25.71% on S2 setting, using only RGB."}}
{"id": "vo3VWw89c0H", "cdate": 1546300800000, "mdate": null, "content": {"title": "Top-down attention recurrent VLAD encoding for action recognition in videos", "abstract": "Most recent approaches for action recognition from video leverage deep architectures to encode the video clip into a fixed length representation vector that is then used for classification. For this to be successful, the network must be capable of suppressing irrelevant scene background and extract the representation from the most discriminative part of the video. Our contribution builds on the observation that spatio-temporal patterns characterizing actions in videos are highly correlated with objects and their location in the video. We propose Top-down Attention Recurrent VLAD Encoder (TA-VLAD), a deep recurrent neural architecture with built-in spatial attention that performs temporally aggregated VLAD encoding for action recognition from videos. We adopt a top-down approach of attention, by using class specific activation maps obtained from a deep Convolutional Neural Network pre-trained for generic image recognition, to weight appearance features before encoding them into a fixed-length video descriptor with a Gated Recurrent Unit. Our method achieves state-of-the-art recognition accuracy on HMDB51 and UCF101 benchmarks."}}
{"id": "rCpzptKnSkZ", "cdate": 1546300800000, "mdate": null, "content": {"title": "Deep Neural Architectures for Video Representation Learning", "abstract": "Automated analysis of videos for content understanding is one of the most challenging and well researched areas in computer vision and multimedia. This thesis addresses the problem of video content understanding in the context of action recognition. The major challenge faced by this research problem is the variations of the spatio-temporal patterns that constitute each action category and the difficulty in generating a succinct representation encapsulating these patterns. This thesis considers two important aspects of videos for addressing this problem: (1) a video is a sequence of images with an inherent temporal dependency that defines the actual pattern to be recognized; (2) not all spatial regions of the video frame are equally important for discriminating one action category from another. The first aspect shows the importance of aggregating frame level features in a sequential manner while the second aspect signifies the importance of selective encoding of frame level features. The first problem is addressed by analyzing popular Convolutional Neural Network (CNN)-Recurrent Neural Network (RNN) architectures for video representation generation and concludes that Convolutional Long Short-Term Memory (ConvLSTM), a variant of the popular Long Short-Term Memory (LSTM) RNN unit, is suitable for encoding spatio-temporal patterns occurring in a video sequence. The second problem is tackled by developing a spatial attention mechanism for the selective encoding of spatial features by weighting spatial regions in the feature tensor that are relevant for identifying the action category. Detailed experimental analysis carried out on two video recognition tasks showed that spatially selective encoding is indeed beneficial. Inspired from the two aforementioned findings, a new recurrent neural unit, called Long Short-Term Attention (LSTA), is developed by augmenting LSTM with built-in spatial attention and a revised output gating. The first enables LSTA to attend to the relevant spatial regions while maintaining a smooth tracking of the attended regions and the latter allows the network to propagate a filtered version of the memory localized on the most discriminative components of the video. LSTA surpasses the recognition accuracy of existing state-of-the-art techniques on popular egocentric activity recognition benchmarks, showing its effectiveness in video representation generation."}}
{"id": "m4iO6onUdU", "cdate": 1546300800000, "mdate": null, "content": {"title": "Hierarchical Feature Aggregation Networks for Video Action Recognition", "abstract": "Most action recognition methods base on a) a late aggregation of frame level CNN features using average pooling, max pooling, or RNN, among others, or b) spatio-temporal aggregation via 3D convolutions. The first assume independence among frame features up to a certain level of abstraction and then perform higher-level aggregation, while the second extracts spatio-temporal features from grouped frames as early fusion. In this paper we explore the space in between these two, by letting adjacent feature branches interact as they develop into the higher level representation. The interaction happens between feature differencing and averaging at each level of the hierarchy, and it has convolutional structure that learns to select the appropriate mode locally in contrast to previous works that impose one of the modes globally (e.g. feature differencing) as a design choice. We further constrain this interaction to be conservative, e.g. a local feature subtraction in one branch is compensated by the addition on another, such that the total feature flow is preserved. We evaluate the performance of our proposal on a number of existing models, i.e. TSN, TRN and ECO, to show its flexibility and effectiveness in improving action recognition performance."}}
{"id": "lwlXyWgudFN", "cdate": 1546300800000, "mdate": null, "content": {"title": "Semantic Guided Deep Unsupervised Image Segmentation", "abstract": "Image segmentation is an important step in many image processing tasks. Inspired by the success of deep learning techniques in image processing tasks, a number of deep supervised image segmentation algorithms have been proposed. However, availability of sufficient labeled training data is not plausible in many application domains. Some application domains are even constrained by the shortage of unlabeled data. Considering such scenarios, we propose a semantic guided unsupervised Convolutional Neural Network (CNN) based approach for image segmentation that does not need any labeled training data and can work on single image input. It uses a pre-trained network to extract mid-level deep features that capture the semantics of the input image. Extracted deep features are further fed to trainable convolutional layers. Segmentation labels are obtained using argmax classification of the final layer and further spatial refinement. Obtained segmentation labels and the weights of the trainable convolutional layers are jointly optimized in iterations in a mechanism that the deep network learns to assign spatially neighboring pixels and pixels of similar feature to the same label. After training, the input image is processed through the same network to obtain the labels that are further refined by a segment score based refinement mechanism. Experimental results show that our method obtains satisfactory results inspite of being unsupervised."}}
{"id": "jnO_OJv6Q5d", "cdate": 1546300800000, "mdate": null, "content": {"title": "An Analysis of Deep Neural Networks with Attention for Action Recognition from a Neurophysiological Perspective", "abstract": "We review three recent deep learning based methods for action recognition and present a brief comparative analysis of the methods from a neurophyisiological point of view. We posit that there are some analogy between the three presented deep learning based methods and some of the existing hypotheses regarding the functioning of human brain."}}
{"id": "dkxiQCLRiF", "cdate": 1546300800000, "mdate": null, "content": {"title": "FBK-HUPBA Submission to the EPIC-Kitchens 2019 Action Recognition Challenge", "abstract": "In this report we describe the technical details of our submission to the EPIC-Kitchens 2019 action recognition challenge. To participate in the challenge we have developed a number of CNN-LSTA [3] and HF-TSN [2] variants, and submitted predictions from an ensemble compiled out of these two model families. Our submission, visible on the public leaderboard with team name FBK-HUPBA, achieved a top-1 action recognition accuracy of 35.54% on S1 setting, and 20.25% on S2 setting."}}
