{"id": "o5rVCwyW1OC", "cdate": 1668681956294, "mdate": 1668681956294, "content": {"title": "Subspace reconstruction based correlation filter for object tracking", "abstract": "The correlation filter (CF) achieves excellent performance, showing high robustness to motion blur or illumination change by learning filters. However, tracking in challenging scenarios with occlusion or out-of-view is still not well resolved. In the scenario of occlusion, the background information is mixed into the image patch to learn the filter, which causes the filter to learn the background. To alleviate this problem, we improve CF trackers by proposing the subspace reconstruction based CF (SRBCF) tracker. In our method, the original image patch for learning filters is replaced by a reconstructed patch when the appearance of the object dramatically changes, such as occlusion or disappearance, so that the filter can learn from the object instead of the background. We construct the subspace with image patches of the searching window in previous frames. To track the changes in the subspace and mitigate the adverse effects of outliers on the subspace during the tracking process, we improve a dynamic L1-PCA method to construct and update the subspace with a slightly extra computational cost. Our method can be embedded in various correlation filter trackers, such as STAPLE and KCF. Extensive experiments on the OTB-100 dataset, UAV123, DTB70, and Temple Color Pure dataset (we removed 49 sequences repeated in OTB-100) validate the effectiveness of our method. The maximum AUC increase reaches 11.2% for the baseline method on DTB70."}}
{"id": "i3WxKIEstu", "cdate": 1668681883619, "mdate": 1668681883619, "content": {"title": "Explore Visual Concept Formation for Image Classification", "abstract": "Human beings acquire the ability of image classification through visual concept learning, in which the process of concept formation involves intertwined searches of common properties and concept descriptions. However, in most image classification algorithms using deep convolutional neural network (ConvNet), the representation space is constructed under the premise that concept descriptions are fixed as one-hot codes, which limits the mining of properties and the ability of identifying unseen samples. Inspired by this,we propose a learning strategy of visual concept formation (LSOVCF) based on the ConvNet, in which the two intertwined parts of concept formation, i.e. feature extraction and concept description, are learned together. First, LSOVCF takes sample response in the last layer of ConvNet to induct concept description being assumed as Gaussian distribution, which is part of the training process. Second, the exploration and experience loss is designed for optimization, which adopts experience cache pool to speed up convergence. Experiments show that LSOVCF improves\nthe ability of identifying unseen samples on cifar10, STL10, flower17 and ImageNet based on several backbones, from the classic VGG to the SOTA Ghostnet. "}}
{"id": "6kfNRYPssl", "cdate": 1668681708405, "mdate": 1668681708405, "content": {"title": "Few-Shot Transfer Learning for SAR Image Classification without Extra SAR Samples", "abstract": "Deep learning-based synthetic aperture radar (SAR)\nimage classi\ufb01cation is an open problem when training samples are\nscarce. Transfer learning-based few-shot methods are effective to\ndeal with this problem by transferring knowledge from the electro\u2013\noptical (EO) to the SAR domain. The performance of such methods\nrelies on extra SAR samples,such as unlabeled novel class\u2019ssamples\nor labeled similar classes samples. However, it is unrealistic to\ncollect suf\ufb01cient extra SAR samples in some application scenarios,\nnamely the extreme few-shot case. In this case, the performance\nof such methods degrades seriously. Therefore, few-shot methods\nthat reduce the dependence on extra SAR samples are critical.\nMotivated by this, a novel few-shot transfer learning method for\nSAR image classi\ufb01cation in the extreme few-shot case is proposed.\nWe propose the connection-free attention module to selectively\ntransfer features shared between EO and SAR samples from a\nsource network to a target network to supplement the loss of\ninformation brought by extra SAR samples. Based on the Bayesian\nconvolutional neural network, we propose a training strategy for\nthe extreme few-shot case, which focuses on updating important\nparameters, namely the accurately updating important parameters.\n The experimental results on the three real-SAR datasets\ndemonstrate the superiority of our method."}}
{"id": "2mB11SBaIKV", "cdate": 1668603306753, "mdate": 1668603306753, "content": {"title": "Repeatable adaptive keypoint detection via self-supervised learning", "abstract": "Keypoint-based matching is a fundamental technology for different computer vision tasks, in which keypoint detection is a crucial step and directly affects the entire performance. Based on deep learning approaches, the learning-based keypoint detectors have been significantly developed. To further improve the accuracy of high-level matching tasks, the extracted keypoints should provide more accurate point-to-point correspondences and maintain a uniform spatial distribution. Based on this idea, a self-supervised learning method of keypoint detection named repeatable adaptive point is proposed. This method consists of a self-supervised objective and an optimization algorithm. The objective maximizes the repeatability measure with the sparsity constraint of keypoints. This sparsity constraint is formulated by combining the non-maximum suppression operation and the penalty function of keypoint number, which generally makes keypoints have a uniform spatial distribution. A novel approximate alternate optimization algorithm is proposed to maximize the above objective, whose convergence is proved in theory. The proposed detector is \u201cadaptive\" because the combinations of it and some existing descriptors can adapt to high-level matching tasks with a fast convergence speed. Specifically, the combinations of it and SuperPoint/HardNet descriptors achieve state-of-the-art accuracy on three high-level tasks based on image matching, namely homography estimation, camera pose estimation, and three-dimensional reconstruction. Furthermore, the proposed method converges faster on new scenes compared with the state-of-the-art method that jointly optimizes the detector and the descriptor."}}
{"id": "o4abzU9meZ", "cdate": 1640995200000, "mdate": 1668682257570, "content": {"title": "Few-Shot Transfer Learning for SAR Image Classification Without Extra SAR Samples", "abstract": "Deep learning-based synthetic aperture radar (SAR) image classification is an open problem when training samples are scarce. Transfer learning-based few-shot methods are effective to deal with this problem by transferring knowledge from the electro\u2013optical (EO) to the SAR domain. The performance of such methods relies on extra SAR samples, such as unlabeled novel class\u2019s samples or labeled similar classes samples. However, it is unrealistic to collect sufficient extra SAR samples in some application scenarios, namely the extreme few-shot case. In this case, the performance of such methods degrades seriously. Therefore, few-shot methods that reduce the dependence on extra SAR samples are critical. Motivated by this, a novel few-shot transfer learning method for SAR image classification in the extreme few-shot case is proposed. We propose the connection-free attention module to selectively transfer features shared between EO and SAR samples from a source network to a target network to supplement the loss of information brought by extra SAR samples. Based on the Bayesian convolutional neural network, we propose a training strategy for the extreme few-shot case, which focuses on updating important parameters, namely the accurately updating important parameters. The experimental results on the three real-SAR datasets demonstrate the superiority of our method."}}
{"id": "eJFO7ujjhzW", "cdate": 1640995200000, "mdate": 1668682257579, "content": {"title": "Learning Soft Estimator of Keypoint Scale and Orientation with Probabilistic Covariant Loss", "abstract": "Estimating keypoint scale and orientation is crucial to extracting invariant features under significant geometric changes. Recently, the estimators based on self-supervised learning have been designed to adapt to complex imaging conditions. Such learning-based estimators generally predict a single scalar for the keypoint scale or orientation, called hard estimators. However, hard estimators are difficult to handle the local patches containing structures of different objects or multiple edges. In this paper, a Soft Self-Supervised Estimator (S3Esti) is proposed to overcome this problem by learning to predict multiple scales and orientations. S3Esti involves three core factors. First, the estimator is constructed to predict the discrete distributions of scales and orientations. The elements with high confidence will be kept as the final scales and orientations. Second, a probabilistic covariant loss is proposed to improve the consistency of the scale and orientation distributions under different transformations. Third, an optimization algorithm is designed to minimize the loss function, whose convergence is proved in theory. When combined with different keypoint extraction models, S3Esti generally improves over 50% accuracy in image matching tasks under significant viewpoint changes. In the 3D reconstruction task, S3Esti decreases more than 10% reprojection error and improves the number of registered images. [code release]"}}
{"id": "A0f_8ZYlb7T", "cdate": 1640995200000, "mdate": 1668682257561, "content": {"title": "Cross-Domain Few-Shot Learning Between Different Imaging Modals for Fine-Grained Target Recognition", "abstract": "Fine-grained target recognition in synthetic aperture radar (SAR) or infrared imaging modal is an open problem in some application scenarios where training samples are scarce. Transferring common features from visible optical (VO) samples is effective for the case that SAR (infrared) samples are scarce. However, for the fine-grained target recognition, transferring common features face two issues: first, common features can be divided into fine-grained features and the coarse-grained features. For the fine-grained target recognition task in the few-shot case, how to transfer fine-grained common features needed to be considered. Second, in the SAR (infrared) imaging modal, parts of samples carry much noise because of the limitation of the imaging mechanism, masking the subtle difference for the fine-grained target recognition task, making such fine-grained common features not easy to be transferred, especially when training samples are scarce. To handle these issues, corresponding solutions are proposed in this article as follows: first, the common-feature-contrastive loss is proposed to transfer fine-grained common features from VO samples; second, based on the modeling of the heteroscedastic uncertainty, the training strategy of sample quality evaluation is proposed to emphasize the training samples with less noise. Experiments on three datasets, including MSTAR, P-openSARship, and P-VAIS, represent the superiority of the proposed algorithm over baseline and other popular cross-domain few-shot learning algorithms."}}
{"id": "y01JhSxfrs", "cdate": 1609459200000, "mdate": 1668682257873, "content": {"title": "Unsupervised learning framework for interest point detection and description via properties optimization", "abstract": ""}}
{"id": "ZlJ6QTpmNJ", "cdate": 1609459200000, "mdate": 1668682257847, "content": {"title": "Explore Visual Concept Formation for Image Classification", "abstract": "Human beings acquire the ability of image classification through visual concept learning, in which the process of concept formation involves intertwined searches of common properties and concept de..."}}
{"id": "VHY2yQknV6T", "cdate": 1609459200000, "mdate": 1668682257925, "content": {"title": "Subtask Attention Based Object Detection in Remote Sensing Images", "abstract": "Object detection in remote sensing images (RSIs) is one of the basic tasks in the field of remote sensing image automatic interpretation. In recent years, the deep object detection frameworks of natural scene images (NSIs) have been introduced into object detection on RSIs, and the detection performance has improved significantly because of the powerful feature representation. However, there are still many challenges concerning the particularities of remote sensing objects. One of the main challenges is the missed detection of small objects which have less than five percent of the pixels of the big objects. Generally, the existing algorithms choose to deal with this problem by multi-scale feature fusion based on a feature pyramid. However, the benefits of this strategy are limited, considering that the location of small objects in the feature map will disappear when the detection task is processed at the end of the network. In this study, we propose a subtask attention network (StAN), which handles the detection task directly on the shallow layer of the network. First, StAN contains one shared feature branch and two subtask attention branches of a semantic auxiliary subtask and a detection subtask based on the multi-task attention network (MTAN). Second, the detection branch uses only low-level features considering small objects. Third, the attention map guidance mechanism is put forward to optimize the network for keeping the identification ability. Fourth, the multi-dimensional sampling module (MdS), global multi-view channel weights (GMulW) and target-guided pixel attention (TPA) are designed for further improvement of the detection accuracy in complex scenes. The experimental results on the NWPU VHR-10 dataset and DOTA dataset demonstrated that the proposed algorithm achieved the SOTA performance, and the missed detection of small objects decreased. On the other hand, ablation experiments also proved the effects of MdS, GMulW and TPA."}}
