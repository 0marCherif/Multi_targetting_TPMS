{"id": "IocYCnz8tHR", "cdate": 1672531200000, "mdate": 1706483981450, "content": {"title": "Profit: Benchmarking Personalization and Robustness Trade-off in Federated Prompt Tuning", "abstract": "In many applications of federated learning (FL), clients desire models that are personalized using their local data, yet are also robust in the sense that they retain general global knowledge. However, the presence of data heterogeneity across clients induces a fundamental trade-off between personalization (i.e., adaptation to a local distribution) and robustness (i.e., not forgetting previously learned general knowledge). It is critical to understand how to navigate this personalization vs robustness trade-off when designing federated systems, which are increasingly moving towards a paradigm of fine-tuning large foundation models. Due to limited computational and communication capabilities in most federated settings, this foundation model fine-tuning must be done using parameter-efficient fine-tuning (PEFT) approaches. While some recent work has studied federated approaches to PEFT, the personalization vs robustness trade-off of federated PEFT has been largely unexplored. In this work, we take a step towards bridging this gap by benchmarking fundamental FL algorithms -- FedAvg and FedSGD plus personalization (via client local fine-tuning) -- applied to one of the most ubiquitous PEFT approaches to large language models (LLMs) -- prompt tuning -- in a multitude of hyperparameter settings under varying levels of data heterogeneity. Our results show that federated-trained prompts can be surprisingly robust when using a small learning rate with many local epochs for personalization, especially when using an adaptive optimizer as the client optimizer during federated training. We also demonstrate that simple approaches such as adding regularization and interpolating two prompts are effective in improving the personalization vs robustness trade-off in computation-limited settings with few local updates allowed for personalization."}}
{"id": "7vTCrWhNU_D", "cdate": 1663939406091, "mdate": null, "content": {"title": "Motley: Benchmarking Heterogeneity and Personalization in Federated Learning", "abstract": "Personalized federated learning considers learning models unique to each client in a heterogeneous network. The resulting client-specific models have been purported to improve metrics such as accuracy, fairness, and robustness in federated networks.  However, despite a plethora of work in this area, it remains unclear: (1) which personalization techniques are most effective in various settings, and (2) how important personalization truly is for realistic federated applications. To better answer these questions, we propose Motley, a benchmark for personalized federated learning. Motley consists of a suite of cross-device and cross-silo federated datasets from varied problem domains, as well as thorough evaluation metrics for better understanding the possible impacts of personalization. We establish baselines on the benchmark by comparing a number of representative personalized federated learning methods. These initial results highlight strengths and weaknesses of existing approaches, and raise several open questions for the community. Motley aims to provide a reproducible means with which to advance developments in personalized and heterogeneity-aware federated learning, as well as the related areas of transfer learning, meta-learning, and multi-task learning. Code for the benchmark is open-source and available at: https://github.com/google-research/federated/tree/master/personalization_benchmark"}}
{"id": "O_psAB_8oPd", "cdate": 1640995200000, "mdate": 1681606962770, "content": {"title": "Motley: Benchmarking Heterogeneity and Personalization in Federated Learning", "abstract": ""}}
{"id": "Hk0olMdZOIU", "cdate": 1621630047920, "mdate": null, "content": {"title": "Federated Reconstruction: Partially Local Federated Learning", "abstract": "Personalization methods in federated learning aim to balance the benefits of federated and local training for data availability, communication cost, and robustness to client heterogeneity. Approaches that require clients to communicate all model parameters can be undesirable due to privacy and communication constraints. Other approaches require always-available or stateful clients, impractical in large-scale cross-device settings. We introduce Federated Reconstruction, the first model-agnostic framework for partially local federated learning suitable for training and inference at scale. We motivate the framework via a connection to model-agnostic meta learning, empirically demonstrate its performance over existing approaches for collaborative filtering and next word prediction, and release an open-source library for evaluating approaches in this setting. We also describe the successful deployment of this approach at scale for federated collaborative filtering in a mobile keyboard application."}}
{"id": "wttER-Yb6N", "cdate": 1609459200000, "mdate": 1681925568106, "content": {"title": "A Field Guide to Federated Optimization", "abstract": "Federated learning and analytics are a distributed approach for collaboratively learning models (or statistics) from decentralized data, motivated by and designed for privacy protection. The distributed learning process can be formulated as solving federated optimization problems, which emphasize communication efficiency, data heterogeneity, compatibility with privacy and system requirements, and other constraints that are not primary considerations in other problem settings. This paper provides recommendations and guidelines on formulating, designing, evaluating and analyzing federated optimization algorithms through concrete examples and practical implementation, with a focus on conducting effective simulations to infer real-world performance. The goal of this work is not to survey the current literature, but to inspire researchers and practitioners to design federated learning algorithms that can be used in various practical applications."}}
{"id": "EbFUVSaQCK", "cdate": 1609459200000, "mdate": 1706483981383, "content": {"title": "Federated Reconstruction: Partially Local Federated Learning", "abstract": "Personalization methods in federated learning aim to balance the benefits of federated and local training for data availability, communication cost, and robustness to client heterogeneity. Approaches that require clients to communicate all model parameters can be undesirable due to privacy and communication constraints. Other approaches require always-available or stateful clients, impractical in large-scale cross-device settings. We introduce Federated Reconstruction, the first model-agnostic framework for partially local federated learning suitable for training and inference at scale. We motivate the framework via a connection to model-agnostic meta learning, empirically demonstrate its performance over existing approaches for collaborative filtering and next word prediction, and release an open-source library for evaluating approaches in this setting. We also describe the successful deployment of this approach at scale for federated collaborative filtering in a mobile keyboard application."}}
{"id": "uo9I9L6tTrc", "cdate": 1577836800000, "mdate": 1681490066516, "content": {"title": "Implicit Regularization and Convergence for Weight Normalization", "abstract": ""}}
{"id": "rkemaESlIH", "cdate": 1567802555288, "mdate": null, "content": {"title": "Learning Distributions Generated by One-Layer ReLU Networks", "abstract": "We consider the problem of estimating the parameters of a $d$-dimensional rectified Gaussian distribution from i.i.d. samples. A rectified Gaussian distribution is defined by passing a standard Gaussian distribution through a one-layer ReLU neural network. We give a simple algorithm to estimate the parameters (i.e., the weight matrix and bias vector of the ReLU neural network) up to an error $\\eps\\norm{W}_F$ using $\\widetilde{O}(1/\\eps^2)$ samples and $\\widetilde{O}(d^2/\\eps^2)$ time (log factors are ignored for simplicity). This implies that we can estimate the distribution up to $\\eps$ in total variation distance using $\\widetilde{O}(\\kappa^2d^2/\\eps^2)$ samples, where $\\kappa$ is the condition number of the covariance matrix. Our only assumption is that the bias vector is non-negative. Without this non-negativity assumption, we show that estimating the bias vector within an error $\\eps$ requires the number of samples at least exponential in $1/\\eps^2$. Our algorithm is based on the key observation that vector norms and pairwise angles can be estimated separately. We use a recent result of learning from truncated samples. Experimental results are provided to support our analysis."}}
{"id": "HJl7TVBlUS", "cdate": 1567802554665, "mdate": null, "content": {"title": "Sparse Logistic Regression Learns All Discrete Pairwise Graphical Models", "abstract": "We characterize the effectiveness of a classical algorithm for recovering the Markov graph of a general discrete pairwise graphical model from i.i.d. samples. The algorithm is (appropriately regularized) maximum conditional log-likelihood, which involves solving a convex program for each node; for Ising models this is $\\ell_1$-constrained logistic regression, while for more general alphabets an $\\ell_{2,1}$ group-norm constraint needs to be used. We show that this algorithm can recover any arbitrary discrete pairwise graphical model, and also characterize its sample complexity as a function of model width, alphabet size, edge parameter accuracy, and the number of variables. We show that along every one of these axes, it matches or improves on all existing results and algorithms for this problem. Our analysis applies a sharp generalization error bound for logistic regression when the weight vector has an $\\ell_1$ (or $\\ell_{2,1}$) constraint and the sample vector has an $\\ell_{\\infty}$ (or $\\ell_{2, \\infty}$) constraint. We also show that the proposed convex programs can be efficiently solved in $\\tilde{O}(n^2)$ running time (where $n$ is the number of variables) under the same statistical guarantees. We provide experimental results to support our analysis."}}
{"id": "S1NI_2bO-H", "cdate": 1546300800000, "mdate": null, "content": {"title": "Learning a Compressed Sensing Measurement Matrix via Gradient Unrolling", "abstract": "Linear encoding of sparse vectors is widely popular, but is commonly data-independent \u2013 missing any possible extra (but a priori unknown) structure beyond sparsity. In this paper we present a new m..."}}
