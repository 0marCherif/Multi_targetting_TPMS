{"id": "i6WWf1c-QX", "cdate": 1683882403750, "mdate": 1683882403750, "content": {"title": "A Selective Portfolio Management Algorithm with Off-Policy Reinforcement Learning Using Dirichlet Distribution", "abstract": "Existing methods in portfolio management deterministically produce an optimal portfolio. However, according to modern portfolio theory, there exists a trade-off between a portfolio\u2019s expected returns and risks. Therefore, the optimal portfolio does not exist definitively, but several exist, and using only one deterministic portfolio is disadvantageous for risk management. We proposed Dirichlet Distribution Trader (DDT), an algorithm that calculates multiple optimal portfolios by taking Dirichlet Distribution as a policy. The DDT algorithm makes several optimal portfolios according to risk levels. In addition, by obtaining the pi value from the distribution and applying importance sampling to off-policy learning, the sample is used efficiently. Furthermore, the architecture of our model is scalable because the feed-forward of information between portfolio stocks occurs independently. This means that even if untrained stocks are added to the portfolio, the optimal weight can be adjusted. We also conducted three experiments. In the scalability experiment, it was shown that the DDT extended model, which is trained with only three stocks, had little difference in performance from the DDT model that learned all the stocks in the portfolio. In an experiment comparing the off-policy algorithm and the on-policy algorithm, it was shown that the off-policy algorithm had good performance regardless of the stock price trend. In an experiment comparing investment results according to risk level, it was shown that a higher return or a better Sharpe ratio could be obtained through risk control.\n"}}
{"id": "OipGAv4_Z6", "cdate": 1683882034473, "mdate": 1683882034473, "content": {"title": "A Selective Portfolio Management Algorithm with Off-Policy Reinforcement Learning Using Dirichlet Distribution", "abstract": "Existing methods in portfolio management deterministically produce an optimal portfolio. However, according to modern portfolio theory, there exists a trade-off between a portfolio\u2019s expected returns and risks. Therefore, the optimal portfolio does not exist definitively, but several exist, and using only one deterministic portfolio is disadvantageous for risk management. We proposed Dirichlet Distribution Trader (DDT), an algorithm that calculates multiple optimal portfolios by taking Dirichlet Distribution as a policy. The DDT algorithm makes several optimal portfolios according to risk levels. In addition, by obtaining the pi value from the distribution and applying importance sampling to off-policy learning, the sample is used efficiently. Furthermore, the architecture of our model is scalable because the feed-forward of information between portfolio stocks occurs independently. This means that even if untrained stocks are added to the portfolio, the optimal weight can be adjusted. We also conducted three experiments. In the scalability experiment, it was shown that the DDT extended model, which is trained with only three stocks, had little difference in performance from the DDT model that learned all the stocks in the portfolio. In an experiment comparing the off-policy algorithm and the on-policy algorithm, it was shown that the off-policy algorithm had good performance regardless of the stock price trend. In an experiment comparing investment results according to risk level, it was shown that a higher return or a better Sharpe ratio could be obtained through risk control.\n"}}
{"id": "-WXCYvc5E-P", "cdate": 1665251222885, "mdate": null, "content": {"title": "Perturbed Quantile Regression for Distributional Reinforcement Learning", "abstract": "Distributional reinforcement learning aims to learn distribution of return under stochastic environments. Since the learned distribution of return contains rich information about the stochasticity of the environment, previous studies have relied on descriptive statistics, such as standard deviation, for optimism in the face of uncertainty. However, using the uncertainty from an empirical distribution can hinder convergence and performance when exploring with the certain criterion that has an one-sided tendency on risk in these methods. In this paper, we propose a novel distributional reinforcement learning that explores by randomizing risk criterion to reach a risk-neutral optimal policy. First, we provide a perturbed distributional Bellman optimality operator by distorting the risk measure in action selection. Second, we prove the convergence and optimality of the proposed method by using the weaker contraction property. Our theoretical results support that the proposed method does not fall into biased exploration and is guaranteed to converge to an optimal return distribution. Finally, we empirically show that our method outperforms other existing distribution-based algorithms in various environments including 55 Atari games."}}
{"id": "oMNrkaOd_tL", "cdate": 1664154053559, "mdate": 1664154053559, "content": {"title": "Domain Generalization by Mutual-Information Regularization with Pre-trained Models", "abstract": "Domain generalization (DG) aims to learn a generalized model to an unseen target domain using only limited source domains. Previous attempts to DG fail to learn domain-invariant representations only from the source domains due to the significant domain shifts between training and test domains. Instead, we re-formulate the DG objective using mutual information with the oracle model, a model generalized to any possible domain. We derive a tractable variational lower bound via approximating the oracle model by a pre-trained model, called Mutual Information Regularization with Oracle (MIRO). Our extensive experiments show that MIRO significantly improves the out-of-distribution performance. Furthermore, our scaling experiments show that the larger the scale of the pre-trained model, the greater the performance improvement of MIRO. Source code is available at this https URL.\n"}}
{"id": "X4DOJ-wL2I", "cdate": 1663850254465, "mdate": null, "content": {"title": "SDAC: Efficient Safe Reinforcement Learning with Low-Biased Distributional Actor-Critic", "abstract": "To apply reinforcement learning (RL) to real-world practical applications, agents are required to adhere to the safety guidelines of their respective domains.\nSafe RL can effectively handle the guidelines by maximizing returns while maintaining safety satisfaction.\nIn this paper, we develop a safe distributional RL method based on the trust region method which has the capability of satisfying safety constraints consistently.\nHowever, importance sampling required for the trust region method can hinder performance due to its significant variance, and policies may not meet the safety guidelines due to the estimation bias of distributional critics.\nHence, we enhance safety performance through the following approaches.\nFirst, we propose novel surrogates for the trust region method expressed with Q-functions using the reparameterization trick.\nSecond, we utilize distributional critics trained with a target distribution where bias-variance can be traded off.\nIn addition, if an initial policy violates safety constraints, there can be no policy satisfying safety constraints within the trust region.\nThus, we propose a gradient integration method which is guaranteed to find a policy satisfying multiple constraints from an unsafe initial policy.\nFrom extensive experiments, the proposed method shows minimal constraint violations while achieving high returns compared to existing safe RL methods.\nFurthermore, we demonstrate the benefit of safe RL for problems in which the reward function cannot be easily specified."}}
{"id": "1ryTomA0iKa", "cdate": 1652737328014, "mdate": null, "content": {"title": "Riemannian Neural SDE: Learning Stochastic Representations on Manifolds", "abstract": "In recent years, the neural stochastic differential equation (NSDE) has gained attention for modeling stochastic representations with great success in various types of applications. However, it typically loses expressivity when the data representation is manifold-valued. To address this issue, we suggest a principled method for expressing the stochastic representation with the Riemannian neural SDE (RNSDE), which extends the conventional Euclidean NSDE. Empirical results for various tasks demonstrate that the proposed method significantly outperforms baseline methods."}}
{"id": "zp-ZdP024EH", "cdate": 1640995200000, "mdate": 1668589099135, "content": {"title": "Neural Markov Controlled SDE: Stochastic Optimization for Continuous-Time Data", "abstract": "We propose a novel probabilistic framework for modeling stochastic dynamics with the rigorous use of stochastic optimal control theory. The proposed model called the neural Markov controlled stochastic differential equation (CSDE) overcomes the fundamental and structural limitations of conventional dynamical models by introducing the following two components: (1) Markov dynamic programming to efficiently train the proposed CSDE and (2) multi-conditional forward-backward losses to provide rich information for accurate inference and to assure theoretical optimality. We demonstrate that our dynamical model efficiently generates a complex time series in the data space without extra networks while showing comparable performance against existing model-based methods on several datasets."}}
{"id": "w6uZEVnguQ", "cdate": 1640995200000, "mdate": 1681747435087, "content": {"title": "Semi-Autonomous Teleoperation via Learning Non-Prehensile Manipulation Skills", "abstract": "In this paper, we present a semi-autonomous teleoperation framework for a pick-and-place task using an RGB-D sensor. In particular, we assume that the target object is located in a cluttered environment where both prehensile grasping and non-prehensile manipulation are combined for efficient teleoperation. A trajectory-based reinforcement learning is utilized for learning the non-prehensile manipulation to rearrange the objects for enabling direct grasping. From the depth image of the cluttered environment and the location of the goal object, the learned policy can provide multiple options of non-prehensile manipulation to the human operator. We carefully design a reward function for the rearranging task where the policy is trained in a simulational environment. Then, the trained policy is transferred to a real-world and evaluated in a number of real-world experiments with the varying number of objects where we show that the proposed method outperforms manual keyboard control in terms of the time duration for the grasping."}}
{"id": "rZn27K5Jrse", "cdate": 1640995200000, "mdate": 1667983866347, "content": {"title": "Domain Generalization by Mutual-Information Regularization with Pre-trained Models", "abstract": "Domain generalization (DG) aims to learn a generalized model to an unseen target domain using only limited source domains. Previous attempts to DG fail to learn domain-invariant representations only from the source domains due to the significant domain shifts between training and test domains. Instead, we re-formulate the DG objective using mutual information with the oracle model, a model generalized to any possible domain. We derive a tractable variational lower bound via approximating the oracle model by a pre-trained model, called Mutual Information Regularization with Oracle (MIRO). Our extensive experiments show that MIRO significantly improves the out-of-distribution performance. Furthermore, our scaling experiments show that the larger the scale of the pre-trained model, the greater the performance improvement of MIRO. Code is available at https://github.com/kakaobrain/miro ."}}
{"id": "Ibw-M8nEHxq", "cdate": 1640995200000, "mdate": 1681747434947, "content": {"title": "Adaptive Tsallis Entropy Regularization for Efficient Reinforcement Learning", "abstract": "In this paper, we present adaptive regularization using Tsallis entropy (ART) for efficient exploration in reinforcement learning (RL) problem. Tsallis entropy represents various types of entropy with an additional parameter called entropic index, which generalizes the Shannon-Gibbs entropy. Tsallis entropy is often utilized as a regularization of the policy function where the resulting policy becomes stochastic and balances exploration and exploitation. Previous work on using Tsallis entropy for exploration only considered the entropic index between zero and one, which is a limited set of entropies. However, in this paper, we extend it to wider range of entropic index and enable to employ all types of Tsallis entropy with a positive entropic index in RL problem. Furthermore, we propose the condition of the optimal entropic index which has the smallest regret bound among all positive entropic index. Using this condition, we can automatically determine the optimal entropic index without demanding a brute force search to find the proper regularization of policy. In the experiment, simply applying the ART to the existing RL methods leads to fast convergence and performance improvement."}}
