{"id": "mnnUIg_usi", "cdate": 1672531200000, "mdate": 1692986426503, "content": {"title": "Modeling Hierarchical Seasonality Through Low-Rank Tensor Decompositions in Time Series Analysis", "abstract": "Accurately representing periodic behavior is a frequently encountered challenge in modeling time series. This is especially true for observations where multiple, nested seasonalities are present, which is often encountered in data that pertain to collective human activity. In this work, we propose a new method that models seasonality through the multilinear representations that characterize low-rank tensor decompositions. We show that the tensor formalism accurately describes multiple nested periodic patterns, and well-known tensor decompositions can be used to parametrize cyclical patterns, leading to superior generalization and parameter efficiency. Furthermore, we develop a Bayesian variant of our approach which facilitates extraction of these seasonal patterns in an interpretable fashion from large-scale datasets, providing insight into the underlying dynamics that create such emergent behavior. We lastly test our method in missing data imputation, where the results show that our method couples interpretability with accuracy in time series analysis."}}
{"id": "aBdT3xfiOJP", "cdate": 1672531200000, "mdate": 1682356422046, "content": {"title": "Algorithmic Stability of Heavy-Tailed Stochastic Gradient Descent on Least Squares", "abstract": "Recent studies have shown that heavy tails can emerge in stochastic optimization and that the heaviness of the tails have links to the generalization error. While these studies have shed light on interesting aspects of the generalization behavior in modern settings, they relied on strong topological and statistical regularity assumptions, which are hard to verify in practice. Furthermore, it has been empirically illustrated that the relation between heavy tails and generalization might not always be monotonic in practice, contrary to the conclusions of existing theory. In this study, we establish novel links between the tail behavior and generalization properties of stochastic gradient descent (SGD), through the lens of algorithmic stability. We consider a quadratic optimization problem and use a heavy-tailed stochastic differential equation (and its Euler discretization) as a proxy for modeling the heavy-tailed behavior emerging in SGD. We then prove uniform stability bounds, which reveal the following outcomes: (i) Without making any exotic assumptions, we show that SGD will not be stable if the stability is measured with the squared-loss $x\\mapsto x^2$, whereas it in turn becomes stable if the stability is instead measured with a surrogate loss $x\\mapsto |x|^p$ with some $p<2$. (ii) Depending on the variance of the data, there exists a \\emph{\u2018threshold of heavy-tailedness\u2019} such that the generalization error decreases as the tails become heavier, as long as the tails are lighter than this threshold. This suggests that the relation between heavy tails and generalization is not globally monotonic. (iii) We prove matching lower-bounds on uniform stability, implying that our bounds are tight in terms of the heaviness of the tails. We support our theory with synthetic and real neural network experiments."}}
{"id": "9GgZiMckEyn", "cdate": 1672531200000, "mdate": 1692986426434, "content": {"title": "Evaluating AI systems under uncertain ground truth: a case study in dermatology", "abstract": "For safety, AI systems in health undergo thorough evaluations before deployment, validating their predictions against a ground truth that is assumed certain. However, this is actually not the case and the ground truth may be uncertain. Unfortunately, this is largely ignored in standard evaluation of AI models but can have severe consequences such as overestimating the future performance. To avoid this, we measure the effects of ground truth uncertainty, which we assume decomposes into two main components: annotation uncertainty which stems from the lack of reliable annotations, and inherent uncertainty due to limited observational information. This ground truth uncertainty is ignored when estimating the ground truth by deterministically aggregating annotations, e.g., by majority voting or averaging. In contrast, we propose a framework where aggregation is done using a statistical model. Specifically, we frame aggregation of annotations as posterior inference of so-called plausibilities, representing distributions over classes in a classification setting, subject to a hyper-parameter encoding annotator reliability. Based on this model, we propose a metric for measuring annotation uncertainty and provide uncertainty-adjusted metrics for performance evaluation. We present a case study applying our framework to skin condition classification from images where annotations are provided in the form of differential diagnoses. The deterministic adjudication process called inverse rank normalization (IRN) from previous work ignores ground truth uncertainty in evaluation. Instead, we present two alternative statistical models: a probabilistic version of IRN and a Plackett-Luce-based model. We find that a large portion of the dataset exhibits significant ground truth uncertainty and standard IRN-based evaluation severely over-estimates performance without providing uncertainty estimates."}}
{"id": "ErNCn2kr1OZ", "cdate": 1621629765165, "mdate": null, "content": {"title": "Heavy Tails in SGD and Compressibility of Overparametrized Neural Networks", "abstract": "Neural network compression techniques have become increasingly popular as they can drastically reduce the storage and computation requirements for very large networks. Recent empirical studies have illustrated that even simple pruning strategies can be surprisingly effective, and several theoretical studies have shown that compressible networks (in specific senses) should achieve a low generalization error. Yet, a theoretical characterization of the underlying causes that make the networks amenable to such simple compression schemes is still missing. In this study, focusing our attention on stochastic gradient descent (SGD), our main contribution is to link compressibility to two recently established properties of SGD: (i) as the network size goes to infinity, the system can converge to a mean-field limit, where the network weights behave independently [DBDF\u015e20], (ii) for a large step-size/batch-size ratio, the SGD iterates can converge to a heavy-tailed stationary distribution  [HM20, G\u015eZ21]. Assuming that both of these phenomena occur simultaneously, we prove that the networks are guaranteed to be '$\\ell_p$-compressible', and the compression errors of different pruning techniques (magnitude, singular value, or node pruning) become arbitrarily small as the network size increases. We further prove generalization bounds adapted to our theoretical framework, which are consistent with the observation that the generalization error will be lower for more compressible networks. Our theory and numerical study on various neural networks show that large step-size/batch-size ratios introduce heavy tails, which, in combination with overparametrization, result in compressibility."}}
{"id": "SqLWPm1wWZ5", "cdate": 1609459200000, "mdate": 1646518047184, "content": {"title": "Bayesian Allocation Model: Marginal Likelihood-Based Model Selection for Count Tensors", "abstract": "In this article, we introduce a dynamic generative model, the Bayesian allocation model (BAM), for modeling count data. BAM covers various probabilistic nonnegative tensor factorization (NTF) and topic models under one general framework. In BAM, allocations are made using a Bayesian network, whose conditional probability tables can be integrated out analytically. We show that, when allocations are viewed as sequential, the resulting marginal process is a special type of Polya urn process, which we name as Polya-Bayes process, an integer valued self-reinforcing process. Exploiting the Polya urn construction, we develop a novel sequential Monte Carlo (SMC) algorithm for marginal likelihood estimation in BAM, leading to a unified scoring method for discrete variable Bayesian networks with hidden nodes, including various NTF and topic models. The SMC estimator for marginal likelihood has the remarkable property of being unbiased in contrast to variational algorithms which are generally biased. We also demonstrate how our novel SMC-based likelihood estimation can be integrated within a Markov chain Monte Carlo algorithm for a principled and correct (in terms of respecting the true posterior distribution) Bayesian model selection and hyperparameter estimation for BAM. We provide several numerical examples, both on artificial and real datasets, that demonstrate the performance of the algorithms for various data regimes."}}
{"id": "BElbDmJD--q", "cdate": 1609459200000, "mdate": 1646518047185, "content": {"title": "Heavy Tails in SGD and Compressibility of Overparametrized Neural Networks", "abstract": "Neural network compression techniques have become increasingly popular as they can drastically reduce the storage and computation requirements for very large networks. Recent empirical studies have illustrated that even simple pruning strategies can be surprisingly effective, and several theoretical studies have shown that compressible networks (in specific senses) should achieve a low generalization error. Yet, a theoretical characterization of the underlying cause that makes the networks amenable to such simple compression schemes is still missing. In this study, we address this fundamental question and reveal that the dynamics of the training algorithm has a key role in obtaining such compressible networks. Focusing our attention on stochastic gradient descent (SGD), our main contribution is to link compressibility to two recently established properties of SGD: (i) as the network size goes to infinity, the system can converge to a mean-field limit, where the network weights behave independently, (ii) for a large step-size/batch-size ratio, the SGD iterates can converge to a heavy-tailed stationary distribution. In the case where these two phenomena occur simultaneously, we prove that the networks are guaranteed to be '$\\ell_p$-compressible', and the compression errors of different pruning techniques (magnitude, singular value, or node pruning) become arbitrarily small as the network size increases. We further prove generalization bounds adapted to our theoretical framework, which indeed confirm that the generalization error will be lower for more compressible networks. Our theory and numerical study on various neural networks show that large step-size/batch-size ratios introduce heavy-tails, which, in combination with overparametrization, result in compressibility."}}
{"id": "2fN0FsvhSU", "cdate": 1609459200000, "mdate": 1682321104655, "content": {"title": "Heavy Tails in SGD and Compressibility of Overparametrized Neural Networks", "abstract": "Neural network compression techniques have become increasingly popular as they can drastically reduce the storage and computation requirements for very large networks. Recent empirical studies have illustrated that even simple pruning strategies can be surprisingly effective, and several theoretical studies have shown that compressible networks (in specific senses) should achieve a low generalization error. Yet, a theoretical characterization of the underlying causes that make the networks amenable to such simple compression schemes is still missing. In this study, focusing our attention on stochastic gradient descent (SGD), our main contribution is to link compressibility to two recently established properties of SGD: (i) as the network size goes to infinity, the system can converge to a mean-field limit, where the network weights behave independently [DBDF\u015e20], (ii) for a large step-size/batch-size ratio, the SGD iterates can converge to a heavy-tailed stationary distribution [HM20, G\u015eZ21]. Assuming that both of these phenomena occur simultaneously, we prove that the networks are guaranteed to be '$\\ell_p$-compressible', and the compression errors of different pruning techniques (magnitude, singular value, or node pruning) become arbitrarily small as the network size increases. We further prove generalization bounds adapted to our theoretical framework, which are consistent with the observation that the generalization error will be lower for more compressible networks. Our theory and numerical study on various neural networks show that large step-size/batch-size ratios introduce heavy tails, which, in combination with overparametrization, result in compressibility."}}
{"id": "ByeB5k2Etr", "cdate": 1571237772894, "mdate": null, "content": {"title": "Bayesian Model Selection for Identifying Markov Equivalent Causal Graphs", "abstract": "Many approaches to causal discovery are limited by their inability to discriminate between Markov equivalent graphs given only observational data. We formulate causal discovery as a marginal likelihood based Bayesian model selection problem. We adopt a parameterization based on the notion of the independence of causal mechanisms which renders Markov equivalent graphs distinguishable. We complement this with an empirical Bayesian approach to setting priors so that the actual underlying causal graph is assigned a higher marginal likelihood than its alternatives. Adopting a Bayesian approach also allows for straightforward modeling of unobserved confounding variables, for which we provide a variational algorithm to approximate the marginal likelihood, since this desirable feat renders the computation of the marginal likelihood intractable. We believe that the Bayesian approach to causal discovery both allows the rich methodology of Bayesian inference to be used in various difficult aspects of this problem and provides a unifying framework to causal discovery research. We demonstrate promising results in experiments conducted on real data, supporting our modeling approach and our inference methodology."}}
{"id": "cPzEuNn3eId", "cdate": 1546300800000, "mdate": null, "content": {"title": "Bayesian Allocation Model: Inference by Sequential Monte Carlo for Nonnegative Tensor Factorizations and Topic Models using Polya Urns", "abstract": "We introduce a dynamic generative model, Bayesian allocation model (BAM), which establishes explicit connections between nonnegative tensor factorization (NTF), graphical models of discrete probability distributions and their Bayesian extensions, and the topic models such as the latent Dirichlet allocation. BAM is based on a Poisson process, whose events are marked by using a Bayesian network, where the conditional probability tables of this network are then integrated out analytically. We show that the resulting marginal process turns out to be a Polya urn, an integer valued self-reinforcing process. This urn processes, which we name a Polya-Bayes process, obey certain conditional independence properties that provide further insight about the nature of NTF. These insights also let us develop space efficient simulation algorithms that respect the potential sparsity of data: we propose a class of sequential importance sampling algorithms for computing NTF and approximating their marginal likelihood, which would be useful for model selection. The resulting methods can also be viewed as a model scoring method for topic models and discrete Bayesian networks with hidden variables. The new algorithms have favourable properties in the sparse data regime when contrasted with variational algorithms that become more accurate when the total sum of the elements of the observed tensor goes to infinity. We illustrate the performance on several examples and numerically study the behaviour of the algorithms for various data regimes."}}
{"id": "hCpo9tTCG9", "cdate": 1356998400000, "mdate": null, "content": {"title": "Linguistic differences in explanation requests and their effects on the evaluation of explanations: the case of English and Turkish", "abstract": "How does language shape thought? In particular, do cross-linguistic differences in how explanations are requested affect how explanations are evaluated by speakers of different languages? To address this question we contrasted English with Turkish, which has three distinct words that correspond to \u201cwhy?\u201d in English. Through two corpus studies and an experimental study, we established that Turkish \u201cwhy\u201d questions tend to appear in different contexts and elicit different kinds of explanations: the \u201cwhy\u201d questions vary in the frequency with which they refer to agents and elicit teleological explanations. In an experimental study investigating whether this cross-linguistic difference affects how explanations are evaluated, we found that while English speakers displayed an overall preference for mechanistic explanations in evaluating the stimuli, Turkish speakers provided similar satisfaction ratings for mechanistic and teleological explanations. Our findings have implications for the cognitive science of explanation and for debates about language and thought."}}
