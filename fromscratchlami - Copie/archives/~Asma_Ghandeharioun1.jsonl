{"id": "whIPbma3cUC", "cdate": 1677598594606, "mdate": 1677598594606, "content": {"title": "Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models", "abstract": "Language models are known to learn a great quantity of factual information during pretraining, and recent work localizes this information to specific model weights like mid-layer MLP weights (Meng et al., 2022a).\nIn this paper, we find that we can change how a fact is stored in a model by editing weights that are in a different location than where existing methods suggest that the fact is stored. This is surprising because we would expect that localizing facts to specific parameters in models would tell us where to manipulate knowledge in models, and this assumption has motivated past work on model editing methods. Specifically, we show that localization conclusions from representation denoising (also known as Causal Tracing) do not provide any insight into which model MLP layer would be best to edit in order to override an existing stored fact with a new one. This finding raises questions about how past work relies on Causal Tracing to select which model layers to edit (Meng et al., 2022a;b). Next, to better understand the discrepancy between representation denoising and weight editing, we develop several variants of the editing problem that appear more and more like representation denoising in their design and objective.\nExperiments show that, for one of our editing problems, editing performance does relate to localization results from representation denoising, but we find that which layer we edit is a far better predictor of performance. Our results suggest, counterintuitively, that better mechanistic understanding of how pretrained language models work may not always translate to insights about how to best change their behavior. Code for all experiments in this paper is available at: https://github.com/google/belief-localization"}}
{"id": "Nd1Wvo4uhU", "cdate": 1672531200000, "mdate": 1682341489455, "content": {"title": "Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models", "abstract": "Language models are known to learn a great quantity of factual information during pretraining, and recent work localizes this information to specific model weights like mid-layer MLP weights (Meng et al., 2022). In this paper, we find that we can change how a fact is stored in a model by editing weights that are in a different location than where existing methods suggest that the fact is stored. This is surprising because we would expect that localizing facts to specific parameters in models would tell us where to manipulate knowledge in models, and this assumption has motivated past work on model editing methods. Specifically, we show that localization conclusions from representation denoising (also known as Causal Tracing) do not provide any insight into which model MLP layer would be best to edit in order to override an existing stored fact with a new one. This finding raises questions about how past work relies on Causal Tracing to select which model layers to edit (Meng et al., 2022). Next, to better understand the discrepancy between representation denoising and weight editing, we develop several variants of the editing problem that appear more and more like representation denoising in their design and objective. Experiments show that, for one of our editing problems, editing performance does relate to localization results from representation denoising, but we find that which layer we edit is a far better predictor of performance. Our results suggest, counterintuitively, that better mechanistic understanding of how pretrained language models work may not always translate to insights about how to best change their behavior. Code is available at: https://github.com/google/belief-localization"}}
{"id": "ExyVLSHCYj", "cdate": 1672531200000, "mdate": 1682341489284, "content": {"title": "Mixed Effects Random Forests for Personalised Predictions of Clinical Depression Severity", "abstract": "This work demonstrates how mixed effects random forests enable accurate predictions of depression severity using multimodal physiological and digital activity data collected from an 8-week study involving 31 patients with major depressive disorder. We show that mixed effects random forests outperform standard random forests and personal average baselines when predicting clinical Hamilton Depression Rating Scale scores (HDRS_17). Compared to the latter baseline, accuracy is significantly improved for each patient by an average of 0.199-0.276 in terms of mean absolute error (p<0.05). This is noteworthy as these simple baselines frequently outperform machine learning methods in mental health prediction tasks. We suggest that this improved performance results from the ability of the mixed effects random forest to personalise model parameters to individuals in the dataset. However, we find that these improvements pertain exclusively to scenarios where labelled patient data are available to the model at training time. Investigating methods that improve accuracy when generalising to new patients is left as important future work."}}
{"id": "Jz1p1UGLCn", "cdate": 1640995200000, "mdate": 1682341489426, "content": {"title": "DISSECT: Disentangled Simultaneous Explanations via Concept Traversals", "abstract": "Explaining deep learning model inferences is a promising venue for scientific understanding, improving safety, uncovering hidden biases, evaluating fairness, and beyond, as argued by many scholars...."}}
{"id": "qY79G8jGsep", "cdate": 1632875734869, "mdate": null, "content": {"title": "DISSECT: Disentangled Simultaneous Explanations via Concept Traversals", "abstract": "Explaining deep learning model inferences is a promising venue for scientific understanding, improving safety, uncovering hidden biases, evaluating fairness, and beyond, as argued by many scholars. One of the principal benefits of counterfactual explanations is allowing users to explore \"what-if\" scenarios through what does not and cannot exist in the data, a quality that many other forms of explanation such as heatmaps and influence functions are inherently incapable of doing. However, most previous work on generative explainability cannot disentangle important concepts effectively, produces unrealistic examples, or fails to retain relevant information. We propose a novel approach, DISSECT, that jointly trains a generator, a discriminator, and a concept disentangler to overcome such challenges using little supervision. DISSECT generates Concept Traversals (CTs), defined as a sequence of generated examples with increasing degrees of concepts that influence a classifier's decision. By training a generative model from a classifier's signal, DISSECT offers a way to discover a classifier's inherent \"notion\" of distinct concepts automatically rather than rely on user-predefined concepts. We show that DISSECT produces CTs that (1) disentangle several concepts, (2) are influential to a classifier's decision and are coupled to its reasoning due to joint training (3), are realistic, (4) preserve relevant information, and (5) are stable across similar inputs. We validate DISSECT on several challenging synthetic and realistic datasets where previous methods fall short of satisfying desirable criteria for interpretability and show that it performs consistently well. Finally, we present experiments showing applications of DISSECT for detecting potential biases of a classifier and identifying spurious artifacts that impact predictions."}}
{"id": "DcQ7KPjdQBu", "cdate": 1620503628265, "mdate": null, "content": {"title": "Human-centric dialog training via offline reinforcement learning", "abstract": "How can we train a dialog model to produce better conversations by learning from human feedback, without the risk of humans\nteaching it harmful chat behaviors? We start\nby hosting models online, and gather human\nfeedback from real-time, open-ended conversations, which we then use to train and improve the models using offline reinforcement\nlearning (RL). We identify implicit conversational cues including language similarity, elicitation of laughter, sentiment, and more, which\nindicate positive human feedback, and embed\nthese in multiple reward functions. A wellknown challenge is that learning an RL policy in an offline setting usually fails due to\nthe lack of ability to explore and the tendency\nto make over-optimistic estimates of future reward. These problems become even harder\nwhen using RL for language models, which\ncan easily have a 20,000 action vocabulary and\nmany possible reward functions. We solve\nthe challenge by developing a novel class of\noffline RL algorithms. These algorithms use\nKL-control to penalize divergence from a pretrained prior language model, and use a new\nstrategy to make the algorithm pessimistic, instead of optimistic, in the face of uncertainty.\nWe test the resulting dialog model with ratings from 80 users in an open-domain setting\nand find it achieves significant improvements\nover existing deep offline RL approaches. The\nnovel offline RL method is viable for improving any existing generative dialog model using\na static dataset of human feedback."}}
{"id": "SSFuTq511d1", "cdate": 1620491506589, "mdate": null, "content": {"title": "Uncertainty modeling in affective computing", "abstract": "This disclosure describes techniques that capture the uncertainty in machine-vision based affect (emotion) perception. The techniques are capable of predicting aleatoric, epistemic, and annotation uncertainty. Measures of uncertainty are important to safety-critical and subjective assessment tasks such as those found in the perception of affective expressions."}}
{"id": "RMu5uFpedo", "cdate": 1620491334077, "mdate": null, "content": {"title": "Characterizing Sources of Uncertainty to Proxy Calibration and Disambiguate Annotator and Data Bias", "abstract": "Supporting model interpretability for complex phenomena where annotators can legitimately disagree, such as emotion recognition, is a challenging machine learning task. In this work, we show that explicitly quantifying the uncertainty in such settings has interpretability benefits. We use a simple modification of a classical network inference using Monte Carlo dropout to give measures of epistemic and aleatoric uncertainty. We identify a significant correlation between aleatoric uncertainty and human annotator disagreement (r\u2248.3). Additionally, we demonstrate how difficult and subjective training samples can be identified using aleatoric uncertainty and how epistemic uncertainty can reveal data bias that could result in unfair predictions. We identify the total uncertainty as a suitable surrogate for model calibration, i.e. the degree we can trust model's predicted confidence. In addition to explainability benefits, we observe modest performance boosts from incorporating model uncertainty."}}
{"id": "J1FUlZjzb3h", "cdate": 1609459200000, "mdate": 1682341489382, "content": {"title": "DISSECT: Disentangled Simultaneous Explanations via Concept Traversals", "abstract": "Explaining deep learning model inferences is a promising venue for scientific understanding, improving safety, uncovering hidden biases, evaluating fairness, and beyond, as argued by many scholars. One of the principal benefits of counterfactual explanations is allowing users to explore \"what-if\" scenarios through what does not and cannot exist in the data, a quality that many other forms of explanation such as heatmaps and influence functions are inherently incapable of doing. However, most previous work on generative explainability cannot disentangle important concepts effectively, produces unrealistic examples, or fails to retain relevant information. We propose a novel approach, DISSECT, that jointly trains a generator, a discriminator, and a concept disentangler to overcome such challenges using little supervision. DISSECT generates Concept Traversals (CTs), defined as a sequence of generated examples with increasing degrees of concepts that influence a classifier's decision. By training a generative model from a classifier's signal, DISSECT offers a way to discover a classifier's inherent \"notion\" of distinct concepts automatically rather than rely on user-predefined concepts. We show that DISSECT produces CTs that (1) disentangle several concepts, (2) are influential to a classifier's decision and are coupled to its reasoning due to joint training (3), are realistic, (4) preserve relevant information, and (5) are stable across similar inputs. We validate DISSECT on several challenging synthetic and realistic datasets where previous methods fall short of satisfying desirable criteria for interpretability and show that it performs consistently well and better than existing methods. Finally, we present experiments showing applications of DISSECT for detecting potential biases of a classifier and identifying spurious artifacts that impact predictions."}}
{"id": "uRs03aWPKT", "cdate": 1577836800000, "mdate": null, "content": {"title": "Human-centric dialog training via offline reinforcement learning", "abstract": "Natasha Jaques, Judy Hanwen Shen, Asma Ghandeharioun, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, Rosalind Picard. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020."}}
