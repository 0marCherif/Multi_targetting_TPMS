{"id": "fuWVySKLM8C", "cdate": 1664294260075, "mdate": null, "content": {"title": "Teacher-generated pseudo human spatial-attention labels boost contrastive learning models", "abstract": "Human spatial attention conveys information about the regions of scenes that are important for performing visual tasks. Prior work has shown that the spatial distribution of human attention can be leveraged to benefit various supervised vision tasks. Might providing this weak form of supervision be useful for self-supervised representation learning? One reason why this question has not been previously addressed is that self-supervised models require large datasets, and no large dataset exists with ground-truth human attentional labels. We therefore construct an auxiliary teacher model to predict human attention, trained on a relatively small labeled dataset. This human-attention model allows us to provide an image (pseudo) attention labels for ImageNet. We then train a model with a primary contrastive objective; to this standard configuration, we add a simple output head trained to predict the attentional map for each image. We measured the quality of learned representations by evaluating classification performance from the frozen learned embeddings. We find that our approach improves accuracy of the contrastive models on ImageNet and its attentional map readout aligns better with human attention compared to vanilla contrastive learning models."}}
{"id": "4H6PZ6lcyr", "cdate": 1640995200000, "mdate": 1668026822341, "content": {"title": "Deep Saliency Prior for Reducing Visual Distraction", "abstract": "Using only a model that was trained to predict where people look at images, and no additional training data, we can produce a range of powerful editing effects for reducing distraction in images. Given an image and a mask specifying the region to edit, we backpropagate through a state-of-the-art saliency model to parameterize a differentiable editing operator, such that the saliency within the masked region is reduced. We demonstrate several operators, including: a recoloring operator, which learns to apply a color transform that camouflages and blends distractors into their surroundings; a warping operator, which warps less salient image regions to cover distractors, gradually collapsing objects into themselves and effectively removing them (an effect akin to inpainting); a GAN operator, which uses a semantic prior to fully replace image regions with plausible, less salient alternatives. The resulting effects are consistent with cognitive research on the human visual system (e.g., since color mismatch is salient, the recoloring operator learns to harmonize objects' colors with their surrounding to reduce their saliency). And importantly, all effects are achieved under a zero-shot learning scenario, solely through the guidance of the pretrained saliency model, with no supervised data of the effects. We present results on a variety of natural images and conduct a perceptual study to evaluate and validate the changes in viewers' eye-gaze between the original images and our edited results. Project Webpage: https://deep-saliency-prior.github.io/"}}
{"id": "yh6719YKsNz", "cdate": 1609459200000, "mdate": 1668026822287, "content": {"title": "Deep Saliency Prior for Reducing Visual Distraction", "abstract": "Using only a model that was trained to predict where people look at images, and no additional training data, we can produce a range of powerful editing effects for reducing distraction in images. Given an image and a mask specifying the region to edit, we backpropagate through a state-of-the-art saliency model to parameterize a differentiable editing operator, such that the saliency within the masked region is reduced. We demonstrate several operators, including: a recoloring operator, which learns to apply a color transform that camouflages and blends distractors into their surroundings; a warping operator, which warps less salient image regions to cover distractors, gradually collapsing objects into themselves and effectively removing them (an effect akin to inpainting); a GAN operator, which uses a semantic prior to fully replace image regions with plausible, less salient alternatives. The resulting effects are consistent with cognitive research on the human visual system (e.g., since color mismatch is salient, the recoloring operator learns to harmonize objects' colors with their surrounding to reduce their saliency), and, importantly, are all achieved solely through the guidance of the pretrained saliency model, with no additional supervision. We present results on a variety of natural images and conduct a perceptual study to evaluate and validate the changes in viewers' eye-gaze between the original images and our edited results."}}
{"id": "wMn0IuYza4", "cdate": 1583689957088, "mdate": null, "content": {"title": "Fast, automatic and fine-grained tampered JPEG image detection via DCT coefficient analysis", "abstract": "The quick advance in image/video editing techniques has enabled people to synthesize realistic images/videos conveniently. Some legal issues may arise when a tampered image cannot be distinguished from a real one by visual examination. In this paper, we focus on JPEG images and propose detecting tampered images by examining the double quantization effect hidden among the discrete cosine transform (DCT) coefficients. To our knowledge, our approach is the only one to date that can automatically locate the tampered region, while it has several additional advantages: fine-grained detection at the scale of 8\u00d7 8 DCT blocks, insensitivity to different kinds of forgery methods (such as alpha matting and inpainting, in addition to simple image cut/paste), the ability to work without fully decompressing the JPEG images, and the fast speed. Experimental results on JPEG images are promising."}}
{"id": "k-BCQYp4il", "cdate": 1583689714336, "mdate": null, "content": {"title": "GazeGAN-Unpaired Adversarial Image Generation for Gaze Estimation", "abstract": "Recent research has demonstrated the ability to estimate gaze on mobile devices by performing inference on the image from the phone's front-facing camera, and without requiring specialized hardware. While this offers wide potential applications such as in human-computer interaction, medical diagnosis and accessibility (eg, hands free gaze as input for patients with motor disorders), current methods are limited as they rely on collecting data from real users, which is a tedious and expensive process that is hard to scale across devices. There have been some attempts to synthesize eye region data using 3D models that can simulate various head poses and camera settings, however these lack in realism.\nIn this paper, we improve upon a recently suggested method, and propose a generative adversarial framework to generate a large dataset of high resolution colorful images with high diversity (eg, in subjects, head pose, camera settings) and realism, while simultaneously preserving the accuracy of gaze labels. The proposed approach operates on extended regions of the eye, and even completes missing parts of the image. Using this rich synthesized dataset, and without using any additional training data from real users, we demonstrate improvements over state-of-the-art for estimating 2D gaze position on mobile devices. We further demonstrate cross-device generalization of model performance, as well as improved robustness to diverse head pose, blur and distance."}}
{"id": "QKCWt3shfI", "cdate": 1583689593941, "mdate": null, "content": {"title": "On-device Few-shot Personalization for Real-time Gaze Estimation", "abstract": "Building fast and accurate gaze estimation models without additional specialized hardware is a hard problem. In this paper, we present on-device few-shot personalization methods for 2D gaze estimation. The proposed supervised method achieves better accuracy using as few as 2-5 calibration points per user compared to prior methods that require more than 13 calibration points. In addition, we propose an unsupervised personalization method which uses only unlabeled facial images to improve gaze estimation accuracy. Our best personalized model achieves 24-26% better accuracy (measured by mean error) on phones compared to the state-of-the-art using<= 5 calibration points per user. It is also computationally efficient, requiring 20x fewer FLOPS when compared to prior methods. This unlocks a variety of important real world applications such as using gaze for accessibility, gaming and human-computer interaction while running entirely on-device in real-time."}}
{"id": "LfmR7X0sw", "cdate": 1583689402382, "mdate": null, "content": {"title": "On-device Few-shot Personalization for Real-time Gaze Estimation", "abstract": "Building fast and accurate gaze estimation models without additional specialized hardware is a hard problem. In\nthis paper, we present on-device few-shot personalization\nmethods for 2D gaze estimation. The proposed supervised\nmethod achieves better accuracy using as few as 2-5 calibration points per user compared to prior methods that require more than 13 calibration points. In addition, we propose an unsupervised personalization method which uses\nonly unlabeled facial images to improve gaze estimation\naccuracy. Our best personalized model achieves 24-26%\nbetter accuracy (measured by mean error) on phones compared to the state-of-the-art using <=5 calibration points\nper user. It is also computationally efficient, requiring 20x\nfewer FLOPS when compared to prior methods. This unlocks a variety of important real world applications such as\nusing gaze for accessibility, gaming and human-computer\ninteraction while running entirely on-device in real-time."}}
{"id": "zmsM_XY79L", "cdate": 1583689223573, "mdate": null, "content": {"title": "On-Device Few-Shot Personalization for Real-Time Gaze Estimation", "abstract": "Building fast and accurate gaze estimation models without additional specialized hardware is a hard problem. In\nthis paper, we present on-device few-shot personalization\nmethods for 2D gaze estimation. The proposed supervised\nmethod achieves better accuracy using as few as 2-5 calibration points per user compared to prior methods that require more than 13 calibration points. In addition, we propose an unsupervised personalization method which uses\nonly unlabeled facial images to improve gaze estimation\naccuracy. Our best personalized model achieves 24-26%\nbetter accuracy (measured by mean error) on phones compared to the state-of-the-art using <=5 calibration points\nper user. It is also computationally efficient, requiring 20x\nfewer FLOPS when compared to prior methods. This unlocks a variety of important real world applications such as\nusing gaze for accessibility, gaming and human-computer\ninteraction while running entirely on-device in real-time."}}
{"id": "M_T75PMpgf", "cdate": 1583688984899, "mdate": null, "content": {"title": "People Search within an Online Social Network: Large Scale Analysis of Facebook Graph Search Query Logs", "abstract": "Popular online social networks (OSN) generate hundreds of\nterabytes of new data per day and connect millions of users.\nTo help users cope with the immense scale and influx of new\ninformation, OSNs provide a search functionality. However,\nmost of the search engines in OSNs today only support keyword queries and provide basic faceted search capabilities\noverlooking serendipitous network exploration and search for\nrelationships between OSN entities. This results in siloed information and a limited search space. In 2013 Facebook introduced its innovative Graph Search product with the goal\nto take the OSN search experience to the next level and facilitate exploration of the Facebook Graph beyond the first\ndegree. In this paper we explore people search on Facebook\nby analyzing an anonymized social graph, anonymized user\nprofiles, and large scale anonymized query logs generated by\nusers of Facebook Graph Search. We uncover numerous insights about people search across several demographics. We\nfind that named entity and structured queries complement\neach other across one\u2019s duration on Facebook, that females\nsearch for people proportionately more than males, and that\nusers submit more queries as they gain more friends. We introduce the concept of a lift predicate and highlight how a\ngraph distance varies with the search goal. Based on these\ninsights, we present a set of design implications to guide the\nresearch and development of the OSN search in the future."}}
{"id": "kPTppVempJ", "cdate": 1583688878397, "mdate": null, "content": {"title": "GazeGAN-Unpaired Adversarial Image Generation for Gaze Estimation", "abstract": "Recent research has demonstrated the ability to estimate\ngaze on mobile devices by performing inference on the image from the phone\u2019s front-facing camera, and without requiring specialized hardware. While this offers wide potential applications such as in human-computer interaction,\nmedical diagnosis and accessibility (e.g., hands free gaze\nas input for patients with motor disorders), current methods are limited as they rely on collecting data from real\nusers, which is a tedious and expensive process that is hard\nto scale across devices. There have been some attempts to\nsynthesize eye region data using 3D models that can simulate various head poses and camera settings, however these\nlack in realism.\nIn this paper, we improve upon a recently suggested\nmethod, and propose a generative adversarial framework to\ngenerate a large dataset of high resolution colorful images\nwith high diversity (e.g., in subjects, head pose, camera\nsettings) and realism, while simultaneously preserving the\naccuracy of gaze labels. The proposed approach operates\non extended regions of the eye, and even completes missing parts of the image. Using this rich synthesized dataset,\nand without using any additional training data from real\nusers, we demonstrate improvements over state-of-the-art\nfor estimating 2D gaze position on mobile devices. We further demonstrate cross-device generalization of model performance, as well as improved robustness to diverse head\npose, blur and distance."}}
