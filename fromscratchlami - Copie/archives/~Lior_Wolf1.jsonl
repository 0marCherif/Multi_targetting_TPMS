{"id": "r4wz6kSARff", "cdate": 1684345485489, "mdate": 1684345485489, "content": {"title": "Adaptive and Iteratively Improving Recurrent Lateral Connections", "abstract": "The current leading computer vision models are typically feed forward neural models, in which the output of one computational block is passed to the next one sequentially. This is in sharp contrast to the organization of the primate visual cortex, in which feedback and lateral connections are abundant. In this work, we propose a computational model for the role of lateral connections in a given block, in which the weights of the block vary dynamically as a function of its activations, and the input from the upstream blocks is iteratively reintroduced. We demonstrate how this novel architectural modification can lead to sizable gains in performance, when applied to visual action recognition without pretraining and that it outperforms the literature architectures with recurrent feedback processing on ImageNet.\n"}}
{"id": "cGAxshfT_0m", "cdate": 1684345440225, "mdate": 1684345440225, "content": {"title": "Mixing between the Cross Entropy and the Expectation Loss Terms", "abstract": "The cross entropy loss is widely used due to its effectiveness and solid theoretical grounding. However, as training progresses, the loss tends to focus on hard to classify samples, which may prevent the network from obtaining gains in performance. While most work in the field suggest ways to classify hard negatives, we suggest to strategically leave hard negatives behind, in order to focus on misclassified samples with higher probabilities. We show that adding to the optimization goal the expectation loss, which is a better approximation of the zero-one loss, helps the network to achieve better accuracy. We, therefore, propose to shift between the two losses during training, focusing more on the expectation loss gradually during the later stages of training. Our experiments show that the new training protocol improves performance across a diverse set of classification domains, including computer vision, natural language processing, tabular data, and sequences. Our code and scripts are available at supplementary.\n"}}
{"id": "DkBVDOYXoyC", "cdate": 1683991960312, "mdate": 1683991960312, "content": {"title": "Recovering AES Keys with a Deep Cold Boot Attack", "abstract": "Cold boot attacks inspect the corrupted random access memory soon after the power has been shut down. While most of the bits have been corrupted, many bits, at random locations, have not. Since the keys in many encryption schemes are being expanded in memory into longer keys with fixed redundancies, the keys can often be restored. In this work, we combine a novel cryptographic variant of a deep error correcting code technique with a modified SAT solver scheme to apply the attack on AES keys. Even though AES consists of Rijndael S-box elements, that are specifically designed to be resistant to linear and differential cryptanalysis, our method provides a novel formalization of the AES key scheduling as a computational graph, which is implemented by a neural message passing network. Our results show that our methods outperform the state of the art attack methods by a very large margin."}}
{"id": "cLcXUvH1qm", "cdate": 1676827072722, "mdate": null, "content": {"title": "Semi-supervised Learning of Partial Differential Operators and Dynamical Flows", "abstract": "The evolution of many dynamical systems is generically governed by nonlinear partial differential equations (PDEs), whose solution, in a simulation framework, requires vast amounts of computational resources. In this work, we present a novel method that combines a hyper-network solver with a Fourier Neural Operator architecture. Our method treats time and space separately and as a result, it successfully propagates initial conditions in continuous time steps by employing the general composition properties of the partial differential operators. Following previous works, supervision is provided at a specific time point. We test our method on various time evolution PDEs, including nonlinear fluid flows in one, two, or three spatial dimensions. The results show that the new method improves the learning accuracy at the time of the supervision point, and can interpolate the solutions to any intermediate time. Our implementation is available at \\href{https://github.com/rotmanmi/Semi-Supervised-Learning-of-Dynamical-Flows}{https://github.com/rotmanmi/Semi-Supervised-Learning-of-Dynamical-Flows}."}}
{"id": "W9qI8DwoUFF", "cdate": 1673287848865, "mdate": null, "content": {"title": "Pre-Training Transformers for Fingerprinting to Improve Stress Prediction in fMRI", "abstract": "We harness a Transformer-based model and a pre-training procedure for fingerprinting on fMRI data, to enhance the accuracy of stress predictions. Our model, called MetricFMRI, first optimizes a pixel-based reconstruction loss. In a second unsupervised training phase, a triplet loss is used to encourage fMRI sequences of the same subject to have closer representations, while sequences from different subjects are pushed away from each other. Finally, supervised learning is used for the target task, based on the learned representation. We evaluate the performance of our model and other alternatives and conclude that the triplet training for the fingerprinting task is key to the improved accuracy of our method for the task of stress prediction. To obtain insights regarding the learned model, gradient-based explainability techniques are used, indicating that sub-cortical brain regions that are known to play a central role in stress-related processes are highlighted by the model. "}}
{"id": "kqHkCVS7wbj", "cdate": 1663850557002, "mdate": null, "content": {"title": "Decision S4: Efficient Sequence-Based RL via State Spaces Layers", "abstract": "Recently, sequence learning methods have been applied to the problem of off-policy\nReinforcement Learning, including the seminal work on Decision Transformers,\nwhich employs transformers for this task. Since transformers are parameter-heavy,\ncannot benefit from history longer than a fixed window size, and are not computed\nusing recurrence, we set out to investigate the suitability of the S4 family of\nmodels, which are based on state-space layers and have been shown to outperform\ntransformers, especially in modeling long-range dependencies. In this work, we\npresent two main algorithms: (i) an off-policy training procedure that works with\ntrajectories, while still maintaining the training efficiency of the S4 model. (ii) An\non-policy training procedure that is trained in a recurrent manner, benefits from\nlong-range dependencies, and is based on a novel stable actor-critic mechanism.\nOur results indicate that our method outperforms multiple variants of decision\ntransformers, as well as the other baseline methods on most tasks, while reducing\nthe latency, number of parameters, and training time by several orders of magnitude,\nmaking our approach more suitable for real-world RL"}}
{"id": "_p6enPE4Xa", "cdate": 1663850071263, "mdate": null, "content": {"title": "OCD: Learning to Overfit with Conditional Diffusion Models", "abstract": "We present a dynamic model in which the weights are conditioned on an input sample $x$ and are learned to match those that would be obtained by finetuning a base model on $x$ and its label $y$. This mapping between an input sample and network weights is shown to be approximated by a linear transformation of the sample distribution, which suggests that a denoising diffusion model can be suitable for this task. The diffusion model we therefore employ focuses on modifying a single layer of the base model and is conditioned on the input, activations, and output of this layer. Our experiments demonstrate the wide applicability of the method for image classification, 3D reconstruction, tabular data, and speech separation. Our code is attached as supplementary."}}
{"id": "-i73LPWa3bD", "cdate": 1663849943365, "mdate": null, "content": {"title": "Semi-supervised learning of partial differential operators and dynamical flows", "abstract": "The evolution of dynamical systems is generically governed by nonlinear partial differential equations (PDEs), whose solution, in a simulation framework, requires vast amounts of computational resources. In this work, we present a novel method that combines a hyper-network solver with a Fourier Neural Operator architecture. Our method treats time and space separately and as a result, it successfully propagates initial conditions in continuous time steps by employing the general composition properties of the partial differential operators. Following previous works, supervision is provided at a specific time point. We test our method on various time evolution PDEs, including nonlinear fluid flows in one, two, or three spatial dimensions. The results show that the new method improves the learning accuracy at the time of the supervision point, and can interpolate the solutions to any intermediate time."}}
{"id": "rLwC0_MG-4w", "cdate": 1663849892094, "mdate": null, "content": {"title": "Denoising Diffusion Error Correction Codes", "abstract": "Error correction code (ECC) is an integral part of the physical communication layer, ensuring reliable data transfer over noisy channels. \nRecently, neural decoders have demonstrated their advantage over classical decoding techniques. \nHowever, recent state-of-the-art neural decoders suffer from high complexity and lack the important iterative scheme characteristic of many legacy decoders. \nIn this work, we propose to employ denoising diffusion models for the soft decoding of linear codes at arbitrary block lengths. \nOur framework models the forward channel corruption as a series of diffusion steps that can be reversed iteratively. \nThree contributions are made: (i) a diffusion process suitable for the decoding setting is introduced, (ii) the neural diffusion decoder is conditioned on the number of parity errors, which indicates the level of corruption at a given step, (iii) a line search procedure based on the code's syndrome obtains the optimal reverse diffusion step size. \nThe proposed approach demonstrates the power of diffusion models for ECC and is able to achieve state-of-the-art accuracy, outperforming the other neural decoders by sizable margins, even for a single reverse diffusion step. "}}
{"id": "upuYKQiyxa_", "cdate": 1652737372066, "mdate": null, "content": {"title": "Optimizing Relevance Maps of Vision Transformers Improves Robustness", "abstract": "It has been observed that visual classification models often rely mostly on spurious cues such as the image background, which hurts their robustness to distribution changes.  \nTo alleviate this shortcoming, we propose to monitor the model's relevancy signal and direct the model to base its prediction on the foreground object.\nThis is done as a finetuning step, involving relatively few samples consisting of pairs of images and their associated foreground masks. Specifically, we encourage the model's relevancy map (i) to assign lower relevance to background regions, (ii) to consider as much information as possible from the foreground, and (iii) we encourage the decisions to have high confidence. When applied to Vision Transformer (ViT) models, a marked improvement in robustness to domain-shifts is observed. Moreover, the foreground masks can be obtained automatically, from a self-supervised variant of the ViT model itself; therefore no additional supervision is required. Our code is available at: https://github.com/hila-chefer/RobustViT."}}
