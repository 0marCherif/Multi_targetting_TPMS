{"id": "dCSFiAl_VO3", "cdate": 1663850347624, "mdate": null, "content": {"title": "Improved Learning-augmented Algorithms for k-means and k-medians Clustering", "abstract": "We consider the problem of clustering in the learning-augmented setting. We are given a data set in $d$-dimensional Euclidean space, and a label for each data point given by a predictor indicating what subsets of points should be clustered together. This setting captures situations where we have access to some auxiliary information about the data set relevant for our clustering objective, for instance the labels output by a neural network. Following prior work, we assume that there are at most an $\\alpha \\in (0,c)$ for some $c<1$ fraction of false positives and false negatives in each  predicted cluster, in the absence of which the labels would attain the optimal clustering cost $\\mathrm{OPT}$. For a dataset of size $m$, we propose a deterministic $k$-means algorithm that produces centers with an improved bound on the clustering cost compared to the previous randomized state-of-the-art algorithm while preserving the $O( d m \\log m)$ runtime. Furthermore, our algorithm works even when the predictions are not very accurate, i.e., our cost bound holds for $\\alpha$ up to $1/2$, an improvement from $\\alpha$ being at most $1/7$ in previous work. For the $k$-medians problem we again improve upon prior work by achieving a biquadratic improvement in the dependence of the approximation factor on the accuracy parameter $\\alpha$ to get a cost of $(1+O(\\alpha))\\mathrm{OPT}$, while requiring essentially just $O(md \\log^3 m/\\alpha)$ runtime."}}
{"id": "nGfpGzZjJQ", "cdate": 1640995200000, "mdate": 1675099519869, "content": {"title": "Universal 1-Bit Compressive Sensing for Bounded Dynamic Range Signals", "abstract": "A {\\em universal 1-bit compressive sensing (CS)} scheme consists of a measurement matrix $A$ such that all signals $x$ belonging to a particular class can be approximately recovered from $\\textrm{sign}(Ax)$. 1-bit CS models extreme quantization effects where only one bit of information is revealed per measurement. We focus on universal support recovery for 1-bit CS in the case of {\\em sparse} signals with bounded {\\em dynamic range}. Specifically, a vector $x \\in \\mathbb{R}^n$ is said to have sparsity $k$ if it has at most $k$ nonzero entries, and dynamic range $R$ if the ratio between its largest and smallest nonzero entries is at most $R$ in magnitude. Our main result shows that if the entries of the measurement matrix $A$ are i.i.d.~Gaussians, then under mild assumptions on the scaling of $k$ and $R$, the number of measurements needs to be $\\tilde{\\Omega}(Rk^{3/2})$ to recover the support of $k$-sparse signals with dynamic range $R$ using $1$-bit CS. In addition, we show that a near-matching $O(R k^{3/2} \\log n)$ upper bound follows as a simple corollary of known results. The $k^{3/2}$ scaling contrasts with the known lower bound of $\\tilde{\\Omega}(k^2 \\log n)$ for the number of measurements to recover the support of arbitrary $k$-sparse signals."}}
{"id": "kVZcg7Ym6Q", "cdate": 1640995200000, "mdate": 1673638301608, "content": {"title": "Streaming Submodular Maximization with Differential Privacy", "abstract": ""}}
{"id": "__eY_E1QYZp", "cdate": 1640995200000, "mdate": 1652403955527, "content": {"title": "Bounded Space Differentially Private Quantiles", "abstract": "Estimating the quantiles of a large dataset is a fundamental problem in both the streaming algorithms literature and the differential privacy literature. However, all existing private mechanisms for distribution-independent quantile computation require space at least linear in the input size $n$. In this work, we devise a differentially private algorithm for the quantile estimation problem, with strongly sublinear space complexity, in the one-shot and continual observation settings. Our basic mechanism estimates any $\\alpha$-approximate quantile of a length-$n$ stream over a data universe $\\mathcal{X}$ with probability $1-\\beta$ using $O\\left( \\frac{\\log (|\\mathcal{X}|/\\beta) \\log (\\alpha \\epsilon n)}{\\alpha \\epsilon} \\right)$ space while satisfying $\\epsilon$-differential privacy at a single time point. Our approach builds upon deterministic streaming algorithms for non-private quantile estimation instantiating the exponential mechanism using a utility function defined on sketch items, while (privately) sampling from intervals defined by the sketch. We also present another algorithm based on histograms that is especially suited to the multiple quantiles case. We implement our algorithms and experimentally evaluate them on synthetic and real-world datasets."}}
{"id": "-25nfwSTMm4", "cdate": 1640995200000, "mdate": 1673638301669, "content": {"title": "Improved Learning-augmented Algorithms for k-means and k-medians Clustering", "abstract": ""}}
{"id": "3CWnd8GWC6", "cdate": 1609459200000, "mdate": 1673638301661, "content": {"title": "Locally Private k-Means Clustering with Constant Multiplicative Approximation and Near-Optimal Additive Error", "abstract": ""}}
{"id": "x4wKH_aSGNR", "cdate": 1577836800000, "mdate": null, "content": {"title": "Differentially Private Decomposable Submodular Maximization", "abstract": "We study the problem of differentially private constrained maximization of decomposable submodular functions. A submodular function is decomposable if it takes the form of a sum of submodular functions. The special case of maximizing a monotone, decomposable submodular function under cardinality constraints is known as the Combinatorial Public Projects (CPP) problem [Papadimitriou et al., 2008]. Previous work by Gupta et al. [2010] gave a differentially private algorithm for the CPP problem. We extend this work by designing differentially private algorithms for both monotone and non-monotone decomposable submodular maximization under general matroid constraints, with competitive utility guarantees. We complement our theoretical bounds with experiments demonstrating empirical performance, which improves over the differentially private algorithms for the general case of submodular maximization and is close to the performance of non-private algorithms."}}
{"id": "rdODMgokDgq", "cdate": 1577836800000, "mdate": 1645832937886, "content": {"title": "Learning Gaussian Graphical Models via Multiplicative Weights", "abstract": "Graphical model selection in Markov random fields is a fundamental problem in statistics and machine learning. Two particularly prominent models, the Ising model and Gaussian model, have largely developed in parallel using different (though often related) techniques, and several practical algorithms with rigorous sample complexity bounds have been established for each. In this paper, we adapt a recently proposed algorithm of Klivans and Meka (FOCS, 2017), based on the method of multiplicative weight updates, from the Ising model to the Gaussian model, via non-trivial modifications to both the algorithm and its analysis. The algorithm enjoys a sample complexity bound that is qualitatively similar to others in the literature, has a low runtime $O(mp^2)$ in the case of $m$ samples and $p$ nodes, and can trivially be implemented in an online manner."}}
{"id": "Z85Eg7_Xf1_", "cdate": 1577836800000, "mdate": null, "content": {"title": "Differentially private k-means clustering via exponential mechanism and max cover", "abstract": "We introduce a new $(\\epsilon_p, \\delta_p)$-differentially private algorithm for the $k$-means clustering problem. Given a dataset in Euclidean space, the $k$-means clustering problem requires one to find $k$ points in that space such that the sum of squares of Euclidean distances between each data point and its closest respective point among the $k$ returned is minimised. Although there exist privacy-preserving methods with good theoretical guarantees to solve this problem [Balcan et al., 2017; Kaplan and Stemmer, 2018], in practice it is seen that it is the additive error which dictates the practical performance of these methods. By reducing the problem to a sequence of instances of maximum coverage on a grid, we are able to derive a new method that achieves lower additive error then previous works. For input datasets with cardinality $n$ and diameter $\\Delta$, our algorithm has an $O(\\Delta^2 (k \\log^2 n \\log(1/\\delta_p)/\\epsilon_p + k\\sqrt{d \\log(1/\\delta_p)}/\\epsilon_p))$ additive error whilst maintaining constant multiplicative error. We conclude with some experiments and find an improvement over previously implemented work for this problem."}}
