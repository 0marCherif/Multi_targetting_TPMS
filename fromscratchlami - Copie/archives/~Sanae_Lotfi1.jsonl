{"id": "o8nYuR8ekFm", "cdate": 1652737643134, "mdate": null, "content": {"title": "PAC-Bayes Compression Bounds So Tight That They Can Explain Generalization", "abstract": "While there has been progress in developing non-vacuous generalization bounds for deep neural networks, these bounds tend to be uninformative about why deep learning works. In this paper, we develop a compression approach based on quantizing neural network parameters in a linear subspace, profoundly improving on previous results to provide state-of-the-art generalization bounds on a variety of tasks, including transfer learning. We use these tight bounds to better understand the role of model size, equivariance, and the implicit biases of optimization, for generalization in deep learning. Notably, we find large models can be compressed to a much greater extent than previously known, encapsulating Occam\u2019s razor."}}
{"id": "9YK9NaFT_q8", "cdate": 1640995200000, "mdate": 1681659356923, "content": {"title": "Bayesian Model Selection, the Marginal Likelihood, and Generalization", "abstract": "How do we compare between hypotheses that are entirely consistent with observations? The marginal likelihood (aka Bayesian evidence), which represents the probability of generating our observations..."}}
{"id": "T1r6y8PnVGk", "cdate": 1621630103012, "mdate": null, "content": {"title": "Dangers of Bayesian Model Averaging under Covariate Shift", "abstract": "Approximate Bayesian inference for neural networks is considered a robust alternative to standard training, often providing good performance on out-of-distribution data. However, Bayesian neural networks (BNNs) with high-fidelity approximate inference via full-batch Hamiltonian Monte Carlo achieve poor generalization under covariate shift, even underperforming classical estimation. We explain this surprising result, showing how a Bayesian model average can in fact be problematic under covariate shift, particularly in cases where linear dependencies in the input features cause a lack of posterior contraction. We additionally show why the same issue does not affect many approximate inference procedures, or classical maximum a-posteriori (MAP) training. Finally, we propose novel priors that improve the robustness of BNNs to many sources of covariate shift."}}
{"id": "2rR3aBnhCaP", "cdate": 1621630103012, "mdate": null, "content": {"title": "Dangers of Bayesian Model Averaging under Covariate Shift", "abstract": "Approximate Bayesian inference for neural networks is considered a robust alternative to standard training, often providing good performance on out-of-distribution data. However, Bayesian neural networks (BNNs) with high-fidelity approximate inference via full-batch Hamiltonian Monte Carlo achieve poor generalization under covariate shift, even underperforming classical estimation. We explain this surprising result, showing how a Bayesian model average can in fact be problematic under covariate shift, particularly in cases where linear dependencies in the input features cause a lack of posterior contraction. We additionally show why the same issue does not affect many approximate inference procedures, or classical maximum a-posteriori (MAP) training. Finally, we propose novel priors that improve the robustness of BNNs to many sources of covariate shift."}}
{"id": "ztmGlhIVmsd", "cdate": 1609459200000, "mdate": 1682604936838, "content": {"title": "Loss Surface Simplexes for Mode Connecting Volumes and Fast Ensembling", "abstract": "With a better understanding of the loss surfaces for multilayer networks, we can build more robust and accurate training procedures. Recently it was discovered that independently trained SGD soluti..."}}
{"id": "KS-pY0VqfOX", "cdate": 1609459200000, "mdate": 1681659357170, "content": {"title": "Evaluating Approximate Inference in Bayesian Deep Learning", "abstract": "Uncertainty representation is crucial to the safe and reliable deployment of deep learning. Bayesian methods provide a natural mechanism to represent epistemic uncertainty, leading to improved gene..."}}
{"id": "2lFQP10KPE", "cdate": 1609459200000, "mdate": 1681659357207, "content": {"title": "Dangers of Bayesian Model Averaging under Covariate Shift", "abstract": "Approximate Bayesian inference for neural networks is considered a robust alternative to standard training, often providing good performance on out-of-distribution data. However, Bayesian neural networks (BNNs) with high-fidelity approximate inference via full-batch Hamiltonian Monte Carlo achieve poor generalization under covariate shift, even underperforming classical estimation. We explain this surprising result, showing how a Bayesian model average can in fact be problematic under covariate shift, particularly in cases where linear dependencies in the input features cause a lack of posterior contraction. We additionally show why the same issue does not affect many approximate inference procedures, or classical maximum a-posteriori (MAP) training. Finally, we propose novel priors that improve the robustness of BNNs to many sources of covariate shift."}}
{"id": "U4-jTL2FGpX", "cdate": 1577836800000, "mdate": 1682604936790, "content": {"title": "Stochastic Damped L-BFGS with Controlled Norm of the Hessian Approximation", "abstract": "We propose a new stochastic variance-reduced damped L-BFGS algorithm, where we leverage estimates of bounds on the largest and smallest eigenvalues of the Hessian approximation to balance its quality and conditioning. Our algorithm, VARCHEN, draws from previous work that proposed a novel stochastic damped L-BFGS algorithm called SdLBFGS. We establish almost sure convergence to a stationary point and a complexity bound. We empirically demonstrate that VARCHEN is more robust than SdLBFGS-VR and SVRG on a modified DavidNet problem -- a highly nonconvex and ill-conditioned problem that arises in the context of deep learning, and their performance is comparable on a logistic regression problem and a nonconvex support-vector machine problem."}}
