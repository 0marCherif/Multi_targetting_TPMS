{"id": "6SR5fTCpAHx", "cdate": 1683882714659, "mdate": 1683882714659, "content": {"title": "Learning Image Relations with Contrast Association Networks", "abstract": "Inferring the relations between two images is an important class of tasks in computer vision. Examples of such tasks include computing optical flow and stereo disparity. We treat the relation inference tasks as a machine learning problem and tackle it with neural networks. A key to the problem is learning a representation of relations. We propose a new neural network module, contrast association unit (CAU), which explicitly models the relations between two sets of input variables. Due to the non-negativity of the weights in CAU, we adopt a multiplicative update algorithm for learning these weights. Experiments show that neural networks with CAUs are more effective in learning five fundamental image transformations than conventional neural networks."}}
{"id": "VIJ4hGzEI8", "cdate": 1683882575153, "mdate": null, "content": {"title": "Doubly Stochastic Neighbor Embedding on Spheres", "abstract": "Stochastic Neighbor Embedding (SNE) methods minimize the divergence between the similarity matrix of a high-dimensional data set and its counterpart from a low-dimensional embedding, leading to widely applied tools for data visualization. Despite their popularity, the current SNE methods experience a crowding problem when the data include highly imbalanced similarities. This implies that the data points with higher total similarity tend to get crowded around the display center. To solve this problem, we introduce a fast normalization method and normalize the similarity matrix to be doubly stochastic such that all the data points have equal total similarities. Furthermore, we show empirically and theoretically that the doubly stochasticity constraint often leads to embeddings which are approximately spherical. This suggests replacing a flat space with spheres as the embedding space. The spherical embedding eliminates the discrepancy between the center and the periphery in visualization, which efficiently resolves the crowding problem. We compared the proposed method (DOSNES) with the state-of-the-art SNE method on three real-world datasets and the results clearly indicate that our method is more favorable in terms of visualization quality."}}
{"id": "saN4UTZUs3", "cdate": 1668589715098, "mdate": 1668589715098, "content": {"title": "Classification of long sequential data using circular dilated convolutional neural networks", "abstract": "Classification of long sequential data is an important Machine Learning task and appears in many application scenarios. Recurrent Neural Networks, Transformers, and Convolutional Neural Networks are three major techniques for learning from sequential data. Among these methods, Temporal Convolutional Networks (TCNs) which are scalable to very long sequences have achieved remarkable progress in time series regression. However, the performance of TCNs for sequence classification is not satisfactory because they use a skewed connection protocol and output classes at the last position. Such asymmetry restricts their performance for classification which depends on the whole sequence. In this work, we propose a symmetric multi-scale architecture called Circular Dilated Convolutional Neural Network (CDIL-CNN), where every position has an equal chance to receive information from other positions at the previous layers. Our model gives classification logits in all positions, and we can apply a simple ensemble learning to achieve a better decision. We have tested CDIL-CNN on various long sequential datasets. The experimental results show that our method has superior performance over many state-of-the-art approaches. The model and experiments are available at (https://github.com/LeiCheng-no/CDIL-CNN)."}}
{"id": "6aPbbtQU6V", "cdate": 1668589407506, "mdate": 1668589407506, "content": {"title": "Sparse factorization of square matrices with application to neural attention modeling", "abstract": "Square matrices appear in many machine learning problems and models. Optimization over a large square matrix is expensive in memory and in time. Therefore an economic approximation is needed. Conventional approximation approaches factorize the square matrix into a number matrices of much lower ranks. However, the low-rank constraint is a performance bottleneck if the approximated matrix is intrinsically high-rank or close to full rank. In this paper, we propose to approximate a large square matrix with a product of sparse full-rank matrices. In the approximation, our method needs only $N(\\log_2 N)^2$ non-zero numbers for an $N\\times N$ full matrix. Our new method is especially useful for scalable neural attention modeling. Different from the conventional scaled dot-product attention methods, we train neural networks to map input data to the non-zero entries of the factorizing matrices. The sparse factorization method is tested for various square matrices, and the experimental results demonstrate that our method gives a better approximation when the approximated matrix is sparse and high-rank. As an attention module, our new method defeats Transformer and its several variants for long sequences in synthetic data sets and in the Long Range Arena benchmarks. Our code is publicly available"}}
{"id": "W1ZiwSlq7H", "cdate": 1668589114409, "mdate": 1668589114409, "content": {"title": "Paramixer: Parameterizing Mixing Links in Sparse Factors Works Better Than Dot-Product Self-Attention ", "abstract": "Self-Attention is a widely used building block in neural modeling to mix long-range data elements. Most self-attention neural networks employ pairwise dot-products to specify the attention coefficients. However, these methods require O(N^2) computing cost for sequence length N. Even though some approximation methods have been introduced to relieve the quadratic cost, the performance of the dot-product approach is still bottlenecked by the low-rank constraint in the attention matrix factorization. In this paper, we propose a novel scalable and effective mixing building block called Paramixer. Our method factorizes the interaction matrix into several sparse matrices, where we parameterize the non-zero entries by MLPs with the data elements as input. The overall computing cost of the new building block is as low as O(N \\log N). Moreover, all factorizing matrices in Paramixer are full-rank, so it does not suffer from the low-rank bottleneck. We have tested the new method on both synthetic and various real-world long sequential data sets and compared it with several state-of-the-art attention networks. The experimental results show that Paramixer has better performance in most learning tasks. "}}
{"id": "E8mzu3JbdR", "cdate": 1663850151902, "mdate": null, "content": {"title": "ChordMixer: A Scalable Neural Attention Model for Sequences with Different Length", "abstract": "Sequential data naturally have different lengths in many domains, with some very long sequences. As an important modeling tool, neural attention should capture long-range interaction in such sequences. However, most existing neural attention models admit only short sequences, or they have to employ chunking or padding to enforce a constant input length. Here we propose a simple neural network building block called ChordMixer which can model the attention for long sequences with variable lengths. Each ChordMixer block consists of a position-wise rotation layer without learnable parameters and an element-wise MLP layer. Repeatedly applying such blocks forms an effective network backbone that mixes the input signals towards the learning targets. We have tested ChordMixer on the synthetic adding problem, long document classification, and DNA sequence-based taxonomy classification. The experiment results show that our method substantially outperforms other neural attention models."}}
{"id": "2ywv2OP1zxX", "cdate": 1594385377585, "mdate": null, "content": {"title": "Doubly Stochastic Matrix Decomposition for Cluster Analysis", "abstract": "Cluster  analysis  by  nonnegative  low-rank  approximations  has  experienced  a  remarkable progress in the past decade.  However, the majority of such approximation approaches are still  restricted  to  nonnegative  matrix  factorization  (NMF)  and  suffer  from  the  following two drawbacks:  1) they are unable to produce balanced partitions for large-scale manifold data which are common in real-world clustering tasks; 2) most existing NMF-type clustering methods cannot automatically determine the number of clusters.  We propose a new low-rank learning method to address these two problems, which is beyond matrix factorization.  Our method approximately decomposes a sparse input similarity in a normalized way and its objective can be used to learn both cluster assignments and the number of clusters.  For efficient optimization, we use a relaxed formulation based on Data-Cluster-Data random walk, which is also shown to be equivalent to low-rank factorization of the doubly-stochastically normalized cluster incidence matrix.  The probabilistic cluster assignments can  thus  be  learned  with  a  multiplicative  majorization-minimization  algorithm.   Experimental  results  show  that  the  new  method  is  more  accurate  both  in  terms  of  clustering large-scale manifold data sets and of selecting the number of clusters."}}
{"id": "HjLVFgHx_aS", "cdate": 1420070400000, "mdate": null, "content": {"title": "Learning the Information Divergence.", "abstract": "Information divergence that measures the difference between two nonnegative matrices or tensors has found its use in a variety of machine learning problems. Examples are Nonnegative Matrix/Tensor Factorization, Stochastic Neighbor Embedding, topic models, and Bayesian network optimization. The success of such a learning task depends heavily on a suitable divergence. A large variety of divergences have been suggested and analyzed, but very few results are available for an objective choice of the optimal divergence for a given task. Here we present a framework that facilitates automatic selection of the best divergence among a given family, based on standard maximum likelihood estimation. We first propose an approximated Tweedie distribution for the \u03b2-divergence family. Selecting the best \u03b2 then becomes a machine learning problem solved by maximum likelihood. Next, we reformulate \u03b1-divergence in terms of \u03b2-divergence, which enables automatic selection of \u03b1 by maximum likelihood with reuse of the learning principle for \u03b2-divergence. Furthermore, we show the connections between \u03b3- and \u03b2-divergences as well as Renyi- and \u03b1-divergences, such that our automatic selection framework is extended to non-separable divergences. Experiments on both synthetic and real-world data demonstrate that our method can quite accurately select information divergence across different learning problems and various divergence families."}}
{"id": "BJWdz2Z_Zr", "cdate": 1388534400000, "mdate": null, "content": {"title": "Optimization Equivalence of Divergences Improves Neighbor Embedding", "abstract": "Visualization methods that arrange data objects in 2D or 3D layouts have followed two main schools, methods oriented for graph layout and methods oriented for vectorial embedding. We show the two p..."}}
{"id": "SyZx_oWOWr", "cdate": 1356998400000, "mdate": null, "content": {"title": "Scalable Optimization of Neighbor Embedding for Visualization", "abstract": "Neighbor embedding (NE) methods have found their use in data visualization but are limited in big data analysis tasks due to their O(n^2) complexity for n data samples. We demonstrate that the obvi..."}}
