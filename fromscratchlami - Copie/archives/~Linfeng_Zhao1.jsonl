{"id": "UsYrDvLeJ0", "cdate": 1675970199456, "mdate": null, "content": {"title": "$\\mathrm{SE}(3)$ Frame Equivariance in Dynamics Modeling and Reinforcement Learning", "abstract": "In this paper, we aim to explore the potential of symmetries in improving the understanding of continuous control tasks in the 3D environment, such as locomotion. \nThe existing work in reinforcement learning on symmetry focuses on pixel-level symmetries in 2D environments or is limited to value-based planning. \nInstead, we considers continuous state and action spaces and continuous symmetry groups, focusing on translational and rotational symmetries.\nWe propose a pipeline to use these symmetries in learning dynamics and control, with the goal of exploiting the underlying symmetry structure to improve dynamics modeling and model-based planning."}}
{"id": "2s-o5cJLO9", "cdate": 1665251228006, "mdate": null, "content": {"title": "Scaling up and Stabilizing Differentiable Planning with Implicit Differentiation", "abstract": "Differentiable planning promises end-to-end differentiability and adaptivity.\nHowever, an issue prevents it from scaling up to larger-scale problems: they\nneed to differentiate through forward iteration layers to compute gradients, which couples forward computation and backpropagation and needs to balance forward planner performance and computational cost of the backward pass.\nTo alleviate this issue, we propose to differentiate through the Bellman fixed-point equation to decouple forward and backward passes for Value Iteration Network and its variants, which enables constant backward cost (in planning horizon) and flexible forward budget and helps scale up to large tasks.\nWe study the convergence stability, scalability, and efficiency of the proposed implicit version of VIN and its variants and demonstrate their superiorities on a range of planning tasks: 2D navigation, visual navigation, and 2-DOF manipulation in configuration space and workspace."}}
{"id": "PYbe4MoHf32", "cdate": 1663850407650, "mdate": null, "content": {"title": "Scaling up and Stabilizing Differentiable Planning with Implicit Differentiation", "abstract": "Differentiable planning promises end-to-end differentiability and adaptivity. However, an issue prevents it from scaling up to larger-scale problems: they need to differentiate through forward iteration layers to compute gradients, which couples forward computation and backpropagation and needs to balance forward planner performance and computational cost of the backward pass. To alleviate this issue, we propose to differentiate through the Bellman fixed-point equation to decouple forward and backward passes for Value Iteration Network and its variants, which enables constant backward cost (in planning horizon) and flexible forward budget and helps scale up to large tasks. We study the convergence stability, scalability, and efficiency of the proposed implicit version of VIN and its variants and demonstrate their superiorities on a range of planning tasks: 2D navigation, visual navigation, and 2-DOF manipulation in configuration space and workspace."}}
{"id": "n7CPzMPKQl", "cdate": 1663850399466, "mdate": null, "content": {"title": "Integrating Symmetry into Differentiable Planning with Steerable Convolutions", "abstract": "To achieve this, we draw inspiration from equivariant convolution networks and model the path planning problem as a set of signals over grids. We demonstrate that value iteration can be treated as a linear equivariant operator, which is effectively a steerable convolution. Building upon Value Iteration Networks (VIN), we propose a new Symmetric Planning (SymPlan) framework that incorporates rotation and reflection symmetry using steerable convolution networks. We evaluate our approach on four tasks: 2D navigation, visual navigation, 2 degrees of freedom (2-DOF) configuration space manipulation, and 2-DOF workspace manipulation. Our experimental results show that our symmetric planning algorithms significantly improve training efficiency and generalization performance compared to non-equivariant baselines, including VINs and GPPN."}}
{"id": "lxdWr1jN8-h", "cdate": 1652737369869, "mdate": null, "content": {"title": "Integrating Symmetry into Differentiable Planning", "abstract": "We study how group symmetry helps improve data efficiency and generalization for end-to-end differentiable planning algorithms, specifically on 2D robotic path planning problems: navigation and manipulation. We first formalize the idea from Value Iteration Networks (VINs) on using convolutional networks for path planning, because it avoids explicitly constructing equivalence classes and enables end-to-end planning. We then show that value iteration can always be represented as some convolutional form for (2D) path planning, and name the resulting paradigm Symmetric Planner (SymPlan). In implementation, we use steerable convolution networks to incorporate symmetry. Our algorithms on navigation and manipulation, with given or learned maps, improve training efficiency and generalization performance by large margins over non-equivariant counterparts, VIN and GPPN."}}
{"id": "kyPMvDp0yIc", "cdate": 1640995200000, "mdate": 1671872328015, "content": {"title": "Integrating Symmetry into Differentiable Planning", "abstract": "We study how group symmetry helps improve data efficiency and generalization for end-to-end differentiable planning algorithms when symmetry appears in decision-making tasks. Motivated by equivariant convolution networks, we treat the path planning problem as \\textit{signals} over grids. We show that value iteration in this case is a linear equivariant operator, which is a (steerable) convolution. This extends Value Iteration Networks (VINs) on using convolutional networks for path planning with additional rotation and reflection symmetry. Our implementation is based on VINs and uses steerable convolution networks to incorporate symmetry. The experiments are performed on four tasks: 2D navigation, visual navigation, and 2 degrees of freedom (2DOFs) configuration space and workspace manipulation. Our symmetric planning algorithms improve training efficiency and generalization by large margins compared to non-equivariant counterparts, VIN and GPPN."}}
{"id": "fOzbzZdwu5k", "cdate": 1640995200000, "mdate": 1671872328016, "content": {"title": "Toward Compositional Generalization in Object-Oriented World Modeling", "abstract": "Compositional generalization is a critical ability in learning and decision-making. We focus on the setting of reinforcement learning in object-oriented environments to study compositional generalization in world modeling. We (1) formalize the compositional generalization problem with an algebraic approach and (2) study how a world model can achieve that. We introduce a conceptual environment, Object Library, and two instances, and deploy a principled pipeline to measure the generalization ability. Motivated by the formulation, we analyze several methods with exact or no compositional generalization ability using our framework, and design a differentiable approach, Homomorphic Object-oriented World Model (HOWM), that achieves soft but more efficient compositional generalization."}}
{"id": "WeCaAgICHs", "cdate": 1640995200000, "mdate": 1671872328020, "content": {"title": "Learning Symmetric Embeddings for Equivariant World Models", "abstract": "Incorporating symmetries can lead to highly data-efficient and generalizable models by defining equivalence classes of data samples related by transformations. However, characterizing how transform..."}}
{"id": "QINRUtq0pW", "cdate": 1640995200000, "mdate": 1671872328020, "content": {"title": "Learning Symmetric Embeddings for Equivariant World Models", "abstract": "Incorporating symmetries can lead to highly data-efficient and generalizable models by defining equivalence classes of data samples related by transformations. However, characterizing how transformations act on input data is often difficult, limiting the applicability of equivariant models. We propose learning symmetric embedding networks (SENs) that encode an input space (e.g. images), where we do not know the effect of transformations (e.g. rotations), to a feature space that transforms in a known manner under these operations. This network can be trained end-to-end with an equivariant task network to learn an explicitly symmetric representation. We validate this approach in the context of equivariant transition models with 3 distinct forms of symmetry. Our experiments demonstrate that SENs facilitate the application of equivariant networks to data with complex symmetry representations. Moreover, doing so can yield improvements in accuracy and generalization relative to both fully-equivariant and non-equivariant baselines."}}
{"id": "8VlpPqT3qe", "cdate": 1640995200000, "mdate": 1671872328017, "content": {"title": "Toward Compositional Generalization in Object-Oriented World Modeling", "abstract": "Compositional generalization is a critical ability in learning and decision-making. We focus on the setting of reinforcement learning in object-oriented environments to study compositional generali..."}}
