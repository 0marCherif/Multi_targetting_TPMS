{"id": "X_AJqHfE1H", "cdate": 1673287852742, "mdate": null, "content": {"title": "Vesselformer: Towards Complete 3D Vessel Graph Generation from Images", "abstract": "The reconstruction of graph representations from Images (Image-to-graph) is a frequent task, especially vessel graph extraction from biomedical images. Traditionally, this problem is tackled by a two-stage process: segmentation followed by skeletonization. However, the ambiguity in the heuristic-based pruning of the centerline graph from the skeleta makes it hard to achieve a compact yet faithful graph representation. Recently, \\textit{Relationformer} proposed an end-to-end solution to extract graphs directly from images. However, it does not consider edge features, particularly radius information, which is crucial in many applications such as flow simulation. Further, Relationformer predicts only patch-based graphs. In this work, we address these two shortcomings. We propose a task-specific token, namely radius-token, which explicitly focuses on capturing radius information between two nodes. Second, we propose an efficient algorithm to infer a large 3D graph from patch inference. Finally, we show experimental results on a synthetic vessel dataset and achieve the first 3D complete graph prediction. Code is available at \\url{https://github.com/chinmay5/vesselformer}.\n"}}
{"id": "vejiCbYFmf9", "cdate": 1640995200000, "mdate": 1682422991432, "content": {"title": "Is it all a cluster game? - Exploring Out-of-Distribution Detection based on Clustering in the Embedding Space", "abstract": "It is essential for safety-critical applications of deep neural networks to determine when new inputs are significantly different from the training distribution. In this paper, we explore this out-of-distribution (OOD) detection problem for image classification using clusters of semantically similar embeddings of the training data and exploit the differences in distance relationships to these clusters between in- and out-of-distribution data. We study the structure and separation of clusters in the embedding space and find that supervised contrastive learning leads to well-separated clusters while its self-supervised counterpart fails to do so. In our extensive analysis of different training methods, clustering strategies, distance metrics, and thresholding approaches, we observe that there is no clear winner. The optimal approach depends on the model architecture and selected datasets for in- and out-of-distribution. While we could reproduce the outstanding results for contrastive training on CIFAR-10 as in-distribution data, we find standard cross-entropy paired with cosine similarity outperforms all contrastive training methods when training on CIFAR-100 instead. Cross-entropy provides competitive results as compared to expensive contrastive training methods."}}
{"id": "r4mflitRE1Z", "cdate": 1640995200000, "mdate": 1682422960139, "content": {"title": "Is it all a cluster game? - Exploring Out-of-Distribution Detection based on Clustering in the Embedding Space", "abstract": ""}}
{"id": "jcwBx0CXNsI", "cdate": 1640995200000, "mdate": 1677745230757, "content": {"title": "Relationformer: A Unified Framework for Image-to-Graph Generation", "abstract": ""}}
{"id": "iBKGXo5gJ4", "cdate": 1640995200000, "mdate": 1682422991516, "content": {"title": "Is it all a cluster game? - Exploring Out-of-Distribution Detection based on Clustering in the Embedding Space", "abstract": ""}}
{"id": "T4xcxIUV-F5", "cdate": 1640995200000, "mdate": 1668022484018, "content": {"title": "InstanceFormer: An Online Video Instance Segmentation Framework", "abstract": "Recent transformer-based offline video instance segmentation (VIS) approaches achieve encouraging results and significantly outperform online approaches. However, their reliance on the whole video and the immense computational complexity caused by full Spatio-temporal attention limit them in real-life applications such as processing lengthy videos. In this paper, we propose a single-stage transformer-based efficient online VIS framework named InstanceFormer, which is especially suitable for long and challenging videos. We propose three novel components to model short-term and long-term dependency and temporal coherence. First, we propagate the representation, location, and semantic information of prior instances to model short-term changes. Second, we propose a novel memory cross-attention in the decoder, which allows the network to look into earlier instances within a certain temporal window. Finally, we employ a temporal contrastive loss to impose coherence in the representation of an instance across all frames. Memory attention and temporal coherence are particularly beneficial to long-range dependency modeling, including challenging scenarios like occlusion. The proposed InstanceFormer outperforms previous online benchmark methods by a large margin across multiple datasets. Most importantly, InstanceFormer surpasses offline approaches for challenging and long datasets such as YouTube-VIS-2021 and OVIS. Code is available at https://github.com/rajatkoner08/InstanceFormer."}}
{"id": "P_u3TH1Ork3", "cdate": 1640995200000, "mdate": 1682422960257, "content": {"title": "Is it all a cluster game? - Exploring Out-of-Distribution Detection based on Clustering in the Embedding Space", "abstract": "It is essential for safety-critical applications of deep neural networks to determine when new inputs are significantly different from the training distribution. In this paper, we explore this out-of-distribution (OOD) detection problem for image classification using clusters of semantically similar embeddings of the training data and exploit the differences in distance relationships to these clusters between in- and out-of-distribution data. We study the structure and separation of clusters in the embedding space and find that supervised contrastive learning leads to well-separated clusters while its self-supervised counterpart fails to do so. In our extensive analysis of different training methods, clustering strategies, distance metrics, and thresholding approaches, we observe that there is no clear winner. The optimal approach depends on the model architecture and selected datasets for in- and out-of-distribution. While we could reproduce the outstanding results for contrastive training on CIFAR-10 as in-distribution data, we find standard cross-entropy paired with cosine similarity outperforms all contrastive training methods when training on CIFAR-100 instead. Cross-entropy provides competitive results as compared to expensive contrastive training methods."}}
{"id": "MoJqukT_lwz", "cdate": 1640995200000, "mdate": 1677745232013, "content": {"title": "Relationformer: A Unified Framework for Image-to-Graph Generation", "abstract": ""}}
{"id": "Jnny7mh_2Yl", "cdate": 1640995200000, "mdate": 1681808547408, "content": {"title": "Box Supervised Video Segmentation Proposal Network", "abstract": "Video Object Segmentation (VOS) has been targeted by various fully-supervised and self-supervised approaches. While fully-supervised methods demonstrate excellent results, self-supervised ones, which do not use pixel-level ground truth, attract much attention. However, self-supervised approaches pose a significant performance gap. Box-level annotations provide a balanced compromise between labeling effort and result quality for image segmentation but have not been exploited for the video domain. In this work, we propose a box-supervised video object segmentation proposal network, which takes advantage of intrinsic video properties. Our method incorporates object motion in the following way: first, motion is computed using a bidirectional temporal difference and a novel bounding box-guided motion compensation. Second, we introduce a novel motion-aware affinity loss that encourages the network to predict positive pixel pairs if they share similar motion and color. The proposed method outperforms the state-of-the-art self-supervised benchmark by 16.4% and 6.9% $\\mathcal{J}$ &$\\mathcal{F}$ score and the majority of fully supervised methods on the DAVIS and Youtube-VOS dataset without imposing network architectural specifications. We provide extensive tests and ablations on the datasets, demonstrating the robustness of our method."}}
{"id": "BkWZjK2Kel", "cdate": 1640995200000, "mdate": 1680025124495, "content": {"title": "Do DALL-E and Flamingo Understand Each Other?", "abstract": ""}}
