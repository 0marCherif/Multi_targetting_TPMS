{"id": "l2cryUoxvaX", "cdate": 1663849925601, "mdate": null, "content": {"title": "Explainability of deep reinforcement learning algorithms in robotic domains by using Layer-wise Relevance Propagation", "abstract": "A key component to the recent success of reinforcement learning is the introduction of neural networks for representation learning. Doing so allows for solving challenging problems in several domains, one of which is robotics. However, a major criticism of deep reinforcement learning (DRL) algorithms is their lack of explainability and interpretability. This problem is even exacerbated in robotics as they oftentimes cohabitate space with humans, making it imperative to be able to reason about their behaviour.\nIn this paper, we propose to analyze the learned representation in a robotic setting by utilizing graph neural networks. Using the graphical neural networks and Layer-wise Relevance Propagation (LRP), we represent the observations as an entity-relationship to allow us to interpret the learned policy. We evaluate our approach in two environments in MuJoCo. These two environments were delicately designed to effectively measure the value of knowledge gained by our approach to analyzing learned representations. This approach allows us to analyze not only how different parts of the observation space contribute to the decision-making process but also differentiate between policies and their differences in performance. This difference in performance also allows for reasoning about the agent's recovery from faults. These insights are key contributions to explainable deep reinforcement learning in robotic settings."}}
{"id": "u3YaVJgbyjM", "cdate": 1640995200000, "mdate": 1682335012380, "content": {"title": "Learning Landmark-Oriented Subgoals for Visual Navigation Using Trajectory Memory", "abstract": "In many deep reinforcement learning (DRL) applications, agents must perform complex and long-horizon tasks that are still challenging in DRL because of temporally ex-tended tasks with sparse rewards. Goal-conditioned hierarchical reinforcement learning (HRL) is a promising approach for control at multiple time scales via a hierarchical structure with subgoals. One of the key issues of goal-conditioned HRL is the definition of subgoals. In this study, we propose a DRL model for learning landmark-oriented subgoals using attention-augmented trajectory memory. In our approach the agent is trained to make decisions based on both 1) current perception, which is a short-term temporal history derived from vanilla long short-term memory (LSTM) and 2) trajectory memory, which represents a contextual summary of long-term historical LSTM states augmented by attention. The experiment on a visual navigation task shows that the short-term LSTM state of the current perception module extracts landmark subgoals as clusters that correspond to lower-level policies, and the long-term context states of trajectory memory extract subgoal transitions that correspond to higher-level policies. Furthermore, the proposed method demonstrated superior adaptability to environmental changes,"}}
{"id": "pqDDqaxqFU", "cdate": 1640995200000, "mdate": 1682335012353, "content": {"title": "Interpretable Navigation Agents Using Attention-Augmented Memory", "abstract": "Deep reinforcement learning (DRL) has achieved remarkable success in various domains, from games to complex tasks. In several DRL applications, the agents must perform long-horizon tasks in partially observable environments. However, owing to the high performance of DRL, the decision-making process for the long-horizon task is unclear and difficult to interpret. Although memory-based models and the Transformer model have been proposed to overcome the interpretability issue, challenges of limited flexibility, lack of stability, and high computational cost remain. To address these concerns, we propose a low-computational-complexity and scalable DRL model that uses attention-augmented memory (AAM) to interpret the long-horizon decision-making process. AAM adds long short-term memory states of past observations to the memory and uses a soft attention bottleneck to combine them into a single contextual vector. The agent is then trained to make decisions based on this AAM together with the current observation. The AAM model is applied to the navigation problem of the Labyrinth [1], and attention and saliency maps are generated to show the areas highlighted by the agent in the current observation and the areas it attends to from memory. The resiliency of the model was evaluated using saliency and it was discovered that the proposed method is more resilient to the noise of visual observations compared with the baseline model. In summary, the proposed method demonstrates an interpretable and noise-robust DRL approach for long-horizon tasks."}}
{"id": "C_BnR2ZkeO5", "cdate": 1483228800000, "mdate": null, "content": {"title": "Distributed Multi-task Learning for Sensor Network", "abstract": "A sensor in a sensor network is expected to be able to make prediction or decision utilizing the models learned from the data observed on this sensor. However, in the early stage of using a sensor, there may be not a lot of data available to train the model for this sensor. A solution is to leverage the observation data from other sensors which have similar conditions and models with the given sensor. We thus propose a novel distributed multi-task learning approach which incorporates neighborhood relations among sensors to learn multiple models simultaneously in which each sensor corresponds to one task. It may be not cheap for each sensor to transfer the observation data from other sensors; broadcasting the observation data of a sensor in the entire network is not satisfied for the reason of privacy protection; each sensor is expected to make real-time prediction independently from neighbor sensors. Therefore, this approach shares the model parameters as regularization terms in the objective function by assuming that neighbor sensors have similar model parameters. We conduct the experiments on two real datasets by predicting the temperature with the regression. They verify that our approach is effective, especially when the bias of an independent model which does not utilize the data from other sensors is high such as when there is not plenty of training data available."}}
{"id": "PdE_GRHUnE6", "cdate": 1451606400000, "mdate": null, "content": {"title": "Investigating recognition accuracy improvement by adding user's acceleration data to location and power consumption-based in-home activity recognition system", "abstract": "Recently, there are many studies on automatic recognition of activities of daily living (ADL) to provide various services such as elderly monitoring, intelligent concierge, and health support. In particular, real-time ADL recognition is essential to realize an intelligent concierge service since the service needs to know user's current or next activity for supporting it. We have been studying real-time ADL recognition using only user's position data and appliances' power consumption data which are considered to include less privacy information than audio and visual data. In the study, we found that some activities such as reading and operating smartphone that happen in similar conditions cannot be classified with only position and power data. In this paper, we propose a new method that adds the acceleration data from wearable devices for classifying activities happening in similar conditions with higher accuracy. In the proposed method, we use the acceleration data from a smart watch and a smartphone worn by user's arm and waist, respectively, in addition to user's position data and appliances' power consumption data, and construct a machine learning model for recognizing 15 types of target activities. We evaluated the recognition accuracy of 3 methods: our previous method (using only position data and power consumption data); the proposed method using the mean value and the standard deviation of the acceleration norm; and the proposed method using the ratio of the activity topics. We collected the sensor data in our smart home facility for 12 days, and applied the proposed method to these sensor data. As a result, the proposed method could recognize the activities with 57% which is 12 % improvement from our previous method without acceleration data."}}
{"id": "z06nREtPuoH", "cdate": 1356998400000, "mdate": null, "content": {"title": "Robust Face Recognition System Using a Reliability Feedback", "abstract": "In the real world there are a variety of lighting conditions, and there exist many directional lights as well as ambient lights. These directional lights cause partial dark and bright regions on faces. Even if auto exposure mode of cameras is used, those uneven pixel intensities are left, and in some cases saturated pixels and black pixels appear. In this paper we propose robust face recognition system using a reliability feedback. The system evaluates the reliability of the input face image using prior distributions of each recognition feature, and if the reliability of the image is not enough for face recognition, it capture multiple images by changing exposure parameters of cameras based on the analysis of saturated pixels and black pixels. As a result the system can cumulates similarity scores of enough amounts of reliable recognition features from multiple face images. By evaluating the system in an office environment, we can achieve three times better EER than the system only with auto exposure control."}}
