{"id": "9APqw4AeXHn", "cdate": 1684155889422, "mdate": 1684155889422, "content": {"title": "Uncertainty Estimates of Predictions via a General Bias-Variance Decomposition", "abstract": "Reliably estimating the uncertainty of a prediction throughout the model lifecycle is crucial in\nmany safety-critical applications. The most common way to measure this uncertainty is via the\npredicted confidence. While this tends to work\nwell for in-domain samples, these estimates are\nunreliable under domain drift and restricted to\nclassification. Alternatively, proper scores can be\nused for most predictive tasks but a bias-variance\ndecomposition for model uncertainty does not\nexist in the current literature. In this work we introduce a general bias-variance decomposition for\nstrictly proper scores, giving rise to the Bregman\nInformation as the variance term. We discover\nhow exponential families and the classification\nlog-likelihood are special cases and provide novel\nformulations. Surprisingly, we can express the\nclassification case purely in the logit space. We\nshowcase the practical relevance of this decomposition on several downstream tasks, including\nmodel ensembles and confidence regions. Further,\nwe demonstrate how different approximations of\nthe instance-level Bregman Information allow out-of-distribution detection for all degrees of domain\ndrift."}}
{"id": "JE1O7JpTvLP", "cdate": 1684155761072, "mdate": 1684155761072, "content": {"title": "Post-hoc Uncertainty Calibration for Domain Drift Scenarios", "abstract": "We address the problem of uncertainty calibration.\nWhile standard deep neural networks typically yield uncalibrated predictions, calibrated confidence scores that are\nrepresentative of the true likelihood of a prediction can be\nachieved using post-hoc calibration methods. However, to\ndate, the focus of these approaches has been on in-domain\ncalibration. Our contribution is two-fold. First, we show\nthat existing post-hoc calibration methods yield highly overconfident predictions under domain shift. Second, we introduce a simple strategy where perturbations are applied to\nsamples in the validation set before performing the post-hoc\ncalibration step. In extensive experiments, we demonstrate\nthat this perturbation step results in substantially better calibration under domain shift on a wide range of architectures\nand modelling tasks."}}
{"id": "PikKk2lF6P", "cdate": 1652737745939, "mdate": null, "content": {"title": "Better Uncertainty Calibration via Proper Scores for Classification and Beyond", "abstract": "With model trustworthiness being crucial for sensitive real-world applications, practitioners are putting more and more focus on improving the uncertainty calibration of deep neural networks.\nCalibration errors are designed to quantify the reliability of probabilistic predictions but their estimators are usually biased and inconsistent.\nIn this work, we introduce the framework of \\textit{proper calibration errors}, which relates every calibration error to a proper score and provides a respective upper bound with optimal estimation properties.\nThis relationship can be used to reliably quantify the model calibration improvement.\nWe theoretically and empirically demonstrate the shortcomings of commonly used estimators compared to our approach.\nDue to the wide applicability of proper scores, this gives a natural extension of recalibration beyond classification."}}
{"id": "tLSn8qmajDe", "cdate": 1640995200000, "mdate": 1682321016681, "content": {"title": "Uncertainty Estimates of Predictions via a General Bias-Variance Decomposition", "abstract": "Reliably estimating the uncertainty of a prediction throughout the model lifecycle is crucial in many safety-critical applications. The most common way to measure this uncertainty is via the predicted confidence. While this tends to work well for in-domain samples, these estimates are unreliable under domain drift and restricted to classification. Alternatively, proper scores can be used for most predictive tasks but a bias-variance decomposition for model uncertainty does not exist in the current literature. In this work we introduce a general bias-variance decomposition for proper scores, giving rise to the Bregman Information as the variance term. We discover how exponential families and the classification log-likelihood are special cases and provide novel formulations. Surprisingly, we can express the classification case purely in the logit space. We showcase the practical relevance of this decomposition on several downstream tasks, including model ensembles and confidence regions. Further, we demonstrate how different approximations of the instance-level Bregman Information allow reliable out-of-distribution detection for all degrees of domain drift."}}
{"id": "mmnYx2ZMtMS", "cdate": 1640995200000, "mdate": 1682321015937, "content": {"title": "Parameterized Temperature Scaling for Boosting the Expressive Power in Post-Hoc Uncertainty Calibration", "abstract": "We address the problem of uncertainty calibration and introduce a novel calibration method, Parametrized Temperature Scaling (PTS). Standard deep neural networks typically yield uncalibrated predictions, which can be transformed into calibrated confidence scores using post-hoc calibration methods. In this contribution, we demonstrate that the performance of accuracy-preserving state-of-the-art post-hoc calibrators is limited by their intrinsic expressive power. We generalize temperature scaling by computing prediction-specific temperatures, parameterized by a neural network. We show with extensive experiments that our novel accuracy-preserving approach consistently outperforms existing algorithms across a large number of model architectures, datasets and metrics (Source code available at: https://github.com/tochris/pts-uncertainty )."}}
{"id": "iZwVWBMzr_7", "cdate": 1640995200000, "mdate": 1682321016673, "content": {"title": "Encoding Domain Knowledge in Multi-view Latent Variable Models: A Bayesian Approach with Structured Sparsity", "abstract": "Many real-world systems are described not only by data from a single source but via multiple data views. In genomic medicine, for instance, patients can be characterized by data from different molecular layers. Latent variable models with structured sparsity are a commonly used tool for disentangling variation within and across data views. However, their interpretability is cumbersome since it requires a direct inspection and interpretation of each factor from domain experts. Here, we propose MuVI, a novel multi-view latent variable model based on a modified horseshoe prior for modeling structured sparsity. This facilitates the incorporation of limited and noisy domain knowledge, thereby allowing for an analysis of multi-view data in an inherently explainable manner. We demonstrate that our model (i) outperforms state-of-the-art approaches for modeling structured sparsity in terms of the reconstruction error and the precision/recall, (ii) robustly integrates noisy domain expertise in the form of feature sets, (iii) promotes the identifiability of factors and (iv) infers interpretable and biologically meaningful axes of variation in a real-world multi-view dataset of cancer patients."}}
{"id": "fFa1k4-Zq90", "cdate": 1640995200000, "mdate": 1682321016707, "content": {"title": "Trustworthy Deep Learning via Proper Calibration Errors: A Unifying Approach for Quantifying the Reliability of Predictive Uncertainty", "abstract": "With model trustworthiness being crucial for sensitive real-world applications, practitioners are putting more and more focus on improving the uncertainty calibration of deep neural networks. Calibration errors are designed to quantify the reliability of probabilistic predictions but their estimators are usually biased and inconsistent. In this work, we introduce the framework of proper calibration errors, which relates every calibration error to a proper score and provides a respective upper bound with optimal estimation properties. This relationship can be used to reliably quantify the model calibration improvement. We theoretically and empirically demonstrate the shortcomings of commonly used estimators compared to our approach. Due to the wide applicability of proper scores, this gives a natural extension of recalibration beyond classification."}}
{"id": "QchkVOoXXjW", "cdate": 1640995200000, "mdate": 1682321016687, "content": {"title": "Grasping Partially Occluded Objects Using Autoencoder-Based Point Cloud Inpainting", "abstract": "Flexible industrial production systems will play a central role in the future of manufacturing due to higher product individualization and customization. A key component in such systems is the robotic grasping of known or unknown objects in random positions. Real-world applications often come with challenges that might not be considered in grasping solutions tested in simulation or lab settings. Partial occlusion of the target object is the most prominent. Examples of occlusion can be supporting structures in the camera\u2019s field of view, sensor imprecision, or parts occluding each other due to the production process. In all these cases, the resulting lack of information leads to shortcomings in calculating grasping points. In this paper, we present an algorithm to reconstruct the missing information. Our inpainting solution facilitates the real-world utilization of robust object matching approaches for grasping point calculation. We demonstrate the benefit of our solution by enabling an existing grasping system embedded in a real-world industrial application to handle occlusions in the input. With our solution, we drastically decrease the number of objects discarded by the process."}}
{"id": "QLvwxnbLzY", "cdate": 1640995200000, "mdate": 1681669993825, "content": {"title": "Workshop on Applied Data Science for Healthcare (DSHealth): Transparent and Human-centered AI", "abstract": "KDD DSHealth 2022, aims to build on the success of the past four years to further catalyze the development of links between academic and commercial data science groups and the rapidly developing translational medicine informatics community. The workshop will stimulate discussion as to strategic areas for development and will lead to future cross-disciplinary collaborations. In accordance with the multi-year goal to continue fostering this community as a series of KDD workshops via timely topics, this year the workshop will focus on the transparency and human-centered AI in healthcare. The workshop invites full papers, as well as work-in-progress on the application of data science in healthcare. The workshop will feature four invited talks from eminent speakers, spanning academia, industry, clinical researchers, and governmental regulatory bodies. In addition, selected papers will be invited to publish in a special issue of Journal of Healthcare Informatics Research. The summary gives a brief description of the full-day workshop to be held on August 14th, 2022."}}
{"id": "BWhRs3kpHGP", "cdate": 1640995200000, "mdate": 1684138659526, "content": {"title": "Better Uncertainty Calibration via Proper Scores for Classification and Beyond", "abstract": "With model trustworthiness being crucial for sensitive real-world applications, practitioners are putting more and more focus on improving the uncertainty calibration of deep neural networks.Calibration errors are designed to quantify the reliability of probabilistic predictions but their estimators are usually biased and inconsistent.In this work, we introduce the framework of \\textit{proper calibration errors}, which relates every calibration error to a proper score and provides a respective upper bound with optimal estimation properties.This relationship can be used to reliably quantify the model calibration improvement.We theoretically and empirically demonstrate the shortcomings of commonly used estimators compared to our approach.Due to the wide applicability of proper scores, this gives a natural extension of recalibration beyond classification."}}
