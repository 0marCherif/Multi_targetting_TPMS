{"id": "UpJV4RjPn-", "cdate": 1671907792353, "mdate": 1671907792353, "content": {"title": "Threading the Needle of On and Off-Manifold Value Functions for Shapley Explanations", "abstract": "A popular explainable AI (XAI) approach to quantify feature importance of a given model is via Shapley values. These Shapley values arose in cooperative games, and hence a critical ingredient to compute these in an XAI context is a so-called value function, that computes the \"value\" of a subset of features, and which connects machine learning models to cooperative games. There are many possible choices for such value functions, which broadly fall into two categories: on-manifold and off-manifold value functions, which take an observational and an interventional viewpoint respectively. Both these classes however have their respective flaws, where on-manifold value functions violate key axiomatic properties and are computationally expensive, while off-manifold value functions pay less heed to the data manifold and evaluate the model on regions for which it wasn't trained. Thus, there is no consensus on which class of value functions to use. In this paper, we show that in addition to these existing issues, both classes of value functions are prone to adversarial manipulations on low density regions. We formalize the desiderata of value functions that respect both the model and the data manifold in a set of axioms and are robust to perturbation on off-manifold regions, and show that there exists a unique value function that satisfies these axioms, which we term the Joint Baseline value function, and the resulting Shapley value the Joint Baseline Shapley (JBshap), and validate the effectiveness of JBshap in experiments."}}
{"id": "JBQAX2OcxZw", "cdate": 1640995200000, "mdate": 1682610508638, "content": {"title": "New Information Inequalities with Applications to Statistics", "abstract": "Author(s): Lee, Kuan-Yun | Advisor(s): Courtade, Thomas | Abstract: We introduce, under a parametric framework, a family of inequalities between mutual information and Fisher information. These inequalities are indexed by reference measures satisfying a log-Sobolev inequality (LSI), and reveal previously unknown connections between LSIs and statistical inequalities. One such connection is shown for the celebrated van Trees inequality by recovering under a Gaussian reference measure a stronger entropic inequality due to Efroimovich. We further present two new inequalities for log-concave priors that do not depend on the Fisher information of the prior and are applicable under certain scenarios where the van Trees inequality and Efroimovich\u2019s inequality cannot be applied. We illustrate a procedure to establish lower bounds on risk under general loss functions, and apply it under several statistical settings, including the Generalized Linear Model and a general pairwise comparison framework."}}
{"id": "EsGOjcePcH", "cdate": 1577836800000, "mdate": 1682610508636, "content": {"title": "Linear Models are Most Favorable among Generalized Linear Models", "abstract": "We establish a nonasymptotic lower bound on the L <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sub> minimax risk for a class of generalized linear models. It is further shown that the minimax risk for the canonical linear model matches this lower bound up to a universal constant. Therefore, the canonical linear model may be regarded as most favorable among the considered class of generalized linear models (in terms of minimax risk). The proof makes use of an information-theoretic Bayesian Cram\u00e9r-Rao bound for log-concave priors, established by Aras et al. (2019)."}}
{"id": "8L7BfvumBe", "cdate": 1577836800000, "mdate": 1682610508637, "content": {"title": "Minimax Bounds for Generalized Linear Models", "abstract": "We establish a new class of minimax prediction error bounds for generalized linear models. Our bounds significantly improve previous results when the design matrix is poorly structured, including natural cases where the matrix is wide or does not have full column rank. Apart from the typical $L_2$ risks, we study a class of entropic risks which recovers the usual $L_2$ prediction and estimation risks, and demonstrate that a tight analysis of Fisher information can uncover underlying structural dependency in terms of the spectrum of the design matrix. The minimax approach we take differs from the traditional metric entropy approach, and can be applied to many other settings."}}
{"id": "PtmMizdq2d", "cdate": 1546300800000, "mdate": 1682610508637, "content": {"title": "A Family of Bayesian Cram\u00e9r-Rao Bounds, and Consequences for Log-Concave Priors", "abstract": "Under minimal regularity assumptions, we establish a family of information-theoretic Bayesian Cram\u00e9r-Rao bounds, indexed by probability measures that satisfy a logarithmic Sobolev inequality. This family includes as a special case the known Bayesian Cram\u00e9r-Rao bound (or van Trees inequality), and its less widely known entropic improvement due to Efroimovich. For the setting of a log-concave prior, we obtain a Bayesian Cram\u00e9r-Rao bound which holds for any (possibly biased) estimator and, unlike the van Trees inequality, does not depend on the Fisher information of the prior."}}
{"id": "MMjp3aOrBZ1", "cdate": 1483228800000, "mdate": 1682610508638, "content": {"title": "Partial image blur detection and segmentation from a single snapshot", "abstract": "In this paper, we address the problem of detecting and segmenting partial image blur from a single input image. Instead of assuming particular image priors or requiring additional user annotation, we propose a novel learning framework which jointly solves the tasks of blur kernel estimation and image blur segmentation, so that partial image blur can be automatically separated from the remaining parts of the input image. By alternating between the two learning tasks, we show that our proposed method would achieve promising detection and segmentation performance, which would benefit further processing or analysis tasks of interest. We also verify that, via both qualitative and quantitative evaluation, our approach would perform favorably against state-of-the-art blur detection or segmentation works."}}
{"id": "W0_lKQR9FY_", "cdate": 1451606400000, "mdate": 1682610508638, "content": {"title": "Extracting sparse data via histogram queries", "abstract": "We investigate the problem of extracting a sparse data set via histogram queries. A data set is a collection of items, and each item carries a piece of data. A data set is called sparse if there are only a small number of items carrying data of interest. We show that the fundamental limit on the query complexity is equation, n being the size of the data set and k <; n being the sparsity level. A counting argument is used to establish the converse part, that is, the lower bound on query complexity. For the achievability part, we analyze a randomized querying method, where in each query, the items to be included in the queried subset are uniformly randomly selected. It is shown that with high probability, the randomly constructed querying method exactly recovers the desired data. Furthermore, we propose an adaptive deterministic algorithm to extract the sparse data set with query complexity equation, achieving the fundamental limit to within a log log k factor."}}
{"id": "HGxxg7zg0SV", "cdate": 1451606400000, "mdate": 1682610508637, "content": {"title": "Data extraction via histogram and arithmetic mean queries: Fundamental limits and algorithms", "abstract": "The problems of extracting information from a data set via histogram queries or arithmetic mean queries are considered. We first show that the fundamental limit on the number of histogram queries, m, so that the entire data set of size n can be extracted losslessly, is m = \u0398(n/log n), sub-linear in the size of the data set. For proving the lower bound (converse), we use standard arguments based on simple counting. For proving the upper bound (achievability), we proposed two query mechanisms. The first mechanism is random sampling, where in each query, the items to be included in the queried subset are uniformly randomly selected. With random sampling, it is shown that the entire data set can be extracted with vanishing error probability using \u03a9(n/log n) queries. The second one is a non-adaptive deterministic algorithm. With this algorithm, it is shown that the entire data set can be extracted exactly (no error) using \u03a9(n/log n) queries. We then extend the results to arithmetic mean queries, and show that for data sets taking values in a real-valued finite arithmetic progression, the fundamental limit on the number of arithmetic mean queries to extract the entire data set is also \u0398(n/log n)."}}
