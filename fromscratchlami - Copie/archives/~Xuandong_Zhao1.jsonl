{"id": "RsYG2DAsc1", "cdate": 1676827098917, "mdate": null, "content": {"title": "\"Private Prediction Strikes Back!\" Private Kernelized Nearest Neighbors with Individual R\\'{e}nyi Filter", "abstract": "Most existing approaches of differentially private (DP) machine learning focus on private training.  Despite its many advantages, private training lacks the flexibility in adapting to incremental changes to the training dataset such as deletion requests from exercising GDPR\u2019s right to be forgotten. We revisit a long-forgotten alternative, known as private prediction, and propose a new algorithm named Individual Kernelized Nearest Neighbor (Ind-KNN). Ind-KNN is easily updatable over dataset changes and it allows precise control of the R\\'{e}nyi DP at an individual user level --- a user's privacy loss is measured by the exact amount of her contribution to predictions; and a user is removed if her prescribed privacy budget runs out. Our results show that Ind-KNN consistently improves the accuracy over existing private prediction methods for a wide range of $\\epsilon$ on four vision and language tasks. We also illustrate several cases under which Ind-KNN is preferable over private training with NoisySGD. "}}
{"id": "3zSaMVmXnWO", "cdate": 1672531200000, "mdate": 1682319291748, "content": {"title": "Protecting Language Generation Models via Invisible Watermarking", "abstract": "Language generation models have been an increasingly powerful enabler for many applications. Many such models offer free or affordable API access, which makes them potentially vulnerable to model extraction attacks through distillation. To protect intellectual property (IP) and ensure fair use of these models, various techniques such as lexical watermarking and synonym replacement have been proposed. However, these methods can be nullified by obvious countermeasures such as \"synonym randomization\". To address this issue, we propose GINSEW, a novel method to protect text generation models from being stolen through distillation. The key idea of our method is to inject secret signals into the probability vector of the decoding steps for each target token. We can then detect the secret message by probing a suspect model to tell if it is distilled from the protected one. Experimental results show that GINSEW can effectively identify instances of IP infringement with minimal impact on the generation quality of protected APIs. Our method demonstrates an absolute improvement of 19 to 29 points on mean average precision (mAP) in detecting suspects compared to previous methods against watermark removal attacks."}}
{"id": "jCpTofV7iY_", "cdate": 1663850231496, "mdate": null, "content": {"title": "Pre-trained Language Models can be Fully Zero-Shot Learners", "abstract": "How can we extend a pre-trained model to many language understanding tasks, without labeled or additional unlabeled data? Pre-trained language models (PLMs) have been effective for a wide range of NLP tasks. However, existing approaches either require fine-tuning on downstream labeled datasets or manually constructing proper prompts. In this paper, we propose nonparametric prompting PLM (NPPrompt) for fully zero-shot language understanding. Unlike previous methods, NPPrompt uses only pre-trained language models and does not require any labeled data or additional raw corpus for further fine-tuning, nor does it rely on humans to construct a comprehensive set of prompt label words. We evaluate NPPrompt against previous major few-shot and zero-shot learning methods on diverse NLP tasks: including text classification, text entailment, similar text retrieval, and paraphrasing. Experimental results demonstrate that our NPPrompt outperforms the previous best fully zero-shot method by big margins, with absolute gains of 12.8% in accuracy on text classification and 18.9% on the GLUE benchmark."}}
{"id": "axg6tCKUubG", "cdate": 1640995200000, "mdate": 1682319291769, "content": {"title": "Compressing Sentence Representation for Semantic Retrieval via Homomorphic Projective Distillation", "abstract": ""}}
{"id": "UOeVXRPf8D", "cdate": 1640995200000, "mdate": 1682319291667, "content": {"title": "Distillation-Resistant Watermarking for Model Protection in NLP", "abstract": ""}}
{"id": "2vjyGLKGIH", "cdate": 1640995200000, "mdate": 1682319291689, "content": {"title": "Provably Confidential Language Modelling", "abstract": ""}}
{"id": "X1MOO6ZCd6B", "cdate": 1609459200000, "mdate": 1682319291770, "content": {"title": "An Optimal Reduction of TV-Denoising to Adaptive Online Learning", "abstract": "We consider the problem of estimating a function from $n$ noisy samples whose discrete Total Variation (TV) is bounded by $C_n$. We reveal a deep connection to the seemingly disparate problem of \\emph{Strongly Adaptive} online learning [Daniely et al 2015] and provide an $O(n \\log n)$ time algorithm that attains the near minimax optimal rate of $\\tilde O (n^{1/3}C_n^{2/3})$ under squared error loss. The resulting algorithm runs online and optimally \\emph{adapts} to the \\emph{unknown} smoothness parameter $C_n$. This leads to a new and more versatile alternative to wavelets-based methods for (1) adaptively estimating TV bounded functions; (2) online forecasting of TV bounded trends in time series."}}
{"id": "tPR57xjySp", "cdate": 1546300800000, "mdate": 1682319291640, "content": {"title": "Predicting Alzheimer's Disease by Hierarchical Graph Convolution from Positron Emission Tomography Imaging", "abstract": "Imaging-based early diagnosis of Alzheimer Disease (AD) has become an effective approach, especially by using nuclear medicine imaging techniques such as Positron Emission Topography (PET). In various literature it has been found that PET images can be better modeled as signals (e.g. uptake of florbetapir) defined on a network (non-Euclidean) structure which is governed by its underlying graph patterns of pathological progression and metabolic connectivity. In order to effectively apply deep learning framework for PET image analysis to overcome its limitation on Euclidean grid, we develop a solution for 3D PET image representation and analysis under a generalized, graph-based CNN architecture (PETNet), which analyzes PET signals defined on a group-wise inferred graph structure. Computations in PETNet are defined in non-Euclidean, graph (network) domain, as it performs feature extraction by convolution operations on spectral-filtered signals on the graph and pooling operations based on hierarchical graph clustering. Effectiveness of the PETNet is evaluated on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset, which shows improved performance over both deep learning and other machine learning-based methods."}}
{"id": "GypnBuNlZPD", "cdate": 1546300800000, "mdate": null, "content": {"title": "Multi-Size Computer-Aided Diagnosis Of Positron Emission Tomography Images Using Graph Convolutional Networks", "abstract": "Recent advancement of deep learning-based algorithms has greatly improved the field of medical image analysis and computer-aided diagnosis/prognosis. Convolutional Neural Network (CNN) has shown superior accuracy and generalizability in performing prediction/classification tasks, thanks to its good utilization of the grid-like structure of input images in Euclidean space. In practice, one of the challenges in using classical CNN is the multi-size nature of medical images, which is especially prominent when the input images are from specific target region of interest (ROI) (e.g. tumor). Image sizes of those ROIs can vary a lot across patients, making the images difficult to be analyzed by CNNs where constant-sized inputs are expected. To address this challenge, we propose the Deep Voxel-Graph Convolution Network (DVGCN). DVGCN represents input images as their affinity graph and performs graph convolution to extract discriminative features. It then utilizes a sortpooling layer to sort the nodes and unifies the feature size used for prediction across images, thus solves multi-size challenge without explicitly resizing images. DVGCN is tested on 3D Positron-Emission Tomography (PET) images to predict the patient's cancer staging, its performance is compared with classical 3DCNN (with image padding) and radiomics models."}}
