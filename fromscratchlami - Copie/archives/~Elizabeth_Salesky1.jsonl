{"id": "lfttad220gm", "cdate": 1672531200000, "mdate": 1681651780178, "content": {"title": "A Holistic Cascade System, benchmark, and Human Evaluation Protocol for Expressive Speech-to-Speech Translation", "abstract": ""}}
{"id": "FkSp8VW8RjH", "cdate": 1663850101431, "mdate": null, "content": {"title": "Language Modelling with Pixels", "abstract": "Language models are defined over a finite set of inputs, which creates a vocabulary bottleneck when we attempt to scale the number of supported languages. Tackling this bottleneck results in a trade-off between what can be represented in the embedding matrix and computational issues in the output layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which suffers from neither of these issues. PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels. PIXEL is trained to reconstruct the pixels of masked patches instead of predicting a distribution over tokens. We pretrain the 86M parameter PIXEL model on the same English data as BERT and evaluate on syntactic and semantic tasks in typologically diverse languages, including various non-Latin scripts. We find that PIXEL substantially outperforms BERT on syntactic and semantic processing tasks on scripts that are not found in the pretraining data, but PIXEL is slightly weaker than BERT when working with Latin scripts. Furthermore, we find that PIXEL is more robust than BERT to orthographic attacks and linguistic code-switching, further confirming the benefits of modelling language with pixels."}}
{"id": "rOF4QtCpSe", "cdate": 1640995200000, "mdate": 1664915183250, "content": {"title": "Findings of the IWSLT 2022 Evaluation Campaign", "abstract": "Antonios Anastasopoulos, Lo\u00efc Barrault, Luisa Bentivogli, Marcely Zanon Boito, Ond\u0159ej Bojar, Roldano Cattoni, Anna Currey, Georgiana Dinu, Kevin Duh, Maha Elbayad, Clara Emmanuel, Yannick Est\u00e8ve, Marcello Federico, Christian Federmann, Souhir Gahbiche, Hongyu Gong, Roman Grundkiewicz, Barry Haddow, Benjamin Hsu, D\u00e1vid Javorsk\u00fd, V\u0115ra Kloudov\u00e1, Surafel Lakew, Xutai Ma, Prashant Mathur, Paul McNamee, Kenton Murray, Maria N\u01cedejde, Satoshi Nakamura, Matteo Negri, Jan Niehues, Xing Niu, John Ortega, Juan Pino, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Sebastian St\u00fcker, Katsuhito Sudoh, Marco Turchi, Yogesh Virkar, Alexander Waibel, Changhan Wang, Shinji Watanabe. Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022). 2022."}}
{"id": "ppfXxkhPq57", "cdate": 1640995200000, "mdate": 1667535273495, "content": {"title": "BibleTTS: a large, high-fidelity, multilingual, and uniquely African speech corpus", "abstract": "BibleTTS is a large, high-quality, open speech dataset for ten languages spoken in Sub-Saharan Africa. The corpus contains up to 86 hours of aligned, studio quality 48kHz single speaker recordings per language, enabling the development of high-quality text-to-speech models. The ten languages represented are: Akuapem Twi, Asante Twi, Chichewa, Ewe, Hausa, Kikuyu, Lingala, Luganda, Luo, and Yoruba. This corpus is a derivative work of Bible recordings made and released by the Open.Bible project from Biblica. We have aligned, cleaned, and filtered the original recordings, and additionally hand-checked a subset of the alignments for each language. We present results for text-to-speech models with Coqui TTS. The data is released under a commercial-friendly CC-BY-SA license."}}
{"id": "Rv7X73rMTD", "cdate": 1640995200000, "mdate": 1681651780151, "content": {"title": "Language Modelling with Pixels", "abstract": ""}}
{"id": "OsUW5UDj-Nd", "cdate": 1640995200000, "mdate": 1681651780141, "content": {"title": "BibleTTS: a large, high-fidelity, multilingual, and uniquely African speech corpus", "abstract": ""}}
{"id": "I2M6971Mq1", "cdate": 1640995200000, "mdate": 1676906772693, "content": {"title": "UniMorph 4.0: Universal Morphology", "abstract": ""}}
{"id": "6Vz_4rUrLu", "cdate": 1640995200000, "mdate": 1676906772697, "content": {"title": "UniMorph 4.0: Universal Morphology", "abstract": ""}}
{"id": "z9ABda7LvjB", "cdate": 1609459200000, "mdate": 1634246552217, "content": {"title": "SIGTYP 2021 Shared Task: Robust Spoken Language Identification", "abstract": "While language identification is a fundamental speech and language processing task, for many languages and language families it remains a challenging task. For many low-resource and endangered languages this is in part due to resource availability: where larger datasets exist, they may be single-speaker or have different domains than desired application scenarios, demanding a need for domain and speaker-invariant language identification systems. This year's shared task on robust spoken language identification sought to investigate just this scenario: systems were to be trained on largely single-speaker speech from one domain, but evaluated on data in other domains recorded from speakers under different recording circumstances, mimicking realistic low-resource scenarios. We see that domain and speaker mismatch proves very challenging for current methods which can perform above 95% accuracy in-domain, which domain adaptation can address to some degree, but that these conditions merit further investigation to make spoken language identification accessible in many scenarios."}}
{"id": "weo0Tn-clV", "cdate": 1609459200000, "mdate": 1681651780156, "content": {"title": "Assessing Evaluation Metrics for Speech-to-Speech Translation", "abstract": ""}}
