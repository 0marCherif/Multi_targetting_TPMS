{"id": "Yj8MqtZ5Gg", "cdate": 1672531200000, "mdate": 1696580655243, "content": {"title": "Neural Modulation Fields for Conditional Cone Beam Neural Tomography", "abstract": "Conventional Computed Tomography (CT) methods require large numbers of noise-free projections for accurate density reconstructions, limiting their applicability to the more complex class of Cone Beam Geometry CT (CBCT) reconstruction. Recently, deep learning methods have been proposed to overcome these limitations, with methods based on neural fields (NF) showing strong performance, by approximating the reconstructed density through a continuous-in-space coordinate based neural network. Our focus is on improving such methods, however, unlike previous work, which requires training an NF from scratch for each new set of projections, we instead propose to leverage anatomical consistencies over different scans by training a single conditional NF on a dataset of projections. We propose a novel conditioning method where local modulations are modeled per patient as a field over the input domain through a Neural Modulation Field (NMF). The resulting Conditional Cone Beam Neural Tomography (CondCBNT) shows improved performance for both high and low numbers of available projections on noise-free and noisy data."}}
{"id": "NpCp0V5Kv2", "cdate": 1672531200000, "mdate": 1681726314303, "content": {"title": "Modelling Long Range Dependencies in N-D: From Task-Specific to a General Purpose CNN", "abstract": "Performant Convolutional Neural Network (CNN) architectures must be tailored to specific tasks in order to consider the length, resolution, and dimensionality of the input data. In this work, we tackle the need for problem-specific CNN architectures. We present the Continuous Convolutional Neural Network (CCNN): a single CNN able to process data of arbitrary resolution, dimensionality and length without any structural changes. Its key component are its continuous convolutional kernels which model long-range dependencies at every layer, and thus remove the need of current CNN architectures for task-dependent downsampling and depths. We showcase the generality of our method by using the same architecture for tasks on sequential ($1{\\rm D}$), visual ($2{\\rm D}$) and point-cloud ($3{\\rm D}$) data. Our CCNN matches and often outperforms the current state-of-the-art across all tasks considered."}}
{"id": "8hgCS_DA2UD", "cdate": 1672531200000, "mdate": 1696580655241, "content": {"title": "Modelling Long Range Dependencies in $N$D: From Task-Specific to a General Purpose CNN", "abstract": ""}}
{"id": "ZW5aK4yCRqU", "cdate": 1663849992576, "mdate": null, "content": {"title": "Modelling Long Range Dependencies in $N$D: From Task-Specific to a General Purpose CNN", "abstract": "Performant Convolutional Neural Network (CNN) architectures must be tailored to specific tasks in order to consider the length, resolution, and dimensionality of the input data. In this work, we tackle the need for problem-specific CNN architectures. We present the Continuous Convolutional Neural Network (CCNN): a single CNN able to process data of arbitrary resolution, dimensionality and length without any structural changes.  Its key component are its continuous convolutional kernels which model long-range dependencies at every layer, and thus remove the need of current CNN architectures for task-dependent downsampling and depths. We showcase the generality of our method by using the same architecture for tasks on sequential ($1{\\rm D}$), visual ($2{\\rm D}$) and point-cloud ($3{\\rm D}$) data. Our CCNN matches and often outperforms the current state-of-the-art across all tasks considered."}}
{"id": "jicBjxVsIL", "cdate": 1640995200000, "mdate": 1681726314490, "content": {"title": "Exploiting Redundancy: Separable Group Convolutional Networks on Lie Groups", "abstract": "Group convolutional neural networks (G-CNNs) have been shown to increase parameter efficiency and model accuracy by incorporating geometric inductive biases. In this work, we investigate the proper..."}}
{"id": "D_uBE_FNAMA", "cdate": 1640995200000, "mdate": 1681726314262, "content": {"title": "Towards a General Purpose CNN for Long Range Dependencies in ND", "abstract": "The use of Convolutional Neural Networks (CNNs) is widespread in Deep Learning due to a range of desirable model properties which result in an efficient and effective machine learning framework. However, performant CNN architectures must be tailored to specific tasks in order to incorporate considerations such as the input length, resolution, and dimentionality. In this work, we overcome the need for problem-specific CNN architectures with our Continuous Convolutional Neural Network (CCNN): a single CNN architecture equipped with continuous convolutional kernels that can be used for tasks on data of arbitrary resolution, dimensionality and length without structural changes. Continuous convolutional kernels model long range dependencies at every layer, and remove the need for downsampling layers and task-dependent depths needed in current CNN architectures. We show the generality of our approach by applying the same CCNN to a wide set of tasks on sequential (1$\\mathrm{D}$) and visual data (2$\\mathrm{D}$). Our CCNN performs competitively and often outperforms the current state-of-the-art across all tasks considered."}}
{"id": "Y-kXbPYtzsg", "cdate": 1639159629775, "mdate": null, "content": {"title": " Learning to Automatically Generate Accurate ECG Captions", "abstract": "The electrocardiogram (ECG) is an affordable, non-invasive and quick method to gain essential information about the electrical activity of the heart. Interpreting ECGs is a time-consuming process even for experienced cardiologists, which motivates the current usage of rule-based methods in clinical practice to automatically describe ECGs. However, in comparison to descriptions created by experts, ECG-descriptions generated by such rule-based methods show considerable limitations. Inspired by image captioning methods, we instead propose a data-driven approach for ECG description generation. We introduce a label-guided Transformer model, and show that it is possible to automatically generate relevant and readable ECG descriptions with a data-driven captioning model. We incorporate prior ECG labels into our model design, and show this improves the overall quality of generated descriptions. We find that training these models on free-text annotations of ECGs - instead of the clinically-used computer generated ECG descriptions - greatly improves performance. Moreover, we perform a human expert evaluation study of our best system, which shows that our data-driven approach improves upon existing rule-based methods."}}
{"id": "WnOLO1f50MH", "cdate": 1632875683327, "mdate": null, "content": {"title": "Exploiting Redundancy: Separable Group Convolutional Networks on Lie Groups", "abstract": "Group convolutional neural networks (G-CNNs) have been shown to increase parameter efficiency and model accuracy by incorporating geometric inductive biases. In this work, we investigate the properties of representations learned by regular G-CNNs, and show considerable parameter redundancy in group convolution kernels. This finding motivates further weight-tying by sharing convolution kernels over subgroups. To this end, we introduce convolution kernels that are separable over the subgroup and channel dimensions. In order to obtain equivariance to arbitrary affine Lie groups we provide a continuous parameterisation of separable convolution kernels. We evaluate our approach across several vision datasets, and show that our weight sharing leads to improved performance and computational efficiency. In many settings, separable G-CNNs outperform their non-separable counterpart, while only using a fraction of their training time. In addition, thanks to the increase in computational efficiency, we are able to implement G-CNNs equivariant to the $\\mathrm{Sim(2)}$ group; the group of dilations, rotations and translations.  $\\mathrm{Sim(2)}$-equivariance further improves performance on all tasks considered."}}
{"id": "jAZtnOvfIk", "cdate": 1609459200000, "mdate": 1681726314494, "content": {"title": "Exploiting Redundancy: Separable Group Convolutional Networks on Lie Groups", "abstract": "Group convolutional neural networks (G-CNNs) have been shown to increase parameter efficiency and model accuracy by incorporating geometric inductive biases. In this work, we investigate the properties of representations learned by regular G-CNNs, and show considerable parameter redundancy in group convolution kernels. This finding motivates further weight-tying by sharing convolution kernels over subgroups. To this end, we introduce convolution kernels that are separable over the subgroup and channel dimensions. In order to obtain equivariance to arbitrary affine Lie groups we provide a continuous parameterisation of separable convolution kernels. We evaluate our approach across several vision datasets, and show that our weight sharing leads to improved performance and computational efficiency. In many settings, separable G-CNNs outperform their non-separable counterpart, while only using a fraction of their training time. In addition, thanks to the increase in computational efficiency, we are able to implement G-CNNs equivariant to the $\\mathrm{Sim(2)}$ group; the group of dilations, rotations and translations. $\\mathrm{Sim(2)}$-equivariance further improves performance on all tasks considered."}}
