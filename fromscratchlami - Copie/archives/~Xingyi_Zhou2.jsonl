{"id": "vzuOFWPEsk", "cdate": 1668603895623, "mdate": 1668603895623, "content": {"title": "Detecting Twenty-thousand Classes using Image-level Supervision", "abstract": "Abstract. Current object detectors are limited in vocabulary size due to\nthe small scale of detection datasets. Image classi\fers, on the other hand,\nreason about much larger vocabularies, as their datasets are larger and\neasier to collect. We propose Detic, which simply trains the classi\fers of a\ndetector on image classi\fcation data and thus expands the vocabulary of\ndetectors to tens of thousands of concepts. Unlike prior work, Detic does\nnot need complex assignment schemes to assign image labels to boxes\nbased on model predictions, making it much easier to implement and\ncompatible with a range of detection architectures and backbones. Our\nresults show that Detic yields excellent detectors even for classes without\nbox annotations. It outperforms prior work on both open-vocabulary and\nlong-tail detection benchmarks. Detic provides a gain of 2.4 mAP for\nall classes and 8.3 mAP for novel classes on the open-vocabulary LVIS\nbenchmark. On the standard LVIS benchmark, Detic obtains 41.7 mAP\nwhen evaluated on all classes, or only rare classes, hence closing the gap in\nperformance for object categories with few samples. For the \frst time, we\ntrain a detector with all the twenty-one-thousand classes of the ImageNet\ndataset and show that it generalizes to new datasets without \fnetuning.\nCode is available at https://github.com/facebookresearch/Detic."}}
{"id": "V8isglQkt74", "cdate": 1663850523996, "mdate": null, "content": {"title": "Towards Learning Implicit Symbolic Representation for Visual Reasoning", "abstract": "Visual reasoning tasks are designed to test a learning algorithm's capability to infer causal relationships, discover object interactions, and understand temporal dynamics, all from visual cues. It is commonly believed that to achieve compositional generalization on visual reasoning, an explicit abstraction of the visual scene must be constructed; for example, object detection can be applied to the visual input to produce representations that are then processed by a neural network or a neuro-symbolic framework. We demonstrate that a simple and general self-supervised approach is able to learn implicit symbolic representations with general-purpose neural networks, enabling the end-to-end learning of visual reasoning directly from raw visual inputs. Our proposed approach ``compresses'' each frame of a video into a small set of tokens with a transformer network. The self-supervised learning objective is to reconstruct each image based on the compressed temporal context. To minimize the reconstruction loss, the network must learn a compact representation for each image, as well as capture temporal dynamics and object permanence from temporal context. We evaluate the proposed approach on two visual reasoning benchmarks, CATER and ACRE. We observe that self-supervised pretraining is essential to achieve compositional generalization for our end-to-end trained neural network, and our proposed method achieves on par or better performance compared to recent neuro-symbolic approaches that often require additional object-level supervision."}}
{"id": "WW7BJ15ivoo", "cdate": 1663850046645, "mdate": null, "content": {"title": "MaskConver: A Universal Panoptic and Semantic Segmentation Model with Pure Convolutions", "abstract": "Universal panoptic segmentation models have achieved state-of-the-art quality by using transformers for predicting masks.\nHowever, in mobile applications, transformer models are not computation-friendly due to the quadratic complexity with respect to the input length. In this work, we present MaskConver, a unified panoptic and semantic segmentation model with pure convolutions, which is optimized for mobile devices. We propose a novel lightweight mask embedding decoder to predict mask embeddings. These mask embeddings are used to predict a set of binary masks for both things and stuff classes. MaskConver achieves \\textbf{37.2\\%} panoptic quality score on COCO validation set, which is \\textbf{6.4\\%} better than Panoptic DeepLab with the same MobileNet backbone.\nAfter mobile-specific optimizations, MaskConver runs at \\textbf{30} FPS and delivers 29.7\\% panoptic quality score on a Pixel 6, making it a real-time model, which is 10$\\times$ faster than Panoptic DeepLab using the same backbone."}}
{"id": "8xoN9ZdSW8", "cdate": 1621629870096, "mdate": null, "content": {"title": "Multimodal Virtual Point 3D Detection", "abstract": "Lidar-based sensing drives current autonomous vehicles. Despite rapid progress, current Lidar sensors still lag two decades behind traditional color cameras in terms of resolution and cost. For autonomous driving, this means that large objects close to the sensors are easily visible, but far-away or small objects comprise only one measurement or two. This is an issue, especially when these objects turn out to be driving hazards. On the other hand, these same objects are clearly visible in onboard RGB sensors. In this work, we present an approach to seamlessly fuse RGB sensors into Lidar-based 3D recognition. Our approach takes a set of 2D detections to generate dense 3D virtual points to augment an otherwise sparse 3D point cloud. These virtual points naturally integrate into any standard Lidar-based 3D detectors along with regular Lidar measurements. The resulting multi-modal detector is simple and effective. Experimental results on the large-scale nuScenes dataset show that our framework improves a strong CenterPoint baseline by a significant $6.6$ mAP, and outperforms competing fusion approaches. Code and more visualizations are available at https://tianweiy.github.io/mvp/\n"}}
{"id": "FlhlcARywRz", "cdate": 1601308030068, "mdate": null, "content": {"title": "Learning a unified label space", "abstract": "How do we build a general and broad object detection system? We use all labels of all concepts ever annotated. These labels span many diverse datasets with potentially inconsistent semantic labels. In this paper, we show how to integrate these datasets and their semantic taxonomies in a completely automated fashion. Once integrated, we train an off-the-shelf object detector on the union of the datasets. This unified recognition system performs as well as dataset-specific models on each training domain, but generalizes much better to new unseen domains. Entries based on the presented methodology ranked first in the object detection and instance segmentation tracks of the ECCV 2020 Robust Vision Challenge."}}
{"id": "UPebtTp3qrm", "cdate": 1577836800000, "mdate": null, "content": {"title": "Center-based 3D Object Detection and Tracking", "abstract": "Three-dimensional objects are commonly represented as 3D boxes in a point-cloud. This representation mimics the well-studied image-based 2D bounding-box detection but comes with additional challenges. Objects in a 3D world do not follow any particular orientation, and box-based detectors have difficulties enumerating all orientations or fitting an axis-aligned bounding box to rotated objects. In this paper, we instead propose to represent, detect, and track 3D objects as points. Our framework, CenterPoint, first detects centers of objects using a keypoint detector and regresses to other attributes, including 3D size, 3D orientation, and velocity. In a second stage, it refines these estimates using additional point features on the object. In CenterPoint, 3D object tracking simplifies to greedy closest-point matching. The resulting detection and tracking algorithm is simple, efficient, and effective. CenterPoint achieved state-of-the-art performance on the nuScenes benchmark for both 3D detection and tracking, with 65.5 NDS and 63.8 AMOTA for a single model. On the Waymo Open Dataset, CenterPoint outperforms all previous single model method by a large margin and ranks first among all Lidar-only submissions. The code and pretrained models are available at https://github.com/tianweiy/CenterPoint."}}
{"id": "SoibkTzgO6r", "cdate": 1546300800000, "mdate": null, "content": {"title": "Bottom-Up Object Detection by Grouping Extreme and Center Points.", "abstract": "With the advent of deep learning, object detection drifted from a bottom-up to a top-down recognition problem. State of the art algorithms enumerate a near-exhaustive list of object locations and classify each into: object or not. In this paper, we show that bottom-up approaches still perform competitively. We detect four extreme points (top-most, left-most, bottom-most, right-most) and one center point of objects using a standard keypoint estimation network. We group the five keypoints into a bounding box if they are geometrically aligned. Object detection is then a purely appearance-based keypoint estimation problem, without region classification or implicit feature learning. The proposed method performs on-par with the state-of-the-art region based detection methods, with a bounding box AP of 43.7% on COCO test-dev. In addition, our estimated extreme points directly span a coarse octagonal mask, with a COCO Mask AP of 18.9%, much better than the Mask AP of vanilla bounding boxes. Extreme point guided segmentation further improves this to 34.6% Mask AP."}}
{"id": "rJZpN9-O-H", "cdate": 1514764800000, "mdate": null, "content": {"title": "Unsupervised Domain Adaptation for 3D Keypoint Estimation via View Consistency", "abstract": "In this paper, we introduce a novel unsupervised domain adaptation technique for the task of 3D keypoint prediction from a single depth scan or image. Our key idea is to utilize the fact that predictions from different views of the same or similar objects should be consistent with each other. Such view consistency can provide effective regularization for keypoint prediction on unlabeled instances. In addition, we introduce a geometric alignment term to regularize predictions in the target domain. The resulting loss function can be effectively optimized via alternating minimization. We demonstrate the effectiveness of our approach on real datasets and present experimental results showing that our approach is superior to state-of-the-art general-purpose domain adaptation techniques."}}
{"id": "HkbojcW_Zr", "cdate": 1514764800000, "mdate": null, "content": {"title": "StarMap for Category-Agnostic Keypoint and Viewpoint Estimation", "abstract": "Semantic keypoints provide concise abstractions for a variety of visual understanding tasks. Existing methods define semantic keypoints separately for each category with a fixed number of semantic labels in fixed indices. As a result, this keypoint representation is in-feasible when objects have a varying number of parts, e.g. chairs with varying number of legs. We propose a category-agnostic keypoint representation, which combines a multi-peak heatmap (StarMap) for all the keypoints and their corresponding features as 3D locations in the canonical viewpoint (CanViewFeature) defined for each instance. Our intuition is that the 3D locations of the keypoints in canonical object views contain rich semantic and compositional information. Using our flexible representation, we demonstrate competitive performance in keypoint detection and localization compared to category-specific state-of-the-art methods. Moreover, we show that when augmented with an additional depth channel (DepthMap) to lift the 2D keypoints to 3D, our representation can achieve state-of-the-art results in viewpoint estimation. Finally, we show that our category-agnostic keypoint representation can be generalized to novel categories."}}
{"id": "SkZwjWz_bS", "cdate": 1483228800000, "mdate": null, "content": {"title": "Towards 3D Human Pose Estimation in the Wild: A Weakly-Supervised Approach", "abstract": "In this paper, we study the task of 3D human pose estimation in the wild. This task is challenging due to lack of training data, as existing datasets are either in the wild images with 2D pose or in the lab images with 3D pose.,, We propose a weakly-supervised transfer learning method that uses mixed 2D and 3D labels in a unified deep neutral network that presents two-stage cascaded structure. Our network augments a state-of-the-art 2D pose estimation sub-network with a 3D depth regression sub-network. Unlike previous two stage approaches that train the two sub-networks sequentially and separately, our training is end-to-end and fully exploits the correlation between the 2D pose and depth estimation sub-tasks. The deep features are better learnt through shared representations. In doing so, the 3D pose labels in controlled lab environments are transferred to in the wild images. In addition, we introduce a 3D geometric constraint to regularize the 3D pose prediction, which is effective in the absence of ground truth depth labels. Our method achieves competitive results on both 2D and 3D benchmarks."}}
