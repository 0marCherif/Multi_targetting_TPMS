{"id": "nlmkHOUP2Z", "cdate": 1648673586550, "mdate": 1648673586550, "content": {"title": "Read the Signs: Towards Invariance to Gradient Descent's Hyperparameter Values", "abstract": "We propose ActiveLR, an optimization meta algorithm that localizes the learning rate, \\(\\alpha\\), and adapts them at each epoch according to whether the gradient at each epoch changes sign or not. This sign-conscious algorithm is aware of whether from the previous step to the current one the update of each parameter has been too large or too small and adjusts the \\(\\alpha\\) accordingly. We implemented the Active version (ours) of widely used and recently published gradient descent optimizers, namely Adam, RAdam, and AdaBelief. Our experiments on ImageNet, CIFAR-10, and WikiText-103 using different model architectures, such as ResNet and Transformers, show an increase in generalizability and training set fit, and a decrease in training time for the Active variant of Adam, RAdam, and AdaBelief. The results also show robustness of the Active variant of these optimizers to different values of the initial learning rate. Furthermore, the detrimental effects of using large batch sizes, which are crucial in training large-scale datasets and high-performance computing, are mitigated. ActiveLR, thus, eliminates the need for hyper-parameter search for the optimal initial learning rate and batch size, two of the most commonly tuned hyper-parameters that require heavy time and computational costs to pick and further tuning of the learning rate throughout the training process. We encourage AI researchers and practitioners to use the Active variant of their optimizer of choice instead of its original, vanilla form for faster training and better generalizability."}}
