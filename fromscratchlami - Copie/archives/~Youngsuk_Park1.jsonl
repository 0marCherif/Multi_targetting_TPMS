{"id": "gy3dO3x-0CZ", "cdate": 1681712922345, "mdate": 1681712922345, "content": {"title": "First De-Trend then Attend: Rethinking Attention for Time-Series Forecasting", "abstract": "Transformer-based models have gained large popularity and demonstrated promising results in long-term time-series forecasting in recent years. In addition to learning attention in time domain, recent works also explore learning attention in frequency domains (e.g., Fourier domain, wavelet domain), given that seasonal patterns can be better captured in these domains. In this work, we seek to understand the relationships between attention models in different time and frequency domains. Theoretically, we show that attention models in different domains are equivalent under linear conditions (i.e., linear kernel to attention scores). Empirically, we analyze how attention models of different domains show different behaviors through various synthetic experiments with seasonality, trend and noise, with emphasis on the role of softmax operation therein. Both these theoretical and empirical analyses motivate us to propose a new method: TDformer (Trend Decomposition Transformer), that first applies seasonal-trend decomposition, and then additively combines an MLP which predicts the trend component with Fourier attention which predicts the seasonal component to obtain the final prediction. Extensive experiments on benchmark time-series forecasting datasets demonstrate that TDformer achieves state-of-the-art performance against existing attention-based models."}}
{"id": "_QcreQjxHi", "cdate": 1665069645330, "mdate": null, "content": {"title": "But Are You Sure? Quantifying Uncertainty in Model Explanations", "abstract": "Even when a black-box model makes accurate predictions (e.g., whether it will rain tomorrow), it is difficult to extract principles from the model that improve human understanding (e.g., what set of atmospheric conditions best predict rainfall). Model explanations via explainability methods (e.g., LIME, Shapley values) can help by highlighting interpretable aspects of the model, such the data features to which the model is most sensitive. However, these methods can be unstable and inconsistent, which often ends up providing unreliable insights. Moreover, under the existence of many near-optimal models, there is no guarantee that explanations for a single model will agree with explanations from the true model that generated the data. In this work, instead of explaining a single best-fitting model, we develop principled methods to construct an uncertainty set for the ``true explanation'': the explanation from the (unknown) true model that generated the data. We show finite-sample guarantees that the uncertainty set we return includes the explanation for the true model with high probability. We show through synthetic experiments that our uncertainty sets have high fidelity to the explanations of the true model. We then report our findings on real-world data. \n"}}
{"id": "oPHuNpJl3c", "cdate": 1664928778720, "mdate": null, "content": {"title": "Adaptive Sampling for Probabilistic Forecasting under Distribution Shift", "abstract": "The world is not static: This causes real-world time series to change over time\nthrough external, and potentially disruptive, events such as macroeconomic cycles\nor the COVID-19 pandemic. We present an adaptive sampling strategy that selects\nthe part of the time series history that is relevant for forecasting. We achieve this by\nlearning a discrete distribution over relevant time steps by Bayesian optimization.\nWe instantiate this idea with a two-step method that is pre-trained with uniform\nsampling and then training a lightweight adaptive architecture with adaptive sam-\npling. We show with synthetic and real-world experiments that this method adapts\nto distribution shift and significantly reduces the forecasting error of the base model\nfor three out of five datasets."}}
{"id": "GLc8Rhney0e", "cdate": 1664902716295, "mdate": null, "content": {"title": "First De-Trend then Attend: Rethinking Attention for Time-Series Forecasting", "abstract": "Transformer-based models have gained large popularity and demonstrated promising results in long-term time-series forecasting in recent years. In addition to learning attention in time domain, recent works also explore learning attention in frequency domains (e.g., Fourier domain, wavelet domain), given that seasonal patterns can be better captured in these domains. In this work, we seek to understand the relationships between attention models in different time and frequency domains. Theoretically, we show that attention models in different domains are equivalent under linear conditions (i.e., linear kernel to attention scores). Empirically, we analyze how attention models of different domains show different behaviors through various synthetic experiments with seasonality, trend and noise, with emphasis on the role of softmax operation therein. Both these theoretical and empirical analyses motivate us to propose a new method: TDformer (Trend Decomposition Transformer), that first applies seasonal-trend decomposition, and then additively combines an MLP which predicts the trend component with Fourier attention which predicts the seasonal component to obtain the final prediction. Extensive experiments on benchmark time-series forecasting datasets demonstrate that TDformer achieves state-of-the-art performance against existing attention-based models."}}
{"id": "aatBG7kdAz6", "cdate": 1664300344121, "mdate": null, "content": {"title": "Towards Reverse Causal Inference on Panel Data: Precise Formulation and Challenges", "abstract": "Seeking causal explanations in panel (or longitudinal/multivariate time-series) data is a difficult problem of both academic and industrial importance. Although there exists a large amount of literature on forward causal inference, where the treatment/outcome/covariates variables are well-defined, it is unclear how to answer the reverse question: which covariates have effects on the outcome? In this paper, we set forth our expedition on this reverse question from the first principles. We formulate the precise problem definition in terms of causal patterns and causal paths, and propose a linear-time greedy meta algorithm that makes use of forward causal inference estimators. We further identify a set of optimality conditions under which the proposed algorithm is able to find the optimal causal path. To substantiate our greedy algorithm, we propose a generalized version of the synthetic control estimator by fitting both synthetic treatments and controls by conditioning on the partial causal paths. Promising results on on synthetic datasets demonstrate the potential of our method."}}
{"id": "ctmLBs8lITa", "cdate": 1663850505992, "mdate": null, "content": {"title": "Robust Multivariate Time-Series Forecasting: Adversarial Attacks and Defense Mechanisms", "abstract": "This work studies the threats of adversarial attack on multivariate probabilistic forecasting models and viable defense mechanisms. Our studies discover a new attack pattern that negatively impact the forecasting of a target time series via making strategic, sparse (imperceptible) modifications to the past observations of a small number of other time series. To mitigate the impact of such attack, we have developed two defense strategies. First, we extend a previously developed randomized smoothing technique in classification to multivariate forecasting scenarios. Second, we develop an adversarial training algorithm that learns to create adversarial examples and at the same time optimizes the forecasting model to improve its robustness against such adversarial simulation. Extensive experiments on real-world datasets confirm that our attack schemes are powerful and our defense algorithms are more effective compared with baseline defense mechanisms.\n"}}
{"id": "a6NvoZ5DLoe", "cdate": 1663850230759, "mdate": null, "content": {"title": "DYNAMIC ENSEMBLE FOR PROBABILISTIC TIME- SERIES FORECASTING VIA DEEP REINFORCEMENT LEARNING", "abstract": "Ensembles from given base learners are known to be indispensable in improving accuracy for most of the prediction tasks, leading to numerous methods. However, the only ensembling strategies that have been considered for time series forecasting in the past have been static methods, ones that have access to the predictions of the base learners but not to the base learners themselves. In this paper, we propose a novel  \\textit{dynamic ensemble policy}, which, unlike static methods, uses the power of the ensemble to improve each of the base learners being ensembled by reducing the error accumulation of each base learner via consecutively feeding an ensembled sample to each base learner. To do so, we adopt a deep Reinforcement Learning (RL) framework with a Markov Decision Process (MDP) designed where the ensemble agent interacts with our environment (\\textit{TS-GYM}) from offline data. The output of our ensemble strategy is a single autoregressive forecaster that supports several desirable properties of uncertainty quantification and sample path, along with notable performance gain. The effectiveness of the proposed framework is demonstrated in multiple synthetic and real-world experiments."}}
{"id": "ytfzRITw-Zm", "cdate": 1609459200000, "mdate": null, "content": {"title": "Attention-based Domain Adaptation for Time Series Forecasting", "abstract": "Recently, deep neural networks have gained increasing popularity in the field of time series forecasting. A primary reason for their success is their ability to effectively capture complex temporal dynamics across multiple related time series. The advantages of these deep forecasters only start to emerge in the presence of a sufficient amount of data. This poses a challenge for typical forecasting problems in practice, where there is a limited number of time series or observations per time series, or both. To cope with this data scarcity issue, we propose a novel domain adaptation framework, Domain Adaptation Forecaster (DAF). DAF leverages statistical strengths from a relevant domain with abundant data samples (source) to improve the performance on the domain of interest with limited data (target). In particular, we use an attention-based shared module with a domain discriminator across domains and private modules for individual domains. We induce domain-invariant latent features (queries and keys) and retrain domain-specific features (values) simultaneously to enable joint training of forecasters on source and target domains. A main insight is that our design of aligning keys allows the target domain to leverage source time series even with different characteristics. Extensive experiments on various domains demonstrate that our proposed method outperforms state-of-the-art baselines on synthetic and real-world datasets, and ablation studies verify the effectiveness of our design choices."}}
{"id": "MlF0DtSlYhx", "cdate": 1609459200000, "mdate": null, "content": {"title": "Variance Reduction in Training Forecasting Models with Subgroup Sampling", "abstract": "In large-scale time series forecasting, one often encounters the situation where the temporal patterns of time series, while drifting over time, differ from one another in the same dataset. In this paper, we provably show under such heterogeneity, training a forecasting model with commonly used stochastic optimizers (e.g. SGD) potentially suffers large variance on gradient estimation, and thus incurs long-time training. We show that this issue can be efficiently alleviated via stratification, which allows the optimizer to sample from pre-grouped time series strata. For better trading-off gradient variance and computation complexity, we further propose SCott (Stochastic Stratified Control Variate Gradient Descent), a variance reduced SGD-style optimizer that utilizes stratified sampling via control variate. In theory, we provide the convergence guarantee of SCott on smooth non-convex objectives. Empirically, we evaluate SCott and other baseline optimizers on both synthetic and real-world time series forecasting problems, and demonstrate SCott converges faster with respect to both iterations and wall clock time."}}
{"id": "irohVI31Q0I", "cdate": 1577836800000, "mdate": null, "content": {"title": "Structured Policy Iteration for Linear Quadratic Regulator", "abstract": "Linear quadratic regulator (LQR) is one of the most popular frameworks to tackle continuous Markov decision process tasks. With its fundamental theory and tractable optimal policy, LQR has been rev..."}}
