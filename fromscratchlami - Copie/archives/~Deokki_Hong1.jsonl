{"id": "EOjCF9cA7q", "cdate": 1667492818155, "mdate": 1667492818155, "content": {"title": "Zero-Shot Quantization Brought Closer to the Teacher", "abstract": "Model quantization is considered as a promising method\nto greatly reduce the resource requirements of deep neural\nnetworks. To deal with the performance drop induced by\nquantization errors, a popular method is to use training data\nto fine-tune quantized networks. In real-world environments,\nhowever, such a method is frequently infeasible because\ntraining data is unavailable due to security, privacy, or confidentiality concerns. Zero-shot quantization addresses such\nproblems, usually by taking information from the weights of\na full-precision teacher network to compensate the performance drop of the quantized networks. In this paper, we first\nanalyze the loss surface of state-of-the-art zero-shot quantization techniques and provide several findings. In contrast\nto usual knowledge distillation problems, zero-shot quantization often suffers from 1) the difficulty of optimizing multiple\nloss terms together, and 2) the poor generalization capability\ndue to the use of synthetic samples. Furthermore, we observe\nthat many weights fail to cross the rounding threshold during\ntraining the quantized networks even when it is necessary\nto do so for better performance. Based on the observations,\nwe propose AIT, a simple yet powerful technique for zeroshot quantization, which addresses the aforementioned two\nproblems in the following way: AIT i) uses a KL distance\nloss only without a cross-entropy loss, and ii) manipulates\ngradients to guarantee that a certain portion of weights are\nproperly updated after crossing the rounding thresholds. Experiments show that AIT outperforms the performance of\nmany existing methods by a great margin, taking over the\noverall state-of-the-art position in the field"}}
{"id": "qco4ekz2Epm", "cdate": 1663850327405, "mdate": null, "content": {"title": "Online Boundary-Free Continual Learning by Scheduled Data Prior", "abstract": "Typical continual learning setup assumes that the dataset is split into multiple discrete tasks. We argue that it is less realistic as the streamed data would have no notion of task boundary in real-world data. Here, we take a step forward to investigate more realistic online continual learning \u2013 learning continuously changing data distribution without explicit task boundary, which we call boundary-free setup. As there is no clear boundary of tasks, it is not obvious when and what information in the past to be preserved as a better remedy for the stability-plasticity dilemma. To this end, we propose a scheduled transfer of previously learned knowledge. We further propose a data-driven balancing between the knowledge in the past and the present in learning objective. Moreover, since it is not straight-forward to use the previously proposed forgetting measure without task boundaries, we further propose a novel forgetting measure based on information theory that can capture forgetting. We empirically evaluate our method on a Gaussian data stream, its periodic extension, which assumes periodic data distribution frequently observed in real-life data, as well as the conventional disjoint task-split. Our method outperforms prior arts by large margins in various setups, using four popular benchmark datasets \u2013 CIFAR-10, CIFAR-100, TinyImageNet and ImageNet."}}
{"id": "wToffYRDXl", "cdate": 1640995200000, "mdate": 1668611494369, "content": {"title": "Enabling hard constraints in differentiable neural network and accelerator co-exploration", "abstract": "Co-exploration of an optimal neural architecture and its hardware accelerator is an approach of rising interest which addresses the computational cost problem, especially in low-profile systems. The large co-exploration space is often handled by adopting the idea of differentiable neural architecture search. However, despite the superior search efficiency of the differentiable co-exploration, it faces a critical challenge of not being able to systematically satisfy hard constraints such as frame rate. To handle the hard constraint problem of differentiable co-exploration, we propose HDX, which searches for hard-constrained solutions without compromising the global design objectives. By manipulating the gradients in the interest of the given hard constraint, high-quality solutions satisfying the constraint can be obtained."}}
{"id": "KIlOQiLuLe", "cdate": 1640995200000, "mdate": 1668611494357, "content": {"title": "It's All In the Teacher: Zero-Shot Quantization Brought Closer to the Teacher", "abstract": "Model quantization is considered as a promising method to greatly reduce the resource requirements of deep neural networks. To deal with the performance drop induced by quantization errors, a popular method is to use training data to fine-tune quantized networks. In real-world environments, however, such a method is frequently infeasible because training data is unavailable due to security, privacy, or confidentiality concerns. Zero-shot quantization addresses such problems, usually by taking information from the weights of a full-precision teacher network to compensate the performance drop of the quantized networks. In this paper, we first analyze the loss surface of state-of-the-art zero-shot quantization techniques and provide several findings. In contrast to usual knowledge distillation problems, zero-shot quantization often suffers from 1) the difficulty of optimizing multiple loss terms together, and 2) the poor generalization capability due to the use of synthetic samples. Furthermore, we observe that many weights fail to cross the rounding threshold during training the quantized networks even when it is necessary to do so for better performance. Based on the observations, we propose AIT, a simple yet powerful technique for zero-shot quantization, which addresses the aforementioned two problems in the following way: AIT i) uses a KL distance loss only without a cross-entropy loss, and ii) manipulates gradients to guarantee that a certain portion of weights are properly updated after crossing the rounding thresholds. Experiments show that AIT outperforms the performance of many existing methods by a great margin, taking over the overall state-of-the-art position in the field."}}
{"id": "6B6RoMo09Hl", "cdate": 1640995200000, "mdate": 1668611568357, "content": {"title": "It's All In the Teacher: Zero-Shot Quantization Brought Closer to the Teacher", "abstract": "Model quantization is considered as a promising method to greatly reduce the resource requirements of deep neural networks. To deal with the performance drop induced by quantization errors, a popular method is to use training data to fine-tune quantized networks. In real-world environments, however, such a method is frequently infeasible because training data is unavailable due to security, privacy, or confidentiality concerns. Zero-shot quantization addresses such problems, usually by taking information from the weights of a full-precision teacher network to compensate the performance drop of the quantized networks. In this paper, we first analyze the loss surface of state-of-the-art zero-shot quantization techniques and provide several findings. In contrast to usual knowledge distillation problems, zero-shot quantization often suffers from 1) the difficulty of optimizing multiple loss terms together, and 2) the poor generalization capability due to the use of synthetic samples. Furthermore, we observe that many weights fail to cross the rounding threshold during training the quantized networks even when it is necessary to do so for better performance. Based on the observations, we propose AIT, a simple yet powerful technique for zero-shot quantization, which addresses the aforementioned two problems in the following way: AIT i) uses a KL distance loss only without a cross-entropy loss, and ii) manipulates gradients to guarantee that a certain portion of weights are properly updated after crossing the rounding thresholds. Experiments show that AIT outperforms the performance of many existing methods by a great margin, taking over the overall state-of-the-art position in the field."}}
{"id": "e1GzwU4W2Kh", "cdate": 1632875505740, "mdate": null, "content": {"title": "ConCoDE: Hard-constrained Differentiable Co-Exploration Method for Neural Architectures and Hardware Accelerators", "abstract": "While DNNs achieve over-human performances in a number of areas, it is often accompanied by the skyrocketing computational costs. \nCo-exploration of an optimal neural architecture and its hardware accelerator is an approach of rising interest which addresses the computational cost problem, especially in low-profile systems (e.g., embedded, mobile). \nThe difficulty of having to search the large co-exploration space is often addressed by adopting the idea of differentiable neural architecture search. \nDespite the superior search efficiency of the differentiable co-exploration, it faces a critical challenge of not being able to systematically satisfy hard constraints, such as frame rate or power budget.\nTo handle the hard constraint problem of differentiable co-exploration, we propose ConCoDE,  \nwhich searches for hard-constrained solutions without compromising the global design objectives.\nBy manipulating the gradients in the interest of the given hard constraint, high-quality solutions satisfying the constraint can be obtained.\nExperimental results show that ConCoDE is able to meet the constraints even in tight conditions. \nWe also show that the solutions searched by ConCoDE exhibit high quality compared to those searched without any constraint. "}}
{"id": "ejo1_Weiart", "cdate": 1621629802022, "mdate": null, "content": {"title": "Qimera: Data-free Quantization with Synthetic Boundary Supporting Samples", "abstract": "Model quantization is known as a promising method to compress deep neural networks, especially for inferences on lightweight mobile or edge devices. \nHowever, model quantization usually requires access to the original training data to maintain the accuracy of the full-precision models, which is often infeasible in real-world scenarios for security and privacy issues.\nA popular approach to perform quantization without access to the original data is to use synthetically generated samples, based on batch-normalization statistics or adversarial learning.\nHowever, the drawback of such approaches is that they primarily rely on random noise input to the generator to attain diversity of the synthetic samples. \nWe find that this is often insufficient to capture the distribution of the original data, especially around the decision boundaries.\nTo this end, we propose Qimera, a method that uses superposed latent embeddings to generate synthetic boundary supporting samples.\nFor the superposed embeddings to better reflect the original distribution, we also propose using an additional disentanglement mapping layer and extracting information from the full-precision model.\nThe experimental results show that Qimera achieves state-of-the-art performances for various settings on data-free quantization. \nCode is available at https://github.com/iamkanghyunchoi/qimera."}}
{"id": "ofMh_tL3N-", "cdate": 1609459200000, "mdate": 1668611494373, "content": {"title": "Qimera: Data-free Quantization with Synthetic Boundary Supporting Samples", "abstract": "Model quantization is known as a promising method to compress deep neural networks, especially for inferences on lightweight mobile or edge devices. However, model quantization usually requires access to the original training data to maintain the accuracy of the full-precision models, which is often infeasible in real-world scenarios for security and privacy issues.A popular approach to perform quantization without access to the original data is to use synthetically generated samples, based on batch-normalization statistics or adversarial learning.However, the drawback of such approaches is that they primarily rely on random noise input to the generator to attain diversity of the synthetic samples. We find that this is often insufficient to capture the distribution of the original data, especially around the decision boundaries.To this end, we propose Qimera, a method that uses superposed latent embeddings to generate synthetic boundary supporting samples.For the superposed embeddings to better reflect the original distribution, we also propose using an additional disentanglement mapping layer and extracting information from the full-precision model.The experimental results show that Qimera achieves state-of-the-art performances for various settings on data-free quantization. Code is available at https://github.com/iamkanghyunchoi/qimera."}}
{"id": "UJ_ptx2YFP", "cdate": 1609459200000, "mdate": 1668611568353, "content": {"title": "Qimera: Data-free Quantization with Synthetic Boundary Supporting Samples", "abstract": "Model quantization is known as a promising method to compress deep neural networks, especially for inferences on lightweight mobile or edge devices. However, model quantization usually requires access to the original training data to maintain the accuracy of the full-precision models, which is often infeasible in real-world scenarios for security and privacy issues. A popular approach to perform quantization without access to the original data is to use synthetically generated samples, based on batch-normalization statistics or adversarial learning. However, the drawback of such approaches is that they primarily rely on random noise input to the generator to attain diversity of the synthetic samples. We find that this is often insufficient to capture the distribution of the original data, especially around the decision boundaries. To this end, we propose Qimera, a method that uses superposed latent embeddings to generate synthetic boundary supporting samples. For the superposed embeddings to better reflect the original distribution, we also propose using an additional disentanglement mapping layer and extracting information from the full-precision model. The experimental results show that Qimera achieves state-of-the-art performances for various settings on data-free quantization. Code is available at https://github.com/iamkanghyunchoi/qimera."}}
{"id": "PeujU0Sjt6l", "cdate": 1609459200000, "mdate": 1668611494368, "content": {"title": "DANCE: Differentiable Accelerator/Network Co-Exploration", "abstract": "This work presents DANCE, a differentiable approach towards the co-exploration of hardware accelerator and network architecture design. At the heart of DANCE is a differentiable evaluator network. By modeling the hardware evaluation software with a neural network, the relation between the accelerator design and the hardware metrics becomes differentiable, allowing the search to be performed with backpropagation. Compared to the naive existing approaches, our method performs co-exploration in a significantly shorter time, while achieving superior accuracy and hardware cost metrics."}}
