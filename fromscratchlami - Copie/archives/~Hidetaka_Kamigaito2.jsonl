{"id": "z_Bq2oHzUci", "cdate": 1672531200000, "mdate": 1687269034976, "content": {"title": "Generative Replay Inspired by Hippocampal Memory Indexing for Continual Language Learning", "abstract": ""}}
{"id": "vNqb8Xoo6Om", "cdate": 1672531200000, "mdate": 1700219345418, "content": {"title": "Table and Image Generation for Investigating Knowledge of Entities in Pre-trained Vision and Language Models", "abstract": "In this paper, we propose a table and image generation task to verify how the knowledge about entities acquired from natural language is retained in Vision & Language (V&L) models. This task consists of two parts: the first is to generate a table containing knowledge about an entity and its related image, and the second is to generate an image from an entity with a caption and a table containing related knowledge of the entity. In both tasks, the model must know the entities used to perform the generation properly. We created the Wikipedia Table and Image Generation (WikiTIG) dataset from about 200,000 infoboxes in English Wikipedia articles to perform the proposed tasks. We evaluated the performance on the tasks with respect to the above research question using the V&L model OFA, which has achieved state-of-the-art results in multiple tasks. Experimental results show that OFA forgets part of its entity knowledge by pre-training as a complement to improve the performance of image related tasks."}}
{"id": "vHcfZRO2li", "cdate": 1672531200000, "mdate": 1700219345423, "content": {"title": "Hierarchical Label Generation for Text Classification", "abstract": ""}}
{"id": "knrjui8PYL3", "cdate": 1672531200000, "mdate": 1700219345575, "content": {"title": "Model-based Subsampling for Knowledge Graph Completion", "abstract": "Subsampling is effective in Knowledge Graph Embedding (KGE) for reducing overfitting caused by the sparsity in Knowledge Graph (KG) datasets. However, current subsampling approaches consider only frequencies of queries that consist of entities and their relations. Thus, the existing subsampling potentially underestimates the appearance probabilities of infrequent queries even if the frequencies of their entities or relations are high. To address this problem, we propose Model-based Subsampling (MBS) and Mixed Subsampling (MIX) to estimate their appearance probabilities through predictions of KGE models. Evaluation results on datasets FB15k-237, WN18RR, and YAGO3-10 showed that our proposed subsampling methods actually improved the KG completion performances for popular KGE models, RotatE, TransE, HAKE, ComplEx, and DistMult."}}
{"id": "dLH5_RrJ5DS", "cdate": 1672531200000, "mdate": 1696655662345, "content": {"title": "Bidirectional Transformer Reranker for Grammatical Error Correction", "abstract": ""}}
{"id": "cBVJkOQEADb", "cdate": 1672531200000, "mdate": 1700219345347, "content": {"title": "Table and Image Generation for Investigating Knowledge of Entities in Pre-trained Vision and Language Models", "abstract": ""}}
{"id": "TVorzfslXt", "cdate": 1672531200000, "mdate": 1700219345415, "content": {"title": "Bidirectional Transformer Reranker for Grammatical Error Correction", "abstract": "Pre-trained seq2seq models have achieved state-of-the-art results in the grammatical error correction task. However, these models still suffer from a prediction bias due to their unidirectional decoding. Thus, we propose a bidirectional Transformer reranker (BTR), that re-estimates the probability of each candidate sentence generated by the pre-trained seq2seq model. The BTR preserves the seq2seq-style Transformer architecture but utilizes a BERT-style self-attention mechanism in the decoder to compute the probability of each target token by using masked language modeling to capture bidirectional representations from the target context. For guiding the reranking, the BTR adopts negative sampling in the objective function to minimize the unlikelihood. During inference, the BTR gives final results after comparing the reranked top-1 results with the original ones by an acceptance threshold. Experimental results show that, in reranking candidates from a pre-trained seq2seq model, T5-base, the BTR on top of T5-base could yield 65.47 and 71.27 F0.5 scores on the CoNLL-14 and BEA test sets, respectively, and yield 59.52 GLEU score on the JFLEG corpus, with improvements of 0.36, 0.76 and 0.48 points compared with the original T5-base. Furthermore, when reranking candidates from T5-large, the BTR on top of T5-base improved the original T5-large by 0.26 points on the BEA test set."}}
{"id": "MLq6LIL9DYo", "cdate": 1672531200000, "mdate": 1700219345415, "content": {"title": "Abstractive Document Summarization with Summary-length Prediction", "abstract": ""}}
{"id": "wKzRi-of3wM", "cdate": 1640995200000, "mdate": 1686187538254, "content": {"title": "Aspect-based Analysis of Advertising Appeals for Search Engine Advertising", "abstract": "Soichiro Murakami, Peinan Zhang, Sho Hoshino, Hidetaka Kamigaito, Hiroya Takamura, Manabu Okumura. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track. 2022."}}
{"id": "jLthbIDQJQ", "cdate": 1640995200000, "mdate": 1700219345573, "content": {"title": "Subsampling for Knowledge Graph Embedding Explained", "abstract": ""}}
