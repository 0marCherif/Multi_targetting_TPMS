{"id": "41j3SivW7Uw", "cdate": 1671933735224, "mdate": 1671933735224, "content": {"title": "SPE: Symmetrical Prompt Enhancement for Fact Probing", "abstract": "Pretrained language models (PLMs) have been shown to accumulate factual knowledge during pretraining (Petroni et al., 2019). Recent works probe PLMs for the extent of this knowledge through prompts either in discrete or continuous forms. However, these methods do not consider symmetry of the task: object prediction and subject prediction. In this work, we propose Symmetrical Prompt Enhancement (SPE), a continuous prompt-based method for factual probing in PLMs that leverages the symmetry of the task by constructing symmetrical prompts for subject and object prediction. Our results on a popular factual probing dataset, LAMA, show significant improvement of SPE over previous probing methods."}}
{"id": "d6mWFGwpHKp", "cdate": 1634310860435, "mdate": 1634310860435, "content": {"title": "Adversarial Scrubbing of Demographic Information for Text Classification", "abstract": "Contextual representations learned by language models can often encode undesirable\nattributes, like demographic associations of\nthe users, while being trained for an unrelated target task. We aim to scrub such undesirable attributes and learn fair representations while maintaining performance on the\ntarget task. In this paper, we present an\nadversarial learning framework \u201cAdversarial\nScrubber\u201d (ADS), to debias contextual representations. We perform theoretical analysis to\nshow that our framework converges without\nleaking demographic information under certain conditions. We extend previous evaluation techniques by evaluating debiasing performance using Minimum Description Length\n(MDL) probing. Experimental evaluations on\n8 datasets show that ADS generates representations with minimal information about demographic attributes while being maximally informative about the target task."}}
{"id": "ukvUDCeZNh5", "cdate": 1577836800000, "mdate": 1631647521413, "content": {"title": "Towards Context-Aware End-to-End Code-Switching Speech Recognition", "abstract": "Code-switching (CS) speech recognition is drawing increasing attention in recent years as it is a common situation in speech where speakers alternate between languages in the context of a single utterance or discourse. In this work, we propose Hierarchical Attention-based Recurrent Decoder (HARD) to build a context-aware end-to-end code-switching speech recognition system. HARD is an attention-based decoder model which employs a hierarchical recurrent network to enhance model\u2019s awareness of previous generated historical sequence (sub-sequence) at decoding. This architecture has two LSTMs to model encoder hidden states at both the character level and sub-sequence level, therefore enables us to generate utterances that switch between languages more precisely from speech. We also employ language identification (LID) as an auxiliary task in multi-task learning (MTL) to boost speech recognition performance. We evaluate the effectiveness of our model on the SEAME dataset, results show that our multi-task learning HARD (MTL-HARD) model improves over the baseline Listen, Attend and Spell (LAS) model by reducing character error rate (CER) from 29.91% to 26.56% and mixed error rate (MER) from 38.99% to 34.50%, and case study shows MTL-HARD can carry historical information in the sub-sequences."}}
{"id": "_ha0sSTRWb-", "cdate": 1577836800000, "mdate": null, "content": {"title": "A Summary of the First Workshop on Language Technology for Language Documentation and Revitalization", "abstract": "Graham Neubig, Shruti Rijhwani, Alexis Palmer, Jordan MacKenzie, Hilaria Cruz, Xinjian Li, Matthew Lee, Aditi Chaudhary, Luke Gessler, Steven Abney, Shirley Anugrah Hayati, Antonios Anastasopoulos, Olga Zamaraeva, Emily Prud\u2019hommeaux, Jennette Child, Sara Child, Rebecca Knowles, Sarah Moeller, Jeffrey Micher, Yiyuan Li, Sydney Zink, Mengzhou Xia, Roshan S Sharma, Patrick Littell. Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL). 2020."}}
{"id": "UWhgT4qN5K0", "cdate": 1577836800000, "mdate": 1631647521482, "content": {"title": "Comparison of Interactive Knowledge Base Spelling Correction Models for Low-Resource Languages", "abstract": "Spelling normalization for low resource languages is a challenging task because the patterns are hard to predict and large corpora are usually required to collect enough examples. This work shows a comparison of a neural model and character language models with varying amounts on target language data. Our usage scenario is interactive correction with nearly zero amounts of training examples, improving models as more data is collected, for example within a chat app. Such models are designed to be incrementally improved as feedback is given from users. In this work, we design a knowledge-base and prediction model embedded system for spelling correction in low-resource languages. Experimental results on multiple languages show that the model could become effective with a small amount of data. We perform experiments on both natural and synthetic data, as well as on data from two endangered languages (Ainu and Griko). Last, we built a prototype system that was used for a small case study on Hinglish, which further demonstrated the suitability of our approach in real world scenarios."}}
{"id": "-OToQeuU6HO", "cdate": 1577836800000, "mdate": 1631647521482, "content": {"title": "Towards Minimal Supervision BERT-based Grammar Error Correction", "abstract": "Current grammatical error correction (GEC) models typically consider the task as sequence generation, which requires large amounts of annotated data and limit the applications in data-limited settings. We try to incorporate contextual information from pre-trained language model to leverage annotation and benefit multilingual scenarios. Results show strong potential of Bidirectional Encoder Representations from Transformers (BERT) in grammatical error correction task."}}
