{"id": "x01dnxUEDRv", "cdate": 1663850299400, "mdate": null, "content": {"title": "How does Uncertainty-aware Sample-selection Help Decision against Action Noise?", "abstract": "Learning from imperfect demonstrations has become a vital problem in imitation learning (IL). Since the assumption of the collected demonstrations are optimal cannot always hold in real-world tasks, many previous works considers learning from a mixture of optimal and sub-optimal demonstrations. On the other hand, video records can be hands-down demonstrations in practice. Leveraging such demonstrations requires labors to output action for each frame. However, action noise always occurs when the labors are not domain experts, or meet confusing state frames. Previous IL methods can be vulnerable to such demonstrations with state-dependent action noise. To tackle this problem, we propose a robust learning paradigm called USN, which bridges Uncertainty-aware Sample-selection with Negative learning. First, IL model feeds forward all demonstration data and estimates its predictive uncertainty. Then, we select large-loss samples in the light of the uncertainty measures. Next, we update the model parameters with additional negative learning on the selected samples. Empirical results on Box2D tasks and Atari games demonstrate that USN improves the performance of state-of-the-art IL methods by more than 10% under a large portion of action noise.  "}}
{"id": "67q9f8gChCF", "cdate": 1601308191768, "mdate": null, "content": {"title": "Learning Efficient Planning-based Rewards for Imitation Learning", "abstract": "Imitation learning from limited demonstrations is challenging. Most inverse reinforcement learning (IRL) methods are unable to perform as good as the demonstrator, especially in a high-dimensional environment, e.g, the Atari domain. To address this challenge, we propose a novel reward learning method, which streamlines a differential planning module with dynamics modeling. Our method learns useful planning computations with a meaningful reward function that focuses on the resulting region of an agent executing an action. Such a planning-based reward function leads to policies with better generalization ability. Empirical results with multiple network architectures and reward instances show that our method can outperform state-of-the-art IRL methods on multiple Atari games and continuous control tasks. Our method achieves performance that is averagely 1,139.1% of the demonstration. "}}
{"id": "xH251EA80go", "cdate": 1601308152150, "mdate": null, "content": {"title": "A Simple Sparse Denoising Layer for Robust Deep Learning", "abstract": "Deep models have achieved great success in many applications. However, vanilla deep models are not well-designed against the input perturbation.   In this work, we take an initial step to designing a simple robust layer as a lightweight plug-in for vanilla deep models.  To achieve this goal, we first propose a fast sparse coding and dictionary learning algorithm for sparse coding problem with an exact $k$-sparse constraint or  $l_0$ norm regularization. Our method comes with a closed-form approximation for the sparse coding phase by taking advantage of a novel structured dictionary.   With this handy approximation,  we propose a simple sparse denoising layer (SDL) as a lightweight robust plug-in.   Extensive experiments on both classification and reinforcement learning tasks manifest the effectiveness of our methods."}}
{"id": "d0WRf-Sb9D4", "cdate": 1594384882383, "mdate": null, "content": {"title": "Intrinsic Reward Driven Imitation Learning via Generative Model", "abstract": "Imitation learning in a high-dimensional environment is challenging. Most inverse reinforcement learning (IRL) methods fail to outperform the demonstrator in such a high-dimensional environment, e.g., Atari domain. To address this challenge, we propose a novel reward learning module to generate intrinsic reward signals via a generative model. Our generative method can perform better forward state transition and backward action encoding, which improves the module\u2019s dynamics modeling ability in the environment. Thus, our module provides the imitation agent both the intrinsic intention of the demonstrator and a better exploration ability, which is critical for the agent to outperform the demonstrator. Empirical results show that our method outperforms state-of-the-art IRL methods on multiple Atari games, even with one-life demonstration. Remarkably, our method achieves performance that is up to 5 times the performance of the demonstration."}}
{"id": "Sy-nAsZObr", "cdate": 1546300800000, "mdate": null, "content": {"title": "How does Disagreement Help Generalization against Label Corruption?", "abstract": "Learning with noisy labels is one of the hottest problems in weakly-supervised learning. Based on memorization effects of deep neural networks, training on small-loss instances becomes very promisi..."}}
{"id": "HyG1_j0cYQ", "cdate": 1538087782990, "mdate": null, "content": {"title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "abstract": "It is challenging to train deep neural networks robustly on the industrial-level data, since labels of such data are heavily noisy, and their label generation processes are normally agnostic. To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. Then, we may employ early stopping or the small-loss trick to train them on selected instances. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization. In this paper, we propose a meta algorithm called Pumpout to overcome the problem of memorizing noisy labels. By using scaled stochastic gradient ascent, Pumpout actively squeezes out the negative effects of noisy labels from the training model, instead of passively forgetting these effects. We leverage Pumpout to upgrade two representative methods: MentorNet and Backward Correction. Empirical results on benchmark vision and text datasets demonstrate that Pumpout can significantly improve the robustness of representative methods."}}
{"id": "HJWlGYb_ZB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Co-teaching: Robust training of deep neural networks with extremely noisy labels", "abstract": "Deep learning with noisy labels is practically challenging, as the capacity of deep models is so high that they can totally memorize these noisy labels sooner or later during training. Nonetheless, recent studies on the memorization effects of deep neural networks show that they would first memorize training data of clean labels and then those of noisy labels. Therefore in this paper, we propose a new deep learning paradigm called ''Co-teaching'' for combating with noisy labels. Namely, we train two deep neural networks simultaneously, and let them teach each other given every mini-batch: firstly, each network feeds forward all data and selects some data of possibly clean labels; secondly, two networks communicate with each other what data in this mini-batch should be used for training; finally, each network back propagates the data selected by its peer network and updates itself. Empirical results on noisy versions of MNIST, CIFAR-10 and CIFAR-100 demonstrate that Co-teaching is much superior to the state-of-the-art methods in the robustness of trained deep models."}}
