{"id": "rB6TpjAuSRy", "cdate": 1663850286703, "mdate": null, "content": {"title": "CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers", "abstract": "In this work, we present CogVideo, a 9B-parameter transformer for text-to-video generation. The CogVideo model has been trained by inheriting a pretrained text-to-image model, CogView2, which significantly reduces the training cost and alleviates the problem of scarcity and weak relevance. We also propose a multi-frame-rate training strategy for better aligning text and video clips. CogVideo achieves state-of-the-art performance in machine evaluation and outperforms publicly available models by a large margin in human evaluation. Its codes and model are also publicly available at https://github.com/THUDM/CogVideo."}}
{"id": "n7XbkHOwKn6", "cdate": 1652737769724, "mdate": null, "content": {"title": "CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers", "abstract": "Large-scale pretrained transformers have created milestones in text (GPT-3) and text-to-image (DALL-E) generation. Its application on video generation is still faced difficulties: The huge computation makes training from scratch unaffordable; The scarcity and weak relevance of text-video datasets hinder the model understanding complex movements. In this work, we present 9-billion-parameter CogVideo, which is trained by inheriting the knowledge from the pretrained large-scale text-to-image model, CogView2. We also propose multi-frame-rate hierarchical training strategy to better align text and video clips. As (probably) the first open-source large-scale pretrained text-to-video model, the CogVideo outperforms the previous public available models at a large margin in both machine and human evaluation. \n"}}
{"id": "GkDbQb6qu_r", "cdate": 1652737405838, "mdate": null, "content": {"title": "CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers", "abstract": "Development of transformer-based text-to-image models is impeded by its slow generation and complexity, for high-resolution images. In this work, we put forward a solution based on hierarchical transformers and local parallel autoregressive generation.  \nWe pretrain a 6B-parameter transformer with a simple and flexible self-supervised task, a cross-modal general language model (CogLM), and fine-tune it for fast super-resolution. \nThe new text-to-image system, CogView2, shows very competitive generation compared to concurrent state-of-the-art DALL-E-2, and naturally supports interactive text-guided editing on images."}}
{"id": "ZMdFwZv44Dm", "cdate": 1640995200000, "mdate": 1681652923137, "content": {"title": "CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers", "abstract": ""}}
{"id": "90qlek2PdB0", "cdate": 1640995200000, "mdate": 1681652923128, "content": {"title": "CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers", "abstract": ""}}
{"id": "cnWSyJNmeCE", "cdate": 1621629847846, "mdate": null, "content": {"title": "CogView: Mastering Text-to-Image Generation via Transformers", "abstract": "Text-to-Image generation in the general domain has long been an open problem, which requires both a powerful generative model and cross-modal understanding. We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks, e.g. style learning, super-resolution, text-image ranking and fashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses. CogView achieves the state-of-the-art FID on the blurred MS COCO dataset, outperforming previous GAN-based models and a recent similar work DALL-E.\n"}}
{"id": "gcoI2O8Tak", "cdate": 1609459200000, "mdate": 1665711477679, "content": {"title": "CogView: Mastering Text-to-Image Generation via Transformers", "abstract": "Text-to-Image generation in the general domain has long been an open problem, which requires both a powerful generative model and cross-modal understanding. We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks, e.g. style learning, super-resolution, text-image ranking and fashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses. CogView achieves the state-of-the-art FID on the blurred MS COCO dataset, outperforming previous GAN-based models and a recent similar work DALL-E."}}
