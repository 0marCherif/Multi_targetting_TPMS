{"id": "jjIJ7Mvd1-C", "cdate": 1675827733897, "mdate": null, "content": {"title": "The SSL Interplay: Augmentations, Inductive Bias, and Generalization", "abstract": "Self-supervised learning (SSL) has emerged as a powerful framework to learn representations from raw data without supervision. Yet in practice, engineers face issues such as instability in tuning optimizers and collapse of representations during training. Such challenges motivate the need for a theory to shed light on the complex interplay between the choice of data augmentation, network architecture, and training algorithm. We study such an interplay with a precise analysis of generalization performance on both pretraining and downstream tasks in a theory friendly setup, and highlight several insights for SSL practitioners that arise from our theory.\n"}}
{"id": "Iqm6AiHPs_z", "cdate": 1652737274373, "mdate": null, "content": {"title": "Active Labeling: Streaming Stochastic Gradients", "abstract": "The workhorse of machine learning is stochastic gradient descent.\nTo access stochastic gradients, it is common to consider iteratively input/output pairs of a training dataset.\nInterestingly, it appears that one does not need full supervision to access stochastic gradients, which is the main motivation of this paper.\nAfter formalizing the \"active labeling\" problem, which focuses on active learning with partial supervision, we provide a streaming technique that provably minimizes the ratio of generalization error over the number of samples.\nWe illustrate our technique in depth for robust regression."}}
{"id": "ZvQRhWK8nmo", "cdate": 1624342093945, "mdate": 1624342093945, "content": {"title": "Fast rates in structured prediction", "abstract": "Discrete supervised learning problems such as classification are often tackled by introducing a continuous surrogate problem akin to regression. Bounding the original error, between estimate and solution, by the surrogate error endows discrete problems with convergence rates already shown for continuous instances. Yet, current approaches do not leverage the fact that discrete problems are essentially predicting a discrete output when continuous problems are predicting a continuous value. In this paper, we tackle this issue for general structured prediction problems, opening the way to \"super fast\" rates, that is, convergence rates for the excess risk faster than $n^{\u22121}$, where n is the number of observations, with even exponential rates with the strongest assumptions. We first illustrate it for predictors based on nearest neighbors, generalizing rates known for binary classification to any discrete problem within the framework of structured prediction. We then consider kernel ridge regression where we improve known rates in $n^{\u22121/4}$ to arbitrarily fast rates, depending on a parameter characterizing the hardness of the problem, thus allowing, under smoothness assumptions, to bypass the curse of dimensionality. "}}
{"id": "kAQdry6nNKE", "cdate": 1624342027378, "mdate": 1624342027378, "content": {"title": "Structured Prediction with Partial Labelling through the Infimum Loss", "abstract": "Annotating datasets is one of the main costs in nowadays supervised learning. The goal of weak supervision is to enable models to learn using only forms of labelling which are cheaper to collect, as partial labelling. This is a type of incomplete annotation where, for each datapoint, supervision is cast as a set of labels containing the real one. The problem of supervised learning with partial labelling has been studied for specific instances such as classification, multi-label, ranking or segmentation, but a general framework is still missing. This paper provides a unified framework based on structured prediction and on the concept of infimum loss to deal with partial labelling over a wide family of learning problems and loss functions. The framework leads naturally to explicit algorithms that can be easily implemented and for which proved statistical consistency and learning rates. Experiments confirm the superiority of the proposed approach over commonly used baselines."}}
{"id": "iLDNO9Tm2u-", "cdate": 1624341973031, "mdate": 1624341973031, "content": {"title": "Disambiguation of weak supervision with exponential convergence rates", "abstract": " Machine learning approached through supervised learning requires expensive annotation of data. This motivates weakly supervised learning, where data are annotated with incomplete yet discriminative information. In this paper, we focus on partial labelling, an instance of weak supervision where, from a given input, we are given a set of potential targets. We review a disambiguation principle to recover full supervision from weak supervision, and propose an empirical disambiguation algorithm. We prove exponential convergence rates of our algorithm under classical learnability assumptions, and we illustrate the usefulness of our method on practical examples. "}}
{"id": "9vur8-1GU7V", "cdate": 1621629692835, "mdate": null, "content": {"title": "Overcoming the curse of dimensionality with Laplacian regularization in semi-supervised learning", "abstract": "As annotations of data can be scarce in large-scale practical problems, leveraging unlabelled examples is one of the most important aspects of machine learning. This is the aim of semi-supervised learning. To benefit from the access to unlabelled data, it is natural to diffuse smoothly knowledge of labelled data to unlabelled one. This induces to the use of Laplacian regularization. Yet, current implementations of Laplacian regularization suffer from several drawbacks, notably the well-known curse of dimensionality. In this paper, we design a new class of algorithms overcoming this issue, unveiling a large body of spectral filtering methods. Additionally, we provide a statistical analysis showing that our estimators exhibit desirable behaviors. They are implemented through (reproducing) kernel methods, for which we provide realistic computational guidelines in order to make our method usable with large amounts of data.  \n"}}
