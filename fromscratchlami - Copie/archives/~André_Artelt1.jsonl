{"id": "rTRW7eyE4b", "cdate": 1672531200000, "mdate": 1683879972987, "content": {"title": "Spatial Graph Convolution Neural Networks for Water Distribution Systems", "abstract": "We investigate the task of missing value estimation in graphs as given by water distribution systems (WDS) based on sparse signals as a representative machine learning challenge in the domain of critical infrastructure. The underlying graphs have a comparably low node degree and high diameter, while information in the graph is globally relevant, hence graph neural networks face the challenge of long term dependencies. We propose a specific architecture based on message passing which displays excellent results for a number of benchmark tasks in the WDS domain. Further, we investigate a multi-hop variation, which requires considerably less resources and opens an avenue towards big WDS graphs."}}
{"id": "KfbrSpLsw7J", "cdate": 1672531200000, "mdate": 1683893002109, "content": {"title": "\"Why Here and not There?\": Diverse Contrasting Explanations of Dimensionality Reduction", "abstract": ""}}
{"id": "Dccu3gXWW1F", "cdate": 1672531200000, "mdate": 1683893001682, "content": {"title": "Let's go to the Alien Zoo: Introducing an experimental framework to study usability of counterfactual explanations for machine learning", "abstract": "IntroductionTo foster usefulness and accountability of machine learning (ML), it is essential to explain a model's decisions in addition to evaluating its performance. Accordingly, the field of explainable artificial intelligence (XAI) has resurfaced as a topic of active research, offering approaches to address the \u201chow\u201d and \u201cwhy\u201d of automated decision-making. Within this domain, counterfactual explanations (CFEs) have gained considerable traction as a psychologically grounded approach to generate post-hoc explanations. To do so, CFEs highlight what changes to a model's input would have changed its prediction in a particular way. However, despite the introduction of numerous CFE approaches, their usability has yet to be thoroughly validated at the human level.MethodsTo advance the field of XAI, we introduce the Alien Zoo, an engaging, web-based and game-inspired experimental framework. The Alien Zoo provides the means to evaluate usability of CFEs for gaining new knowledge from an automated system, targeting novice users in a domain-general context. As a proof of concept, we demonstrate the practical efficacy and feasibility of this approach in a user study.ResultsOur results suggest the efficacy of the Alien Zoo framework for empirically investigating aspects of counterfactual explanations in a game-type scenario and a low-knowledge domain. The proof of concept study reveals that users benefit from receiving CFEs compared to no explanation, both in terms of objective perf..."}}
{"id": "1wZZtCtlIAs", "cdate": 1672531200000, "mdate": 1695969016991, "content": {"title": "\"How to Make Them Stay?\": Diverse Counterfactual Explanations of Employee Attrition", "abstract": ""}}
{"id": "shUBMxaWnhT", "cdate": 1640995200000, "mdate": 1683893002095, "content": {"title": "One Explanation to Rule them All - Ensemble Consistent Explanations", "abstract": "Transparency is a major requirement of modern AI based decision making systems deployed in real world. A popular approach for achieving transparency is by means of explanations. A wide variety of different explanations have been proposed for single decision making systems. In practice it is often the case to have a set (i.e. ensemble) of decisions that are used instead of a single decision only, in particular in complex systems. Unfortunately, explanation methods for single decision making systems are not easily applicable to ensembles -- i.e. they would yield an ensemble of individual explanations which are not necessarily consistent, hence less useful and more difficult to understand than a single consistent explanation of all observed phenomena. We propose a novel concept for consistently explaining an ensemble of decisions locally with a single explanation -- we introduce a formal concept, as well as a specific implementation using counterfactual explanations."}}
{"id": "kKsu8nbCVg", "cdate": 1640995200000, "mdate": 1695969016921, "content": {"title": "Model Agnostic Local Explanations of Reject", "abstract": ""}}
{"id": "jG5NEbtIykN", "cdate": 1640995200000, "mdate": 1683893002503, "content": {"title": "Keep Your Friends Close and Your Counterfactuals Closer: Improved Learning From Closest Rather Than Plausible Counterfactual Explanations in an Abstract Setting", "abstract": "Counterfactual explanations (CFEs) highlight changes to a model\u2019s input that alter its prediction in a particular way. s have gained considerable traction as a psychologically grounded solution for explainable artificial intelligence (XAI). Recent innovations introduce the notion of plausibility for automatically generated s, enhancing their robustness by exclusively creating plausible explanations. However, practical benefits of this constraint on user experience are yet unclear. In this study, we evaluate objective and subjective usability of plausible s in an iterative learning task. We rely on a game-like experimental design, revolving around an abstract scenario. Our results show that novice users benefit less from receiving plausible rather than closest s that induce minimal changes leading to the desired outcome. Responses in a post-game survey reveal no differences for subjective usability between both groups. Following the view of psychological plausibility as comparative similarity, users in the closest condition may experience their s as more psychologically plausible than the computationally plausible counterpart. In sum, our work highlights a little-considered divergence of definitions of computational plausibility and psychological plausibility, critically confirming the need to incorporate human behavior, preferences and mental models already at the design stages of XAI. All source code and data of the current study are available: https://github.com/ukuhl/PlausibleAlienZoo"}}
{"id": "f-7C2BCUNm", "cdate": 1640995200000, "mdate": 1683883331905, "content": {"title": "Explaining Reject Options of Learning Vector Quantization Classifiers", "abstract": ""}}
{"id": "b3BfZbjf8Rv", "cdate": 1640995200000, "mdate": 1683893001680, "content": {"title": "SAM-kNN Regressor for Online Learning in Water Distribution Networks", "abstract": "Water distribution networks are a key component of modern infrastructure for housing and industry. They transport and distribute water via widely branched networks from sources to consumers. In order to guarantee a working network at all times, the water supply company continuously monitors the network and takes actions when necessary \u2013 e.g. reacting to leakages, sensor faults and drops in water quality. Since real world networks are too large and complex to be monitored by a human, algorithmic monitoring systems have been developed. A popular type of such systems are residual based anomaly detection systems that can detect events such as leakages and sensor faults. For a continuous high quality monitoring, it is necessary for these systems to adapt to changed demands and presence of various anomalies. In this work, we propose an adaption of the incremental SAM-kNN classifier for regression to build a residual based anomaly detection system for water distribution networks that is able to adapt to any kind of change."}}
{"id": "_iZMUleKAD", "cdate": 1640995200000, "mdate": 1695969016840, "content": {"title": "Contrasting Explanation of Concept Drift", "abstract": ""}}
