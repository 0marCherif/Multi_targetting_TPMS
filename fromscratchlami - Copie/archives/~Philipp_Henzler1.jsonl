{"id": "WepSV6R_Tp-", "cdate": 1609459200000, "mdate": 1667487317753, "content": {"title": "Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction", "abstract": "Traditional approaches for learning 3D object categories have been predominantly trained and evaluated on synthetic datasets due to the unavailability of real 3D-annotated category-centric data. Our main goal is to facilitate advances in this field by collecting real-world data in a magnitude similar to the existing synthetic counterparts. The principal contribution of this work is thus a large-scale dataset, called Common Objects in 3D, with real multi-view images of object categories annotated with camera poses and ground truth 3D point clouds. The dataset contains a total of 1.5 million frames from nearly 19,000 videos capturing objects from 50 MS-COCO categories and, as such, it is significantly larger than alternatives both in terms of the number of categories and objects.We exploit this new dataset to conduct one of the first large-scale \"in-the-wild\" evaluations of several new-view-synthesis and category-centric 3D reconstruction methods. Finally, we contribute NerFormer - a novel neural rendering method that leverages the powerful Transformer to reconstruct an object given a small number of its views."}}
{"id": "Uo4_z8aeEQQ", "cdate": 1609459200000, "mdate": 1667487317749, "content": {"title": "Unsupervised Learning of 3D Object Categories From Videos in the Wild", "abstract": "Recently, numerous works have attempted to learn 3D reconstructors of textured 3D models of visual categories given a training set of annotated static images of objects. In this paper, we seek to decrease the amount of needed supervision by leveraging a collection of object-centric videos captured in-the-wild without requiring any manual 3D annotations. Since existing category-centric datasets are insufficient for this problem, we contribute with a large-scale crowd-sourced dataset of object-centric videos suitable for this task. We further propose a novel method that learns via differentiable rendering of a predicted implicit surface of the scene. Here, inspired by classic multi-view stereo methods, our key technical contribution is a novel warp-conditioned implicit shape function, which is robust to the noise in the SfM video reconstructions that supervise our learning. Our evaluation demonstrates performance improvements over several deep monocular reconstruction baselines on 2 existing benchmarks and on our novel dataset."}}
{"id": "DnKwsQecqBw", "cdate": 1609459200000, "mdate": 1667487317744, "content": {"title": "Generative modelling of BRDF textures from flash images", "abstract": "We learn a latent space for easy capture, consistent interpolation, and efficient reproduction of visual material appearance. When users provide a photo of a stationary natural material captured under flashlight illumination, first it is converted into a latent material code. Then, in the second step, conditioned on the material code, our method produces an infinite and diverse spatial field of BRDF model parameters (diffuse albedo, normals, roughness, specular albedo) that subsequently allows rendering in complex scenes and illuminations, matching the appearance of the input photograph. Technically, we jointly embed all flash images into a latent space using a convolutional encoder, and -conditioned on these latent codes- convert random spatial fields into fields of BRDF parameters using a convolutional neural network (CNN). We condition these BRDF parameters to match the visual characteristics (statistics and spectra of visual features) of the input under matching light. A user study compares our approach favorably to previous work, even those with access to BRDF supervision. Project webpage: https://henzler.github.io/publication/neuralmaterial/."}}
{"id": "gtoXz5fK97G", "cdate": 1577836800000, "mdate": 1667487317758, "content": {"title": "Learning a Neural 3D Texture Space From 2D Exemplars", "abstract": "We suggest a generative model of 2D and 3D natural textures with diversity, visual fidelity and at high computational efficiency. This is enabled by a family of methods that extend ideas from classic stochastic procedural texturing (Perlin noise) to learned, deep, non-linearities. Our model encodes all exemplars from a diverse set of textures without a need to be re-trained for each exemplar. Applications include texture interpolation, and learning 3D textures from 2D exemplars."}}
{"id": "z2pAwvf2dyL", "cdate": 1546300800000, "mdate": 1667487317724, "content": {"title": "Escaping Plato's Cave: 3D Shape From Adversarial Rendering", "abstract": "We introduce PLATONICGAN to discover the 3D structure of an object class from an unstructured collection of 2D images, i. e., where no relation between photos is known, except that they are showing instances of the same category. The key idea is to train a deep neural network to generate 3D shapes which, when rendered to images, are indistinguishable from ground truth images (for a discriminator) under various camera poses. Discriminating 2D images instead of 3D shapes allows tapping into unstructured 2D photo collections instead of relying on accurated (e. g., aligned, annotated, etc.) 3D data sets. To establish constraints between 2D image observation and their 3D interpretation, we suggest a family of rendering layers that are effectively differentiable. This family includes visual hull, absorption-only (akin to x-ray), and emission-absorption. We can successfully reconstruct 3D shapes from unstructured 2D images and extensively evaluate PLATONICGAN on a range of synthetic and real data sets achieving consistent improvements over baseline methods. We further show that PLATONICGAN can be combined with 3D supervision to improve on and in some cases even surpass the quality of 3D-supervised methods."}}
{"id": "ib4gztp0nu", "cdate": 1514764800000, "mdate": 1667487317812, "content": {"title": "Object Detection on Dynamic Occupancy Grid Maps Using Deep Learning and Automatic Label Generation", "abstract": "We tackle the problem of object detection and pose estimation in a shared space downtown environment. For perception multiple laser scanners with $360^{o}$ coverage were fused in a dynamic occupancy grid map (DOGMa). A single-stage deep convolutional neural network is trained to provide object hypotheses comprising of shape, position, orientation and an existence score from a single input DOGMa. Furthermore, an algorithm for offline object extraction was developed to automatically label several hours of training data. The algorithm is based on a two-pass trajectory extraction, forward and backward in time. Typical for engineered algorithms, the automatic label generation suffers from misdetections, which makes hard negative mining impractical. Therefore, we propose a loss function counteracting the high imbalance between mostly static background and extremely rare dynamic grid cells. Experiments indicate, that the trained network has good generalization capabilities since it detects objects occasionally lost by the label algorithm. Evaluation reaches an average precision (AP) of 75.9%."}}
{"id": "BSOw27B-yTs", "cdate": 1514764800000, "mdate": 1667487317808, "content": {"title": "Deep Object Tracking on Dynamic Occupancy Grid Maps Using RNNs", "abstract": "The comprehensive representation and understanding of the driving environment is crucial to improve the safety and reliability of autonomous vehicles. In this paper, we present a new approach to establish an environment model containing a segmentation between static and dynamic background and objects modeled with shape, position and orientation parameters. Multiple laser scanners are fused into a dynamic occupancy grid map resulting in a 360\u00b0perception of the environment. A single-stage deep convolutional neural network is combined with a recurrent neural network, which takes a time series of the occupancy grid map as input and tracks cell states and its corresponding object hypotheses. The labels for training are created unsupervised with an automatic label generation algorithm. The proposed methods are evaluated in real-world experiments in complex inner city scenarios using the aforementioned 360\u00b0 laser perception. The results show a better object detection accuracy in comparison with our old approach as well as an AUC score of 0.946 for the dynamic and static segmentation. Furthermore, we gain an improved detection for occluded objects and a more consistent size estimate due to the usage of time series as input and the memory about previous states introduced by the recurrent neural network."}}
{"id": "AvHr1qe6r0v", "cdate": 1514764800000, "mdate": 1667487317820, "content": {"title": "Single-image Tomography: 3D Volumes from 2D Cranial X-Rays", "abstract": ""}}
{"id": "5FJsYmW0q3", "cdate": 1451606400000, "mdate": 1667487317820, "content": {"title": "Unconstrained Pedestrian Navigation based on Vibro-tactile Feedback around the Wristband of a Smartwatch", "abstract": "We present a bearing-based pedestrian navigation approach that utilizes vibro-tactile feedback around the user's wrist to convey information about the general direction of a target. Unlike traditional navigation, no route is pre-defined so that users can freely explore the surrounding. Our solution can be worn as a wristband for smartwatches or as a standalone device. We describe a mobile prototype with four tactors and show its feasibility in a preliminary navigation study."}}
