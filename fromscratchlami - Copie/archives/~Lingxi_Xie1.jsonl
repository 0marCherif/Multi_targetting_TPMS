{"id": "A9qBylnnUFj", "cdate": 1684307309751, "mdate": 1684307309751, "content": {"title": "Stabilizing DARTS with Amended Gradient Estimation on Architectural Parameters", "abstract": "DARTS is a popular algorithm for neural architecture search (NAS). Despite its great advantage in search efficiency, DARTS often suffers weak stability, which reflects in the large variation among individual trials as well as the sensitivity to the hyper-parameters of the search process. This paper owes such instability to an optimization gap between the super-network and its sub-networks, namely, improving the validation accuracy of the super-network does not necessarily lead to a higher expectation on the performance of the sampled sub-networks. Then, we point out that the gap is due to the inaccurate estimation of the architectural gradients, based on which we propose an amended estimation method. Mathematically, our method guarantees a bounded error from the true gradients while the original estimation does not. Our approach bridges the gap from two aspects, namely, amending the estimation on the architectural gradients, and unifying the hyper-parameter settings in the search and re-training stages. Experiments on CIFAR10 and ImageNet demonstrate that our approach largely improves search stability and, more importantly, enables DARTS-based approaches to explore much larger search spaces that have not been investigated before."}}
{"id": "lV4EMfZGgdH", "cdate": 1677628800000, "mdate": 1681668538433, "content": {"title": "Seed the Views: Hierarchical Semantic Alignment for Contrastive Representation Learning", "abstract": "Self-supervised learning based on instance discrimination has shown remarkable progress. In particular, contrastive learning, which regards each image as well as its augmentations as an individual class and tries to distinguish them from all other images, has been verified effective for representation learning. However, conventional contrastive learning does not model the relation between semantically similar samples explicitly. In this paper, we propose a general module that considers the semantic similarity among images. This is achieved by expanding the views generated by a single image to <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Cross-Samples and Multi-Levels</i> , and modeling the invariance to semantically similar images in a hierarchical way. Specifically, the cross-samples are generated by a data mixing operation, which is constrained within samples that are semantically similar, while the multi-level samples are expanded at the intermediate layers of a network. In this way, the contrastive loss is extended to allow for multiple positives per anchor, and explicitly pulling semantically similar images together at different layers of the network. Our method, termed as CSML, has the ability to integrate multi-level representations across samples in a robust way. CSML is applicable to current contrastive based methods and consistently improves the performance. Notably, using MoCo v2 as an instantiation, CSML achieves <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">76.6%</b> top-1 accuracy with linear evaluation using ResNet-50 as backbone, <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">66.7%</b> and <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">75.1%</b> top-1 accuracy with only 1% and 10% labels, respectively. <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">All these numbers set the new state-of-the-art.</i> The code is available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/haohang96/CSML</uri> ."}}
{"id": "ySSFBB4bts", "cdate": 1672531200000, "mdate": 1681668538297, "content": {"title": "Focus on Your Target: A Dual Teacher-Student Framework for Domain-adaptive Semantic Segmentation", "abstract": "We study unsupervised domain adaptation (UDA) for semantic segmentation. Currently, a popular UDA framework lies in self-training which endows the model with two-fold abilities: (i) learning reliable semantics from the labeled images in the source domain, and (ii) adapting to the target domain via generating pseudo labels on the unlabeled images. We find that, by decreasing/increasing the proportion of training samples from the target domain, the 'learning ability' is strengthened/weakened while the 'adapting ability' goes in the opposite direction, implying a conflict between these two abilities, especially for a single model. To alleviate the issue, we propose a novel dual teacher-student (DTS) framework and equip it with a bidirectional learning strategy. By increasing the proportion of target-domain data, the second teacher-student model learns to 'Focus on Your Target' while the first model is not affected. DTS is easily plugged into existing self-training approaches. In a standard UDA scenario (training on synthetic, labeled data and real, unlabeled data), DTS shows consistent gains over the baselines and sets new state-of-the-art results of 76.5\\% and 75.1\\% mIoUs on GTAv$\\rightarrow$Cityscapes and SYNTHIA$\\rightarrow$Cityscapes, respectively."}}
{"id": "jJ-gMET_8F", "cdate": 1672531200000, "mdate": 1681668537497, "content": {"title": "USAGE: A Unified Seed Area Generation Paradigm for Weakly Supervised Semantic Segmentation", "abstract": "Seed area generation is usually the starting point of weakly supervised semantic segmentation (WSSS). Computing the Class Activation Map (CAM) from a multi-label classification network is the de facto paradigm for seed area generation, but CAMs generated from Convolutional Neural Networks (CNNs) and Transformers are prone to be under- and over-activated, respectively, which makes the strategies to refine CAMs for CNNs usually inappropriate for Transformers, and vice versa. In this paper, we propose a Unified optimization paradigm for Seed Area GEneration (USAGE) for both types of networks, in which the objective function to be optimized consists of two terms: One is a generation loss, which controls the shape of seed areas by a temperature parameter following a deterministic principle for different types of networks; The other is a regularization loss, which ensures the consistency between the seed areas that are generated by self-adaptive network adjustment from different views, to overturn false activation in seed areas. Experimental results show that USAGE consistently improves seed area generation for both CNNs and Transformers by large margins, e.g., outperforming state-of-the-art methods by a mIoU of 4.1% on PASCAL VOC. Moreover, based on the USAGE-generated seed areas on Transformers, we achieve state-of-the-art WSSS results on both PASCAL VOC and MS COCO."}}
{"id": "gbZieVhotoV", "cdate": 1672531200000, "mdate": 1678015352087, "content": {"title": "VoxSeP: semi-positive voxels assist self-supervised 3D medical segmentation", "abstract": ""}}
{"id": "pBrCKz14uI", "cdate": 1668603675322, "mdate": 1668603675322, "content": {"title": "DATA: Domain-Aware and Task-Aware Self-supervised Learning", "abstract": "The paradigm of training models on massive data with-\nout label through self-supervised learning (SSL) and fine-\ntuning on many downstream tasks has become a trend re-\ncently. However, due to the high training costs and the un-\nconsciousness of downstream usages, most self-supervised\nlearning methods lack the capability to correspond to the\ndiversities of downstream scenarios, as there are various\ndata domains, different vision tasks and latency constraints\non models. Neural architecture search (NAS) is one uni-\nversally acknowledged fashion to conquer the issues above,\nbut applying NAS on SSL seems impossible as there is no\nlabel or metric provided for judging model selection. In\nthis paper, we present DATA, a simple yet effective NAS\napproach specialized for SSL that provides Domain-Aware\nand Task-Aware pre-training. Specifically, we (i) train a\nsupernet which could be deemed as a set of millions of\nnetworks covering a wide range of model scales without\nany label, (ii) propose a flexible searching mechanism com-\npatible with SSL that enables finding networks of differ-\nent computation costs, for various downstream vision tasks\nand data domains without explicit metric provided. Instan-\ntiated With MoCo v2, our method achieves promising re-\nsults across a wide range of computation costs on down-\nstream tasks, including image classification, object detec-\ntion and semantic segmentation. DATA is orthogonal to\nmost existing SSL methods and endows them the ability\nof customization on downstream needs. Extensive experi-\nments on other SSL methods demonstrate the generalizabil-\nity of the proposed method. Code is released at https:\n//github.com/GAIA-vision/GAIA-s"}}
{"id": "s7NykP5RXr", "cdate": 1668390618626, "mdate": null, "content": {"title": "Can Semantic Labels Assist Self-Supervised Visual Representation Learning?", "abstract": "Recently, contrastive learning has largely advanced the progress of unsupervised visual representation learning. Pre-trained on ImageNet, some self-supervised algorithms reported higher transfer learning performance compared to fully-supervised methods, seeming to deliver the message that human labels hardly contribute to learning transferrable visual features. In this paper, we defend the usefulness of semantic labels but point out that fully-supervised and self-supervised methods are pursuing different kinds of features. To alleviate this issue, we present a new algorithm named Supervised Contrastive Adjustment in Neighborhood (SCAN) that maximally prevents the semantic guidance from damaging the appearance feature embedding. In a series of downstream tasks, SCAN achieves superior performance compared to previous fully-supervised and self-supervised methods, and sometimes the gain is significant. More importantly, our study reveals that semantic labels are useful in assisting self-supervised methods, opening a new direction for the community."}}
{"id": "rii-JL3iR-j", "cdate": 1668390563196, "mdate": 1668390563196, "content": {"title": "MVP: Multimodality-guided Visual Pre-training", "abstract": "Recently, masked image modeling (MIM) has become a promising direction for visual pre-training. In the context of vision transformers, MIM learns effective visual representation by aligning the token-level features with a pre-defined space (e.g., BEIT used a d-VAE trained on a large image corpus as the tokenizer). In this paper, we go one step further by introducing guidance from other modalities and validating that such additional knowledge leads to impressive gains for visual pre-training. The proposed approach is named Multimodality-guided Visual Pre-training (MVP), in which we replace the tokenizer with the vision branch of CLIP, a vision-language model pre-trained on 400 million image-text pairs. We demonstrate the effectiveness of MVP by performing standard experiments, i.e., pre-training the ViT models on ImageNet and fine-tuning them on a series of downstream visual recognition tasks. In particular, pre-training ViT-Base/16 for 300 epochs, MVP reports a 52.4% mIoU on ADE20K, surpassing BEIT (the baseline and previous state-of-the-art) with an impressive margin of 6.8%."}}
{"id": "U1cPSPskTR", "cdate": 1665626344525, "mdate": null, "content": {"title": "Conformer: Local Features Coupling Global Representations for Visual Recognition", "abstract": "Within Convolutional Neural Network (CNN), the convolution operations are good at extracting local features but experience difficulty to capture global representations. Within visual transformer, the cascaded self-attention modules can capture long-distance feature dependencies but unfortunately deteriorate local feature details. In this paper, we propose a hybrid network structure, termed Conformer,\nto take advantage of convolutional operations and selfattention mechanisms for enhanced representation learning. Conformer roots in the Feature Coupling Unit (FCU), which fuses local features and global representations under different resolutions in an interactive fashion. Conformer adopts a concurrent structure so that local features and global representations are retained to the maximum extent. Experiments show that Conformer, under the comparable parameter complexity, outperforms the visual transformer (DeiT-B) by 2.3% on ImageNet. On MSCOCO, it outperforms ResNet-101 by 3.7% and 3.6% mAPs for object detection and instance segmentation, respectively, demonstrating\nthe great potential to be a general backbone network."}}
{"id": "3F6I-0-57SC", "cdate": 1663849970677, "mdate": null, "content": {"title": "HiViT: A Simpler and More Efficient Design of Hierarchical Vision Transformer", "abstract": "There has been a debate on the choice of plain vs. hierarchical vision transformers, where researchers often believe that the former (e.g., ViT) has a simpler design but the latter (e.g., Swin) enjoys higher recognition accuracy. Recently, the emerge of masked image modeling (MIM), a self-supervised visual pre-training method, raised a new challenge to vision transformers in terms of flexibility, i.e., part of image patches or tokens are to be discarded, which seems to claim the advantages of plain vision transformers. In this paper, we delve deep into the comparison between ViT and Swin, revealing that (i) the performance gain of Swin is mainly brought by a deepened backbone and relative positional encoding, (ii) the hierarchical design of Swin can be simplified into hierarchical patch embedding (proposed in this work), and (iii) other designs such as shifted-window attentions can be removed. By removing the unnecessary operations, we come up with a new architecture named HiViT (short for hierarchical ViT), which is simpler and more efficient than Swin yet further improves its performance on fully-supervised and self-supervised visual representation learning. In particular, after pre-trained using masked autoencoder (MAE) on ImageNet-1K, HiViT-B reports a 84.6% accuracy on ImageNet-1K classification, a 53.3% box AP on COCO detection, and a 52.8% mIoU on ADE20K segmentation, significantly surpassing the baseline. Code is available at https://github.com/zhangxiaosong18/hivit."}}
