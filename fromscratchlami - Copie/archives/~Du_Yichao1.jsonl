{"id": "zDTNMLvgv9h", "cdate": 1672531200000, "mdate": 1681650527560, "content": {"title": "Federated Nearest Neighbor Machine Translation", "abstract": ""}}
{"id": "Re9yHOeYFum", "cdate": 1672531200000, "mdate": 1681650527593, "content": {"title": "Simple and Scalable Nearest Neighbor Machine Translation", "abstract": ""}}
{"id": "R1U5G2spbLd", "cdate": 1663850203072, "mdate": null, "content": {"title": "Federated Nearest Neighbor Machine Translation", "abstract": "To protect user privacy and meet legal regulations, federated learning (FL) is attracting significant attention. Training neural machine translation (NMT) models with traditional FL algorithm (e.g., FedAvg) typically relies on multi-round model-based interactions. However, it is impractical and inefficient for machine translation tasks due to the vast communication overheads and heavy synchronization. In this paper, we propose a novel federated nearest neighbor (FedNN) machine translation framework that, instead of multi-round model-based interactions, leverages one-round memorization-based interaction to share knowledge across different clients to build low-overhead privacy-preserving systems. The whole approach equips the public NMT model trained on large-scale accessible data with a $k$-nearest-neighbor ($k$NN) classifier and integrates the external datastore constructed by private text data in all clients to form the final FL model.  A two-phase datastore encryption strategy is introduced to achieve privacy-preserving during this process.  Extensive experiments show that FedNN significantly reduces computational and communication costs compared with FedAvg, while maintaining promising performance in different FL settings."}}
{"id": "uu1GBD9SlLe", "cdate": 1663850103976, "mdate": null, "content": {"title": "Simple and Scalable Nearest Neighbor Machine Translation", "abstract": "$k$NN-MT is a straightforward yet powerful approach for fast domain adaptation, which directly plugs the pre-trained neural machine translation (NMT) models with domain-specific token-level $k$-nearest-neighbor ($k$NN) retrieval to achieve domain adaptation without retraining. Despite being conceptually attractive, $k$NN-MT is burdened with massive storage requirements and high computational complexity since it conducts nearest neighbor searches over the entire reference corpus. In this paper, we propose a simple and scalable nearest neighbor machine translation framework to drastically promote the decoding and storage efficiency of $k$NN-based models while maintaining the translation performance. To this end, we dynamically construct a extremely small datastore for each input via sentence-level retrieval to avoid searching the entire datastore in vanilla $k$NN-MT, based on which we further introduce a distance-aware adapter to adaptively incorporate the $k$NN retrieval results into the pre-trained NMT models. Experiments on machine translation in two general settings, static domain adaptation, and online learning, demonstrate that our proposed approach not only achieves almost 90% speed as the NMT model without performance degradation, but also significantly reduces the storage requirements of $k$NN-MT. "}}
{"id": "KoEa6h1o6D1", "cdate": 1663849945618, "mdate": null, "content": {"title": "Interventional Rationalization", "abstract": "Selective rationalizations improve the explainability of neural networks by selecting a subsequence of the input (i.e., rationales) to explain the prediction results. Although existing methods have achieved promising results, they still suffer from adopting the spurious correlations in data (aka., shortcuts) to compose rationales and make predictions. Inspired by the causal theory, in this paper, we develop an interventional rationalization (Inter-RAT) to discover the causal rationales. Specifically, we first analyse the causalities among the input, rationales and results with a structural causal model. Then, we discover spurious correlations between input and rationales, and between rationales and results, respectively, by identifying the confounder in the causalities. Next, based on the backdoor adjustment, we propose a causal intervention method to remove the spurious correlations in input and rationales. Further, we discuss reasons why spurious correlations between the selected rationales and results exist by analysing the limitations of the sparsity constraint in the rationalization, and employ the causal intervention method to remove these correlations. Extensive experimental results on three real-world datasets clearly validate the effectiveness of our proposed method.\n"}}
{"id": "6OhjECfqt2", "cdate": 1652737670486, "mdate": null, "content": {"title": "DARE: Disentanglement-Augmented Rationale Extraction", "abstract": "Rationale extraction can be considered as a straightforward method of improving the model explainability, where rationales are a subsequence of the original inputs, and can be extracted to support the prediction results. Existing methods are mainly cascaded with the selector which extracts the rationale tokens, and the predictor which makes the prediction based on selected tokens. Since previous works fail to fully exploit the original input, where the information of non-selected tokens is ignored, in this paper, we propose a Disentanglement-Augmented Rationale Extraction (DARE) method, which encapsulates more information from the input to extract rationales. Specifically, it first disentangles the input into the rationale representations and the non-rationale ones, and then learns more comprehensive rationale representations for extracting by minimizing the mutual information (MI) between the two disentangled representations. Besides, to improve the performance of MI minimization, we develop a new MI estimator by exploring existing MI estimation methods. Extensive experimental results on three real-world datasets and simulation studies clearly validate the effectiveness of our proposed method. Code is released at https://github.com/yuelinan/DARE."}}
{"id": "wpNnajVqqw", "cdate": 1640995200000, "mdate": 1680513977718, "content": {"title": "Non-Parametric Domain Adaptation for End-to-End Speech Translation", "abstract": ""}}
{"id": "WxMw6-nE8E", "cdate": 1640995200000, "mdate": 1667918888121, "content": {"title": "Regularizing End-to-End Speech Translation with Triangular Decomposition Agreement", "abstract": "End-to-end speech-to-text translation (E2E-ST) is becoming increasingly popular due to the potential of its less error propagation, lower latency, and fewer parameters. Given the triplet training corpus\u3008speech, transcription, translation\u3009, the conventional high-quality E2E-ST system leverages the\u3008speech, transcription\u3009pair to pre-train the model and then utilizes the\u3008speech, translation\u3009pair to optimize it further. However, this process only involves two-tuple data at each stage, and this loose coupling fails to fully exploit the association between triplet data. In this paper, we attempt to model the joint probability of transcription and translation based on the speech input to directly leverage such triplet data. Based on that, we propose a novel regularization method for model training to improve the agreement of dual-path decomposition within triplet data, which should be equal in theory. To achieve this goal, we introduce two Kullback-Leibler divergence regularization terms into the model training objective to reduce the mismatch between output probabilities of dual-path. Then the well-trained model can be naturally transformed as the E2E-ST models by a pre-defined early stop tag. Experiments on the MuST-C benchmark demonstrate that our proposed approach significantly outperforms state-of-the-art E2E-ST baselines on all 8 language pairs while achieving better performance in the automatic speech recognition task."}}
{"id": "ogIS1NrsHZA", "cdate": 1609459200000, "mdate": 1664980020385, "content": {"title": "Rethinking Zero-shot Neural Machine Translation: From a Perspective of Latent Variables", "abstract": ""}}
{"id": "Qsy4p-lqMOV", "cdate": 1609459200000, "mdate": 1667918888176, "content": {"title": "Knowledge Powered Cooperative Semantic Fusion for Patent Classification", "abstract": "Patent classification is beneficial for many patent applications, such as patent quality valuation, retrieval, and litigation analysis. Recently, many automatic patent classification methods have been proposed to save labor costs, which usually formulate this task as a multi-label text classification problem. In reality, patent language is highly terminological, full of scientific entities and domain knowledge. However, existing works seldom consider such unique property of patents, which reduces the classification performance. To this end, we propose a novel framework named Knowledge Powered Cooperative Semantic Fusion to capture deeper knowledge semantics for patent classification. Specifically, we first exploit knowledge graphs to enrich the patent with related entities. Then we design a mutual attention mechanism between entities and original texts to emphasize the crucial semantics of entities with the guide of texts, and vice versa. Finally, we introduce the graph convolutional network further to enhance the fusion representation of entities and texts. Extensive experiments on large-scale patent data demonstrate the superior performance of our model on the patent classification task."}}
