{"id": "imJN0MUZkbA", "cdate": 1672531200000, "mdate": 1681153106149, "content": {"title": "Continual Learning of Language Models", "abstract": ""}}
{"id": "SnBDX5k-KuJ", "cdate": 1663850488319, "mdate": null, "content": {"title": "Solving Continual Learning via Problem Decomposition", "abstract": "This paper is concerned with class incremental learning (CIL) in continual learning (CL). CIL is the popular continual learning paradigm in which a system receives a sequence of tasks with different classes in each task and is expected to learn to predict the class of each test instance without given any task related information for the instance. Although many techniques have been proposed to solve CIL, it remains to be highly challenging due to the difficulty of dealing with catastrophic forgetting (CF). This paper starts from the first principle and proposes a novel method to solve the problem. The definition of CIL reveals that the problem can be decomposed into two probabilities: within-task prediction probability and task-id prediction probability. This paper proposes an effective technique to estimate these two probabilities based on the estimation of feature distributions in the latent space using incremental PCA and Mahalanobis distance. The proposed method does not require a memory buffer to save replay data and it outperforms strong baselines including replay-based methods."}}
{"id": "m_GDIItaI3o", "cdate": 1663850463925, "mdate": null, "content": {"title": "Continual Pre-training of Language Models", "abstract": "Language models (LMs) have been instrumental for the rapid advance of natural language processing. This paper studies continual pre-training of LMs, in particular, continual domain-adaptive pre-training (or continual DAP-training). Existing research has shown that further pre-training an LM using a domain corpus to adapt the LM to the domain can improve the end-task performance in the domain. This paper proposes a novel method to continually DAP-train an LM with a sequence of unlabeled domain corpora to adapt the LM to these domains to improve their end-task performances. The key novelty of our method is a soft-masking mechanism that directly controls the update to the LM. A novel proxy is also proposed to preserve the general knowledge in the original LM. Additionally, it contrasts the representations of the previously learned domain knowledge (including the general knowledge in the pre-trained LM) and the knowledge from the current full network to achieve knowledge integration. The method not only overcomes catastrophic forgetting, but also achieves knowledge transfer to improve end-task performances. Empirical evaluation demonstrates the effectiveness of the proposed method."}}
{"id": "Zz8_2A4iPS", "cdate": 1663850399703, "mdate": null, "content": {"title": "Continual Learning with Soft-Masking of Parameter-Level Gradient Flow", "abstract": "Existing research on task incremental learning in continual learning has primarily focused on preventing catastrophic forgetting (CF). Several techniques have achieved learning with no CF. However, they attain it by letting each task monopolize a sub-network in a shared network, which seriously limits knowledge transfer (KT) and causes over-consumption of the network capacity, i.e., as more tasks are learned, the performance deteriorates. The goal of this paper is threefold: (1) overcoming CF, (2) encouraging KT, and (3) tackling the capacity problem. A novel and simple technique (called SPG) is proposed that soft-masks (partially blocks) parameter updating in training based on the importance of each parameter to old tasks. Each task still uses the full network, i.e., no monopoly of any part of the network by any task, which enables maximum KT and reduction of capacity usage. Extensive experiments demonstrate the effectiveness of SPG in achieving all three objectives. More notably, it attains significant transfer of knowledge not only among similar tasks (with shared knowledge) but also among dissimilar tasks (with little shared knowledge) while preventing CF."}}
{"id": "bA8CYH5uEn_", "cdate": 1652737770763, "mdate": null, "content": {"title": "A Theoretical Study on Solving Continual Learning", "abstract": "Continual learning (CL) learns a sequence of tasks incrementally. There are two popular CL settings, class incremental learning (CIL) and task incremental learning (TIL). A major challenge of CL is catastrophic forgetting (CF). While a number of techniques are already available to effectively overcome CF for TIL, CIL remains to be highly challenging. So far, little theoretical study has been done to provide a principled guidance on how to solve the CIL problem. This paper performs such a study. It first shows that probabilistically, the CIL problem can be decomposed into two sub-problems: Within-task Prediction (WP) and Task-id Prediction (TP). It further proves that TP is correlated with out-of-distribution (OOD) detection, which connects CIL and OOD detection. The key conclusion of this study is that regardless of whether WP and TP or OOD detection are defined explicitly or implicitly by a CIL algorithm, good WP and good TP or OOD detection are necessary and sufficient for good CIL performances. Additionally, TIL is simply WP. Based on the theoretical result, new CIL methods are also designed, which outperform strong baselines in both CIL and TIL settings by a large margin."}}
{"id": "pwIuBE6ePI", "cdate": 1640995200000, "mdate": 1668171758748, "content": {"title": "Continual Learning Based on OOD Detection and Task Masking", "abstract": ""}}
{"id": "b7P12-3EXns", "cdate": 1640995200000, "mdate": 1681153106041, "content": {"title": "A Multi-Head Model for Continual Learning via Out-of-Distribution Replay", "abstract": ""}}
{"id": "ao6CWyOgiP", "cdate": 1640995200000, "mdate": 1681153106149, "content": {"title": "A Multi-Head Model for Continual Learning via Out-of-Distribution Replay", "abstract": ""}}
{"id": "_iZGX0x01t-", "cdate": 1640995200000, "mdate": 1681153106155, "content": {"title": "A Theoretical Study on Solving Continual Learning", "abstract": ""}}
{"id": "LDyzxeDJLKT", "cdate": 1640995200000, "mdate": 1668171758797, "content": {"title": "Continual Learning Based on OOD Detection and Task Masking", "abstract": ""}}
