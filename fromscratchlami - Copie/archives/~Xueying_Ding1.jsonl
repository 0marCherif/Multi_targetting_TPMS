{"id": "YXvGXEmtZ5N", "cdate": 1654304266601, "mdate": null, "content": {"title": "BOND: Benchmarking Unsupervised Outlier Node Detection on Static Attributed Graphs", "abstract": "Detecting which nodes in graphs are outliers is a relatively new machine learning task with numerous applications. Despite the proliferation of algorithms developed in recent years for this task, there has been no standard comprehensive setting for performance evaluation. Consequently, it has been difficult to understand which methods work well and when under a broad range of settings. To bridge this gap, we present\u2014to the best of our knowledge\u2014the first comprehensive benchmark for unsupervised outlier node detection on static attributed graphs called BOND, with the following highlights. (1) We benchmark the outlier detection performance of 14 methods ranging from classical matrix factorization to the latest graph neural networks. (2) Using nine real datasets, our benchmark assesses how the different detection methods respond to two major types of synthetic outliers and separately to \u201corganic\u201d (real non-synthetic) outliers. (3) Using an existing random graph generation technique, we produce a family of synthetically generated datasets of different graph sizes that enable us to compare the running time and memory usage of the different outlier detection algorithms. Based on our experimental results, we discuss the pros and cons of existing graph outlier detection algorithms, and we highlight opportunities for future research. Importantly, our code is freely available and meant to be easily extendable: https://github.com/pygod-team/pygod/tree/main/benchmark"}}
{"id": "cUY5OkP3VR", "cdate": 1652737837062, "mdate": null, "content": {"title": "Hyperparameter Sensitivity in Deep Outlier Detection: Analysis and a Scalable Hyper-Ensemble Solution", "abstract": "Outlier detection (OD) literature exhibits numerous algorithms as it applies to diverse domains. However, given a new detection task, it is unclear how to choose an algorithm to use, nor how to set its hyperparameter(s) (HPs) in unsupervised settings. HP tuning is an ever-growing problem with the arrival of many new detectors based on deep learning, which usually come with a long list of HPs. Surprisingly, the issue of model selection in the outlier mining literature has been \u201cthe elephant in the room\u201d; a significant factor in unlocking the utmost potential of deep methods, yet little said or done to systematically tackle the issue. In the first part of this paper, we conduct the first large-scale analysis on the HP sensitivity of deep OD methods, and through more than 35,000 trained models, quantitatively demonstrate that model selection is inevitable. Next, we design a HP-robust and scalable deep hyper-ensemble model called ROBOD that assembles models with varying HP configurations, bypassing the choice paralysis. Importantly, we introduce novel strategies to speed up ensemble training, such as parameter sharing, batch/simultaneous training, and data subsampling, that allow us to train fewer models with fewer parameters. Extensive experiments on both image and tabular datasets show that ROBOD achieves and retains robust, state-of-the-art detection performance as compared to its modern counterparts, while taking only 2-10% of the time by the na\u00efve hyper-ensemble with independent training."}}
{"id": "gLEgKWySYId", "cdate": 1637368589914, "mdate": null, "content": {"title": "Physics Informed Machine Learning with Misspecified Priors: \\\\An analysis of Turning Operation in Lathe Machines", "abstract": "The recent development of physics informed neural networks (PINNs) has explored the inclusion of prior physics knowledge into the objective function of deep learning models as differential equation loss component to supervise learning of complex systems under data-constrained settings. However, PINN framework requires that expert-provided knowledge about the physical system is perfectly accurate, neglecting cases where there is potential for fallibility in expert judgment. We extend this research to consider the effect of explicit fallible expert judgment in the learning process. First, we theoretically upper bound the effect of fallible expert-provided information on the convergence of PINNs to the true solution. We show how to opportunistically leverage fallible expert knowledge when data are scarce, and gracefully diminish reliance on inaccurate expert judgment as more data are acquired. Second, we examine the limitations of the PINN in learning noisy real-world physical systems, and apply a modified Seq2seq learning with applications in turning operation in lathe machines. We also propose a combination of PINN framework with recurrent neural networks for predicting system behavior outside the training domain."}}
