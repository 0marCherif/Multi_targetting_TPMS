{"id": "GYwOtBGHcRV", "cdate": 1683700231012, "mdate": 1683700231012, "content": {"title": "A Riemannian geometric framework for manifold learning of non-Euclidean data", "abstract": "A growing number of problems in data analysis and classification involve data that\nare non-Euclidean. For such problems, a naive application of vector space analysis\nalgorithms will produce results that depend on the choice of local coordinates used to\nparametrize the data. At the same time, many data analysis and classification problems\neventually reduce to an optimization, in which the criteria being minimized can be\ninterpreted as the distortion associated with a mapping between two curved spaces.\nExploiting this distortion minimizing perspective, we first show that manifold learning\nproblems involving non-Euclidean data can be naturally framed as seeking a mapping\nbetween two Riemannian manifolds that is closest to being an isometry. A family of\ncoordinate-invariant first-order distortion measures is then proposed that measure the\nproximity of the mapping to an isometry, and applied to manifold learning for non\u0002Euclidean data sets. Case studies ranging from synthetic data to human mass-shape\ndata demonstrate the many performance advantages of our Riemannian distortion\nminimization framework."}}
{"id": "szpdL1613YU", "cdate": 1683700117766, "mdate": 1683700117766, "content": {"title": "Learning to increase matching efficiency in identifying additional b-jets in the ttbb process", "abstract": "The t\u00aftH(bb) process is an essential channel in revealing the Higgs boson properties; however, its final state has an irreducible \u00af\nbackground from the t\u00aftbb process, which produces a top quark pair in association with a b quark pair. Therefore, understanding the \u00af\nt\u00aftbb process is crucial for improving the sensitivity of a search for the t \u00af \u00aftH(bb) process. To this end, when measuring the differential \u00af\ncross section of the t\u00aftbb process, we need to distinguish the b-jets originating from top quark decays and additional b-jets originating \u00af\nfrom gluon splitting. In this paper, we train deep neural networks that identify the additional b-jets in the t\u00aftbb events under the \u00af\nsupervision of a simulated t\u00aftbb event data set in which true additional b-jets are indicated. By exploiting the special structure of \u00af\nthe t\u00aftbb event data, several loss functions are proposed and minimized to directly increase matching efficiency, i.e., the accuracy \u00af\nof identifying additional b-jets. We show that, via a proof-of-concept experiment using synthetic data, our method can be more\nadvantageous for improving matching efficiency than the deep learning-based binary classification approach presented in [1]. Based\non simulated t\u00aftbb event data in the lepton+jets channel from pp collision at \u00af \u221as \u0003 13 TeV, we then verify that our method can identify\nadditional b-jets more accurately: compared with the approach in [1], the matching efficiency improves from 62.1% to 64.5% and\nfrom 59.9% to 61.7% for the leading order and the next-to-leading order simulations, respectively."}}
{"id": "UKQfubE9Nq", "cdate": 1683699957551, "mdate": null, "content": {"title": "Advantages of deep learning with convolutional neural network in detecting disc displacement of the temporomandibular joint in magnetic resonance imaging", "abstract": "This study investigated the usefulness of deep learning-based automatic detection of anterior disc\ndisplacement (ADD) from magnetic resonance imaging (MRI) of patients with temporomandibular\njoint disorder (TMD). Sagittal MRI images of 2520 TMJs were collected from 861 men and 399 women\n(average age 37.33 \u00b1 18.83 years). A deep learning algorithm with a convolutional neural network was\ndeveloped. Data augmentation and the Adam optimizer were applied to reduce the risk of overfitting\nthe deep-learning model. The prediction performances were compared between the models and\nhuman experts based on areas under the curve (AUCs). The fine-tuning model showed excellent\nprediction performance (AUC = 0.8775) and acceptable accuracy (approximately 77%). Comparing the\nAUC values of the from-scratch (0.8269) and freeze models (0.5858) showed lower performances of the\nother models compared to the fine-tuning model. In Grad-CAM visualizations, the fine-tuning scheme\nfocused more on the TMJ disc when judging ADD, and the sparsity was higher than that of the fromscratch\nscheme (84.69% vs. 55.61%, p < 0.05). The three fine-tuned ensemble models using different\ndata augmentation techniques showed a prediction accuracy of 83%. Moreover, the AUC values of\nADD were higher when patients with TMD were divided by age (0.8549\u20130.9275) and sex (male: 0.8483,\nfemale: 0.9276). While the accuracy of the ensemble model was higher than that of human experts,\nthe difference was not significant (p = 0.1987\u20130.0671). Learning from pre-trained weights allowed the\nfine-tuning model to outperform the from-scratch model. Another benefit of the fine-tuning model\nfor diagnosing ADD of TMJ in Grad-CAM analysis was the deactivation of unwanted gradient values\nto provide clearer visualizations compared to the from-scratch model. The Grad-CAM visualizations\nalso agreed with the model learned through important features in the joint disc area. The accuracy\nwas further improved by an ensemble of three fine-tuning models using diversified data. The main\nbenefits of this model were the higher specificity compared to human experts, which may be useful for\npreventing true negative cases, and the maintenance of its prediction accuracy across sexes and ages,\nsuggesting a generalized prediction."}}
{"id": "bSbKRWl5uzY", "cdate": 1683699825287, "mdate": null, "content": {"title": "Age group prediction with panoramic radiomorphometric parameters using machine learning algorithms", "abstract": "The aim of this study is to investigate the relationship of 18 radiomorphometric parameters of\npanoramic radiographs based on age, and to estimate the age group of people with permanent\ndentition in a non-invasive, comprehensive, and accurate manner using five machine learning\nalgorithms. For the study population (209 men and 262 women; mean age, 32.12 \u00b1 18.71 years), 471\ndigital panoramic radiographs of Korean individuals were applied. The participants were divided into\nthree groups (with a 20-year age gap) and six groups (with a 10-year age gap), and each age group\nwas estimated using the following five machine learning models: a linear discriminant analysis,\nlogistic regression, kernelized support vector machines, multilayer perceptron, and extreme gradient\nboosting. Finally, a Fisher discriminant analysis was used to visualize the data configuration. In\nthe prediction of the three age-group classification, the areas under the curve (AUCs) obtained for\nclassifying young ages (10\u201319 years) ranged from 0.85 to 0.88 for five different machine learning\nmodels. The AUC values of the older age group (50\u201369 years) ranged from 0.82 to 0.88, and those of\nadults (20\u201349 years) were approximately 0.73. In the six age-group classification, the best scores were\nalso found in age groups 1 (10\u201319 years) and 6 (60\u201369 years), with mean AUCs ranging from 0.85 to\n0.87 and 80 to 0.90, respectively. A feature analysis based on LDA weights showed that the L-Pulp\nArea was important for discriminating young ages (10\u201349 years), and L-Crown, U-Crown, L-Implant,\nU-Implant, and Periodontitis were used as predictors for discriminating older ages (50\u201369 years).\nWe established acceptable linear and nonlinear machine learning models for a dental age group\nestimation using multiple maxillary and mandibular radiomorphometric parameters. Since certain\nradiomorphological characteristics of young and the elderly were linearly related to age, young and\nold groups could be easily distinguished from other age groups with automated machine learning\nmodels."}}
{"id": "_q7A0m3vXH0", "cdate": 1663850551553, "mdate": null, "content": {"title": "Geometrically regularized autoencoders for non-Euclidean data", "abstract": "Regularization is almost {\\it de rigueur} when designing autoencoders that are sparse and robust to noise. Given the recent surge of interest in machine learning problems involving non-Euclidean data, in this paper we address the regularization of autoencoders on curved spaces. We show that by ignoring the underlying geometry of the data and applying standard vector space regularization techniques, autoencoder performance can be severely degraded, or worse, training can fail to converge. Assuming that both the data space and latent space can be modeled as Riemannian manifolds, we show how to construct regularization terms in a coordinate-invariant way, and develop geometric generalizations of the denoising autoencoder and reconstruction contractive autoencoder such that the essential properties that enable the estimation of the derivative of the log-probability density are preserved. Drawing upon various non-Euclidean data sets, we show that our geometric autoencoder regularization techniques can have important performance advantages over vector-spaced methods while avoiding other breakdowns that can result from failing to account for the underlying geometry."}}
{"id": "MloVsjTjlUY", "cdate": 1652737738758, "mdate": null, "content": {"title": "Local Metric Learning for Off-Policy Evaluation in Contextual Bandits with Continuous Actions", "abstract": "We consider local kernel metric learning for off-policy evaluation (OPE) of deterministic policies in contextual bandits with continuous action spaces. Our work is motivated by practical scenarios where the target policy needs to be deterministic due to domain requirements, such as prescription of treatment dosage and duration in medicine. Although importance sampling (IS) provides a basic principle for OPE, it is ill-posed for the deterministic target policy with continuous actions. Our main idea is to relax the target policy and pose the problem as kernel-based estimation, where we learn the kernel metric in order to minimize the overall mean squared error (MSE). We present an analytic solution for the optimal metric, based on the analysis of bias and variance. Whereas prior work has been limited to scalar action spaces or kernel bandwidth selection, our work takes a step further being capable of vector action spaces and metric optimization. We show that our estimator is consistent, and significantly reduces the MSE compared to baseline OPE methods through experiments on various domains."}}
{"id": "AVh_HTC76u", "cdate": 1652737717766, "mdate": null, "content": {"title": "A Reparametrization-Invariant Sharpness Measure Based on Information Geometry", "abstract": "It has been observed that the generalization performance of neural networks correlates with the sharpness of their loss landscape. Dinh et al. (2017) have observed that existing formulations of sharpness measures fail to be invariant with respect to scaling and reparametrization. While some scale-invariant measures have recently been proposed, reparametrization-invariant measures are still lacking. Moreover, they often do not provide any theoretical insights into generalization performance nor lead to practical use to improve the performance. Based on an information geometric analysis of the neural network parameter space, in this paper we propose a reparametrization-invariant sharpness measure that captures the change in loss with respect to changes in the probability distribution modeled by neural networks, rather than with respect to changes in the parameter values. We reveal some theoretical connections of our measure to generalization performance. In particular, experiments confirm that using our measure as a regularizer in neural network training significantly improves performance."}}
{"id": "INO8hGXD2M", "cdate": 1632875764486, "mdate": null, "content": {"title": "Adversarial Distributions Against Out-of-Distribution Detectors", "abstract": "Out-of-distribution (OOD) detection is the task of determining whether an input lies outside the training data distribution. As an outlier may deviate from the training distribution in unexpected ways, an ideal OOD detector should be able to detect all types of outliers. However, current evaluation protocols test a detector over OOD datasets that cover only a small fraction of all possible outliers, leading to overly optimistic views of OOD detector performance.  In this paper, we propose a novel evaluation framework for OOD detection that tests a detector over a larger, unexplored space of outliers.  In our framework, a detector is evaluated with samples from its adversarial distribution, which generates diverse outlier samples that are likely to be misclassified as in-distribution by the detector. Using adversarial distributions, we investigate OOD detectors with reported near-perfect performance on standard benchmarks like CIFAR-10 vs SVHN. Our methods discover a wide range of samples that are obviously outlier but recognized as in-distribution by the detectors, indicating that current state-of-the-art detectors are not as perfect as they seem on existing benchmarks."}}
{"id": "gkOYZpeGEK", "cdate": 1601308280267, "mdate": null, "content": {"title": "Uniform Manifold Approximation with Two-phase Optimization", "abstract": "We present a dimensionality reduction algorithm called Uniform Manifold Approximation with Two-phase Optimization (UMATO) which produces less biased global structures in the embedding results and is robust over diverse initialization methods than previous methods such as $t$-SNE and UMAP. We divide the optimization into two phases to alleviate the bias by establishing the global structure early using the representatives of the high-dimensional structures. The phases are 1) global optimization to obtain the overall skeleton of data and 2) local optimization to identify the regional characteristics of local areas. In our experiments with one synthetic and three real-world datasets, UMATO outperformed widely-used baseline algorithms, such as PCA, red, $t$-SNE, UMAP, topological autoencoders and Anchor $t$-SNE, in terms of quality metrics and 2D projection results."}}
{"id": "HQoCa9WODc0", "cdate": 1601308121071, "mdate": null, "content": {"title": "Suppressing Outlier Reconstruction in Autoencoders for Out-of-Distribution Detection", "abstract": "While only trained to reconstruct training data, autoencoders may produce high-quality reconstructions of inputs that are well outside the training data distribution.  This phenomenon, which we refer to as outlier reconstruction, has a detrimental effect on the use of autoencoders for outlier detection, as an autoencoder will misclassify a clear outlier as being in-distribution.  In this paper, we introduce the Energy-Based Autoencoder (EBAE), an autoencoder that is considerably less susceptible to outlier reconstruction. \nThe core idea of EBAE is to treat the reconstruction error as an energy function of a normalized density and to strictly enforce the normalization constraint. We show that the reconstruction of non-training inputs can be suppressed, and the reconstruction error made highly discriminative to outliers, by enforcing this constraint.  We empirically show that EBAE significantly outperforms both existing autoencoders and other generative models for several out-of-distribution detection tasks."}}
