{"id": "YLGaN7tzQO", "cdate": 1693526400000, "mdate": 1700033652538, "content": {"title": "MAL-Net: Multiscale Attention Link Network for accurate eye center detection", "abstract": ""}}
{"id": "W8u9Y8Xc_cm", "cdate": 1693526400000, "mdate": 1699162911210, "content": {"title": "Learning From Interaction-Enhanced Scene Graph for Pedestrian Collision Risk Assessment", "abstract": "Collision risk assessment aims to provide a subjective cognitive comprehension of the risk level in driving scenarios, which is critical for the safety of autonomous driving systems. Pedestrian crossing scenarios contain intricate human-vehicle interactions. Hence, it is important to capture the rich relations between traffic entities and to assess the collision risk promptly to ensure safety. Existing studies focus on modeling the spatial relationships between the ego-vehicle and other vehicles in typical traffic scenarios, while ignoring the complex interactions between pedestrians and the ego-vehicle in critical driving scenarios. To address this issue, we propose a novel approach that involves constructing traffic scene graphs with enhanced vehicle-pedestrian interactions, along with introducing an innovative deep model built upon Transformer and GCN for pedestrian collision risk assessment. Specifically, to facilitate spatio-temporal modeling of traffic scene graph sequence, we propose a novel unified framework that integrates Multi-Relation Graph Convolution Network (MR-GCN) and Temporal Transformer Encoder. In addition, two variants of traffic scene graph datasets termed as Interaction-Enhanced Scene Graph (IESG) and None-Interaction-Enhanced Scene Graph (Non-IESG) are created for the purpose of assessing pedestrian collision risk, utilizing the CAP-DATA and JAAD respectively. Experiments are conducted on our newly created traffic scene graph datasets of pedestrian crossing scenes. The results on the IESG dataset show that our model outperforms the baseline model with higher accuracy (94% vs. 84%), higher AUC (98% vs. 89%), and higher F1-score (93% vs. 84%). IESG and Non-IESG datasets are available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/Pedestrian-Crossing-Collision-Risk-Assessment-Datasets</uri> ."}}
{"id": "L0bxBRe02B", "cdate": 1688169600000, "mdate": 1700033652722, "content": {"title": "Research on Adaptive 1DCNN Network Intrusion Detection Technology Based on BSGM Mixed Sampling", "abstract": "The development of internet technology has brought us benefits, but at the same time, there has been a surge in network attack incidents, posing a serious threat to network security. In the real world, the amount of attack data is much smaller than normal data, leading to a severe class imbalance problem that affects the performance of classifiers. Additionally, when using CNN for detection and classification, manual adjustment of parameters is required, making it difficult to obtain the optimal number of convolutional kernels. Therefore, we propose a hybrid sampling technique called Borderline-SMOTE and Gaussian Mixture Model (GMM), referred to as BSGM, which combines the two approaches. We utilize the Quantum Particle Swarm Optimization (QPSO) algorithm to automatically determine the optimal number of convolutional kernels for each one-dimensional convolutional layer, thereby enhancing the detection rate of minority classes. In our experiments, we conducted binary and multi-class experiments using the KDD99 dataset. We compared our proposed BSGM-QPSO-1DCNN method with ROS-CNN, SMOTE-CNN, RUS-SMOTE-CNN, RUS-SMOTE-RF, and RUS-SMOTE-MLP as benchmark models for intrusion detection. The experimental results show the following: (i) BSGM-QPSO-1DCNN achieves high accuracy rates of 99.93% and 99.94% in binary and multi-class experiments, respectively; (ii) the precision rates for the minority classes R2L and U2R are improved by 68% and 66%, respectively. Our research demonstrates that BSGM-QPSO-1DCNN is an efficient solution for addressing the imbalanced data issue in this field, and it outperforms the five intrusion detection methods used in this study."}}
{"id": "wfuFPRYENz", "cdate": 1680307200000, "mdate": 1682325877224, "content": {"title": "Fusion of forehead EEG with machine vision for real-time fatigue detection in an automatic processing pipeline", "abstract": "Driving fatigue is a leading contributor to traffic accidents and fatalities. For automatic detection of fatigue, multimodal data fusion is a potential key technique, especially the merging of electroencephalogram (EEG) and eye movement. Although EEG can produce objective measures of fatigue with very high temporal resolution, an unfriendly multichannel system limits its practical application while portable wearables and machine vision receive much attention since they are pervasive and user friendly. Hence, this study aims to construct a novel pipeline by using machine vision technique to improve the quality of driver fatigue detection based on forehead EEG. By coupling the Karolinska Sleepiness Scale (KSS) and percentage of eyelid closure (PERCLOS), the precise and reliable dataset for reflecting drivers\u2019 fatigue levels were obtained. Moreover, major artifact contamination related to blink activity for frontal-channel EEG was removed by a synchronous video-based eyeblink event marker. In addition, the scale-invariant feature transform (SIFT) features of eyelid keypoints was applied to fuse with the EEG-driven features. Sixteen subjects participated in a realistic driving simulation experiment. Both machine and deep learning methods were used to implement the intra-subject and inter-subject cross validation. It demonstrated that our proposed method can achieve a significant performance. The present work showed the usefulness of forehead EEG and eye images that was originally merged for detecting fatigue. It provides a new strategy of using forehead EEG combined with machine vision to design a potential and automatic pipeline for driver fatigue detection."}}
{"id": "1TUQ7OrhfOF", "cdate": 1677628800000, "mdate": 1683165557093, "content": {"title": "Chat With ChatGPT on Intelligent Vehicles: An IEEE TIV Perspective", "abstract": "This letter reports on a TIV DHW (decentralized and hybrid workshop) that explores the prospective influence of ChatGPT on research and development in intelligent vehicles. To assess the update capabilities of ChatGPT, we conducted tests involving both basic and technically relevant questions. Our preliminary testing revealed that ChatGPT's information can be updated and corrected at one time, but it may take some time for the changes to be reflected in ChatGPT's responses, so it may not always possess the latest knowledge regarding specific topics. We further discuss the prospective influence of ChatGPT on the field of intelligent vehicles, particularly possible applications of ChatGPT in areas like autonomous driving, human-vehicle interaction, and intelligent transportation systems, highlighting challenges and opportunities associated with these applications. Additionally, we address technical questions, such as the feasibility of training intelligent vehicles using the same methods as ChatGPT and the reflection of the intelligence of intelligent vehicles in the context of human-machine shared control. In conclusion, this letter presents a preliminary exploration of the potential of ChatGPT for intelligent vehicle research, from an IEEE TIV perspective, acknowledging the limitations and uncertainties of this emerging technology."}}
{"id": "T11DrhrPAL", "cdate": 1672531200000, "mdate": 1682325877221, "content": {"title": "Cascade Learning for Driver Facial Monitoring", "abstract": "As a non-invasive method, vision-based driver monitoring aims to identify risky maneuvers for intelligent vehicles and it has gained an increasing interest over recent years. However, most existing methods tend to design models for specific tasks, such as head pose or gaze estimation, which results in redundant models hampering real time applications. Besides, most driver facial monitoring methods ignore the correlation of different tasks. In this work, we propose a unified framework based on cascade learning for simultaneous facial landmark detection and head pose estimation, as well as simultaneous eye center detection and gaze estimation. In particular, built upon the key idea that facial landmark locations and 3D face model parameters are implicitly correlated, we introduce a cascade regression framework to achieve these two tasks simultaneously. After coarsely extracting the driver\u2019s eye region from the detected facial landmarks, we perform cascade regression for simultaneous eye center detection and gaze estimation. Leveraging the power of cascade learning allows our method to alternatively optimize facial landmark detection, head pose estimation, eye center localization, and gaze prediction. The comparison experiments conducted on benchmark datasets of 300-W, GI4E, BU, MPIIGaze, and driving dataset of SHRP2 demonstrate that our proposed method can achieve state-of-the-art performance with robust effectiveness on the real driver monitoring applications."}}
{"id": "NExGKlfNn4", "cdate": 1672531200000, "mdate": 1682325877221, "content": {"title": "An integrated monitoring scheme for wind turbine main bearing using acoustic emission", "abstract": ""}}
{"id": "HZuzNX6Uxx", "cdate": 1672531200000, "mdate": 1699162911199, "content": {"title": "Learning from Easy to Hard Pairs: Multi-step Reasoning Network for Human-Object Interaction Detection", "abstract": "Human-object interaction (HOI) detection aims to interpret the interactions of human-object pairs. Existing methods adopt a one-step reasoning paradigm that simultaneously outputs multi-label results for all HOI pairs without distinguishing difficulties. However, there are significant variations among HOI pairs in the same image, making their performance degrade in challenging situations. In this paper, we argue that the model should prioritize hard samples after inferring easy ones, and hard samples can benefit from easy ones. To this end, we propose a novel Multi-step Reasoning Network that progressively learns from easy to hard samples. In particular, an Easy-to-Hard Learning Block is introduced to enhance the representation of hard HOI pairs by prior associations. Additionally, we propose a Multi-step Reasoning Probability Transfer mechanism to enhance multi-label interaction classifications, which leverages cognitive associations and semantic dependencies. Extensive experiments demonstrate that our method outperforms other state-of-the-art on two challenging benchmark datasets."}}
{"id": "ypXcBggOpG_", "cdate": 1640995200000, "mdate": 1682325877421, "content": {"title": "An Adaptive Staying Point Recognition Algorithm Based on Spatiotemporal Characteristics Using Cellular Signaling Data", "abstract": "Cellular signaling data (CSD) have attracted unprecedented attention due to their large size, long observation period, and high followability. Before applying CSD, a series of data processing steps are indispensable; among those steps, staying point recognition is the basis for recognizing individual travel states and thus the influence of further application of CSD. Previous work indicates that the existing staying point recognition algorithms have two common aspects. One is the requirement of a fixed spatiotemporal threshold to analyze the user\u2019s travel characteristics. The other is the insufficiency of accuracy assessment, which indicates that further studies are expected owing to the lack of ground truth data in CSD. In this work, a \u201cspatiotemporal window\u201d-based algorithm is proposed to recognize individual staying and moving states. First, an iterative-learning-based model is designed to cluster individual trajectory points without predefined spatiotemporal thresholds. Then, rules to distinguish the staying or moving cluster are made from individual travel characteristics. Moreover, verification work is carried out by collecting volunteers\u2019 ground truth data using our developed smartphone application, which achieves an accuracy of 91.3%. Finally, the results demonstrate the effectiveness and robustness of the algorithm through the performance of comparison and sensitivity analyses."}}
{"id": "wT9a-GE0mT5", "cdate": 1640995200000, "mdate": 1682325877422, "content": {"title": "A Joint Cascaded Framework for Simultaneous Eye State, Eye Center, and Gaze Estimation", "abstract": "Eye tracking technology is widely used in a range of potential interactive applications, containing biometrics recognition, emotion recognition, and virtual reality. Eye tracking includes various tasks, but most existing methods cannot accomplish multiple tasks simultaneously. The related works conduct eye localization first, followed by performing gaze estimation or eye state prediction sequentially. In this paper, we propose a unified method based on cascade regression framework to achieve multi-task of eye detection, eye state prediction, and gaze estimation simultaneously. We hypothesize that there is a correspondence between each task, hence introducing a cascade regression framework to capture the implicit relation. At each iteration, we extract appearance features and shape features from eye region to estimate eye state and gaze direction. Based on previous eye state information and gaze vectors, we further use the cascade regression to map these information to update eye location. The proposed method accomplishes three tasks, namely eye state prediction, gaze estimation, and eye localization, through learning the cascade regression. Experimental results on the benchmarks of BioID, Gi4E, MPIIGaze and UT-Multiview demonstrate that the proposed approach achieves superior performance in the aforementioned three tasks."}}
