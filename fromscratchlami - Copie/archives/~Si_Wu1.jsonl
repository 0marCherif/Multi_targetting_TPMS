{"id": "CqCZvseS198", "cdate": 1683883365926, "mdate": 1683883365926, "content": {"title": "Spiking Continuous Attractor Neural Networks with Spike Frequency Adaptation for Anticipative Tracking", "abstract": "Continuous attractor neural network (CANN) is a canonical model for neural information representation and processing, which has been applied to describe the encoding of continuous features, such as orientation, head direction and spatial location in neural systems. Specifically, theoretical studies based on a firing-rate model have found that a CANN with negative feedback, such as spike frequency adaptation (SFA), has the capability of tracking a continuously moving stimulus anticipatively. In this study, facing the booming development of neuromorphic computing using spiking neural networks (SNNs), we built a spiking continuous attractor neural network (S-CANN) with SFA to implement anticipative tracking. Further, we simplified the model, in terms of connection weights, external inputs, and network size, to facilitate its implementation with neuromorphic hardware."}}
{"id": "ivUZi5tTWF", "cdate": 1683882865228, "mdate": 1683882865228, "content": {"title": "Neural implementation of categorization in a motion discrimination task", "abstract": "Human being can categorize one object into different classes depending on the reference used, a cognitive capacity, i.e., context-dependent categorization, which is fundamental in our daily life. In the present study, we explore one possible neural mechanism underlying a motion discrimination task, in which the neural system needs to judge whether a motion direction embedded in a random dot kinematogram is clockwise or anticlockwise with respect to a reference direction that varies over time. We construct a spiking-neuron network model to implement this task. The model consists of three parts: (1) a working memory circuit, which holds the information of the reference direction; (2) two information extraction circuits, referred to as clockwise-preferred circuit and anticlockwise-preferred circuit, respectively, which extract either the clockwise or anticlockwise information about the test direction; and (3) a decision-making circuit, which reads out the category decision. At the core of the network is the assumption of an asymmetric offset and rotational invariance of the connectivity profile. Our model successfully implements the context-dependent categorization of motion direction where the reference varies over time. And it reproduces the experimental results that with higher similarity between the reference and test direction or lower coherence level of the random dot kinematogram, the performance gets worse (lower accuracy and longer reaction time)."}}
{"id": "_vfyuJaXFug", "cdate": 1652737436006, "mdate": null, "content": {"title": "Translation-equivariant Representation in Recurrent Networks with a Continuous Manifold of Attractors", "abstract": "Equivariant representation is necessary for the brain and artificial perceptual systems to faithfully represent the stimulus under some (Lie) group transformations. However, it remains unknown how recurrent neural circuits in the brain represent the stimulus equivariantly, nor the neural representation of abstract group operators. The present study uses a one-dimensional (1D) translation group as an example to explore the general recurrent neural circuit mechanism of the equivariant stimulus representation. We found that a continuous attractor network (CAN), a canonical neural circuit model, self-consistently generates a continuous family of stationary population responses (attractors) that represents the stimulus equivariantly. Inspired by the Drosophila's compass circuit, we found that the 1D translation operators can be represented by extra speed neurons besides the CAN, where speed neurons' responses represent the moving speed (1D translation group parameter), and their feedback connections to the CAN represent the translation generator (Lie algebra). We demonstrated that the network responses are consistent with experimental data. Our model for the first time demonstrates how recurrent neural circuitry in the brain achieves equivariant stimulus representation."}}
{"id": "WOuGTb9QswS", "cdate": 1652737412568, "mdate": null, "content": {"title": "Oscillatory Tracking of Continuous Attractor Neural Networks Account for Phase Precession and Procession of Hippocampal Place Cells", "abstract": "Hippocampal place cells of freely moving rodents display an intriguing temporal organization in their responses known as `theta phase precession', in which individual neurons fire at progressively earlier phases in successive theta cycles as the animal traverses the place fields. Recent experimental studies found that in addition to phase precession, many place cells also exhibit accompanied phase procession, but the underlying neural mechanism remains unclear. Here, we propose a neural circuit model to elucidate the generation of both kinds of phase shift in place cells' firing. Specifically, we consider a continuous attractor neural network (CANN) with feedback inhibition, which is inspired by the reciprocal interaction between the hippocampus and the medial septum. The feedback inhibition induces intrinsic mobility of the CANN which competes with the extrinsic mobility arising from the external drive. Their interplay generates an oscillatory tracking state, that is, the network bump state (resembling the decoded virtual position of the animal) sweeps back and forth around the external moving input (resembling the physical position of the animal). We show that this oscillatory tracking naturally explains the forward and backward sweeps of the decoded position during the animal's locomotion.  At the single neuron level, the forward and backward sweeps account for, respectively, theta phase precession and procession. Furthermore, by tuning the feedback inhibition strength, we also explain the emergence of bimodal cells and unimodal cells, with the former having co-existed phase precession and procession, and the latter having only significant phase precession. We hope that this study facilitates our understanding of hippocampal temporal coding and lays foundation for unveiling their computational functions."}}
{"id": "Y0Bm5tL92lg", "cdate": 1652737412141, "mdate": null, "content": {"title": "Adaptation Accelerating Sampling-based Bayesian Inference in Attractor Neural Networks", "abstract": "The brain performs probabilistic Bayesian inference to interpret the external world. The sampling-based view assumes that the brain represents the stimulus posterior distribution via samples of stochastic neuronal responses. Although the idea of sampling-based inference is appealing, it faces a critical challenge of whether stochastic sampling is fast enough to match the rapid computation of the brain. In this study, we explore how latent stimulus sampling can be accelerated in neural circuits. Specifically, we consider a canonical neural circuit model called continuous attractor neural networks (CANNs) and investigate how sampling-based inference of latent continuous variables is accelerated in CANNs. Intriguingly, we find that by including noisy adaptation in the neuronal dynamics, the CANN is able to speed up the sampling process significantly. We theoretically derive that the CANN with noisy adaptation implements the efficient sampling method called Hamiltonian dynamics with friction, where noisy adaption effectively plays the role of momentum. We theoretically analyze the sampling performances of the network and derive the condition when the acceleration has the maximum effect. Simulation results confirm our theoretical analyses. We further extend the model to coupled CANNs and demonstrate that noisy adaptation accelerates the sampling of the posterior distribution of multivariate stimuli. We hope that this study enhances our understanding of how Bayesian inference is realized in the brain."}}
{"id": "H4gx0N87Sgq", "cdate": 1645717046421, "mdate": 1645717046421, "content": {"title": "Sampling-based Bayesian inference in recurrent circuits of stochastic spiking neurons", "abstract": "Two facts about cortex are widely accepted: neuronal responses show large spiking variability with near Poisson statistics and cortical circuits feature abundant recurrent connections between neurons. How these spiking and circuit properties combine to support sensory representation and information processing is not well understood. We build a theoretical framework showing that these two ubiquitous features of cortex combine to produce optimal sampling-based Bayesian inference. Recurrent connections store an internal model of the external world, and Poissonian variability of spike responses drives flexible sampling from the posterior stimulus distributions obtained by combining feedforward and recurrent neuronal inputs. We illustrate how this framework for sampling-based inference can be used by cortex to represent latent multivariate stimuli organized either hierarchically or in parallel. A neural signature of such network sampling are internally generated differential correlations whose amplitude is determined by the prior stored in the circuit, which provides an experimentally testable prediction for our framework."}}
{"id": "gmO2W_wEK4n", "cdate": 1640995200000, "mdate": 1683881087401, "content": {"title": "Oscillatory Tracking of Continuous Attractor Neural Networks Account for Phase Precession and Procession of Hippocampal Place Cells", "abstract": "Hippocampal place cells of freely moving rodents display an intriguing temporal organization in their responses known as `theta phase precession', in which individual neurons fire at progressively earlier phases in successive theta cycles as the animal traverses the place fields. Recent experimental studies found that in addition to phase precession, many place cells also exhibit accompanied phase procession, but the underlying neural mechanism remains unclear. Here, we propose a neural circuit model to elucidate the generation of both kinds of phase shift in place cells' firing. Specifically, we consider a continuous attractor neural network (CANN) with feedback inhibition, which is inspired by the reciprocal interaction between the hippocampus and the medial septum. The feedback inhibition induces intrinsic mobility of the CANN which competes with the extrinsic mobility arising from the external drive. Their interplay generates an oscillatory tracking state, that is, the network bump state (resembling the decoded virtual position of the animal) sweeps back and forth around the external moving input (resembling the physical position of the animal). We show that this oscillatory tracking naturally explains the forward and backward sweeps of the decoded position during the animal's locomotion. At the single neuron level, the forward and backward sweeps account for, respectively, theta phase precession and procession. Furthermore, by tuning the feedback inhibition strength, we also explain the emergence of bimodal cells and unimodal cells, with the former having co-existed phase precession and procession, and the latter having only significant phase precession. We hope that this study facilitates our understanding of hippocampal temporal coding and lays foundation for unveiling their computational functions."}}
{"id": "_FenXVHTLO7", "cdate": 1640995200000, "mdate": 1683882911258, "content": {"title": "Towards a New Paradigm for Brain-inspired Computer Vision", "abstract": "Brain-inspired computer vision aims to learn from biological systems to develop advanced image processing techniques. However, its progress so far is not impressing. We recognize that a main obstacle comes from that the current paradigm for brain-inspired computer vision has not captured the fundamental nature of biological vision, i.e., the biological vision is targeted for processing spatio-temporal patterns. Recently, a new paradigm for developing brain-inspired computer vision is emerging, which emphasizes on the spatio-temporal nature of visual signals and the brain-inspired models for processing this type of data. In this paper, we review some recent primary works towards this new paradigm, including the development of spike cameras which acquire spiking signals directly from visual scenes, and the development of computational models learned from neural systems that are specialized to process spatio-temporal patterns, including models for object detection, tracking, and recognition. We also discuss about the future directions to improve the paradigm."}}
{"id": "9ENFonmMPsD", "cdate": 1640995200000, "mdate": 1683882911166, "content": {"title": "Neural feedback facilitates rough-to-fine information retrieval", "abstract": ""}}
{"id": "6oYWQUZR5uY", "cdate": 1640995200000, "mdate": 1683882911173, "content": {"title": "Adaptation Accelerating Sampling-based Bayesian Inference in Attractor Neural Networks", "abstract": "The brain performs probabilistic Bayesian inference to interpret the external world. The sampling-based view assumes that the brain represents the stimulus posterior distribution via samples of stochastic neuronal responses. Although the idea of sampling-based inference is appealing, it faces a critical challenge of whether stochastic sampling is fast enough to match the rapid computation of the brain. In this study, we explore how latent stimulus sampling can be accelerated in neural circuits. Specifically, we consider a canonical neural circuit model called continuous attractor neural networks (CANNs) and investigate how sampling-based inference of latent continuous variables is accelerated in CANNs. Intriguingly, we find that by including noisy adaptation in the neuronal dynamics, the CANN is able to speed up the sampling process significantly. We theoretically derive that the CANN with noisy adaptation implements the efficient sampling method called Hamiltonian dynamics with friction, where noisy adaption effectively plays the role of momentum. We theoretically analyze the sampling performances of the network and derive the condition when the acceleration has the maximum effect. Simulation results confirm our theoretical analyses. We further extend the model to coupled CANNs and demonstrate that noisy adaptation accelerates the sampling of the posterior distribution of multivariate stimuli. We hope that this study enhances our understanding of how Bayesian inference is realized in the brain."}}
