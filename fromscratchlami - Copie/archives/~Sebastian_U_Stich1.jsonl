{"id": "05g7mnKJyJ7", "cdate": 1683910859733, "mdate": 1683910859733, "content": {"title": "Is Local SGD Better than Minibatch SGD?", "abstract": "We study local SGD (also known as parallel SGD and federated averaging), a natural and frequently\nused stochastic distributed optimization method. Its theoretical foundations are currently lacking and\nwe highlight how all existing error guarantees in the convex setting are dominated by a simple baseline,\nminibatch SGD. (1) For quadratic objectives we prove that local SGD strictly dominates minibatch SGD\nand that accelerated local SGD is minimax optimal for quadratics; (2) For general convex objectives we\nprovide the first guarantee that at least sometimes improves over minibatch SGD; (3) We show that\nindeed local SGD does not dominate minibatch SGD by presenting a lower bound on the performance\nof local SGD that is worse than the minibatch SGD guarantee."}}
{"id": "Oo5FwBc7fa9", "cdate": 1672531200000, "mdate": 1681925567675, "content": {"title": "Decentralized Gradient Tracking with Local Steps", "abstract": "Gradient tracking (GT) is an algorithm designed for solving decentralized optimization problems over a network (such as training a machine learning model). A key feature of GT is a tracking mechanism that allows to overcome data heterogeneity between nodes. We develop a novel decentralized tracking mechanism, $K$-GT, that enables communication-efficient local updates in GT while inheriting the data-independence property of GT. We prove a convergence rate for $K$-GT on smooth non-convex functions and prove that it reduces the communication overhead asymptotically by a linear factor $K$, where $K$ denotes the number of local steps. We illustrate the robustness and effectiveness of this heterogeneity correction on convex and non-convex benchmark problems and on a non-convex neural network training task with the MNIST dataset."}}
{"id": "Cd1Og0zeLf", "cdate": 1672531200000, "mdate": 1681925567008, "content": {"title": "Stochastic distributed learning with gradient quantization and double-variance reduction", "abstract": "We consider distributed optimization over several devices, each sending incremental model updates to a central server. This setting is considered, for instance, in federated learning. Various schem..."}}
{"id": "zc0GcYXS6Q", "cdate": 1668534972544, "mdate": 1668534972544, "content": {"title": "ProgFed: Effective, Communication, and Computation Efficient Federated Learning by Progressive Training", "abstract": "Federated learning is a powerful distributed learning scheme that allows numerous edge devices to collaboratively train a model without sharing their data. However, training is resource-intensive for edge devices, and limited network bandwidth is often the main bottleneck. Prior work often overcomes the constraints by condensing the models or messages into compact formats, e.g., by gradient compression or distillation. In contrast, we propose ProgFed, the first progressive training framework for efficient and effective federated learning. It inherently reduces computation and two-way communication costs while maintaining the strong performance of the final models. We theoretically prove that ProgFed converges at the same asymptotic rate as standard training on full models. Extensive results on a broad range of architectures, including CNNs (VGG, ResNet, ConvNets) and U-nets, and diverse tasks from simple classification to medical image segmentation show that our highly effective training approach saves up to 20% computation and up to 63% communication costs for converged models. As our approach is also complimentary to prior work on compression, we can achieve a wide range of trade-offs by combining these techniques, showing reduced communication of up to 50\u00d7 at only 0.1% loss in utility. Code is available at https://github.com/a514514772/ProgFed."}}
{"id": "NvlUqen8Cya", "cdate": 1664928783671, "mdate": null, "content": {"title": "Preserving privacy with PATE for heterogeneous data", "abstract": "Differential privacy has become the standard system to provide privacy guarantees for user data in machine learning models. One of the popular techniques to ensure  privacy is the Private Aggregation of Teacher Ensembles (PATE) framework. PATE trains an ensemble of teacher models on private data and transfers the knowledge to a student model, with rigorous privacy guarantees derived using differential privacy. So far, PATE has been shown to work assuming the public and private data are distributed homogeneously. We show that in the case of high mismatch (non iid-ness) in these distributions, the teachers suffer from high variance in their individual training updates, causing them to converge to vastly different optimum states. This leads to lower consensus and accuracy for data labelling. To address this, we propose a modification to the teacher training process in PATE, that incorporates teacher averaging and update correction which reduces the variance in teacher updates. Our technique leads to improved  prediction accuracy of the teacher aggregation mechanism, especially for highly heterogeneous data. Furthermore, our evaluation shows our technique is necessary to sustain the student model performance, and allows it to achieve considerable gains over the original PATE in the utility-privacy metric."}}
{"id": "Quz3n455QZt", "cdate": 1664731454554, "mdate": null, "content": {"title": "Data-heterogeneity-aware Mixing for Decentralized Learning", "abstract": "Decentralized learning provides an effective framework to train machine learning models with data distributed over arbitrary communication graphs. However, most existing approaches towards decentralized learning disregard the interaction between data heterogeneity and graph topology. In this paper, we characterize the dependence of convergence on the relationship between the mixing weights of the graph and the data heterogeneity across nodes. We propose a metric that quantifies the ability of a graph to mix the current gradients. We further prove that the metric controls the convergence rate, particularly in settings where the heterogeneity across nodes dominates the stochasticity between updates for a given node. Motivated by our analysis, we propose an approach that periodically and efficiently optimizes the metric using standard convex constrained optimization and sketching techniques. "}}
{"id": "zTdpArkX84p", "cdate": 1664731453464, "mdate": null, "content": {"title": "Bidirectional Adaptive Communication for Heterogeneous Distributed Learning", "abstract": "Communication is a key bottleneck in distributed optimization, and, in particular, bandwidth and latency can be limiting factors when devices are connected over commodity networks, such as in Federated Learning. State-of-the-art techniques tackle these challenges by advanced compression techniques or delaying communication rounds according to predefined schedules. We present a new scheme that adaptively skips communication (broadcast and client uploads) by detecting slow-varying updates. The scheme automatically adjusts the communication frequency independently for each worker and the server. By utilizing an error-feedback mechanism~-- borrowed from the compression literature~--~we prove that the convergence rate is the same as for batch gradient descent %strongly-convex,  in the convex and nonconvex smooth cases. We show that the total number of communication rounds between server and clients needed to achieve a targeted accuracy is reduced, even in the case when the data distribution is highly non-IID."}}
{"id": "JsrvkgM8gO2", "cdate": 1663850184673, "mdate": null, "content": {"title": "Large Learning Rate Matters for Non-Convex Optimization", "abstract": "When training neural networks, it has been widely observed that a large step size is essential in stochastic gradient descent (SGD) for obtaining superior models. However, the effect of large step sizes on the success of SGD is not well understood theoretically. \nSeveral previous works have attributed this success to the stochastic noise present in SGD.  However, we show through a novel set of experiments that the stochastic noise is not sufficient to explain good non-convex training, and that instead the effect of a large learning rate itself is essential for obtaining best performance.\nWe demonstrate the same effects also in the noise-less case, i.e. for full-batch GD. We formally prove that GD with large step size---on certain non-convex function classes---follows a different trajectory than GD with a small step size, which can lead to convergence to a global minimum instead of a local one. \nFinally, we also demonstrate the difference in trajectories for small and large learning rates for real neural networks, again observing that large learning rates allow escaping from a local minimum, confirming this behavior is indeed relevant in practice."}}
{"id": "4_oCZgBIVI", "cdate": 1652737721975, "mdate": null, "content": {"title": "Sharper Convergence Guarantees for Asynchronous SGD for Distributed and Federated Learning", "abstract": "We study the asynchronous stochastic gradient descent algorithm, for distributed training over $n$ workers that might be heterogeneous. In this algorithm, workers compute stochastic gradients in parallel at their own pace and return them to the server without any synchronization.\n\nExisting convergence rates of this algorithm for non-convex smooth objectives depend on the maximum delay $\\tau_{\\max}$ and reach an $\\epsilon$-stationary point after $O\\!\\left(\\sigma^2\\epsilon^{-2}+ \\tau_{\\max}\\epsilon^{-1}\\right)$ iterations,  where $\\sigma$ is the variance of stochastic gradients. In this work (i) we obtain a tighter convergence rate of $O\\!\\left(\\sigma^2\\epsilon^{-2}+ \\sqrt{\\tau_{\\max}\\tau_{avg}}\\epsilon^{-1}\\right)$ *without any change in the algorithm* where $\\tau_{avg}$ is the average delay, which can be significantly smaller than $\\tau_{\\max}$. We also provide (ii) a simple delay-adaptive learning rate scheme, under which asynchronous SGD achieves a convergence rate of $O\\!\\left(\\sigma^2\\epsilon^{-2}+ \\tau_{avg}\\epsilon^{-1}\\right)$, and does not require any extra hyperparameter tuning nor extra communications. Our result allows to show *for the first time* that asynchronous SGD is *always faster* than mini-batch SGD. In addition, (iii) we consider the case of heterogeneous functions motivated by federated learning applications and improve the convergence rate by proving a weaker dependence on the maximum delay compared to prior works."}}
{"id": "Y4vT7m4e3d", "cdate": 1652737337903, "mdate": null, "content": {"title": "Decentralized Local Stochastic Extra-Gradient for Variational Inequalities", "abstract": "We consider distributed stochastic variational inequalities (VIs) on unbounded domains with the problem data that is heterogeneous (non-IID) and distributed across many devices. We make a very general assumption on the computational network that, in particular, covers the settings of fully decentralized calculations with time-varying networks and centralized topologies commonly used in Federated Learning. Moreover, multiple local updates on the workers can be made for reducing the communication frequency between the workers.\nWe extend the stochastic extragradient method to this very general setting and theoretically analyze its convergence rate in the strongly-monotone, monotone, and non-monotone (when a Minty solution exists) settings. The provided rates explicitly exhibit the dependence on network characteristics (e.g., mixing time), iteration counter, data heterogeneity, variance, number of devices, and other standard parameters. As a special case, our method and analysis apply to distributed stochastic saddle-point problems (SPP), e.g., to the training of Deep Generative Adversarial Networks (GANs) for which decentralized training has been reported to be extremely challenging. In experiments for the decentralized training of GANs we demonstrate the effectiveness of our proposed approach."}}
