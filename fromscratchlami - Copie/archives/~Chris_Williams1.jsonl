{"id": "lV8OAj6xl9", "cdate": 1680797013943, "mdate": 1680797013943, "content": {"title": "Learning Direct Optimization for Scene Understanding ", "abstract": "We develop a Learning Direct Optimization (LiDO) method for the refinement of a latent variable model that describes input image x. Our goal is to explain a single image x with an interpretable 3D computer graphics model having scene graph latent variables z (such as object appearance, camera position). Given a current estimate of z we can render a prediction of the image g(z), which can be compared to the image x. The standard way to proceed is then to measure the error E(x,g(z)) between the two, and use an optimizer to minimize the error. However, it is unknown which error measure E would be most effective for simultaneously addressing issues such as misaligned objects, occlusions, textures, etc. In contrast, the LiDO approach trains a Prediction Network to predict an update directly to correct z, rather than minimizing the error with respect to z.\nExperiments show that LiDO converges rapidly as it does not need to perform a search on the error landscape, produces better solutions than error-based competitors, and is able to handle the mismatch between the data and the fitted scene model. We apply LiDO to a realistic synthetic dataset, and show that the method also transfers to work well with real images."}}
{"id": "nyKwIy2M3hD", "cdate": 1680796892957, "mdate": 1680796892957, "content": {"title": "The Effect of Class Imbalance on Precision-Recall Curves", "abstract": "In this note I study how the precision of a binary classifier depends on the ratio r of positive to negative cases in the test set, as well as the classifier\u2019s true and false positive rates. This relationship allows prediction of how the precision-recall curve will change with r, which seems not to be well known. It also allows prediction of how F\u03b2 and the Precision Gain and Recall Gain measures of Flach and Kull (2015) vary with r."}}
{"id": "lL_3JLvbc7", "cdate": 1680796752736, "mdate": 1680796752736, "content": {"title": "On Suspicious Coincidences and Pointwise Mutual Information", "abstract": "Barlow (1985) hypothesized that the co-occurrence of two events A and B is \u2018suspicious\u2019 if P (A, B) \u226b P (A)P (B). We first review classical measures of association for 2 \u00d7 2 contingency tables, including Yule\u2019s Y (Yule, 1912), which depends only on the odds ratio \u03bb, and is indepen- dent of the marginal probabilities of the table. We then discuss the mutual information (MI) and pointwise mutual information (PMI), which depend on the ratio P (A, B)/P (A)P (B), as mea- sures of association. We show that, once the effect of the marginals is removed, MI and PMI behave similarly to Y as functions of \u03bb. The pointwise mutual information is used extensively in some research communities for flagging suspicious coincidences. We discuss the pros and cons of using it in this way, bearing in mind the sensitivity of the PMI to the marginals, with increased scores for sparser events."}}
{"id": "idwDbkU8TQ", "cdate": 1680796686264, "mdate": 1680796686264, "content": {"title": "Inference and Learning for Generative Capsule Models", "abstract": "Capsule networks (see e.g. Hinton et al., 2018) aim to encode knowledge of and reason about the relationship between an object and its parts. In this paper we specify a generative model for such data, and derive a variational algorithm for inferring the transformation of each model object in a scene, and the assignments of observed parts to the objects. We derive a learning algorithm for the object models, based on variational expectation maximization (Jordan et al., 1999). We also study an alternative inference algorithm based on the RANSAC method of Fischler and Bolles (1981). We apply these inference methods to (i) data generated from multiple geometric objects like squares and triangles (\u201cconstellations\u201d), and (ii) data from a parts-based model of faces. Recent work by Kosiorek et al. (2019) has used amortized inference via stacked capsule autoencoders (SCAEs) to tackle this problem\u2014our results show that we significantly outperform them where we can make comparisons (on the constellations data)."}}
{"id": "ot_aEDRm9h", "cdate": 1680796577927, "mdate": 1680796577927, "content": {"title": "Automating Data Science", "abstract": "Given the complexity of typical data science projects and the associated demand for human expertise, automation has the potential to transform the data science process."}}
{"id": "z55LwjGWLt", "cdate": 1672531200000, "mdate": 1681651418447, "content": {"title": "Inference and Learning for Generative Capsule Models", "abstract": ""}}
{"id": "H8mUISOIqec", "cdate": 1646057534034, "mdate": null, "content": {"title": "Align-Deform-Subtract: An interventional framework for explaining object differences", "abstract": "Given two object images, how can we explain their differences in terms of the underlying object properties? To address this question, we propose Align-Deform-Subtract (ADS)---an interventional framework for explaining object differences. By leveraging semantic alignments in image-space as counterfactual interventions on the underlying object properties, ADS iteratively quantifies and removes differences in object properties. The result is a set of \"disentangled\" error measures which explain object differences in terms of their underlying properties. Experiments on real and synthetic data illustrate the efficacy of the framework."}}
{"id": "EfypkM4Wcl", "cdate": 1640995200000, "mdate": 1681651418445, "content": {"title": "Source-Free Adaptation to Measurement Shift via Bottom-Up Feature Restoration", "abstract": ""}}
{"id": "4-k3z962h29", "cdate": 1640995200000, "mdate": 1652872497305, "content": {"title": "Automating data science", "abstract": "Given the complexity of data science projects and related demand for human expertise, automation has the potential to transform the data science process."}}
{"id": "1JDiK_TbV4S", "cdate": 1632875459989, "mdate": null, "content": {"title": "Source-Free Adaptation to Measurement Shift via Bottom-Up Feature Restoration", "abstract": "Source-free domain adaptation (SFDA) aims to adapt a model trained on labelled data in a source domain to unlabelled data in a target domain without access to the source-domain data during adaptation. Existing methods for SFDA leverage entropy-minimization techniques which: (i) apply only to classification; (ii) destroy model calibration; and (iii) rely on the source model achieving a good level of feature-space class-separation in the target domain. We address these issues for a particularly pervasive type of domain shift called measurement shift which can be resolved by restoring the source features rather than extracting new ones. In particular, we propose Feature Restoration (FR) wherein we: (i) store a lightweight and flexible approximation of the feature distribution under the source data; and (ii) adapt the feature-extractor such that the approximate feature distribution under the target data realigns with that saved on the source. We additionally propose a bottom-up training scheme which boosts performance, which we call Bottom-Up Feature Restoration (BUFR). On real and synthetic data, we demonstrate that BUFR outperforms existing SFDA methods in terms of accuracy, calibration, and data efficiency, while being less reliant on the performance of the source model in the target domain.\n"}}
