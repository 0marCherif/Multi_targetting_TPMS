{"id": "Jtwem5nz-mz", "cdate": 1676827110258, "mdate": null, "content": {"title": "Incentivizing honest performative predictions with proper scoring rules", "abstract": "Proper scoring rules incentivize experts to accurately report beliefs, assuming predictions cannot influence outcomes. We relax this assumption and investigate incentives when predictions are performative, i.e., when they can influence the outcome of the prediction, such as when making public predictions about the stock market. We say a prediction is a fixed point if it accurately reflects the expert's beliefs after that prediction has been made. We show that in this setting, reports maximizing expected score generally do not reflect an expert's beliefs, and we give bounds on the inaccuracy of such reports. We show that, for binary predictions, if the influence of the expert's prediction on outcomes is bounded, it is possible to define scoring rules under which optimal reports are arbitrarily close to fixed points. However, this is impossible for predictions over more than two outcomes. We also perform numerical simulations in a toy setting, showing that our bounds are tight in some situations and that prediction error is often substantial (greater than 5-10\\%). Lastly, we discuss alternative notions of optimality, including performative stability, and show that they incentivize reporting fixed points."}}
{"id": "r0LQFDOwfbU", "cdate": 1663850490800, "mdate": null, "content": {"title": "Similarity-Based Cooperation", "abstract": "As ML agents act more autonomously in the world, they will increasingly interact with each other. Unfortunately, in many social dilemmas like the one-shot Prisoner\u2019s Dilemma, standard game theory predicts that ML agents will fail to cooperate with each other. Prior work has shown that one way to enable cooperative outcomes in the one-shot Prisoner\u2019s Dilemma is to make the agents mutually transparent to each other, i.e., to allow them to access one another\u2019s source code (Rubinstein 1997; Tennenholtz 2004) or weights in the case of ML agents. However, full transparency is often unrealistic, whereas partial transparency is commonplace. Moreover, it is difficult to machine-learn to cooperate in the full transparency setting. In this paper, we introduce a more realistic setting in which agents only observe a single number indicating how similar they are to each other. We prove that this allows for the same set of cooperative outcomes as the full transparency setting. We also demonstrate experimentally that cooperation can be learned using simple ML methods."}}
{"id": "t0l_4_r0J0X", "cdate": 1640995200000, "mdate": 1671986214615, "content": {"title": "A Note on the Compatibility of Different Robust Program Equilibria of the Prisoner's Dilemma", "abstract": "We study a program game version of the Prisoner's Dilemma, i.e., a two-player game in which each player submits a computer program, the programs are given read access to each other's source code and then choose whether to cooperate or defect. Prior work has introduced various programs that form cooperative equilibria against themselves in this game. For example, the $\\epsilon$-grounded Fair Bot cooperates with probability $\\epsilon$ and with the remaining probability runs its opponent's program and copies its action. If both players submit this program, then this is a Nash equilibrium in which both players cooperate. Others have proposed cooperative equilibria based on proof-based Fair Bots, which cooperate if they can prove that the opponent cooperates (and defect otherwise). We here show that these different programs are compatible with each other. For example, if one player submits $\\epsilon$-grounded Fair Bot and the other submits a proof-based Fair Bot, then this is also a cooperative equilibrium of the program game version of the Prisoner's Dilemma."}}
{"id": "gmBH63XDJN", "cdate": 1640995200000, "mdate": 1671986214508, "content": {"title": "For Learning in Symmetric Teams, Local Optima are Global Nash Equilibria", "abstract": "Although it has been known since the 1970s that a globally optimal strategy profile in a common-payoff game is a Nash equilibrium, global optimality is a strict requirement that limits the result's applicability. In this work, we show that any locally optimal symmetric strategy profile is also a (global) Nash equilibrium. Furthermore, we show that this result is robust to perturbations to the common payoff and to the local optimum. Applied to machine learning, our result provides a global guarantee for any gradient method that finds a local optimum in symmetric strategy space. While this result indicates stability to unilateral deviation, we nevertheless identify broad classes of games where mixed local optima are unstable under joint, asymmetric deviations. We analyze the prevalence of instability by running learning algorithms in a suite of symmetric games, and we conclude by discussing the applicability of our results to multi-agent RL, cooperative inverse RL, and decentralized POMDPs."}}
{"id": "AzE3UU2OKy", "cdate": 1640995200000, "mdate": 1671986214497, "content": {"title": "Similarity-based Cooperation", "abstract": "As machine learning agents act more autonomously in the world, they will increasingly interact with each other. Unfortunately, in many social dilemmas like the one-shot Prisoner's Dilemma, standard game theory predicts that ML agents will fail to cooperate with each other. Prior work has shown that one way to enable cooperative outcomes in the one-shot Prisoner's Dilemma is to make the agents mutually transparent to each other, i.e., to allow them to access one another's source code (Rubinstein 1998, Tennenholtz 2004) -- or weights in the case of ML agents. However, full transparency is often unrealistic, whereas partial transparency is commonplace. Moreover, it is challenging for agents to learn their way to cooperation in the full transparency setting. In this paper, we introduce a more realistic setting in which agents only observe a single number indicating how similar they are to each other. We prove that this allows for the same set of cooperative outcomes as the full transparency setting. We also demonstrate experimentally that cooperation can be learned using simple ML methods."}}
{"id": "4juJUeBaR8", "cdate": 1640995200000, "mdate": 1671986214496, "content": {"title": "For Learning in Symmetric Teams, Local Optima are Global Nash Equilibria", "abstract": "Although it has been known since the 1970s that a <em>globally</em> optimal strategy profile in a common-payoff game is a Nash equilibrium, global optimality is a strict requirement that limits the..."}}
{"id": "cx2q4cOBnne", "cdate": 1621630060081, "mdate": null, "content": {"title": "Reinforcement Learning in Newcomblike Environments", "abstract": "Newcomblike decision problems have been studied extensively in the decision theory literature, but they have so far been largely absent in the reinforcement learning literature. In this paper we study value-based reinforcement learning algorithms in the Newcomblike setting, and answer some of the fundamental theoretical questions about the behaviour of such algorithms in these environments. We show that a value-based reinforcement learning agent cannot converge to a policy that is not \\emph{ratifiable}, i.e., does not only choose actions that are optimal given that policy. This gives us a powerful tool for reasoning about the limit behaviour of agents -- for example, it lets us show that there are Newcomblike environments in which a reinforcement learning agent cannot converge to any optimal policy. We show that a ratifiable policy always exists in our setting, but that there are cases in which a reinforcement learning agent normally cannot converge to it (and hence cannot converge at all). We also prove several results about the possible limit behaviours of agents in cases where they do not converge to any policy."}}
{"id": "abzlOMd92o", "cdate": 1609459200000, "mdate": 1671986214499, "content": {"title": "A New Formalism, Method and Open Issues for Zero-Shot Coordination", "abstract": "In many coordination problems, independently reasoning humans are able to discover mutually compatible policies. In contrast, independently trained self-play policies are often mutually incompatibl..."}}
{"id": "WGmk9bTzNbp", "cdate": 1609459200000, "mdate": 1671986214617, "content": {"title": "Approval-directed agency and the decision theory of Newcomb-like problems", "abstract": "Decision theorists disagree about how instrumentally rational agents, i.e., agents trying to achieve some goal, should behave in so-called Newcomb-like problems, with the main contenders being causal and evidential decision theory. Since the main goal of artificial intelligence research is to create machines that make instrumentally rational decisions, the disagreement pertains to this field. In addition to the more philosophical question of what the right decision theory is, the goal of AI poses the question of how to implement any given decision theory in an AI. For example, how would one go about building an AI whose behavior matches evidential decision theory\u2019s recommendations? Conversely, we can ask which decision theories (if any) describe the behavior of any existing AI design. In this paper, we study what decision theory an approval-directed agent, i.e., an agent whose goal it is to maximize the score it receives from an overseer, implements. If we assume that the overseer rewards the agent based on the expected value of some von Neumann\u2013Morgenstern utility function, then such an approval-directed agent is guided by two decision theories: the one used by the agent to decide which action to choose in order to maximize the reward and the one used by the overseer to compute the expected utility of a chosen action. We show which of these two decision theories describes the agent\u2019s behavior in which situations."}}
{"id": "ESzLbD1J50", "cdate": 1609459200000, "mdate": 1671986214618, "content": {"title": "Safe Pareto Improvements for Delegated Game Playing", "abstract": ""}}
