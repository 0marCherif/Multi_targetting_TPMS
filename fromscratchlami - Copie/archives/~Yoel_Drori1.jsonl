{"id": "vyqq58GRMa", "cdate": 1672531200000, "mdate": 1681829528077, "content": {"title": "Improving Training Stability for Multitask Ranking Models in Recommender Systems", "abstract": "Recommender systems play an important role in many content platforms. While most recommendation research is dedicated to designing better models to improve user experience, we found that research on stabilizing the training for such models is severely under-explored. As recommendation models become larger and more sophisticated, they are more susceptible to training instability issues, i.e., loss divergence, which can make the model unusable, waste significant resources and block model developments. In this paper, we share our findings and best practices we learned for improving the training stability of a real-world multitask ranking model for YouTube recommendations. We show some properties of the model that lead to unstable training and conjecture on the causes. Furthermore, based on our observations of training dynamics near the point of training instability, we hypothesize why existing solutions would fail, and propose a new algorithm to mitigate the limitations of existing solutions. Our experiments on YouTube production dataset show the proposed algorithm can significantly improve training stability while not compromising convergence, comparing with several commonly used baseline methods."}}
{"id": "yVvjde0X6ij", "cdate": 1640995200000, "mdate": 1681829528082, "content": {"title": "On the oracle complexity of smooth strongly convex minimization", "abstract": ""}}
{"id": "5tSmnxXb0cx", "cdate": 1621629982724, "mdate": null, "content": {"title": "Asynchronous Stochastic Optimization Robust to Arbitrary Delays", "abstract": "We consider the problem of stochastic optimization with delayed gradients in which, at each time step $t$, the algorithm makes an update using a stale stochastic gradient from step $t - d_t$ for some arbitrary delay $d_t$.   This setting abstracts asynchronous distributed optimization where a central server receives gradient updates computed by worker machines. These machines can experience computation and communication loads that might vary significantly over time.   In the general non-convex smooth optimization setting, we give a simple and efficient algorithm that requires $O( \\sigma^2/\\epsilon^4 + \\tau/\\epsilon^2 )$ steps for finding an $\\epsilon$-stationary point $x$.   Here, $\\tau$ is the \\emph{average} delay $\\frac{1}{T}\\sum_{t=1}^T d_t$ and $\\sigma^2$ is the variance of the stochastic gradients.   This improves over previous work, which showed that stochastic gradient decent achieves the same rate but with respect to the \\emph{maximal} delay $\\max_{t} d_t$, that can be significantly larger than the average delay especially in heterogeneous distributed systems.   Our experiments demonstrate the efficacy and robustness of our algorithm in cases where the delay distribution is skewed or heavy-tailed."}}
{"id": "kKgumxsfy-", "cdate": 1609459200000, "mdate": 1681829528069, "content": {"title": "Asynchronous Stochastic Optimization Robust to Arbitrary Delays", "abstract": "We consider the problem of stochastic optimization with delayed gradients in which, at each time step $t$, the algorithm makes an update using a stale stochastic gradient from step $t - d_t$ for some arbitrary delay $d_t$. This setting abstracts asynchronous distributed optimization where a central server receives gradient updates computed by worker machines. These machines can experience computation and communication loads that might vary significantly over time. In the general non-convex smooth optimization setting, we give a simple and efficient algorithm that requires $O( \\sigma^2/\\epsilon^4 + \\tau/\\epsilon^2 )$ steps for finding an $\\epsilon$-stationary point $x$. Here, $\\tau$ is the \\emph{average} delay $\\frac{1}{T}\\sum_{t=1}^T d_t$ and $\\sigma^2$ is the variance of the stochastic gradients. This improves over previous work, which showed that stochastic gradient decent achieves the same rate but with respect to the \\emph{maximal} delay $\\max_{t} d_t$, that can be significantly larger than the average delay especially in heterogeneous distributed systems. Our experiments demonstrate the efficacy and robustness of our algorithm in cases where the delay distribution is skewed or heavy-tailed."}}
{"id": "EjBNyWJlSZJ", "cdate": 1609459200000, "mdate": 1681829528080, "content": {"title": "An optimal gradient method for smooth (possibly strongly) convex minimization", "abstract": "We present an optimal gradient method for smooth strongly convex optimization. The method is optimal in the sense that its worst-case bound on the distance to an optimal point exactly matches the lower bound on the oracle complexity for the class of problems, meaning that no black-box first-order method can have a better worst-case guarantee without further assumptions on the class of problems at hand. In addition, we provide a constructive recipe for obtaining the algorithmic parameters of the method and illustrate that it can be used for deriving methods for other optimality criteria as well."}}
{"id": "3SLLgc4jYuF", "cdate": 1609459200000, "mdate": 1648711205424, "content": {"title": "Asynchronous Stochastic Optimization Robust to Arbitrary Delays", "abstract": "We consider stochastic optimization with delayed gradients where, at each time step $t$, the algorithm makes an update using a stale stochastic gradient from step $t - d_t$ for some arbitrary delay $d_t$. This setting abstracts asynchronous distributed optimization where a central server receives gradient updates computed by worker machines. These machines can experience computation and communication loads that might vary significantly over time. In the general non-convex smooth optimization setting, we give a simple and efficient algorithm that requires $O( \\sigma^2/\\epsilon^4 + \\tau/\\epsilon^2 )$ steps for finding an $\\epsilon$-stationary point $x$, where $\\tau$ is the \\emph{average} delay $\\smash{\\frac{1}{T}\\sum_{t=1}^T d_t}$ and $\\sigma^2$ is the variance of the stochastic gradients. This improves over previous work, which showed that stochastic gradient decent achieves the same rate but with respect to the \\emph{maximal} delay $\\max_{t} d_t$, that can be significantly larger than the average delay especially in heterogeneous distributed systems. Our experiments demonstrate the efficacy and robustness of our algorithm in cases where the delay distribution is skewed or heavy-tailed."}}
{"id": "zVezanVNzr", "cdate": 1577836800000, "mdate": 1681829528221, "content": {"title": "Efficient first-order methods for convex minimization: a constructive approach", "abstract": ""}}
{"id": "jd-meSJXl-F", "cdate": 1577836800000, "mdate": 1681829528225, "content": {"title": "The Complexity of Finding Stationary Points with Stochastic Gradient Descent", "abstract": "We study the iteration complexity of stochastic gradient descent (SGD) for minimizing the gradient norm of smooth, possibly nonconvex functions. We provide several results, implying that the classi..."}}
{"id": "n7_CYY30Vy", "cdate": 1546300800000, "mdate": 1681829528223, "content": {"title": "The Complexity of Finding Stationary Points with Stochastic Gradient Descent", "abstract": "We study the iteration complexity of stochastic gradient descent (SGD) for minimizing the gradient norm of smooth, possibly nonconvex functions. We provide several results, implying that the $\\mathcal{O}(\\epsilon^{-4})$ upper bound of Ghadimi and Lan~\\cite{ghadimi2013stochastic} (for making the average gradient norm less than $\\epsilon$) cannot be improved upon, unless a combination of additional assumptions is made. Notably, this holds even if we limit ourselves to convex quadratic functions. We also show that for nonconvex functions, the feasibility of minimizing gradients with SGD is surprisingly sensitive to the choice of optimality criteria."}}
{"id": "SSaYFkfzdbJ", "cdate": 1546300800000, "mdate": 1642350921341, "content": {"title": "A Joint Named-Entity Recognizer for Heterogeneous Tag-sets Using a Tag Hierarchy", "abstract": "Genady Beryozkin, Yoel Drori, Oren Gilon, Tzvika Hartman, Idan Szpektor. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019."}}
