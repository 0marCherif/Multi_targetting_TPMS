{"id": "pIW2m4DKiIf", "cdate": 1696635904910, "mdate": 1696635904910, "content": {"title": "Climbing the WOL: Training for Cheaper Inference", "abstract": "Efficient inference for wide output layers (WOLs) is an essential yet challenging task in large scale machine learning. Most approaches reduce this problem to approximate maximum inner product search (MIPS), which relies heavily on the observation that for a given model, ground truth labels correspond to logits of highest value during full model inference. However, such an assumption is restrictive in practice. In this paper, we argue that approximate MIPS subroutines, despite having sub-linear computation time, are sub-optimal because they are tailored for retrieving large inner products with high recall instead of retrieving the correct labels. With WOL, the labels often have moderate inner products, which makes approximate MIPS more challenging. We propose an alternative problem formulation, called Label Superior Sampling (LSS), where the objective is to tailor the system to ensure retrieval of the correct label. Accordingly, we propose a novel learned hash approach, which is significantly more efficient and sufficient for high inference accuracy than MIPS baselines. Our extensive evaluation indicates that LSS can match or even outperform full inference accuracy with around 5x speed up and 87% energy reduction."}}
{"id": "lgtqFR1rOj", "cdate": 1696635750185, "mdate": null, "content": {"title": "HALOS: Hashing Large Output Space for Cheap Inference", "abstract": "Efficient inference in large output space is an essential yet challenging task in large scale machine learning.\nPrevious approaches reduce this problem to Approximate Maximum Inner Product Search (AMIPS), which is\nbased on the observation that the prediction of a given model corresponds to the logit with the largest value.\nHowever, models are not perfect in accuracy, and the successful retrievals of the largest logit may not lead to\nthe correct predictions. We argue that approximate MIPS approaches are sub-optimal because they are tailored\nfor retrieving largest inner products class instead of retrieving the correct class. Moreover, the logits generated\nfrom neural networks with large output space lead to extra challenges for the AMIPS method to achieve a high\nrecall rate within the computation budget of efficient inference. In this paper, we propose HALOS, which reduces\ninference into sub-linear computation by selectively activating a small set of output layer neurons that are likely to\ncorrespond to the correct classes rather than to yield the largest logit. Our extensive evaluations show that HALOS\nmatches or even outperforms the accuracy of given models with 21\u00d7 speed up and 87% energy reduction."}}
{"id": "6SRDbbvU8s", "cdate": 1663850175128, "mdate": null, "content": {"title": "Learning Multimodal Data Augmentation in Feature Space", "abstract": "The ability to jointly learn from multiple modalities, such as text, audio, and visual data, is a defining feature of intelligent systems. While there have been promising advances in designing neural networks to harness multimodal data, the enormous success of data augmentation currently remains limited to single-modality tasks like image classification. Indeed, it is particularly difficult to augment each modality while preserving the overall semantic structure of the data; for example, a caption may no longer be a good description of an image after standard augmentations have been applied, such as translation. Moreover, it is challenging to specify reasonable transformations that are not tailored to a particular modality. In this paper, we introduce LeMDA, Learning Multimodal Data Augmentation, an easy-to-use method that automatically learns to jointly augment multimodal data in feature space, with no constraints on the identities of the modalities or the relationship between modalities. We show that LeMDA can (1) profoundly improve the performance of multimodal deep learning architectures, (2) apply to combinations of modalities that have not been previously considered, and (3) achieve state-of-the-art results on a wide range of applications comprised of image, text, and tabular data."}}
{"id": "9PQ13zJ1HME", "cdate": 1652737652667, "mdate": null, "content": {"title": "Retaining Knowledge for Learning with Dynamic Definition", "abstract": "Machine learning models are often deployed in settings where they must be constantly updated in response to the changes in class definitions while retaining high accuracy on previously learned definitions. A classical use case is fraud detection, where new fraud schemes come one after another. While such an update can be accomplished by re-training on the complete data, the process is inefficient and prevents real-time and on-device learning. On the other hand, efficient methods that incrementally learn from new data often result in the forgetting of previously-learned knowledge. We define this problem as Learning with Dynamic Definition (LDD) and demonstrate that popular models, such as the Vision Transformer and Roberta, exhibit substantial forgetting of past definitions.  We present the first practical \nand provable solution to LDD. Our proposal is a hash-based sparsity model \\textit{RIDDLE} that solves evolving definitions by associating samples only to relevant parameters. We prove that our model is a universal function approximator and theoretically bounds the knowledge lost during the update process. On practical tasks with evolving class definition in vision and natural language processing, \\textit{RIDDLE} outperforms baselines by up to 30\\% on the original dataset while providing competitive accuracy on the update dataset."}}
{"id": "lU0ZCDCfvbx", "cdate": 1640995200000, "mdate": 1681789028376, "content": {"title": "Learning Multimodal Data Augmentation in Feature Space", "abstract": "The ability to jointly learn from multiple modalities, such as text, audio, and visual data, is a defining feature of intelligent systems. While there have been promising advances in designing neural networks to harness multimodal data, the enormous success of data augmentation currently remains limited to single-modality tasks like image classification. Indeed, it is particularly difficult to augment each modality while preserving the overall semantic structure of the data; for example, a caption may no longer be a good description of an image after standard augmentations have been applied, such as translation. Moreover, it is challenging to specify reasonable transformations that are not tailored to a particular modality. In this paper, we introduce LeMDA, Learning Multimodal Data Augmentation, an easy-to-use method that automatically learns to jointly augment multimodal data in feature space, with no constraints on the identities of the modalities or the relationship between modalities. We show that LeMDA can (1) profoundly improve the performance of multimodal deep learning architectures, (2) apply to combinations of modalities that have not been previously considered, and (3) achieve state-of-the-art results on a wide range of applications comprised of image, text, and tabular data."}}
{"id": "SCuDeavm5uF", "cdate": 1640995200000, "mdate": 1682354986184, "content": {"title": "HALOS: Hashing Large Output Space for Cheap Inference", "abstract": ""}}
{"id": "tj7SrgWYBQ", "cdate": 1609459200000, "mdate": 1682354993999, "content": {"title": "Efficient Inference via Universal LSH Kernel", "abstract": "Large machine learning models achieve unprecedented performance on various tasks and have evolved as the go-to technique. However, deploying these compute and memory hungry models on resource constraint environments poses new challenges. In this work, we propose mathematically provable Representer Sketch, a concise set of count arrays that can approximate the inference procedure with simple hashing computations and aggregations. Representer Sketch builds upon the popular Representer Theorem from kernel literature, hence the name, providing a generic fundamental alternative to the problem of efficient inference that goes beyond the popular approach such as quantization, iterative pruning and knowledge distillation. A neural network function is transformed to its weighted kernel density representation, which can be very efficiently estimated with our sketching algorithm. Empirically, we show that Representer Sketch achieves up to 114x reduction in storage requirement and 59x reduction in computation complexity without any drop in accuracy."}}
{"id": "cfYn1u9kalC", "cdate": 1609459200000, "mdate": 1658241707311, "content": {"title": "Neighbor Oblivious Learning (NObLe) for Device Localization and Tracking", "abstract": "On-device localization and tracking are increasingly crucial for various applications. Machine learning (ML) techniques are widely adopted along with the rapidly growing amount of data. However, during training, almost none of ML techniques incorporate the known structural information such as floor plan, which can be especially useful in indoor or other structured environments. The problem is incredibly hard because the structural properties are not explicitly available, making most structural learning approaches inapplicable. We study our method through the intuitions from manifold learning. Whereas existing manifold methods utilizes neighborhood information such as Euclidean distances, we quantize the output space to measure closeness on the structure. We propose Neighbor Oblivious Learning (NObLe) and demonstrate our approach's effectiveness on two applications, Wi-Fi-based fingerprint localization and inertial measurement unit(IMU) based device tracking. We show that NObLe gives significant improvement over state-of-art prediction accuracy."}}
{"id": "P7Ga9pbbV8z", "cdate": 1609459200000, "mdate": 1658241707312, "content": {"title": "Efficient and Less Centralized Federated Learning", "abstract": "With the rapid growth in mobile computing, massive amounts of data and computing resources are now located at the edge. To this end, Federated learning (FL) is becoming a widely adopted distributed machine learning (ML) paradigm, which aims to harness this expanding skewed data locally in order to develop rich and informative models. In centralized FL, a collection of devices collaboratively solve a ML task under the coordination of a central server. However, existing FL frameworks make an over-simplistic assumption about network connectivity and ignore the communication bandwidth of the different links in the network. In this paper, we present and study a novel FL algorithm, in which devices mostly collaborate with other devices in a pairwise manner. Our nonparametric approach is able to exploit network topology to reduce communication bottlenecks. We evaluate our approach on various FL benchmarks and demonstrate that our method achieves 10 $$\\times $$ better communication efficiency and around 8% increase in accuracy compared to the centralized approach."}}
{"id": "LLoonq1EUr", "cdate": 1609459200000, "mdate": 1682352709270, "content": {"title": "MONGOOSE: A Learnable LSH Framework for Efficient Neural Network Training", "abstract": "Recent advances by practitioners in the deep learning community have breathed new life into Locality Sensitive Hashing (LSH), using it to reduce memory and time bottlenecks in neural network (NN) training. However, while LSH has sub-linear guarantees for approximate near-neighbor search in theory, it is known to have inefficient query time in practice due to its use of random hash functions. Moreover, when model parameters are changing, LSH suffers from update overhead. This work is motivated by an observation that model parameters evolve slowly, such that the changes do not always require an LSH update to maintain performance. This phenomenon points to the potential for a reduction in update time and allows for a modified learnable version of data-dependent LSH to improve query time at a low cost. We use the above insights to build MONGOOSE, an end-to-end LSH framework for efficient NN training. In particular, MONGOOSE is equipped with a scheduling algorithm to adaptively perform LSH updates with provable guarantees and learnable hash functions to improve query efficiency. Empirically, we validate MONGOOSE on large-scale deep learning models for recommendation systems and language modeling. We find that it achieves up to 8% better accuracy compared to previous LSH approaches, with $6.5 \\times$ speed-up and $6\\times$ reduction in memory usage."}}
