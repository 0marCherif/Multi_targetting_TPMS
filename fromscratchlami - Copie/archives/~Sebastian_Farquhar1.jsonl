{"id": "tWS-S_aRDRe", "cdate": 1668734793622, "mdate": null, "content": {"title": "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation", "abstract": "We introduce a method to measure uncertainty in large language models.\nFor tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models.\nWe show that measuring uncertainty in natural language is challenging because of semantic equivalence\u2014different sentences can mean the same thing.\nTo overcome these challenges we introduce semantic entropy\u2014an entropy which incorporates linguistic invariances created by shared meanings.\nOur method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models.\nIn comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines."}}
{"id": "XCS_zBHQA2i", "cdate": 1668734793208, "mdate": null, "content": {"title": "What 'Out-of-distribution' Is and Is Not", "abstract": "Researchers want to generalize robustly to \u2018out-of-distribution\u2019 (OOD) data. Unfortunately, this term is used ambiguously causing confusion and creating risk\u2014people might believe they have made progress on OOD data and not realize this progress only holds in limited cases. We critique a standard definition of OOD\u2014difference-in-distribution\u2014and then disambiguate four meaningful types of OOD data: transformed-distributions, related-distributions, complement-distributions, and synthetic-distributions. We describe how existing OOD datasets, evaluations, and techniques fit into this framework. We provide a template for researchers to carefully present the scope of distribution shift considered in their work."}}
{"id": "VD-AYtP0dve", "cdate": 1663850390291, "mdate": null, "content": {"title": "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation", "abstract": "We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of \"semantic equivalence\"\u2014different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy\u2014an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines. "}}
{"id": "uIXyp4Ip9fG", "cdate": 1652737514546, "mdate": null, "content": {"title": "Active Surrogate Estimators: An Active Learning Approach to Label-Efficient Model Evaluation", "abstract": "We propose Active Surrogate Estimators (ASEs), a new method for label-efficient model evaluation. Evaluating model performance is a challenging and important problem when labels are expensive. ASEs address this active testing problem using a surrogate-based estimation approach that interpolates the errors of points with unknown labels, rather than forming a Monte Carlo estimator. ASEs actively learn the underlying surrogate, and we propose a novel acquisition strategy, XWED, that tailors this learning to the final estimation task. We find that ASEs offer greater label-efficiency than the current state-of-the-art when applied to challenging model evaluation problems for deep neural networks.\n"}}
{"id": "AIgn9uwfcD1", "cdate": 1632875669825, "mdate": null, "content": {"title": "Prospect Pruning: Finding Trainable Weights at Initialization using Meta-Gradients", "abstract": "Pruning neural networks at initialization would enable us to find sparse models that retain the accuracy of the original network while consuming fewer computational resources for training and inference. However, current methods are insufficient to enable this optimization and lead to a large degradation in model performance. In this paper, we identify a fundamental limitation in the formulation of current methods, namely that their saliency criteria look at a single step at the start of training without taking into account the trainability of the network. While pruning iteratively and gradually has been shown to improve pruning performance, explicit consideration of the training stage that will immediately follow pruning has so far been absent from the computation of the saliency criterion. To overcome the short-sightedness of existing methods, we propose Prospect Pruning (ProsPr), which uses meta-gradients through the first few steps of optimization to determine which weights to prune. ProsPr combines an estimate of the higher-order effects of pruning on the loss and the optimization trajectory to identify the trainable sub-network. Our method achieves state-of-the-art pruning performance on a variety of vision classification tasks, with less data and in a single shot compared to existing pruning-at-initialization methods."}}
{"id": "Y0cGpgUhSvp", "cdate": 1632875496469, "mdate": null, "content": {"title": "Prioritized training on points that are learnable, worth learning, and not yet learned", "abstract": "We introduce reducible held-out loss selection (RHOLS), a technique for faster model training which selects a sequence of training points that are \u201cjust right\u201d. We propose a tractable information-theoretic acquisition function\u2014the reducible heldout loss\u2014to efficiently choose training points that maximize information about a holdout set. We show that the \u201chard\u201d (e.g. high loss) points usually selected in the optimization literature are typically noisy, leading to deterioration on real-world datasets. At the same time, \u201ceasy\u201d (e.g. low noise) samples, often prioritized for curriculum learning, confer less information. In contrast, RHOLS chooses points that are \u201cjust right\u201d and trains in fewer steps than the above approaches."}}
{"id": "JiYq3eqTKY", "cdate": 1601308351974, "mdate": null, "content": {"title": "On Statistical Bias In Active Learning: How and When to Fix It", "abstract": "Active learning is a powerful tool when labelling data is expensive, but it introduces a bias because the training data no longer follows the population distribution. We formalize this bias and investigate the situations in which it can be harmful and sometimes even helpful. We further introduce novel corrective weights to remove bias when doing so is beneficial. Through this, our work not only provides a useful mechanism that can improve the active learning approach, but also an explanation for the empirical successes of various existing approaches which ignore this bias. In particular, we show that this bias can be actively helpful when training overparameterized models---like neural networks---with relatively modest dataset sizes."}}
{"id": "5EyX9LZi-R7", "cdate": 1577836800000, "mdate": null, "content": {"title": "Try Depth Instead of Weight Correlations: Mean-field is a Less Restrictive Assumption for Deeper Networks", "abstract": "We challenge the longstanding assumption that the mean-field approximation for variational inference in Bayesian neural networks is severely restrictive, and show this is not the case in deep networks. We prove several results indicating that deep mean-field variational weight posteriors can induce similar distributions in function-space to those induced by shallower networks with complex weight posteriors. We validate our theoretical contributions empirically, both through examination of the weight posterior using Hamiltonian Monte Carlo in small models and by comparing diagonal- to structured-covariance in large settings. Since complex variational posteriors are often expensive and cumbersome to implement, our results suggest that using mean-field variational inference in a deeper model is both a practical and theoretically justified alternative to structured approximations."}}
{"id": "PbNbb6LITS", "cdate": 1546300800000, "mdate": null, "content": {"title": "A Unifying Bayesian View of Continual Learning", "abstract": "Some machine learning applications require continual learning - where data comes in a sequence of datasets, each is used for training and then permanently discarded. From a Bayesian perspective, continual learning seems straightforward: Given the model posterior one would simply use this as the prior for the next task. However, exact posterior evaluation is intractable with many models, especially with Bayesian neural networks (BNNs). Instead, posterior approximations are often sought. Unfortunately, when posterior approximations are used, prior-focused approaches do not succeed in evaluations designed to capture properties of realistic continual learning use cases. As an alternative to prior-focused methods, we introduce a new approximate Bayesian derivation of the continual learning loss. Our loss does not rely on the posterior from earlier tasks, and instead adapts the model itself by changing the likelihood term. We call these approaches likelihood-focused. We then combine prior- and likelihood-focused methods into one objective, tying the two views together under a single unifying framework of approximate Bayesian continual learning."}}
{"id": "GJRQCczBwFi", "cdate": 1546300800000, "mdate": null, "content": {"title": "Differentially Private Continual Learning", "abstract": "Catastrophic forgetting can be a significant problem for institutions that must delete historic data for privacy reasons. For example, hospitals might not be able to retain patient data permanently. But neural networks trained on recent data alone will tend to forget lessons learned on old data. We present a differentially private continual learning framework based on variational inference. We estimate the likelihood of past data given the current model using differentially private generative models of old datasets."}}
