{"id": "EHZSf64Jrps", "cdate": 1672531200000, "mdate": 1682323839991, "content": {"title": "Real-time 3D Semantic Scene Completion Via Feature Aggregation and Conditioned Prediction", "abstract": "Semantic Scene Completion (SSC) aims to simultaneously predict the volumetric occupancy and semantic category of a 3D scene. In this paper, we propose a real-time semantic scene completion method with a feature aggregation strategy and conditioned prediction module. Feature aggregation fuses feature with different receptive fields and gathers context to improve scene completion performance. And the conditioned prediction module adopts a two-step prediction scheme that takes volumetric occupancy as a condition to enhance semantic completion prediction. We conduct experiments on three recognized benchmarks NYU, NYUCAD, and SUNCG. Our method achieves competitive performance at a speed of 110 FPS on one GTX 1080 Ti GPU."}}
{"id": "BXFb8VzEPT", "cdate": 1672531200000, "mdate": 1681696070534, "content": {"title": "Delicate Textured Mesh Recovery from NeRF via Adaptive Surface Refinement", "abstract": "Neural Radiance Fields (NeRF) have constituted a remarkable breakthrough in image-based 3D reconstruction. However, their implicit volumetric representations differ significantly from the widely-adopted polygonal meshes and lack support from common 3D software and hardware, making their rendering and manipulation inefficient. To overcome this limitation, we present a novel framework that generates textured surface meshes from images. Our approach begins by efficiently initializing the geometry and view-dependency decomposed appearance with a NeRF. Subsequently, a coarse mesh is extracted, and an iterative surface refining algorithm is developed to adaptively adjust both vertex positions and face density based on re-projected rendering errors. We jointly refine the appearance with geometry and bake it into texture images for real-time rendering. Extensive experiments demonstrate that our method achieves superior mesh quality and competitive rendering quality."}}
{"id": "Awr30IxHh_", "cdate": 1672531200000, "mdate": 1681696070530, "content": {"title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning", "abstract": "In this paper, we are interested in understanding self-supervised pretraining through studying the capability that self-supervised representation pretraining methods learn part-aware representations. The study is mainly motivated by that random views, used in contrastive learning, and random masked (visible) patches, used in masked image modeling, are often about object parts. We explain that contrastive learning is a part-to-whole task: the projection layer hallucinates the whole object representation from the object part representation learned from the encoder, and that masked image modeling is a part-to-part task: the masked patches of the object are hallucinated from the visible patches. The explanation suggests that the self-supervised pretrained encoder is required to understand the object part. We empirically compare the off-the-shelf encoders pretrained with several representative methods on object-level recognition and part-level recognition. The results show that the fully-supervised model outperforms self-supervised models for object-level recognition, and most self-supervised contrastive learning and masked image modeling methods outperform the fully-supervised method for part-level recognition. It is observed that the combination of contrastive learning and masked image modeling further improves the performance."}}
{"id": "w9aS8WVXXaf", "cdate": 1667357900567, "mdate": 1667357900567, "content": {"title": "Group DETR: Fast DETR Training with Group-Wise One-to-Many Assignment", "abstract": "Detection Transformer (DETR) relies on One-to-One assignment, i.e., assigning one ground-truth object to only one positive object query, for end-to-end object detection and lacks the capability of exploiting multiple positive object queries. We present a novel DETR training approach, named {\\em Group DETR}, to support Group-wise One-to-Many assignment. We make simple modifications during training: (i) adopt K groups of object queries; (ii) conduct decoder self-attention on each group of object queries with the same parameters; (iii) perform One-to-One label assignment for each group, leading to K positive object queries for each ground-truth object. In inference, we only use one group of object queries, making no modifications to DETR architecture and processes. We validate the effectiveness of the proposed approach on DETR variants, including Conditional DETR, DAB-DETR, DN-DETR, and DINO. Code will be available."}}
{"id": "Gb2Rndy5595", "cdate": 1663849892705, "mdate": null, "content": {"title": "Context Autoencoder for Self-Supervised Representation Learning", "abstract": "We present a novel masked image modeling (MIM) approach, context autoencoder (CAE), for self-supervised representation pretraining. The goal is to pretrain an encoder by solving the pretext task: estimate the masked patches from the visible patches in an image. Our approach first feeds the visible patches into the encoder, extracting the representations. Then, we make predictions from visible patches to masked patches in the encoded representation space. We introduce an alignment constraint, encouraging that the representations for masked patches, predicted from the encoded representations of visible patches, are aligned with the masked patch presentations computed from the encoder. In other words, the predicted representations are expected to lie in the encoded representation space, which empirically shows the benefit to representation learning. Last, the predicted masked patch representations are mapped to the targets of the pretext task through a decoder.\nOne additional characteristic is that our approach encourages the separation of the representation learning part (encoder), and the pretext task completion part that will be replaced by the downstream task part. In contrast, previous MIM methods (e.g., BEiT and MAE) couple the two parts, potentially limiting the representation learning quality. We demonstrate the effectiveness of our CAE through superior transfer performance in downstream tasks: semantic segmentation, and object detection and instance segmentation."}}
{"id": "3tYvDb4dwab", "cdate": 1663849839887, "mdate": null, "content": {"title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning", "abstract": "In this paper, we are interested in understanding self-supervised pretraining through studying the capability that self-supervised representation pretraining methods learn part-aware representations. The study is mainly motivated by that random views, used in contrastive learning, and random masked (visible) patches, used in masked image modeling, are often about object parts.\n\nWe explain that masked image modeling is a part-to-part task: the masked patches of the object are hallucinated from the visible patches, and that contrastive learning is a part-to-whole task: the projection layer hallucinates the whole object representation from the object part representation learned from the encoder. The explanation suggests that the self-supervised pretrained encoder is required to understand the object part. We empirically compare the off-the-shelf encoders pretrained with several representative methods on object-level recognition and part-level recognition. The results show that the fully-supervised model outperforms self-supervised models for object-level recognition, and most self-supervised contrastive learning and masked image modeling methods outperform the fully-supervised method for part-level recognition. It is observed that the combination of contrastive learning and masked image modeling further improves the performance."}}
{"id": "ptbePrczhRt", "cdate": 1663849811949, "mdate": null, "content": {"title": "Group DETR: Fast DETR Training with Group-Wise One-to-Many Assignment", "abstract": "Detection Transformer (DETR) relies on one-to-one assignment for end-to-end object detection and lacks the capability of exploiting multiple positive object queries. We present a novel DETR training approach, named {\\em Group DETR}, to support one-to-many assignment in a group-wise manner. To achieve it, we make simple modifications during training: (i) adopt $K$ groups of object queries; (ii) conduct decoder self-attention on each group of object queries with the same parameters; (iii) perform one-to-one assignment for each group, leading to $K$ positive object queries for each ground-truth object. In inference, we only use one group of object queries, making no modifications to model architectures and inference processes. Group DETR is a versatile training method and is applicable to various DETR variants. Our experiments show that Group DETR significantly speeds up the training convergences and improves the performances of various DETR-based methods."}}
{"id": "aPXMGv7aeOn", "cdate": 1652737373036, "mdate": null, "content": {"title": "Compressible-composable NeRF via Rank-residual Decomposition", "abstract": "Neural Radiance Field (NeRF) has emerged as a compelling method to represent 3D objects and scenes for photo-realistic rendering. \nHowever, its implicit representation causes difficulty in manipulating the models like the explicit mesh representation.\nSeveral recent advances in NeRF manipulation are usually restricted by a shared renderer network, or suffer from large model size. \nTo circumvent the hurdle, in this paper, we present a neural field representation that enables efficient and convenient manipulation of models.\nTo achieve this goal, we learn a hybrid tensor rank decomposition of the scene without neural networks. \nMotivated by the low-rank approximation property of the SVD algorithm, we propose a rank-residual learning strategy to encourage the preservation of primary information in lower ranks. \nThe model size can then be dynamically adjusted by rank truncation to control the levels of detail, achieving near-optimal compression without extra optimization.\nFurthermore, different models can be arbitrarily transformed and composed into one scene by concatenating along the rank dimension.\nThe growth of storage cost can also be mitigated by compressing the unimportant objects in the composed scene. \nWe demonstrate that our method is able to achieve comparable rendering quality to state-of-the-art methods, while enabling extra capability of compression and composition.\nCode is available at https://github.com/ashawkey/CCNeRF."}}
{"id": "pq06hQ5fpwk", "cdate": 1648732141437, "mdate": 1648732141437, "content": {"title": "Context Autoencoder for Self-Supervised Representation Learning", "abstract": "We present a novel masked image modeling (MIM) approach, called context autoencoder (CAE), for self-supervised representation learning. We randomly partition the image into two sets of patches: visible patches and masked patches. The architecture consists of: (i) an encoder that takes visible patches as the input and outputs the latent representations, (ii) a latent context regressor that regresses the masked patch representations from the visible patch representations that are not updated in this regressor, (iii) a decoder that takes the estimated masked patch representations as the input and makes predictions for the masked patches, and (iv) an alignment module that aligns the estimated masked patch representations with the masked patch representations computed from the encoder.\n\nIn comparison to previous MIM methods that couple the encoding and decoding roles, e.g., using a single module in BEiT, our approach attempts to separate the encoding role (content understanding) from the task decoding role (making predictions for masked patches) using different modules, improving the content understanding capability. In addition, our approach makes predictions from the visible patches to the masked patches in the latent representation space that is expected to take on semantics. We demonstrate the effectivess of our CAE through superior transfer performance in downstream tasks, semantic segmentation and object detection."}}
{"id": "xEn1xp8iu6", "cdate": 1648731321006, "mdate": 1648731321006, "content": {"title": "Conditional DETR for Fast Training Convergence", "abstract": "The recently-developed DETR approach applies the\ntransformer encoder and decoder architecture to object detection and achieves promising performance. In this paper, we handle the critical issue, slow training convergence,\nand present a conditional cross-attention mechanism for\nfast DETR training. Our approach is motivated by that the\ncross-attention in DETR relies highly on the content embeddings for localizing the four extremities and predicting the\nbox, which increases the need for high-quality content embeddings and thus the training difficulty.\nOur approach, named conditional DETR, learns a conditional spatial query from the decoder embedding for\ndecoder multi-head cross-attention. The benefit is that\nthrough the conditional spatial query, each cross-attention\nhead is able to attend to a band containing a distinct region, e.g., one object extremity or a region inside the object box. This narrows down the spatial range for localizing the distinct regions for object classification and box\nregression, thus relaxing the dependence on the content embeddings and easing the training. Empirical results show\nthat conditional DETR converges 6.7\u00d7 faster for the backbones R50 and R101 and 10\u00d7 faster for stronger backbones\nDC5-R50 and DC5-R101. Code is available at https:\n//github.com/Atten4Vis/ConditionalDETR."}}
