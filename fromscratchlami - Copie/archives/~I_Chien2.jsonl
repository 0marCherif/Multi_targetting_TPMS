{"id": "0Ng07WUCRXq", "cdate": 1680307200000, "mdate": 1681774441784, "content": {"title": "Provably accurate and scalable linear classifiers in hyperbolic spaces", "abstract": "Many high-dimensional practical data sets have hierarchical structures induced by graphs or time series. Such data sets are hard to process in Euclidean spaces, and one often seeks low-dimensional embeddings in other space forms to perform the required learning tasks. For hierarchical data, the space of choice is a hyperbolic space because it guarantees low-distortion embeddings for tree-like structures. The geometry of hyperbolic spaces has properties not encountered in Euclidean spaces that pose challenges when trying to rigorously analyze algorithmic solutions. We propose a unified framework for learning scalable and simple hyperbolic linear classifiers with provable performance guarantees. The gist of our approach is to focus on Poincar\u00e9 ball models and formulate the classification problems using tangent space formalisms. Our results include a new hyperbolic perceptron algorithm as well as an efficient and highly accurate convex optimization setup for hyperbolic support vector machine classifiers. Furthermore, we adapt our approach to accommodate second-order perceptrons, where data are preprocessed based on second-order information (correlation) to accelerate convergence, and strategic perceptrons, where potentially manipulated data arrive in an online manner and decisions are made sequentially. The excellent performance of the Poincar\u00e9 second-order and strategic perceptrons shows that the proposed framework can be extended to general machine learning problems in hyperbolic spaces. Our experimental results, pertaining to synthetic, single-cell RNA-seq expression measurements, CIFAR10, Fashion-MNIST and mini-ImageNet, establish that all algorithms provably converge and have complexity comparable to those of their Euclidean counterparts."}}
{"id": "7NdpPy307N", "cdate": 1675209600000, "mdate": 1682317985794, "content": {"title": "Small-Sample Estimation of the Mutational Support and Distribution of SARS-CoV-2", "abstract": "We consider the problem of determining the <i>mutational support and distribution</i> of the SARS-CoV-2 viral genome in the small-sample regime. The mutational support refers to the unknown number of sites that may eventually mutate in the SARS-CoV-2 genome while mutational distribution refers to the distribution of point mutations in the viral genome across a population. The mutational support may be used to assess the virulence of the virus and guide primer selection for real-time RT-PCR testing. Estimating the distribution of mutations in the genome of different subpopulations while accounting for the unseen may also aid in discovering new variants. To estimate the mutational support in the small-sample regime, we use GISAID sequencing data and our state-of-the-art polynomial estimation techniques based on new weighted and regularized Chebyshev approximation methods. For distribution estimation, we adapt the well-known Good-Turing estimator. Our analysis reveals several findings: First, the mutational supports exhibit significant differences in the ORF6 and ORF7a regions (older versus younger patients), ORF1b and ORF10 regions (females versus males) and in almost all ORFs (Asia/Europe/North America). Second, even though the N region of SARS-CoV-2 has a predicted <inline-formula><tex-math notation=\"LaTeX\">$10\\%$</tex-math></inline-formula> mutational support, mutations fall outside of the primer regions recommended by the CDC."}}
{"id": "wCxlGc9ZCwi", "cdate": 1664046166601, "mdate": null, "content": {"title": "Certified Graph Unlearning", "abstract": "Graph-structured data is ubiquitous in practice and often processed using graph neural networks (GNNs). With the adoption of recent laws ensuring the ``right to be forgotten'', the problem of graph data removal has become of significant importance. To address the problem, we introduce the first known framework for \\emph{certified graph unlearning} of GNNs. In contrast to standard machine unlearning, new analytical and heuristic unlearning challenges arise when dealing with complex graph data. First, three different types of unlearning requests need to be considered, including node feature, edge and node unlearning. Second, to establish provable performance guarantees, one needs to address challenges associated with feature mixing during propagation. The underlying analysis is illustrated on the example of simple graph convolutions (SGC) and their generalized PageRank (GPR) extensions, thereby laying the theoretical foundation for certified unlearning of GNNs. Our empirical studies on six benchmark datasets demonstrate excellent performance-complexity trade-offs when compared to complete retraining methods and approaches that do not leverage graph information. For example, when unlearning $20\\%$ of the nodes on the Cora dataset, our approach suffers only a $0.1\\%$ loss in test accuracy while offering a $4$-fold speed-up compared to complete retraining. Our scheme also outperforms unlearning methods that do not leverage graph information with a $12\\%$ increase in test accuracy for comparable time complexity."}}
{"id": "fhcu4FBLciL", "cdate": 1663850145545, "mdate": null, "content": {"title": "Efficient Model Updates for Approximate Unlearning of Graph-Structured Data", "abstract": "With the adoption of recent laws ensuring the ``right to be forgotten'', the problem of machine unlearning has become of significant importance. This is particularly the case for graph-structured data, and learning tools specialized for such data, including graph neural networks (GNNs). This work introduces the first known approach for \\emph{approximate graph unlearning} with provable theoretical guarantees. The challenges in addressing the problem are two-fold. First, there exist multiple different types of unlearning requests that need to be considered, including node feature, edge and node unlearning. Second, to establish provable performance guarantees, one needs to carefully evaluate the process of feature mixing during propagation. We focus on analyzing Simple Graph Convolutions (SGC) and their generalized PageRank (GPR) extensions, thereby laying the theoretical foundations for unlearning GNNs. Empirical evaluations of six benchmark datasets demonstrate excellent performance/complexity/privacy trade-offs of our approach compared to complete retraining and general methods that do not leverage graph information. For example, unlearning $200$ out of $1208$ training nodes of the Cora dataset only leads to a $0.1\\%$ loss in test accuracy, but offers a $4$-fold speed-up compared to complete retraining with a $(\\epsilon,\\delta)=(1,10^{-4})$ ``privacy cost''. We also exhibit a $12\\%$ increase in test accuracy for the same dataset when compared to unlearning methods that do not leverage graph information, with comparable time complexity and the same privacy guarantee."}}
{"id": "xjgR2GvFEe", "cdate": 1640995200000, "mdate": 1673996079211, "content": {"title": "HyperAid: Denoising in Hyperbolic Spaces for Tree-fitting and Hierarchical Clustering", "abstract": ""}}
{"id": "vyGtLsmkr1f", "cdate": 1640995200000, "mdate": 1673996079214, "content": {"title": "Provably Accurate and Scalable Linear Classifiers in Hyperbolic Spaces", "abstract": ""}}
{"id": "ky7UnMavkA", "cdate": 1640995200000, "mdate": 1673996079212, "content": {"title": "You are AllSet: A Multiset Function Framework for Hypergraph Neural Networks", "abstract": ""}}
{"id": "kwsl8WPkiD", "cdate": 1640995200000, "mdate": 1673996079216, "content": {"title": "HyperAid: Denoising in hyperbolic spaces for tree-fitting and hierarchical clustering", "abstract": ""}}
{"id": "gDCJg-tHUA", "cdate": 1640995200000, "mdate": 1673996079221, "content": {"title": "Unlearning Nonlinear Graph Classifiers in the Limited Training Data Regime", "abstract": ""}}
{"id": "DMTc7FmYHp", "cdate": 1640995200000, "mdate": 1673996079204, "content": {"title": "Node Feature Extraction by Self-Supervised Multi-scale Neighborhood Prediction", "abstract": ""}}
