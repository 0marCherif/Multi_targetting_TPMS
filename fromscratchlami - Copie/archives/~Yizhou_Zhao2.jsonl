{"id": "7wsmVHbLISi", "cdate": 1702819979677, "mdate": 1702819979677, "content": {"title": "Towards Noise-Tolerant Speech-Referring Video Object Segmentation: Bridging Speech and Text", "abstract": "Linguistic communication is prevalent in Human-Computer Interaction (HCI). Speech (spoken language) serves as a convenient yet potentially ambiguous form due to noise and accents, exposing a gap compared to text. In this study, we investigate the prominent HCI task, Referring Video Object Segmentation (R-VOS), which aims to segment and track objects using linguistic references. While text input is well-investigated, speech input is under-explored. Our objective is to bridge the gap between speech and text, enabling the adaptation of existing text-input R-VOS models to accommodate noisy speech input effectively. Specifically, we propose a method to align the semantic spaces between speech and text by incorporating two key modules: 1) Noise-Aware Semantic Adjustment (NSA) for clear semantics extraction from noisy speech; and 2) Semantic Jitter Suppression (SJS) enabling R-VOS models to tolerate noisy queries. Comprehensive experiments conducted on the challenging AVOS benchmarks reveal that our proposed method outperforms state-of-the-art approaches."}}
{"id": "pcgMNVhRslj", "cdate": 1652737364299, "mdate": null, "content": {"title": "Alignment-guided Temporal Attention for Video Action Recognition", "abstract": "Temporal modeling is crucial for various video learning tasks. Most recent approaches employ either factorized (2D+1D) or joint (3D) spatial-temporal operations to extract temporal contexts from the input frames. While the former is more efficient in computation, the latter often obtains better performance. In this paper, we attribute this to a dilemma between the sufficiency and the efficiency of interactions among various positions in different frames. These interactions affect the extraction of task-relevant information shared among frames. To resolve this issue, we prove that frame-by-frame alignments have the potential to increase the mutual information between frame representations, thereby including more task-relevant information to boost effectiveness. Then we propose Alignment-guided Temporal Attention (ATA) to extend 1-dimensional temporal attention with parameter-free patch-level alignments between neighboring frames. It can act as a general plug-in for image backbones to conduct the action recognition task without any model-specific design. Extensive experiments on multiple benchmarks demonstrate the superiority and generality of our module."}}
{"id": "xv14GObjNl1", "cdate": 1640995200000, "mdate": 1669136269893, "content": {"title": "Alignment-guided Temporal Attention for Video Action Recognition", "abstract": "Temporal modeling is crucial for various video learning tasks. Most recent approaches employ either factorized (2D+1D) or joint (3D) spatial-temporal operations to extract temporal contexts from the input frames. While the former is more efficient in computation, the latter often obtains better performance. In this paper, we attribute this to a dilemma between the sufficiency and the efficiency of interactions among various positions in different frames. These interactions affect the extraction of task-relevant information shared among frames. To resolve this issue, we prove that frame-by-frame alignments have the potential to increase the mutual information between frame representations, thereby including more task-relevant information to boost effectiveness. Then we propose Alignment-guided Temporal Attention (ATA) to extend 1-dimensional temporal attention with parameter-free patch-level alignments between neighboring frames. It can act as a general plug-in for image backbones to conduct the action recognition task without any model-specific design. Extensive experiments on multiple benchmarks demonstrate the superiority and generality of our module."}}
{"id": "mKQRF3EJj7", "cdate": 1640995200000, "mdate": 1669136269913, "content": {"title": "Semantic-aligned Fusion Transformer for One-shot Object Detection", "abstract": "One-shot object detection aims at detecting novel objects according to merely one given instance. With extreme data scarcity, current approaches explore various feature fusions to obtain directly transferable meta-knowledge. Yet, their performances are often unsatisfactory. In this paper, we attribute this to inappropriate correlation methods that misalign query-support semantics by overlooking spatial structures and scale variances. Upon analysis, we leverage the attention mechanism and propose a simple but effective architecture named Semantic-aligned Fusion Transformer (SaFT) to resolve these issues. Specifically, we equip SaFT with a vertical fusion module (VFM) for cross-scale semantic enhancement and a horizontal fusion module (HFM) for cross-sample feature fusion. Together, they broaden the vision for each feature point from the support to a whole augmented feature pyramid from the query, facilitating semantic-aligned associations. Extensive experiments on multiple benchmarks demonstrate the superiority of our framework. Without fine-tuning on novel classes, it brings significant performance gains to one-stage baselines, lifting state-of-the-art results to a higher level."}}
{"id": "Irz55o9MR5", "cdate": 1640995200000, "mdate": 1669136269945, "content": {"title": "Semantic-aligned Fusion Transformer for One-shot Object Detection", "abstract": "One-shot object detection aims at detecting novel objects according to merely one given instance. With extreme data scarcity, current approaches explore various feature fusions to obtain directly transferable meta-knowledge. Yet, their performances are often unsatisfactory. In this paper, we attribute this to inappropriate correlation methods that misalign query-support semantics by overlooking spatial structures and scale variances. Upon analysis, we leverage the attention mechanism and propose a simple but effective architecture named Semantic-aligned Fusion Transformer (SaFT) to resolve these issues. Specifically, we equip SaFT with a vertical fusion module (VFM) for cross-scale semantic enhancement and a horizontal fusion module (HFM) for cross-sample feature fusion. Together, they broaden the vision for each feature point from the support to a whole augmented feature pyramid from the query, facilitating semantic-aligned associations. Extensive experiments on multiple benchmarks demonstrate the superiority of our framework. Without fine-tuning on novel classes, it brings significant performance gains to one-stage baselines, lifting state-of-the-art results to a higher level."}}
{"id": "lBCZn1n2DX", "cdate": 1546300800000, "mdate": 1669136269953, "content": {"title": "On Integration of Any Factor with Distance for Navigation : Walk Safely and Fast Enough", "abstract": "With the popularization of personal navigation systems and mobile devices, multifarious location-based services and datasets are being advanced. In this paper, we propose a new method for navigating based on combination of any factor (e.g. safety, enjoyment, air quality, etc.) with distance. A data structure based on intensity of the factor is introduced. This data structure can be extended easily to cover different sources of data (e.g. government open data, users' feedback). Any kind of shortest path algorithm can be used in proposed method. We apply this method to a situation where travelers are seeking safer routes. The incoming traffic data of each station from Paris in 2017 is utilized to model a location-based safety status of the city. The shortest route is then modulated to a safer route to help pedestrians bypass unsafe zones in the city, thereby reducing the risk of encountering incidents. As a proof-of-concept, we produce a web application based on Google Maps API, which illustrates the performance of our method and application."}}
