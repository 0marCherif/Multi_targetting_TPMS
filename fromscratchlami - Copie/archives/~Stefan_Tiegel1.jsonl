{"id": "YM_6gYU0a7", "cdate": 1672531200000, "mdate": 1680275593707, "content": {"title": "Robust Mean Estimation Without a Mean: Dimension-Independent Error in Polynomial Time for Symmetric Distributions", "abstract": ""}}
{"id": "4njLr-ZlyHM", "cdate": 1672531200000, "mdate": 1683881997723, "content": {"title": "Private estimation algorithms for stochastic block models and mixture models", "abstract": "We introduce general tools for designing efficient private estimation algorithms, in the high-dimensional settings, whose statistical guarantees almost match those of the best known non-private algorithms. To illustrate our techniques, we consider two problems: recovery of stochastic block models and learning mixtures of spherical Gaussians. For the former, we present the first efficient $(\\epsilon, \\delta)$-differentially private algorithm for both weak recovery and exact recovery. Previously known algorithms achieving comparable guarantees required quasi-polynomial time. For the latter, we design an $(\\epsilon, \\delta)$-differentially private algorithm that recovers the centers of the $k$-mixture when the minimum separation is at least $ O(k^{1/t}\\sqrt{t})$. For all choices of $t$, this algorithm requires sample complexity $n\\geq k^{O(1)}d^{O(t)}$ and time complexity $(nd)^{O(t)}$. Prior work required minimum separation at least $O(\\sqrt{k})$ as well as an explicit upper bound on the Euclidean norm of the centers."}}
{"id": "vspRe_Hsu3Y", "cdate": 1640995200000, "mdate": 1682939381517, "content": {"title": "Optimal SQ Lower Bounds for Learning Halfspaces with Massart Noise", "abstract": "We give tight statistical query (SQ) lower bounds for learnining halfspaces in the presence of Massart noise. In particular, suppose that all labels are corrupted with probability at most $\\eta$. We show that for arbitrary $\\eta \\in [0,1/2]$ every SQ algorithm achieving misclassification error better than $\\eta$ requires queries of superpolynomial accuracy or at least a superpolynomial number of queries. Further, this continues to hold even if the information-theoretically optimal error $\\mathrm{OPT}$ is as small as $\\exp\\left(-\\log^c(d)\\right)$, where $d$ is the dimension and $0 < c < 1$ is an arbitrary absolute constant, and an overwhelming fraction of examples are noiseless. Our lower bound matches known polynomial time algorithms, which are also implementable in the SQ framework. Previously, such lower bounds only ruled out algorithms achieving error $\\mathrm{OPT} + \\epsilon$ or error better than $\\Omega(\\eta)$ or, if $\\eta$ is close to $1/2$, error $\\eta - o_\\eta(1)$, where the term $o_\\eta(1)$ is constant in $d$ but going to 0 for $\\eta$ approaching $1/2$. As a consequence, we also show that achieving misclassification error better than $1/2$ in the $(A,\\alpha)$-Tsybakov model is SQ-hard for $A$ constant and $\\alpha$ bounded away from 1."}}
{"id": "fe_niTfSZ6", "cdate": 1640995200000, "mdate": 1683894240391, "content": {"title": "Hardness of Agnostically Learning Halfspaces from Worst-Case Lattice Problems", "abstract": "We show hardness of improperly learning halfspaces in the agnostic model, both in the distribution-independent as well as the distribution-specific setting, based on the assumption that worst-case lattice problems, such as GapSVP or SIVP, are hard. In particular, we show that under this assumption there is no efficient algorithm that outputs any binary hypothesis, not necessarily a halfspace, achieving misclassfication error better than $\\frac 1 2 - \\gamma$ even if the optimal misclassification error is as small is as small as $\\delta$. Here, $\\gamma$ can be smaller than the inverse of any polynomial in the dimension and $\\delta$ as small as $exp(-\\Omega(\\log^{1-c}(d)))$, where $0 < c < 1$ is an arbitrary constant and $d$ is the dimension. For the distribution-specific setting, we show that if the marginal distribution is standard Gaussian, for any $\\beta > 0$ learning halfspaces up to error $OPT_{LTF} + \\epsilon$ takes time at least $d^{\\tilde{\\Omega}(1/\\epsilon^{2-\\beta})}$ under the same hardness assumptions. Similarly, we show that learning degree-$\\ell$ polynomial threshold functions up to error $OPT_{{PTF}_\\ell} + \\epsilon$ takes time at least $d^{\\tilde{\\Omega}(\\ell^{2-\\beta}/\\epsilon^{2-\\beta})}$. $OPT_{LTF}$ and $OPT_{{PTF}_\\ell}$ denote the best error achievable by any halfspace or polynomial threshold function, respectively. Our lower bounds qualitively match algorithmic guarantees and (nearly) recover known lower bounds based on non-worst-case assumptions. Previously, such hardness results [Daniely16, DKPZ21] were based on average-case complexity assumptions or restricted to the statistical query model. Our work gives the first hardness results basing these fundamental learning problems on worst-case complexity assumptions. It is inspired by a sequence of recent works showing hardness of learning well-separated Gaussian mixtures based on worst-case lattice problems."}}
{"id": "byHEC8OPYd", "cdate": 1640995200000, "mdate": 1683885193068, "content": {"title": "Fast algorithm for overcomplete order-3 tensor decomposition", "abstract": "We develop the first fast spectral algorithm to decompose a random third-order tensor over $\\mathbb{R}^d$ of rank up to $O(d^{3/2}/\\text{polylog}(d))$. Our algorithm only involves simple linear algebra operations and can recover all components in time $O(d^{6.05})$ under the current matrix multiplication time. Prior to this work, comparable guarantees could only be achieved via sum-of-squares [Ma, Shi, Steurer 2016]. In contrast, fast algorithms [Hopkins, Schramm, Shi, Steurer 2016] could only decompose tensors of rank at most $O(d^{4/3}/\\text{polylog}(d))$. Our algorithmic result rests on two key ingredients. A clean lifting of the third-order tensor to a sixth-order tensor, which can be expressed in the language of tensor networks. A careful decomposition of the tensor network into a sequence of rectangular matrix multiplications, which allows us to have a fast implementation of the algorithm."}}
{"id": "Qo-eEbRxGzj", "cdate": 1640995200000, "mdate": 1683885193019, "content": {"title": "Fast algorithm for overcomplete order-3 tensor decomposition", "abstract": "We develop the first fast spectral algorithm to decompose a random third-order tensor over of rank up to $$O(d^{3/2}/polylog(d))$$. Our algorithm only involves simple linear algebra operations and ..."}}
{"id": "8UPPtE9NTi", "cdate": 1640995200000, "mdate": 1682939381517, "content": {"title": "Optimal SQ Lower Bounds for Learning Halfspaces with Massart Noise", "abstract": "We give tight statistical query (SQ) lower bounds for learnining halfspaces in the presence of Massart noise. In particular, suppose that all labels are corrupted with probability at most $\\eta$. W..."}}
{"id": "BaHth99Sp45", "cdate": 1621630045081, "mdate": null, "content": {"title": "Consistent Estimation for PCA and Sparse Regression with Oblivious Outliers", "abstract": "We develop machinery to design efficiently computable and \\emph{consistent} estimators, achieving estimation error approaching zero as the number of observations grows, when facing an oblivious adversary that may corrupt responses in all but an $\\alpha$ fraction of the samples.\nAs concrete examples, we investigate two problems: \nsparse regression and principal component analysis (PCA).\nFor sparse regression, we achieve consistency for optimal sample size $n\\gtrsim (k\\log d)/\\alpha^2$ \nand optimal error rate $O(\\sqrt{(k\\log d)/(n\\cdot \\alpha^2)})$\nwhere $n$ is the number of observations, $d$ is the number of dimensions and $k$ is the sparsity of the parameter vector, allowing the fraction of inliers to be inverse-polynomial in the number of samples.\nPrior to this work, no estimator was known to be consistent when the fraction of inliers $\\alpha$ is $o(1/\\log \\log n)$, even for (non-spherical) Gaussian design matrices.\nResults holding under weak design assumptions and in the presence of such general noise have only been shown in dense setting (i.e., general linear regression) very recently by d'Orsi et al.~\\cite{ICML-linear-regression}.\nIn the context of PCA, we attain optimal error guarantees under broad spikiness assumptions on the parameter matrix (usually used in matrix completion). \nPrevious works could obtain non-trivial guarantees only under the assumptions that the measurement noise corresponding to the inliers is polynomially small in $n$ (e.g., Gaussian with variance $1/n^2$).\n\nTo devise our estimators, we equip the Huber loss with non-smooth regularizers such as the $\\ell_1$ norm or the nuclear norm, and extend d'Orsi et al.'s approach~\\cite{ICML-linear-regression} in a novel way to analyze the loss function.\nOur machinery appears to be easily applicable to a wide range of estimation problems.\nWe complement these algorithmic results with statistical lower bounds showing that the fraction of inliers that our PCA estimator can deal with is optimal up to a constant factor."}}
{"id": "I7kTNMdtwk", "cdate": 1609459200000, "mdate": 1680275593704, "content": {"title": "Consistent Estimation for PCA and Sparse Regression with Oblivious Outliers", "abstract": ""}}
{"id": "ElW3QXT_f2", "cdate": 1609459200000, "mdate": 1682413153705, "content": {"title": "Consistent Estimation for PCA and Sparse Regression with Oblivious Outliers", "abstract": "We develop machinery to design efficiently computable and consistent estimators, achieving estimation error approaching zero as the number of observations grows, when facing an oblivious adversary that may corrupt responses in all but an $\\alpha$ fraction of the samples. As concrete examples, we investigate two problems: sparse regression and principal component analysis (PCA). For sparse regression, we achieve consistency for optimal sample size $n\\gtrsim (k\\log d)/\\alpha^2$ and optimal error rate $O(\\sqrt{(k\\log d)/(n\\cdot \\alpha^2)})$ where $n$ is the number of observations, $d$ is the number of dimensions and $k$ is the sparsity of the parameter vector, allowing the fraction of inliers to be inverse-polynomial in the number of samples. Prior to this work, no estimator was known to be consistent when the fraction of inliers $\\alpha$ is $o(1/\\log \\log n)$, even for (non-spherical) Gaussian design matrices. Results holding under weak design assumptions and in the presence of such general noise have only been shown in dense setting (i.e., general linear regression) very recently by d'Orsi et al. [dNS21]. In the context of PCA, we attain optimal error guarantees under broad spikiness assumptions on the parameter matrix (usually used in matrix completion). Previous works could obtain non-trivial guarantees only under the assumptions that the measurement noise corresponding to the inliers is polynomially small in $n$ (e.g., Gaussian with variance $1/n^2$). To devise our estimators, we equip the Huber loss with non-smooth regularizers such as the $\\ell_1$ norm or the nuclear norm, and extend d'Orsi et al.'s approach [dNS21] in a novel way to analyze the loss function. Our machinery appears to be easily applicable to a wide range of estimation problems."}}
