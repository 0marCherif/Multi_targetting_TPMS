{"id": "F0UQv_MNWCt", "cdate": 1663850471252, "mdate": null, "content": {"title": "ORCA: Interpreting Prompted Language Models via Locating Supporting Evidence in the Ocean of Pretraining Data", "abstract": "Prompting large pretrained language models leads to strong performance in a variety of downstream tasks. However, it is still unclear from where the model learns task-specific knowledge, especially in zero-shot setups. In this work, we propose a novel method ORCA to identify evidence of the model's task-specific competence in prompt-based learning. Through an instance attribution approach to model interpretability, by iteratively using gradient information related to the downstream task, ORCA locates a very small subset of pretraining data that directly supports the model's predictions in a given task; we call this subset supporting data evidence. We show that supporting data evidence offers new insights about the prompted language models. For example, in the tasks of sentiment analysis and textual entailment, BERT shows a substantial reliance on BookCorpus---the smaller corpus of BERT's two pretraining corpora---as well as on pretraining examples that mask out synonyms to the task labels used in prompts."}}
{"id": "rUqNvMLlwnJ", "cdate": 1640995200000, "mdate": 1680041185541, "content": {"title": "SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control", "abstract": ""}}
{"id": "fZ1azNZDdvT", "cdate": 1640995200000, "mdate": 1680041185636, "content": {"title": "Toward Human Readable Prompt Tuning: Kubrick's The Shining is a good movie, and a good prompt too?", "abstract": ""}}
{"id": "LBnSY0-4cRu", "cdate": 1640995200000, "mdate": 1664784847429, "content": {"title": "ORCA: Interpreting Prompted Language Models via Locating Supporting Data Evidence in the Ocean of Pretraining Data", "abstract": "Large pretrained language models have been performing increasingly well in a variety of downstream tasks via prompting. However, it remains unclear from where the model learns the task-specific knowledge, especially in a zero-shot setup. In this work, we want to find evidence of the model's task-specific competence from pretraining and are specifically interested in locating a very small subset of pretraining data that directly supports the model in the task. We call such a subset supporting data evidence and propose a novel method ORCA to effectively identify it, by iteratively using gradient information related to the downstream task. This supporting data evidence offers interesting insights about the prompted language models: in the tasks of sentiment analysis and textual entailment, BERT shows a substantial reliance on BookCorpus, the smaller corpus of BERT's two pretraining corpora, as well as on pretraining examples that mask out synonyms to the task verbalizers."}}
{"id": "kyIMb-BmOCF", "cdate": 1609459200000, "mdate": 1636965460826, "content": {"title": "Influence Tuning: Demoting Spurious Correlations via Instance Attribution and Instance-Driven Updates", "abstract": "Among the most critical limitations of deep learning NLP models are their lack of interpretability, and their reliance on spurious correlations. Prior work proposed various approaches to interpreting the black-box models to unveil the spurious correlations, but the research was primarily used in human-computer interaction scenarios. It still remains underexplored whether or how such model interpretations can be used to automatically \"unlearn\" confounding features. In this work, we propose influence tuning--a procedure that leverages model interpretations to update the model parameters towards a plausible interpretation (rather than an interpretation that relies on spurious patterns in the data) in addition to learning to predict the task labels. We show that in a controlled setup, influence tuning can help deconfounding the model from spurious patterns in data, significantly outperforming baseline methods that use adversarial training."}}
{"id": "EGeNVp2MEC", "cdate": 1609459200000, "mdate": 1664784847429, "content": {"title": "Influence Tuning: Demoting Spurious Correlations via Instance Attribution and Instance-Driven Updates", "abstract": ""}}
{"id": "ldqxqqLsw-a", "cdate": 1577836800000, "mdate": 1631921397200, "content": {"title": "Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions", "abstract": "Xiaochuang Han, Byron C. Wallace, Yulia Tsvetkov. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020."}}
{"id": "kbJhb7QGwH", "cdate": 1577836800000, "mdate": 1631921397207, "content": {"title": "Fortifying Toxic Speech Detectors Against Veiled Toxicity", "abstract": "Xiaochuang Han, Yulia Tsvetkov. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020."}}
{"id": "r1VFxXZdWS", "cdate": 1546300800000, "mdate": null, "content": {"title": "No Permanent Friends or Enemies: Tracking Relationships between Nations from News", "abstract": "Xiaochuang Han, Eunsol Choi, Chenhao Tan. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019."}}
{"id": "iK81RQZ-X-Ec", "cdate": 1546300800000, "mdate": 1632877153900, "content": {"title": "Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence Labeling", "abstract": "Xiaochuang Han, Jacob Eisenstein. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019."}}
