{"id": "Au1gNqq4brw", "cdate": 1601308318972, "mdate": null, "content": {"title": "SEQUENCE-LEVEL FEATURES: HOW GRU AND LSTM CELLS CAPTURE N-GRAMS", "abstract": "Modern recurrent neural networks (RNN) such as Gated Recurrent Units (GRU) and Long Short-term Memory (LSTM) have demonstrated impressive results on tasks involving sequential data in practice. Despite continuous efforts on interpreting their behaviors, the exact mechanism underlying their successes in capturing sequence-level information have not been thoroughly understood. In this work, we present a study on understanding the essential features captured by GRU/LSTM cells by mathematically expanding and unrolling the hidden states. Based on the expanded and unrolled hidden states, we find there was a type of sequence-level representations brought in by the gating mechanism, which enables the cells to encode sequence-level features along with token-level features. Specifically, we show that the cells would consist of such sequence-level features similar to those of N-grams. Based on such a finding, we also found that replacing the hidden states of the standard cells with N-gram representations does not necessarily degrade performance on the sentiment analysis and language modeling tasks, indicating such features may play a significant role for GRU/LSTM cells."}}
{"id": "b3oSJk62nXo", "cdate": 1577836800000, "mdate": 1642382326260, "content": {"title": "Understanding Attention for Text Classification", "abstract": "Xiaobing Sun, Wei Lu. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020."}}
