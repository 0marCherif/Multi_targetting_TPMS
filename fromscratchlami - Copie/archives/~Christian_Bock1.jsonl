{"id": "TCcuN59Neh", "cdate": 1672531200000, "mdate": 1699142302209, "content": {"title": "Training Large-scale Foundation Models on Emerging AI Chips", "abstract": "Foundation models such as ChatGPT and GPT-4 have garnered significant interest from both academia and industry due to their emergent capabilities, such as few-shot prompting, multi-step reasoning, instruction following, and model calibration. Such capabilities were previously only attainable with specially designed models, such as those using knowledge graphs, but can now be achieved on a much larger scale with foundation models. As the capabilities of foundation models have increased, so too have their sizes at a rate much faster than Moore's law. For example, the BERT large model was initially released as a 334M model in 2018, and by 2023, the largest GPT-4 models are estimated to range between 200-300B, representing an increase of three orders of magnitude in just five years. The training of foundation models requires massive computing power. For instance, training a BERT model on a single state-of-the-art GPU machine with multi-A100 chips can take several days, while training GPT-3 models on a large multi-instance GPU cluster can take several months to complete the estimated 3 X 1023 flops. This tutorial provides an overview of the latest progress in supporting foundation model training and inference with new AI chips. It reviews progress on the modeling side, with an emphasis on the transformer architecture, and presents the system architecture supporting training and serving foundation models. This includes programming language frameworks such as PyTorch and Tensorflow, graph compilers, 3D parallelisms, and accelerators such as the GPU H100, TPU, and Trainium. Finally, the tutorial presents our experience of training foundation models using different systems."}}
{"id": "uL59O1tfTz", "cdate": 1640995200000, "mdate": 1648673213639, "content": {"title": "Online Time Series Anomaly Detection with State Space Gaussian Processes", "abstract": "We propose r-ssGPFA, an unsupervised online anomaly detection model for uni- and multivariate time series building on the efficient state space formulation of Gaussian processes. For high-dimensional time series, we propose an extension of Gaussian process factor analysis to identify the common latent processes of the time series, allowing us to detect anomalies efficiently in an interpretable manner. We gain explainability while speeding up computations by imposing an orthogonality constraint on the mapping from the latent to the observed. Our model's robustness is improved by using a simple heuristic to skip Kalman updates when encountering anomalous observations. We investigate the behaviour of our model on synthetic data and show on standard benchmark datasets that our method is competitive with state-of-the-art methods while being computationally cheaper."}}
{"id": "WSXH30BvgC", "cdate": 1609459200000, "mdate": 1681715713882, "content": {"title": "Motifs and Manifolds Statistical and Topological Machine Learning for Characterising and Classifying Biomedical Time Series", "abstract": "The increased focus on evidence-based practice in the health sciences led to a plethora of (un)organised and digitised data. In conjunction with the availability of technological advances in the life sciences, this resulted in extraordinary access to biomedical data. Due to efficient measurement devices, the frequency at which data can be obtained is at an unprecedented high, leading to the adage that data, indeed, could be the new gold. Examples of such high-resolution time series data are the continuous monitoring of patient vital parameters or a single electrocardiogram (ECG) itself. The temporal component introduced by time series data is both a chance and a challenge, necessitating the development of appropriate data analysis techniques. A chance, as it allows us to utilise a measurement\u2019s temporal evolution to characterise or classify the object of interest (e.g. patients, cells, or other organisms). A challenge because local and global correlation structures exacerbate obtaining a complete picture of a time-evolving phenomenon. Moreover, many dynamically changing systems exhibit alterations that occur at multiple scales and in multiple channels. This thesis presents a set of novel methods to help characterise and classify time-varying data with the express purpose of answering questions at the intersection of machine learning and healthcare. Recognising that time series arise from different categories, we first separate them into real-valued and object-valued time series and investigate both types separately. For the analysis of the first type, we propose a novel method to mine time series patterns efficiently. Driven by a statistical approach, we will introduce a way to identify temporal biomarkers and illustrate their utility in a data set of intensive care unit patients. For this, we leverage the expressive power of subsequences to obtain a high-dimensional time series representation. This feature representation is subsequently used to develop a kernel method based on optimal transport theory. The developed algorithm is of general applicability for medium-sized data sets and has proven particularly effective in the classification setting. The first part of this thesis ends with the presentation of a collaborative machine learning system to predict myocardial ischaemia from stress test ECGs. We develop a deep learning-based approach to significantly reduce the number of patients that unnecessarily undergo myocardial perfusion imaging. A subsequent interpretability analysis presents a potential path towards explainable and trustworthy artificial intelligence in cardiology. The second part of this thesis describes an effort to improve our understanding of artificial neural networks. By treating the network as a composition of time-varying graphs, we develop a method that characterises the change of its structural complexity over time. Our method captures the benefit of deep-learning best practices and can be used as an earlystopping criterion without the need for a validation data set. We thus manage to improve our understanding of artificial neural networks and shed light on the properties linked to their generalisation capabilities. Throughout this thesis, we demonstrate and highlight that in the analysis of (biomedical) time series, it is crucial to take the end-user into account. Interpretability and statistical analyses are of utter importance to make the otherwise opaque field of machine learning transparent to clinicians, physicians, and biologists. Moreover, we also hold up the mirror to ourselves as machine learning researchers: Comprehending the underlying mechanisms of our algorithms is at least as important as their empirical successes. The present thesis paves the path towards a better understanding of artificial neural networks and sheds light on complex phenotypes such as sepsis and myocardial ischaemia in clinically relevant ways."}}
{"id": "-o_AU7dpzQN", "cdate": 1609459200000, "mdate": 1681715713909, "content": {"title": "Machine Learning for Biomedical Time Series Classification: From Shapelets to Deep Learning", "abstract": "With the biomedical field generating large quantities of time series data, there has been a growing interest in developing and refining machine learning methods that allow its mining and exploitation. Classification is one of the most important and challenging machine learning tasks related to time series. Many biomedical phenomena, such as the brain\u2019s activity or blood pressure, change over time. The objective of this chapter is to provide a gentle introduction to time series classification. In the first part we describe the characteristics of time series data and challenges in its analysis. The second part provides an overview of common machine learning methods used for time series classification. A real-world use case, the early recognition of sepsis, demonstrates the applicability of the methods discussed."}}
{"id": "P0DL7M6T57o", "cdate": 1589917733361, "mdate": null, "content": {"title": "Path Imputation Strategies for Signature Models", "abstract": "The signature transform is a 'universal nonlinearity' on the space of continuous vector-valued paths, and has received attention for use in machine learning. However real-world temporal data is typically discretised, and must first be transformed into a continuous path before signature techniques can be applied.  We characterise this as an imputation problem, and empirically assess the impact of various imputation techniques when applying signatures to irregular time series data. In our experiments, we find that the choice of imputation drastically affects shallow signature models, whereas deeper architectures are more robust. We also observe that uncertainty-aware predictions are overall beneficial, even compared to the uncertainty-aware training of Gaussian process (GP) adapters.  Hence, we propose an extension of GP adapters by integrating uncertainty to the prediction step. This leads to competitive performance in general, and improves robustness in signature models in particular."}}
{"id": "S4hWCoKSUx5", "cdate": 1577836800000, "mdate": 1645791654221, "content": {"title": "Uncovering the Topology of Time-Varying fMRI Data using Cubical Persistence", "abstract": "Functional magnetic resonance imaging (fMRI) is a crucial technology for gaining insights into cognitive processes in humans. Data amassed from fMRI measurements result in volumetric data sets that vary over time. However, analysing such data presents a challenge due to the large degree of noise and person-to-person variation in how information is represented in the brain. To address this challenge, we present a novel topological approach that encodes each time point in an fMRI data set as a persistence diagram of topological features, i.e. high-dimensional voids present in the data. This representation naturally does not rely on voxel-by-voxel correspondence and is robust towards noise. We show that these time-varying persistence diagrams can be clustered to find meaningful groupings between participants, and that they are also useful in studying within-subject brain state trajectories of subjects performing a particular task. Here, we apply both clustering and trajectory analysis techniques to a group of participants watching the movie 'Partly Cloudy'. We observe significant differences in both brain state trajectories and overall topological activity between adults and children watching the same movie."}}
{"id": "K5MDMmyu4_T", "cdate": 1577836800000, "mdate": null, "content": {"title": "Uncovering the Topology of Time-Varying fMRI Data using Cubical Persistence", "abstract": "Functional magnetic resonance imaging (fMRI) is a crucial technology for gaining insights into cognitive processes in humans. Data amassed from fMRI measurements result in volumetric data sets that vary over time. However, analysing such data presents a challenge due to the large degree of noise and person-to-person variation in how information is represented in the brain. To address this challenge, we present a novel topological approach that encodes each time point in an fMRI data set as a persistence diagram of topological features, i.e. high-dimensional voids present in the data. This representation naturally does not rely on voxel-by-voxel correspondence and is robust to noise. We show that these time-varying persistence diagrams can be clustered to find meaningful groupings between participants, and that they are also useful in studying within-subject brain state trajectories of subjects performing a particular task. Here, we apply both clustering and trajectory analysis techniques to a group of participants watching the movie 'Partly Cloudy'. We observe significant differences in both brain state trajectories and overall topological activity between adults and children watching the same movie."}}
{"id": "B_bZCjKB8e5", "cdate": 1577836800000, "mdate": 1645791654100, "content": {"title": "Set Functions for Time Series", "abstract": "Despite the eminent successes of deep neural networks, many architectures are often hard to transfer to irregularly-sampled and asynchronous time series that commonly occur in real-world datasets, ..."}}
{"id": "ByxCrerKvS", "cdate": 1569439814037, "mdate": null, "content": {"title": "Set Functions for Time Series", "abstract": "Despite the eminent successes of deep neural networks, many architectures are often hard to transfer to irregularly-sampled and asynchronous time series that occur in many real-world datasets, such as healthcare applications. This paper proposes a novel framework for classifying irregularly sampled time series with unaligned measurements, focusing on high scalability and data efficiency.\nOur method SeFT (Set Functions for Time Series) is based on recent advances in differentiable set function learning, extremely parallelizable, and scales well to very large datasets and online monitoring scenarios.\nWe extensively compare our method to competitors on multiple healthcare time series datasets and show that it performs competitively whilst significantly reducing runtime."}}
{"id": "rpvvCiKr8l5", "cdate": 1546300800000, "mdate": 1645791654175, "content": {"title": "Neural Persistence: A Complexity Measure for Deep Neural Networks Using Algebraic Topology", "abstract": "While many approaches to make neural networks more fathomable have been proposed, they are restricted to interrogating the network with input data. Measures for characterizing and monitoring structural properties, however, have not been developed. In this work, we propose neural persistence, a complexity measure for neural network architectures based on topological data analysis on weighted stratified graphs. To demonstrate the usefulness of our approach, we show that neural persistence reflects best practices developed in the deep learning community such as dropout and batch normalization. Moreover, we derive a neural persistence-based stopping criterion that shortens the training process while achieving comparable accuracies as early stopping based on validation loss."}}
