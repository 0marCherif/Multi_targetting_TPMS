{"id": "Ctq0d9LEuT", "cdate": 1685532018438, "mdate": null, "content": {"title": "Stochastic Rising Bandits: A Best Arm Identification Approach", "abstract": "Stochastic Rising Bandits (SRBs) model sequential decision-making problems in which the expected rewards of the available options increase every time they are selected. This setting captures a wide range of scenarios in which the available options are learning entities whose performance improves (in expectation) over time. While previous works addressed the regret minimization problem, this paper, focuses on the fixed-budget Best Arm Identification (BAI) problem for SRBs. In this scenario, given a fixed budget of rounds, we are asked to provide a recommendation about the best option at the end of the identification process. We propose two algorithms to tackle the above-mentioned setting, namely R-UCBE, which resorts to a UCB-like approach, and R-SR, which employs a successive reject procedure. Then, we prove that, with a sufficiently large budget, they provide guarantees on the probability of properly identifying the optimal option at the end of the learning process. Furthermore, we derive a lower bound on the error probability, matched by our R-SR (up to logarithmic factors), and illustrate how the need for a sufficiently large budget is unavoidable in the SRB setting. Finally, we numerically validate the proposed algorithms in both synthetic and real-world environments and compare them with the currently available BAI strategies."}}
{"id": "YrHpQWpwsy", "cdate": 1685532017922, "mdate": null, "content": {"title": "Online Learning in Autoregressive Dynamics", "abstract": "Autoregressive processes naturally arise in a large variety of real-world scenarios, including stock markets, sales forecasting, weather prediction, advertising, and pricing. When facing a sequential decision-making problem in such a context, the temporal dependence between consecutive observations should be properly accounted for guaranteeing convergence to the optimal policy. In this work, we propose a novel online learning setting, namely, Autoregressive Bandits (ARBs), in which the observed reward is governed by an autoregressive process of order $k$, whose parameters depend on the chosen action. We show that, under mild assumptions on the reward process, the optimal policy can be conveniently computed. Then, we devise a new optimistic regret minimization algorithm, namely, AutoRegressive Upper Confidence Bound (AR-UCB), that suffers sublinear regret of order $\\widetilde{\\mathcal{O}} \\left( \\frac{(k+1)^{3/2}\\sqrt{nT}}{(1-\\Gamma)^2}\\right)$, where $T$ is the optimization horizon, $n$ is the number of actions, and $\\Gamma < 1$ is a stability index of the process. Finally, we empirically evaluate our algorithm in both synthetic and real-world domains, illustrating its advantages w.r.t. relevant bandit baselines."}}
{"id": "D8slvx44yfT", "cdate": 1672531200000, "mdate": 1681490124753, "content": {"title": "Best Arm Identification for Stochastic Rising Bandits", "abstract": ""}}
{"id": "wyVROCz9mP", "cdate": 1640995200000, "mdate": 1681490125042, "content": {"title": "Autoregressive Bandits", "abstract": ""}}
{"id": "ap8v-_YZaF", "cdate": 1640995200000, "mdate": 1681490124899, "content": {"title": "ARLO: A Framework for Automated Reinforcement Learning", "abstract": ""}}
{"id": "Yw89V0rWAs", "cdate": 1640995200000, "mdate": 1681490124891, "content": {"title": "Pricing the Long Tail by Explainable Product Aggregation and Monotonic Bandits", "abstract": ""}}
{"id": "MNEmNjH7UR0", "cdate": 1640995200000, "mdate": 1681490125010, "content": {"title": "Dynamic Pricing with Volume Discounts in Online Settings", "abstract": ""}}
{"id": "7agI-vOC6e3", "cdate": 1640995200000, "mdate": 1681490125012, "content": {"title": "Dynamical Linear Bandits", "abstract": ""}}
