{"id": "VznCjj4JJtW", "cdate": 1680160562188, "mdate": 1680160562188, "content": {"title": "Multiple-Instance Learning from Similar and Dissimilar Bags", "abstract": "Multiple-instance learning (MIL) is an important weakly supervised binary classification problem, where training instances are arranged in bags, and each bag is assigned a positive or negative label. Most of the previous studies for MIL assume that training bags are fully labeled. However, in some real-world scenarios, it could be difficult to collect fully labeled bags, due to the expensive time and labor consumption of the labeling task. Fortunately, it could be much easier for us to collect similar and dissimilar bags (indicating whether two bags share the same label or not), because we do not need to figure out the underlying label of each bag in this case. Therefore, in this paper, we for the first time investigate MIL from only similar and dissimilar bags. To solve this new MIL problem, we propose a convex formulation to train a bag-level classifier based on empirical risk minimization and theoretically derive a generalization error bound. In addition, we also propose a strong baseline for this new MIL problem, which aims to train an instance-level classifier by minimizing the instance-level empirical risk. Extensive experimental results clearly demonstrate that our proposed baseline works well, while our proposed convex formulation is even better."}}
{"id": "DwHIcEyias", "cdate": 1652737754869, "mdate": null, "content": {"title": "Generalizing Consistent Multi-Class Classification with Rejection to be Compatible with Arbitrary Losses", "abstract": "\\emph{Classification with rejection} (CwR) refrains from making a prediction to avoid critical misclassification when encountering test samples that are difficult to classify. Though previous methods for CwR have been provided with theoretical guarantees, they are only compatible with certain loss functions, making them not flexible enough when the loss needs to be changed with the dataset in practice. In this paper, we derive a novel formulation for CwR that can be equipped with arbitrary loss functions while maintaining the theoretical guarantees. First, we show that $K$-class CwR is equivalent to a $(K\\!+\\!1)$-class classification problem on the original data distribution with an augmented class, and propose an empirical risk minimization formulation to solve this problem with an estimation error bound. Then, we find necessary and sufficient conditions for the learning \\emph{consistency} of the surrogates constructed on our proposed formulation equipped with any classification-calibrated multi-class losses, where consistency means the surrogate risk minimization implies the target risk minimization for CwR. Finally, experiments on benchmark datasets validate the effectiveness of our proposed method. "}}
{"id": "ywEx0OiJflS", "cdate": 1632875718086, "mdate": null, "content": {"title": "Multi-Class Classification from Single-Class Data with Confidences", "abstract": "Can we learn a multi-class classifier from only \\emph{data from a single class}? We show that without any assumptions on the loss functions, models, and optimizers, we can successfully learn a multi-class classifier from only data from a single class with a rigorous consistency guarantee when \\emph{confidences} (i.e., the class-posterior probabilities) are available. Specifically, we propose an empirical risk minimization framework that is loss-/model-/optimizer-independent. Instead of constructing a boundary between the given class and all the other classes, our method can conduct discriminative classification between all the classes even if no data from the other classes are given. We further theoretically and experimentally show that our method can be Bayes-consistent with a simple modification even if the provided confidences are highly noisy. Then, we provide an extension of our method for the case where data from a subset of all the classes are available. Experimental results demonstrate the effectiveness of our methods."}}
{"id": "qQ86PDqU8AP", "cdate": 1609459200000, "mdate": 1627887121857, "content": {"title": "Learning from Similarity-Confidence Data", "abstract": "Weakly supervised learning has drawn considerable attention recently to reduce the expensive time and labor consumption of labeling massive data. In this paper, we investigate a novel weakly superv..."}}
{"id": "r98Yv25gnIn", "cdate": 1577836800000, "mdate": null, "content": {"title": "Multi-Complementary and Unlabeled Learning for Arbitrary Losses and Models", "abstract": "A weakly-supervised learning framework named as complementary-label learning has been proposed recently, where each sample is equipped with a single complementary label that denotes one of the classes the sample does not belong to. However, the existing complementary-label learning methods cannot learn from the easily accessible unlabeled samples and samples with multiple complementary labels, which are more informative. In this paper, to remove these limitations, we propose the novel multi-complementary and unlabeled learning framework that allows unbiased estimation of classification risk from samples with any number of complementary labels and unlabeled samples, for arbitrary loss functions and models. We first give an unbiased estimator of the classification risk from samples with multiple complementary labels, and then further improve the estimator by incorporating unlabeled samples into the risk formulation. The estimation error bounds show that the proposed methods are in the optimal parametric convergence rate. Finally, the experiments on both linear and deep models show the effectiveness of our methods."}}
{"id": "Jwe7l13r5r1", "cdate": 1577836800000, "mdate": null, "content": {"title": "Multi-variable estimation-based safe screening rule for small sphere and large margin support vector machine", "abstract": "Small Sphere and Large Margin (SSLM) SVM is one of the most competitive methods for Novelty Detection. However, the existing solvers for SSLM cannot deal with large data due to the expensive time cost. Although recently emerged safe screening methods can effectively enhance the computational speed, it is not available for SSLM because SSLM has multiple variables which cannot be represented explicitly by the linear combination of training samples. In this work, we construct a new safe screening rule for SSLM (MVE-SSR-SSLM) by integrating the \u03bd - p r o p e r t y , KKT conditions and variational inequalities. It is the first safe screening rule for a family of hypersphere support vector machine with multiple variables. The inactive samples are removed before actually solving the problem to accelerate the solving procedure without any loss of safety. Numerical experiments on fifteen benchmark datasets and Chinese wine dataset are conducted to show the validity and stability of the proposed MVE-SSR-SSLM."}}
