{"id": "3TzMQBjJkt", "cdate": 1682403069411, "mdate": 1682403069411, "content": {"title": "Learning Generalized Knowledge from a Single Domain on Urban-Scene Segmentation", "abstract": "Deep neural networks have made significant progress in various tasks under the assumption of the same distribution between training and testing data. However, the obtained domain-specific knowledge often suffers from performance degradation when facing out-of-distribution data. Towards addressing the  degradation,  a critical requirement of such  networks is the generalization capability to unseen domains,  which is the goal of domain generalization (DG). This paper attempts to learn generalized knowledge from a single synthetic domain and then apply it to real and unknown scenarios. Specifically, we propose a contour-aware instance normalization module to effectively learn domain-invariant features via a novel weight-updating strategy,which  can  largely exploit the generalized  information  from  the observed  data.  In addition, a category-level contrastive learning mechanism is proposed  through  understanding the semantic discrepancy and relevance among samples  to  mitigate  the interference of domain-specific features on classification. Extensive experiments together with ablation  studies on widely-adopted datasets are conducted to demonstrate the effectiveness of our design and show the superiority of our method over other state-of-the-art schemes on the task of urban-scene segmentation. The source code  is  available at https://github.com/leelxh/DG-LGK."}}
{"id": "EPl7WBJRzb", "cdate": 1668652612671, "mdate": 1668652612671, "content": {"title": "Image dehazing via enhancement, restoration, and fusion: A survey", "abstract": "Haze usually causes severe interference to image visibility. Such degradation on images troubles both human observers and computer vision systems. To seek high-quality images from degraded ones, a large number of image dehazing algorithms have been proposed from different perspectives like image enhancement, restoration, and fusion. Especially in recent years, with the rapid development of deep learning, CNN-based methods have already dominated the mainstream of image dehazing and gained significant progress on benchmark datasets. This paper firstly presents a comprehensive survey of existing image dehazing methods, and then conducts both qualitative and quantitative comparisons among representative methods, from classic methods to recent advanced approaches. We expect the literature survey and benchmark analysis could help readers better understand the advantages and limitations of existing dehazing methods. Moreover, a discussion on possible trends in single image dehazing is put forward to innovate further works."}}
{"id": "iEM8uetC1yy", "cdate": 1668652501029, "mdate": 1668652501029, "content": {"title": "Self-Augmented Unpaired Image Dehazing via Density and Depth Decomposition", "abstract": "To overcome the overfitting issue of dehazing models trained on synthetic hazy-clean image pairs, many recent methods attempted to improve models' generalization ability by training on unpaired data. Most of them simply formulate dehazing and rehazing cycles, yet ignore the physical properties of the real-world hazy environment, i.e. the haze varies with density and depth. In this paper, we propose a self-augmented image dehazing framework, termed D^4 (Dehazing via Decomposing transmission map into Density and Depth) for haze generation and removal. Instead of merely estimating transmission maps or clean content, the proposed framework focuses on exploring scattering coefficient and depth information contained in hazy and clean images. With estimated scene depth, our method is capable of re-rendering hazy images with different thicknesses which further benefits the training of the dehazing network. It is worth noting that the whole training process needs only unpaired hazy and clean images, yet succeeded in recovering the scattering coefficient, depth map and clean content from a single hazy image. Comprehensive experiments demonstrate our method outperforms state-of-the-art unpaired dehazing methods with much fewer parameters and FLOPs.Our code is available at https://github.com/YaN9-Y/D4"}}
{"id": "CAcPvdVaZxJ", "cdate": 1668518374643, "mdate": 1668518374643, "content": {"title": "Beyond brightening low-light images", "abstract": "Images captured under low-light conditions often suffer from (partially) poor visibility. Besides unsatisfactory lightings, multiple types of degradation, such as noise and color distortion due to the limited quality of cameras, hide in the dark. In other words, solely turning up the brightness of dark regions will inevitably amplify pollution. Thus, low-light image enhancement should not only brighten dark regions, but also remove hidden artifacts. To achieve the goal, this work builds a simple yet effective network, which, inspired by Retinex theory, decomposes images into two components. Following a divide-and-conquer principle, one component (illumination) is responsible for light adjustment, while the other (reflectance) for degradation removal. In such a way, the original space is decoupled into two smaller subspaces, expecting for better regularization/learning. It is worth noticing that our network is trained with paired images shot under different exposure conditions, instead of using any ground-truth reflectance and illumination information. Extensive experiments are conducted to demonstrate the efficacy of our design and its superiority over the state-of-the-art alternatives, especially in terms of the robustness against severe visual defects and the flexibility in adjusting light levels. Our code is made publicly available at https://github.com/zhangyhuaee/KinD_plus."}}
{"id": "MYRouzcBYG", "cdate": 1668518293934, "mdate": 1668518293934, "content": {"title": "U2Fusion: A Unified Unsupervised Image Fusion Network", "abstract": "This study proposes a novel unified and unsupervised end-to-end image fusion network, termed as U2Fusion , which is capable of solving different fusion problems, including multi-modal, multi-exposure, and multi-focus cases. Using feature extraction and information measurement, U2Fusion automatically estimates the importance of corresponding source images and comes up with adaptive information preservation degrees. Hence, different fusion tasks are unified in the same framework. Based on the adaptive degrees, a network is trained to preserve the adaptive similarity between the fusion result and source images. Therefore, the stumbling blocks in applying deep learning for image fusion, e.g., the requirement of ground-truth and specifically designed metrics, are greatly mitigated. By avoiding the loss of previous fusion capabilities when training a single model for different tasks sequentially, we obtain a unified model that is applicable to multiple fusion tasks. Moreover, a new aligned infrared and visible image dataset, RoadScene (available at https://github.com/hanna-xu/RoadScene ), is released to provide a new option for benchmark evaluation. Qualitative and quantitative experimental results on three typical image fusion tasks validate the effectiveness and universality of U2Fusion. Our code is publicly available at https://github.com/hanna-xu/U2Fusion ."}}
{"id": "ibXRMCp-muE", "cdate": 1668518209783, "mdate": 1668518209783, "content": {"title": "Self-Augmented Unpaired Image Dehazing via Density and Depth Decomposition", "abstract": "To overcome the overfitting issue of dehazing models trained on synthetic hazy-clean image pairs, many recent methods attempted to improve models' generalization ability by training on unpaired data. Most of them simply formulate dehazing and rehazing cycles, yet ignore the physical properties of the real-world hazy environment, ie the haze varies with density and depth. In this paper, we propose a self-augmented image dehazing framework, termed D^ 4 (Dehazing via Decomposing transmission map into Density and Depth) for haze generation and removal. Instead of merely estimating transmission maps or clean content, the proposed framework focuses on exploring scattering coefficient and depth information contained in hazy and clean images. With estimated scene depth, our method is capable of re-rendering hazy images with different thicknesses which further benefits the training of the dehazing network. It is worth noting that the whole training process needs only unpaired hazy and clean images, yet succeeded in recovering the scattering coefficient, depth map and clean content from a single hazy image. Comprehensive experiments demonstrate our method outperforms state-of-the-art unpaired dehazing methods with much fewer parameters and FLOPs. Our code is available at https://github. com/YaN9-Y/D4"}}
{"id": "Qbidfi8UFLg", "cdate": 1668518044669, "mdate": 1668518044669, "content": {"title": "LIME: Low-Light Image Enhancement via Illumination Map Estimation", "abstract": "When one captures images in low-light conditions, the images often suffer from low visibility. Besides degrading the visual aesthetics of images, this poor quality may also significantly degenerate the performance of many computer vision and multimedia algorithms that are primarily designed for high-quality inputs. In this paper, we propose a simple yet effective low-light image enhancement (LIME) method. More concretely, the illumination of each pixel is first estimated individually by finding the maximum value in R, G, and B channels. Furthermore, we refine the initial illumination map by imposing a structure prior on it, as the final illumination map. Having the well-constructed illumination map, the enhancement can be achieved accordingly. Experiments on a number of challenging low-light images are present to reveal the efficacy of our LIME and show its superiority over several state-of-the-arts in terms of enhancement quality and efficiency."}}
{"id": "SM70KHTBG-0", "cdate": 1655376321429, "mdate": null, "content": {"title": "Vision-based Uneven BEV Representation Learning with Polar Rasterization and Surface Estimation", "abstract": "In this work, we propose PolarBEV  for vision-based uneven BEV representation learning. To adapt to the foreshortening effect of camera imaging, we rasterize the BEV space both angularly and radially, and introduce polar embedding decomposition to model the associations among polar grids.  Polar grids are rearranged to an array-like regular representation for efficient processing. Besides, to determine the 2D-to-3D correspondence, we iteratively update the BEV surface based on a hypothetical plane, and adopt height-based  feature transformation. PolarBEV keeps real-time inference speed on a single 2080Ti GPU, and outperforms other methods for both BEV semantic segmentation and BEV instance segmentation. Thorough ablations  are presented to validate the design. The code will be released for facilitating further research."}}
{"id": "Xy3cqR6ON1G", "cdate": 1640995200000, "mdate": 1668589305361, "content": {"title": "Image dehazing via enhancement, restoration, and fusion: A survey", "abstract": ""}}
{"id": "Aqi2V87gHUg", "cdate": 1640995200000, "mdate": 1668589305351, "content": {"title": "U2Fusion: A Unified Unsupervised Image Fusion Network", "abstract": "This study proposes a novel <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">unified</i> and <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">unsupervised</i> end-to-end image <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">fusion</i> network, termed as <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">U2Fusion</i> , which is capable of solving different fusion problems, including multi-modal, multi-exposure, and multi-focus cases. Using feature extraction and information measurement, U2Fusion automatically estimates the importance of corresponding source images and comes up with adaptive information preservation degrees. Hence, different fusion tasks are unified in the same framework. Based on the adaptive degrees, a network is trained to preserve the adaptive similarity between the fusion result and source images. Therefore, the stumbling blocks in applying deep learning for image fusion, e.g., the requirement of ground-truth and specifically designed metrics, are greatly mitigated. By avoiding the loss of previous fusion capabilities when training a single model for different tasks sequentially, we obtain a unified model that is applicable to multiple fusion tasks. Moreover, a new aligned infrared and visible image dataset, <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">RoadScene</i> (available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/hanna-xu/RoadScene</uri> ), is released to provide a new option for benchmark evaluation. Qualitative and quantitative experimental results on three typical image fusion tasks validate the effectiveness and universality of U2Fusion. Our code is publicly available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/hanna-xu/U2Fusion</uri> ."}}
