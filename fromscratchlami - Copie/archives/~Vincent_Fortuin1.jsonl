{"id": "LH76pl-OUj", "cdate": 1681833044653, "mdate": null, "content": {"title": "Uncertainty in Graph Contrastive Learning with Bayesian Neural Networks", "abstract": "Graph contrastive learning has shown great promise when labeled data is scarce, but large unlabeled datasets are available. However, it often does not take uncertainty estimation into account. We show that a variational Bayesian  neural network approach can be used to improve not only the uncertainty estimates but also the downstream performance on semi-supervised node-classification tasks. Moreover, we propose a new measure of uncertainty for contrastive learning, that is based on the disagreement in likelihood due to different positive samples."}}
{"id": "E-sb8SHo2LL", "cdate": 1681833044578, "mdate": null, "content": {"title": "Linearized Laplace Inference in Neural Additive Models", "abstract": "Deep neural networks are highly effective but suffer from a lack of interpretability due to their black-box nature. Neural additive models (NAMs) solve this by separating into additive sub-networks, revealing the interactions between features and predictions. In this paper, we approach the NAM from a Bayesian perspective in order to quantify the uncertainty in the recovered interactions. Linearized Laplace approximation enables inference of these interactions directly in function space and yields a tractable estimate of the marginal likelihood, which can be used to perform implicit feature selection through an empirical Bayes procedure. Empirically, we show that Laplace-approximated NAMs (LA-NAM) are both more robust to noise and easier to interpret than their non-Bayesian counterpart for tabular regression and classification tasks."}}
{"id": "Wv3XoUtqL_", "cdate": 1681833044369, "mdate": null, "content": {"title": "Promises and Pitfalls of the Linearized Laplace in Bayesian Optimization", "abstract": "The linearized-Laplace approximation (LLA) has been shown to be effective and efficient in constructing Bayesian neural networks. It is theoretically compelling since it can be seen as a Gaussian process posterior with the mean function given by the neural network's \\emph{maximum-a-posteriori} predictive function and the covariance function induced by the empirical neural tangent kernel. However, while its efficacy has been studied in large-scale tasks like image classification, it has not been studied in sequential decision-making problems like Bayesian optimization where Gaussian processes---with simple mean functions and kernels such as the radial basis function---are the \\emph{de-facto} surrogate models. In this work, we study the usefulness of the LLA in Bayesian optimization and highlight its strong performance and flexibility. However, we also present some pitfalls that might arise and a potential problem with the LLA when the search space is unbounded."}}
{"id": "i3ewAfTbCxJ", "cdate": 1652737762566, "mdate": null, "content": {"title": "Invariance Learning in Deep Neural Networks with Differentiable Laplace Approximations", "abstract": "Data augmentation is commonly applied to improve performance of deep learning by enforcing the knowledge that certain transformations on the input preserve the output. Currently, the data augmentation parameters are chosen by human effort and costly cross-validation, which makes it cumbersome to apply to new datasets. We develop a convenient gradient-based method for selecting the data augmentation without validation data during training of a deep neural network. Our approach relies on phrasing data augmentation as an invariance in the prior distribution on the functions of a neural network, which allows us to learn it using Bayesian model selection. This has been shown to work in Gaussian processes, but not yet for deep neural networks. We propose a differentiable Kronecker-factored Laplace approximation to the marginal likelihood as our objective, which can be optimised without human supervision or validation data. We show that our method can successfully recover invariances present in the data, and that this improves generalisation and data efficiency on image datasets."}}
{"id": "rZEM7ULs5x5", "cdate": 1646077515131, "mdate": null, "content": {"title": "Data augmentation in Bayesian neural networks and the cold posterior effect", "abstract": "Bayesian neural networks that incorporate data augmentation implicitly use a \"randomly perturbed log-likelihood [which] does not have a clean interpretation as a valid likelihood function\" (Izmailov et al. 2021). Here, we provide several approaches to developing principled Bayesian neural networks incorporating data augmentation. We introduce a \"finite orbit\" setting which allows valid likelihoods to be computed exactly, and for the more usual \"full orbit\" setting we derive multi-sample bounds tighter than those used previously for Bayesian neural networks with data augmentation. These models cast light on the origin of the cold posterior effect. In particular, we find that the cold posterior effect persists even in these principled models incorporating data augmentation. This suggests that the cold posterior effect cannot be dismissed as an artifact of data augmentation using incorrect likelihoods."}}
{"id": "U86Yz_o4wMI", "cdate": 1640995200000, "mdate": 1651154204178, "content": {"title": "Invariance Learning in Deep Neural Networks with Differentiable Laplace Approximations", "abstract": "Data augmentation is commonly applied to improve performance of deep learning by enforcing the knowledge that certain transformations on the input preserve the output. Currently, the data augmentation parameters are chosen by human effort and costly cross-validation, which makes it cumbersome to apply to new datasets. We develop a convenient gradient-based method for selecting the data augmentation without validation data during training of a deep neural network. Our approach relies on phrasing data augmentation as an invariance in the prior distribution on the functions of a neural network, which allows us to learn it using Bayesian model selection. This has been shown to work in Gaussian processes, but not yet for deep neural networks. We propose a differentiable Kronecker-factored Laplace approximation to the marginal likelihood as our objective, which can be optimised without human supervision or validation data. We show that our method can successfully recover invariances present in the data, and that this improves generalisation and data efficiency on image datasets."}}
{"id": "y7zFbVdkP7E", "cdate": 1637576009242, "mdate": null, "content": {"title": "Meta-learning richer priors for VAEs", "abstract": "Variational auto-encoders have proven to capture complicated data distributions and useful latent representations, while advances in meta-learning have made it possible to extract prior knowledge from data. We incorporate these two approaches and propose a novel flexible prior, namely the Pseudo-inputs prior, to obtain a richer latent space. We train VAEs using the Model-Agnostic Meta-Learning (MAML) algorithm and show that it achieves comparable reconstruction performance with standard training. However, we show that this MAML-VAE model learns richer latent representations, which we evaluate in terms of unsupervised few-shot classification as a downstream task. Moreover, we show that our proposed Pseudo-inputs prior outperforms baseline priors, including the VampPrior, in both models, while also encouraging high-level representations through its pseudo-inputs."}}
{"id": "MlmbNOw36QL", "cdate": 1637576009068, "mdate": null, "content": {"title": "Quantum Bayesian Neural Networks", "abstract": "Quantum machine learning promises great speedups over classical algorithms, but it often requires repeated computations to achieve a desired level of accuracy for its point estimates. Bayesian learning focuses more on sampling from posterior distributions than on point estimation, thus it might be more forgiving in the face of additional quantum noise. We propose a quantum algorithm for Bayesian neural network inference, drawing on recent advances in quantum deep learning, and simulate its empirical performance on several tasks. We find that already for small numbers of qubits, our algorithm approximates the true posterior well, while it does not require any repeated computations and thus fully realizes the quantum speedups."}}
{"id": "2fcbA5AO_tZ", "cdate": 1637576008708, "mdate": null, "content": {"title": "On Disentanglement in Gaussian Process Variational Autoencoders", "abstract": "Complex multivariate time series arise in many fields, ranging from computer vision to robotics or medicine. Often we are interested in the independent underlying factors that give rise to the high-dimensional data we are observing.\nWhile many models have been introduced to learn such \\emph{disentangled} representations, only few attempt to explicitly exploit the structure of sequential data. We investigate the disentanglement properties of Gaussian process variational autoencoders, a class of models recently introduced that have been successful in different tasks on time series data.\nOur model exploits the temporal structure of the data by modeling each latent channel with a Gaussian process prior and employing a structured variational distribution that can capture dependencies in time.\nWe show that such priors can improve disentanglement and demonstrate the competitiveness of our approach against state-of-the-art unsupervised and weakly-supervised disentanglement methods on a benchmark task. Moreover, we provide evidence that we can learn disentangled representations on real-world medical time series data."}}
{"id": "oG0vTBw58ic", "cdate": 1637576008650, "mdate": null, "content": {"title": "Neural Variational Gradient Descent", "abstract": "Particle-based approximate Bayesian inference approaches such as Stein Variational Gradient Descent (SVGD) combine the flexibility and convergence guarantees of sampling methods with the computational benefits of variational inference. In practice, SVGD relies on the choice of an appropriate kernel function, which impacts its ability to model the target distribution---a challenging problem with only heuristic solutions. We propose Neural Variational Gradient Descent (NVGD), which is based on parametrizing the witness function of the Stein discrepancy by a deep neural network whose parameters are learned in parallel to the inference, mitigating the necessity to make any kernel choices whatsoever. We empirically validate our method on synthetic and real-world inference problems."}}
