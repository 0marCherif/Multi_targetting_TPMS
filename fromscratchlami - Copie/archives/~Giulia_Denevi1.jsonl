{"id": "0Uejkm1GB1U", "cdate": 1652737526677, "mdate": null, "content": {"title": "Conditional Meta-Learning of Linear Representations", "abstract": "Standard meta-learning for representation learning aims to find a common representation to be shared across multiple tasks. The effectiveness of these methods is often limited when the nuances of the tasks\u2019 distribution cannot be captured by a single representation. In this work we overcome this issue by inferring a conditioning function, mapping the tasks\u2019 side information (such as the tasks\u2019 training dataset itself) into a representation tailored to the task at hand. We study environments in which our conditional strategy outperforms standard meta-learning, such as those in which tasks can be organized in separate clusters according to the representation they share. We then propose a meta-algorithm capable of leveraging this advantage in practice. In the unconditional setting, our method yields a new estimator enjoying faster learning rates and requiring less hyper-parameters to tune than current state-of-the-art methods. Our results are supported by preliminary experiments."}}
{"id": "24HwPLgt7bZ", "cdate": 1617956723509, "mdate": null, "content": {"title": "Conditional Meta-Learning of Linear Representations", "abstract": "Standard meta-learning for representation learning aims to find a common representation to be shared across multiple tasks. The effectiveness of these methods is often limited when the nuances of the tasks\u2019 distribution cannot be captured by a single representation. In this work we overcome this issue by inferring a conditioning function, mapping the tasks\u2019 side information (such as the tasks\u2019 training dataset itself) into a representation tailored to the task at hand. We study environments in which our conditional strategy outperforms standard meta-learning, such as those in which tasks can be organized in separate clusters according to the representation they share. We then propose a meta-algorithm capable of leveraging this advantage in practice. In the unconditional setting, our method yields a new estimator enjoying faster learning rates and requiring less hyper-parameters to tune than current state-of-the-art methods. Our results are supported by preliminary experiments."}}
{"id": "L4MgtVpgmdX", "cdate": 1609459200000, "mdate": 1682452348361, "content": {"title": "Conditional Meta-Learning of Linear Representations", "abstract": "Standard meta-learning for representation learning aims to find a common representation to be shared across multiple tasks. The effectiveness of these methods is often limited when the nuances of the tasks' distribution cannot be captured by a single representation. In this work we overcome this issue by inferring a conditioning function, mapping the tasks' side information (such as the tasks' training dataset itself) into a representation tailored to the task at hand. We study environments in which our conditional strategy outperforms standard meta-learning, such as those in which tasks can be organized in separate clusters according to the representation they share. We then propose a meta-algorithm capable of leveraging this advantage in practice. In the unconditional setting, our method yields a new estimator enjoying faster learning rates and requiring less hyper-parameters to tune than current state-of-the-art methods. Our results are supported by preliminary experiments."}}
{"id": "vU3vgPkssSa", "cdate": 1577836800000, "mdate": null, "content": {"title": "The Advantage of Conditional Meta-Learning for Biased Regularization and Fine-Tuning", "abstract": "Biased regularization and fine-tuning are two recent meta-learning approaches. They have been shown to be effective to tackle distributions of tasks, in which the tasks' target vectors are all close to a common meta-parameter vector. However, these methods may perform poorly on heterogeneous environments of tasks, where the complexity of the tasks' distribution cannot be captured by a single meta-parameter vector. We address this limitation by conditional meta-learning, inferring a conditioning function mapping task's side information into a meta-parameter vector that is appropriate for that task at hand. We characterize properties of the environment under which the conditional approach brings a substantial advantage over standard meta-learning and we highlight examples of environments, such as those with multiple clusters, satisfying these properties. We then propose a convex meta-algorithm providing a comparable advantage also in practice. Numerical experiments confirm our theoretical findings."}}
{"id": "NpD5k8WUCF8", "cdate": 1577836800000, "mdate": null, "content": {"title": "Online Parameter-Free Learning of Multiple Low Variance Tasks", "abstract": "We propose a method to learn a common bias vector for a growing sequence of low-variance tasks. Unlike state-of-the-art approaches, our method does not require tuning any hyper-parameter. Our approach is presented in the non-statistical setting and can be of two variants. The \"aggressive\" one updates the bias after each datapoint, the \"lazy\" one updates the bias only at the end of each task. We derive an across-tasks regret bound for the method. When compared to state-of-the-art approaches, the aggressive variant returns faster rates, the lazy one recovers standard rates, but with no need of tuning hyper-parameters. We then adapt the methods to the statistical setting: the aggressive variant becomes a multi-task learning method, the lazy one a meta-learning method. Experiments confirm the effectiveness of our methods in practice."}}
{"id": "8dEIEseN5mm", "cdate": 1577836800000, "mdate": null, "content": {"title": "The Advantage of Conditional Meta-Learning for Biased Regularization and Fine Tuning", "abstract": "Biased regularization and fine tuning are two recent meta-learning approaches. They have been shown to be effective to tackle distributions of tasks, in which the tasks\u2019 target vectors are all close to a common meta-parameter vector. However, these methods may perform poorly on heterogeneous environments of tasks, where the complexity of the tasks\u2019 distribution cannot be captured by a single meta- parameter vector. We address this limitation by conditional meta-learning, inferring a conditioning function mapping task\u2019s side information into a meta-parameter vector that is appropriate for that task at hand. We characterize properties of the environment under which the conditional approach brings a substantial advantage over standard meta-learning and we highlight examples of environments, such as those with multiple clusters, satisfying these properties. We then propose a convex meta-algorithm providing a comparable advantage also in practice. Numerical experiments confirm our theoretical findings."}}
{"id": "1t9bcEh5qYu", "cdate": 1577836800000, "mdate": null, "content": {"title": "Online Parameter-Free Learning of Multiple Low Variance Tasks", "abstract": "We propose a method to learn a common bias vector for a growing sequence of low-variance tasks. Unlike state-of-the-art approaches, our method does not require tuning any hyper-parameter. Our appro..."}}
{"id": "kjA7u-uxt4d", "cdate": 1546300800000, "mdate": null, "content": {"title": "Online-Within-Online Meta-Learning", "abstract": "We study the problem of learning a series of tasks in a fully online Meta-Learning setting. The goal is to exploit similarities among the tasks to incrementally adapt an inner online algorithm in order to incur a low averaged cumulative error over the tasks. We focus on a family of inner algorithms based on a parametrized variant of online Mirror Descent. The inner algorithm is incrementally adapted by an online Mirror Descent meta-algorithm using the corresponding within-task minimum regularized empirical risk as the meta-loss. In order to keep the process fully online, we approximate the meta-subgradients by the online inner algorithm. An upper bound on the approximation error allows us to derive a cumulative error bound for the proposed method. Our analysis can also be converted to the statistical setting by online-to-batch arguments. We instantiate two examples of the framework in which the meta-parameter is either a common bias vector or feature map. Finally, preliminary numerical experiments confirm our theoretical findings."}}
{"id": "hOXbrQn2Oe", "cdate": 1546300800000, "mdate": 1682452348323, "content": {"title": "Efficient Lifelong Learning Algorithms: Regret Bounds and Statistical Guarantees", "abstract": ""}}
{"id": "BkZTyi-_WH", "cdate": 1546300800000, "mdate": null, "content": {"title": "Learning-to-Learn Stochastic Gradient Descent with Biased Regularization", "abstract": "We study the problem of learning-to-learn: infer- ring a learning algorithm that works well on a family of tasks sampled from an unknown distribution. As class of algorithms we consider Stochastic ..."}}
