{"id": "WIjuP6PvrJ_", "cdate": 1672531200000, "mdate": 1680553334215, "content": {"title": "Translating Natural Language to Planning Goals with Large-Language Models", "abstract": ""}}
{"id": "82N_rasrUT_", "cdate": 1652737868305, "mdate": null, "content": {"title": "Explicable Policy Search", "abstract": "Human teammates often form conscious and subconscious expectations of each other during interaction. Teaming success is contingent on whether such expectations can be met. Similarly, for an intelligent agent to operate beside a human, it must consider the human\u2019s expectation of its behavior. Disregarding such expectations can lead to the loss of trust and degraded team performance. A key challenge here is that the human\u2019s expectation may not align with the agent\u2019s optimal behavior, e.g., due to the human\u2019s partial or inaccurate understanding of the task domain. Prior work on explicable planning described the ability of agents to respect their human teammate\u2019s expectations by trading off task performance for more expected or \u201cexplicable\u201d behaviors. In this paper, we introduce Explicable Policy Search (EPS) to significantly extend such an ability to stochastic domains in a reinforcement learning (RL) setting with continuous state and action spaces. Furthermore, in contrast to the traditional RL methods, EPS must at the same time infer the human\u2019s hidden expectations. Such inferences require information about the human\u2019s belief about the domain dynamics and her reward model but directly querying them is impractical. We demonstrate that such information can be necessarily and sufficiently encoded by a surrogate reward function for EPS, which can be learned based on the human\u2019s feedback on the agent\u2019s behavior. The surrogate reward function is then used to reshape the agent\u2019s reward function, which is shown to be equivalent to searching for an explicable policy. We evaluate EPS in a set of navigation domains with synthetic human models and in an autonomous driving domain with a user study. The results suggest that our method can generate explicable behaviors that reconcile task performance with human expectations intelligently and has real-world relevance in human-agent teaming domains."}}
{"id": "KnfXRngc-U", "cdate": 1609459200000, "mdate": 1681113279886, "content": {"title": "Order Matters: Generating Progressive Explanations for Planning Tasks in Human-Robot Teaming", "abstract": ""}}
{"id": "rBoCpU85dp_", "cdate": 1577836800000, "mdate": 1681113279873, "content": {"title": "Online Explanation Generation for Planning Tasks in Human-Robot Teaming", "abstract": ""}}
{"id": "cpKxQ2xBUm", "cdate": 1577836800000, "mdate": 1681113279879, "content": {"title": "What Is It You Really Want of Me? Generalized Reward Learning with Biased Beliefs about Domain Dynamics", "abstract": ""}}
{"id": "Jjeq-CJwKP", "cdate": 1577836800000, "mdate": 1681113279886, "content": {"title": "Correlated Equilibria for Approximate Variational Inference in MRFs", "abstract": ""}}
{"id": "wH9ceu4EnS9", "cdate": 1514764800000, "mdate": 1681113279905, "content": {"title": "Temporal Spatial Inverse Semantics for Robots Communicating with Humans", "abstract": ""}}
{"id": "UPriSVS-id", "cdate": 1514764800000, "mdate": 1681113279902, "content": {"title": "Behavior Explanation as Intention Signaling in Human-Robot Teaming", "abstract": ""}}
{"id": "HDFtau0kkq", "cdate": 1514764800000, "mdate": 1681113279905, "content": {"title": "Interactive Plan Explicability in Human-Robot Teaming", "abstract": ""}}
