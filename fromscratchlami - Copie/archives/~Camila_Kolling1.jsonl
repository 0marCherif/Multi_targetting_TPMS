{"id": "gfm3wPgq_AP", "cdate": 1640995200000, "mdate": 1683879467328, "content": {"title": "Mitigating Bias in Facial Analysis Systems by Incorporating Label Diversity", "abstract": "Facial analysis models are increasingly applied in real-world applications that have significant impact on peoples' lives. However, as literature has shown, models that automatically classify facial attributes might exhibit algorithmic discrimination behavior with respect to protected groups, potentially posing negative impacts on individuals and society. It is therefore critical to develop techniques that can mitigate unintended biases in facial classifiers. Hence, in this work, we introduce a novel learning method that combines both subjective human-based labels and objective annotations based on mathematical definitions of facial traits. Specifically, we generate new objective annotations from two large-scale human-annotated dataset, each capturing a different perspective of the analyzed facial trait. We then propose an ensemble learning method, which combines individual models trained on different types of annotations. We provide an in-depth analysis of the annotation procedure as well as the datasets distribution. Moreover, we empirically demonstrate that, by incorporating label diversity, our method successfully mitigates unintended biases, while maintaining significant accuracy on the downstream tasks."}}
{"id": "doqibbrPfS", "cdate": 1640995200000, "mdate": 1683879467345, "content": {"title": "Measuring Representational Robustness of Neural Networks Through Shared Invariances", "abstract": "A major challenge in studying robustness in deep learning is defining the set of ``meaningless'' perturbations to which a given Neural Network (NN) should be invariant. Most work on robustness implicitly uses a human as the reference model to define such perturbations. Our work offers a new view on robustness by using another reference NN to define the set of perturbations a given NN should be invariant to, thus generalizing the reliance on a reference ``human NN'' to any NN. This makes measuring robustness equivalent to measuring the extent to which two NNs share invariances, for which we propose a measure called STIR. STIR re-purposes existing representation similarity measures to make them suitable for measuring shared invariances. Using our measure, we are able to gain insights into how shared invariances vary with changes in weight initialization, architecture, loss functions, and training dataset. Our implementation is available at: \\url{https://github.com/nvedant07/STIR}."}}
{"id": "8wsBunjGXij", "cdate": 1640995200000, "mdate": 1668021831092, "content": {"title": "Efficient Counterfactual Debiasing for Visual Question Answering", "abstract": "Despite the success of neural architectures for Visual Question Answering (VQA), several recent studies have shown that VQA models are mostly driven by superficial correlations that are learned by exploiting undesired priors within training datasets. They often lack sufficient image grounding or tend to overly-rely on textual information, failing to capture knowledge from the images. This affects their generalization to test sets with slight changes in the distribution of facts. To address such an issue, some bias mitigation methods have relied on new training procedures that are capable of synthesizing counterfactual samples by masking critical objects within the images, and words within the questions, while also changing the corresponding ground truth. We propose a novel model-agnostic counterfactual training procedure, namely Efficient Counterfactual Debiasing (ECD), in which we introduce a new negative answer-assignment mechanism that exploits the probability distribution of the answers based on their frequencies, as well as an improved counterfactual sample synthesizer. Our experiments demonstrate that ECD is a simple, computationally-efficient counterfactual sample-synthesizer training procedure that establishes itself as the new state of the art for unbiased VQA."}}
{"id": "7eOVQy4kdr", "cdate": 1640995200000, "mdate": 1683879467321, "content": {"title": "Measuring Representational Robustness of Neural Networks Through Shared Invariances", "abstract": "A major challenge in studying robustness in deep learning is defining the set of \u201cmeaningless\u201d perturbations to which a given Neural Network (NN) should be invariant. Most work on robustness implic..."}}
{"id": "GoEXeq6T8_", "cdate": 1620667459197, "mdate": null, "content": {"title": "Adaptive cross-modal embeddings for image-text alignment", "abstract": "In this paper, we introduce a novel approach for training image-text alignment models, namely ADAPT. Image-text\nalignment methods are often used for cross-modal retrieval, i.e., to retrieve an image given a query text, or captions that\nsuccessfully label an image. ADAPT is designed to adjust an intermediate representation of instances from a modality\na using an embedding vector of an instance from modality b. Such an adaptation is designed to filter and enhance important information across internal features, allowing for guided vector representations \u2013 which resembles the working of attention modules, though far more computationally efficient. Experimental results on two large-scale Image-Text alignment datasets show that ADAPT-models outperform all the baseline approaches by large margins. Particularly, for Image Retrieval, ADAPT, with a single model, outperforms the state-of-the-art approach by a relative improvement of R@1 \u2248 24% and for Image Annotation, R@1 \u2248 8% on Flickr30k dataset. On MS COCO it provides an improvement of R@1 \u2248 12% for Image Retrieval, and \u2248 7% R@1 for Image Annotation. Code is available at https://github.com/jwehrmann/retrieval.pytorch."}}
{"id": "fqFUwDQOE6", "cdate": 1609459200000, "mdate": 1672238168698, "content": {"title": "Exploring Alignment of Representations with Human Perception", "abstract": ""}}
{"id": "dElgtgv0Wd", "cdate": 1577836800000, "mdate": 1668021831059, "content": {"title": "Component Analysis for Visual Question Answering Architectures", "abstract": "Recent research advances in Computer Vision and Natural Language Processing have introduced novel tasks that are paving the way for solving AI-complete problems. One of those tasks is called Visual Question Answering (VQA). This system takes an image and a free-form, open-ended natural-language question about the image, and produce a natural language answer as the output. Such a task has drawn great attention from the scientific community, which generated a plethora of approaches that aim to improve the VQA predictive accuracy. Most of them comprise three major components: (i) independent representation learning of images and questions; (ii) feature fusion so the model can use information from both sources to answer visual questions; and (iii) the generation of the correct answer in natural language. With so many approaches being recently introduced, it became unclear the real contribution of each component for the ultimate performance of the model. The main goal of this paper is to provide a comprehensive analysis regarding the impact of each component in VQA models. Our extensive set of experiments cover both visual and textual elements, as well as the combination of these representations in form of fusion and attention mechanisms. Our major contribution is to identify core components for training VQA models so as to maximize their predictive performance."}}
{"id": "as65HcIUWfn", "cdate": 1577836800000, "mdate": null, "content": {"title": "How Does Computer Animation Affect Our Perception of Emotions in Video Summarization?", "abstract": "With the exponential growth of film productions and the popularization of the web, the summary of films has become a useful and important resource. Movies data specifically has become one of the most entertaining sources for viewers, especially during quarantine. However, browsing a movie in enormous collections and searching for a desired scene within a complete movie is a tedious and time-consuming task. As a result, automatic and personalized movie summarization has become a common research topic. In this paper, we focus on emotion summarization for videos with one shot and apply three independent methods for its summarization. We provide two different ways to visualize the main emotions of the generated summary and compare both approaches. The first one uses the original frames of the video and the other uses an open source facial animation tool to create a virtual assistant that provides the emotion summarization. For evaluation, we conducted an extrinsic evaluation using a questionnaire to measure the quality of each generated video summary. Experimental results show that even though both videos had similar answers, a different technique for each video had the most satisfying and informative summary."}}
{"id": "CY6ElUbb9jE", "cdate": 1577836800000, "mdate": null, "content": {"title": "Adaptive Cross-Modal Embeddings for Image-Text Alignment", "abstract": "a using an embedding vector of an instance from modality b. Such an adaptation is designed to filter and enhance important information across internal features, allowing for guided vector representations \u2013 which resembles the working of attention modules, though far more computationally efficient. Experimental results on two large-scale Image-Text alignment datasets show that ADAPT models outperform all the baseline approaches by large margins. Particularly, for Image Retrieval, ADAPT, with a single model, outperforms the state-of-the-art approach by a relative improvement of R@1 \u2248 24% and for Image Annotation, R@1 \u2248 8% on Flickr30k dataset. On MS COCO it provides an improvement of R@1 \u2248 12% for Image Retrieval, and \u2248 7% R@1 for Image Annotation. Code is available at https://github.com/jwehrmann/retrieval.pytorch."}}
{"id": "3UHXGkivpL", "cdate": 1577836800000, "mdate": null, "content": {"title": "Component Analysis for Visual Question Answering Architectures", "abstract": "Recent research advances in Computer Vision and Natural Language Processing have introduced novel tasks that are paving the way for solving AI-complete problems. One of those tasks is called Visual Question Answering (VQA). A VQA system must take an image and a free-form, open-ended natural language question about the image, and produce a natural language answer as the output. Such a task has drawn great attention from the scientific community, which generated a plethora of approaches that aim to improve the VQA predictive accuracy. Most of them comprise three major components: (i) independent representation learning of images and questions; (ii) feature fusion so the model can use information from both sources to answer visual questions; and (iii) the generation of the correct answer in natural language. With so many approaches being recently introduced, it became unclear the real contribution of each component for the ultimate performance of the model. The main goal of this paper is to provide a comprehensive analysis regarding the impact of each component in VQA models. Our extensive set of experiments cover both visual and textual elements, as well as the combination of these representations in form of fusion and attention mechanisms. Our major contribution is to identify core components for training VQA models so as to maximize their predictive performance."}}
