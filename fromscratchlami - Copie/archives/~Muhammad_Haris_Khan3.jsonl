{"id": "x4MbtV1AwTv", "cdate": 1695501369709, "mdate": 1695501369709, "content": {"title": "Domain Adaptive Object Detection via Balancing between Self-Training and Adversarial Learning", "abstract": "Deep learning based object detectors struggle generalizing to a new target domain bearing significant variations in object and\nbackground. Most current methods align domains by using image or instance-level adversarial feature alignment. This often suffers due to\nunwanted background and lacks class-specific alignment. A straightforward approach to promote class-level alignment is to use high\nconfidence predictions on unlabeled domain as pseudo-labels. These predictions are often noisy since model is poorly calibrated under\ndomain shift. In this paper, we propose to leverage model\u2019s predictive uncertainty to strike the right balance between adversarial feature\nalignment and class-level alignment. We develop a technique to quantify predictive uncertainty on class assignments and bounding-box\npredictions. Model predictions with low uncertainty are used to generate pseudo-labels for self-training, whereas the ones with higher\nuncertainty are used to generate tiles for adversarial feature alignment. This synergy between tiling around uncertain object regions and\ngenerating pseudo-labels from highly certain object regions allows capturing both image and instance-level context during the model\nadaptation. We report thorough ablation study to reveal the impact of different components in our approach. Results on five diverse and\nchallenging adaptation scenarios show that our approach outperforms existing state-of-the-art methods with noticeable margins"}}
{"id": "3-AGUqFKRS", "cdate": 1668689581058, "mdate": 1668689581058, "content": {"title": "Cross-Domain Transferability of Adversarial Perturbations", "abstract": "Adversarial examples reveal the blind spots of deep neural networks (DNNs) and represent a major concern for security-critical applications. The transferability of adversarial examples makes real-world attacks possible in black-box settings, where the attacker is forbidden to access the internal parameters of the model. The underlying assumption in most adversary generation methods, whether learning an instance-specific or an instance-agnostic perturbation, is the direct or indirect reliance on the original domain-specific data distribution. In this work, for the first time, we demonstrate the existence of domain-invariant adversaries, thereby showing common adversarial space among different datasets and models. To this end, we propose a framework capable of launching highly transferable attacks that crafts adversarial patterns to mislead networks trained on wholly different domains. For instance, an adversarial function learned on Paintings, Cartoons or Medical images can successfully perturb ImageNet samples to fool the classifier, with success rates as high as 99%(). The core of our proposed adversarial function is a generative network that is trained using a relativistic supervisory signal that enables domain-invariant perturbations. Our approach sets the new state-of-the-art for fooling rates, both under the white-box and black-box scenarios. Furthermore, despite being an instance-agnostic perturbation function, our attack outperforms the conventionally much stronger instance-specific attack methods."}}
{"id": "sf-9HDhApW", "cdate": 1668688665206, "mdate": 1668688665206, "content": {"title": "Self-Distilled Vision Transformer for Domain Generalization", "abstract": "In the recent past, several domain generalization (DG) methods have been proposed, showing encouraging performance, however, almost all of them build on convolutional neural networks (CNNs). There is little to no progress on studying the DG performance of vision transformers (ViTs), which are challenging the supremacy of CNNs on standard benchmarks, often built on i.i.d assumption. This renders the real-world deployment of ViTs doubtful. In this paper, we attempt to explore ViTs towards addressing the DG problem. Similar to CNNs, ViTs also struggle in out-of-distribution scenarios and the main culprit is overfitting to source domains. Inspired by the modular architecture of ViTs, we propose a simple DG approach for ViTs, coined as self-distillation for ViTs. It reduces the overfitting of source domains by easing the learning of input-output mapping problem through curating non-zero entropy supervisory signals for intermediate transformer blocks. Further, it does not introduce any new parameters and can be seamlessly plugged into the modular composition of different ViTs. We empirically demonstrate notable performance gains with different DG baselines and various ViT backbones in five challenging datasets. Moreover, we report favorable performance against recent state-of-the-art DG methods."}}
{"id": "TlpZtF2tB2z", "cdate": 1668525222447, "mdate": 1668525222447, "content": {"title": "Deep contextual attention for human-object interaction detection", "abstract": "Human-object interaction detection is an important and relatively new class of visual relationship detection tasks, essential for deeper scene understanding. Most existing approaches decompose the problem into object localization and interaction recognition. Despite showing progress, these approaches only rely on the appearances of humans and objects and overlook the available context information, crucial for capturing subtle interactions between them. We propose a contextual attention framework for human-object interaction detection. Our approach leverages context by learning contextually-aware appearance features for human and object instances. The proposed attention module then adaptively selects relevant instance-centric context information to highlight image regions likely to contain human-object interactions. Experiments are performed on three benchmarks: V-COCO, HICO-DET and HCVRD. Our approach outperforms the state-of-the-art on all datasets. On the V-COCO dataset, our method achieves a relative gain of 4.4% in terms of role mean average precision (mAP role), compared to the existing best approach."}}
{"id": "Lp109Bn7o_", "cdate": 1667362269316, "mdate": 1667362269316, "content": {"title": "Pushing the boundaries of audiovisual word recognition using residual networks and LSTMs", "abstract": "Visual and audiovisual speech recognition are witnessing a renaissance which is largely due to the ad-vent of deep learning methods. In this paper, we present a deep learning architecture for lipreading and audiovisual word recognition, which combines Residual Networks equipped with spatiotemporal in-put layers and Bidirectional LSTMs. The lipreading architecture attains 11.92% misclassification rate on the challenging Lipreading-In-The-Wild database, which is composed of excerpts from BBC-TV ,each containing one of the 500 target words.  Audiovisual experiments are performed using both intermediate and late integration, as well as several types and levels of environmental noise, and notable improvements over the audio-only network are reported, even in the case of clean speech.  A further analysis on the utility of target word boundaries is provided, as well as on the capacity of the network in modeling the linguistic context of the target word.  Finally, we examine difficult word pairs and discuss how visual information helps towards attaining higher recognition accuracy."}}
{"id": "a7YeDeacHpL", "cdate": 1652737678034, "mdate": null, "content": {"title": "Towards Improving Calibration in Object Detection Under Domain Shift", "abstract": "With deep neural network based solution more readily being incorporated in real-world applications, it has been pressing requirement that predictions by such models, especially in safety-critical environments, be  highly accurate and well-calibrated. Although some techniques addressing DNN calibration have been proposed, they are only limited to visual classification applications and in-domain predictions. Unfortunately, very little to no attention is paid towards addressing calibration of DNN-based visual object detectors, that occupy similar space and importance in many decision making systems as their visual classification counterparts. In this work, we study the calibration of DNN-based object detection models, particularly under domain shift. To this end, we first propose a new, plug-and-play, train-time calibration loss for object detection (coined as TCD). It can be used with various application-specific loss functions as an auxiliary loss function to improve detection calibration. Second, we devise a new implicit technique for improving calibration in self-training based domain adaptive detectors, featuring a new uncertainty quantification mechanism for object detection. We demonstrate TCD is capable of enhancing calibration with notable margins (1) across different DNN-based object detection paradigms both in in-domain and out-of-domain predictions, and (2) in different domain-adaptive detectors across challenging adaptation scenarios. Finally, we empirically show that our implicit calibration technique can be used in tandem with TCD during adaptation to further boost calibration in diverse domain shift scenarios."}}
{"id": "yXT-hNAW_v", "cdate": 1640995200000, "mdate": 1667980709772, "content": {"title": "Towards Improving Calibration in Object Detection Under Domain Shift", "abstract": "With deep neural network based solution more readily being incorporated in real-world applications, it has been pressing requirement that predictions by such models, especially in safety-critical environments, be highly accurate and well-calibrated. Although some techniques addressing DNN calibration have been proposed, they are only limited to visual classification applications and in-domain predictions. Unfortunately, very little to no attention is paid towards addressing calibration of DNN-based visual object detectors, that occupy similar space and importance in many decision making systems as their visual classification counterparts. In this work, we study the calibration of DNN-based object detection models, particularly under domain shift. To this end, we first propose a new, plug-and-play, train-time calibration loss for object detection (coined as TCD). It can be used with various application-specific loss functions as an auxiliary loss function to improve detection calibration. Second, we devise a new implicit technique for improving calibration in self-training based domain adaptive detectors, featuring a new uncertainty quantification mechanism for object detection. We demonstrate TCD is capable of enhancing calibration with notable margins (1) across different DNN-based object detection paradigms both in in-domain and out-of-domain predictions, and (2) in different domain-adaptive detectors across challenging adaptation scenarios. Finally, we empirically show that our implicit calibration technique can be used in tandem with TCD during adaptation to further boost calibration in diverse domain shift scenarios."}}
{"id": "jJhxBBadwR", "cdate": 1640995200000, "mdate": 1667980709694, "content": {"title": "HMFS: Hybrid Masking for Few-Shot Segmentation", "abstract": "We study few-shot semantic segmentation that aims to segment a target object from a query image when provided with a few annotated support images of the target class. Several recent methods resort to a feature masking (FM) technique to discard irrelevant feature activations which eventually facilitates the reliable prediction of segmentation mask. A fundamental limitation of FM is the inability to preserve the fine-grained spatial details that affect the accuracy of segmentation mask, especially for small target objects. In this paper, we develop a simple, effective, and efficient approach to enhance feature masking (FM). We dub the enhanced FM as hybrid masking (HM). Specifically, we compensate for the loss of fine-grained spatial details in FM technique by investigating and leveraging a complementary basic input masking method. Experiments have been conducted on three publicly available benchmarks with strong few-shot segmentation (FSS) baselines. We empirically show improved performance against the current state-of-the-art methods by visible margins across different benchmarks. Our code and trained models are available at: https://github.com/moonsh/HM-Hybrid-Masking"}}
{"id": "Yu9Bg4NMZ5M", "cdate": 1640995200000, "mdate": 1667980709694, "content": {"title": "Moving objects segmentation using generative adversarial modeling", "abstract": ""}}
{"id": "WEBkpW9Cjc", "cdate": 1640995200000, "mdate": 1667980709776, "content": {"title": "Video Instance Segmentation via Multi-Scale Spatio-Temporal Split Attention Transformer", "abstract": "State-of-the-art transformer-based video instance segmentation (VIS) approaches typically utilize either single-scale spatio-temporal features or per-frame multi-scale features during the attention computations. We argue that such an attention computation ignores the multi-scale spatio-temporal feature relationships that are crucial to tackle target appearance deformations in videos. To address this issue, we propose a transformer-based VIS framework, named MS-STS VIS, that comprises a novel multi-scale spatio-temporal split (MS-STS) attention module in the encoder. The proposed MS-STS module effectively captures spatio-temporal feature relationships at multiple scales across frames in a video. We further introduce an attention block in the decoder to enhance the temporal consistency of the detected instances in different frames of a video. Moreover, an auxiliary discriminator is introduced during training to ensure better foreground-background separability within the multi-scale spatio-temporal feature space. We conduct extensive experiments on two benchmarks: Youtube-VIS (2019 and 2021). Our MS-STS VIS achieves state-of-the-art performance on both benchmarks. When using the ResNet50 backbone, our MS-STS achieves a mask AP of 50.1%, outperforming the best reported results in literature by 2.7% and by 4.8% at higher overlap threshold of $$\\text {AP}_{\\texttt{75}}$$ , while being comparable in model size and speed on Youtube-VIS 2019 val. set. When using the Swin Transformer backbone, MS-STS VIS achieves mask AP of 61.0% on Youtube-VIS 2019 val. set. Source code is available at https://github.com/OmkarThawakar/MSSTS-VIS ."}}
