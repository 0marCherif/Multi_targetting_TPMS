{"id": "UfGbICkkWE", "cdate": 1672531200000, "mdate": 1683905310947, "content": {"title": "Constant regret for sequence prediction with limited advice", "abstract": "We investigate the problem of cumulative regret minimization for individual sequence prediction with respect to the best expert in a finite family of size K under limited access to information. We ..."}}
{"id": "q4l5hGFWMY", "cdate": 1640995200000, "mdate": 1683905310945, "content": {"title": "Contributions to Frugal Learning. (Quelques contributions \u00e0 l'apprentissage frugal)", "abstract": ""}}
{"id": "7m6qvNqFjr", "cdate": 1621630189277, "mdate": null, "content": {"title": "Fast rates for prediction with limited expert advice", "abstract": "We investigate the problem of minimizing the excess generalization error with respect to the best expert prediction in a finite family in the stochastic setting,  under limited access to information. We consider that the learner has only access to a limited number of expert advices per training round, as well as for prediction. Assuming that the loss function is Lipschitz and strongly convex, we show that if we are allowed to see the advice of only one expert per round in the training phase, or to use the advice of only one expert for prediction in the test phase, the worst-case excess risk is ${\\Omega}(1/\\sqrt{T})$ with probability lower bounded by a constant. However, if we are allowed to see at least two actively chosen expert advices per training round and use at least two experts for prediction, the fast rate $\\mathcal{O}(1/T)$ can be achieved. We design novel algorithms achieving this rate in this setting, and in the setting where the learner have a budget constraint on the total number of observed experts advices,  and give precise instance-dependent bounds on the number of training rounds needed to achieve a given generalization error precision."}}
{"id": "f-KOXoVgRJW", "cdate": 1609459200000, "mdate": 1683905310947, "content": {"title": "Fast rates for prediction with limited expert advice", "abstract": "We investigate the problem of minimizing the excess generalization error with respect to the best expert prediction in a finite family in the stochastic setting, under limited access to information. We consider that the learner has only access to a limited number of expert advices per training round, as well as for prediction. Assuming that the loss function is Lipschitz and strongly convex, we show that if we are allowed to see the advice of only one expert per round in the training phase, or to use the advice of only one expert for prediction in the test phase, the worst-case excess risk is ${\\Omega}(1/\\sqrt{T})$ with probability lower bounded by a constant. However, if we are allowed to see at least two actively chosen expert advices per training round and use at least two experts for prediction, the fast rate $\\mathcal{O}(1/T)$ can be achieved. We design novel algorithms achieving this rate in this setting, and in the setting where the learner have a budget constraint on the total number of observed experts advices, and give precise instance-dependent bounds on the number of training rounds needed to achieve a given generalization error precision."}}
{"id": "ub-4JBRvY6", "cdate": 1577836800000, "mdate": 1683905311003, "content": {"title": "Online Orthogonal Matching Pursuit", "abstract": "Greedy algorithms for feature selection are widely used for recovering sparse high-dimensional vectors in linear models. In classical procedures, the main emphasis was put on the sample complexity, with little or no consideration of the computation resources required. We present a novel online algorithm: Online Orthogonal Matching Pursuit (OOMP) for online support recovery in the random design setting of sparse linear regression. Our procedure selects features sequentially, alternating between allocation of samples only as needed to candidate features, and optimization over the selected set of variables to estimate the regression coefficients. Theoretical guarantees about the output of this algorithm are proven and its computational complexity is analysed."}}
