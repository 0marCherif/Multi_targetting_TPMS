{"id": "qperoXi2g5W", "cdate": 1695441787392, "mdate": 1695441787392, "content": {"title": "Self-supervised AutoFlow", "abstract": "Recently, AutoFlow has shown promising results on learning a training set for optical flow, but requires ground truth labels in the target domain to compute its search metric. Observing a strong correlation between the ground truth search metric and self-supervised losses, we introduce self-supervised AutoFlow to handle real-world videos without ground truth labels. Using self-supervised loss as the search metric, our self-supervised AutoFlow performs on par with AutoFlow on Sintel and KITTI where ground truth is available, and performs better on the real-world DAVIS dataset. We further explore using self-supervised AutoFlow in the (semi-) supervised setting and obtain competitive results against the state of the art."}}
{"id": "0E25DxoDdQd", "cdate": 1695441677431, "mdate": 1695441677431, "content": {"title": "A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence", "abstract": "Text-to-image diffusion models have made significant advances in generating and editing high-quality images. As a result, numerous approaches have explored the ability of diffusion model features to understand and process single images for downstream tasks, e.g., classification, semantic segmentation, and stylization. However, significantly less is known about what these features reveal across multiple, different images and objects. In this work, we exploit Stable Diffusion (SD) features for semantic and dense correspondence and discover that with simple post-processing, SD features can perform quantitatively similar to SOTA representations. Interestingly, the qualitative analysis reveals that SD features have very different properties compared to existing representation learning features, such as the recently released DINOv2: while DINOv2 provides sparse but accurate matches, SD features provide high-quality spatial information but sometimes inaccurate semantic matches. We demonstrate that a simple fusion of these two features works surprisingly well, and a zero-shot evaluation using nearest neighbors on these fused features provides a significant performance gain over state-of-the-art methods on benchmark datasets, e.g., SPair-71k, PF-Pascal, and TSS. We also show that these correspondences can enable interesting applications such as instance swapping in two images."}}
{"id": "VAr19J0s2J", "cdate": 1684991690811, "mdate": 1684991690811, "content": {"title": "Diffusion Models: A Comprehensive Survey of Methods and Applications", "abstract": "Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many\napplications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly\nexpanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood\nestimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative\nmodels for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer\nvision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This\nsurvey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing\nto potential areas for further exploration."}}
{"id": "eqqc0To5CZ", "cdate": 1683719709272, "mdate": 1683719709272, "content": {"title": "Learning Object-level Point Augmentor for Semi-supervised 3D Object Detection", "abstract": "Semi-supervised object detection is important for 3D scene understanding because obtaining large-scale 3D bounding box annotations on point clouds is time-consuming and labor-intensive. Existing semi-supervised methods usually employ teacher-student knowledge distillation together with an augmentation strategy to leverage unlabeled point clouds. However, these methods adopt global augmentation with scene-level transformations and hence are sub-optimal for instance-level object detection. In this work, we propose an object-level point augmentor (OPA) that performs local transformations for semi-supervised 3D object detection. In this way, the resultant augmentor is derived to emphasize object instances rather than irrelevant backgrounds, making the augmented data more useful for object detector training. Extensive experiments on the ScanNet and SUN RGB-D datasets show that the proposed OPA performs favorably against the state-of-the-art methods under various experimental settings. The source code will be available at https://github.com/nomiaro/OPA."}}
{"id": "w25Q9Ttjrs", "cdate": 1675715128588, "mdate": null, "content": {"title": "Exploiting Category Names for Few-Shot Classification with Vision-Language Models", "abstract": "Vision-language foundation models pretrained on large-scale data provide a powerful tool for many visual understanding tasks.\nNotably, many vision-language models build two encoders (visual and textual) that can map two modalities into the same embedding space. As a result, the learned representations achieve good zero-shot performance on tasks like image classification. However, when there are only a few examples per category, the potential of large vision-language models is often underperformed, mainly due to the gap between a large number of parameters and a relatively small amount of training data. This paper shows that we can significantly improve the performance of few-shot classification by using the category names to initialize the classification head.  With the proposed category name initialization method, our model obtains the state-of-the-art performance on a number of few-shot image classification benchmarks (e.g., 87.37\\% on ImageNet and 96.08\\% on Stanford Cars, both using five-shot learning). "}}
{"id": "oyFeEp63Yys", "cdate": 1668734782408, "mdate": null, "content": {"title": "Improving Zero-shot Generalization and Robustness of Multi-modal Models", "abstract": "Multi-modal image-text models such as CLIP and LiT have demonstrated impressive performance on image classification benchmarks and their zero-shot generalization ability is particularly exciting. While the top-5 zero-shot accuracies of these models are very high, the top-1 accuracies are much lower (over 25% gap in some cases). We investigate the reason for this performance gap and find that many of the failure cases are caused by ambiguity in the text prompts. First,we develop a simple and efficient zero-shot post-hoc method to identify images where the top-1 prediction is likely to be incorrect, by measuring consistency of the predictions w.r.t. multiple prompts and image transformations. We show that our procedure better predicts mistakes, outperforming the popular max logit baseline on selective prediction tasks. Next, we propose a simple and efficient way to improve accuracy on such uncertain images by making use of the WordNet hierarchy; specifically we use information from parents in the hierarchy to add superclass to prompts, and use information from children in the hierarchy to devise fine-grained prompts. We conduct experiments on both CLIP and LiT models with five different ImageNet-based datasets. For CLIP, our method improves the top-1 accuracy by 17.13% on the uncertain subset and 3.6% on the entire ImageNet validation set. We also show that our method consistently improvement on other ImageNet shifted datasets and other model architectures such as LiT. Our proposed method is hyperparameter-free, requires no additional model training and can be easily scaled to other large multi-modal architectures. Code for our experiments is opensourced at link hidden for anonymity."}}
{"id": "ANJyIk1rndP", "cdate": 1667811785604, "mdate": 1667811785604, "content": {"title": "Decoupled Dynamic Filter Networks", "abstract": "Convolution is one of the basic building blocks of CNN architectures. Despite its common use, standard convolution has two main shortcomings: Content-agnostic and Computation-heavy. Dynamic filters are content-adaptive, while further increasing the computational overhead. Depth-wise convolution is a lightweight variant, but it usually leads to a drop in CNN performance or requires a larger number of channels. In this work, we propose the Decoupled Dynamic Filter (DDF) that can simultaneously tackle both of these shortcomings. Inspired by recent advances in attention, DDF decouples a depth-wise dynamic filter into spatial and channel dynamic filters. This decomposition considerably reduces the number of parameters and limits computational costs to the same level as depth-wise convolution. Meanwhile, we observe a significant boost in performance when replacing standard convolution with DDF in classification networks. ResNet50 / 101 get improved by 1.9% and 1.3% on the top-1 accuracy, while their computational costs are reduced by nearly half. Experiments on the detection and joint upsampling networks also demonstrate the superior performance of the DDF upsampling variant (DDF-Up) in comparison with standard convolution and specialized content-adaptive layers."}}
{"id": "g0wVh8fVg_x", "cdate": 1667344535248, "mdate": 1667344535248, "content": {"title": "Show, Match and Segment: Joint Weakly Supervised Learning of Semantic Matching and Object Co-segmentation", "abstract": "We present an approach for jointly matching and segmenting object instances of the same category within a collection of images. In contrast to existing algorithms that tackle the tasks of semantic matching and object co-segmentation in isolation, our method exploits the complementary nature of the two tasks. The key insights of our method are two-fold. First, the estimated dense correspondence fields from semantic matching provide supervision for object co-segmentation by enforcing consistency between the predicted masks from a pair of images. Second, the predicted object masks from object co-segmentation in turn allow us to reduce the adverse effects due to background clutters for improving semantic matching. Our model is end-to-end trainable and does not require supervision from manually annotated correspondences and object masks. We validate the efficacy of our approach on five benchmark datasets: TSS, Internet, PF-PASCAL, PF-WILLOW, and SPair-71k, and show that our algorithm performs favorably against the state-of-the-art methods on both semantic matching and object co-segmentation tasks."}}
{"id": "VXyzRA_Zaj1", "cdate": 1663850342297, "mdate": null, "content": {"title": "Burstormer: Burst Image Restoration and Enhancement Transformer", "abstract": "On a shutter press, modern handheld cameras capture multiple images in rapid succession and merge them to generate a single image. However, individual frames in a burst are misaligned due to inevitable motions and contain multiple degradations. The challenge is to properly align the successive image shots and merge their complimentary information to achieve high-quality outputs. Towards this direction, we propose Burstormer: a novel transformer-based architecture for burst image restoration and enhancement. In comparison to existing works, our approach exploits multi-scale local and non-local features to achieve improved alignment and feature fusion. Our key idea is to enable inter-frame communication in the burst neighborhoods for information aggregation and progressive fusion while modeling the burst-wide context. However, the input burst frames need to be properly aligned before fusing their information. Therefore, we propose an enhanced deformable alignment module for aligning burst features with regards to the reference frame. Unlike existing techniques, the proposed alignment module not only aligns burst features but also exchanges feature information and maintains focused communication with the reference frame through the proposed reference-based feature enrichment mechanism. This additional exchange of information helps in aligning multi-frame features under complex motions. After multi-level alignment and enrichment, we re-emphasize on inter-frame communication within burst frames using a new cyclic burst sampling technique. Finally, the inter-frame information is aggregated using our proposed burst feature fusion module followed by progressive increase in the spatial resolution by shuffling the feature information available in burst frames. The proposed Burstormer outperforms the existing state-of-the-art approaches on three popular tasks of burst super-resolution, burst denoising and burst low-light enhancement. Our codes will\nbe made public."}}
{"id": "u2e4grt3aKm", "cdate": 1663850143114, "mdate": null, "content": {"title": "Multimodal Open-Vocabulary Video Classification via Vision and Language Models", "abstract": "Utilizing vision and language models (VLMs) pre-trained on internet-scale image-text pairs is becoming a promising paradigm for open-vocabulary vision tasks. This work conducts an extensive study for multimodal open-vocabulary video classification via pre-trained VLMs by leveraging motion and audio that naturally exist in the video. We design an asymmetrical cross-modal fusion mechanism to aggregate multimodal information differently for video and optical flow / audio. Experiments on Kinetics and VGGSound show that introducing more modalities significantly improves the accuracy on seen classes, while generalizing better to unseen classes over existing approaches. Despite its simplicity, our method achieves state-of-the-art results on UCF and HMDB zero-shot video action recognition benchmarks, significantly outperforming traditional zero-shot techniques, video-text pre-training methods and recent VLM-based approaches. Code and models will be released."}}
