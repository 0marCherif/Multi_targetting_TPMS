{"id": "UmHG2bD7X3w", "cdate": 1663850340962, "mdate": null, "content": {"title": "Dynamic Scheduled Sampling with Imitation Loss for Neural Text Generation", "abstract": "State-of-the-art neural text generation models are typically trained to maximize the likelihood of each token in the ground-truth sequence conditioned on the previous target tokens. However, during inference, the model needs to make a prediction conditioned on the tokens generated by itself. This train-test discrepancy is referred to as exposure bias. Scheduled sampling is a curriculum learning strategy that gradually exposes the model to its own predictions during training to mitigate this bias. Most of the proposed approaches design a scheduler based on training steps, which generally requires careful tuning depending on the training setup. In this work, we introduce Dynamic Scheduled Sampling with Imitation Loss (DySI), which maintains the schedule based solely on the training time accuracy, while enhancing the curriculum learning by introducing an imitation loss, which attempts to make the behavior of the decoder indistinguishable from the behavior of a teacher-forced decoder. DySI is universally applicable across training setups with minimal tuning.  Extensive experiments and analysis show that DySI not only achieves notable improvements on standard machine translation benchmarks, but also significantly improves the robustness of other text generation models. "}}
{"id": "cJPkX1g9PQS", "cdate": 1632875520071, "mdate": null, "content": {"title": "Rethinking Self-Supervision Objectives for Generalizable Coherence Modeling", "abstract": "Although large-scale pre-trained neural models have shown impressive performances in a variety of tasks, their ability to generate coherent text that appropriately models discourse phenomena is harder to evaluate and less understood. Given the claims of improved text generation quality across various systems, we consider the coherence evaluation of machine generated text to be one of the principal applications of coherence models that needs to be investigated. We explore training data and self-supervision objectives that result in a model that generalizes well across tasks and can be used off-the-shelf to perform such evaluations.\nPrior work in neural coherence modeling has primarily focused on devising new architectures, and trained the model to distinguish coherent and incoherent text through pairwise self-supervision on the permuted documents task. We instead use a basic model architecture and show significant improvements over state of the art within the same training regime. We then design a harder self-supervision objective by increasing the ratio of negative samples within a contrastive learning setup, and  enhance the model further through automatic hard negative mining coupled with a large global negative queue encoded by a momentum encoder. We show empirically that increasing the density of negative samples improves the basic model, and using a global negative queue further improves and stabilizes the model while training with hard negative samples. We evaluate the coherence model on task-independent test sets that resemble real-world use cases and show significant improvements in coherence evaluations of downstream applications. "}}
{"id": "vT1ij35h5zx", "cdate": 1609459200000, "mdate": 1634474185778, "content": {"title": "Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation", "abstract": "Advanced large-scale neural language models have led to significant success in many language generation tasks. However, the most commonly used training objective, Maximum Likelihood Estimation (MLE..."}}
{"id": "V-C7AVu9II", "cdate": 1609459200000, "mdate": 1634474185771, "content": {"title": "Rethinking Coherence Modeling: Synthetic vs. Downstream Tasks", "abstract": "Tasnim Mohiuddin, Prathyusha Jwalapuram, Xiang Lin, Shafiq Joty. Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 2021."}}
{"id": "FVhZIBWqykk", "cdate": 1601308201371, "mdate": null, "content": {"title": "Resurrecting Submodularity for Neural Text Generation", "abstract": "Submodularity is a desirable property for a variety of objectives in content selection where the current neural encoder-decoder framework is inadequate. We define a class of novel attention mechanisms with submodular functions and in turn, prove the submodularity of the effective neural coverage. The resulting attention module offers an architecturally simple and empirically effective method to improve the coverage of neural text generation. We run experiments on three directed text generation tasks with different levels of recovering rate, across two modalities, three different neural model architectures and two training strategy variations. The results and analyses demonstrate that our method generalizes well across these settings, produces texts of good quality, outperforms comparable baselines and achieves state-of-the-art performance. "}}
{"id": "JAlqRs9duhz", "cdate": 1601308185413, "mdate": null, "content": {"title": "Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation", "abstract": "Advanced large-scale neural language models have led to significant success in many natural language generation tasks. However, the most commonly used training objective, Maximum Likelihood Estimation (MLE), has been shown to be problematic, where the trained model prefers using dull and repetitive phrases. In this work, we introduce ScaleGrad, a modification straight to the gradient of the loss function, to remedy the degeneration issues of the standard MLE objective. By directly maneuvering the gradient information, ScaleGrad makes the model learn to use novel tokens during training. Empirical results show the effectiveness of our method not only in open-ended generation, but also in directed generation. With the simplicity in architecture, our method can serve as a general training objective that is applicable to most of the neural text generation tasks."}}
{"id": "xSdoCI_WZHP", "cdate": 1546300800000, "mdate": 1634474186029, "content": {"title": "A Unified Linear-Time Framework for Sentence-Level Discourse Parsing", "abstract": "Xiang Lin, Shafiq Joty, Prathyusha Jwalapuram, M Saiful Bari. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019."}}
{"id": "94HpuBuoBY9", "cdate": 1546300800000, "mdate": null, "content": {"title": "Hierarchical Pointer Net Parsing", "abstract": "Linlin Liu, Xiang Lin, Shafiq Joty, Simeng Han, Lidong Bing. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019."}}
