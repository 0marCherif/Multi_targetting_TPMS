{"id": "u89Eq-_3oE4", "cdate": 1663850055708, "mdate": null, "content": {"title": "AutoShot: A Short Video Dataset and State-of-the-Art Shot Boundary Detection", "abstract": "The short-form videos have explosive popularity and have dominated the new social media trends. Prevailing short-video platforms, e.g., TikTok, Instagram Reels, and YouTube Shorts, have changed the way we consume and create content. For video content creation and understanding, the shot boundary detection (SBD) is one of the most essential components in various scenarios. In this work, we release a new public Short video sHot bOundary deTection dataset, named SHOT, consisting of 853 complete short videos and 11,606 shot annotations, with 2,716 high quality shot boundary annotations in 200 test videos. Leveraging this new data wealth, we propose to optimize the model design for video SBD, by conducting neural architecture search in a search space encapsulating various advanced 3D ConvNets and Transformers. Our proposed approach, named AutoShot, achieves higher F1 scores than previous state-of-the-art approaches, e.g., outperforming TransNetV2 by 4.2%, when being derived and evaluated on our newly constructed SHOT dataset. Moreover, to validate the generalizability of the AutoShot architecture, we directly evaluate it on another three public datasets: ClipShots, BBC and RAI, and the F1 scores of AutoShot outperform previous state-of-the-art approaches by 1.1%, 0.9% and 1.2%, respectively. The SHOT dataset and code will be released."}}
{"id": "w7Rr0coWx7J", "cdate": 1577836800000, "mdate": null, "content": {"title": "Partial Weight Adaptation for Robust DNN Inference", "abstract": "Mainstream video analytics uses a pre-trained DNN model with an assumption that inference input and training data follow the same probability distribution. However, this assumption does not always hold in the wild: autonomous vehicles may capture video with varying brightness; unstable wireless bandwidth calls for adaptive bitrate streaming of video; and, inference servers may serve inputs from heterogeneous IoT devices/cameras. In such situations, the level of input distortion changes rapidly, thus reshaping the probability distribution of the input. We present GearNN, an adaptive inference architecture that accommodates heterogeneous DNN inputs. GearNN employs an optimization algorithm to identify a small set of \"distortion-sensitive\" DNN parameters, given a memory budget. Based on the distortion level of the input, GearNN then adapts only the distortion-sensitive parameters, while reusing the rest of constant parameters across all input qualities. In our evaluation of DNN inference with dynamic input distortions, GearNN improves the accuracy (mIoU) by an average of 18.12% over a DNN trained with the undistorted dataset and 4.84% over stability training from Google, with only 1.8% extra memory overhead."}}
{"id": "Y1xrE-pFHqf", "cdate": 1577836800000, "mdate": null, "content": {"title": "Partial Weight Adaptation for Robust DNN Inference", "abstract": "Mainstream video analytics uses a pre-trained DNN model with an assumption that inference input and training data follow the same probability distribution. However, this assumption does not always hold in the wild: autonomous vehicles may capture video with varying brightness; unstable wireless bandwidth calls for adaptive bitrate streaming of video; and, inference servers may serve inputs from heterogeneous IoT devices/cameras. In such situations, the level of input distortion changes rapidly, thus reshaping the probability distribution of the input. We present GearNN, an adaptive inference architecture that accommodates DNN inputs with varying distortions. GearNN employs an optimization algorithm to identify a tiny set of \"distortion-sensitive\" DNN parameters, given a memory budget. Based on the distortion level of the input, GearNN then adapts only the distortion-sensitive parameters, while reusing the rest of constant parameters across all input qualities. In our evaluation of DNN inference with dynamic input distortions, GearNN improves the accuracy (mIoU) by an average of 18.12% over a DNN trained with the undistorted dataset and 4.84% over stability training from Google, with only 1.8% extra memory overhead."}}
{"id": "_FDtl6N0JO3", "cdate": 1546300800000, "mdate": null, "content": {"title": "Poster: Optimizing Mobile Video Telephony Using Deep Imitation Learning", "abstract": "Despite the pervasive use of real-time video telephony services, their quality of experience (QoE) remains unsatisfactory, especially over the mobile Internet. We conduct a large-scale measurement campaign on \\appname, an operational mobile video telephony service. Our analysis shows that the application-layer video codec and transport-layer protocols remain highly uncoordinated, which represents one major reason for the low QoE. We thus propose \\name, a machine learning based framework to resolve the issue. We train \\name with the massive data traces from the measurement campaign using a custom-designed imitation learning algorithm, which enables \\name to learn from past experience following an expert's iterative demonstration/supervision. We have implemented and incorporated \\name into the \\appname. Our experiments show that \\name outperforms state-of-the-art solutions, improving video quality while reducing stalling time by multi-folds under various practical scenarios."}}
{"id": "C1WmUKTLib", "cdate": 1546300800000, "mdate": null, "content": {"title": "Source Compression with Bounded DNN Perception Loss for IoT Edge Computer Vision", "abstract": "IoT and deep learning based computer vision together create an immense market opportunity, but running deep neural networks (DNNs) on resource-constrained IoT devices remains challenging. Offloading DNN inference to an edge server is a promising solution, but limited wireless bandwidth bottlenecks its end-to-end performance and scalability. While IoT devices can adopt source compression to cope with the limited bandwidth, existing compression algorithms (or codecs) are not designed for DNN (but for human eyes), and thus, suffer from either low compression rates or high DNN inference errors. This paper presents GRACE, a DNN-aware compression algorithm that facilitates the edge inference by significantly saving the network bandwidth consumption without disturbing the inference performance. Given a target DNN, GRACE (i) analyzes DNN's perception model w.r.t both spatial frequencies and colors and (ii) generates an optimized compression strategy for the model -- one-time offline process. Next, GRACE deploys thus-generated compression strategy at IoT devices (or source) to perform online source compression within the existing codec framework, adding no extra overhead. We prototype GRACE on JPEG (the most popular image codec framework), and our evaluation results show that GRACE indeed achieves the superior compression performance over existing strategies for key DNN applications. For semantic segmentation tasks, GRACE reduces a source size by 23% compared to JPEG with similar interference accuracy (0.38% lower than GRACE). Further, GRACE even achieves 7.5% higher inference accuracy than JPEG with a commonly used quality level of 75 does. For classification tasks, GRACE reduces the bandwidth consumption by 90% over JPEG with the same inference accuracy."}}
{"id": "5dMhH4qV4Ny", "cdate": 1546300800000, "mdate": null, "content": {"title": "Learning to Coordinate Video Codec with Transport Protocol for Mobile Video Telephony", "abstract": "Despite the pervasive use of real-time video telephony services, the users' quality of experience (QoE) remains unsatisfactory, especially over the mobile Internet. Previous work studied the problem via controlled experiments, while a systematic and in-depth investigation in the wild is still missing. To bridge the gap, we conduct a large-scale measurement campaign on \\appname, an operational mobile video telephony service. Our measurement logs fine-grained performance metrics over 1 million video call sessions. Our analysis shows that the application-layer video codec and transport-layer protocols remain highly uncoordinated, which represents one major reason for the low QoE. We thus propose \\name, a machine learning based framework to resolve the issue. Instead of blindly following the transport layer's estimation of network capacity, \\name reviews historical logs of both layers, and extracts high-level features of codec/network dynamics, based on which it determines the highest bitrates for forthcoming video frames without incurring congestion. To attain the ability, we train \\name with the aforementioned massive data traces using a custom-designed imitation learning algorithm, which enables \\name to learn from past experience. We have implemented and incorporated \\name into \\appname. Our experiments show that \\name outperforms state-of-the-art solutions, improving video quality while reducing stalling time by multi-folds under various practical scenarios."}}
{"id": "-82LyAmFVO", "cdate": 1546300800000, "mdate": null, "content": {"title": "An Active-Passive Measurement Study of TCP Performance over LTE on High-speed Rails", "abstract": "High-speed rail (HSR) systems potentially provide a more efficient way of door-to-door transportation than airplane. However, they also pose unprecedented challenges in delivering seamless Internet service for on-board passengers. In this paper, we conduct a large-scale active-passive measurement study of TCP performance over LTE on HSR. Our measurement targets the HSR routes in China operating at above 300 km/h. We performed extensive data collection through both controlled setting and passive monitoring, obtaining 1732.9 GB data collected over 135719 km of trips. Leveraging such a unique dataset, we measure important performance metrics such as TCP goodput, latency, loss rate, as well as key characteristics of TCP flows, application breakdown, and users' behaviors. We further quantitatively study the impact of frequent cellular handover on HSR networking performance, and conduct in-depth examination of the performance of two widely deployed transport-layer protocols: TCP CUBIC and TCP BBR. Our findings reveal the performance of today's commercial HSR networks \"in the wild'', as well as identify several performance inefficiencies, which motivate us to design a simple yet effective congestion control algorithm based on BBR to further boost the throughput by up to 36.5%. They together highlight the need to develop dedicated protocol mechanisms that are friendly to extreme mobility."}}
{"id": "UWbnCfTAbf2", "cdate": 1514764800000, "mdate": null, "content": {"title": "An Active-Passive Measurement Study of TCP Performance over LTE on High-speed Rails", "abstract": "High-speed rail (HSR) systems potentially provide a more efficient way of door-to-door transportation than airplane. However, they also pose unprecedented challenges in delivering seamless Internet service for on-board passengers. In this paper, we conduct a large-scale active-passive measurement study of TCP performance over LTE on HSR. Our measurement targets the HSR routes in China operating at above 300 km/h. We performed extensive data collection through both controlled setting and passive monitoring, obtaining 1732.9 GB data collected over 135719 km of trips. Leveraging such a unique dataset, we measure important performance metrics such as TCP goodput, latency, loss rate, as well as key characteristics of TCP flows, application breakdown, and users' behaviors. We further quantitatively study the impact of frequent cellular handover on HSR networking performance, and conduct in-depth examination of the performance of two widely deployed transport-layer protocols: TCP CUBIC and TCP BBR. Our findings reveal the performance of today's commercial HSR networks \"in the wild\", as well as identify several performance inefficiencies, which motivate us to design a simple yet effective congestion control algorithm based on BBR to further boost the throughput by up to 36.5%. They together highlight the need to develop dedicated protocol mechanisms that are friendly to extreme mobility."}}
{"id": "011cwfgxob0", "cdate": 1514764800000, "mdate": null, "content": {"title": "Wireless CSI-based head tracking in the driver seat", "abstract": "In recent years, augmented reality (AR) has drawn significant attention in the automotive industry and shown great potential for a variety of driver-assistance applications. Tracking the driver's head is vital to seamlessly merge the AR content in the driver's view but still remains an open problem. Specifically, most existing in-vehicle AR solutions rely on cameras for head tracking, which suffer from low sampling rate, weak performance at night, and even high computation costs. Wearing a dedicated headset, on the other hand, is intrusive and inconvenient for daily driving. To overcome these limitations, we propose ViHOT, a novel wireless CSI-based predictive & device-free head tracking system for in-vehicle use. Given that drivers usually mount phones on the dashboard for navigation, ViHOT leverages the CSI of the phone's WiFi signal to track the driver's head, with a light-weight design suited for real-time driving assistance. Thanks to the high WiFi frame rate, ViHOT achieves more than 10x sampling rate over conventional camera-based approaches and thus eliminates motion blur. Moreover, ViHOT's novel tracking algorithm accurately translates CSI phase readings to head orientations (merely 4\u00b0 -10\u00b0 median error) without relying on any head-mounted device."}}
{"id": "t9FLMvYl1pz", "cdate": 1483228800000, "mdate": null, "content": {"title": "POI360: Panoramic Mobile Video Telephony over LTE Cellular Networks", "abstract": "Panoramic or 360\u00b0 video streaming has been supported by a wide range of content providers and mobile devices. Yet existing work primarily focused on streaming on-demand 360\u00b0 videos stored on servers. In this paper, we examine a more challenging problem: Can we stream real-time interactive 360\u00b0 videos across existing LTE cellular networks, so as to trigger new applications such as ubiquitous 360\u00b0 video chat and panoramic outdoor experience sharing? To explore the feasibility and challenges underlying this vision, we design POI360, a portable interactive 360\u00b0 video telephony system that jointly investigates both panoramic video compression and responsive video stream rate control. For the challenge that the legacy spatial compression algorithms for 360\u00b0 video suffer from severe quality fluctuations as the user changes her region-of-interest (ROI), we design an adaptive compression scheme, which dynamically adjusts the compression strategy to stabilize the video quality within ROI under various user input and network condition. In addition, to meet the responsiveness requirement of panoramic video telephony, we leverage the diagnostic statistics on commodity phones to promptly detect cellular link congestion, hence significantly boosting the rate control responsiveness. Extensive field tests for our real-time POI360 prototype validate its effectiveness in enabling panoramic video telephony over the highly dynamic cellular networks."}}
