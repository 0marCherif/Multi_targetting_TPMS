{"id": "AnDDMQgM7-", "cdate": 1686324866775, "mdate": null, "content": {"title": "Equivariant Reinforcement Learning under Partial Observability", "abstract": "Incorporating inductive biases is a promising approach for tackling challenging robot learning domains with sample-efficient solutions. This paper identifies partially observable domains where symmetries can be a useful inductive bias for efficient learning. Specifically, by encoding the equivariance regarding specific group symmetries into the neural networks, our actor-critic reinforcement learning agents can reuse solutions in the past for related scenarios. Consequently, our equivariant agents outperform non-equivariant approaches significantly in terms of sample efficiency and final performance, demonstrated through experiments on a range of robotic tasks in simulation and real hardware."}}
{"id": "OFoo4631KAo", "cdate": 1685111486635, "mdate": null, "content": {"title": "Edge Grasp Network: A Graph-Based SE(3)-invariant Approach to Grasp Detection", "abstract": "Given point cloud input, the problem of 6-DoF grasp pose detection is to identify a set of hand poses in $\\SE(3)$ from which an object can be successfully grasped. This important problem has many practical applications. Here we propose a novel method and neural network model that enables better grasp success rates relative to what is available in the literature. The method takes standard point cloud data as input and works well with single-view point clouds observed from arbitrary viewing directions. Videos and code are available at \\url{https://haojhuang.github.io/edge_grasp_page/}."}}
{"id": "P4MUGRM4Acu", "cdate": 1663850378886, "mdate": null, "content": {"title": "The Surprising Effectiveness of Equivariant Models in Domains with Latent Symmetry", "abstract": "Extensive work has demonstrated that equivariant neural networks can significantly improve sample efficiency and generalization by enforcing an inductive bias in the network architecture. These applications typically assume that the domain symmetry is fully described by explicit transformations of the model inputs and outputs. However, many real-life applications contain only latent or partial symmetries which cannot be easily described by simple transformations of the input. In these cases, it is necessary to learn symmetry in the environment instead of imposing it mathematically on the network architecture. We discover, surprisingly, that imposing equivariance constraints that do not exactly match the domain symmetry is very helpful in learning the true symmetry in the environment. We differentiate between extrinsic and incorrect symmetry constraints and show that while imposing incorrect symmetry can impede the model's performance, imposing extrinsic symmetry can actually improve performance. We demonstrate that an equivariant model can significantly outperform non-equivariant methods on domains with latent symmetries both in supervised learning and in reinforcement learning for robotic manipulation and control problems."}}
{"id": "pn-HOPBioUE", "cdate": 1655376341782, "mdate": null, "content": {"title": "Leveraging Fully Observable Policies for Learning under Partial Observability", "abstract": "Reinforcement learning in partially observable domains is challenging due to the lack of observable state information. Thankfully, learning offline in a simulator with such state information is often possible. In particular, we propose a method for partially observable reinforcement learning that uses a fully observable policy (which we call a \\emph{state expert}) during training to improve performance. Based on Soft Actor-Critic (SAC), our agent balances performing actions similar to the state expert and getting high returns under partial observability. Our approach can leverage the fully-observable policy for exploration and parts of the domain that are fully observable while still being able to learn under partial observability. On six robotics domains, our method outperforms pure imitation, pure reinforcement learning, the sequential or parallel combination of both types, and a recent state-of-the-art method in the same setting. A successful policy transfer to a physical robot in a manipulation task from pixels shows our approach's practicality in learning interesting policies under partial observability."}}
{"id": "K8W6ObPZQyh", "cdate": 1655376335955, "mdate": null, "content": {"title": "On-Robot Learning With Equivariant Models", "abstract": "Recently, equivariant neural network models have been shown to improve sample efficiency for tasks in computer vision and reinforcement learning. This paper explores this idea in the context of on-robot policy learning in which a policy must be learned entirely on a physical robotic system without reference to a model, a simulator, or an offline dataset. We focus on applications of Equivariant SAC to robotic manipulation and explore a number of variations of the algorithm. Ultimately, we demonstrate the ability to learn several non-trivial manipulation tasks completely through on-robot experiences in less than an hour or two of wall clock time. "}}
{"id": "yv2G3XpIlL", "cdate": 1640995200000, "mdate": 1681653414267, "content": {"title": "Edge Grasp Network: A Graph-Based SE(3)-invariant Approach to Grasp Detection", "abstract": ""}}
{"id": "xSfboBTXoq", "cdate": 1640995200000, "mdate": 1681653414293, "content": {"title": "BulletArm: An Open-Source Robotic Manipulation Benchmark and Learning Framework", "abstract": ""}}
{"id": "wUPKZ1d_9-", "cdate": 1640995200000, "mdate": 1681653414291, "content": {"title": "SEIL: Simulation-augmented Equivariant Imitation Learning", "abstract": ""}}
{"id": "Y3RPLaZ9fxw", "cdate": 1640995200000, "mdate": 1681653414264, "content": {"title": "Sample Efficient Grasp Learning Using Equivariant Models", "abstract": ""}}
{"id": "NBv9U2dDHDl", "cdate": 1640995200000, "mdate": 1681653414291, "content": {"title": "Equivariant Transporter Network", "abstract": ""}}
