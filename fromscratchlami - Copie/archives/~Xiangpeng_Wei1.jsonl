{"id": "Uu1Nw-eeTxJ", "cdate": 1601308044500, "mdate": null, "content": {"title": "On Learning Universal Representations Across Languages", "abstract": "Recent studies have demonstrated the overwhelming advantage of cross-lingual pre-trained models (PTMs), such as multilingual BERT and XLM, on cross-lingual NLP tasks. However, existing approaches essentially capture the co-occurrence among tokens through involving the masked language model (MLM) objective with token-level cross entropy. In this work, we extend these approaches to learn sentence-level representations and show the effectiveness on cross-lingual understanding and generation. Specifically, we propose a Hierarchical Contrastive Learning (HiCTL) method to (1) learn universal representations for parallel sentences distributed in one or multiple languages and (2) distinguish the semantically-related words from a shared cross-lingual vocabulary for each sentence. We conduct evaluations on two challenging cross-lingual tasks, XTREME and machine translation. Experimental results show that the HiCTL outperforms the state-of-the-art XLM-R by an absolute gain of 4.2% accuracy on the XTREME benchmark as well as achieves substantial improvements on both of the high resource and low-resource English$\\rightarrow$X translation tasks over strong baselines."}}
{"id": "syT263wHHia", "cdate": 1577836800000, "mdate": null, "content": {"title": "Multiscale Collaborative Deep Models for Neural Machine Translation", "abstract": "Recent evidence reveals that Neural Machine Translation (NMT) models with deeper neural networks can be more effective but are difficult to train. In this paper, we present a MultiScale Collaborative (MSC) framework to ease the training of NMT models that are substantially deeper than those used previously. We explicitly boost the gradient back-propagation from top to bottom levels by introducing a block-scale collaboration mechanism into deep NMT models. Then, instead of forcing the whole encoder stack directly learns a desired representation, we let each encoder block learns a fine-grained representation and enhance it by encoding spatial dependencies using a context-scale collaboration. We provide empirical evidence showing that the MSC nets are easy to optimize and can obtain improvements of translation quality from considerably increased depth. On IWSLT translation tasks with three translation directions, our extremely deep models (with 72-layer encoders) surpass strong baselines by +2.2~+3.1 BLEU points. In addition, our deep MSC achieves a BLEU score of 30.56 on WMT14 English-to-German task that significantly outperforms state-of-the-art deep NMT models. We have included the source code in supplementary materials."}}
{"id": "_2bDaY7__yC", "cdate": 1577836800000, "mdate": null, "content": {"title": "Uncertainty-Aware Semantic Augmentation for Neural Machine Translation", "abstract": "As a sequence-to-sequence generation task, neural machine translation (NMT) naturally contains intrinsic uncertainty, where a single sentence in one language has multiple valid counterparts in the other. However, the dominant methods for NMT only observe one of them from the parallel corpora for the model training but have to deal with adequate variations under the same meaning at inference. This leads to a discrepancy of the data distribution between the training and the inference phases. To address this problem, we propose uncertainty-aware semantic augmentation, which explicitly captures the universal semantic information among multiple semantically-equivalent source sentences and enhances the hidden representations with this information for better translations. Extensive experiments on various translation tasks reveal that our approach significantly outperforms the strong baselines and the existing methods."}}
{"id": "WYvOp4JSho", "cdate": 1577836800000, "mdate": null, "content": {"title": "IIE's Neural Machine Translation Systems for WMT20", "abstract": "In this paper we introduce the systems IIE submitted for the WMT20 shared task on German-French news translation. Our systems are based on the Transformer architecture with some effective improvements. Multiscale collaborative deep architecture, data selection, back translation, knowledge distillation, domain adaptation, model ensemble and re-ranking are employed and proven effective in our experiments. Our German-to-French system achieved 35.0 BLEU and ranked the second among all anonymous submissions, and our French-to-German system achieved 36.6 BLEU and ranked the fourth in all anonymous submissions."}}
{"id": "QZIZqdINipk", "cdate": 1577836800000, "mdate": null, "content": {"title": "Towards Enhancing Faithfulness for Neural Machine Translation", "abstract": "Neural machine translation (NMT) has achieved great success due to the ability to generate high-quality sentences. Compared with human translations, one of the drawbacks of current NMT is that translations are not usually faithful to the input, e.g., omitting information or generating unrelated fragments, which inevitably decreases the overall quality, especially for human readers. In this paper, we propose a novel training strategy with a multi-task learning paradigm to build a faithfulness enhanced NMT model (named FEnmt). During the NMT training process, we sample a subset from the training set and translate them to get fragments that have been mistranslated. Afterward, the proposed multi-task learning paradigm is employed on both encoder and decoder to guide NMT to correctly translate these fragments. Both automatic and human evaluations verify that our FEnmt could improve translation quality by effectively reducing unfaithful translations."}}
{"id": "BedWOvYLBXS", "cdate": 1577836800000, "mdate": null, "content": {"title": "Enhancing Pre-trained Language Models by Self-supervised Learning for Story Cloze Test", "abstract": "Story Cloze Test (SCT) gains increasing attention in evaluating the ability of story comprehension, which requires a story comprehension model to select the correct ending to a story context from two candidate endings. Recent advances, such as GPT and BERT, have shown success in incorporating a pre-trained transformer language model and fine-tuning operation to improve SCT. However, this framework still has some fundamental problems in effectively incorporating story-level knowledge from related corpus. In this paper, we introduce three self-supervised learning tasks (Drop, Replace and TOV) to transfer the story-level knowledge of ROCStories into the backbone model including vanilla BERT and Multi-Choice Head. We evaluate our approach on both SCT-v1.0 and SCT-v1.5 benchmarks. The experimental results demonstrate that our approach achieves state-of-the-art results compared with baseline models."}}
{"id": "6MSu9XUBWb", "cdate": 1577836800000, "mdate": null, "content": {"title": "Bi-directional CognitiveThinking Network for Machine Reading Comprehension", "abstract": "We propose a novel Bi-directional Cognitive Knowledge Framework (BCKF) for reading comprehension from the perspective of complementary learning systems theory. It aims to simulate two ways of thinking in the brain to answer questions, including reverse thinking and inertial thinking. To validate the effectiveness of our framework, we design a corresponding Bi-directional Cognitive Thinking Network (BCTN) to encode the passage and generate a question (answer) given an answer (question) and decouple the bi-directional knowledge. The model has the ability to reverse reasoning questions which can assist inertial thinking to generate more accurate answers. Competitive improvement is observed in DuReader dataset, confirming our hypothesis that bi-directional knowledge helps the QA task. The novel framework shows an interesting perspective on machine reading comprehension and cognitive science."}}
{"id": "2YoWN3l-Ly8", "cdate": 1577836800000, "mdate": null, "content": {"title": "Dynamic Attention Aggregation with BERT for Neural Machine Translation", "abstract": "The recently proposed BERT has demonstrated great power in various natural language processing tasks. However, the model does not perform effectively on cross-lingual tasks, especially on machine translation. In this work, we propose three methods to introduce pre-trained BERT into neural machine translation without fine-tuning. Our approach consists of a) a linear-attention aggregation that leverages a parameter matrix to capture the key knowledge of BERT, b) a self-attention aggregation which aims to learn what is vital for input and output, and c) a switch-gate aggregation to dynamically control the balance of the information flowing from the pre-trained BERT or the NMT model. We conduct experiments on several translation benchmarks and substantially improve over 2 BELU points on the IWSLT\u201914 English - German task with switch-gate aggregation method compared to a strong baseline, while our proposed model also performs remarkably on the other tasks."}}
{"id": "iYLjMMbe3D", "cdate": 1546300800000, "mdate": null, "content": {"title": "Dynamic Task-Specific Factors for Meta-Embedding", "abstract": "Meta-embedding is a technology to create a new embedding by combining different existing embeddings, which captures complementary aspects of lexical semantics. The supervised learning of task-specific meta-embedding is a convenient way to make use of accessible pre-trained word embeddings. However, the weights for different word embeddings are hard to calculate. We introduce the dynamic task-specific factors into meta-embedding (DTFME), which are utilized to calculate appropriate weights of different embedding sets without increasing complexity. Then, we evaluate the performance of DTFME on sentence representation tasks. Experiments show that our method outperforms prior works in several benchmark datasets."}}
{"id": "N4kF_vlalX", "cdate": 1546300800000, "mdate": null, "content": {"title": "Unsupervised Neural Machine Translation with Future Rewarding", "abstract": "In this paper, we alleviate the local optimality of back-translation by learning a policy (takes the form of an encoder-decoder and is defined by its parameters) with future rewarding under the reinforcement learning framework, which aims to optimize the global word predictions for unsupervised neural machine translation. To this end, we design a novel reward function to characterize high-quality translations from two aspects: n-gram matching and semantic adequacy. The n-gram matching is defined as an alternative for the discrete BLEU metric, and the semantic adequacy is used to measure the adequacy of conveying the meaning of the source sentence to the target. During training, our model strives for earning higher rewards by learning to produce grammatically more accurate and semantically more adequate translations. Besides, a variational inference network (VIN) is proposed to constrain the corresponding sentences in two languages have the same or similar latent semantic code. On the widely used WMT\u201914 English-French, WMT\u201916 English-German and NIST Chinese-to-English benchmarks, our models respectively obtain 27.59/27.15, 19.65/23.42 and 22.40 BLEU points without using any labeled data, demonstrating consistent improvements over previous unsupervised NMT models."}}
