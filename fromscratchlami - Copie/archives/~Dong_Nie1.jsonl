{"id": "_k0CnK5V7F", "cdate": 1663849831283, "mdate": null, "content": {"title": "Faster Neural Architecture \"Search\" for Deep Image Prior", "abstract": "Deep image prior (DIP) is known for leveraging the spectral bias of the convolutional neural network (CNN) towards lower frequencies in various single-image restoration tasks.  Such inductive bias has been widely attributed to the network architecture. Existing studies therefore either handcraft the architecture or use automated neural architecture search (NAS). However, there is still a lack of understanding on how the architectural choice corresponds to the image to be restored, leading to an excessively large search space that is both time and computationally-expensive for typical NAS techniques. As a result, the architecture is often searched and fixed for the whole dataset, while the best-performing one could be image-dependent. Moreover, common architecture search requires ground truth supervision, which is often not accessible. In this work, we present a simple yet effective \\emph{training-free} approach to estimate the required architecture for \\emph{every image} in advance. This is motivated by our empirical findings that the width and depth of a good network prior are correlated with the texture of the image, which can be estimated during pre-processings. Accordingly, the design space is substantially shrunk to a handful of subnetworks within a given large network. The experiments on denoising across different noise levels show that a subnetwork with proper setups could be a more effective network prior than the original network while being highly under-parameterized, making it not critically require early-stopping as with the original large network."}}
{"id": "vCaJ50EgznT", "cdate": 1640995200000, "mdate": 1669771232881, "content": {"title": "Weakly-supervised Metric Learning with Cross-Module Communications for the Classification of Anterior Chamber Angle Images", "abstract": "As the basis for developing glaucoma treatment strategies, Anterior Chamber Angle (ACA) evaluation is usually dependent on experts' Judgements. However, experienced ophthalmologists needed for these Judgements are not widely available. Thus, computer-aided ACA evaluations become a pressing and efficient solution for this issue. In this paper, we propose a novel end-to-end frame-work GCNet for automated Glaucoma Classification based on ACA images or other Glaucoma-related medical images. We first collect and label an ACA image dataset with some pixel-level annotations. Next, we introduce a segmentation module and an embedding module to enhance the performance of classifying ACA images. Within GCNet, we design a Cross-Module Aggregation Net (CMANet) which is a weakly-supervised metric learning network to capture contextual information exchanging across these modules. We conduct experiments on the ACA dataset and two public datasets REFUGE and SIGF. Our experimental results demonstrate that GCNet outperforms several state-of-the-art deep models in the tasks of glaucoma medical image classifications. The source code of GCNet can be found at https://github.com/Jingqi-H/GCNet."}}
{"id": "d0Q9guiV2v", "cdate": 1640995200000, "mdate": 1668704462754, "content": {"title": "EDDIE-Transformer: Enriched Disease Embedding Transformer for X-Ray Report Generation", "abstract": "Automatic medical report generation is an emerging field that aims to generate medical reports based on medical images. The report writing process can be tedious for senior radiologists and challenging for junior ones. Thus it is of great importance to expedite the process. In this work, we propose an EnricheD DIsease Embedding based Transformer (Eddie-Transformer) model, which jointly performs disease detection and medical report generation. This is done by decoupling the latent visual features into semantic disease embeddings and disease states via our state-aware mechanism. Then, our model entangles the learned diseases and their states, enabling explicit and precise disease representations. Finally, the Transformer model receives the enriched disease representations to generate high-quality medical reports. Our approach shows promising results on the widely-used Open-I benchmark and COVID-19 dataset."}}
{"id": "bBmqCxpd0U8", "cdate": 1640995200000, "mdate": 1662155076590, "content": {"title": "Multi-Label Clinical Time-Series Generation via Conditional GAN", "abstract": "With wide applications of electronic health records (EHR), deep learning methods have been adopted to analyze EHR data on various tasks such as representation learning, clinical event prediction, and phenotyping. However, due to privacy constraints, limited access to EHR becomes a bottleneck for deep learning research. Recently, generative adversarial networks (GANs) have been successful in generating EHR data. However, there are still challenges in high-quality EHR generation, including generating time-series EHR and uncommon diseases given imbalanced datasets. In this work, we propose a Multi-label Time-series GAN (MTGAN) to generate EHR data and simultaneously improve the quality of uncommon disease generation. The generator of MTGAN uses a gated recurrent unit (GRU) with a smooth conditional matrix to generate sequences and uncommon diseases. The critic gives scores using Wasserstein distance to recognize real samples from synthetic samples by considering both data and temporal features. We also propose a training strategy to calculate temporal features for real data and stabilize GAN training. Furthermore, we design multiple statistical metrics and prediction tasks to evaluate the generated data. Experimental results demonstrate the quality of the synthetic data and the effectiveness of MTGAN in generating realistic sequential EHR data, especially for uncommon diseases."}}
{"id": "JwKkxonJpUA", "cdate": 1640995200000, "mdate": 1669771232913, "content": {"title": "Semantic instance segmentation with discriminative deep supervision for medical images", "abstract": ""}}
{"id": "9c0EC8OyWzd", "cdate": 1640995200000, "mdate": 1669771232882, "content": {"title": "Pyramid Architecture for Multi-Scale Processing in Point Cloud Segmentation", "abstract": "Semantic segmentation of point cloud data is a critical task for autonomous driving and other applications. Recent advances of point cloud segmentation are mainly driven by new designs of local aggregation operators and point sampling methods. Unlike image segmentation, few efforts have been made to understand the fundamental issue of scale and how scales should interact and be fused. In this work, we investigate how to efficiently and effectively integrate features at varying scales and varying stages in a point cloud segmentation network. In particular, we open up the commonly used encoder-decoder architecture, and design scale pyramid architectures that allow information to flow more freely and systematically, both laterally and upward/downward in scale. Moreover, a cross-scale attention feature learning block has been designed to enhance the multi-scale feature fusion which occurs everywhere in the network. Such a design of multi-scale processing and fusion gains large improvements in accuracy without adding much additional computation. When built on top of the popular KPConv network, we see consistent improvements on a wide range of datasets, including achieving state-of-the-art performance on NPM3D and S3DIS. Moreover, the pyramid architecture is generic and can be applied to other network designs: we show an example of similar improvements over RandLANet."}}
{"id": "6f253yUoU4", "cdate": 1640995200000, "mdate": 1668232184664, "content": {"title": "An Efficient Semi-Supervised Framework with Multi-Task and Curriculum Learning for Medical Image Segmentation", "abstract": "A practical problem in supervised deep learning for medical image segmentation is the lack of labeled data which is expensive and time-consuming to acquire. In contrast, there is a considerable amo..."}}
{"id": "2Uwlc_FZ5Xc", "cdate": 1640995200000, "mdate": 1669771232879, "content": {"title": "Doubly-Fused ViT: Fuse Information from Vision Transformer Doubly with Local Representation", "abstract": "Vision Transformer (ViT) has recently emerged as a new paradigm for computer vision tasks, but is not as efficient as convolutional neural networks (CNN). In this paper, we propose an efficient ViT architecture, named Doubly-Fused ViT (DFvT), where we feed low-resolution feature maps to self-attention (SA) to achieve larger context with efficiency (by moving downsampling prior to SA), and enhance it with fine-detailed spatial information. SA is a powerful mechanism that extracts rich context information, thus could and should operate at a low spatial resolution. To make up for the loss of details, convolutions are fused into the main ViT pipeline, without incurring high computational costs. In particular, a Context Module (CM), consisting of fused downsampling operator and subsequent SA, is introduced to effectively capture global features with high efficiency. A Spatial Module (SM) is proposed to preserve fine-grained spatial information. To fuse the heterogeneous features, we specially design a Dual AtteNtion Enhancement (DANE) module to selectively fuse low-level and high-level features. Experiments demonstrate that DFvT achieves state-of-the-art accuracy with much higher efficiency across a spectrum of different model sizes. Ablation study validates the effectiveness of our designed components."}}
{"id": "1Ea3qjidNa", "cdate": 1640995200000, "mdate": 1669771232973, "content": {"title": "Explainable attention guided adversarial deep network for 3D radiotherapy dose distribution prediction", "abstract": ""}}
{"id": "-fMiXHWCtC", "cdate": 1640995200000, "mdate": 1669771232948, "content": {"title": "Unified medical image segmentation by learning from uncertainty in an end-to-end manner", "abstract": ""}}
