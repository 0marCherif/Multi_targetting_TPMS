{"id": "RId30q0TFMp", "cdate": 1698561439101, "mdate": 1698561439101, "content": {"title": "Visual Recognition-Driven Image Restoration for Multiple Degradation with Intrinsic Semantics Recovery", "abstract": "Deep image recognition models suffer a significant performance drop when applied to low-quality images since they are trained on high-quality images. Although many studies have investigated to solve the issue through image restoration or domain adaptation, the former focuses on visual quality rather than recognition quality, while the latter requires semantic annotations for task-specific training. In this paper, to address more practical scenarios, we propose a Visual Recognition-Driven Image Restoration network for multiple degradation, dubbed VRD-IR, to recover high-quality images from various unknown corruption types from the perspective of visual recognition within one model. Concretely, we harmonize the semantic representations of diverse degraded images into a unified space in a dynamic manner, and then optimize them towards intrinsic semantics recovery. Moreover, a prior-ascribing optimization strategy is introduced to encourage VRD-IR to couple with various downstream recognition tasks better. Our VRD-IR is corruption- and recognition-agnostic, and can be inserted into various recognition tasks directly as an image enhancement module. Extensive experiments on multiple image distortions demonstrate that our VRD-IR surpasses existing image restoration methods and show superior performance on diverse high-level tasks, including classification, detection, and person re-identification."}}
{"id": "vQn0t-OmWRp", "cdate": 1698561011600, "mdate": 1698561011600, "content": {"title": "Ingredient-oriented Multi-Degradation Learning for Image Restoration", "abstract": "Learning to leverage the relationship among diverse image restoration tasks  is quite beneficial for unraveling the intrinsic ingredients behind the degradation. Recent years have witnessed the flourish of various All-in-one methods, which handle multiple image degradations within a single model. In practice, however, few attempts have been made to excavate task correlations in that exploring the underlying fundamental ingredients of various image degradations, resulting in poor scalability as more tasks are involved. In this paper, we propose a novel perspective to delve into the degradation via an ingredients-oriented rather than previous task-oriented manner for scalable learning. Specifically, our method, named Ingredients-oriented Degradation Reformulation framework (IDR), consists of two stages, namely task-oriented knowledge collection and ingredients-oriented knowledge integration. In the first stage, we conduct ad hoc operations on different degradations according to the underlying physics principles, and establish the corresponding prior hubs for each type of degradation. While the second stage progressively reformulates the preceding task-oriented hubs into single ingredients-oriented hub via learnable Principal Component Analysis (PCA), and employs a dynamic routing mechanism for probabilistic unknown degradation removal. Extensive experiments on various image restoration tasks demonstrate the effectiveness and scalability of our method. More importantly, our IDR exhibits the favorable generalization ability to unknown downstream tasks."}}
{"id": "58MQdPGGnK", "cdate": 1681436542643, "mdate": 1681436542643, "content": {"title": "Deep Fourier-based Exposure Correction Network with Spatial-Frequency Interaction", "abstract": "Images captured under incorrect exposures unavoidably suffer from mixed degradations of lightness and structures. Most existing deep learning-based exposure correction methods separately restore such degradations in the spatial domain. In this paper, we present a new perspective for exposure correction with spatial-frequency interaction. Specifically, we first revisit the frequency properties of different exposure images via Fourier transform where the amplitude component contains most lightness information and the phase component is relevant to structure information. To this end, we propose a deep Fourier-based Exposure Correction Network (FECNet) consisting of an amplitude subnetwork and a phase sub-network to progressively reconstruct the representation of lightness and structure components. To facilitate learning these two representations, we introduce a Spatial-Frequency Interaction (SFI) block in two formats tailored to these two sub-networks, which interactively process the local spatial features and the global frequency information to encourage the complementary learning. Extensive experiments demonstrate that our method achieves superior results than other approaches with fewer parameters and can be extended to other image enhancement tasks, validating its potential in wide-range applications. Code will be available at https://github.com/KevinJ-Huang/FECNet."}}
{"id": "D-lLrJ-bnN7", "cdate": 1681436176733, "mdate": 1681436176733, "content": {"title": "Normalization-based Feature Selection and Restitution for Pan-sharpening", "abstract": "Pan-sharpening is essentially a panchromatic (PAN) image-guided low-spatial resolution MS image super-resolution problem. The commonly challenging issue of pan-sharpening is how to correctly select consistent features and propagate them, and properly handle inconsistent ones between PAN and MS modalities. To solve this issue, we propose a Normalization-based Feature Selection and Restitution mechanism, which is capable of filtering out the inconsistent features and promoting to learn the consistent ones. Specifically, we first modulate the PAN feature as the MS style in feature space by AdaIN operation \\citeAdaIN. However, such operation inevitably removes the favorable features. We thus propose to distill the effective information from the removed part and restitute it back to the modulated part. To better distillation, we enforce a contrastive learning constraint to close the distance between the restituted feature and the ground truth, and push the removed part away from the ground truth. In this way, the consistent features of PAN images are correctly selected and the inconsistent ones are filtered out, thus relieving the over-transferred artifacts in the process of PAN-guided MS super-resolution. Extensive experiments validate the effectiveness of the proposed network and demonstrate its favorable performance against other state-of-the-art methods. The source code will be released at https://github.com/manman1995/pansharpening."}}
{"id": "JTIiwguYMfe", "cdate": 1681435902581, "mdate": 1681435902581, "content": {"title": "Mutual Information-driven Pan-sharpening", "abstract": "Pan-sharpening aims to integrate the complementary information of texture-rich PAN images and multi-spectral (MS) images to produce the texture-rich MS images. Despite the remarkable progress, existing state-of-the-art Pan-sharpening methods don't explicitly enforce the complementary information learning between two modalities of PAN and MS images. This leads to information redundancy not being handled well, which further limits the performance of these methods. To address the above issue, we propose a novel mutual information-driven Pan-sharpening framework in this paper. To be specific, we first project the PAN and MS image into modality-aware feature space independently, and then impose the mutual information minimization over them to explicitly encourage the complementary information learning. Such operation is capable of reducing the information redundancy and improving the model performance. Extensive experimental results over multiple satellite datasets demonstrate that the proposed algorithm outperforms other state-of-the-art methods qualitatively and quantitatively with great generalization ability to real-world scenes."}}
{"id": "MxXikmxbyU", "cdate": 1680307200000, "mdate": 1699180935516, "content": {"title": "Counterfactual attention alignment for visible-infrared cross-modality person re-identification", "abstract": ""}}
{"id": "wZc9Kj4hPQ", "cdate": 1677628800000, "mdate": 1681700811966, "content": {"title": "Whole-Body Control of an Autonomous Mobile Manipulator Using Model Predictive Control and Adaptive Fuzzy Technique", "abstract": "Whole-body control (WBC) has emerged as an important framework in manipulation for mobile manipulators. However, most existing WBC frameworks require known dynamics. Considering whole-body manipulation and optimization with unknown dynamics, this article presents the WBC of a nonholonomic mobile manipulator using model predictive control (MPC) and fuzzy logic system. First, by constructing a dynamics-based feedback linearized robotic multi-input-multi-output (MIMO) system, an MPC-based WBC strategy is proposed for mobile manipulator. Such a strategy can provide the optimal control inputs with the specified optimization index and constraints. Thereafter, a primal-dual neural network effectively addresses the constrained quadratic programming (QP) problem over a finite receding horizon brought by the MPC. Then, in order to convert the intermediate control signals into the optimal control torques that can be executed by actuators, an adaptive FLS is employed to approximate the unknown dynamics. The novel elements of the current design control approach refer to the dynamics-based feedback linearized robotic MIMO system and the combination of an MPC module with an adaptive fuzzy controller. Finally, the trajectory tracking experiments performed on a mobile dual-arm robot demonstrate the effectiveness of the proposed method."}}
{"id": "vcchBs7mip8", "cdate": 1672531200000, "mdate": 1699180935429, "content": {"title": "Deep Adaptive Pansharpening via Uncertainty-Aware Image Fusion", "abstract": "Pansharpening is a procedure that fuses high-resolution panchromatic (PAN) images and low-resolution multispectral (LMS) images to derive high-resolution multispectral (HMS) images. Despite its rapid development, most existing pansharpening techniques integrate the information of PAN and LMS invariantly in the spatial dimension, ignoring the uneven spatial dependence of restoring HMS with the aid of PAN information and resulting in ineffective fusion results. In this work, we propose an uncertainty-aware adaptive pansharpening network (UAPN) that integrates PAN information spatial variantly to restore LMS information with an uncertainty mechanism. Specifically, we first estimate the epistemic and aleatoric uncertainties together, which model the spatial -variant distributions of restoring the LMS image to the HMS image. Then, we introduce uncertainty-conditioned adaptive convolution (UAC) to adaptively integrate LMS and PAN information, where its parameters are spatially variable by conditioning on the uncertainty estimations. Furthermore, we propose a multistage uncertainty-driven loss function to explicitly force the network to concentrate on restoring challenging areas of the LMS image. Extensive experimental results demonstrate the superiority of our UAPN with fewer parameters and FLOPs, outperforming other state-of-the-art methods both qualitatively and quantitatively on multiple satellite datasets. The code is available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/keviner1/UAPN</uri> ."}}
{"id": "t3RmyyBmtTk", "cdate": 1672531200000, "mdate": 1681700812108, "content": {"title": "Modality-Aware Feature Integration for Pan-Sharpening", "abstract": "Pan-sharpening aims to super-solve low-spatial resolution multiple spectral (MS) images with the guidance of high-resolution (HR) texture-rich panchromatic (PAN) images. Recently, deep-learning-based pan-sharpening approaches have dominated this field and achieved remarkable advancement. However, most promising algorithms are devised in one-way mapping and have not fully explored the mutual dependencies between PAN and MS modalities, thus impacting the model performance. To address this issue, we propose a novel information compensation and integration network for pan-sharpening by effective cross-modality joint learning in this work. First, the cross-central difference convolution is employed to explicitly extract the texture details of the PAN images. Second, we implement the compensation process by imitating the classical back-projection (BP) technique where the extracted PAN textures are employed to guide the intrinsic information learning of MS images iteratively. Subsequently, we devise the hierarchical transformer to integrate the comprehensive relations of stage-iteration information from spatial and temporal contexts. Extensive experiments over multiple satellite datasets demonstrate the superiority of our method to the existing state-of-the-art methods. The source code is available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/manman1995/pansharpening</uri> ."}}
{"id": "rz4KRh2SsW", "cdate": 1672531200000, "mdate": 1699180935709, "content": {"title": "Intensity-Aware Loss for Dynamic Facial Expression Recognition in the Wild", "abstract": "Compared with the image-based static facial expression recognition (SFER) task, the dynamic facial expression recognition (DFER) task based on video sequences is closer to the natural expression recognition scene. However, DFER is often more challenging. One of the main reasons is that video sequences often contain frames with different expression intensities, especially for the facial expressions in the real-world scenarios, while the images in SFER frequently present uniform and high expression intensities. Nevertheless, if the expressions with different intensities are treated equally, the features learned by the networks will have large intra-class and small inter-class differences, which are harmful to DFER. To tackle this problem, we propose the global convolution-attention block (GCA) to rescale the channels of the feature maps. In addition, we introduce the intensity-aware loss (IAL) in the training process to help the network distinguish the samples with relatively low expression intensities. Experiments on two in-the-wild dynamic facial expression datasets (i.e., DFEW and FERV39k) indicate that our method outperforms the state-of-the-art DFER approaches. The source code will be available at https://github.com/muse1998/IAL-for-Facial-Expression-Recognition."}}
