{"id": "YApfBH6uH3x", "cdate": 1609459200000, "mdate": 1633032716514, "content": {"title": "Fight Fire with Fire: Fine-tuning Hate Detectors using Large Samples of Generated Hate Speech", "abstract": "Automatic hate speech detection is hampered by the scarcity of labeled datasetd, leading to poor generalization. We employ pretrained language models (LMs) to alleviate this data bottleneck. We utilize the GPT LM for generating large amounts of synthetic hate speech sequences from available labeled examples, and leverage the generated data in fine-tuning large pretrained LMs on hate detection. An empirical study using the models of BERT, RoBERTa and ALBERT, shows that this approach improves generalization significantly and consistently within and across data distributions. In fact, we find that generating relevant labeled hate speech sequences is preferable to using out-of-domain, and sometimes also within-domain, human-labeled examples."}}
{"id": "VCDUZ_czSE", "cdate": 1609459200000, "mdate": 1631217865924, "content": {"title": "What's the Best Place for an AI Conference, Vancouver or _______: Why Completing Comparative Questions is Difficult", "abstract": "Although large neural language models (LMs) like BERT can be finetuned to yield state-of-the-art results on many NLP tasks, it is often unclear what these models actually learn. Here we study using such LMs to fill in entities in human-authored comparative questions, like ``Which country is older, India or _____?''---i.e., we study the ability of neural LMs to ask (not answer) reasonable questions. We show that accuracy in this fill-in-the-blank task is well-correlated with human judgements of whether a question is reasonable, and that these models can be trained to achieve nearly human-level performance in completing comparative questions in three different subdomains. However, analysis shows that what they learn fails to model any sort of broad notion of which entities are semantically comparable or similar---instead the trained models are very domain-specific, and performance is highly correlated with co-occurrences between specific entities observed in the training set. This is true both for models that are pretrained on general text corpora, as well as models trained on a large corpus of comparison questions. Our study thus reinforces recent results on the difficulty of making claims about a deep model's world knowledge or linguistic competence based on performance on specific benchmark problems. We make our evaluation datasets publicly available to foster future research on complex understanding and reasoning in such models at standards of human interaction."}}
{"id": "3bl7drrKypq", "cdate": 1609459200000, "mdate": 1633032716310, "content": {"title": "Towards Hate Speech Detection at Large via Deep Generative Modeling", "abstract": "Hate speech detection is a critical problem in social media, being often accused for enabling the spread of hatred and igniting violence. Hate speech detection requires overwhelming computing resources for online monitoring as well as thousands of human experts for daily screening of suspected posts or tweets. Recently, deep learning (DL)-based solutions have been proposed for hate speech detection, using modest-sized datasets of few thousands of sequences. While these methods perform well on the specific datasets, their ability to generalize to new hate speech sequences is limited. Being a data-driven approach, it is known that DL surpasses other methods whenever scale-up in trainset size and diversity is achieved. Therefore, we first present a dataset of 1 million hate and nonhate sequences, produced by a deep generative model. We further utilize the generated data to train a well-studied DL detector, demonstrating significant performance improvements across five hate speech datasets."}}
{"id": "MvFVcoxIX-l", "cdate": 1577836800000, "mdate": 1633032716405, "content": {"title": "Towards Hate Speech Detection at Large via Deep Generative Modeling", "abstract": "Hate speech detection is a critical problem in social media platforms, being often accused for enabling the spread of hatred and igniting physical violence. Hate speech detection requires overwhelming resources including high-performance computing for online posts and tweets monitoring as well as thousands of human experts for daily screening of suspected posts or tweets. Recently, Deep Learning (DL)-based solutions have been proposed for automatic detection of hate speech, using modest-sized training datasets of few thousands of hate speech sequences. While these methods perform well on the specific datasets, their ability to detect new hate speech sequences is limited and has not been investigated. Being a data-driven approach, it is well known that DL surpasses other methods whenever a scale-up in train dataset size and diversity is achieved. Therefore, we first present a dataset of 1 million realistic hate and non-hate sequences, produced by a deep generative language model. We further utilize the generated dataset to train a well-studied DL-based hate speech detector, and demonstrate consistent and significant performance improvements across five public hate speech datasets. Therefore, the proposed solution enables high sensitivity detection of a very large variety of hate speech sequences, paving the way to a fully automatic solution."}}
{"id": "4Mxq0i6Vdk", "cdate": 1577836800000, "mdate": 1633032716478, "content": {"title": "Assessing the Contribution of Subject-matter Experts to Wikipedia", "abstract": "Attempts to explain the success of knowledge co-production communities have focused on organizational design, including structure, motivation, roles, and coordination mechanisms. Meantime, the role that subject-matter-experts play in these knowledge production settings has largely been left in a theoretical and empirical void; its existence has been assumed, but we know little about its nature and scope, as it is difficult to observe. In this article, we start filling that void, using Wikipedia as the setting for our empirical investigation. First, we carefully crossed information from individual Wikipedia editor pages with external sources such as Google Scholar to reliably identify editors who are credentialed experts. Matching these credentialed experts with their Wikipedia editing patterns, we used this dataset to train a machine learning classifier that we then employed to identify additional expert editors and assess the nature and the scope of their work across Wikipedia. Our results suggest that the scope of expert involvement is substantial, albeit with considerable differences across topics. We estimate that approximately 10%--30% of Wikipedia\u2019s contributors have substantial subject-matter expertise in the topics that they edit. We discuss implications for theory and practice of peer-production."}}
{"id": "e9EkIBOk-0", "cdate": 1546300800000, "mdate": 1633032716370, "content": {"title": "Relational social recommendation: Application to the academic domain", "abstract": "Highlights \u2022 Relational social recommendation provides highly detailed explanations to users. \u2022 Users find these explanations to be intriguing and often surprising. \u2022 Linking people based on entity associations reveals novel connections of interest. \u2022 Recommendation relevance and serendipity may be improved given user feedbacks. Abstract This paper outlines RSR, a relational social recommendation approach applied to a social graph comprised of relational entity profiles. RSR uses information extraction and learning methods to obtain relational facts about persons of interest from the Web, and generates an associative entity-relation social network from their extracted personal profiles. As a case study, we consider the task of peer recommendation at scientific conferences. Given a social graph of scholars, RSR employs graph similarity measures to rank conference participants by their relatedness to a user. Unlike other recommender systems that perform social rankings, RSR provides the user with detailed supporting explanations in the form of relational connecting paths. In a set of user studies, we collected feedbacks from participants onsite of scientific conferences, pertaining to RSR quality of recommendations and explanations. The feedbacks indicate that users appreciate and benefit from RSR explainability features. The feedbacks further indicate on recommendation serendipity using RSR, having it recommend persons of interest who are not apriori known to the user, oftentimes exposing surprising inter-personal associations. Finally, we outline and assess potential gains in recommendation relevance and serendipity using path-based relational learning within RSR."}}
{"id": "K4b9cIju7G", "cdate": 1483228800000, "mdate": 1633032716439, "content": {"title": "Assessing the Contribution of Twitter's Textual Information to Graph-based Recommendation", "abstract": "Graph-based recommendation approaches can model associations between users and items alongside additional contextual information. Recent studies demonstrated that representing features extracted from social media (SM) auxiliary data, like friendships, jointly with traditional users/items ratings in the graph, contribute to recommendation accuracy. In this work, we take a step further and propose an extended graph representation that includes socio-demographic and personal traits extracted from the content posted by the user on SM. Empirical results demonstrate that processing unstructured textual information collected from Twitter and representing it in structured form in the graph improves recommendation performance, especially in cold start conditions."}}
{"id": "p8aq5XTQGHT", "cdate": 1451606400000, "mdate": 1633032716494, "content": {"title": "Multi-source named entity typing for social media", "abstract": "Reuth Vexler, Einat Minkov. Proceedings of the Sixth Named Entity Workshop. 2016."}}
{"id": "ryZgq3g_-H", "cdate": 1420070400000, "mdate": null, "content": {"title": "Learning Relational Features with Backward Random Walks", "abstract": "The path ranking algorithm (PRA) has been recently proposed to address relational classification and retrieval tasks at large scale. We describe Cor-PRA, an enhanced system that can model a larger space of relational rules, including longer relational rules and a class of first order rules with constants, while maintaining scalability. We describe and test faster algorithms for searching for these features. A key contribution is to leverage backward random walks to efficiently discover these types of rules. An empirical study is conducted on the tasks of graph-based knowledge base inference, and person named entity extraction from parsed text. Our results show that learning paths with constants improves performance on both tasks, and that modeling longer paths dramatically improves performance for the named entity extraction task."}}
{"id": "qcK-omNBKbA", "cdate": 1420070400000, "mdate": 1633032716458, "content": {"title": "Automatic Gloss Finding for a Knowledge Base using Ontological Constraints", "abstract": "While there has been much research on automatically constructing structured Knowledge Bases (KBs), most of it has focused on generating facts to populate a KB. However, a useful KB must go beyond facts. For example, glosses (short natural language definitions) have been found to be very useful in tasks such as Word Sense Disambiguation. However, the important problem of Automatic Gloss Finding, i.e., assigning glosses to entities in an initially gloss-free KB, is relatively unexplored. We address that gap in this paper. In particular, we propose GLOFIN, a hierarchical semi-supervised learning algorithm for this problem which makes effective use of limited amounts of supervision and available ontological constraints. To the best of our knowledge, GLOFIN is the first system for this task. Through extensive experiments on real-world datasets, we demonstrate GLOFIN's effectiveness. It is encouraging to see that GLOFIN outperforms other state-of-the-art SSL algorithms, especially in low supervision settings. We also demonstrate GLOFIN's robustness to noise through experiments on a wide variety of KBs, ranging from user contributed (e.g., Freebase) to automatically constructed (e.g., NELL). To facilitate further research in this area, we have made the datasets and code used in this paper publicly available."}}
