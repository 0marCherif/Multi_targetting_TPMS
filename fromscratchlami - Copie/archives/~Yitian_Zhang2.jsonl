{"id": "yojn_h1N7n1", "cdate": 1640995200000, "mdate": 1683899074770, "content": {"title": "Contrastive Learning for Time Series on Dynamic Graphs", "abstract": "There have been several recent efforts towards developing representations for multivariate time-series in an unsupervised learning framework. Such representations can prove beneficial in tasks such as activity recognition, health monitoring, and anomaly detection. In this paper, we consider a setting where we observe time-series at each node in a dynamic graph. We propose a framework called GraphTNC for unsupervised learning of joint representations of the graph and the time-series. Our approach employs a contrastive learning strategy. Based on an assumption that the time-series and graph evolution dynamics are piecewise smooth, we identify local windows of time where the signals exhibit approximate stationarity. We then train an encoding that allows the distribution of signals within a neighborhood to be distinguished from the distribution of non-neighboring signals. We first demonstrate the performance of our proposed framework using synthetic data, and subsequently we show that it can prove beneficial for the classification task with real-world datasets."}}
{"id": "H9EWv4sDSd", "cdate": 1640995200000, "mdate": 1681664519221, "content": {"title": "On the Design of Channel Coding Autoencoders With Arbitrary Rates for ISI Channels", "abstract": "This letter presents an autoencoder-based channel coding scheme in the presence of inter-symbol interference (ISI) and additive white Gaussian noise (AWGN), supporting arbitrary coding rates. Both the transmitter and receiver of the proposed autoencoder employ bi-directional gated recurrent unit (Bi-GRU) layers. Additional extra dense layers are applied at the end of the transmitter and at the beginning of the receiver, serving as learnable puncture and depuncture modules, respectively. Different code rates can be achieved by adjusting the output dimension of the extra dense layers. Experimental results demonstrate that the proposed autoencoder significantly outperforms conventional convolutional codes over ISI channels, for multiple code rates. The proposed autoencoder also outperforms LDPC codes in the low signal-to-noise ratio (SNR) regime. The neural codes still require improvement to be competitive in the high SNR regime."}}
{"id": "76rUiE2UlF", "cdate": 1640995200000, "mdate": 1683899074728, "content": {"title": "Contrastive Learning for Time Series on Dynamic Graphs", "abstract": "There have been several recent efforts towards developing representations for multivariate time-series in an unsupervised learning framework. Such representations can prove beneficial in tasks such as activity recognition, health monitoring, and anomaly detection. In this paper, we consider a setting where we observe time-series at each node in a dynamic graph. We propose a framework called GraphTNC for unsupervised learning of joint representations of the graph and the time-series. Our approach employs a contrastive learning strategy. Based on an assumption that the time-series and graph evolution dynamics are piecewise smooth, we identify local windows of time where the signals exhibit approximate stationarity. We then train an encoding that allows the distribution of signals within a neighborhood to be distinguished from the distribution of non-neighboring signals. We first demonstrate the performance of our proposed framework using synthetic data, and subsequently we show that it can prove beneficial for the classification task with real-world datasets."}}
{"id": "x08UkZhBARv", "cdate": 1577836800000, "mdate": null, "content": {"title": "End-to-end Physical Layer Communication using Bi-directional GRUs for ISI Channels", "abstract": "This paper presents a deep learning autoencoder for end-to-end physical-layer communications, in the presence of intersymbol interference (ISI) and additive white Gaussian noise (AWGN). Both the proposed transmitter and receiver employ the bi-directional gated recurrent unit (Bi-GRU) layers, and they are trained jointly with binary cross-entropy loss. By doing so, the transmitter learns tailored modulation constellations considering the channel impulse response while the receiver performs equalization and demodulation simultaneously. Experiments conducted over representative ISI channels reveal that the proposed autoencoder outperforms the Viterbi-based maximum-likelihood sequence estimation algorithm with perfect channel state information, when E <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">b</sub> /N <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">0</sub> takes low to medium values, i.e., less than 13 dB."}}
{"id": "zRy-KxZjKNs", "cdate": 1546300800000, "mdate": 1655786335980, "content": {"title": "Privacy Preserving Deep Learning with Distributed Encoders", "abstract": "In this paper, we propose a distributed machine learning framework for training and inference in machine learning models using distributed data while preserving privacy of the data owner. In the training mode, we deploy an encoder on the end-user device which extracts high level features from input data. The extracted features along with the corresponding annotation are sent to a centralized machine learning server. In the inference mode, the users submit the extracted features from encoder instead of the original data for inference to the server. This approach enables users to contributed in training a machine learning model and use inference services without sharing their original data with the server or a third party. We have studied this approach on MNIST, Fashion, SVHN and CIFAR-10 datasets. The results show high classification accuracy of neural networks, trained with encoded features, and high encryption performance of the encoders."}}
