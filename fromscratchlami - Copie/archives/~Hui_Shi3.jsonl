{"id": "usa87QW3_r9", "cdate": 1663849836684, "mdate": null, "content": {"title": "Everyone's Preference Changes Differently: Weighted Multi-Interest Retrieval Model", "abstract": "User embeddings (vectorized representations of a user) are essential in recommendation systems. Numerous approaches have been proposed to construct a representation for the user in order to find similar items for retrieval tasks, and they have been proven effective in industrial recommendation systems. Recently people have discovered the power of using multiple embeddings to represent a user, with the hope that each embedding represents the user's interest in a certain topic. With multi-interest representation, it's important to model the user's preference over the different topics and how the preference change with time. However, existing approaches either fail to estimate the user's affinity to each interest or unreasonably assume every interest of every user fades with an equal rate with time, thus hurting the performance of candidate retrieval. In this paper, we propose the Multi-Interest Preference (MIP) model, an approach that not only produces multi-interest for users by using the user's sequential engagement more effectively but also automatically learns a set of weights to represent the preference over each embedding so that the candidates can be retrieved from each interest proportionally. Extensive experiments have been done on various industrial-scale datasets to demonstrate the effectiveness of our approach. "}}
{"id": "aLq_K3wn2d", "cdate": 1640995200000, "mdate": 1673991610247, "content": {"title": "Every Preference Changes Differently: Neural Multi-Interest Preference Model with Temporal Dynamics for Recommendation", "abstract": ""}}
{"id": "NNO_0A4O4EE", "cdate": 1640995200000, "mdate": 1681499269426, "content": {"title": "Learning Bounded Context-Free-Grammar via LSTM and the Transformer: Difference and the Explanations", "abstract": ""}}
{"id": "H1qWtx7TAoq", "cdate": 1609459200000, "mdate": 1681499269499, "content": {"title": "Learning Bounded Context-Free-Grammar via LSTM and the Transformer: Difference and Explanations", "abstract": ""}}
{"id": "74KgsM5a-r", "cdate": 1609459200000, "mdate": 1681755075326, "content": {"title": "Continuous Cnn For Nonuniform Time Series", "abstract": "CNN for time series data implicitly assumes that the data are uniformly sampled, whereas many event-based and multi-modal data are nonuniform or have heterogeneous sampling rates. Directly applying regular CNN to nonuniform time series is ungrounded, because it is unable to recognize and extract common patterns from the nonuniform input signals. In this paper, we propose the Continuous CNN (CCNN), which estimates the inherent continuous inputs by interpolation, and performs continuous convolution on the continuous input. The interpolation and convolution kernels are learned in an end-to-end manner, and are able to learn useful patterns despite the nonuniform sampling rate. Results of several experiments verify that CCNN achieves a better performance on nonuniform data, and learns meaningful continuous kernels. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>"}}
{"id": "l8HGWQGvDpf", "cdate": 1577836800000, "mdate": 1668700780141, "content": {"title": "Driving Scenario Perception-Aware Computing System Design in Autonomous Vehicles", "abstract": "Recently, autonomous driving ignited competitions among car makers and technical corporations. Low-level autonomous vehicles are already commercially available. However, high autonomous vehicles where the vehicle drives by itself without human monitoring is still at infancy. Such autonomous vehicles (AVs) fully rely on the computing system in the car to perceive the environment and make driving decisions. In AV computing systems, the latency is an essential metric for ensuring the efficiency and safety, because a timely decision with low latency will avoid accidents and save lives. Moreover, we perform a field study by running industrial Level-4 autonomous driving fleets in various locations, road conditions, and traffic patterns. We observe that the perception module consumes the longest latency, and it is highly sensitive to surrounding obstacles. To study the correlation between perception latency and surrounding obstacles, we propose a perception latency model. Moreover, we demonstrate the use of our latency model, by developing and evaluating a driving scenario perception-aware AV computing system that efficiently manages computation hardware resource. Our evaluation results show that the proposed AV system resource management improves performance significantly."}}
{"id": "SY6VzQWy10", "cdate": 1577836800000, "mdate": 1668700780176, "content": {"title": "Safety Score: A Quantitative Approach to Guiding Safety-Aware Autonomous Vehicle Computing System Design", "abstract": "High automated vehicles rely on the computing system in the car to understand the environment and make driving decisions. Therefore, computing system design is essential for ensuring the driving safety. However, to our knowledge, no clear guideline exists so far regarding how to guide the safety-aware autonomous vehicle (AV) computing system design. To understand the safety requirement of AV computing system, we performed a field study by operating industrial Level-4 AV fleets in multiple locations for three months. The field study indicates that traditional computing system performance metrics, such as tail latency, average latency, maximum latency, and timeout, cannot fully satisfy the safety requirement for AV computing system design. To address this issue, we propose the \u201csafety score\u201d as a primary metric for measuring the level of safety in AV computing system design."}}
{"id": "-g6pHbB6Xxb", "cdate": 1577836800000, "mdate": 1681499269490, "content": {"title": "Deep Symbolic Superoptimization Without Human Knowledge", "abstract": ""}}
{"id": "r1egIyBFPS", "cdate": 1569439560512, "mdate": null, "content": {"title": "Deep Symbolic Superoptimization Without Human Knowledge", "abstract": "Deep  symbolic superoptimization refers to the task of applying deep learning methods to simplify symbolic expressions.   Existing approaches either perform supervised training on human-constructed datasets that defines equivalent expression pairs, or apply reinforcement learning with human-defined equivalent trans-formation actions.  In short,  almost all existing methods rely on human knowledge to define equivalence, which suffers from large labeling cost and learning bias, because it is almost impossible to define and comprehensive equivalent set. We thus propose HISS, a reinforcement learning framework for symbolic super-optimization that keeps human outside the loop.  HISS introduces a tree-LSTM encoder-decoder network with attention to ensure tractable learning.   Our experiments show that HISS can discover more simplification rules than existing human-dependent methods, and can learn meaningful embeddings for symbolic expressions, which are indicative of equivalence."}}
{"id": "r1e4MkSFDr", "cdate": 1569439499596, "mdate": null, "content": {"title": "Continuous Convolutional Neural Network forNonuniform Time Series", "abstract": "Convolutional neural network (CNN) for time series data implicitly assumes that the data are uniformly sampled, whereas many event-based and multi-modal data are nonuniform or have heterogeneous sampling rates. Directly applying regularCNN to nonuniform time series is ungrounded, because it is unable to recognize and extract common patterns from the nonuniform input signals. Converting the nonuniform time series to uniform ones by interpolation preserves the pattern extraction capability of CNN, but the interpolation kernels are often preset and may be unsuitable for the data or tasks. In this paper, we propose the ContinuousCNN (CCNN), which estimates the inherent continuous inputs by interpolation, and performs continuous convolution on the continuous input. The interpolation and convolution kernels are learned in an end-to-end manner, and are able to learn useful patterns despite the nonuniform sampling rate. Besides, CCNN is a strict generalization to CNN. Results of several experiments verify that CCNN achieves abetter performance on nonuniform data, and learns meaningful continuous kernels"}}
