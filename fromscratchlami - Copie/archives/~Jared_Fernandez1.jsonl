{"id": "AeGmk9y9DD3", "cdate": 1672531200000, "mdate": 1680388799912, "content": {"title": "The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment", "abstract": ""}}
{"id": "SF8ZuGhEGXq", "cdate": 1609459200000, "mdate": 1648671760364, "content": {"title": "CIGLI: Conditional Image Generation from Language & Image", "abstract": "Multi-modal generation has been widely explored in recent years. Current research directions involve generating text based on an image or vice versa. In this paper, we propose a new task called CIGLI: Conditional Image Generation from Language and Image. Instead of generating an image based on text as in text-image generation, this task requires the generation of an image from a textual description and an image prompt. We designed a new dataset to ensure that the text description describes information from both images, and that solely analyzing the description is insufficient to generate an image. We then propose a novel language-image fusion model which improves the performance over two established baseline methods, as evaluated by quantitative (automatic) and qualitative (human) evaluations. The code and dataset is available at https://github.com/vincentlux/CIGLI."}}
{"id": "S9UZ_znEGm5", "cdate": 1577836800000, "mdate": 1648671760372, "content": {"title": "G-DAug: Generative Data Augmentation for Commonsense Reasoning", "abstract": "Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras, Ji-Ping Wang, Chandra Bhagavatula, Yejin Choi, Doug Downey. Findings of the Association for Computational Linguistics: EMNLP 2020. 2020."}}
{"id": "SnM-df2VzQq", "cdate": 1546300800000, "mdate": 1648671760370, "content": {"title": "CODAH: An Adversarially Authored Question-Answer Dataset for Common Sense", "abstract": "Commonsense reasoning is a critical AI capability, but it is difficult to construct challenging datasets that test common sense. Recent neural question answering systems, based on large pre-trained models of language, have already achieved near-human-level performance on commonsense knowledge benchmarks. These systems do not possess human-level common sense, but are able to exploit limitations of the datasets to achieve human-level scores. We introduce the CODAH dataset, an adversarially-constructed evaluation dataset for testing common sense. CODAH forms a challenging extension to the recently-proposed SWAG dataset, which tests commonsense knowledge using sentence-completion questions that describe situations observed in video. To produce a more difficult dataset, we introduce a novel procedure for question acquisition in which workers author questions designed to target weaknesses of state-of-the-art neural question answering systems. Workers are rewarded for submissions that models fail to answer correctly both before and after fine-tuning (in cross-validation). We create 2.8k questions via this procedure and evaluate the performance of multiple state-of-the-art question answering systems on our dataset. We observe a significant gap between human performance, which is 95.3%, and the performance of the best baseline accuracy of 67.5% by the BERT-Large model."}}
{"id": "rJNl_2l_WS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Sampling Informative Training Data for RNN Language Models", "abstract": ""}}
{"id": "SJWyWGz_WS", "cdate": 1483228800000, "mdate": null, "content": {"title": "VecShare: A Framework for Sharing Word Representation Vectors", "abstract": ""}}
