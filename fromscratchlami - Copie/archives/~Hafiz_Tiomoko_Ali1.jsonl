{"id": "OO2OlzQ9_I", "cdate": 1640995200000, "mdate": 1681833088832, "content": {"title": "Random matrices in service of ML footprint: ternary random features with no performance loss", "abstract": "In this article, we investigate the spectral behavior of random features kernel matrices of the type ${\\bf K} = \\mathbb{E}_{{\\bf w}} \\left[\\sigma\\left({\\bf w}^{\\sf T}{\\bf x}_i\\right)\\sigma\\left({\\bf w}^{\\sf T}{\\bf x}_j\\right)\\right]_{i,j=1}^n$, with nonlinear function $\\sigma(\\cdot)$, data ${\\bf x}_1, \\ldots, {\\bf x}_n \\in \\mathbb{R}^p$, and random projection vector ${\\bf w} \\in \\mathbb{R}^p$ having i.i.d. entries. In a high-dimensional setting where the number of data $n$ and their dimension $p$ are both large and comparable, we show, under a Gaussian mixture model for the data, that the eigenspectrum of ${\\bf K}$ is independent of the distribution of the i.i.d.(zero-mean and unit-variance) entries of ${\\bf w}$, and only depends on $\\sigma(\\cdot)$ via its (generalized) Gaussian moments $\\mathbb{E}_{z\\sim \\mathcal N(0,1)}[\\sigma'(z)]$ and $\\mathbb{E}_{z\\sim \\mathcal N(0,1)}[\\sigma''(z)]$. As a result, for any kernel matrix ${\\bf K}$ of the form above, we propose a novel random features technique, called Ternary Random Features (TRFs), that (i) asymptotically yields the same limiting kernel as the original ${\\bf K}$ in a spectral sense and (ii) can be computed and stored much more efficiently, by wisely tuning (in a data-dependent manner) the function $\\sigma$ and the random vector ${\\bf w}$, both taking values in $\\{-1,0,1\\}$. The computation of the proposed random features requires no multiplication, and a factor of $b$ times less bits for storage compared to classical random features such as random Fourier features, with $b$ the number of bits to store full precision values. Besides, it appears in our experiments on real data that the substantial gains in computation and storage are accompanied with somewhat improved performances compared to state-of-the-art random features methods."}}
{"id": "qwULHx9zld", "cdate": 1632875428929, "mdate": null, "content": {"title": "Random matrices in service of ML footprint: ternary random features with no performance loss", "abstract": "In this article, we investigate the spectral behavior of random features kernel matrices of the type ${\\bf K} = \\mathbb{E}_{{\\bf w}} \\left[\\sigma\\left({\\bf w}^{\\sf T}{\\bf x}_i\\right)\\sigma\\left({\\bf w}^{\\sf T}{\\bf x}_j\\right)\\right]_{i,j=1}^n$, with nonlinear function $\\sigma(\\cdot)$, data ${\\bf x}_1, \\ldots, {\\bf x}_n \\in \\mathbb{R}^p$, and random projection vector ${\\bf w} \\in \\mathbb{R}^p$ having i.i.d. entries. In a high-dimensional setting where the number of data $n$ and their dimension $p$ are both large and comparable, we show, under a Gaussian mixture model for the data, that the eigenspectrum of ${\\bf K}$ is independent of the distribution of the i.i.d.(zero-mean and unit-variance) entries of ${\\bf w}$, and only depends on $\\sigma(\\cdot)$ via its (generalized) Gaussian moments $\\mathbb{E}_{z\\sim \\mathcal N(0,1)}[\\sigma'(z)]$ and $\\mathbb{E}_{z\\sim \\mathcal N(0,1)}[\\sigma''(z)]$. As a result, for any kernel matrix ${\\bf K}$ of the form above, we propose a novel random features technique, called Ternary Random Features (TRFs), that (i) asymptotically yields the same limiting kernel as the original ${\\bf K}$ in a spectral sense and (ii) can be computed and stored much more efficiently, by wisely tuning (in a data-dependent manner) the function $\\sigma$ and the random vector ${\\bf w}$, both taking values in $\\{-1,0,1\\}$. The computation of the proposed random features requires no multiplication, and a factor of $b$ times less bits for storage compared to classical random features such as random Fourier features, with $b$ the number of bits to store full precision values. Besides, it appears in our experiments on real data that the substantial gains in computation and storage are accompanied with somewhat improved performances compared to state-of-the-art random features methods."}}
{"id": "nOAIA5aOm3", "cdate": 1609459200000, "mdate": 1681833088262, "content": {"title": "Random matrices in service of ML footprint: ternary random features with no performance loss", "abstract": "In this article, we investigate the spectral behavior of random features kernel matrices of the type ${\\bf K} = \\mathbb{E}_{{\\bf w}} \\left[\\sigma\\left({\\bf w}^{\\sf T}{\\bf x}_i\\right)\\sigma\\left({\\bf w}^{\\sf T}{\\bf x}_j\\right)\\right]_{i,j=1}^n$, with nonlinear function $\\sigma(\\cdot)$, data ${\\bf x}_1, \\ldots, {\\bf x}_n \\in \\mathbb{R}^p$, and random projection vector ${\\bf w} \\in \\mathbb{R}^p$ having i.i.d. entries. In a high-dimensional setting where the number of data $n$ and their dimension $p$ are both large and comparable, we show, under a Gaussian mixture model for the data, that the eigenspectrum of ${\\bf K}$ is independent of the distribution of the i.i.d.(zero-mean and unit-variance) entries of ${\\bf w}$, and only depends on $\\sigma(\\cdot)$ via its (generalized) Gaussian moments $\\mathbb{E}_{z\\sim \\mathcal N(0,1)}[\\sigma'(z)]$ and $\\mathbb{E}_{z\\sim \\mathcal N(0,1)}[\\sigma''(z)]$. As a result, for any kernel matrix ${\\bf K}$ of the form above, we propose a novel random features technique, called Ternary Random Feature (TRF), that (i) asymptotically yields the same limiting kernel as the original ${\\bf K}$ in a spectral sense and (ii) can be computed and stored much more efficiently, by wisely tuning (in a data-dependent manner) the function $\\sigma$ and the random vector ${\\bf w}$, both taking values in $\\{-1,0,1\\}$. The computation of the proposed random features requires no multiplication, and a factor of $b$ times less bits for storage compared to classical random features such as random Fourier features, with $b$ the number of bits to store full precision values. Besides, it appears in our experiments on real data that the substantial gains in computation and storage are accompanied with somewhat improved performances compared to state-of-the-art random features compression/quantization methods."}}
{"id": "O6t28_lJsF", "cdate": 1609459200000, "mdate": 1681833088242, "content": {"title": "Deciphering and Optimizing Multi-Task Learning: a Random Matrix Approach", "abstract": "This article provides theoretical insights into the inner workings of multi-task and transfer learning methods, by studying the tractable least-square support vector machine multi-task learning (LS-SVM MTL) method, in the limit of large ($p$) and numerous ($n$) data. By a random matrix analysis applied to a Gaussian mixture data model, the performance of MTL LS-SVM is shown to converge, as $n,p\\to\\infty$, to a deterministic limit involving simple (small-dimensional) statistics of the data. We prove (i) that the standard MTL LS-SVM algorithm is in general strongly biased and may dramatically fail (to the point that individual single-task LS-SVMs may outperform the MTL approach, even for quite resembling tasks): our analysis provides a simple method to correct these biases, and that we reveal (ii) the sufficient statistics at play in the method, which can be efficiently estimated, even for quite small datasets. The latter result is exploited to automatically optimize the hyperparameters without resorting to any cross-validation procedure. Experiments on popular datasets demonstrate that our improved MTL LS-SVM method is computationally-efficient and outperforms sometimes much more elaborate state-of-the-art multi-task and transfer learning techniques."}}
{"id": "Cri3xz59ga", "cdate": 1601308138106, "mdate": null, "content": {"title": "Deciphering and Optimizing Multi-Task Learning: a Random Matrix Approach", "abstract": "This article provides theoretical insights into the inner workings of multi-task and transfer learning methods, by studying the tractable least-square support vector machine multi-task learning (LS-SVM MTL) method, in the limit of large ($p$) and numerous ($n$) data. By a random matrix analysis applied to a Gaussian mixture data model, the performance of MTL LS-SVM is shown to converge, as $n,p\\to\\infty$, to a deterministic limit involving simple (small-dimensional) statistics of the data.\n\nWe prove (i) that the standard MTL LS-SVM algorithm is in general strongly biased and may dramatically fail (to the point that individual single-task LS-SVMs may outperform the MTL approach, even for quite resembling tasks): our analysis provides a simple method to correct these biases, and that we reveal (ii) the sufficient statistics at play in the method, which can be efficiently estimated, even for quite small datasets. The latter result is exploited to automatically optimize the hyperparameters without resorting to any cross-validation procedure. \n\nExperiments on popular datasets demonstrate that our improved MTL LS-SVM method is computationally-efficient and outperforms sometimes much more elaborate state-of-the-art multi-task and transfer learning techniques."}}
{"id": "feEgVnv03cN", "cdate": 1594451074175, "mdate": null, "content": {"title": "Coulomb Autoencoders", "abstract": "Learning the true density in high-dimensional feature spaces is a well-known problem in machine learning. In this work, we consider generative autoencoders based on maximum-mean discrepancy (MMD) and provide theoretical insights. In particular, (i) we prove that MMD coupled with Coulomb kernels has optimal convergence properties, which are similar to convex functionals, thus improving the training of autoencoders, and (ii) we provide a probabilistic bound on the generalization performance, highlighting some fundamental conditions to achieve better generalization. We validate the theory on synthetic examples and on the popular dataset of celebrities' faces, showing that our model, called Coulomb autoencoders, outperform the state-of-the-art."}}
{"id": "mGmNWqorL4Q", "cdate": 1577836800000, "mdate": 1681833088245, "content": {"title": "Coulomb Autoencoders", "abstract": "Learning the true density in high-dimensional feature spaces is a well-known problem in machine learning. In this work, we consider generative autoencoders based on maximum-mean discrepancy (MMD) and provide theoretical insights. In particular, (i) we prove that MMD coupled with Coulomb kernels has optimal convergence properties, which are similar to convex functionals, thus improving the training of autoencoders, and (ii) we provide a probabilistic bound on the generalization performance, highlighting some fundamental conditions to achieve better generalization. We validate the theory on synthetic examples and on the popular dataset of celebrities faces, showing that our model, called Coulomb autoencoders, outperform the state-of-the-art."}}
{"id": "tp1Bua6oq_C", "cdate": 1546300800000, "mdate": 1681833088227, "content": {"title": "Latent Heterogeneous Multilayer Community Detection", "abstract": "We propose a method for simultaneously detecting shared and unshared communities in heterogeneous multilayer weighted and undirected networks. The multilayer network is assumed to follow a generative probabilistic model that takes into account the similarities and dissimilarities between the communities. We make use of a variational Bayes approach for jointly inferring the shared and unshared hidden communities from multilayer network observations. We show that our approach outperforms state-of-the-art algorithms in detecting disparate (shared and private) communities on synthetic data as well as on real genome-wide fibroblast proliferation dataset."}}
{"id": "rQ-Cibo__MV", "cdate": 1514764800000, "mdate": 1681833089101, "content": {"title": "Random Matrix Asymptotics of Inner Product Kernel Spectral Clustering", "abstract": "We study in this article the asymptotic performance of spectral clustering with inner product kernel for Gaussian mixture models of high dimension with numerous samples. As is now classical in large dimensional spectral analysis, we establish a phase transition phenomenon by which a minimum distance between the class means and covariances is required for clustering to be possible from the dominant eigenvectors. Beyond this phase transition, we evaluate the asymptotic content of the dominant eigenvectors thus allowing for a full characterization of clustering performance. However, a surprising finding is that in some particular scenarios, the phase transition does not occur and clustering can be achieved irrespective of the class means and covariances. This is evidenced here in the case of the mixture of two Gaussian datasets having the same means and arbitrary difference between covariances."}}
{"id": "nV5g_XtjF32", "cdate": 1514764800000, "mdate": 1681833088867, "content": {"title": "Latent heterogeneous multilayer community detection", "abstract": "We propose a method for simultaneously detecting shared and unshared communities in heterogeneous multilayer weighted and undirected networks. The multilayer network is assumed to follow a generative probabilistic model that takes into account the similarities and dissimilarities between the communities. We make use of a variational Bayes approach for jointly inferring the shared and unshared hidden communities from multilayer network observations. We show that our approach outperforms state-of-the-art algorithms in detecting disparate (shared and private) communities on synthetic data as well as on real genome-wide fibroblast proliferation dataset."}}
