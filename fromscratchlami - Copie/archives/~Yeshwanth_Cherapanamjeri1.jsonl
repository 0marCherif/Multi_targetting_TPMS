{"id": "I29Kt0RwChs", "cdate": 1663850503018, "mdate": null, "content": {"title": "Robust Algorithms on Adaptive Inputs from Bounded Adversaries", "abstract": "We study dynamic algorithms robust to adaptive input generated from sources with bounded capabilities, such as sparsity or limited interaction. For example, we consider robust linear algebraic algorithms when the updates to the input are sparse but given by an adversary with access to a query oracle. We also study robust algorithms in the standard centralized setting, where an adversary queries an algorithm in an adaptive manner, but the number of interactions between the adversary and the algorithm is bounded. We first recall a unified framework of (Hassidim et al., 2020; Beimel et al., 2022; Attias et al., 2023) for answering $Q$ adaptive queries that incurs $\\widetilde{\\mathcal{O}}(\\sqrt{Q})$ overhead in space, which is roughly a quadratic improvement over the na\\\"{i}ve implementation, and only incurs a logarithmic overhead in query time. Although the general framework has diverse applications in machine learning and data science, such as adaptive distance estimation, kernel density estimation, linear regression, range queries, point queries,  and serves as a preliminary benchmark, we demonstrate even better algorithmic improvements for (1) reducing the pre-processing time for adaptive distance estimation and (2) permitting an unlimited number of adaptive queries for kernel density estimation. Finally, we complement our theoretical results with additional empirical evaluations. "}}
{"id": "NE0YlkgRo9x", "cdate": 1621630096723, "mdate": null, "content": {"title": "A single gradient step finds adversarial examples on random two-layers neural networks", "abstract": "Daniely and Schacham recently showed that gradient descent finds adversarial examples on random undercomplete two-layers ReLU neural networks. The term \u201cundercomplete\u201d refers to the fact that their proof only holds when the number of neurons is a vanishing fraction of the ambient dimension. We extend their result to the overcomplete case, where the number of neurons is larger than the dimension (yet also subexponential in the dimension). In fact we prove that a single step of gradient descent suffices. We also show this result for any subexponential width random neural network with smooth activation function."}}
{"id": "BKpNqR19JgD", "cdate": 1621630047249, "mdate": null, "content": {"title": "Adversarial Examples in Multi-Layer Random ReLU Networks", "abstract": "We consider the phenomenon of adversarial examples in ReLU networks with independent Gaussian parameters.  For networks of constant depth and with a large range of widths (for instance, it suffices if the width of each layer is polynomial in that of any other layer), small perturbations of input vectors lead to large changes of outputs.  This generalizes results of Daniely and Schacham (2020) for networks of rapidly decreasing width and of Bubeck et al (2021) for two-layer networks. Our proof shows that adversarial examples arise in these networks because the functions they compute are \\emph{locally} very similar to random linear functions. Bottleneck layers play a key role: the minimal width up to some point in the network determines scales and sensitivities of mappings computed up to that point.  The main result is for networks with constant depth, but we also show that some constraint on depth is necessary for a result of this kind, because there are suitably deep networks that, with constant probability, compute a function that is close to constant."}}
{"id": "IniuL6IfGLG", "cdate": 1609459200000, "mdate": null, "content": {"title": "A single gradient step finds adversarial examples on random two-layers neural networks", "abstract": "Daniely and Schacham recently showed that gradient descent finds adversarial examples on random undercomplete two-layers ReLU neural networks. The term \"undercomplete\" refers to the fact that their proof only holds when the number of neurons is a vanishing fraction of the ambient dimension. We extend their result to the overcomplete case, where the number of neurons is larger than the dimension (yet also subexponential in the dimension). In fact we prove that a single step of gradient descent suffices. We also show this result for any subexponential width random neural network with smooth activation function."}}
{"id": "tBD0XWQY2mY", "cdate": 1577836800000, "mdate": null, "content": {"title": "Algorithms for heavy-tailed statistics: regression, covariance estimation, and beyond", "abstract": "We study polynomial-time algorithms for linear regression and covariance estimation in the absence of strong (Gaussian) assumptions on the underlying distributions of samples, making assumptions instead about only finitely-many moments. We focus on how many samples are required to perform estimation and regression with high accuracy and exponentially-good success probability in the face of heavy-tailed data. For covariance estimation, linear regression, and several other problems in high-dimensional statistics, estimators have recently been constructed whose sample complexities and rates of statistical error match what is possible when the underlying distribution is Gaussian, but known algorithms for these estimators require exponential time. We narrow the gap between the Gaussian and heavy-tailed settings for polynomial-time estimators with: (a) a polynomial-time estimator which takes n samples from a d-dimensional random vector X with covariance \u03a3 and produces \u03a3 such that in spectral norm ||\u03a3 \u2212 \u03a3 ||2 \u2264 \u00d5(d 3/4/\u221an) w.p. 1\u22122\u2212d where the information-theoretically optimal error bound is \u00d5(\u221ad/n), while previous approaches to polynomial-time algorithms were stuck at \u00d5(d/\u221an) and (b) a polynomial-time algorithm which takes n samples (X i ,Y i ) where Y i = \u27e8 u,X i \u27e9 + i where both X and have a constant number of bounded moments and produces \u00fb such that the loss ||u \u2212 \u00fb||2 \u2264 O(d/n) w.p. 1\u22122\u2212d for any n \u2265 d 3/2 log(d). This (information-theoretically optimal) error is achieved by inefficient algorithms for any n \u226b d, while previous approaches to polynomial-time algorithms suffer loss \u03a9(d 2/n) and require n \u226b d 2. Our algorithms make crucial use of degree-8 sum-of-squares semidefinite programs. Both apply to any X which has constantly-many certifiably hypercontractive moments. We offer preliminary evidence that improving on these rates of error in polynomial time is not possible in the median of means framework our algorithms employ. Our work introduces new techniques to high-probability estimation, and suggests numerous new algorithmic questions in the following vein: when is it computationally feasible to do statistics in high dimensions with Gaussian-style errors when data is far from Gaussian?"}}
{"id": "hO4rQs86q3S", "cdate": 1577836800000, "mdate": null, "content": {"title": "Optimal Robust Linear Regression in Nearly Linear Time", "abstract": "We study the problem of high-dimensional robust linear regression where a learner is given access to $n$ samples from the generative model $Y = \\langle X,w^* \\rangle + \\epsilon$ (with $X \\in \\mathbb{R}^d$ and $\\epsilon$ independent), in which an $\\eta$ fraction of the samples have been adversarially corrupted. We propose estimators for this problem under two settings: (i) $X$ is L4-L2 hypercontractive, $\\mathbb{E} [XX^\\top]$ has bounded condition number and $\\epsilon$ has bounded variance and (ii) $X$ is sub-Gaussian with identity second moment and $\\epsilon$ is sub-Gaussian. In both settings, our estimators: (a) Achieve optimal sample complexities and recovery guarantees up to log factors and (b) Run in near linear time ($\\tilde{O}(nd / \\eta^6)$). Prior to our work, polynomial time algorithms achieving near optimal sample complexities were only known in the setting where $X$ is Gaussian with identity covariance and $\\epsilon$ is Gaussian, and no linear time estimators were known for robust linear regression in any setting. Our estimators and their analysis leverage recent developments in the construction of faster algorithms for robust mean estimation to improve runtimes, and refined concentration of measure arguments alongside Gaussian rounding techniques to improve statistical sample complexities."}}
{"id": "hGLXGogk15M", "cdate": 1577836800000, "mdate": null, "content": {"title": "List Decodable Mean Estimation in Nearly Linear Time", "abstract": "Learning from data in the presence of outliers is a fundamental problem in statistics. Until recently, no computationally efficient algorithms were known to compute the mean of a high dimensional distribution under natural assumptions in the presence of even a small fraction of outliers. In this paper, we consider robust statistics in the presence of overwhelming outliers where the majority of the dataset is introduced adversarially. With only an $\\alpha < 1/2$ fraction of \"inliers\" (clean data) the mean of a distribution is unidentifiable. However, in their influential work, [CSV17] introduces a polynomial time algorithm recovering the mean of distributions with bounded covariance by outputting a succinct list of $O(1/\\alpha)$ candidate solutions, one of which is guaranteed to be close to the true distributional mean; a direct analog of 'List Decoding' in the theory of error correcting codes. In this work, we develop an algorithm for list decodable mean estimation in the same setting achieving up to constants the information theoretically optimal recovery, optimal sample complexity, and in nearly linear time up to polylogarithmic factors in dimension. Our conceptual innovation is to design a descent style algorithm on a nonconvex landscape, iteratively removing minima to generate a succinct list of solutions. Our runtime bottleneck is a saddle-point optimization for which we design custom primal dual solvers for generalized packing and covering SDP's under Ky-Fan norms, which may be of independent interest."}}
{"id": "F7MevOK4am", "cdate": 1577836800000, "mdate": null, "content": {"title": "On Adaptive Distance Estimation", "abstract": "We provide a static data structure for distance estimation which supports {\\it adaptive} queries. Concretely, given a dataset $X = \\{x_i\\}_{i = 1}^n$ of $n$ points in $\\mathbb{R}^d$ and $0 < p \\leq 2$, we construct a randomized data structure with low memory consumption and query time which, when later given any query point $q \\in \\mathbb{R}^d$, outputs a $(1+\\epsilon)$-approximation of $\\lVert q - x_i \\rVert_p$ with high probability for all $i\\in[n]$. The main novelty is our data structure's correctness guarantee holds even when the sequence of queries can be chosen adaptively: an adversary is allowed to choose the $j$th query point $q_j$ in a way that depends on the answers reported by the data structure for $q_1,\\ldots,q_{j-1}$. Previous randomized Monte Carlo methods do not provide error guarantees in the setting of adaptively chosen queries. Our memory consumption is $\\tilde O((n+d)d/\\epsilon^2)$, slightly more than the $O(nd)$ required to store $X$ in memory explicitly, but with the benefit that our time to answer queries is only $\\tilde O(\\epsilon^{-2}(n + d))$, much faster than the naive $\\Theta(nd)$ time obtained from a linear scan in the case of $n$ and $d$ very large. Here $\\tilde O$ hides $\\log(nd/\\epsilon)$ factors. We discuss applications to nearest neighbor search and nonparametric estimation. Our method is simple and likely to be applicable to other domains: we describe a generic approach for transforming randomized Monte Carlo data structures which do not support adaptive queries to ones that do, and show that for the problem at hand, it can be applied to standard nonadaptive solutions to $\\ell_p$ norm estimation with negligible overhead in query time and a factor $d$ overhead in memory."}}
{"id": "6KDq-LBSpuB", "cdate": 1577836800000, "mdate": null, "content": {"title": "On Adaptive Distance Estimation", "abstract": "We provide a static data structure for distance estimation which supports {\\it adaptive} queries. Concretely, given a dataset $X = \\{x_i\\}_{i = 1}^n$ of $n$ points in $\\mathbb{R}^d$ and $0 &lt; p \\leq 2$, we construct a randomized data structure with low memory consumption and query time which, when later given any query point $q \\in \\mathbb{R}^d$, outputs a $(1+\\varepsilon)$-approximation of $\\|q - x_i\\|_p$ with high probability for all $i\\in[n]$. The main novelty is our data structure's correctness guarantee holds even when the sequence of queries can be chosen adaptively: an adversary is allowed to choose the $j$th query point $q_j$ in a way that depends on the answers reported by the data structure for $q_1,\\ldots,q_{j-1}$. Previous randomized Monte Carlo methods do not provide error guarantees in the setting of adaptively chosen queries. Our memory consumption is $\\tilde O(nd/\\varepsilon^2)$, slightly more than the $O(nd)$ required to store $X$ in memory explicitly, but with the benefit that our time to answer queries is only $\\tilde O(\\varepsilon^{-2}(n + d))$, much faster than the naive $\\Theta(nd)$ time obtained from a linear scan in the case of $n$ and $d$ very large. Here $\\tilde O$ hides $\\log(nd/\\varepsilon)$ factors. We discuss applications to nearest neighbor search and nonparametric estimation. Our method is simple and likely to applicable to other domains: we describe a generic approach for transforming randomized Monte Carlo data structures which do not support adaptive queries to ones that do, and show that for the problem at hand it can be applied to standard nonadaptive solutions to $\\ell_p$ norm estimation with negligible overhead in query time and a factor $d$ overhead in memory."}}
{"id": "4mUmxeQzfpN", "cdate": 1577836800000, "mdate": null, "content": {"title": "List Decodable Mean Estimation in Nearly Linear Time", "abstract": "Learning from data in the presence of outliers is a fundamental problem in statistics. Until recently, no computationally efficient algorithms were known to compute the mean of a high dimensional distribution under natural assumptions in the presence of even a small fraction of outliers. In this paper, we consider robust statistics in the presence of overwhelming outliers where the majority of the dataset is introduced adversarially. With only an fraction of \u201cin-liers\u201d (clean data) the mean of a distribution is unidentifiable. However, in their influential work, [1] introduces a polynomial time algorithm recovering the mean of distributions with bounded covariance by outputting a succinct list of O(1/\u03b1) candidate solutions, one of which is guaranteed to be close to the true distributional mean; a direct analog of `List Decoding' in the theory of error correcting codes. In this work, we develop an algorithm for list decodable mean estimation in the same setting achieving up to constants the information theoretically optimal recovery, optimal sample complexity, and in nearly linear time up to polylogarithmic factors in dimension. Our conceptual innovation is to design a descent style algorithm on a nonconvex landscape, iteratively removing minima to generate a succinct list of solutions. Our runtime bottleneck is a saddle-point optimization for which we design custom primal dual solvers for generalized packing and covering SDP's under Ky-Fan norms, which may be of independent interest. We refer the reader to [2] for the full version of this paper."}}
