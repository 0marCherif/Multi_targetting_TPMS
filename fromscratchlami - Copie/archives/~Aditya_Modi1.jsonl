{"id": "js2viORRVN", "cdate": 1664601836624, "mdate": 1664601836624, "content": {"title": "On the Statistical Efficiency of Reward-Free Exploration in Non-Linear RL", "abstract": "We study reward-free reinforcement learning (RL) under general non-linear function approximation, and establish sample efficiency and hardness results under various standard structural assumptions. On the positive side, we propose the RFOLIVE (Reward-Free OLIVE) algorithm for sample-efficient reward-free exploration under minimal structural assumptions, which covers the previously studied settings of linear MDPs (Jin et al., 2020b), linear completeness (Zanette et al., 2020b) and low-rank MDPs with unknown representation (Modi et al., 2021). Our analyses indicate that the explorability or reachability assumptions, previously made for the latter two settings, are not necessary statistically for reward-free exploration. On the negative side, we provide a statistical hardness result for both reward-free and reward-aware exploration under linear completeness assumptions when the underlying features are unknown, showing an exponential separation between low-rank and linear completeness settings."}}
{"id": "65eqtvEShR8", "cdate": 1652737630617, "mdate": null, "content": {"title": "On the Statistical Efficiency of Reward-Free Exploration in Non-Linear RL", "abstract": "We study reward-free reinforcement learning (RL) under general non-linear function approximation, and establish sample efficiency and hardness results under various standard structural assumptions. On the positive side, we propose the RFOLIVE (Reward-Free OLIVE) algorithm for sample-efficient reward-free exploration under minimal structural assumptions, which covers the previously studied settings of linear MDPs (Jin et al., 2020b), linear completeness (Zanette et al., 2020b) and low-rank MDPs with unknown representation (Modi et al., 2021). Our analyses indicate that the explorability or reachability assumptions, previously made for the latter two settings, are not necessary statistically for reward-free exploration. On the negative side, we provide a statistical hardness result for both reward-free and reward-aware exploration under linear completeness assumptions when the underlying features are unknown, showing an exponential separation between low-rank and linear completeness settings."}}
{"id": "eeeL7fQq8Zv", "cdate": 1642451605812, "mdate": 1642451605812, "content": {"title": "Joint Learning-Based Stabilization of Multiple Unknown Linear Systems", "abstract": "Learning-based control of linear systems received a lot of attentions recently. In popular settings,\nthe true dynamical models are unknown to the decision-maker and need to be interactively learned by\napplying control inputs to the systems. Unlike the matured literature of efficient reinforcement learning\npolicies for adaptive control of a single system, results on joint learning of multiple systems are not\ncurrently available. Especially, the important problem of fast and reliable joint-stabilization remains\nunaddressed and so is the focus of this work. We propose a novel joint learning-based stabilization\nalgorithm for quickly learning stabilizing policies for all systems understudy, from the data of unstable\nstate trajectories. The presented procedure is shown to be notably effective such that it stabilizes the\nfamily of dynamical systems in an extremely short time period."}}
{"id": "1fKniWg76_b", "cdate": 1642451526848, "mdate": 1642451526848, "content": {"title": "Joint Learning of Linear Time-Invariant Dynamical Systems", "abstract": "Learning the parameters of a linear time-invariant dynamical system (LTIDS) is a problem of current interest. In many applications, one is interested in jointly learning the parameters of multiple related LTIDS, which remains unexplored to date. To that end, we develop a joint estimator for learning the transition matrices of LTIDS that share common basis matrices. Further, we establish finite-time error bounds that depend on the underlying sample size, dimension, number of tasks, and spectral properties of the transition matrices. The results are obtained under mild regularity assumptions and showcase the gains from pooling information across LTIDS, in comparison to learning each system separately. We also study the impact of misspecifying the joint structure of the transition matrices and show that the established results are robust in the presence of moderate misspecifications."}}
{"id": "sj5oI8KDDAG", "cdate": 1640995200000, "mdate": 1682318210986, "content": {"title": "Joint Learning-Based Stabilization of Multiple Unknown Linear Systems", "abstract": "Learning-based control of linear systems received a lot of attentions recently. In popular settings, the true dynamical models are unknown to the decision-maker and need to be interactively learned by applying control inputs to the systems. Unlike the matured literature of efficient reinforcement learning policies for adaptive control of a single system, results on joint learning of multiple systems are not currently available. Especially, the important problem of fast and reliable joint-stabilization remains unaddressed and so is the focus of this work. We propose a novel joint learning-based stabilization algorithm for quickly learning stabilizing policies for all systems understudy, from the data of unstable state trajectories. The presented procedure is shown to be notably effective such that it stabilizes the family of dynamical systems in an extremely short time period."}}
{"id": "79FSGOjdur", "cdate": 1640995200000, "mdate": 1681501096626, "content": {"title": "On the Statistical Efficiency of Reward-Free Exploration in Non-Linear RL", "abstract": ""}}
{"id": "xoiODx6gahk", "cdate": 1620338129735, "mdate": null, "content": {"title": "Model-free Representation Learning and Exploration in Low-rank MDPs", "abstract": "The low rank MDP has emerged as an important model for studying representation learning and exploration in reinforcement learning. With a known representation, several model-free exploration strategies exist. In contrast, all algorithms for the unknown representation setting are model-based, thereby requiring the ability to model the full dynamics. In this work, we present the first model-free representation learning algorithms for low rank MDPs. The key algorithmic contribution is a new minimax representation learning objective, for which we provide variants with differing tradeoffs in their statistical and computational properties. We interleave this representation learning step with an exploration strategy to cover the state space in a reward-free manner. The resulting algorithms are provably sample efficient and can accommodate general function approximation to scale to complex environments."}}
{"id": "Yzm5PoJnZvS", "cdate": 1609459200000, "mdate": 1682318211421, "content": {"title": "Provably Efficient Reinforcement Learning Under Linear Model Structures: From Tabular to Feature Based Exploration", "abstract": ""}}
{"id": "EyVrmY89Q1", "cdate": 1609459200000, "mdate": null, "content": {"title": "Model-free Representation Learning and Exploration in Low-rank MDPs", "abstract": "The low rank MDP has emerged as an important model for studying representation learning and exploration in reinforcement learning. With a known representation, several model-free exploration strategies exist. In contrast, all algorithms for the unknown representation setting are model-based, thereby requiring the ability to model the full dynamics. In this work, we present the first model-free representation learning algorithms for low rank MDPs. The key algorithmic contribution is a new minimax representation learning objective, for which we provide variants with differing tradeoffs in their statistical and computational properties. We interleave this representation learning step with an exploration strategy to cover the state space in a reward-free manner. The resulting algorithms are provably sample efficient and can accommodate general function approximation to scale to complex environments."}}
{"id": "DO1zrFuDC8", "cdate": 1609459200000, "mdate": 1681503828044, "content": {"title": "Joint Learning of Linear Time-Invariant Dynamical Systems", "abstract": ""}}
