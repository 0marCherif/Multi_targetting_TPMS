{"id": "SaRbsVSaaxr", "cdate": 1635535597416, "mdate": null, "content": {"title": "ETC: Encoding Long and Structured Inputs in Transformers", "abstract": "Transformer models have advanced the state\nof the art in many Natural Language Processing (NLP) tasks. In this paper, we\npresent a new Transformer architecture, Extended Transformer Construction (ETC), that\naddresses two key challenges of standard\nTransformer architectures, namely scaling input length and encoding structured inputs. To\nscale attention to longer inputs, we introduce\na novel global-local attention mechanism between global tokens and regular input tokens.\nWe also show that combining global-local attention with relative position encodings and\na Contrastive Predictive Coding (CPC) pretraining objective allows ETC to encode structured inputs. We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs."}}
{"id": "H4ph4BG-Hfs", "cdate": 1635535329337, "mdate": null, "content": {"title": "Big Bird: Transformers for Longer Sequences", "abstract": "Transformers-based models, such as BERT, have been one of the most successful\ndeep learning models for NLP. Unfortunately, one of their core limitations is the\nquadratic dependency (mainly in terms of memory) on the sequence length due to\ntheir full attention mechanism. To remedy this, we propose, BIGBIRD, a sparse\nattention mechanism that reduces this quadratic dependency to linear. We show\nthat BIGBIRD is a universal approximator of sequence functions and is Turing\ncomplete, thereby preserving these properties of the quadratic, full attention model.\nAlong the way, our theoretical analysis reveals some of the benefits of having\nO(1) global tokens (such as CLS), that attend to the entire sequence as part of the\nsparse attention mechanism. The proposed sparse attention can handle sequences\nof length up to 8x of what was previously possible using similar hardware. As\na consequence of the capability to handle longer context, BIGBIRD drastically\nimproves performance on various NLP tasks such as question answering and\nsummarization. We also propose novel applications to genomics data."}}
{"id": "RQLRAHOrIDA", "cdate": 1634408440259, "mdate": 1634408440259, "content": {"title": "ReadTwice: Reading Very Large Documents with Memories", "abstract": "Knowledge-intensive tasks such as question answering often require assimilating information from different sections of large inputs such as books or article collections. We propose ReadTwice, a simple and effective technique that combines several strengths of prior approaches to model long-range dependencies with Transformers. The main idea is to read text in small segments, in parallel, summarizing each segment into a memory table to be used in a second read of the text. We show that the method outperforms models of comparable size on several question answering (QA) datasets and sets a new state of the art on the challenging NarrativeQA task, with questions about entire books. Source code and pre-trained checkpoints for ReadTwice can be found at https://goo.gle/research-readtwice."}}
{"id": "9t6OZi-PrR", "cdate": 1622132518036, "mdate": null, "content": {"title": "Fair Hierarchical Clustering", "abstract": "As machine learning has become more prevalent, researchers have begun to recog-nize the necessity of ensuring machine learning systems are fair. Recently, there hasbeen an interest in defining a notion of fairness that mitigates over-representationin traditional clustering.\n\nIn this paper we extend this notion to hierarchical clustering, where the goal is torecursively partition the data to optimize a specific objective. For various naturalobjectives, we obtain simple, efficient algorithms to find a provably good fairhierarchical clustering. Empirically, we show that our algorithms can find a fairhierarchical clustering, with only a negligible loss in the objective."}}
{"id": "bjhR7F7ThhZ", "cdate": 1609459200000, "mdate": null, "content": {"title": "OmniNet: Omnidirectional Representations from Transformers", "abstract": "This paper proposes Omnidirectional Representations from Transformers (OmniNet). In OmniNet, instead of maintaining a strictly horizontal receptive field, each token is allowed to attend to all tokens in the entire network. This process can also be interpreted as a form of extreme or intensive attention mechanism that has the receptive field of the entire width and depth of the network. To this end, the omnidirectional attention is learned via a meta-learner, which is essentially another self-attention based model. In order to mitigate the computationally expensive costs of full receptive field attention, we leverage efficient self-attention models such as kernel-based (Choromanski et al.), low-rank attention (Wang et al.) and/or Big Bird (Zaheer et al.) as the meta-learner. Extensive experiments are conducted on autoregressive language modeling (LM1B, C4), Machine Translation, Long Range Arena (LRA), and Image Recognition. The experiments show that OmniNet achieves considerable improvements across these tasks, including achieving state-of-the-art performance on LM1B, WMT'14 En-De/En-Fr, and Long Range Arena. Moreover, using omnidirectional representation in Vision Transformers leads to significant improvements on image recognition tasks on both few-shot learning and fine-tuning setups."}}
{"id": "qVyeW-grC2k", "cdate": 1601308179150, "mdate": null, "content": {"title": "Long Range Arena : A Benchmark for Efficient Transformers ", "abstract": "Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. To this date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. This paper proposes a systematic and unified benchmark, Long Range Arena, specifically focused on evaluating model quality under long-context scenarios. Our benchmark is a suite of tasks consisting of sequences ranging from $1K$ to $16K$ tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on our newly proposed benchmark suite. Long Range Arena paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle."}}
{"id": "rbRVBO2IQom", "cdate": 1577836800000, "mdate": null, "content": {"title": "Neural Structured Learning: Training Neural Networks with Structured Signals", "abstract": "We present Neural Structured Learning (NSL) in TensorFlow [2], a new learning paradigm to train neural networks by leveraging structured signals in addition to feature inputs. Structure can be explicit as represented by a graph, or implicit, either induced by adversarial perturbation or inferred using techniques like embedding learning. NSL is open-sourced as part of the TensorFlow [3] ecosystem and is widely used in Google across many products and services. In this tutorial, we provide an overview of the NSL framework including various libraries, tools, and APIs as well as demonstrate the practical use of NSL in different applications. The NSL website is hosted at www.tensorflow.org/neural_structured_learning, which includes details about the theoretical foundations of the technology, extensive API documentation, and hands-on tutorials."}}
{"id": "nuF-4shB4ZC", "cdate": 1577836800000, "mdate": null, "content": {"title": "Big Bird: Transformers for Longer Sequences", "abstract": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data."}}
{"id": "jBoapqxlZi", "cdate": 1577836800000, "mdate": null, "content": {"title": "Long Range Arena: A Benchmark for Efficient Transformers", "abstract": "Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. To this date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. This paper proposes a systematic and unified benchmark, LRA, specifically focused on evaluating model quality under long-context scenarios. Our benchmark is a suite of tasks consisting of sequences ranging from $1K$ to $16K$ tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on our newly proposed benchmark suite. LRA paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle. Our benchmark code will be released at https://github.com/google-research/long-range-arena."}}
{"id": "imlk618ddQU", "cdate": 1577836800000, "mdate": null, "content": {"title": "ETC: Encoding Long and Structured Inputs in Transformers", "abstract": "Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, Li Yang. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020."}}
