{"id": "8rZYMpFUgK", "cdate": 1652737271759, "mdate": null, "content": {"title": "DAGMA: Learning DAGs via M-matrices and a Log-Determinant Acyclicity Characterization", "abstract": "The combinatorial problem of learning directed acyclic graphs (DAGs) from data was recently framed as a purely continuous optimization problem by leveraging a differentiable acyclicity characterization of DAGs based on the trace of a matrix exponential function. Existing acyclicity characterizations are based on the idea that powers of an adjacency matrix contain information about walks and cycles. In this work, we propose a new acyclicity characterization based on the log-determinant (log-det) function, which leverages the nilpotency property of DAGs. To deal with the inherent asymmetries of a DAG, we relate the domain of our log-det characterization to the set of $\\textit{M-matrices}$, which is a key difference to the classical log-det function defined over the cone of positive definite matrices.\nSimilar to acyclicity functions previously proposed, our characterization is also exact and differentiable. However, when compared to existing characterizations, our log-det function: (1) Is better at detecting large cycles; (2) Has better-behaved gradients; and (3) Its runtime is in practice about an order of magnitude faster. From the optimization side, we drop the typically used augmented Lagrangian scheme and propose DAGMA ($\\textit{Directed Acyclic Graphs via M-matrices for Acyclicity}$), a method that resembles the central path for barrier methods. Each point in the central path of DAGMA is a solution to an unconstrained problem regularized by our log-det function, then we show that at the limit of the central path the solution is guaranteed to be a DAG. Finally, we provide extensive experiments for $\\textit{linear}$ and $\\textit{nonlinear}$ SEMs and show that our approach can reach large speed-ups and smaller structural Hamming distances against state-of-the-art methods. Code implementing the proposed method is open-source and publicly available at https://github.com/kevinsbello/dagma."}}
{"id": "fzOVSvWDA3f", "cdate": 1640995200000, "mdate": 1681694081393, "content": {"title": "DAGMA: Learning DAGs via M-matrices and a Log-Determinant Acyclicity Characterization", "abstract": "The combinatorial problem of learning directed acyclic graphs (DAGs) from data was recently framed as a purely continuous optimization problem by leveraging a differentiable acyclicity characterization of DAGs based on the trace of a matrix exponential function. Existing acyclicity characterizations are based on the idea that powers of an adjacency matrix contain information about walks and cycles. In this work, we propose a new acyclicity characterization based on the log-determinant (log-det) function, which leverages the nilpotency property of DAGs. To deal with the inherent asymmetries of a DAG, we relate the domain of our log-det characterization to the set of $\\textit{M-matrices}$, which is a key difference to the classical log-det function defined over the cone of positive definite matrices. Similar to acyclicity functions previously proposed, our characterization is also exact and differentiable. However, when compared to existing characterizations, our log-det function: (1) Is better at detecting large cycles; (2) Has better-behaved gradients; and (3) Its runtime is in practice about an order of magnitude faster. From the optimization side, we drop the typically used augmented Lagrangian scheme and propose DAGMA ($\\textit{DAGs via M-matrices for Acyclicity}$), a method that resembles the central path for barrier methods. Each point in the central path of DAGMA is a solution to an unconstrained problem regularized by our log-det function, then we show that at the limit of the central path the solution is guaranteed to be a DAG. Finally, we provide extensive experiments for $\\textit{linear}$ and $\\textit{nonlinear}$ SEMs and show that our approach can reach large speed-ups and smaller structural Hamming distances against state-of-the-art methods. Code implementing the proposed method is open-source and publicly available at https://github.com/kevinsbello/dagma."}}
{"id": "TUVJ3T_Achf", "cdate": 1640995200000, "mdate": 1681694081541, "content": {"title": "A View of Exact Inference in Graphs from the Degree-4 Sum-of-Squares Hierarchy", "abstract": "Performing inference in graphs is a common task within several machine learning problems, e.g., image segmentation, community detection, among others. For a given undirected connected graph, we tackle the statistical problem of exactly recovering an unknown ground-truth binary labeling of the nodes from a single corrupted observation of each edge. Such problem can be formulated as a quadratic combinatorial optimization problem over the boolean hypercube, where it has been shown before that one can (with high probability and in polynomial time) exactly recover the ground-truth labeling of graphs that have an isoperimetric number that grows with respect to the number of nodes (e.g., complete graphs, regular expanders). In this work, we apply a powerful hierarchy of relaxations, known as the sum-of-squares (SoS) hierarchy, to the combinatorial problem. Motivated by empirical evidence on the improvement in exact recoverability, we center our attention on the degree-4 SoS relaxation and set out to understand the origin of such improvement from a graph theoretical perspective. We show that the solution of the dual of the relaxed problem is related to finding edge weights of the Johnson and Kneser graphs, where the weights fulfill the SoS constraints and intuitively allow the input graph to increase its algebraic connectivity. Finally, as byproduct of our analysis, we derive a novel Cheeger-type lower bound for the algebraic connectivity of graphs with signed edge weights."}}
{"id": "DzVPxyKgWmQ", "cdate": 1640995200000, "mdate": 1681694081406, "content": {"title": "On the Fundamental Limits of Exact Inference in Structured Prediction", "abstract": "Inference in structured prediction is naturally modeled with a graph, where the goal is to recover the unknown true label for each node given noisy observations corresponding to nodes and edges. The focus of this paper is on the fundamental limits of exact recovery irrespective of computational efficiency, assuming the generative process proposed by [1]. Analyzing the fundamental limits is crucial for algorithm evaluation and development. In this regard, we establish the information-theoretic limit bounds and show that there exists a gap between the limits and the performance of the existent tractable method [2], implying the need for further development of algorithms for exact inference. The fundamental limit we suggest applies to general connected graphs and involves graphical metrics such as the Cheeger constant and the maximum degree. Finally, we reveal that the sufficient and necessary conditions derived from the limit bounds are tight up to a logarithmic factor for a wide range of graphs."}}
{"id": "-DyvEp1VsmT", "cdate": 1621629855644, "mdate": null, "content": {"title": "Inverse Reinforcement Learning in a Continuous State Space with Formal Guarantees", "abstract": "Inverse Reinforcement Learning (IRL) is the problem of finding a reward function which describes observed/known expert behavior.  The IRL setting is remarkably useful for automated control, in situations where the reward function is difficult to specify manually or as a means to extract agent preference. In this work, we provide a new IRL algorithm for the continuous state space setting with unknown transition dynamics by modeling the system using a basis of orthonormal functions. Moreover, we provide a proof of correctness and formal guarantees on the sample and time complexity of our algorithm.  Finally, we present synthetic experiments to corroborate our theoretical guarantees."}}
{"id": "s3Ss2vzF4fQ", "cdate": 1609459200000, "mdate": null, "content": {"title": "On the Fundamental Limits of Exact Inference in Structured Prediction", "abstract": "Inference is a main task in structured prediction and it is naturally modeled with a graph. In the context of Markov random fields, noisy observations corresponding to nodes and edges are usually involved, and the goal of exact inference is to recover the unknown true label for each node precisely. The focus of this paper is on the fundamental limits of exact recovery irrespective of computational efficiency, assuming the generative process proposed by Globerson et al. (2015). We derive the necessary condition for any algorithm and the sufficient condition for maximum likelihood estimation to achieve exact recovery with high probability, and reveal that the sufficient and necessary conditions are tight up to a logarithmic factor for a wide range of graphs. Finally, we show that there exists a gap between the fundamental limits and the performance of the computationally tractable method of Bello and Honorio (2019), which implies the need for further development of algorithms for exact inference."}}
{"id": "TP8Wex0fDDm", "cdate": 1609459200000, "mdate": 1653018803342, "content": {"title": "A Le Cam Type Bound for Adversarial Learning and Applications", "abstract": "Robustness of machine learning methods is essential for modern practical applications. Given the arms race between attack and defense mechanisms, it is essential to understand the fundamental limits of any conceivable learning method used in an adversarial setting. In this work, we focus on the problem of learning from noise-injected data, where the existing literature falls short by either assuming a specific adversary model or by over-specifying the learning problem. We shed light on the information-theoretic limits of adversarial learning without assuming a particular adversary. Specifically, we derive a general Le Cam type bound for learning from noise-injected data. Finally, we apply our general bounds to a canonical set of non-trivial learning problems and provide examples of common types of noise-injected data."}}
{"id": "Ml5og4-EuAU", "cdate": 1609459200000, "mdate": null, "content": {"title": "Inverse Reinforcement Learning in the Continuous Setting with Formal Guarantees", "abstract": "Inverse Reinforcement Learning (IRL) is the problem of finding a reward function which describes observed/known expert behavior. The IRL setting is remarkably useful for automated control, in situations where the reward function is difficult to specify manually or as a means to extract agent preference. In this work, we provide a new IRL algorithm for the continuous state space setting with unknown transition dynamics by modeling the system using a basis of orthonormal functions. Moreover, we provide a proof of correctness and formal guarantees on the sample and time complexity of our algorithm. Finally, we present synthetic experiments to corroborate our theoretical guarantees."}}
{"id": "DAO_X6KVTUX", "cdate": 1609459200000, "mdate": null, "content": {"title": "A Thorough View of Exact Inference in Graphs from the Degree-4 Sum-of-Squares Hierarchy", "abstract": "Performing inference in graphs is a common task within several machine learning problems, e.g., image segmentation, community detection, among others. For a given undirected connected graph, we tackle the statistical problem of exactly recovering an unknown ground-truth binary labeling of the nodes from a single corrupted observation of each edge. Such problem can be formulated as a quadratic combinatorial optimization problem over the boolean hypercube, where it has been shown before that one can (with high probability and in polynomial time) exactly recover the ground-truth labeling of graphs that have an isoperimetric number that grows with respect to the number of nodes (e.g., complete graphs, regular expanders). In this work, we apply a powerful hierarchy of relaxations, known as the sum-of-squares (SoS) hierarchy, to the combinatorial problem. Motivated by empirical evidence on the improvement in exact recoverability, we center our attention on the degree-4 SoS relaxation and set out to understand the origin of such improvement from a graph theoretical perspective. We show that the solution of the dual of the relaxed problem is related to finding edge weights of the Johnson and Kneser graphs, where the weights fulfill the SoS constraints and intuitively allow the input graph to increase its algebraic connectivity. Finally, as byproduct of our analysis, we derive a novel Cheeger-type lower bound for the algebraic connectivity of graphs with signed edge weights."}}
{"id": "A2CmhoQfWa", "cdate": 1609459200000, "mdate": 1681694081497, "content": {"title": "Inverse Reinforcement Learning in a Continuous State Space with Formal Guarantees", "abstract": "Inverse Reinforcement Learning (IRL) is the problem of finding a reward function which describes observed/known expert behavior. The IRL setting is remarkably useful for automated control, in situations where the reward function is difficult to specify manually or as a means to extract agent preference. In this work, we provide a new IRL algorithm for the continuous state space setting with unknown transition dynamics by modeling the system using a basis of orthonormal functions. Moreover, we provide a proof of correctness and formal guarantees on the sample and time complexity of our algorithm. Finally, we present synthetic experiments to corroborate our theoretical guarantees."}}
