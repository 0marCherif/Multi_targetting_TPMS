{"id": "qdpssm3f-W", "cdate": 1672531200000, "mdate": 1682675765905, "content": {"title": "Extending the Pre-Training of BLOOM for Improved Support of Traditional Chinese: Models, Methods and Results", "abstract": "In this paper we present the multilingual language model BLOOM-zh that features enhanced support for Traditional Chinese. BLOOM-zh has its origins in the open-source BLOOM models presented by BigScience in 2022. Starting from released models, we extended the pre-training of BLOOM by additional 7.4 billion tokens in Traditional Chinese and English covering a variety of domains such as news articles, books, encyclopedias, educational materials as well as spoken language. In order to show the properties of BLOOM-zh, both existing and newly created benchmark scenarios are used for evaluating the performance. BLOOM-zh outperforms its predecessor on most Traditional Chinese benchmarks while maintaining its English capability. We release all our models to the research community."}}
{"id": "gbjtivlzMaZ", "cdate": 1640995200000, "mdate": 1682675765837, "content": {"title": "HanTrans: An Empirical Study on Cross-Era Transferability of Chinese Pre-trained Language Model", "abstract": ""}}
{"id": "qF_oCEgkId", "cdate": 1609459200000, "mdate": 1682675765876, "content": {"title": "Roof-BERT: Divide Understanding Labour and Join in Work", "abstract": "Recent work on enhancing BERT-based language representation models with knowledge graphs (KGs) and knowledge bases (KBs) has yielded promising results on multiple NLP tasks. State-of-the-art approaches typically integrate the original input sentences with KG triples and feed the combined representation into a BERT model. However, as the sequence length of a BERT model is limited, such a framework supports little knowledge other than the original input sentences and is thus forced to discard some knowledge. This problem is especially severe for downstream tasks for which the input is a long paragraph or even a document, such as QA or reading comprehension tasks. We address this problem with Roof-Transformer, a model with two underlying BERTs and a fusion layer on top. One underlying BERT encodes the knowledge resources and the other one encodes the original input sentences, and the fusion layer integrates the two resultant encodings. Experimental results on a QA task and the GLUE benchmark attest the effectiveness of the proposed model."}}
{"id": "fWxdGEmXPYY", "cdate": 1609459200000, "mdate": 1639482846140, "content": {"title": "H-FND: Hierarchical False-Negative Denoising for Distant Supervision Relation Extraction", "abstract": "Jhih-wei Chen, Tsu-Jui Fu, Chen-Kang Lee, Wei-Yun Ma. Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021."}}
{"id": "cH7PVwP_89", "cdate": 1609459200000, "mdate": 1682675765864, "content": {"title": "DCT: Dynamic Compressive Transformer for Modeling Unbounded Sequence", "abstract": "In this paper, we propose Dynamic Compressive Transformer (DCT), a transformer-based framework for modeling the unbounded sequence. In contrast to the previous baselines which append every sentence representation to memory, conditionally selecting and appending them is a more reasonable solution to deal with unlimited long sequences. Our model uses a policy that determines whether the sequence should be kept in memory with a compressed state or discarded during the training process. With the benefits of retaining semantically meaningful sentence information in the memory system, our experiment results on Enwik8 benchmark show that DCT outperforms the previous state-of-the-art (SOTA) model."}}
{"id": "oY7yP8POCH4", "cdate": 1577836800000, "mdate": null, "content": {"title": "Relation Extraction Exploiting Full Dependency Forests", "abstract": "Dependency syntax has long been recognized as a crucial source of features for relation extraction. Previous work considers 1-best trees produced by a parser during preprocessing. However, error propagation from the out-of-domain parser may impact the relation extraction performance. We propose to leverage full dependency forests for this task, where a full dependency forest encodes all possible trees. Such representations of full dependency forests provide a differentiable connection between a parser and a relation extraction model, and thus we are also able to study adjusting the parser parameters based on end-task loss. Experiments on three datasets show that full dependency forests and parser adjustment give significant improvements over carefully designed baselines, showing state-of-the-art or competitive performances on biomedical or newswire benchmarks."}}
{"id": "mtsrzibFGl", "cdate": 1577836800000, "mdate": null, "content": {"title": "Why Attention? Analyze BiLSTM Deficiency and Its Remedies in the Case of NER", "abstract": "BiLSTM has been prevalently used as a core module for NER in a sequence-labeling setup. State-of-the-art approaches use BiLSTM with additional resources such as gazetteers, language-modeling, or multi-task supervision to further improve NER. This paper instead takes a step back and focuses on analyzing problems of BiLSTM itself and how exactly self-attention can bring improvements. We formally show the limitation of (CRF-)BiLSTM in modeling cross-context patterns for each word \u2013 the XOR limitation. Then, we show that two types of simple cross-structures \u2013 self-attention and Cross-BiLSTM \u2013 can effectively remedy the problem. We test the practical impacts of the deficiency on real-world NER datasets, OntoNotes 5.0 and WNUT 2017, with clear and consistent improvements over the baseline, up to 8.7% on some of the multi-token entity mentions. We give in-depth analyses of the improvements across several aspects of NER, especially the identification of multi-token mentions. This study should lay a sound foundation for future improvements on sequence-labeling NER1."}}
{"id": "m1tggG8BBo3", "cdate": 1577836800000, "mdate": null, "content": {"title": "Predict and Use Latent Patterns for Short-Text Conversation", "abstract": "Many neural network models nowadays have achieved promising performances in Chit-chat settings. The majority of them rely on an encoder for understanding the post and a decoder for generating the response. Without given assigned semantics, the models lack the fine-grained control over responses as the semantic mapping between posts and responses is hidden on the fly within the end-to-end manners. Some previous works utilize sampled latent words as a controllable semantic form to drive the generated response around the work, but few works attempt to use more complex semantic patterns to guide the generation. In this paper, we propose to use more detailed semantic forms, including latent responses and part-of-speech sequences sampled from the corresponding distributions, as the controllable semantics to guide the generation. Our results show that the richer semantics are not only able to provide informative and diverse responses, but also increase the overall performance of response quality, including fluency and coherence."}}
{"id": "jCcVX12uII", "cdate": 1577836800000, "mdate": null, "content": {"title": "Headword-Oriented Entity Linking: A Special Entity Linking Task with Dataset and Baseline", "abstract": "In this paper, we design headword-oriented entity linking (HEL), a specialized entity linking problem in which only the headwords of the entities are to be linked to knowledge bases; mention scopes of the entities do not need to be identified in the problem setting. This special task is motivated by the fact that in many articles referring to specific products, the complete full product names are rarely written; instead, they are often abbreviated to shorter, irregular versions or even just to their headwords, which are usually their product types, such as \u201cstick\u201d or \u201cmask\u201d in a cosmetic context. To fully design the special task, we construct a labeled cosmetic corpus as a public benchmark for this problem, and propose a product embedding model to address the task, where each product corresponds to a dense representation to encode the different information on products and their context jointly. Besides, to increase training data, we propose a special transfer learning framework in which distant supervision with heuristic patterns is first utilized, followed by supervised learning using a small amount of manually labeled data. The experimental results show that our model provides a strong benchmark performance on the special task."}}
{"id": "fDxJfUV10l5", "cdate": 1577836800000, "mdate": null, "content": {"title": "H-FND: Hierarchical False-Negative Denoising for Distant Supervision Relation Extraction", "abstract": "Although distant supervision automatically generates training data for relation extraction, it also introduces false-positive (FP) and false-negative (FN) training instances to the generated datasets. Whereas both types of errors degrade the final model performance, previous work on distant supervision denoising focuses more on suppressing FP noise and less on resolving the FN problem. We here propose H-FND, a hierarchical false-negative denoising framework for robust distant supervision relation extraction, as an FN denoising solution. H-FND uses a hierarchical policy which first determines whether non-relation (NA) instances should be kept, discarded, or revised during the training process. For those learning instances which are to be revised, the policy further reassigns them appropriate relations, making them better training inputs. Experiments on SemEval-2010 and TACRED were conducted with controlled FN ratios that randomly turn the relations of training and validation instances into negatives to generate FN instances. In this setting, H-FND can revise FN instances correctly and maintains high F1 scores even when 50% of the instances have been turned into negatives. Experiment on NYT10 is further conducted to shows that H-FND is applicable in a realistic setting."}}
