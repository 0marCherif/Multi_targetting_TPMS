{"id": "-MQ7cSuKlF", "cdate": 1699161268129, "mdate": 1699161268129, "content": {"title": "Correspondence Transformers With Asymmetric Feature Learning and Matching Flow Super-Resolution", "abstract": "This paper solves the problem of learning dense visual correspondences between different object instances of the same category with only sparse annotations. We decompose this pixel-level semantic matching problem into two easier ones:(i) First, local feature descriptors of source and target images need to be mapped into shared semantic spaces to get coarse matching flows.(ii) Second, matching flows in low resolution should be refined to generate accurate point-to-point matching results. We propose asymmetric feature learning and matching flow super-resolution based on vision transformers to solve the above problems. The asymmetric feature learning module exploits a biased cross-attention mechanism to encode token features of source images with their target counterparts. Then matching flow in low resolutions is enhanced by a super-resolution network to get accurate correspondences. Our pipeline is built upon vision transformers and can be trained in an end-to-end manner. Extensive experimental results on several popular benchmarks, such as PF-PASCAL, PF-WILLOW, and SPair-71K, demonstrate that the proposed method can catch subtle semantic differences in pixels efficiently. Code is available on https://github. com/YXSUNMADMAX/ACTR."}}
{"id": "j3w0Pw-FQxv", "cdate": 1640995200000, "mdate": 1669121085908, "content": {"title": "FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos", "abstract": "Current benchmarks for facial expression recognition (FER) mainly focus on static images, while there are limited datasets for FER in videos. It is still ambiguous to evaluate whether performances of existing methods remain satisfactory in real-world application-oriented scenes. For example, the \u201cHappy\u201d expression with high intensity in Talk-Show is more discriminating than the same expression with low intensity in Official-Event. To fill this gap, we build a large-scale multi-scene dataset, coined as FERV39k. We analyze the important ingredients of constructing such a novel dataset in three aspects: (1) multi-scene hierarchy and expression class, (2) generation of candidate video clips, (3) trusted manual labelling process. Based on these guidelines, we select 4 scenarios subdivided into 22 scenes, annotate 86k samples automatically obtained from 4k videos based on the well-designed workflow, and finally build 38,935 video clips labeled with 7 classic expressions. Experiment benchmarks on four kinds of baseline frame-works were also provided and further analysis on their performance across different scenes and some challenges for future research were given. Besides, we systematically investigate key components of DFER by ablation studies. The baseline framework and our project are available on https://github.com/wangyanckxx/FERV39k."}}
{"id": "Z6tRnf2l5g", "cdate": 1640995200000, "mdate": 1669121085908, "content": {"title": "A survey of human-in-the-loop for machine learning", "abstract": ""}}
