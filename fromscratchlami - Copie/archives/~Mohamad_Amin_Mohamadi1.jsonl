{"id": "HN0ehX-ov5Q", "cdate": 1663850483627, "mdate": null, "content": {"title": "A Fast, Well-Founded Approximation to the Empirical Neural Tangent Kernel", "abstract": "Empirical neural tangent kernels (eNTKs) can provide a good understanding of a given network's representation: they are often far less expensive to compute and applicable more broadly than infinite-width NTKs. For networks with $O$ output units (e.g. an $O$-class classifier), however, the eNTK on $N$ inputs is of size $NO \\times NO$, taking $\\mathcal{O}\\big( (N O)^2\\big)$ memory and up to $\\mathcal{O}\\big( (N O)^3 \\big)$ computation. Most existing applications have therefore used one of a handful of approximations yielding $N \\times N$ kernel matrices, saving orders of magnitude of computation, but with limited to no justification. We prove that one such approximation, which we call ``sum of logits,'' converges to the true eNTK at initialization. Our experiments demonstrate the quality of this approximation for various uses across a range of settings."}}
{"id": "AKp6ZKrs_1", "cdate": 1652737668143, "mdate": null, "content": {"title": "Making Look-Ahead Active Learning Strategies Feasible with Neural Tangent Kernels", "abstract": "We propose a new method for approximating active learning acquisition strategies that are based on retraining with hypothetically-labeled candidate data points. Although this is usually infeasible with deep networks, we use the neural tangent kernel to approximate the result of retraining, and prove that this approximation works asymptotically even in an active learning setup -- approximating ``look-ahead'' selection criteria with far less computation required. This also enables us to conduct sequential active learning, i.e.\\ updating the model in a streaming regime, without needing to retrain the model with SGD after adding each new data point. Moreover, our querying strategy, which better understands how the model's predictions will change by adding new data points in comparison to the standard (``myopic'') criteria, \nbeats other look-ahead strategies by large margins, and achieves equal or better performance compared to state-of-the-art methods on several benchmark datasets in pool-based active learning."}}
{"id": "azftNJUACV2", "cdate": 1640995200000, "mdate": 1683616021166, "content": {"title": "A Fast, Well-Founded Approximation to the Empirical Neural Tangent Kernel", "abstract": "Empirical neural tangent kernels (eNTKs) can provide a good understanding of a given network's representation: they are often far less expensive to compute and applicable more broadly than infinite width NTKs. For networks with O output units (e.g. an O-class classifier), however, the eNTK on N inputs is of size $NO \\times NO$, taking $O((NO)^2)$ memory and up to $O((NO)^3)$ computation. Most existing applications have therefore used one of a handful of approximations yielding $N \\times N$ kernel matrices, saving orders of magnitude of computation, but with limited to no justification. We prove that one such approximation, which we call \"sum of logits\", converges to the true eNTK at initialization for any network with a wide final \"readout\" layer. Our experiments demonstrate the quality of this approximation for various uses across a range of settings."}}
{"id": "N4SYUFqR59", "cdate": 1640995200000, "mdate": 1668539764743, "content": {"title": "Making Look-Ahead Active Learning Strategies Feasible with Neural Tangent Kernels", "abstract": "We propose a new method for approximating active learning acquisition strategies that are based on retraining with hypothetically-labeled candidate data points. Although this is usually infeasible with deep networks, we use the neural tangent kernel to approximate the result of retraining, and prove that this approximation works asymptotically even in an active learning setup -- approximating \"look-ahead\" selection criteria with far less computation required. This also enables us to conduct sequential active learning, i.e. updating the model in a streaming regime, without needing to retrain the model with SGD after adding each new data point. Moreover, our querying strategy, which better understands how the model's predictions will change by adding new data points in comparison to the standard (\"myopic\") criteria, beats other look-ahead strategies by large margins, and achieves equal or better performance compared to state-of-the-art methods on several benchmark datasets in pool-based active learning."}}
{"id": "FEMPN8eaZZJ", "cdate": 1640995200000, "mdate": 1683616021171, "content": {"title": "Making Look-Ahead Active Learning Strategies Feasible with Neural Tangent Kernels", "abstract": "We propose a new method for approximating active learning acquisition strategies that are based on retraining with hypothetically-labeled candidate data points. Although this is usually infeasible with deep networks, we use the neural tangent kernel to approximate the result of retraining, and prove that this approximation works asymptotically even in an active learning setup -- approximating <code>look-ahead'' selection criteria with far less computation required. This also enables us to conduct sequential active learning, i.e.\\ updating the model in a streaming regime, without needing to retrain the model with SGD after adding each new data point. Moreover, our querying strategy, which better understands how the model's predictions will change by adding new data points in comparison to the standard (</code>myopic'') criteria, beats other look-ahead strategies by large margins, and achieves equal or better performance compared to state-of-the-art methods on several benchmark datasets in pool-based active learning."}}
