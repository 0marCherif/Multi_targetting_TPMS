{"id": "aX7Sv75uUJY", "cdate": 1672531200000, "mdate": 1674926976037, "content": {"title": "DIRECT: Learning from Sparse and Shifting Rewards using Discriminative Reward Co-Training", "abstract": "We propose discriminative reward co-training (DIRECT) as an extension to deep reinforcement learning algorithms. Building upon the concept of self-imitation learning (SIL), we introduce an imitation buffer to store beneficial trajectories generated by the policy determined by their return. A discriminator network is trained concurrently to the policy to distinguish between trajectories generated by the current policy and beneficial trajectories generated by previous policies. The discriminator's verdict is used to construct a reward signal for optimizing the policy. By interpolating prior experience, DIRECT is able to act as a surrogate, steering policy optimization towards more valuable regions of the reward landscape thus learning an optimal policy. Our results show that DIRECT outperforms state-of-the-art algorithms in sparse- and shifting-reward environments being able to provide a surrogate reward to the policy and direct the optimization towards valuable areas."}}
{"id": "TbBcD32k9i", "cdate": 1672531200000, "mdate": 1674926976010, "content": {"title": "Attention-Based Recurrency for Multi-Agent Reinforcement Learning under State Uncertainty", "abstract": "Stochastic partial observability poses a major challenge for decentralized coordination in multi-agent reinforcement learning but is largely neglected in state-of-the-art research due to a strong focus on state-based centralized training for decentralized execution (CTDE) and benchmarks that lack sufficient stochasticity like StarCraft Multi-Agent Challenge (SMAC). In this paper, we propose Attention-based Embeddings of Recurrence In multi-Agent Learning (AERIAL) to approximate value functions under stochastic partial observability. AERIAL replaces the true state with a learned representation of multi-agent recurrence, considering more accurate information about decentralized agent decisions than state-based CTDE. We then introduce MessySMAC, a modified version of SMAC with stochastic observations and higher variance in initial states, to provide a more general and configurable benchmark regarding stochastic partial observability. We evaluate AERIAL in Dec-Tiger as well as in a variety of SMAC and MessySMAC maps, and compare the results with state-based CTDE. Furthermore, we evaluate the robustness of AERIAL and state-based CTDE against various stochasticity configurations in MessySMAC."}}
{"id": "H_hpOcP_Qv", "cdate": 1640995200000, "mdate": 1674926976011, "content": {"title": "Towards Anomaly Detection in Reinforcement Learning", "abstract": ""}}
{"id": "AkhuAcKg8Xj", "cdate": 1640995200000, "mdate": 1674926976100, "content": {"title": "Emergent Cooperation from Mutual Acknowledgment Exchange", "abstract": ""}}
{"id": "hyJKKIhfxxT", "cdate": 1621630190846, "mdate": null, "content": {"title": "VAST: Value Function Factorization with Variable Agent Sub-Teams", "abstract": "Value function factorization (VFF) is a popular approach to cooperative multi-agent reinforcement learning in order to learn local value functions from global rewards. However, state-of-the-art VFF is limited to a handful of agents in most domains. We hypothesize that this is due to the flat factorization scheme, where the VFF operator becomes a performance bottleneck with an increasing number of agents. Therefore, we propose VFF with variable agent sub-teams (VAST). VAST approximates a factorization for sub-teams which can be defined in an arbitrary way and vary over time, e.g., to adapt to different situations. The sub-team values are then linearly decomposed for all sub-team members. Thus, VAST can learn on a more focused and compact input representation of the original VFF operator. We evaluate VAST in three multi-agent domains and show that VAST can significantly outperform state-of-the-art VFF, when the number of agents is sufficiently large."}}
{"id": "orkC1cA4ia", "cdate": 1609459200000, "mdate": 1674927764556, "content": {"title": "SAT-MARL: Specification Aware Training in Multi-Agent Reinforcement Learning", "abstract": ""}}
{"id": "gc_E1YQH1", "cdate": 1609459200000, "mdate": 1674926976043, "content": {"title": "Resilient Multi-Agent Reinforcement Learning with Adversarial Value Decomposition", "abstract": "We focus on resilience in cooperative multi-agent systems, where agents can change their behavior due to udpates or failures of hardware and software components. Current state-of-the-art approaches to cooperative multi-agent reinforcement learning (MARL) have either focused on idealized settings without any changes or on very specialized scenarios, where the number of changing agents is fixed, e.g., in extreme cases with only one productive agent. Therefore, we propose Resilient Adversarial value Decomposition with Antagonist-Ratios (RADAR). RADAR offers a value decomposition scheme to train competing teams of varying size for improved resilience against arbitrary agent changes. We evaluate RADAR in two cooperative multi-agent domains and show that RADAR achieves better worst case performance w.r.t. arbitrary agent changes than state-of-the-art MARL."}}
{"id": "Xa7UZR13zf", "cdate": 1609459200000, "mdate": 1674926976134, "content": {"title": "VAST: Value Function Factorization with Variable Agent Sub-Teams", "abstract": "Value function factorization (VFF) is a popular approach to cooperative multi-agent reinforcement learning in order to learn local value functions from global rewards. However, state-of-the-art VFF is limited to a handful of agents in most domains. We hypothesize that this is due to the flat factorization scheme, where the VFF operator becomes a performance bottleneck with an increasing number of agents. Therefore, we propose VFF with variable agent sub-teams (VAST). VAST approximates a factorization for sub-teams which can be defined in an arbitrary way and vary over time, e.g., to adapt to different situations. The sub-team values are then linearly decomposed for all sub-team members. Thus, VAST can learn on a more focused and compact input representation of the original VFF operator. We evaluate VAST in three multi-agent domains and show that VAST can significantly outperform state-of-the-art VFF, when the number of agents is sufficiently large."}}
{"id": "8cCCm7SF_6", "cdate": 1609459200000, "mdate": 1674927764527, "content": {"title": "Specification Aware Multi-Agent Reinforcement Learning", "abstract": "Engineering intelligent industrial systems is challenging due to high complexity and uncertainty with respect to domain dynamics and multiple agents. If industrial systems act autonomously, their choices and results must be within specified bounds to satisfy these requirements. Reinforcement learning (RL) is promising to find solutions that outperform known or handcrafted heuristics. However in industrial scenarios, it also is crucial to prevent RL from inducing potentially undesired or even dangerous behavior. This paper considers specification alignment in industrial scenarios with multi-agent reinforcement learning (MARL). We propose to embed functional and non-functional requirements into the reward function, enabling the agents to learn to align with the specification. We evaluate our approach in a smart factory simulation representing an industrial lot-size-one production facility, where we train up to eight agents using DQN, VDN, and QMIX. Our results show that the proposed approach enables agents to satisfy a given set of requirements."}}
{"id": "35Ob5qeEKZ", "cdate": 1609459200000, "mdate": 1674927764570, "content": {"title": "A Sustainable Ecosystem through Emergent Cooperation in Multi-Agent Reinforcement Learning", "abstract": ""}}
