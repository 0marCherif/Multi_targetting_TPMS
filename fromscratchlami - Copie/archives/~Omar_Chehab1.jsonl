{"id": "QZ1Miy1bH-D", "cdate": 1672531200000, "mdate": 1681652937171, "content": {"title": "Optimizing the Noise in Self-Supervised Learning: from Importance Sampling to Noise-Contrastive Estimation", "abstract": ""}}
{"id": "SEef8wIj5lc", "cdate": 1646077533692, "mdate": null, "content": {"title": "The Optimal Noise in Noise-Contrastive Learning Is Not What You Think", "abstract": "Learning a parametric model of a data distribution is a well-known statistical problem that has seen renewed interest as it is brought to scale in deep learning. Framing the problem as a self-supervised task, where data samples are discriminated from noise  samples,  is at  the  core  of  state-of-the-art methods, beginning with Noise-Contrastive Estimation (NCE). Yet, such contrastive learning requires a good noise distribution, which is hard to specify; domain-specific heuristics are therefore widely used. While a comprehensive theory is missing, it is widely assumed that the optimal noise should in practice be made equal to the data, both in distribution and proportion. This setting underlies Generative Adversarial Networks (GANs) in particular. Here, we empirically and theoretically challenge this assumption on the optimal noise. We show that deviating from this assumption can actually lead to better statistical estimators, in terms of asymptotic variance. In particular, the optimal noise distribution is different from the data\u2019s and even from a different family."}}
{"id": "o6mM8iZFxb_", "cdate": 1640995200000, "mdate": 1681652937173, "content": {"title": "The optimal noise in noise-contrastive learning is not what you think", "abstract": ""}}
{"id": "HZWh-i2qx9", "cdate": 1609459200000, "mdate": 1646082820180, "content": {"title": "Deep Recurrent Encoder: A scalable end-to-end network to model brain signals", "abstract": "Understanding how the brain responds to sensory inputs is challenging: brain recordings are partial, noisy, and high dimensional; they vary across sessions and subjects and they capture highly nonlinear dynamics. These challenges have led the community to develop a variety of preprocessing and analytical (almost exclusively linear) methods, each designed to tackle one of these issues. Instead, we propose to address these challenges through a specific end-to-end deep learning architecture, trained to predict the brain responses of multiple subjects at once. We successfully test this approach on a large cohort of magnetoencephalography (MEG) recordings acquired during a one-hour reading task. Our Deep Recurrent Encoding (DRE) architecture reliably predicts MEG responses to words with a three-fold improvement over classic linear methods. To overcome the notorious issue of interpretability of deep learning, we describe a simple variable importance analysis. When applied to DRE, this method recovers the expected evoked responses to word length and word frequency. The quantitative improvement of the present deep learning approach paves the way to better understand the nonlinear dynamics of brain activity from large datasets."}}
{"id": "8JqDuZ5-2xc", "cdate": 1609459200000, "mdate": 1681652937251, "content": {"title": "Learning with self-supervision on EEG data", "abstract": ""}}
{"id": "H_W4nbsh9g9", "cdate": 1577836800000, "mdate": 1646082820193, "content": {"title": "Uncovering the structure of clinical EEG signals with self-supervised learning", "abstract": "Objective. Supervised learning paradigms are often limited by the amount of labeled data that is available. This phenomenon is particularly problematic in clinically-relevant data, such as electroencephalography (EEG), where labeling can be costly in terms of specialized expertise and human processing time. Consequently, deep learning architectures designed to learn on EEG data have yielded relatively shallow models and performances at best similar to those of traditional feature-based approaches. However, in most situations, unlabeled data is available in abundance. By extracting information from this unlabeled data, it might be possible to reach competitive performance with deep neural networks despite limited access to labels. Approach. We investigated self-supervised learning (SSL), a promising technique for discovering structure in unlabeled data, to learn representations of EEG signals. Specifically, we explored two tasks based on temporal context prediction as well as contrastive predictive coding on two clinically-relevant problems: EEG-based sleep staging and pathology detection. We conducted experiments on two large public datasets with thousands of recordings and performed baseline comparisons with purely supervised and hand-engineered approaches. Main results. Linear classifiers trained on SSL-learned features consistently outperformed purely supervised deep neural networks in low-labeled data regimes while reaching competitive performance when all labels were available. Additionally, the embeddings learned with each method revealed clear latent structures related to physiological and clinical phenomena, such as age effects. Significance. We demonstrate the benefit of self-supervised learning approaches on EEG data. Our results suggest that SSL may pave the way to a wider use of deep learning models on EEG data."}}
