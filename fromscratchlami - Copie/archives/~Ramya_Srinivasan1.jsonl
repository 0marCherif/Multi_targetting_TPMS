{"id": "0oubWlDUIa", "cdate": 1680732799716, "mdate": null, "content": {"title": "Examining LLM's Awareness of the United Nations Sustainable Development Goals (SDGs)", "abstract": "Utilization of Large Language Models (LLMs) is rapidly growing in diverse domains and each LLM may show different performance across different topics. \nAmidst this progress, biases and other ethical concerns surrounding LLMs have raised questions regarding trust and reliability, thereby necessitating human verification and audit.\nIn this study, we empirically investigate six important topics of the United Nations Sustainable Development Goals (UN SDG) by utilizing ChatGPT LLM as a facilitator for generating statements needed for evaluation of eight different LLMs.  We also compare the performance of these LLMs on human-written statements and questions.  In addition, we study the tendency of producing of true and false statement for the eight LLMs considered. Although LLMs show comparative performance on ChatGPT and human input for relatively common issues, they are not sophisticated enough in understanding nuanced and advanced issues that demand critical and wholistic introspection."}}
{"id": "vKrGCS5uUhb", "cdate": 1632499903911, "mdate": null, "content": {"title": "Apertures in Agriculture Seeking Attention", "abstract": "Agriculture is arguably one economic activity that serves as the backbone of human civilization---It is the provider of the most essential lifeline for human survival, namely, food. Today, we stand at a juncture where world over, all key stakeholders involved in this activity, i.e., producers (farmers), consumers, and the planet are facing grave concerns. Formulating these concerns concretely and leveraging AI methods in developing solution strategies can potentially help alleviate major risks to food systems. However, local contexts such as cultural practices, physical terrains, and socio-economic status pose unique challenges in being able to directly employ existing AI techniques across geographies. In this position paper, we highlight some such key challenges, specifically focusing on India and the developing world. We outline the problems of key stakeholders, and identify some gaps that need to be filled in addressing these problems."}}
{"id": "K7ke_GZ_6N", "cdate": 1629413116972, "mdate": null, "content": {"title": "Artsheets for Art Datasets", "abstract": "Machine learning (ML) techniques are increasingly being employed within a variety of creative domains. For example, ML tools are being used to analyze the authenticity of artworks, to simulate artistic styles, and to augment human creative processes. While this progress has opened up new creative avenues, it has also paved the way for adverse downstream effects such as cultural appropriation (e.g., cultural misrepresentation, offense, and undervaluing) and representational harm. Many such concerning issues stem from the training data in ways that diligent evaluation can uncover, prevent, and mitigate. We posit that, when developing an arts-based dataset, it is essential to consider the social factors that influenced the process of conception and design, and the resulting gaps must be examined in order to maximize understanding of the dataset's meaning and future impact. Each dataset creator's decision produces opportunities, but also omissions. Each choice, moreover, builds on preexisting histories of the data's formation and handling across time by prior actors including, but not limited to, art collectors, galleries, libraries, archives, museums, and digital repositories. To illuminate the aforementioned aspects, we provide a checklist of questions customized for use with art datasets in order to help guide assessment of the ways that dataset design may either perpetuate or shift exclusions found in repositories of art data. The checklist is organized to address the dataset creator's motivation together with dataset provenance, composition, collection, pre-processing, cleaning, labeling, use (including data generation), distribution, and maintenance. Two case studies exemplify the value and application of our questionnaire."}}
{"id": "AgobJz_pcyd", "cdate": 1609459200000, "mdate": null, "content": {"title": "Biases in Generative Art: A Causal Look from the Lens of Art History", "abstract": "With rapid progress in artificial intelligence (AI), popularity of generative art has grown substantially. From creating paintings to generating novel art styles, AI based generative art has showcased a variety of applications. However, there has been little focus concerning the ethical impacts of AI based generative art. In this work, we investigate biases in the generative art AI pipeline right from those that can originate due to improper problem formulation to those related to algorithm design. Viewing from the lens of art history, we discuss the socio-cultural impacts of these biases. Leveraging causal models, we highlight how current methods fall short in modeling the process of art creation and thus contribute to various types of biases. We illustrate the same through case studies, in particular those related to style transfer. To the best of our knowledge, this is the first extensive analysis that investigates biases in the generative art AI pipeline from the perspective of art history. We hope our work sparks interdisciplinary discussions related to accountability of generative art."}}
{"id": "5gdkvwKcQPl", "cdate": 1609459200000, "mdate": null, "content": {"title": "Quantifying Confounding Bias in Generative Art: A Case Study", "abstract": "In recent years, AI generated art has become very popular. From generating art works in the style of famous artists like Paul Cezanne and Claude Monet to simulating styles of art movements like Ukiyo-e, a variety of creative applications have been explored using AI. Looking from an art historical perspective, these applications raise some ethical questions. Can AI model artists' styles without stereotyping them? Does AI do justice to the socio-cultural nuances of art movements? In this work, we take a first step towards analyzing these issues. Leveraging directed acyclic graphs to represent potential process of art creation, we propose a simple metric to quantify confounding bias due to the lack of modeling the influence of art movements in learning artists' styles. As a case study, we consider the popular cycleGAN model and analyze confounding bias across various genres. The proposed metric is more effective than state-of-the-art outlier detection method in understanding the influence of art movements in artworks. We hope our work will elucidate important shortcomings of computationally modeling artists' styles and trigger discussions related to accountability of AI generated art."}}
{"id": "69eJcSG2R7w", "cdate": 1600115812533, "mdate": null, "content": {"title": "Working with Beliefs: AI Transparency in the Enterprise", "abstract": "Enterprises are increasingly recognizing that they must integrate AI into all of their operational workflows to remain competitive. As enterprises consider competing AIs to support a particular business function, explainability is an advantage which gets a candidate AI a foot in the door. Our experience working with enterprise decision makers considering AI in a decision augmentation role reveals an additional and possibly more crucial aspect of choosing an AI: the ability of decision makers to interact fluidly with an AI. Fluid interactions are necessary when an AI\u2019s recommendation does not match a human decision maker\u2019s existing beliefs. Interactions that allow the (typically nontechnical) human to edit the AI, as well as allow the AI to guide the human, enable a collaborative exploration of the data that leads to common ground where both the AI and the human beliefs have been updated. We outline an illustrative example from our experience that models this dance. Based on our experiences, we suggest requirements for AI systems that would greatly facilitate their adoption in the enterprise."}}
{"id": "6GJm-exbGQg", "cdate": 1598647691422, "mdate": null, "content": {"title": "Explanation Perspectives from the Cognitive Sciences---A Survey", "abstract": "With growing adoption of AI across fields such as healthcare, finance, and the justice system, ex- plaining an AI decision has become more important than ever before. Development of human-centric explainable AI (XAI) systems necessitates an un- derstanding of the requirements of the human-in- the-loop seeking the explanation. This includes the cognitive behavioral purpose that the explanation serves for its recipients, and the structure that the explanation uses to reach those ends. An under- standing of the psychological foundations of expla- nations is thus vital for the development of effec- tive human-centric XAI systems. Towards this end, we survey papers from the cognitive science litera- ture that address the following broad questions: (1) what is an explanation, (2) what are explanations for, and 3) what are the characteristics of good and bad explanations. We organize the insights gained therein by means of highlighting the advantages and shortcomings of various explanation structures and theories, discuss their applicability across dif- ferent domains, and analyze their utility to various types of humans-in-the-loop. We summarize the key takeaways for human-centric design of XAI systems, and recommend strategies to bridge the existing gap between XAI research and practical needs. We hope this work will spark the develop- ment of novel human-centric XAI systems."}}
{"id": "ZVYk-Lc8M7r", "cdate": 1546300800000, "mdate": null, "content": {"title": "Understanding Bias in Datasets using Topological Data Analysis", "abstract": ""}}
{"id": "Vxikf1OQAWD", "cdate": 1546300800000, "mdate": null, "content": {"title": "Generating User-friendly Explanations for Loan Denials using GANs", "abstract": "Financial decisions impact our lives, and thus everyone from the regulator to the consumer is interested in fair, sound, and explainable decisions. There is increasing competitive desire and regulatory incentive to deploy AI mindfully within financial services. An important mechanism towards that end is to explain AI decisions to various stakeholders. State-of-the-art explainable AI systems mostly serve AI engineers and offer little to no value to business decision makers, customers, and other stakeholders. Towards addressing this gap, in this work we consider the scenario of explaining loan denials. We build the first-of-its-kind dataset that is representative of loan-applicant friendly explanations. We design a novel Generative Adversarial Network (GAN) that can accommodate smaller datasets, to generate user-friendly textual explanations. We demonstrate how our system can also generate explanations serving different purposes: those that help educate the loan applicants, or help them take appropriate action towards a future approval."}}
{"id": "Bk1fbrw54B", "cdate": 1546300800000, "mdate": null, "content": {"title": "Creation of User Friendly Datasets: Insights from a Case Study concerning Explanations of Loan Denials", "abstract": "Most explainable AI (XAI) techniques are concerned with the design of algorithms to explain the AI's decision. However, the data that is used to train these algorithms may contain features that are often incomprehensible to an end-user even with the best XAI algorithms. Thus, the problem of explainability has to be addressed starting right from the data creation step. In this paper, we studied this problem considering the use-case of explaining loan denials to end-users as opposed to AI engineers or domain experts. Motivated by the lack of datasets that are representative of user-friendly explanations, we build the first-of-its-kind dataset that is representative of user-friendly explanations for loan denials. The paper shares some of the insights gained in curating the dataset. First, existing datasets seldom contain features that end users consider as acceptable in understanding a model's decision. Second, understanding of the explanation's context such as the human-in-the-loop seeking the explanation, and the purpose for which an explanation is sought, aids in the creation of user-friendly datasets. Thus, our dataset, which we call Xnet, also contains explanations that serve different purposes: those that educate the loan applicants, and help them take appropriate action towards a future approval. We hope this work will trigger the creation of new user friendly datasets, and serve as a guide for the curation of such datasets."}}
