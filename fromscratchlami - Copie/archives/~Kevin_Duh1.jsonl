{"id": "LYaoccx6G6D", "cdate": 1687834389990, "mdate": 1687834389990, "content": {"title": "Attention-Focused Adversarial Training for Robust Temporal Reasoning", "abstract": "We propose an enhanced adversarial training algorithm for fine-tuning transformer-based language models (i.e., RoBERTa) and apply it to the temporal reasoning task. Current adversarial training approaches for NLP add the adversarial perturbation only to the embedding layer, ignoring the other layers of the model, which might limit the generalization power of adversarial training. Instead, our algorithm searches for the best combination of layers to add the adversarial perturbation. We add the adversarial perturbation to multiple hidden states or attention representations of the model layers. Adding the perturbation to the attention representations performed best in our experiments. Our model can improve performance on several temporal reasoning benchmarks, and establishes new state-of-the-art results."}}
{"id": "TYEHy7_-jlg", "cdate": 1665520926258, "mdate": 1665520926258, "content": {"title": "Transfer learning approaches for building cross-language dense retrieval models", "abstract": "The advent of transformer-based models such as BERT has led to the rise of neural ranking models.\nThese models have improved the effectiveness of retrieval systems well beyond that of lexical term matching models such as BM25.\nWhile monolingual retrieval tasks have benefited from large-scale training collections such as MS MARCO \nand advances in neural architectures,\ncross-language retrieval tasks have fallen behind these advancements. \nThis paper introduces ColBERT-X,\na generalization of the ColBERT multi-representation dense retrieval model\nthat uses the XLM-RoBERTa (XLM-R) encoder to support cross-language information retrieval (CLIR).\nColBERT-X can be trained in two ways.\nIn zero-shot training, the system is trained on the English MS MARCO collection,\nrelying on the XLM-R encoder for cross-language mappings.\nIn translate-train, the system is trained on the MS MARCO English queries\ncoupled with machine translations of the associated MS MARCO passages.\nResults on ad hoc document ranking tasks in several languages\ndemonstrate substantial and statistically significant improvements of these trained dense retrieval models\nover traditional lexical CLIR baselines."}}
{"id": "b72I38niRk", "cdate": 1636952688094, "mdate": 1636952688094, "content": {"title": "Synthetic Word Parsing Improves Chinese Word Segmentation", "abstract": "We present a novel solution to improve the performance of Chinese word segmentation (CWS) using a synthetic word parser. The parser analyses the internal structure of words, and attempts to convert out-of-vocabulary words (OOVs) into in-vocabulary fine-grained sub-words. We propose a pipeline CWS system that first predicts this fine-grained segmentation, then chunks the output to reconstruct the original word segmentation standard. We achieve competitive results on the PKU and MSR datasets, with substantial improvements in OOV recall."}}
{"id": "V5iDeHXvGlZ", "cdate": 1635476474548, "mdate": 1635476474548, "content": {"title": "Identifying collocations using cross-lingual association measures", "abstract": "We introduce a simple and effective cross-lingual approach to identifying collocations. This approach is based on the observation that true collocations, which cannot be translated word for word, will exhibit very different association scores before and after literal translation. Our experiments in Japanese demonstrate that our cross-lingual association measure can successfully exploit the combination of bilingual dictionary and large monolingual corpora, outperforming monolingual association measures."}}
{"id": "IKA7MLxsLSu", "cdate": 1621171410293, "mdate": null, "content": {"title": "Data and Parameter Scaling Laws for Neural Machine Translation", "abstract": "We observe that the development cross-entropy loss of supervised neural machine translation models scales like a power law with the amount of training data and the number of non-embedding parameters in the model. We discuss some practical implications of these results, such as predicting BLEU achieved by large scale models and predicting the ROI of labeling data in low-resource language pairs."}}
{"id": "T0wGvS6jIoH", "cdate": 1609459200000, "mdate": null, "content": {"title": "Leveraging End-to-End ASR for Endangered Language Documentation: An Empirical Study on Yolox\u00f3chitl Mixtec", "abstract": "Transcription bottlenecks\", created by a shortage of effective human transcribers are one of the main challenges to endangered language (EL) documentation. Automatic speech recognition (ASR) has been suggested as a tool to overcome such bottlenecks. Following this suggestion, we investigated the effectiveness for EL documentation of end-to-end ASR, which unlike Hidden Markov Model ASR systems, eschews linguistic resources but is instead more dependent on large-data settings. We open source a Yolox\\'ochitl Mixtec EL corpus. First, we review our method in building an end-to-end ASR system in a way that would be reproducible by the ASR community. We then propose a novice transcription correction task and demonstrate how ASR systems and novice transcribers can work together to improve EL documentation. We believe this combinatory methodology would mitigate the transcription bottleneck and transcriber shortage that hinders EL documentation."}}
{"id": "C82Dkx_KeB1", "cdate": 1609459200000, "mdate": null, "content": {"title": "Adaptive Mixed Component LDA for Low Resource Topic Modeling", "abstract": "Probabilistic topic models in low data resource scenarios are faced with less reliable estimates due to sparsity of discrete word co-occurrence counts, and do not have the luxury of retraining word or topic embeddings using neural methods. In this challenging resource constrained setting, we explore mixture models which interpolate between the discrete and continuous topic-word distributions that utilise pre-trained embeddings to improve topic coherence. We introduce an automatic trade-off between the discrete and continuous representations via an adaptive mixture coefficient, which places greater weight on the discrete representation when the corpus statistics are more reliable. The adaptive mixture coefficient takes into account global corpus statistics, and the uncertainty in each topic\u2019s continuous distributions. Our approach outperforms the fully discrete, fully continuous, and static mixture model on topic coherence in low resource settings. We additionally demonstrate the generalisability of our method by extending it to handle multilingual document collections."}}
{"id": "6ZiPt-9fMT", "cdate": 1609459200000, "mdate": null, "content": {"title": "Leveraging End-to-End ASR for Endangered Language Documentation: An Empirical Study on Yol\u00f3xochitl Mixtec", "abstract": "Jiatong Shi, Jonathan D. Amith, Rey Castillo Garc\u00eda, Esteban Guadalupe Sierra, Kevin Duh, Shinji Watanabe. Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 2021."}}
{"id": "weeuCzTBfxl", "cdate": 1577836800000, "mdate": null, "content": {"title": "Benchmarking Neural and Statistical Machine Translation on Low-Resource African Languages", "abstract": "Research in machine translation (MT) is developing at a rapid pace. However, most work in the community has focused on languages where large amounts of digital resources are available. In this study, we benchmark state of the art statistical and neural machine translation systems on two African languages which do not have large amounts of resources: Somali and Swahili. These languages are of social importance and serve as test-beds for developing technologies that perform reasonably well despite the low-resource constraint. Our findings suggest that statistical machine translation (SMT) and neural machine translation (NMT) can perform similarly in low-resource scenarios, but neural systems require more careful tuning to match performance. We also investigate how to exploit additional data, such as bilingual text harvested from the web, or user dictionaries; we find that NMT can significantly improve in performance with the use of these additional data. Finally, we survey the landscape of machine translation resources for the languages of Africa and provide some suggestions for promising future research directions."}}
{"id": "uYkTdXu54tR", "cdate": 1577836800000, "mdate": null, "content": {"title": "Distill, Adapt, Distill: Training Small, In-Domain Models for Neural Machine Translation", "abstract": "We explore best practices for training small, memory efficient machine translation models with sequence-level knowledge distillation in the domain adaptation setting. While both domain adaptation and knowledge distillation are widely-used, their interaction remains little understood. Our large-scale empirical results in machine translation (on three language pairs with three domains each) suggest distilling twice for best performance: once using general-domain data and again using in-domain data with an adapted teacher."}}
