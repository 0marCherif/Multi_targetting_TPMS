{"id": "vaJCmIf54d", "cdate": 1686042952203, "mdate": null, "content": {"title": "On the importance of data collection for training general goal-reaching policies", "abstract": "Recent advances in ML suggest that the quantity of data available to a model is one of the primary bottlenecks to high performance.  Although for language-based tasks there exist almost unlimited amounts of reasonably coherent data to train from, this is generally not the case for Reinforcement Learning, especially when dealing with a novel environment.  In effect, even a relatively trivial continuous environment has an almost limitless number of states, but simply sampling random states and actions will likely not provide transitions that are interesting or useful for any potential downstream task.  How should one generate massive amounts of useful data given only an MDP with no indication of downstream tasks? Are the quantity and quality of data truly transformative to the performance of a general controller?  We propose to answer both of these questions.  First, we introduce a principled unsupervised exploration method, ChronoGEM, which aims to achieve uniform coverage over the manifold of achievable states, which we believe is the most reasonable goal given no prior task information.  Secondly, we investigate the effects of both data quantity and data quality on the training of a downstream goal-achievement policy, and show that both large quantities and high-quality of data are essential to train a general controller: a high-precision pose-achievement policy capable of attaining a large number of poses over numerous continuous control embodiments including humanoid."}}
{"id": "kx8x43_1ftI", "cdate": 1663849938170, "mdate": null, "content": {"title": "C3PO: Learning to Achieve Arbitrary Goals via Massively Entropic Pretraining", "abstract": "Given a particular embodiment, we propose a novel method (C3PO) that learns policies able to achieve any arbitrary position and pose.  Such a policy would allow for easier control, and would be re-useable as a key building block for downstream tasks.  The method is two-fold: First, we introduce a novel exploration algorithm that optimizes for uniform coverage, is able to discover a set of achievable states, and investigates its abilities in attaining both high coverage, and hard-to-discover states;  Second,  we leverage this set of achievable states as training data for a universal goal-achievement policy, a goal-based SAC variant. We demonstrate the trained policy's performance in achieving a large number of novel states. Finally, we showcase the influence of massive unsupervised training of a goal-achievement policy with state-of-the-art pose-based control of the Hopper, Walker, Halfcheetah, Humanoid and Ant embodiments."}}
{"id": "SkW23ob_bS", "cdate": 1546300800000, "mdate": null, "content": {"title": "Learning from a Learner", "abstract": "In this paper, we propose a novel setting for Inverse Reinforcement Learning (IRL), namely \"Learning from a Learner\" (LfL). As opposed to standard IRL, it does not consist in learning a reward by observing an optimal agent but from observations of another learning (and thus sub-optimal) agent. To do so, we leverage the fact that the observed agent\u2019s policy is assumed to improve over time. The ultimate goal of this approach is to recover the actual environment\u2019s reward and to allow the observer to outperform the learner. To recover that reward in practice, we propose methods based on the entropy-regularized policy iteration framework. We discuss different approaches to learn solely from trajectories in the state-action space. We demonstrate the genericity of our method by observing agents implementing various reinforcement learning algorithms. Finally, we show that, on both discrete and continuous state/action tasks, the observer\u2019s performance (that optimizes the recovered reward) can surpass those of the observed agent."}}
