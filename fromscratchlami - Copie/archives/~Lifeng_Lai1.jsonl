{"id": "853DUBT9Ht", "cdate": 1675209600000, "mdate": 1684166152914, "content": {"title": "Guest Editorial: AI-Powered Telco Network Automation: 5G Evolution and 6G", "abstract": "The fifth generation (5G) of cellular networks is significantly more complex than its predecessors due to several factors, such as increased cell density, differentiated service requirements, and coexistence with legacy networks. As a result, traditional operation and management (O&M) solutions, which heavily rely on human intervention, are no longer feasible to support such complex networks at reasonable operating expense (OPEX). Over the past few years, the telecommunication industry has come to the realization that leveraging artificial intelligence (AI) technology to enable a fully automated network O&M is a must to lowering OPEX and enhancing network key performance indicators (KPIs) for 5G, Beyond 5G (B5G), and the sixth generation (6G) of cellular networks. There have been numerous research efforts from both industry and academia to develop AI-powered network automation solutions. Many telecommunication operators and vendors have already adopted AI technology to automate some repetitive operational tasks and reduce reliance on personnel experience, such as cell planning, network deployment simplification, fault detection, and KPI optimization. While there has been notable progress for certain network O&M applications, the development of network automation solutions still faces several unique technical challenges that arise from telecommunication fields, including overwhelming network complexity, massive and diverse proprietary data, lack of industry-wide standards for radio access network (RAN) interfaces, and scarcity of labeled datasets."}}
{"id": "ukPviwr7MW", "cdate": 1640995200000, "mdate": 1684166153150, "content": {"title": "Optimal Accuracy-Privacy Trade-Off of Inference as Service", "abstract": "In this paper, we propose a general framework to provide a desirable trade-off between inference accuracy and privacy protection in the inference as service scenario (IAS). Instead of sending data directly to the server, the user will preprocess the data through a privacy-preserving mapping, which will increase privacy protection but reduce inference accuracy. To properly address the trade-off between privacy protection and inference accuracy, we formulate an optimization problem to find the privacy-preserving mapping. Even though the problem is non-convex in general, we characterize nice structures of the problem and develop an iterative algorithm to find the desired privacy-preserving mapping, with convergence analysis provided under certain assumptions. From numerical examples, we observe that the proposed method has better performance than gradient ascent method in the convergence speed, solution quality and algorithm stability."}}
{"id": "raz-u1smfX9", "cdate": 1640995200000, "mdate": 1648667359624, "content": {"title": "Efficiently Escaping Saddle Points in Bilevel Optimization", "abstract": "Bilevel optimization is one of the fundamental problems in machine learning and optimization. Recent theoretical developments in bilevel optimization focus on finding the first-order stationary points for nonconvex-strongly-convex cases. In this paper, we analyze algorithms that can escape saddle points in nonconvex-strongly-convex bilevel optimization. Specifically, we show that the perturbed approximate implicit differentiation (AID) with a warm start strategy finds $\\epsilon$-approximate local minimum of bilevel optimization in $\\tilde{O}(\\epsilon^{-2})$ iterations with high probability. Moreover, we propose an inexact NEgative-curvature-Originated-from-Noise Algorithm (iNEON), a pure first-order algorithm that can escape saddle point and find local minimum of stochastic bilevel optimization. As a by-product, we provide the first nonasymptotic analysis of perturbed multi-step gradient descent ascent (GDmax) algorithm that converges to local minimax point for minimax problems."}}
{"id": "ZrsO3nVeXFp", "cdate": 1640995200000, "mdate": 1684166151995, "content": {"title": "Machine Learning Algorithms - Adversarial Robustness in Signal Processing", "abstract": ""}}
{"id": "Yntj6aC_UM", "cdate": 1640995200000, "mdate": 1684166152443, "content": {"title": "Fairness-aware Regression Robust to Adversarial Attacks", "abstract": "In this paper, we take a first step towards answering the question of how to design fair machine learning algorithms that are robust to adversarial attacks. Using a minimax framework, we aim to design an adversarially robust fair regression model that achieves optimal performance in the presence of an attacker who is able to add a carefully designed adversarial data point to the dataset or perform a rank-one attack on the dataset. By solving the proposed nonsmooth nonconvex-nonconcave minimax problem, the optimal adversary as well as the robust fairness-aware regression model are obtained. For both synthetic data and real-world datasets, numerical results illustrate that the proposed adversarially robust fair models have better performance on poisoned datasets than other fair machine learning models in both prediction accuracy and group-based fairness measure."}}
{"id": "Y93cIxeeU3D", "cdate": 1640995200000, "mdate": 1684166153391, "content": {"title": "Privacy Protection In Learning Fair Representations", "abstract": "In this paper, we develop a framework to achieve a desirable trade-off between fairness, inference accuracy and privacy protection in the inference as service scenario. Instead of sending raw data to the cloud, we conduct a random mapping of the data, which will increase privacy protection and mitigate bias but reduce inference accuracy. To properly address the trade-off, we formulate an optimization problem to find the optimal transformation map. As the problem is non-convex in general, we develop an iterative algorithm to find the desired map. Numerical examples show that the proposed method has better performance than gradient ascent in the convergence speed, solution quality and algorithm stability."}}
{"id": "VrXHzegk1Fw", "cdate": 1640995200000, "mdate": 1683879237195, "content": {"title": "Analysis of KNN Density Estimation", "abstract": "We analyze the convergence rates of <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$k$ </tex-math></inline-formula> nearest neighbor density estimation method, under <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\ell _{\\alpha} $ </tex-math></inline-formula> norm with <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\alpha \\in [1,\\infty]$ </tex-math></inline-formula> . Our analysis includes two different cases depending on whether the support set is bounded or not. In the first case, the probability density function has a bounded support. We show that if the support set is known, then the kNN density estimator is minimax optimal under <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\ell _{\\alpha} $ </tex-math></inline-formula> with both <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\alpha \\in \\big[1,\\infty\\big)$ </tex-math></inline-formula> and <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\alpha =\\infty $ </tex-math></inline-formula> . If the support is unknown, the kNN density estimator is still minimax optimal under <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\ell _{1}$ </tex-math></inline-formula> , but is suboptimal under <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\ell _{\\alpha} $ </tex-math></inline-formula> for <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\alpha &gt;1$ </tex-math></inline-formula> , and not consistent under <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\ell _\\infty $ </tex-math></inline-formula> . In the second case, the support is unbounded and the probability density function is smooth everywhere. Moreover, the Hessian is assumed to decay with the density values. For this case, our result shows that the <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\ell _\\infty $ </tex-math></inline-formula> error of kNN density estimation is nearly minimax optimal. The <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\ell _{\\alpha} $ </tex-math></inline-formula> error for the original kNN density estimator is not consistent. To address this issue, we design a new adaptive kNN estimator, which can select different <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$k$ </tex-math></inline-formula> for different samples. Using this adaptive estimator, the <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\ell _{\\alpha} $ </tex-math></inline-formula> bound is minimax optimal. For comparison, we show that the popular kernel density estimator is not minimax optimal for this case."}}
{"id": "BXsBB3CB7na", "cdate": 1640995200000, "mdate": 1684166152034, "content": {"title": "Risk-Sensitive Reinforcement Learning Via Entropic- VaR Optimization", "abstract": "In the existing work on risk-sensitive reinforcement learning (RL) problems, in order to take uncertainty into consideration, risk measure such as conditional value-at-risk (CVaR) has been widely used to design robust RL algorithms. However, the uncertainty set in the dual representation of CVaR is defined by distributions whose Radon-Nikodym derivative is constrained to a certain range. This is a less common way to de-fine distribution neighborhood in machine learning applications. This paper applies a recently developed risk measure named entropic value-at-risk (EVaR) to risk-sensitive RL problems. One appealing feature of EVaR is that the uncertainty set in its dual representation is defined by distributions whose Kullback-Leibler (KL) distance to the nominal distribution is less or equal to a certain level. In this paper, we address the EVaR optimization problem based on Markov decision process (MDP) by proposing a value iteration algorithm. Numerical examples are also provided to illustrate the practicality of our approach."}}
{"id": "7oIdQzXMyl", "cdate": 1640995200000, "mdate": 1684166152063, "content": {"title": "Policy Gradient Based Entropic-VaR Optimization in Risk-Sensitive Reinforcement Learning", "abstract": "In risk-sensitive reinforcement learning, it is important to develop algorithms based on risk measures that are conceptually meaningful and computationally tractable. In this paper, we apply a recently developed risk measure named entropic value-at-risk (EVaR) for the design of robust RL algorithms under the MDP framework. In particular, we develop policy gradient method to optimize the risk-sensitive criterion induced by EVaR. Towards this goal, we first derive the gradients of the EVaR involved objective function and then propose a trajectory-based policy gradient method to estimate gradients as well as update the policy until it converges to a local optimal policy. We prove the convergence of the policy by adopting the stochastic approximation approach. We also provide numerical results to illustrate the proposed algorithms."}}
{"id": "gciJWCp3z1s", "cdate": 1632875481042, "mdate": null, "content": {"title": "On the Convergence of Projected Alternating Maximization for Equitable and Optimal Transport", "abstract": "This paper studies the equitable and optimal transport (EOT) problem, which has many applications such as fair division problems and optimal transport with multiple agents etc. In the discrete distributions case, the EOT problem can be formulated as a linear program (LP). Since this LP is prohibitively large for general LP solvers, Scetbon \\etal suggests to perturb the problem by adding an entropy regularization. They proposed a projected alternating maximization algorithm (PAM) to solve the dual of the entropy regularized EOT. In this paper, we provide the first convergence analysis of PAM. A novel rounding procedure is proposed to help construct the primal solution for the original EOT problem. We also propose a variant of PAM by incorporating the extrapolation technique that can numerically improve the performance of PAM. Results in this paper may shed lights on block coordinate (gradient) descent methods for general optimization problems. "}}
