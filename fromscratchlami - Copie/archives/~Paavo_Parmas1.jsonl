{"id": "GKsNIC_mQRG", "cdate": 1663850532782, "mdate": null, "content": {"title": "Emergence of Exploration in Policy Gradient Reinforcement Learning via Resetting", "abstract": "In reinforcement learning (RL), many exploration methods explicitly promote stochastic policies, e.g., by adding an entropy bonus. We argue that exploration only matters in RL because the agent repeatedly encounters the same or similar states, so that it is beneficial to gradually improve the performance over the encounters; otherwise, the greedy policy would be optimal. Based on this intuition, we propose ReMax, an objective for RL whereby stochastic exploration arises as an emergent property, without adding any explicit exploration bonus. In ReMax, an episode is modified so that the agent can reset to previous states in the trajectory, and the agent\u2019s goal is to maximize the best return in the trajectory tree. We show that this ReMax objective can be directly optimized with an unbiased policy gradient method. Experiments confirm that ReMax leads to the emergence of a stochastic exploration policy, and improves the performance compared to RL with no exploration bonus."}}
{"id": "uOJZ_zU9qZm", "cdate": 1652737776092, "mdate": null, "content": {"title": "Proppo: a Message Passing Framework for Customizable and Composable Learning Algorithms", "abstract": "While existing automatic differentiation (AD) frameworks allow flexibly composing model architectures, they do not provide the same flexibility for composing learning algorithms---everything has to be implemented in terms of back propagation. To address this gap, we invent Automatic Propagation (AP) software, which generalizes AD, and allows custom and composable construction of complex learning algorithms. The framework allows packaging custom learning algorithms into propagators that automatically implement the necessary computations, and can be reused across different computation graphs. We implement Proppo, a prototype AP software package built on top of the Pytorch AD framework. To demonstrate the utility of Proppo, we use it to implement Monte Carlo gradient estimation techniques, such as reparameterization and likelihood ratio gradients, as well as the total propagation algorithm and Gaussian shaping gradients, which were previously used in model-based reinforcement learning, but do not have any publicly available implementation. Finally, in minimalistic experiments, we show that these methods allow increasing the gradient accuracy by orders of magnitude, particularly when the machine learning system is at the edge of chaos."}}
{"id": "E44rnebRje3", "cdate": 1622202914002, "mdate": null, "content": {"title": "A unified view of likelihood ratio and reparameterization gradients", "abstract": "  Reparameterization (RP) and likelihood ratio (LR) gradient\n  estimators are used to estimate gradients of expectations throughout\n  machine learning and reinforcement learning; however, they are usually\n  explained as simple mathematical tricks, with no insight into their\n  nature. We use a first principles approach to explain that LR and RP\n  are alternative methods of keeping track of the movement of\n  probability mass, and the two are connected via the divergence\n  theorem. Moreover, we show that the space of all possible estimators\n  combining LR and RP can be completely parameterized by a flow field\n  $u(x)$ and an importance sampling distribution\n  $q(x)$. We prove that there cannot exist a single-sample\n  estimator of this type outside our characterized space, thus,\n  clarifying where we should be searching for better Monte Carlo\n  gradient estimators."}}
{"id": "8rrnDGjOP49", "cdate": 1609459200000, "mdate": 1652714298077, "content": {"title": "A unified view of likelihood ratio and reparameterization gradients", "abstract": "Reparameterization (RP) and likelihood ratio (LR) gradient estimators are used to estimate gradients of expectations throughout machine learning and reinforcement learning; however, they are usually explained as simple mathematical tricks, with no insight into their nature. We use a first principles approach to explain that LR and RP are alternative methods of keeping track of the movement of probability mass, and the two are connected via the divergence theorem. Moreover, we show that the space of all possible estimators combining LR and RP can be completely parameterized by a flow field u(x) and importance sampling distribution q(x). We prove that there cannot exist a single-sample estimator of this type outside our characterized space, thus, clarifying where we should be searching for better Monte Carlo gradient estimators."}}
{"id": "TjZKo-bQLn_", "cdate": 1577836800000, "mdate": 1652714298029, "content": {"title": "Neural Replicator Dynamics: Multiagent Learning via Hedging Policy Gradients", "abstract": "Policy gradient and actor-critic algorithms form the basis of many commonly used training techniques in deep reinforcement learning. Using these algorithms in multiagent environments poses problems such as nonstationarity and instability. In this paper, we first demonstrate that standard softmax-based policy gradient can be prone to poor performance in the presence of even the most benign nonstationarity. By contrast, it is known that the replicator dynamics, a well-studied model from evolutionary game theory, eliminates dominated strategies and exhibits convergence of the time-averaged trajectories to interior Nash equilibria in zero-sum games. Thus, using the replicator dynamics as a foundation, we derive an elegant one-line change to policy gradient methods that simply bypasses the gradient step through the softmax, yielding a new algorithm titled Neural Replicator Dynamics (NeuRD). NeuRD reduces to the exponential weights/Hedge algorithm in the single-state all-actions case. Additionally, NeuRD has formal equivalence to softmax counterfactual regret minimization, which guarantees convergence in the sequential tabular case. Importantly, our algorithm provides a straightforward way of extending the replicator dynamics to the function approximation setting. Empirical results show that NeuRD quickly adapts to nonstationarities, outperforming policy gradient significantly in both tabular and function approximation settings, when evaluated on the standard imperfect information benchmarks of Kuhn Poker, Leduc Poker, and Goofspiel."}}
{"id": "SkEZnUbdWB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Total stochastic gradient algorithms and applications in reinforcement learning", "abstract": "Backpropagation and the chain rule of derivatives have been prominent; however, the total derivative rule has not enjoyed the same amount of attention. In this work we show how the total derivative rule leads to an intuitive visual framework for creating gradient estimators on graphical models. In particular, previous \u201dpolicy gradient theorems\u201d are easily derived. We derive new gradient estimators based on density estimation, as well as a likelihood ratio gradient, which \u201djumps\u201d to an intermediate node, not directly to the objective function. We evaluate our methods on model-based policy gradient algorithms, achieve good performance, and present evidence towards demystifying the success of the popular PILCO algorithm."}}
{"id": "Hyb9lhbdWr", "cdate": 1514764800000, "mdate": null, "content": {"title": "PIPPS: Flexible Model-Based Policy Search Robust to the Curse of Chaos", "abstract": "Previously, the exploding gradient problem has been explained to be central in deep learning and model-based reinforcement learning, because it causes numerical issues and instability in optimizati..."}}
