{"id": "sDtOa9RoBQ", "cdate": 1672531200000, "mdate": 1707117740567, "content": {"title": "Coordinate Descent for SLOPE", "abstract": "The lasso is the most famous sparse regression and feature selection method. One reason for its popularity is the speed at which the underlying optimization problem can be solved. Sorted L-One Pena..."}}
{"id": "1uSzacpyWLH", "cdate": 1652737787285, "mdate": null, "content": {"title": "Benchopt: Reproducible, efficient and collaborative optimization benchmarks", "abstract": "Numerical validation is at the core of machine learning research as it allows us to assess the actual impact of new methods, and to confirm the agreement between theory and practice. Yet, the rapid development of the field poses several challenges: researchers are confronted with a profusion of methods to compare, limited transparency and consensus on best practices, as well as tedious re-implementation work. As a result, validation is often very partial, which can lead to wrong conclusions that slow down the progress of research. We propose Benchopt, a collaborative framework to automatize, publish and reproduce optimization benchmarks in machine learning across programming languages and hardware architectures. Benchopt simplifies benchmarking for the community by providing an off-the-shelf tool for running, sharing and extending experiments. To demonstrate its broad usability, we showcase benchmarks on three standard ML tasks: $\\ell_2$-regularized logistic regression, Lasso and ResNet18 training for image classification. These benchmarks highlight key practical findings that give a more nuanced view of state-of-the-art for these problems, showing that for practical evaluation, the devil is in the details."}}
{"id": "IpBjWtJp40j", "cdate": 1652737695110, "mdate": null, "content": {"title": "The Hessian Screening Rule", "abstract": "Predictor screening rules, which discard predictors before fitting a model, have had considerable impact on the speed with which sparse regression problems, such as the lasso, can be solved. In this paper we present a new screening rule for solving the lasso path: the Hessian Screening Rule. The rule uses second-order information from the model to provide both effective screening, particularly in the case of high correlation, as well as accurate warm starts. The proposed rule outperforms all alternatives we study on simulated data sets with both low and high correlation for \\(\\ell_1\\)-regularized least-squares (the lasso) and logistic regression. It also performs best in general on the real data sets that we examine. "}}
{"id": "dT9T49uu3h", "cdate": 1640995200000, "mdate": 1671902109669, "content": {"title": "Benchopt: Reproducible, efficient and collaborative optimization benchmarks", "abstract": "Numerical validation is at the core of machine learning research as it allows to assess the actual impact of new methods, and to confirm the agreement between theory and practice. Yet, the rapid development of the field poses several challenges: researchers are confronted with a profusion of methods to compare, limited transparency and consensus on best practices, as well as tedious re-implementation work. As a result, validation is often very partial, which can lead to wrong conclusions that slow down the progress of research. We propose Benchopt, a collaborative framework to automate, reproduce and publish optimization benchmarks in machine learning across programming languages and hardware architectures. Benchopt simplifies benchmarking for the community by providing an off-the-shelf tool for running, sharing and extending experiments. To demonstrate its broad usability, we showcase benchmarks on three standard learning tasks: $\\ell_2$-regularized logistic regression, Lasso, and ResNet18 training for image classification. These benchmarks highlight key practical findings that give a more nuanced view of the state-of-the-art for these problems, showing that for practical evaluation, the devil is in the details. We hope that Benchopt will foster collaborative work in the community hence improving the reproducibility of research findings."}}
{"id": "9wYNRWNHF8", "cdate": 1640995200000, "mdate": 1707117740550, "content": {"title": "The Hessian Screening Rule", "abstract": "Predictor screening rules, which discard predictors before fitting a model, have had considerable impact on the speed with which sparse regression problems, such as the lasso, can be solved. In this paper we present a new screening rule for solving the lasso path: the Hessian Screening Rule. The rule uses second-order information from the model to provide both effective screening, particularly in the case of high correlation, as well as accurate warm starts. The proposed rule outperforms all alternatives we study on simulated data sets with both low and high correlation for (\\ell_1)-regularized least-squares (the lasso) and logistic regression. It also performs best in general on the real data sets that we examine."}}
{"id": "qdxo7EBgN5", "cdate": 1577836800000, "mdate": 1707117740562, "content": {"title": "The Strong Screening Rule for SLOPE", "abstract": "Extracting relevant features from data sets where the number of observations (n) is much smaller then the number of predictors (p) is a major challenge in modern statistics. Sorted L-One Penalized Estimation (SLOPE)\u2014a generalization of the lasso\u2014is a promising method within this setting. Current numerical procedures for SLOPE, however, lack the efficiency that respective tools for the lasso enjoy, particularly in the context of estimating a complete regularization path. A key component in the efficiency of the lasso is predictor screening rules: rules that allow predictors to be discarded before estimating the model. This is the first paper to establish such a rule for SLOPE. We develop a screening rule for SLOPE by examining its subdifferential and show that this rule is a generalization of the strong rule for the lasso. Our rule is heuristic, which means that it may discard predictors erroneously. In our paper, however, we show that such situations are rare and easily safeguarded against by a simple check of the optimality conditions. Our numerical experiments show that the rule performs well in practice, leading to improvements by orders of magnitude for data in the (p &gt;&gt; n) domain, as well as incurring no additional computational overhead when (n &gt; p)."}}
{"id": "9Yufq41B5K", "cdate": 1514764800000, "mdate": 1707117740554, "content": {"title": "A Case Study in Fitting Area-Proportional Euler Diagrams with Ellipses using eulerr", "abstract": ""}}
