{"id": "RXzzipcQ8Kt", "cdate": 1672531200000, "mdate": 1681681342295, "content": {"title": "Improved Best-of-Both-Worlds Guarantees for Multi-Armed Bandits: FTRL with General Regularizers and Multiple Optimal Arms", "abstract": "We study the problem of designing adaptive multi-armed bandit algorithms that perform optimally in both the stochastic setting and the adversarial setting simultaneously (often known as a best-of-both-world guarantee). A line of recent works shows that when configured and analyzed properly, the Follow-the-Regularized-Leader (FTRL) algorithm, originally designed for the adversarial setting, can in fact optimally adapt to the stochastic setting as well. Such results, however, critically rely on an assumption that there exists one unique optimal arm. Recently, Ito (2021) took the first step to remove such an undesirable uniqueness assumption for one particular FTRL algorithm with the $\\frac{1}{2}$-Tsallis entropy regularizer. In this work, we significantly improve and generalize this result, showing that uniqueness is unnecessary for FTRL with a broad family of regularizers and a new learning rate schedule. For some regularizers, our regret bounds also improve upon prior results even when uniqueness holds. We further provide an application of our results to the decoupled exploration and exploitation problem, demonstrating that our techniques are broadly applicable."}}
{"id": "VE8QRTrWAMb", "cdate": 1652737556919, "mdate": null, "content": {"title": "Near-Optimal Regret for Adversarial MDP with Delayed Bandit Feedback", "abstract": "The standard assumption in reinforcement learning (RL) is that agents observe feedback for their actions immediately. However, in practice feedback is often observed in delay. This paper studies online learning in episodic Markov decision process (MDP) with unknown transitions, adversarially changing costs, and unrestricted delayed bandit feedback. More precisely, the feedback for the agent in episode $k$ is revealed only in the end of episode $k + d^k$, where the delay $d^k$ can be changing over episodes and chosen by an oblivious adversary. We present the first algorithms that achieve near-optimal $\\sqrt{K + D}$ regret, where $K$ is the number of episodes and $D = \\sum_{k=1}^K d^k$ is the total delay, significantly improving upon the best known regret bound of $(K + D)^{2/3}$."}}
{"id": "gbgZVL_q8Lj", "cdate": 1640995200000, "mdate": 1652993365833, "content": {"title": "Near-Optimal Regret for Adversarial MDP with Delayed Bandit Feedback", "abstract": "The standard assumption in reinforcement learning (RL) is that agents observe feedback for their actions immediately. However, in practice feedback is often observed in delay. This paper studies online learning in episodic Markov decision process (MDP) with unknown transitions, adversarially changing costs, and unrestricted delayed bandit feedback. More precisely, the feedback for the agent in episode $k$ is revealed only in the end of episode $k + d^k$, where the delay $d^k$ can be changing over episodes and chosen by an oblivious adversary. We present the first algorithms that achieve near-optimal $\\sqrt{K + D}$ regret, where $K$ is the number of episodes and $D = \\sum_{k=1}^K d^k$ is the total delay, significantly improving upon the best known regret bound of $(K + D)^{2/3}$."}}
{"id": "-zALR_-372y", "cdate": 1621630014606, "mdate": null, "content": {"title": "The best of both worlds: stochastic and adversarial episodic MDPs with unknown transition", "abstract": "We consider the best-of-both-worlds problem for learning an episodic Markov Decision Process through $T$ episodes, with the goal of achieving $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret when the losses are adversarial and simultaneously $\\mathcal{O}(\\log T)$ regret when the losses are (almost) stochastic. Recent work by [Jin and Luo, 2020]  achieves this goal when the fixed transition is known, and leaves the case of unknown transition as a major open question. In this work, we resolve this open problem by using the same Follow-the-Regularized-Leader (FTRL) framework together with a set of new techniques. Specifically, we first propose a loss-shifting trick in the FTRL analysis, which greatly simplifies the approach of [Jin and Luo, 2020] and already improves their results for the known transition case. Then, we extend this idea to the unknown transition case and develop a novel analysis which upper bounds the transition estimation error by the regret itself in the stochastic setting, a key property to ensure $\\mathcal{O}(\\log T)$ regret."}}
{"id": "_Md4LWRev8", "cdate": 1609459200000, "mdate": 1681681342580, "content": {"title": "The best of both worlds: stochastic and adversarial episodic MDPs with unknown transition", "abstract": "We consider the best-of-both-worlds problem for learning an episodic Markov Decision Process through $T$ episodes, with the goal of achieving $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret when the losses are adversarial and simultaneously $\\mathcal{O}(\\text{polylog}(T))$ regret when the losses are (almost) stochastic. Recent work by [Jin and Luo, 2020] achieves this goal when the fixed transition is known, and leaves the case of unknown transition as a major open question. In this work, we resolve this open problem by using the same Follow-the-Regularized-Leader ($\\text{FTRL}$) framework together with a set of new techniques. Specifically, we first propose a loss-shifting trick in the $\\text{FTRL}$ analysis, which greatly simplifies the approach of [Jin and Luo, 2020] and already improves their results for the known transition case. Then, we extend this idea to the unknown transition case and develop a novel analysis which upper bounds the transition estimation error by (a fraction of) the regret itself in the stochastic setting, a key property to ensure $\\mathcal{O}(\\text{polylog}(T))$ regret."}}
{"id": "NW9qKrdyjy", "cdate": 1609459200000, "mdate": 1681681342374, "content": {"title": "The best of both worlds: stochastic and adversarial episodic MDPs with unknown transition", "abstract": "We consider the best-of-both-worlds problem for learning an episodic Markov Decision Process through $T$ episodes, with the goal of achieving $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret when the losses are adversarial and simultaneously $\\mathcal{O}(\\log T)$ regret when the losses are (almost) stochastic. Recent work by [Jin and Luo, 2020] achieves this goal when the fixed transition is known, and leaves the case of unknown transition as a major open question. In this work, we resolve this open problem by using the same Follow-the-Regularized-Leader (FTRL) framework together with a set of new techniques. Specifically, we first propose a loss-shifting trick in the FTRL analysis, which greatly simplifies the approach of [Jin and Luo, 2020] and already improves their results for the known transition case. Then, we extend this idea to the unknown transition case and develop a novel analysis which upper bounds the transition estimation error by the regret itself in the stochastic setting, a key property to ensure $\\mathcal{O}(\\log T)$ regret."}}
{"id": "cFdFFZHrjI", "cdate": 1577836800000, "mdate": 1681681342380, "content": {"title": "Simultaneously Learning Stochastic and Adversarial Episodic MDPs with Known Transition", "abstract": "This work studies the problem of learning episodic Markov Decision Processes with known transition and bandit feedback. We develop the first algorithm with a ``best-of-both-worlds'' guarantee: it achieves O(log T) regret when the losses are stochastic, and simultaneously enjoys worst-case robustness with \\tilde{O}(\\sqrt{T}) regret even when the losses are adversarial, where T is the number of episodes. More generally, it achieves \\tilde{O}(\\sqrt{C}) regret in an intermediate setting where the losses are corrupted by a total amount of C. Our algorithm is based on the Follow-the-Regularized-Leader method from Zimin and Neu (2013), with a novel hybrid regularizer inspired by recent works of Zimmert et al. (2019a, 2019b) for the special case of multi-armed bandits. Crucially, our regularizer admits a non-diagonal Hessian with a highly complicated inverse. Analyzing such a regularizer and deriving a particular self-bounding regret guarantee is our key technical contribution and might be of independent interest."}}
{"id": "_dSx6zUpx2", "cdate": 1577836800000, "mdate": 1681681342305, "content": {"title": "Simultaneously Learning Stochastic and Adversarial Episodic MDPs with Known Transition", "abstract": "This work studies the problem of learning episodic Markov Decision Processes with known transition and bandit feedback. We develop the first algorithm with a ``best-of-both-worlds'' guarantee: it achieves $\\mathcal{O}(log T)$ regret when the losses are stochastic, and simultaneously enjoys worst-case robustness with $\\tilde{\\mathcal{O}}(\\sqrt{T})$ regret even when the losses are adversarial, where $T$ is the number of episodes. More generally, it achieves $\\tilde{\\mathcal{O}}(\\sqrt{C})$ regret in an intermediate setting where the losses are corrupted by a total amount of $C$. Our algorithm is based on the Follow-the-Regularized-Leader method from Zimin and Neu (2013), with a novel hybrid regularizer inspired by recent works of Zimmert et al. (2019a, 2019b) for the special case of multi-armed bandits. Crucially, our regularizer admits a non-diagonal Hessian with a highly complicated inverse. Analyzing such a regularizer and deriving a particular self-bounding regret guarantee is our key technical contribution and might be of independent interest."}}
{"id": "1mHW6MZ8NE0", "cdate": 1577836800000, "mdate": null, "content": {"title": "Learning Adversarial Markov Decision Processes with Bandit Feedback and Unknown Transition", "abstract": "We consider the task of learning in episodic finite-horizon Markov decision processes with an unknown transition function, bandit feedback, and adversarial losses. We propose an efficient algorithm..."}}
{"id": "iVl2CyRzEcx", "cdate": 1546300800000, "mdate": 1673927260939, "content": {"title": "Deep Reinforcement Learning for Multi-driver Vehicle Dispatching and Repositioning Problem", "abstract": ""}}
