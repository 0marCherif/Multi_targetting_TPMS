{"id": "gb6ocYuVhk1", "cdate": 1665251224872, "mdate": null, "content": {"title": "Transformer-based World Models Are Happy With 100k Interactions", "abstract": "Deep neural networks have been successful in many reinforcement learning settings. However, compared to human learners they are overly data hungry. To build a sample-efficient world model, we apply a transformer to real-world episodes in an autoregressive manner: not only the compact latent states and the taken actions but also the experienced or predicted rewards are fed into the transformer, so that it can attend flexibly to all three modalities at different time steps. The transformer allows our world model to access previous states directly, instead of viewing them through a compressed recurrent state. By utilizing the Transformer-XL architecture, it is able to learn long-term dependencies while staying computationally efficient. Our transformer-based world model (TWM) generates meaningful, new experience, which is used to train a policy that outperforms previous model-free and model-based reinforcement learning algorithms on the Atari 100k benchmark."}}
{"id": "Jxn-jKvml4", "cdate": 1664725481727, "mdate": null, "content": {"title": "Evaluating Robust Perceptual Losses for Image Reconstruction", "abstract": "Nowadays, many deep neural networks (DNNs) for image reconstructing tasks are trained using a combination of pixel-wise loss functions and perceptual image losses like learned perceptual image patch similarity (LPIPS). As these perceptual image losses compare the features of a pre-trained DNN, it is unsurprising that they are vulnerable to adversarial examples. It is known that: (i) DNNs can be robustified against adversarial examples using adversarial training, and (ii) adversarial examples are imperceptible by the human eye. Thus, we hypothesize that perceptual metrics, based on a robustly trained DNN, are more aligned with human perception than those based on non-robust models. Our extensive experiments on an image super resolution task show, however, that this is not the case. We observe that models trained with a robust perceptual loss tend to produce more artifacts in the reconstructed image. Furthermore, we were unable to find reliable image similarity metrics or evaluation methods to quantify these observations (which are known open problems)."}}
{"id": "h-CGz4iblJi", "cdate": 1664248826587, "mdate": null, "content": {"title": "Optimizing Intermediate Representations of Generative Models for Phase Retrieval", "abstract": "Fourier phase retrieval is the problem of reconstructing images from magnitude-only measurements. It is relevant in many areas of science, e.g., in X-ray crystallography, astronomy, microscopy, array imaging and optics. When training data is available, generative models can be used to constrain the solution set. However, not all possible solutions are within the range of the generator. Instead, they are represented with some error. To reduce this representation error in the context of phase retrieval, we first leverage a novel variation of intermediate layer optimization (ILO) to extend the range of the generator while still producing images consistent with the training data. Second, we introduce new initialization schemes that further improve the quality of the reconstruction. With extensive experiments, we can show the benefits of our modified ILO and the new initialization schemes."}}
{"id": "TdBaDGCpjly", "cdate": 1663850420130, "mdate": null, "content": {"title": "Transformer-based World Models Are Happy With 100k Interactions", "abstract": "Deep neural networks have been successful in many reinforcement learning settings. However, compared to human learners they are overly data hungry. To build a sample-efficient world model, we apply a transformer to real-world episodes in an autoregressive manner: not only the compact latent states and the taken actions but also the experienced or predicted rewards are fed into the transformer, so that it can attend flexibly to all three modalities at different time steps. The transformer allows our world model to access previous states directly, instead of viewing them through a compressed recurrent state. By utilizing the Transformer-XL architecture, it is able to learn long-term dependencies while staying computationally efficient. Our transformer-based world model (TWM) generates meaningful, new experience, which is used to train a policy that outperforms previous model-free and model-based reinforcement learning algorithms on the Atari 100k benchmark. Our code is available at https://github.com/jrobine/twm."}}
{"id": "rlWzUnM72RF", "cdate": 1644077742172, "mdate": null, "content": {"title": "[Re] Solving Phase Retrieval With a Learned Reference", "abstract": "Scope of Reproducibility\nThis report reproduces the experiments and validates the results of the ECCV 2020 paper \"Solving Phase Retrieval with a Learned Reference\" by Hyder et al. The authors consider the task of recovering an unknown signal from its Fourier magnitudes, where the measurements are obtained after a reference image is added onto the signal. In order to solve this task a novel, iterative phase retrieval algorithm, presented as an unrolled network, that can train a such reference on a small amount of data is proposed. It is shown that the learned reference generalizes well to unseen data distributions and is robust to spatial data augmentation like shifting and rotation.\n\nMethodology\nWe use the provided original code to reproduce the experiments from Hyder et al. that validate the proposed claims. Nevertheless, we refactor the code base to accelerate the performance and we extent it to carry out experiments where no code is available. We perform a hyperparameter search to investigate the influence and optimal values of the learning rates in both the training and retrieval process. Additionally, we do an ablation study to evaluate the necessary parts of the proposed algorithm. For our experiments we use a single NVIDIA TESLA P100 GPU with 16 GB RAM and approximately 100 computational hours for all experiments together.\n\nResults\nIn general, we are able to reproduce the results of Hyder et al. Because of the hyperparameter search, we are certain that the results are not cherry-picked and mostly reproducible using the authors' implementation of the algorithm. With our additional experiments, we further strengthen the validity of the proposed method and help future researchers and practitioners by providing additional information on the learning rates in the training and retrieval process.\n\nWhat Was Easy\nThe authors provide an implementation of their algorithm that is executable in our environment after exchanging deprecated functions. The considered datasets are open access, hence easy to use. Furthermore, the computational cost is fairly low such that we could run extensive experiments and even compare different hyperparameter settings.\n\nWhat Was Difficult\nWe spend some effort to understand the authors' implementation, as it is marginally documented and the used computational tricks are not explained in detail. Moreover, it contains some redundant code which slows down computation. Beyond refactoring, we had to extent the implementation to be able to run our experiments. The lack of information about the learning rates slowed down the reproduction of the results, as we first had to investigate the influences on the training and retrieval process before we could adjust the parameters effectively.\n\nCommunication With Original Authors\nWe were in contact with the authors via mail and we would like to thank the authors for helping us.  Especially, we thank Rakib Hyder who kindly answered all our questions regarding implementation details and hyperparameters and Salman Asif who was open for our implementation suggestions and provided useful feedback for this report.\n"}}
{"id": "5KYdC4a3Rn3", "cdate": 1634622667691, "mdate": null, "content": {"title": "A Closer Look at Reference Learning for Fourier Phase Retrieval", "abstract": "Reconstructing images from their Fourier magnitude measurements is a problem that often arises in different research areas. This process is also referred to as phase retrieval. In this work, we consider a modified version of the phase retrieval problem, which allows for a reference image to be added onto the image before the Fourier magnitudes are measured. We analyze an unrolled Gerchberg-Saxton (GS) algorithm that can be used to learn a good reference image from a dataset. Furthermore, we take a closer look at the learned reference images and propose a simple and efficient heuristic to construct reference images that, in some cases, yields reconstructions of comparable quality as approaches that learn references. Our code is available at https://github.com/tuelwer/reference-learning."}}
{"id": "uw4Gt0jsCdQ", "cdate": 1609459200000, "mdate": 1640260989356, "content": {"title": "Non-iterative Phase Retrieval with Cascaded Neural Networks", "abstract": "Fourier phase retrieval is the problem of recovering an image given only the magnitude of its Fourier transformation. Optimization-based approaches, like the well-established Gerchberg-Saxton or the hybrid input output algorithm, struggle at reconstructing images from magnitudes that are not oversampled. This motivates the application of learned methods, which allow reconstruction from non-oversampled magnitude measurements after a learning phase. In this paper, we want to push the limits of these learned methods by means of a deep neural network cascade that reconstructs the image successively on different resolutions from its non-oversampled Fourier magnitude. We evaluate our method on four different datasets (MNIST, EMNIST, Fashion-MNIST, and KMNIST) and demonstrate that it yields improved performance over other non-iterative methods and optimization-based methods."}}
{"id": "bW-DD6XV-6F", "cdate": 1609459200000, "mdate": 1640260989494, "content": {"title": "Learning to Detect Adversarial Examples Based on Class Scores", "abstract": "Given the increasing threat of adversarial attacks on deep neural networks (DNNs), research on efficient detection methods is more important than ever. In this work, we take a closer look at adversarial attack detection based on the class scores of an already trained classification model. We propose to train a support vector machine (SVM) on the class scores to detect adversarial examples. Our method is able to detect adversarial examples generated by various attacks, and can be easily adopted to a plethora of deep classification models. We show that our approach yields an improved detection rate compared to an existing method, whilst being easy to implement. We perform an extensive empirical analysis on different deep classification models, investigating various state-of-the-art adversarial attacks. Moreover, we observe that our proposed method is better at detecting a combination of adversarial attacks. This work indicates the potential of detecting various adversarial attacks simply by using the class scores of an already trained classification model."}}
{"id": "Um5VirYfF8b", "cdate": 1609459200000, "mdate": 1640260989493, "content": {"title": "Learning to Plan via a Multi-step Policy Regression Method", "abstract": "We propose a new approach to increase inference performance in environments that require a specific sequence of actions in order to be solved. This is for example the case for maze environments where ideally an optimal path is determined. Instead of learning a policy for a single step, we want to learn a policy that can predict n actions in advance. Our proposed method called policy horizon regression (PHR) uses knowledge of the environment sampled by A2C to learn an n dimensional policy vector in a policy distillation setup which yields n sequential actions per observation. We test our method on the MiniGrid and Pong environments and show drastic speedup during inference time by successfully predicting sequences of actions on a single observation."}}
{"id": "yp57Kki5Hja", "cdate": 1577836800000, "mdate": null, "content": {"title": "Fast Multi-Level Foreground Estimation", "abstract": "Alpha matting aims to estimate the translucency of an object in a given image. The resulting alpha matte describes pixel-wise to what amount foreground and background colors contribute to the color of the composite image. While most methods in literature focus on estimating the alpha matte, the process of estimating the foreground colors given the input image and its alpha matte is often neglected, although foreground estimation is an essential part of many image editing workflows. In this work, we propose a novel method for foreground estimation given the alpha matte. We demonstrate that our fast multi-level approach yields results that are comparable with the state-of-the-art while outperforming those methods in computational runtime and memory usage."}}
