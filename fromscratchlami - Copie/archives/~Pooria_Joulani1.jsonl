{"id": "0ca2bIfMVQs", "cdate": 1640995200000, "mdate": 1683554441183, "content": {"title": "Faster Rates, Adaptive Algorithms, and Finite-Time Bounds for Linear Composition Optimization and Gradient TD Learning", "abstract": "Gradient temporal difference (GTD) algorithms are provably convergent policy evaluation methods for off-policy reinforcement learning. Despite much progress, proper tuning of the stochastic approximation methods used to solve the resulting saddle point optimization problem requires the knowledge of several (unknown) problem-dependent parameters. In this paper we apply adaptive step-size tuning strategies to greatly reduce this dependence on prior knowledge, and provide algorithms with adaptive convergence guarantees. In addition, we use the underlying refined analysis technique to obtain new O(1/T) rates that do not depend on the strong-convexity parameter of the problem, and also apply to the Markov noise setting, as well as the unbounded i.i.d. noise setting."}}
{"id": "foopa_Ki6E", "cdate": 1609459200000, "mdate": 1683554440994, "content": {"title": "Adaptive Approximate Policy Iteration", "abstract": "Model-free reinforcement learning algorithms combined with value function approximation have recently achieved impressive performance in a variety of application domains. However, the theoretical understanding of such algorithms is limited, and existing results are largely focused on episodic or discounted Markov decision processes (MDPs). In this work, we present adaptive approximate policy iteration (AAPI), a learning scheme which enjoys a O(T^{2/3}) regret bound for undiscounted, continuing learning in uniformly ergodic MDPs. This is an improvement over the best existing bound of O(T^{3/4}) for the average-reward case with function approximation. Our algorithm and analysis rely on online learning techniques, where value functions are treated as losses. The main technical novelty is the use of a data-dependent adaptive learning rate coupled with a so-called optimistic prediction of upcoming losses. In addition to theoretical guarantees, we demonstrate the advantages of our approach empirically on several environments."}}
{"id": "O4YZlUrbkU", "cdate": 1609459200000, "mdate": 1683554441059, "content": {"title": "Adapting to Delays and Data in Adversarial Multi-Armed Bandits", "abstract": "We consider the adversarial multi-armed bandit problem under delayed feedback. We analyze variants of the Exp3 algorithm that tune their step size using only information (about the losses and delay..."}}
{"id": "rZ7UwSkRrgs", "cdate": 1577836800000, "mdate": null, "content": {"title": "Provably Efficient Adaptive Approximate Policy Iteration", "abstract": "Model-free reinforcement learning algorithms combined with value function approximation have recently achieved impressive performance in a variety of application domains. However, the theoretical understanding of such algorithms is limited, and existing results are largely focused on episodic or discounted Markov decision processes (MDPs). In this work, we present adaptive approximate policy iteration (AAPI), a learning scheme which enjoys a $\\tilde{O}(T^{2/3})$ regret bound for undiscounted, continuing learning in uniformly ergodic MDPs. This is an improvement over the best existing bound of $\\tilde{O}(T^{3/4})$ for the average-reward case with function approximation. Our algorithm and analysis rely on online learning techniques, where value functions are treated as losses. The main technical novelty is the use of a data-dependent adaptive learning rate coupled with a so-called optimistic prediction of upcoming losses. In addition to theoretical guarantees, we demonstrate the advantages of our approach empirically on several environments."}}
{"id": "qbC2GL6XeaQ", "cdate": 1577836800000, "mdate": 1683554440930, "content": {"title": "A modular analysis of adaptive (non-)convex optimization: Optimism, composite objectives, variance reduction, and variational bounds", "abstract": ""}}
{"id": "W_8AbTQu8RQ", "cdate": 1577836800000, "mdate": null, "content": {"title": "A simpler approach to accelerated optimization: iterative averaging meets optimism", "abstract": "Recently there have been several attempts to extend Nesterov\u2019s accelerated algorithm to smooth stochastic and variance-reduced optimization. In this paper, we show that there is a simpler approach ..."}}
{"id": "4xW7brdcIH7", "cdate": 1577836800000, "mdate": null, "content": {"title": "Adapting to Delays and Data in Adversarial Multi-Armed Bandits", "abstract": "We consider the adversarial multi-armed bandit problem under delayed feedback. We analyze variants of the Exp3 algorithm that tune their step-size using only information (about the losses and delays) available at the time of the decisions, and obtain regret guarantees that adapt to the observed (rather than the worst-case) sequences of delays and/or losses. First, through a remarkably simple proof technique, we show that with proper tuning of the step size, the algorithm achieves an optimal (up to logarithmic factors) regret of order $\\sqrt{\\log(K)(TK + D)}$ both in expectation and in high probability, where $K$ is the number of arms, $T$ is the time horizon, and $D$ is the cumulative delay. The high-probability version of the bound, which is the first high-probability delay-adaptive bound in the literature, crucially depends on the use of implicit exploration in estimating the losses. Then, following Zimmert and Seldin [2019], we extend these results so that the algorithm can \"skip\" rounds with large delays, resulting in regret bounds of order $\\sqrt{TK\\log(K)} + |R| + \\sqrt{D_{\\bar{R}}\\log(K)}$, where $R$ is an arbitrary set of rounds (which are skipped) and $D_{\\bar{R}}$ is the cumulative delay of the feedback for other rounds. Finally, we present another, data-adaptive (AdaGrad-style) version of the algorithm for which the regret adapts to the observed (delayed) losses instead of only adapting to the cumulative delay (this algorithm requires an a priori upper bound on the maximum delay, or the advance knowledge of the delay for each decision when it is made). The resulting bound can be orders of magnitude smaller on benign problems, and it can be shown that the delay only affects the regret through the loss of the best arm."}}
{"id": "rygZ9SHxLr", "cdate": 1567802760604, "mdate": null, "content": {"title": "Think out of the \"Box\": Generically-Constrained Asynchronous Composite Optimization and Hedging", "abstract": "We present two new algorithms, ASYNCADA and HEDGEHOG, for asynchronous sparse online and stochastic optimization. ASYNCADA is the first asynchronous stochastic optimization algorithm with finite-time data-dependent convergence guarantees for generic convex constraints that, in addition: (a) allows for proximal (i.e., composite-objective) updates and adaptive step-sizes; (b) enjoys any-time convergence guarantees without requiring an exact global clock; and (c) when the data is sufficiently sparse, its convergence rate for (non-)smooth, (non-)strongly-convex, and even a limited class of non-convex objectives matches the corresponding serial rate, implying a theoretical \u201clinear speed-up\u201d. The second algorithm, HEDGEHOG, is an asynchronous parallel version of the Exponentiated Gradient (EG) algorithm for optimization over the probability simplex (a.k.a. Hedge in online learning), and the first asynchronous algorithm without SGD-style updates enjoying linear speed-ups under sparsity. Unlike previous work, ASYNCADA and HEDGEHOG and their convergence and speed-up analyses are not limited to individual coordinate-wise (i.e., \u201cbox-shaped\u201d) constraints or smooth and strongly-convex objectives. Underlying both results is a generic analysis framework that is of independent interest and further applicable to distributed and delayed feedback optimization."}}
{"id": "JN5sRBRcCq", "cdate": 1483228800000, "mdate": 1683554440933, "content": {"title": "A Modular Analysis of Adaptive (Non-)Convex Optimization: Optimism, Composite Objectives, and Variational Bounds", "abstract": "Recently, much work has been done on extending the scope of online learning and incremental stochastic optimization algorithms. In this paper we contribute to this effort in two ways: First, based ..."}}
{"id": "BbNJZxy72v", "cdate": 1483228800000, "mdate": 1683554440937, "content": {"title": "A Modular Analysis of Adaptive (Non-)Convex Optimization: Optimism, Composite Objectives, and Variational Bounds", "abstract": "Recently, much work has been done on extending the scope of online learning and incremental stochastic optimization algorithms. In this paper we contribute to this effort in two ways: First, based on a new regret decomposition and a generalization of Bregman divergences, we provide a self-contained, modular analysis of the two workhorses of online learning: (general) adaptive versions of Mirror Descent (MD) and the Follow-the-Regularized-Leader (FTRL) algorithms. The analysis is done with extra care so as not to introduce assumptions not needed in the proofs and allows to combine, in a straightforward way, different algorithmic ideas (e.g., adaptivity, optimism, implicit updates) and learning settings (e.g., strongly convex or composite objectives). This way we are able to reprove, extend and refine a large body of the literature, while keeping the proofs concise. The second contribution is a byproduct of this careful analysis: We present algorithms with improved variational bounds for smooth, composite objectives, including a new family of optimistic MD algorithms with only one projection step per round. Furthermore, we provide a simple extension of adaptive regret bounds to practically relevant non-convex problem settings with essentially no extra effort."}}
