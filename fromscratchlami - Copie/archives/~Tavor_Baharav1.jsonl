{"id": "SmepiVXBW-", "cdate": 1672345586799, "mdate": 1672345586799, "content": {"title": "A statistical reference-free genomic algorithm subsumes common workflows and enables novel discovery", "abstract": "We introduce a probabilistic model that enables study of myriad, disparate and\nfundamental problems in genome science and expands the scope of inference currently possible.\nOur model formulates an unrecognized unifying goal of many biological studies \u2013 to discover\nsample-specific sequence diversification \u2013 and subsumes many application-specific models.\nWith it, we develop a novel algorithm, NOMAD, that performs valid statistical inference on raw\nreads, completely bypassing references and sample metadata. NOMAD's reference-free approach\nenables data-scientifically driven discovery with previously unattainable generality, illustrated\nwith de novo prediction of adaptation in SARS-CoV-2, novel single-cell resolved,\ncell-type-specific isoform expression, including in the major histocompatibility complex, and de\nnovo identification of V(D)J recombination. NOMAD is a unifying, provably valid and highly\nefficient algorithmic solution that enables expansive discovery."}}
{"id": "gqlXraGjzq4", "cdate": 1672345459153, "mdate": 1672345459153, "content": {"title": "Unsupervised reference-free inference reveals unrecognized regulated transcriptomic complexity in human single cells", "abstract": "Myriad mechanisms diversify the sequence content of eukaryotic transcripts at the DNA and RNA level\nwith profound functional consequences. Examples include diversity generated by RNA splicing and V(D)J\nrecombination. Today, these and other events are detected with fragmented bioinformatic tools that require\npredefining a form of transcript diversification; moreover, they rely on alignment to a necessarily incomplete\nreference genome, filtering out unaligned sequences which can be among the most interesting. Each of these\nsteps introduces blindspots for discovery. Here, we develop NOMAD+, a new analytic method that performs\nunified, reference-free statistical inference directly on raw sequencing reads, extending the core NOMAD\nalgorithm to include a micro-assembly and interpretation framework. NOMAD+ discovers broad and new\nexamples of transcript diversification in single cells, bypassing genome alignment and without requiring cell\ntype metadata and impossible with current algorithms. In 10,326 primary human single cells in 19 tissues\nprofiled with SmartSeq2, NOMAD+ discovers a set of splicing and histone regulators with highly conserved\nintronic regions that are themselves targets of complex splicing regulation and unreported transcript diversity in\nthe heat shock protein HSP90AA1. NOMAD+ simultaneously discovers diversification in centromeric RNA\nexpression, V(D)J recombination, RNA editing, and repeat expansions missed by or impossible to measure\nwith existing bioinformatic methods. NOMAD+ is a unified, highly efficient algorithm enabling unbiased\ndiscovery of an unprecedented breadth of RNA regulation and diversification in single cells through a new\nparadigm to analyze the transcriptome."}}
{"id": "q16HXpXtjJn", "cdate": 1652737400706, "mdate": null, "content": {"title": "Beyond the Best:  Distribution Functional Estimation in Infinite-Armed Bandits", "abstract": "In the infinite-armed bandit problem, each arm's average reward is sampled from an unknown distribution, and each arm can be sampled further to obtain noisy estimates of the average reward of that arm. Prior work focuses on the best arm, i.e. estimating the maximum of the average reward distribution. We consider a general class of distribution functionals beyond the maximum and obtain optimal sample complexities in both offline and online settings. We show that online estimation, where the learner can sequentially choose whether to sample a new or existing arm, offers no advantage over the offline setting for estimating the mean functional, but significantly reduces the sample complexity for other functionals such as the median, maximum, and trimmed mean. We propose unified meta algorithms for the online and offline settings and derive matching lower bounds using different Wasserstein distances. For the special case of median estimation, we identify a curious thresholding phenomenon on the indistinguishability between Gaussian convolutions with respect to the noise level, which may be of independent interest."}}
{"id": "wL5LEK7J-8e", "cdate": 1640995200000, "mdate": 1672297554718, "content": {"title": "Adaptive Data Depth via Multi-Armed Bandits", "abstract": ""}}
{"id": "t0LUZC4sMU", "cdate": 1640995200000, "mdate": 1672297554730, "content": {"title": "Beyond the Best: Estimating Distribution Functionals in Infinite-Armed Bandits", "abstract": ""}}
{"id": "aqOQPjs9C3", "cdate": 1640995200000, "mdate": 1672297554717, "content": {"title": "Approximate Function Evaluation via Multi-Armed Bandits", "abstract": ""}}
{"id": "gLN8Z2ifmEq", "cdate": 1609459200000, "mdate": 1632955599406, "content": {"title": "Bandit-Based Monte Carlo Optimization for Nearest Neighbors", "abstract": "The celebrated Monte Carlo method estimates an expensive-to-compute quantity by random sampling. Bandit-based Monte Carlo optimization is a general technique for computing the minimum of many such expensive-to-compute quantities by adaptive random sampling. The technique converts an optimization problem into a statistical estimation problem which is then solved via multi-armed bandits. We apply this technique to solve the problem of high-dimensional k-nearest neighbors, developing an algorithm which we prove is able to identify exact nearest neighbors with high probability. We show that under regularity assumptions on a dataset of n points in d-dimensional space, the complexity of our algorithm scales logarithmically with the dimension of the data as O((n+d)log <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup> nd/\u03b4) for error probability \u03b4, rather than linearly as in exact computation requiring O(nd). We corroborate our theoretical results with numerical simulations, showing that our algorithm outperforms both exact computation and state-of-the-art algorithms such as kGraph, NGT, and LSH on real datasets."}}
{"id": "XXfJMxfpSBY", "cdate": 1609459200000, "mdate": 1623607064476, "content": {"title": "Enabling Efficiency-Precision Trade-offs for Label Trees in Extreme Classification", "abstract": "Extreme multi-label classification (XMC) aims to learn a model that can tag data points with a subset of relevant labels from an extremely large label set. Real world e-commerce applications like personalized recommendations and product advertising can be formulated as XMC problems, where the objective is to predict for a user a small subset of items from a catalog of several million products. For such applications, a common approach is to organize these labels into a tree, enabling training and inference times that are logarithmic in the number of labels. While training a model once a label tree is available is well studied, designing the structure of the tree is a difficult task that is not yet well understood, and can dramatically impact both model latency and statistical performance. Existing approaches to tree construction fall at an extreme point, either optimizing exclusively for statistical performance, or for latency. We propose an efficient information theory inspired algorithm to construct intermediary operating points that trade off between the benefits of both. Our algorithm enables interpolation between these objectives, which was not previously possible. We corroborate our theoretical analysis with numerical results, showing that on the Wiki-500K benchmark dataset our method can reduce a proxy for expected latency by up to 28% while maintaining the same accuracy as Parabel. On several datasets derived from e-commerce customer logs, our modified label tree is able to improve this expected latency metric by up to 20% while maintaining the same accuracy. Finally, we discuss challenges in realizing these latency improvements in deployed models."}}
{"id": "Djc5zyGVYB", "cdate": 1609459200000, "mdate": 1632955599409, "content": {"title": "One for All and All for One: Distributed Learning of Fair Allocations With Multi-Player Bandits", "abstract": "Consider N cooperative but non-communicating players where each plays one out of M arms for T turns. Players have different utilities for each arm, represented as an N\u00d7M matrix. These utilities are unknown to the players. In each turn, players select an arm and receive a noisy observation of their utility for it. However, if any other players selected the same arm in that turn, all colliding players will receive zero utility due to the conflict. No communication between the players is possible. We propose two distributed algorithms which learn fair matchings between players and arms while minimizing the regret. We show that our first algorithm learns a max-min fairness matching with near- O(logT) regret (up to a loglogT factor). However, if one has a known target Quality of Service (QoS) (which may vary between players) then we show that our second algorithm learns a matching where all players obtain an expected reward of at least their QoS with constant regret, given that such a matching exists. In particular, if the max-min value is known, a max-min fairness matching can be learned with O(1) regret."}}
{"id": "zh6ZqSKmdTC", "cdate": 1577836800000, "mdate": null, "content": {"title": "Adaptive Learning of Rank-One Models for Efficient Pairwise Sequence Alignment", "abstract": "Pairwise alignment of DNA sequencing data is a ubiquitous task in bioinformatics and typically represents a heavy computational burden. State-of-the-art approaches to speed up this task use hashing to identify short segments (k-mers) that are shared by pairs of reads, which can then be used to estimate alignment scores. However, when the number of reads is large, accurately estimating alignment scores for all pairs is still very costly. Moreover, in practice, one is only interested in identifying pairs of reads with large alignment scores. In this work, we propose a new approach to pairwise alignment estimation based on two key new ingredients. The first ingredient is to cast the problem of pairwise alignment estimation under a general framework of rank-one crowdsourcing models, where the workers' responses correspond to k-mer hash collisions. These models can be accurately solved via a spectral decomposition of the response matrix. The second ingredient is to utilise a multi-armed bandit algorithm to adaptively refine this spectral estimator only for read pairs that are likely to have large alignments. The resulting algorithm iteratively performs a spectral decomposition of the response matrix for adaptively chosen subsets of the read pairs."}}
