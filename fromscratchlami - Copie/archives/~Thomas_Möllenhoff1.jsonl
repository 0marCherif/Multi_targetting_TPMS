{"id": "k4fevFqSQcX", "cdate": 1663850206526, "mdate": null, "content": {"title": "SAM as an Optimal Relaxation of Bayes", "abstract": "Sharpness-aware minimization (SAM) and related adversarial deep-learning methods can drastically improve generalization, but their underlying mechanisms are not yet fully understood. Here, we establish SAM as a relaxation of the Bayes objective where the expected negative-loss is replaced by the optimal convex lower bound, obtained by using the so-called Fenchel biconjugate. The connection enables a new Adam-like extension of SAM to automatically obtain reasonable uncertainty estimates, while sometimes also improving its accuracy. By connecting adversarial and Bayesian methods, our work opens a new path to robustness.  "}}
{"id": "HdqzA-vAVw", "cdate": 1580485621556, "mdate": null, "content": {"title": "Controlling Neural Networks via Energy Dissipation", "abstract": "The last decade has shown a tremendous success in solving various computer vision problems with the help of deep learning techniques. Lately, many works have demonstrated that learning-based approaches with suitable network architectures even exhibit superior performance for the solution of (ill-posed) image reconstruction problems such as deblurring, super-resolution, or medical image reconstruction. The drawback of purely learning-based methods, however, is that they cannot provide provable guarantees for the trained network to follow a given data formation process during inference. In this work we propose energy dissipating networks that iteratively compute a descent direction with respect to a given cost function or energy at the currently estimated reconstruction. Therefore, an adaptive step size rule such as a line-search, along with a suitable number of iterations can guarantee the reconstruction to follow a given data formation model encoded in the energy to arbitrary precision, and hence control the model's behavior even during test time. We prove that under standard assumptions, descent using the direction predicted by the network converges (linearly) to the global minimum of the energy. We illustrate the effectiveness of the proposed approach in experiments on single image super resolution and computed tomography (CT) reconstruction, and further illustrate extensions to convex feasibility problems."}}
{"id": "jn4b1WeoRSO", "cdate": 1577836800000, "mdate": null, "content": {"title": "Optimization of Graph Total Variation via Active-Set-based Combinatorial Reconditioning", "abstract": "Structured convex optimization on weighted graphs finds numerous applications in machine learning and computer vision. In this work, we propose a novel adaptive preconditioning strategy for proxima..."}}
{"id": "cWOZFOmoGWG", "cdate": 1577836800000, "mdate": null, "content": {"title": "Efficient Lifting Methods for Variational Problems", "abstract": "Variational methods are a well-established paradigm to solve practical problems in computer vision. Based on a novel sublabel-accurate multilabeling approach, we obtain efficient convex relaxations for nonconvex variational problems. Further, we demonstrate that such relaxations can also be derived by a dual finite-element approximation. We also propose a novel convex formulation for vectorial problems via a lifting to spaces of currents. Finally, we demonstrate that the introduced notions from geometric measure theory find further applications in machine learning."}}
{"id": "ry-Ew2WdZH", "cdate": 1546300800000, "mdate": null, "content": {"title": "Flat Metric Minimization with Applications in Generative Modeling", "abstract": "We take the novel perspective to view data not as a probability distribution but rather as a current. Primarily studied in the field of geometric measure theory, k-currents are continuous linear fu..."}}
{"id": "Bp8UtOgc84i", "cdate": 1546300800000, "mdate": null, "content": {"title": "Informative GANs via Structured Regularization of Optimal Transport", "abstract": "We tackle the challenge of disentangled representation learning in generative adversarial networks (GANs) from the perspective of regularized optimal transport (OT). Specifically, a smoothed OT loss gives rise to an implicit transportation plan between the latent space and the data space. Based on this theoretical observation, we exploit a structured regularization on the transportation plan to encourage a prescribed latent subspace to be informative. This yields the formulation of a novel informative OT-based GAN. By convex duality, we obtain the equivalent view that this leads to perturbed ground costs favoring sparsity in the informative latent dimensions. Practically, we devise a stable training algorithm for the proposed informative GAN. Our experiments support the hypothesis that such regularizations effectively yield the discovery of disentangled and interpretable latent representations. Our work showcases potential power of a regularized OT framework in the context of generative modeling through its access to the transport plan. Further challenges are addressed in this line."}}
{"id": "ByeqORgAW", "cdate": 1518730177028, "mdate": null, "content": {"title": "Proximal Backpropagation", "abstract": "We propose proximal backpropagation (ProxProp) as a novel algorithm that takes implicit instead of explicit gradient steps to update the network parameters during neural network training. Our algorithm is motivated by the step size limitation of explicit gradient descent, which poses an impediment for optimization. ProxProp is developed from a general point of view on the backpropagation algorithm, currently the most common technique to train neural networks via stochastic gradient descent and variants thereof. Specifically, we show that backpropagation of a prediction error is equivalent to sequential gradient descent steps on a quadratic penalty energy, which comprises the network activations as variables of the optimization. We further analyze theoretical properties of ProxProp and in particular prove that the algorithm yields a descent direction in parameter space and can therefore be combined with a wide variety of convergent algorithms. Finally, we devise an efficient numerical implementation that integrates well with popular deep learning frameworks. We conclude by demonstrating promising numerical results and show that ProxProp can be effectively combined with common first order optimizers such as Adam."}}
{"id": "ry4wu6WubH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Fight Ill-Posedness With Ill-Posedness: Single-Shot Variational Depth Super-Resolution From Shading", "abstract": "We put forward a principled variational approach for up-sampling a single depth map to the resolution of the companion color image provided by an RGB-D sensor. We combine heterogeneous depth and color data in order to jointly solve the ill-posed depth super-resolution and shape-from-shading problems. The low-frequency geometric information necessary to disambiguate shape-from-shading is extracted from the low-resolution depth measurements and, symmetrically, the high-resolution photometric clues in the RGB image provide the high-frequency information required to disambiguate depth super-resolution."}}
{"id": "HJW9LWGd-r", "cdate": 1483228800000, "mdate": null, "content": {"title": "Sublabel-Accurate Discretization of Nonconvex Free-Discontinuity Problems", "abstract": "In this work we show how sublabel-accurate multilabeling approaches [15, 18] can be derived by approximating a classical label-continuous convex relaxation of nonconvex free-discontinuity problems. This insight allows to extend these sublabel-accurate approaches from total variation to general convex and nonconvex regularizations. Furthermore, it leads to a systematic approach to the discretization of continuous convex relaxations. We study the relationship to existing discretizations and to discrete-continuous MRFs. Finally, we apply the proposed approach to obtain a sublabel-accurate and convex solution to the vectorial Mumford-Shah functional and show in several experiments that it leads to more precise solutions using fewer labels."}}
{"id": "ry4FocWuWS", "cdate": 1451606400000, "mdate": null, "content": {"title": "Sublabel-Accurate Convex Relaxation of Vectorial Multilabel Energies", "abstract": "Convex relaxations of multilabel problems have been demonstrated to produce provably optimal or near-optimal solutions to a variety of computer vision problems. Yet, they are of limited practical use as they require a fine discretization of the label space, entailing a huge demand in memory and runtime. In this work, we propose the first sublabel accurate convex relaxation for vectorial multilabel problems. Our key idea is to approximate the dataterm in a piecewise convex (rather than piecewise linear) manner. As a result we have a more faithful approximation of the original cost function that provides a meaningful interpretation for fractional solutions of the relaxed convex problem."}}
