{"id": "Zh21fp1B0vv", "cdate": 1652737575251, "mdate": null, "content": {"title": "Rate-Optimal Online Convex Optimization in Adaptive Linear Control", "abstract": "We consider the problem of controlling an unknown linear dynamical system under adversarially-changing convex costs and full feedback of both the state and cost function. We present the first computationally-efficient algorithm that attains an optimal $\\sqrt{T}$-regret rate compared to the best stabilizing linear controller in hindsight, while avoiding stringent assumptions on the costs such as strong convexity. Our approach is based on a careful design of non-convex lower confidence bounds for the online costs, and uses a novel technique for computationally-efficient regret minimization of these bounds that leverages their particular non-convex structure."}}
{"id": "5gDl-q1CyvJ", "cdate": 1640995200000, "mdate": 1649365843829, "content": {"title": "Efficient Online Linear Control with Stochastic Convex Costs and Unknown Dynamics", "abstract": "We consider the problem of controlling an unknown linear dynamical system under a stochastic convex cost and full feedback of both the state and cost function. We present a computationally efficient algorithm that attains an optimal $\\sqrt{T}$ regret-rate compared to the best stabilizing linear controller in hindsight. In contrast to previous work, our algorithm is based on the Optimism in the Face of Uncertainty paradigm. This results in a substantially improved computational complexity and a simpler analysis."}}
{"id": "6YUpXDNEm8", "cdate": 1609459200000, "mdate": 1649365843835, "content": {"title": "Online Policy Gradient for Model Free Learning of Linear Quadratic Regulators with \u221aT Regret", "abstract": "We consider the task of learning to control a linear dynamical system under fixed quadratic costs, known as the Linear Quadratic Regulator (LQR) problem. While model-free approaches are often favor..."}}
{"id": "yHXzUF1S5AB", "cdate": 1577836800000, "mdate": 1649365843794, "content": {"title": "Logarithmic Regret for Learning Linear Quadratic Regulators Efficiently", "abstract": "We consider the problem of learning in Linear Quadratic Control systems whose transition parameters are initially unknown. Recent results in this setting have demonstrated efficient learning algori..."}}
{"id": "Hme8ahs-8Vj", "cdate": 1577836800000, "mdate": 1649365843828, "content": {"title": "The Pendulum Arrangement: Maximizing the Escape Time of Heterogeneous Random Walks", "abstract": "We identify a fundamental phenomenon of heterogeneous one dimensional random walks: the escape (traversal) time is maximized when the heterogeneity in transition probabilities forms a pyramid-like potential barrier. This barrier corresponds to a distinct arrangement of transition probabilities, sometimes referred to as the pendulum arrangement. We reduce this problem to a sum over products, combinatorial optimization problem, proving that this unique structure always maximizes the escape time. This general property may influence studies in epidemiology, biology, and computer science to better understand escape time behavior and construct intruder-resilient networks."}}
{"id": "7uCwt7HOjSN", "cdate": 1577836800000, "mdate": 1649365843780, "content": {"title": "Bandit Linear Control", "abstract": "We consider the problem of controlling a known linear dynamical system under stochastic noise, adversarially chosen costs, and bandit feedback. Unlike the full feedback setting where the entire cost function is revealed after each decision, here only the cost incurred by the learner is observed. We present a new and efficient algorithm that, for strongly convex and smooth costs, obtains regret that grows with the square root of the time horizon T. We also give extensions of this result to general convex, possibly non-smooth costs, and to non-stochastic system noise. A key component of our algorithm is a new technique for addressing bandit optimization of loss functions with memory."}}
{"id": "rdMNdei7bfq", "cdate": 1514764800000, "mdate": 1649365843832, "content": {"title": "A General Approach to Multi-Armed Bandits Under Risk Criteria", "abstract": "Different risk-related criteria have received recent interest in learning problems, where typically each case is treated in a customized manner. In this paper we provide a more systematic approach to analyzing such risk criteria within a stochastic multi-armed bandit (MAB) formulation. We identify a set of general conditions that yield a simple characterization of the oracle rule (which serves as the regret benchmark), and facilitate the design of upper confidence bound (UCB) learning policies. The conditions are derived from problem primitives, primarily focusing on the relation between the arm reward distributions and the (risk criteria) performance metric. Among other things, the work highlights some (possibly non-intuitive) subtleties that differentiate various criteria in conjunction with statistical properties of the arms. Our main findings are illustrated on several widely used objectives such as conditional value-at-risk, mean-variance, Sharpe-ratio, and more."}}
