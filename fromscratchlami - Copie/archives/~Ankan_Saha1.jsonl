{"id": "Gz97YHG2hC", "cdate": 1577836800000, "mdate": null, "content": {"title": "Edge formation in Social Networks to Nurture Content Creators.", "abstract": "Social networks act as major content marketplaces where creators and consumers come together to share and consume various kinds of content. Content ranking applications (e.g., newsfeed, moments, notifications) and edge recommendation products (e.g., connect to members, follow celebrities or groups or hashtags) on such platforms aim at improving the consumer experience. In this work, we focus on the creator experience and specifically on improving edge recommendations to better serve creators in such ecosystems. The audience and reach of creators \u2013 individuals, celebrities, publishers and companies \u2013 are critically shaped by these edge recommendation products. Hence, incorporating creator utility in such recommendations can have a material impact on their success, and in turn, on the marketplace. In this paper, we (i) propose a general framework to incorporate creator utility in edge recommendations, (ii) devise a specific method to estimate edge-level creator utilities for currently unformed edges, (iii) outline the challenges of measurement and propose a practical experiment design, and finally (iv) discuss the implementation of our proposal at scale on LinkedIn, a professional network with 645M+ members, and report our findings."}}
{"id": "B2oP76E9QN", "cdate": 1546300800000, "mdate": null, "content": {"title": "Using AI to build communities around interests on LinkedIn.", "abstract": "At LinkedIn, our mission is to connect the world's professionals to make them more productive and successful. Our team, Communities Artificial Intelligence (AI), at LinkedIn helps our members achieve this goal is by providing a platform where communities can form around common interests and shared experiences. Fostering active communities at LinkedIn can be broken down into the following components: (1) Discover: Help members find new entities (members, companies, hashtags, and more) to follow that will expose them to communities that share their interests. (2) Engage: Engage members in the conversations taking place in their communities by recommending content from their areas of interest. (3) Contribute: Help members effectively engage with the right communities when they create or share content. These three components form the main pillars of a content-driven ecosystem and our goal is to use AI to successfully close the loop between Discover (via providing relevant follow recommendations), Engage (via delivering engaging content to users from their areas of interest), and Contribute (via suggesting hashtags to content creators to target the right audience). A diverse set of AI techniques is required to address the challenges that arise in each of these components. These techniques include: Supervised Learning (XGBoost, Logistic Regression, Linear Regression), Wide and Deep Models, Natural Language Processing (e.g., Word Embeddings, ngram matching), and Unsupervised Learning. In this presentation, we will provide an overview of the AI techniques we use to form active communities on LinkedIn. We will describe two solutions in detail. First, we will describe how we have built our Follow Recommendations product. The goal of the Follow Recommendations product is to recommend entities to a member that the member finds both immediately relevant (i.e., increase the probability the member will follow the recommended entity) as well as engaging in the long run (i.e., the recommended entity produces content that the member finds relevant). Our analysis of the performance of our follow recommendations models has shown the superiority of nonlinear models compared to their linear counterparts. To manage the explosion of data emanating from terabytes of features generated from (viewer, entity) pairs, we use an innovative 2-D hash join algorithm that was developed at LinkedIn. We are also moving towards a hybrid scoring architecture. This allows us to score candidates with complex offline models and then re-rank these candidates based on more time-sensitive contextual features online. This generates more relevant and timely recommendations for the members based on their recent activity on different parts of the LinkedIn ecosystem. Second, we will describe our approach to solve the problem of Hashtag Suggestion and Typeahead. Hashtags are a great tool that allows members to expand the reach of their posts to the right audience (or communities). Our Hashtag Suggestion and Typeahead (HST) product was built to aid members in adding hashtags to their posts. We do not only recommend hashtags that the member is likely to select into their post, but also hashtags that are more likely to get the member the most online feedback. We call the latter aspect downstream utility (or engagement). However, before realizing this utility, the member has to actually select from the recommended hashtags. Therefore, the HST product is produced by combining two models. The first model maximizes the probability that the member will select the suggested hashtag and the second one optimizes for downstream utility. Based on content consumption behavior on LinkedIn, we have a good understanding of the supply and demand of content tagged with a specific hashtag. This information enables us to shape the inventory as well as traffic in individual hashtag domains, thus providing a better experience to content-starved communities."}}
{"id": "r1V_a8-ObB", "cdate": 1483228800000, "mdate": null, "content": {"title": "Large-Scale Quadratically Constrained Quadratic Program via Low-Discrepancy Sequences", "abstract": "We consider the problem of solving a large-scale Quadratically Constrained Quadratic Program. Such problems occur naturally in many scientific and web applications. Although there are efficient methods which tackle this problem, they are mostly not scalable. In this paper, we develop a method that transforms the quadratic constraint into a linear form by a sampling a set of low-discrepancy points. The transformed problem can then be solved by applying any state-of-the-art large-scale solvers. We show the convergence of our approximate solution to the true solution as well as some finite sample error bounds. Experimental results are also shown to prove scalability in practice."}}
{"id": "HyZuCSW_ZH", "cdate": 1483228800000, "mdate": null, "content": {"title": "Generalized Mixed Effect Models for Personalizing Job Search", "abstract": "Job Search is a core product at LinkedIn which makes it essential to generate highly relevant search results when a user searches for jobs on Linkedin. Historically job results were ranked using linear models consisting of a combination of user, job and query features. This paper talks about a new generalized mixed effect models introduced in the context of ranking candidate job results for a job search performed on LinkedIn. We build a per-query model which is populated with coefficients corresponding to job-features in addition to the existing global model features. We describe the details of the new method along with the challenges faced in launching such a model into production and making it efficient at a very large scale. Our experiments show improvement over previous baseline ranking models, in terms of offline metrics (both AUC and [email\u00a0protected] metrics) as well as online metrics in production (Job Applies) which are of interest to us. The resulting method is more powerful and has also been adopted in other applications at LinkedIn successfully."}}
{"id": "hFmxUGOSVT3", "cdate": 1388534400000, "mdate": null, "content": {"title": "Accelerated training of max-margin Markov networks with kernels.", "abstract": "Structured output prediction is an important machine learning problem both in theory and practice, and the max-margin Markov network ( M 3 N ) is an effective approach. All state-of-the-art algorithms for optimizing M 3 N objectives take at least O ( 1 / \u03f5 ) number of iterations to find an \u03f5 accurate solution. Nesterov [1] broke this barrier by proposing an excessive gap reduction technique (EGR) which converges in O ( 1 / \u03f5 ) iterations. However, it is restricted to Euclidean projections which consequently requires an intractable amount of computation for each iteration when applied to solve M 3 N . In this paper, we show that by extending EGR to Bregman projection, this faster rate of convergence can be retained, and more importantly, the updates can be performed efficiently by exploiting graphical model factorization. Further, we design a kernelized procedure which allows all computations per iteration to be performed at the same cost as the state-of-the-art approaches. Previous article in issue Next article in issue"}}
{"id": "WM_rK0YtbCL", "cdate": 1356998400000, "mdate": null, "content": {"title": "On the Nonasymptotic Convergence of Cyclic Coordinate Descent Methods.", "abstract": "Cyclic coordinate descent is a classic optimization method that has witnessed a resurgence of interest in signal processing, statistics, and machine learning. Reasons for this renewed interest include the simplicity, speed, and stability of the method, as well as its competitive performance on $\\ell_1$ regularized smooth optimization problems. Surprisingly, very little is known about its nonasymptotic convergence behavior on these problems. Most existing results either just prove convergence or provide asymptotic rates. We fill this gap in the literature by proving $O(1/k)$ convergence rates (where $k$ is the iteration count) for two variants of cyclic coordinate descent under an isotonicity assumption. Our analysis proceeds by comparing the objective values attained by the two variants with each other, as well as with the gradient descent algorithm. We show that the iterates generated by the cyclic coordinate descent methods remain better than those of gradient descent uniformly over time."}}
{"id": "Nw9_eU0_FnR", "cdate": 1325376000000, "mdate": null, "content": {"title": "Smoothing multivariate performance measures.", "abstract": "Optimizing multivariate performance measure is an important task in Machine Learning. Joachims (2005) introduced a Support Vector Method whose underlying optimization problem is commonly solved by cutting plane methods (CPMs) such as SVM-Perf and BMRM. It can be shown that CPMs converge to an \u03b5 accurate solution in O(1/\u03bb\u03b5) iterations, where \u03bb is the trade-off parameter between the regularizer and the loss function. Motivated by the impressive convergence rate of CPM on a number of practical problems, it was conjectured that these rates can be further improved. We disprove this conjecture in this paper by constructing counter examples. However, surprisingly, we further discover that these problems are not inherently hard, and we develop a novel smoothing strategy, which in conjunction with Nesterov's accelerated gradient method, can find an \u03b5 accurate solution in O* (min{1/\u03b5, 1/\u221a\u03bb\u03b5}) iterations. Computationally, our smoothing technique is also particularly advantageous for optimizing multivariate performance scores such as precision/recall break-even point and ROCArea; the cost per iteration remains the same as that of CPMs. Empirical evaluation on some of the largest publicly available data sets shows that our method converges significantly faster than CPMs without sacrificing generalization ability."}}
{"id": "GhH071zCB_e", "cdate": 1325376000000, "mdate": null, "content": {"title": "Smoothing Multivariate Performance Measures", "abstract": "A Support Vector Method for multivariate performance measures was recently introduced by Joachims (2005). The underlying optimization problem is currently solved using cutting plane methods such as SVM-Perf and BMRM. One can show that these algorithms converge to an eta accurate solution in O(1/Lambda*e) iterations, where lambda is the trade-off parameter between the regularizer and the loss function. We present a smoothing strategy for multivariate performance scores, in particular precision/recall break-even point and ROCArea. When combined with Nesterov's accelerated gradient algorithm our smoothing strategy yields an optimization algorithm which converges to an eta accurate solution in O(min{1/e,1/sqrt(lambda*e)}) iterations. Furthermore, the cost per iteration of our scheme is the same as that of SVM-Perf and BMRM. Empirical evaluation on a number of publicly available datasets shows that our method converges significantly faster than cutting plane methods without sacrificing generalization ability."}}
{"id": "58viw9r-fVWf", "cdate": 1325376000000, "mdate": null, "content": {"title": "The Interplay Between Stability and Regret in Online Learning", "abstract": "This paper considers the stability of online learning algorithms and its implications for learnability (bounded regret). We introduce a novel quantity called {\\em forward regret} that intuitively measures how good an online learning algorithm is if it is allowed a one-step look-ahead into the future. We show that given stability, bounded forward regret is equivalent to bounded regret. We also show that the existence of an algorithm with bounded regret implies the existence of a stable algorithm with bounded regret and bounded forward regret. The equivalence results apply to general, possibly non-convex problems. To the best of our knowledge, our analysis provides the first general connection between stability and regret in the online setting that is not restricted to a particular class of algorithms. Our stability-regret connection provides a simple recipe for analyzing regret incurred by any online learning algorithm. Using our framework, we analyze several existing online learning algorithms as well as the \"approximate\" versions of algorithms like RDA that solve an optimization problem at each iteration. Our proofs are simpler than existing analysis for the respective algorithms, show a clear trade-off between stability and forward regret, and provide tighter regret bounds in some cases. Furthermore, using our recipe, we analyze \"approximate\" versions of several algorithms such as follow-the-regularized-leader (FTRL) that requires solving an optimization problem at each step."}}
{"id": "-EpuoKY57AE", "cdate": 1325376000000, "mdate": null, "content": {"title": "Learning evolving and emerging topics in social media: a dynamic nmf approach with temporal regularization.", "abstract": "As massive repositories of real-time human commentary, social media platforms have arguably evolved far beyond passive facilitation of online social interactions. Rapid analysis of information content in online social media streams (news articles, blogs,tweets etc.) is the need of the hour as it allows business and government bodies to understand public opinion about products and policies. In most of these settings, data points appear as a stream of high dimensional feature vectors. Guided by real-world industrial deployment scenarios, we revisit the problem of online learning of topics from streaming social media content. On one hand, the topics need to be dynamically adapted to the statistics of incoming datapoints, and on the other hand, early detection of rising new trends is important in many applications. We propose an online nonnegative matrix factorizations framework to capture the evolution and emergence of themes in unstructured text under a novel temporal regularization framework. We develop scalable optimization algorithms for our framework, propose a new set of evaluation metrics, and report promising empirical results on traditional TDT tasks as well as streaming Twitter data. Our system is able to rapidly capture emerging themes, track existing topics over time while maintaining temporal consistency and continuity in user views, and can be explicitly configured to bound the amount of information being presented to the user."}}
