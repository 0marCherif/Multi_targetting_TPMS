{"id": "wNCOHjUaIVJ", "cdate": 1684671117356, "mdate": 1684671117356, "content": {"title": "Human Uncertainty in Concept-Based AI Systems", "abstract": "Placing a human in the loop may abate the risks of deploying AI systems in safety-critical settings (e.g., a clinician working with a medical AI system). However, mitigating risks arising from human error and uncertainty within such human-AI interactions is an important and understudied issue. In this work, we study human uncertainty in the context of concept-based models, a family of AI systems that enable human feedback via concept interventions where an expert intervenes on human-interpretable concepts relevant to the task. Prior work in this space often assumes that humans are oracles who are always certain and correct. Yet, real-world decision-making by humans is prone to occasional mistakes and uncertainty. We study how existing concept-based models deal with uncertain interventions from humans using two novel datasets: UMNIST, a visual dataset with controlled simulated uncertainty based on the MNIST dataset, and CUB-S, a relabeling of the popular CUB concept dataset with rich, densely-annotated soft labels from humans. We show that training with uncertain concept labels may help mitigate weaknesses of concept-based systems when handling uncertain interventions. These results allow us to identify several open challenges, which we argue can be tackled through future multidisciplinary research on building interactive uncertainty-aware systems. To facilitate further research, we release a new elicitation platform, UElic, to collect uncertain feedback from humans in collaborative prediction tasks."}}
{"id": "tUmgFiBvZs", "cdate": 1609459200000, "mdate": 1681151581487, "content": {"title": "Data-Driven Methods for Balancing Fairness and Efficiency in Ride-Pooling", "abstract": ""}}
{"id": "mNpKfEp8E69", "cdate": 1609459200000, "mdate": 1681151581487, "content": {"title": "Data-Driven Methods for Balancing Fairness and Efficiency in Ride-Pooling", "abstract": ""}}
{"id": "XSoA9JmOETS", "cdate": 1609459200000, "mdate": 1681151581448, "content": {"title": "Improving Learning-to-Defer Algorithms Through Fine-Tuning", "abstract": ""}}
{"id": "W90CyZLnAE", "cdate": 1609459200000, "mdate": 1681151581460, "content": {"title": "Investigating Methods of Balancing Inequality and Efficiency in Ride Pooling", "abstract": ""}}
{"id": "v41L3VKF07O", "cdate": 1577836800000, "mdate": 1631921397201, "content": {"title": "Stress and burnout in open source: toward finding, understanding, and mitigating unhealthy interactions", "abstract": "Developers from open-source communities have reported high stress levels from frequent demands for features and bug fixes and from the sometimes aggressive tone of these demands. Toxic conversations may demotivate and burn out developers, creating challenges for sustaining open source. We outline a path toward finding, understanding, and possibly mitigating such unhealthy interactions. We take a first step toward finding them, by developing and demonstrating a measurement instrument (an SVM classifier tailored for software engineering) to detect toxic discussions in GitHub issues. We used our classifier to analyze trends over time and in different GitHub communities, finding that toxicity varies by community and that toxicity decreased between 2012 and 2018."}}
{"id": "uvYMBie-qm", "cdate": 1514764800000, "mdate": 1681151581525, "content": {"title": "A Muffin-Theorem Generator", "abstract": ""}}
