{"id": "c09lxGznVa", "cdate": 1675827739057, "mdate": null, "content": {"title": "Text-to-Image Diffusion Models are Zero-Shot Classifiers", "abstract": "The excellent generative capabilities of text-to-image diffusion models suggest they learn informative representations of image-text data.\nHowever, what knowledge their representations capture is not fully understood, and they have not been thoroughly explored on downstream tasks.\nWe investigate diffusion models by proposing a method for evaluating them as zero-shot classifiers.\nThe key idea is using a diffusion model's ability to denoise a noised image given a text description of a label as a proxy for that label's likelihood.\nWe apply our method to Imagen, using it to probe fine-grained aspects of Imagen's knowledge and comparing it with CLIP's zero-shot abilities. \nImagen performs competitively with CLIP on a wide range of zero-shot image classification datasets. \nAdditionally, it achieves state-of-the-art results on shape/texture bias tests and can successfully perform attribute binding while CLIP cannot.\nAlthough generative pre-training is prevalent in NLP, visual foundation models often use other methods such as contrastive learning. \nBased on our findings, we argue that generative pre-training should be explored as a compelling alternative for vision and vision-language problems."}}
{"id": "laWYA-LXlNb", "cdate": 1675715129237, "mdate": null, "content": {"title": "Text-to-Image Diffusion Models are Zero-Shot Classifiers", "abstract": "Text-to-image diffusion models have demonstrated remarkable generative capabilities, suggesting they learn informative representations of image-text data. \nHowever, their abilities are not fully understood and they have not been thoroughly explored on downstream tasks.\nWe investigate diffusion models by proposing a method for evaluating them as zero-shot classifiers.\nThe key idea is using a diffusion model's ability to denoise a noised image given a textual description of a label as a proxy for that label's likelihood.\nWe apply our method to Imagen, using it to probe fine-grain aspects of Imagen's knowledge and comparing it with CLIP's zero-shot abilities.  \nImagen performs competitively with CLIP on a wide range of zero-shot image classification datasets. Additionally, it is more robust than CLIP and can successfully perform attribute binding while CLIP does not. \nAlthough generative pre-training is common in NLP, visual foundation models often use other methods such as contrastive learning. \nBased on our findings, we argue that generative pre-training should be explored as a compelling alternative for visual and vision-language problems."}}
{"id": "TnIZfXSFJAh", "cdate": 1663849957622, "mdate": null, "content": {"title": "PIPS: Path Integral Stochastic Optimal Control for Path Sampling in Molecular Dynamics", "abstract": "We consider the problem of Sampling Transition Paths: Given two metastable conformational states of a molecular system, \\eg\\ a folded and unfolded protein, we aim to sample the most likely transition path between the two states. Sampling such a transition path is computationally expensive due to the existence of high free energy barriers between the two states. To circumvent this, previous work has focused on simplifying the trajectories to occur along specific molecular descriptors called Collective Variables (CVs). However, finding CVs is non trivial and requires chemical intuition. For larger molecules, where intuition is not sufficient, using these CV-based methods biases the transition along possibly irrelevant dimensions. In this work, we propose a method for sampling transition paths that considers the entire geometry of the molecules. We achieve this by relating the problem to recent works on the Schr\\\"odinger bridge problem and stochastic optimal control. Using this relation, we construct a path integral method that incorporates important characteristics of molecular systems such as second-order dynamics and invariance to rotations and translations. We demonstrate our method on commonly studied protein structures like Alanine Dipeptide, and also consider larger proteins such as Polyproline and Chignolin."}}
{"id": "AUiOd6F53NE", "cdate": 1653100933875, "mdate": null, "content": {"title": "Path Integral Stochastic Optimal Control for Sampling Transition Paths", "abstract": "We consider the problem of Sampling Transition Paths. Given two metastable conformational states of a molecular system, \\eg\\ a folded and unfolded protein, we aim to sample the most likely transition path between the two states. Sampling such a transition path is computationally expensive due to the existence of high free energy barriers between the two states. To circumvent this, previous work has focused on simplifying the trajectories to occur along specific molecular descriptors called Collective Variables (CVs). However, finding CVs is not trivial and requires chemical intuition. For larger molecules, where intuition is not sufficient, using these CV-based methods biases the transition along possibly irrelevant dimensions. Instead, this work proposes a method for sampling transition paths that consider the entire geometry of the molecules. To achieve this, we first relate the problem to recent work on the Schrodinger bridge problem and stochastic optimal control. Using this relation, we construct a method that takes into account important characteristics of molecular systems such as second-order dynamics and invariance to rotations and translations. We demonstrate our method on the commonly studied Alanine Dipeptide, but also consider larger proteins such as Polyproline and Chignolin. \n"}}
{"id": "kOk5iXi2Qr", "cdate": 1634503077993, "mdate": null, "content": {"title": "Particle Dynamics for Learning EBMs", "abstract": "Energy-based modeling is a promising approach to unsupervised learning, which yields many downstream applications from a single model. The main difficulty in learning energy-based models with the \"contrastive approaches\" is the generation of samples from the current energy function at each iteration. Many advances have been made to accomplish this subroutine cheaply. Nevertheless, all such sampling paradigms run MCMC targeting the current model, which requires infinitely long chains to generate samples from the true energy distribution and is problematic in practice. This paper proposes an alternative approach to getting these samples and avoiding crude MCMC sampling from the current model. We accomplish this by viewing the evolution of the modeling distribution as (i) the evolution of the energy function, and (ii) the evolution of the samples from this distribution along some vector field. We subsequently derive this time-dependent vector field such that the particles following this field are approximately distributed as the current density model. Thereby we match the evolution of the particles with the evolution of the energy function prescribed by the learning procedure. Importantly, unlike Monte Carlo sampling, our method targets to match the current distribution in a finite time. Finally, we demonstrate its effectiveness empirically comparing to MCMC-based learning methods."}}
{"id": "syu7m80S_CA", "cdate": 1621629852946, "mdate": null, "content": {"title": "Learning Equivariant Energy Based Models with Equivariant Stein Variational Gradient Descent", "abstract": "We focus on the problem of efficient sampling and learning of probability densities by incorporating symmetries in probabilistic models. We first introduce Equivariant Stein Variational Gradient Descent algorithm -- an equivariant sampling method based on Stein's identity for sampling from densities with symmetries. Equivariant SVGD explicitly incorporates symmetry information in a density through equivariant kernels which makes the resultant sampler efficient both in terms of sample complexity and the quality of generated samples. Subsequently, we define equivariant energy based models to model invariant densities that are learned using contrastive divergence. By utilizing our equivariant SVGD for training equivariant EBMs, we propose new ways of improving and scaling up training of energy based models. We apply these equivariant energy models for modelling joint densities in regression and classification tasks for image datasets, many-body particle systems and molecular structure generation.  "}}
{"id": "6nbpPqUCIi7", "cdate": 1621629707775, "mdate": null, "content": {"title": "Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions", "abstract": "Generative flows and diffusion models have been predominantly trained on ordinal data, for example natural images. This paper introduces two extensions of flows and diffusion for categorical data such as language or image segmentation: Argmax Flows and Multinomial Diffusion. Argmax Flows are defined by a composition of a continuous distribution (such as a normalizing flow), and an argmax function. To optimize this model, we learn a probabilistic inverse for the argmax that lifts the categorical data to a continuous space. Multinomial Diffusion gradually adds categorical noise in a diffusion process, for which the generative denoising process is learned. We demonstrate that our method outperforms existing dequantization approaches on text modelling and modelling on image segmentation maps in log-likelihood. "}}
{"id": "JjRETuTSLUy", "cdate": 1609459200000, "mdate": null, "content": {"title": "Argmax Flows and Multinomial Diffusion: Towards Non-Autoregressive Language Models", "abstract": "Generative flows and diffusion models have been predominantly trained on ordinal data, for example natural images. This paper introduces two extensions of flows and diffusion for categorical data such as language or image segmentation: Argmax Flows and Multinomial Diffusion. Argmax Flows are defined by a composition of a continuous distribution (such as a normalizing flow), and an argmax function. To optimize this model, we learn a probabilistic inverse for the argmax that lifts the categorical data to a continuous space. Multinomial Diffusion gradually adds categorical noise in a diffusion process, for which the generative denoising process is learned. We demonstrate that our method outperforms existing dequantization approaches on text modelling and modelling on image segmentation maps in log-likelihood."}}
{"id": "DrUXbvu-b17", "cdate": 1609459200000, "mdate": null, "content": {"title": "Sampling in Combinatorial Spaces with SurVAE Flow Augmented MCMC", "abstract": "Hybrid Monte Carlo is a powerful Markov Chain Monte Carlo method for sampling from complex continuous distributions. However, a major limitation of HMC is its inability to be applied to discrete domains due to the lack of gradient signal. In this work, we introduce a new approach based on augmenting Monte Carlo methods with SurVAE Flows to sample from discrete distributions using a combination of neural transport methods like normalizing flows and variational dequantization, and the Metropolis-Hastings rule. Our method first learns a continuous embedding of the discrete space using a surjective map and subsequently learns a bijective transformation from the continuous space to an approximately Gaussian distributed latent variable. Sampling proceeds by simulating MCMC chains in the latent space and mapping these samples to the target discrete space via the learned transformations. We demonstrate the efficacy of our algorithm on a range of examples from statistics, computational physics and machine learning, and observe improvements compared to alternative algorithms."}}
{"id": "fdsXhAy5Cp", "cdate": 1606146129735, "mdate": null, "content": {"title": "Argmax Flows: Learning Categorical Distributions with Normalizing Flows", "abstract": "This paper introduces a new method to define and train continuous distributions such as normalizing flows directly on categorical data, for example text and image segmentation. The generative model is defined by a composition of a normalizing flow and an argmax function. To optimize this model, we dequantize the argmax using a distribution that is a probabilistic right-inverse to the argmax. This distribution lifts the categorical data to a continuous space on which the flow can be trained. We demonstrate that applying existing dequantization techniques na\u00efvely to categorical data leads to suboptimal solutions. In addition, the model is fast both in generative (for sampling) and inference direction (for training), as opposed to autoregressive models."}}
