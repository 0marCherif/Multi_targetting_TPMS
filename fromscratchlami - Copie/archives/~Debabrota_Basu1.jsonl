{"id": "dns3wNFt49m", "cdate": 1686250301151, "mdate": null, "content": {"title": "Pure Exploration in Bandits with Linear Constraints", "abstract": "We address the problem of identifying the optimal policy with a fixed confidence level in a multi-armed bandit setup, when \\emph{the arms are subject to linear constraints}. Unlike the standard best-arm identification problem which is well studied, the optimal policy in this case may not be deterministic and could mix between several arms. This changes the geometry of the problem which we characterize via an information-theoretic lower bound. We introduce two asymptotically optimal algorithms for this setting, one based on the Track-and-Stop method and the other based on a game-theoretic approach. Both these algorithms try to track an optimal allocation based on the lower bound and computed by a weighted projection onto the boundary of a normal cone. Finally, we provide empirical results that validate our bounds and visualize how constraints change the hardness of the problem."}}
{"id": "fV8xtET1zUo", "cdate": 1685982300470, "mdate": null, "content": {"title": "On the Complexity of Differentially Private Best-Arm Identification with Fixed Confidence", "abstract": "Best Arm Identification (BAI) problems are progressively used for data-sensitive applications, such as designing adaptive clinical trials, tuning hyper-parameters, and conducting user studies to name a few. Motivated by the data privacy concerns invoked by these applications, we study the problem of BAI with fixed confidence under $\\epsilon$-global Differential Privacy (DP). First, to quantify the cost of privacy, we derive a lower bound on the sample complexity of any $\\delta$-correct BAI algorithm satisfying $\\epsilon$-global DP. Our lower bound suggests the existence of two privacy regimes depending on the privacy budget $\\epsilon$. In the high-privacy regime (small $\\epsilon$), the hardness depends on a coupled effect of privacy and a novel information-theoretic quantity, called the Total Variation Characteristic Time. In the low-privacy regime (large $\\epsilon$), the sample complexity lower bound reduces to the classical non-private lower bound. Second, we propose AdaP-TT, an $\\epsilon$-global DP variant of the Top Two algorithm. AdaP-TT runs in arm-dependent adaptive episodes and adds Laplace noise to ensure a good privacy-utility trade-off. We derive an asymptotic upper bound on the sample complexity of AdaP-TT that matches with the lower bound up to multiplicative constants in the high-privacy regime. Finally, we provide an experimental analysis of AdaP-TT that validates our theoretical results."}}
{"id": "lqfTQzDRug", "cdate": 1685982300406, "mdate": null, "content": {"title": "Interactive and Concentrated Differential Privacy for Bandits", "abstract": "Bandits play a crucial role in interactive learning schemes and modern recommender systems. However, these systems often rely on sensitive user data, making privacy a critical concern. This paper investigates privacy in bandits with a trusted centralized decision-maker through the lens of interactive Differential Privacy (DP). While bandits under pure $\\epsilon$-global DP have been well-studied, we contribute to the understanding of bandits under zero Concentrated DP (zCDP). We provide minimax and problem-dependent lower bounds on regret for finite-armed and linear bandits, which quantify the cost of $\\rho$-global zCDP in these settings. These lower bounds reveal two hardness regimes based on the privacy budget $\\rho$ and suggest that $\\rho$-global zCDP incurs less regret than pure $\\epsilon$-global DP. We propose two $\\rho$-global zCDP bandit algorithms, AdaC-UCB and AdaC-GOPE, for finite-armed and linear bandits respectively. Both algorithms use a common recipe of Gaussian mechanism and adaptive episodes. We analyze the regret of these algorithms to show that AdaC-UCB achieves the problem-dependent regret lower bound up to multiplicative constants, while AdaC-GOPE achieves the minimax regret lower bound up to poly-logarithmic factors. Finally, we provide experimental validation of our theoretical results under different settings."}}
{"id": "Ev4NOEeTYL", "cdate": 1668480871630, "mdate": null, "content": {"title": "Farm-gym: A modular reinforcement learning platform for stochastic agronomic games", "abstract": "We introduce Farm-gym, an open-source farming environment written in Python, that models sequential decision-making in farms using Reinforcement Learning (RL). Farm-gym conceptualizes a farm as a dynamical system with many interacting entities. Leveraging a modular design, it enables us to instantiate from very simple to highly complicated environments. \nContrasting many available gym environments, Farm-gym features intrinsically stochastic games, using stochastic growth models and weather data. Further, it enables to create farm games in a modular way, activating or not the entities (e.g. weeds, pests, pollinators), and yielding non-trivial coupled dynamics. Finally, every game can be customized with .yaml files for rewards, feasible actions, and initial/end-game conditions. We illustrate some interesting features on simple farms. We also showcase the challenges posed by Farm-gym to the deep RL algorithms, in order to stimulate studies in the RL community."}}
{"id": "kocBczDfBeT", "cdate": 1663850455171, "mdate": null, "content": {"title": "Marich: A Query-efficient & Online Model Extraction Attack using Public Data", "abstract": "In this paper, we study black-box model stealing attacks where the attacker is only able to query a machine learning model through publicly available APIs. Specifically, our aim is to design a black-box model stealing attack that uses a minimal number of queries to create an informative replica of the target model. First, we reduce this problem to an online variational optimization problem. At every step, the attacker solves this problem to select the most informative query that maximizes the entropy of the selected queries and simultaneously reduces the mismatch between the target and the stolen models. We propose an online and adaptive algorithm, Marich, that leverages active learning to select the queries. We instantiate efficiency of our attack against different models, including logistic regression, BERT and ResNet18, trained on different text and image datasets. Marich is able to steal a model that can achieve 70-96$\\%$ of true model's accuracy using 0.8-10$\\%$ samples from the attack datasets which are publicly available and different from the training datasets. Our stolen models also achieve 75-98$\\%$ accuracy of membership inference and also show 70-90$\\%$ agreement of membership inference with direct membership inference on the target models. Our experiments validate that Marich is query-efficient and capable of creating an informative replica of the target model."}}
{"id": "TjVU5Lipt8F", "cdate": 1652737368282, "mdate": null, "content": {"title": "When Privacy Meets Partial Information: A Refined Analysis of Differentially Private Bandits", "abstract": "We study the problem of multi-armed bandits with \u03b5-global Differential Privacy (DP). First, we prove the minimax and problem-dependent regret lower bounds for stochastic and linear bandits that quantify the hardness of bandits with \u03b5-global DP. These bounds suggest the existence of two hardness regimes depending on the privacy budget \u03b5. In the high-privacy regime (small \u03b5), the hardness depends on a coupled effect of privacy and partial information about the reward distributions. In the low-privacy regime (large \u03b5), bandits with \u03b5-global DP are not harder than the bandits without privacy. For stochastic bandits, we further propose a generic framework to design a near-optimal \u03b5 global DP extension of an index-based optimistic bandit algorithm. The framework consists of three ingredients: the Laplace mechanism, arm-dependent adaptive episodes, and usage of only the rewards collected in the last episode for computing private statistics. Specifically, we instantiate \u03b5-global DP extensions of UCB and KL-UCB algorithms, namely AdaP-UCB and AdaP-KLUCB. AdaP-KLUCB is the first algorithm that both satisfies \u03b5-global DP and yields a regret upper bound that matches the problem-dependent lower bound up to multiplicative constants."}}
{"id": "LT6-Mxgb3QB", "cdate": 1652737342799, "mdate": null, "content": {"title": "Bilinear Exponential Family of MDPs: Frequentist Regret Bound with Tractable Exploration $\\&$ Planning", "abstract": "We study the problem of episodic reinforcement learning in continuous state-action spaces with unknown rewards and transitions. Specifically, we consider the setting where the rewards and transitions are modeled using parametric bilinear exponential families. We propose an algorithm, $\\texttt{BEF-RLSVI}$, that a) uses penalized maximum likelihood estimators to learn the unknown parameters, b) injects a calibrated Gaussian noise in the parameter of rewards to ensure exploration, and c) leverages linearity of the exponential family with respect to an underlying RKHS to perform tractable planning. We further provide a frequentist regret analysis of $\\texttt{BEF-RLSVI}$ that yields an upper bound of $\\tilde{\\mathcal{O}}(\\sqrt{d^3H^3K})$, where $d$ is the dimension of the parameters, $H$ is the episode length, and $K$ is the number of episodes. Our analysis improves the existing bounds for the bilinear exponential family of MDPs by $\\sqrt{H}$ and removes the handcrafted clipping deployed in existing $\\texttt{RLSVI}$-type algorithms. Our regret bound is order-optimal with respect to $H$ and $K$."}}
{"id": "r0gxrLIoqgc", "cdate": 1646077517238, "mdate": null, "content": {"title": "SENTINEL: Taming Uncertainty with Ensemble based Distributional Reinforcement Learning", "abstract": "    In this paper, we consider risk-sensitive sequential decision-making in Reinforcement Learning (RL). \n    Our contributions are two-fold. First, we introduce a novel and \\emph{coherent} quantification of risk, namely \\emph{composite risk}, which quantifies the joint effect of aleatory and epistemic risk during the learning process.\n    Existing works considered either aleatory or epistemic risk individually, or as an additive combination.\n    We prove that the additive formulation is a particular case of the composite risk when the epistemic risk measure is replaced with expectation.\n    Thus, the composite risk is more sensitive to both aleatory and epistemic uncertainty than the individual and additive formulations.\n    We also propose an algorithm, SENTINEL-K, based on ensemble bootstrapping and distributional RL for representing epistemic and aleatory uncertainty respectively. The ensemble of K learners uses Follow The Regularised Leader (FTRL) to aggregate the return distributions and obtain the composite risk.\n    We experimentally verify that SENTINEL-K estimates the return distribution better, and while used with composite risk estimates, demonstrates higher risk-sensitive performance than state-of-the-art risk-sensitive and distributional RL algorithms."}}
{"id": "Zu1Ev5CxFVD", "cdate": 1640995200000, "mdate": 1651070351544, "content": {"title": "SAAC: Safe Reinforcement Learning as an Adversarial Game of Actor-Critics", "abstract": "Although Reinforcement Learning (RL) is effective for sequential decision-making problems under uncertainty, it still fails to thrive in real-world systems where risk or safety is a binding constraint. In this paper, we formulate the RL problem with safety constraints as a non-zero-sum game. While deployed with maximum entropy RL, this formulation leads to a safe adversarially guided soft actor-critic framework, called SAAC. In SAAC, the adversary aims to break the safety constraint while the RL agent aims to maximize the constrained value function given the adversary's policy. The safety constraint on the agent's value function manifests only as a repulsion term between the agent's and the adversary's policies. Unlike previous approaches, SAAC can address different safety criteria such as safe exploration, mean-variance risk sensitivity, and CVaR-like coherent risk sensitivity. We illustrate the design of the adversary for these constraints. Then, in each of these variations, we show the agent differentiates itself from the adversary's unsafe actions in addition to learning to solve the task. Finally, for challenging continuous control tasks, we demonstrate that SAAC achieves faster convergence, better efficiency, and fewer failures to satisfy the safety constraints than risk-averse distributional RL and risk-neutral soft actor-critic algorithms."}}
{"id": "SV07942wyw6", "cdate": 1640995200000, "mdate": 1651070351544, "content": {"title": "Bandits Corrupted by Nature: Lower Bounds on Regret and Robust Optimistic Algorithm", "abstract": "In this paper, we study the stochastic bandits problem with $k$ unknown heavy-tailed and corrupted reward distributions or arms with time-invariant corruption distributions. At each iteration, the player chooses an arm. Given the arm, the environment returns an uncorrupted reward with probability $1-\\varepsilon$ and an arbitrarily corrupted reward with probability $\\varepsilon$. In our setting, the uncorrupted reward might be heavy-tailed and the corrupted reward might be unbounded. We prove a lower bound on the regret indicating that the corrupted and heavy-tailed bandits are strictly harder than uncorrupted or light-tailed bandits. We observe that the environments can be categorised into hardness regimes depending on the suboptimality gap $\\Delta$, variance $\\sigma$, and corruption proportion $\\epsilon$. Following this, we design a UCB-type algorithm, namely HuberUCB, that leverages Huber's estimator for robust mean estimation. HuberUCB leads to tight upper bounds on regret in the proposed corrupted and heavy-tailed setting. To derive the upper bound, we prove a novel concentration inequality for Huber's estimator, which might be of independent interest."}}
