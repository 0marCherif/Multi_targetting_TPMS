{"id": "rZEM7ULs5x5", "cdate": 1646077515131, "mdate": null, "content": {"title": "Data augmentation in Bayesian neural networks and the cold posterior effect", "abstract": "Bayesian neural networks that incorporate data augmentation implicitly use a \"randomly perturbed log-likelihood [which] does not have a clean interpretation as a valid likelihood function\" (Izmailov et al. 2021). Here, we provide several approaches to developing principled Bayesian neural networks incorporating data augmentation. We introduce a \"finite orbit\" setting which allows valid likelihoods to be computed exactly, and for the more usual \"full orbit\" setting we derive multi-sample bounds tighter than those used previously for Bayesian neural networks with data augmentation. These models cast light on the origin of the cold posterior effect. In particular, we find that the cold posterior effect persists even in these principled models incorporating data augmentation. This suggests that the cold posterior effect cannot be dismissed as an artifact of data augmentation using incorrect likelihoods."}}
{"id": "xkjqJYqRJy", "cdate": 1632875650081, "mdate": null, "content": {"title": "Bayesian Neural Network Priors Revisited", "abstract": "Isotropic Gaussian priors are the de facto standard for modern Bayesian neural network inference. However, it is unclear whether these priors accurately reflect our true beliefs about the weight distributions or give optimal performance. To find better priors, we study summary statistics of neural network weights in networks trained using stochastic gradient descent (SGD). We find that convolutional neural network (CNN) and ResNet weights display strong spatial correlations, while fully connected networks (FCNNs) display heavy-tailed weight distributions. We show that building these observations into priors can lead to improved performance on a variety of image classification datasets. Surprisingly, these priors mitigate the cold posterior effect in FCNNs, but slightly increase the cold posterior effect in ResNets."}}
{"id": "I860dGFud1b", "cdate": 1606146135951, "mdate": null, "content": {"title": "Correlated Weights in Infinite Limits of Deep Convolutional Neural Networks", "abstract": "Infinite width limits of deep neural networks often have tractable forms. They have been used to analyse the behaviour of finite networks, as well as being useful methods in their own right. Currently used limits of deep convolutional networks lose correlating contributions from different spatial locations in the image, unlike their finite counterparts, even when those only use convolutions. We argue that this is undesirable, and remedy it by introducing spatial correlations in the prior over weights. This leads to correlated contributions being preserved in the wide limit. Varying the amount of correlation in the convolution weights allows interpolation between independent-weight limits and mean pooling (which is equivalent to complete correlation in the weights). Empirical evaluation of the infinitely wide network shows that optimal performance is achieved between the extremes, indicating the usefulness of considering correlations in the weights.\n"}}
{"id": "Rprd8aVUYkE", "cdate": 1606146134276, "mdate": null, "content": {"title": "Exact Langevin Dynamics with Stochastic Gradients", "abstract": "Stochastic gradient Markov Chain Monte Carlo algorithms are popular samplers for approximate inference, but they are generally biased. We show that many recent versions of these methods (e.g. Chen et al. (2014)) cannot be corrected using Metropolis-Hastings rejection sampling, because their acceptance probability is always zero. We can fix this by employing a sampler with realizable backwards trajectories, such as Gradient-Guided Monte Carlo (Horowitz, 1991), which generalizes stochastic gradient Langevin dynamics (Welling and Teh, 2011) and Hamiltonian Monte Carlo. We show that this sampler can be used with stochastic gradients, yielding nonzero acceptance probabilities which can be computed even across multiple steps."}}
{"id": "7P9y3sRa5Mk", "cdate": 1606146131957, "mdate": null, "content": {"title": "Understanding Variational Inference in Function-Space", "abstract": "Recent work has attempted to directly approximate the 'function-space' or predictive posterior distribution of Bayesian models, without approximating the posterior distribution over the parameters. This is appealing in e.g. Bayesian neural networks, where we only need the former, and the latter is hard to represent. In this work, we highlight some advantages and limitations of employing the Kullback-Leibler divergence in this setting. For example, we show that minimizing the KL divergence between a wide class of parametric distributions and the posterior induced by a (non-degenerate) Gaussian process prior leads to an ill-defined objective function. Then, we propose (featurized) Bayesian linear regression as a benchmark for 'function-space' inference methods that directly measures approximation quality. We apply this methodology to assess aspects of the objective function and inference scheme considered in Sun et al. (2018), emphasizing the quality of approximation to Bayesian inference as opposed to predictive performance."}}
{"id": "xaqKWHcoOGP", "cdate": 1606146131784, "mdate": null, "content": {"title": "Bayesian Neural Network Priors Revisited", "abstract": "Isotropic Gaussian priors are the de facto standard for modern Bayesian neural network inference.\nHowever, such simplistic priors are unlikely to either accurately reflect our true beliefs about the weight distributions, or to give optimal performance.\nWe study summary statistics of (convolutional) neural network weights in networks trained using SGD.\nWe find that in certain circumstances, these networks have heavy-tailed weight distributions, while convolutional neural network weights often display strong spatial correlations.\nBuilding these observations into the respective priors, we get improved performance on MNIST classification.   \nRemarkably, we find that using a more accurate prior partially mitigates the cold posterior effect, by improving performance at high temperatures corresponding to exact Bayesian inference, while having less of an effect at small temperatures."}}
{"id": "KWF4Slxui0s", "cdate": 1603119168241, "mdate": null, "content": {"title": "Bayesian Neural Network Priors Revisited", "abstract": "Isotropic Gaussian priors are the de facto standard for modern Bayesian neural network inference. However, such simplistic priors are unlikely to either accurately reflect our true beliefs about the weight distributions, or to give optimal performance. We study summary statistics of (convolutional) neural network weights in networks trained using SGD. We find that in certain circumstances, these networks have heavy-tailed weight distributions, while convolutional neural network weights often display strong spatial correlations. Building these observations into the respective priors, we get improved performance on MNIST classification. Remarkably, we find that using a more accurate prior partially mitigates the cold posterior effect, by improving performance at high temperatures corresponding to exact Bayesian inference, while having less of an effect at small temperatures."}}
{"id": "Bklfsi0cKm", "cdate": 1538087833655, "mdate": null, "content": {"title": "Deep Convolutional Networks as shallow Gaussian Processes", "abstract": "We show that the output of a (residual) CNN with an appropriate prior over the weights and biases is a GP in the limit of infinitely many convolutional filters, extending similar results for dense networks. For a CNN, the equivalent kernel can be computed exactly and, unlike \"deep kernels\", has very few parameters: only the hyperparameters of the original CNN. Further, we show that this kernel has two properties that allow it to be computed efficiently; the cost of evaluating the kernel for a pair of images is similar to a single forward pass through the original CNN with only one filter per layer. The kernel equivalent to a 32-layer ResNet obtains 0.84% classification error on MNIST, a new record for GP with a comparable number of parameters."}}
