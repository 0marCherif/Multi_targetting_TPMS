{"id": "IajGRJuM7D3", "cdate": 1663849854676, "mdate": null, "content": {"title": "Stable, Efficient, and Flexible Monotone Operator Implicit Graph Neural Networks", "abstract": "Implicit graph neural networks (IGNNs) that solve a fixed-point equilibrium equation for representation learning can learn the long-range dependencies (LRD) in the underlying graphs and show remarkable performance for various graph learning tasks. However, the expressivity of IGNNs is limited by the constraints for their well-posedness guarantee. Moreover, when IGNNs become effective for learning LRD, their eigenvalues converge to the value that slows down the convergence, and their performance is unstable across different tasks. In this paper, we provide a new well-posedness condition of IGNNs leveraging monotone operator theory. The new well-posedness characterization informs us to design effective parameterizations to improve the accuracy, efficiency, and stability of IGNNs. Leveraging accelerated operator splitting schemes and graph diffusion convolution, we design efficient and flexible implementations of monotone operator IGNNs that are significantly faster and more accurate than existing IGNNs."}}
{"id": "GCNIm4cKoRx", "cdate": 1652737360268, "mdate": null, "content": {"title": "Finite-Time Analysis of Adaptive Temporal Difference Learning with Deep Neural Networks", "abstract": "Temporal difference (TD) learning with function approximations (linear functions or neural networks) has achieved remarkable empirical success, giving impetus to the development of finite-time analysis. As an accelerated version of TD, the adaptive TD has been proposed and proved to enjoy finite-time convergence under the linear function approximation. Existing numerical results have demonstrated the superiority of adaptive algorithms to vanilla ones. Nevertheless, the performance guarantee of adaptive TD with neural network approximation remains widely unknown. This paper establishes the finite-time analysis for the adaptive TD with multi-layer ReLU network approximation whose samples are generated from a Markov decision process. Our established theory shows that if the width of the deep neural network is large enough, the adaptive TD using neural network approximation can find the (optimal) value function with high probabilities under the same iteration complexity as TD in general cases. Furthermore, we show that the adaptive TD using neural network approximation, with the same width and searching area, can achieve theoretical acceleration when the stochastic semi-gradients decay fast."}}
{"id": "EMxu-dzvJk", "cdate": 1632875535689, "mdate": null, "content": {"title": "GRAND++: Graph Neural Diffusion with A Source Term", "abstract": "We propose GRAph Neural Diffusion with a source term (GRAND++) for graph deep learning with a limited number of labeled nodes, i.e., low-labeling rate. GRAND++ is a class of continuous-depth graph deep learning architectures whose theoretical underpinning is the diffusion process on graphs with a source term. The source term guarantees two interesting theoretical properties of GRAND++: (i) the representation of graph nodes, under the dynamics of GRAND++, will not converge to a constant vector over all nodes even as the time goes to infinity, which mitigates the over-smoothing issue of graph neural networks and enables graph learning in very deep architectures. (ii) GRAND++ can provide accurate classification even when the model is trained with a very limited number of labeled training data. We experimentally verify the above two advantages on various graph deep learning benchmark tasks, showing a significant improvement over many existing graph neural networks."}}
{"id": "YTKwvw7XI1", "cdate": 1621629931856, "mdate": null, "content": {"title": "FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention", "abstract": "We propose FMMformers, a class of efficient and flexible transformers inspired by the celebrated fast multipole method (FMM) for accelerating interacting particle simulation. FMM decomposes particle-particle interaction into near-field and far-field components and then performs direct and coarse-grained computation, respectively. Similarly, FMMformers decompose the attention into near-field and far-field attention, modeling the near-field attention by a banded matrix and the far-field attention by a low-rank matrix. Computing the attention matrix for FMMformers requires linear complexity in computational time and memory footprint with respect to the sequence length. In contrast, standard transformers suffer from quadratic complexity. We analyze and validate the advantage of FMMformers over the standard transformer on the Long Range Arena and language modeling benchmarks. FMMformers can even outperform the standard transformer in terms of accuracy by a significant margin. For instance, FMMformers achieve an average classification accuracy of $60.74\\%$ over the five Long Range Arena tasks, which is significantly better than the standard transformer's average accuracy of $58.70\\%$.\n"}}
{"id": "fYLfs9yrtMQ", "cdate": 1621629910942, "mdate": null, "content": {"title": "Heavy Ball Neural Ordinary Differential Equations", "abstract": "We propose heavy ball neural ordinary differential equations (HBNODEs), leveraging the continuous limit of the classical momentum accelerated gradient descent, to improve neural ODEs (NODEs) training and inference. HBNODEs have two properties that imply practical advantages over NODEs: (i) The adjoint state of an HBNODE also satisfies an HBNODE, accelerating both forward and backward ODE solvers, thus significantly reducing the number of function evaluations (NFEs) and improving the utility of the trained models. (ii) The spectrum of HBNODEs is well structured, enabling effective learning of long-term dependencies from complex sequential data. We verify the advantages of HBNODEs over NODEs on benchmark tasks, including image classification, learning complex dynamics, and sequential modeling. Our method requires remarkably fewer forward and backward NFEs, is more accurate, and learns long-term dependencies more effectively than the other ODE-based neural network models. Code is available at \\url{https://github.com/hedixia/HeavyBallNODE}."}}
{"id": "4xzY5yod28y", "cdate": 1601308106247, "mdate": null, "content": {"title": "Scheduled Restart Momentum for Accelerated Stochastic Gradient Descent", "abstract": "Stochastic gradient descent (SGD) algorithms, with constant momentum and its variants such as Adam, are the optimization methods of choice for training deep neural networks (DNNs). There is great interest in speeding up the convergence of these methods due to their high computational expense. Nesterov accelerated gradient (NAG) with a time-varying momentum, denoted as NAG below, improves the convergence rate of gradient descent (GD) for convex optimization using a specially designed momentum; however, it accumulates error when an inexact gradient is used (such as in SGD), slowing convergence at best and diverging at worst. In this paper, we propose scheduled restart SGD (SRSGD), a new NAG-style scheme for training DNNs. SRSGD replaces the constant momentum in SGD by the increasing momentum in NAG but stabilizes the iterations by resetting the momentum to zero according to a schedule. Using a variety of models and benchmarks for image classification, we demonstrate that, in training DNNs, SRSGD significantly improves convergence and generalization; for instance, in training ResNet-200 for ImageNet classification, SRSGD achieves an error rate of 20.93% vs. the benchmark of 22.13%. These improvements become more significant as the network grows deeper. Furthermore, on both CIFAR and ImageNet, SRSGD reaches similar or even better error rates with  significantly fewer training epochs compared to the SGD baseline."}}
{"id": "eNSpdJeR_J", "cdate": 1601308086321, "mdate": null, "content": {"title": "Deep Learning with Data Privacy via Residual Perturbation", "abstract": "Protecting data privacy in deep learning (DL) is at its urgency. Several celebrated privacy notions have been established and used for privacy-preserving DL. However, many of the existing mechanisms achieve data privacy at the cost of significant utility degradation. In this paper, we propose a stochastic differential equation principled \\emph{residual perturbation} for privacy-preserving DL, which injects Gaussian noise into each residual mapping of ResNets. Theoretically, we prove that residual perturbation guarantees differential privacy (DP) and reduces the generalization gap for DL. Empirically, we show that residual perturbation outperforms the state-of-the-art DP stochastic gradient descent (DPSGD) in both membership privacy protection and maintaining the DL models' utility. For instance, in the process of training ResNet8 for the IDC dataset classification, residual perturbation obtains an accuracy of 85.7\\% and protects the perfect membership privacy; in contrast, DPSGD achieves an accuracy of 82.8\\% and protects worse membership privacy. "}}
{"id": "cQyybLUoXxc", "cdate": 1601308039728, "mdate": null, "content": {"title": "Withdraw", "abstract": "we have withdrawn our paper."}}
{"id": "LjERk_-Rexd", "cdate": 1597189990139, "mdate": null, "content": {"title": "MomentumRNN: Integrating Momentum into Recurrent Neural Networks", "abstract": "Designing deep neural networks is an art that often involves an expensive search over candidate\narchitectures. To overcome this for recurrent neural nets (RNNs), we establish a connection between\nthe hidden state dynamics in an RNN and gradient descent (GD). We then integrate momentum into\nthis framework and propose a new family of RNNs, called MomentumRNNs. We theoretically prove\nand numerically demonstrate that MomentumRNNs alleviate the vanishing gradient issue in training\nRNNs. We study the momentum long-short term memory (MomentumLSTM) and verify its advantages\nin convergence speed and accuracy over its LSTM counterpart across a variety of benchmarks, with\nlittle compromise in computational or memory efficiency. We also demonstrate that MomentumRNN\nis applicable to many types of recurrent cells, including those in the state-of-the-art orthogonal RNNs.\nFinally, we show that other advanced momentum-based optimization methods, such as Adam and\nNesterov accelerated gradients with a restart, can be easily incorporated into the MomentumRNN\nframework for designing new recurrent cells with even better performance. The code is available at\nhttps://github.com/minhtannguyen/MomentumRNN."}}
{"id": "tFwJEUemXs9", "cdate": 1597189739102, "mdate": null, "content": {"title": "Scheduled Restart Momentum for Accelerated Stochastic Gradient Descent", "abstract": "Stochastic gradient descent (SGD) with constant momentum and its variants such as Adam are the\noptimization algorithms of choice for training deep neural networks (DNNs). Since DNN training is\nincredibly computationally expensive, there is great interest in speeding up the convergence. Nesterov\naccelerated gradient (NAG) improves the convergence rate of gradient descent (GD) for convex optimization\nusing a specially designed momentum; however, it accumulates error when an inexact gradient is used\n(such as in SGD), slowing convergence at best and diverging at worst. In this paper, we propose Scheduled\nRestart SGD (SRSGD), a new NAG-style scheme for training DNNs. SRSGD replaces the constant\nmomentum in SGD by the increasing momentum in NAG but stabilizes the iterations by resetting\nthe momentum to zero according to a schedule. Using a variety of models and benchmarks for image\nclassification, we demonstrate that, in training DNNs, SRSGD significantly improves convergence and\ngeneralization; for instance in training ResNet200 for ImageNet classification, SRSGD achieves an error\nrate of 20.93% vs. the benchmark of 22.13%. These improvements become more significant as the network\ngrows deeper. Furthermore, on both CIFAR and ImageNet, SRSGD reaches similar or even better error\nrates with significantly fewer training epochs compared to the SGD baseline."}}
