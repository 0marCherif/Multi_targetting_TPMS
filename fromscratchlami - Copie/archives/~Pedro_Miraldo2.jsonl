{"id": "6i5WGC1CrJ", "cdate": 1691538390547, "mdate": 1691538390547, "content": {"title": "Robust Frame-to-Frame Camera Rotation Estimation in Crowded Scenes", "abstract": "We present an approach to estimating camera rotation\nin crowded, real-world scenes from handheld monocular\nvideo. While camera rotation estimation is a well-studied\nproblem, no previous methods exhibit both high accuracy\nand acceptable speed in this setting. Because the setting\nis not addressed well by other datasets, we provide a new\ndataset and benchmark, with high-accuracy, rigorously verified\nground truth, on 17 video sequences. Methods developed\nfor wide baseline stereo (e.g., 5-point methods)\nperform poorly on monocular video. On the other hand,\nmethods used in autonomous driving (e.g., SLAM) leverage\nspecific sensor setups, specific motion models, or local\noptimization strategies (lagging batch processing) and\ndo not generalize well to handheld video. Finally, for dynamic\nscenes, commonly used robustification techniques\nlike RANSAC require large numbers of iterations, and become\nprohibitively slow. We introduce a novel generalization\nof the Hough transform on SO(3) to efficiently and robustly\nfind the camera rotation most compatible with optical\nflow. Among comparably fast methods, ours reduces error\nby almost 50% over the next best, and is more accurate than\nany method, irrespective of speed. This represents a strong\nnew performance point for crowded scenes, an important\nsetting for computer vision. Code for our method will be\nprovided as part of the supplementary material."}}
{"id": "_vqjUUICIUo", "cdate": 1623676103555, "mdate": null, "content": {"title": "3DRegNet: A Deep Neural Network for 3D Point Registration", "abstract": "We present 3DRegNet, a novel deep learning architecture for the registration of 3D scans. Given a set of 3D point correspondences, we build a deep neural network to address the following two challenges: (i) classification of the point correspondences into inliers/outliers, and (ii) regression of the motion parameters that align the scans into a common reference frame. With regard to regression, we present two alternative approaches: (i) a Deep Neural Network (DNN) registration and (ii) a Procrustes approach using SVD to estimate the transformation. Our correspondence-based approach achieves a higher speedup compared to competing baselines. We further propose the use of a refinement network, which consists of a smaller 3DRegNet as a refinement to improve the accuracy of the registration. Extensive experiments on two challenging datasets demonstrate that we outperform other methods and achieve state-of-the-art results. The code is available."}}
{"id": "BsoVmxmlu6H", "cdate": 1546300800000, "mdate": null, "content": {"title": "Minimal Solvers for Mini-Loop Closures in 3D Multi-Scan Alignment.", "abstract": "3D scan registration is a classical, yet a highly useful problem in the context of 3D sensors such as Kinect and Velodyne. While there are several existing methods, the techniques are usually incremental where adjacent scans are registered first to obtain the initial poses, followed by motion averaging and bundle-adjustment refinement. In this paper, we take a different approach and develop minimal solvers for jointly computing the initial poses of cameras in small loops such as 3-, 4-, and 5-cycles. Note that the classical registration of 2 scans can be done using a minimum of 3 point matches to compute 6 degrees of relative motion. On the other hand, to jointly compute the 3D registrations in n-cycles, we take 2 point matches between the first n-1 consecutive pairs (i.e., Scan 1 & Scan 2, ... , and Scan n-1 & Scan n) and 1 or 2 point matches between Scan 1 and Scan n. Overall, we use 5, 7, and 10 point matches for 3-, 4-, and 5-cycles, and recover 12, 18, and 24 degrees of transformation variables, respectively. Using simulations and real-data we show that the 3D registration using mini n-cycles are computationally efficient, and can provide alternate and better initial poses compared to standard pairwise methods."}}
{"id": "HyZPV0bO-H", "cdate": 1514764800000, "mdate": null, "content": {"title": "Analytical Modeling of Vanishing Points and Curves in Catadioptric Cameras", "abstract": "Vanishing points and vanishing lines are classical geometrical concepts in perspective cameras that have a lineage dating back to 3 centuries. A vanishing point is a point on the image space where parallel lines in 3D space appear to converge, whereas a vanishing line passes through 2 or more vanishing points. While such concepts are simple and intuitive in perspective cameras, their counterparts in catadioptric cameras (obtained using mirrors and lenses) are more involved. For example, lines in the 3D space map to higher degree curves in catadioptric cameras. The projection of a set of 3D parallel lines converges on a single point in perspective images, whereas they converge to more than one point in catadioptric cameras. To the best of our knowledge, we are not aware of any systematic development of analytical models for vanishing points and vanishing curves in different types of catadioptric cameras. In this paper, we derive parametric equations for vanishing points and vanishing curves using the calibration parameters, mirror shape coefficients, and direction vectors of parallel lines in 3D space. We show compelling experimental results on vanishing point estimation and absolute pose estimation for a wide variety of catadioptric cameras in both simulations and real experiments."}}
{"id": "BkNWFFWuZB", "cdate": 1514764800000, "mdate": null, "content": {"title": "A Minimal Closed-Form Solution for Multi-perspective Pose Estimation using Points and Lines", "abstract": "We propose a minimal solution for pose estimation using both points and lines for a multi-perspective camera. In this paper, we treat the multi-perspective camera as a collection of rigidly attached perspective cameras. These type of imaging devices are useful for several computer vision applications that require a large coverage such as surveillance, self-driving cars, and motion-capture studios. While prior methods have considered the cases using solely points or lines, the hybrid case involving both points and lines has not been solved for multi-perspective cameras. We present the solutions for two cases. In the first case, we are given 2D to 3D correspondences for two points and one line. In the later case, we are given 2D to 3D correspondences for one point and two lines. We show that the solution for the case of two points and one line can be formulated as a fourth degree equation. This is interesting because we can get a closed-form solution and thereby achieve high computational efficiency. The later case involving two lines and one point can be mapped to an eighth degree equation. We show simulations and real experiments to demonstrate the advantages and benefits over existing methods."}}
{"id": "rjsPUiBlu6r", "cdate": 1356998400000, "mdate": null, "content": {"title": "Calibration of Smooth Camera Models.", "abstract": "Generic imaging models can be used to represent any camera. Current generic models are discrete and define a mapping between each pixel in the image and a straight line in 3D space. This paper presents a modification of the generic camera model that allows the simplification of the calibration procedure. The only requirement is that the coordinates of the 3D projecting lines are related by functions that vary smoothly across space. Such a model is obtained by modifying the general imaging model using radial basis functions (RBFs) to interpolate image coordinates and 3D lines, thereby allowing both an increase in resolution (due to their continuous nature) and a more compact representation. Using this variation of the general imaging model, we also develop a calibration procedure. This procedure only requires that a 3D point be matched to each pixel. In addition, not all the pixels need to be calibrated. As a result, the complexity of the procedure is significantly decreased. Normalization is applied to the coordinates of both image and 3D points, which increases the accuracy of the calibration. Results with both synthetic and real datasets show that the model and calibration procedure are easily applicable and provide accurate calibration results."}}
{"id": "BJZ6tZf_-S", "cdate": 1293840000000, "mdate": null, "content": {"title": "Point-based calibration using a parametric representation of the general imaging model", "abstract": "Generic imaging models can be used to represent any camera. These models are specially suited for non-central cameras for which closed-form models do not exist. Current models are discrete and define a mapping between each pixel in the image and a straight line in 3D space. Due to difficulties in the calibration procedure and model complexity these methods have not been used in practice. The focus of our work was to relax these drawbacks. In this paper we modify the general imaging model using radial basis functions to interpolate image coordinates and 3D lines allowing both an increase in resolution (due to their continuous nature) and a more compact representation. Using this new variation of the general imaging model we also develop a new linear calibration procedure. In this process it is only required to match one 3D point to each image pixel. Also it is not required the calibration of every image pixel. As a result the complexity of the procedure is significantly decreased."}}
