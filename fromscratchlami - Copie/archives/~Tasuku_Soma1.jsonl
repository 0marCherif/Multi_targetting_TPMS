{"id": "ulOk2lbWAzp", "cdate": 1672531200000, "mdate": 1680680807128, "content": {"title": "Algebraic Algorithms for Fractional Linear Matroid Parity via Non-commutative Rank", "abstract": ""}}
{"id": "q28zzmC9IN", "cdate": 1672531200000, "mdate": 1680680807131, "content": {"title": "Shrunk subspaces via operator Sinkhorn iteration", "abstract": ""}}
{"id": "cFF2WOjkjJC", "cdate": 1672531200000, "mdate": 1680680807127, "content": {"title": "Online risk-averse submodular maximization", "abstract": ""}}
{"id": "Ms6QZafNv01", "cdate": 1652737424446, "mdate": null, "content": {"title": "Optimal algorithms for group distributionally robust optimization and beyond", "abstract": "Distributionally robust optimization (DRO) can improve the robustness and fairness of learning methods. In this paper, we devise stochastic algorithms for a class of DRO problems including group DRO, subpopulation fairness, and empirical conditional value at risk (CVaR) optimization. Our new algorithms achieve faster convergence rates than existing algorithms for multiple DRO settings. We also provide a new information-theoretic lower bound that implies our bounds are tight for group DRO. Empirically, too, our algorithms outperform known methods."}}
{"id": "WdI6GRpdju8", "cdate": 1609459200000, "mdate": 1648736356649, "content": {"title": "Polynomial-time algorithms for submodular Laplacian systems", "abstract": "Let G = ( V , E ) be an undirected graph, L G \u2208 R V \u00d7 V be the associated Laplacian matrix, and b \u2208 R V be a vector. Solving the Laplacian system L G x = b has numerous applications in theoretical computer science, machine learning, and network analysis. Recently, the notion of the Laplacian operator L F : R V \u2192 2 R V for a submodular transformation F : 2 V \u2192 R + E was introduced, which can handle undirected graphs, directed graphs, hypergraphs, and joint distributions in a unified manner. In this study, we show that the submodular Laplacian system L F ( x ) \u220b b can be solved in polynomial time. Furthermore, we prove that even when the submodular Laplacian system has no solution, we can solve its regression form in polynomial time. Finally, we discuss potential applications of submodular Laplacian systems in machine learning and network analysis."}}
{"id": "JfYg2BwYnl7", "cdate": 1609459200000, "mdate": 1648736356647, "content": {"title": "Online Risk-Averse Submodular Maximization", "abstract": "We present a polynomial-time online algorithm for maximizing the conditional value at risk (CVaR) of a monotone stochastic submodular function. Given T i.i.d. samples from an underlying distribution arriving online, our algorithm produces a sequence of solutions that converges to a (1\u22121/e)-approximate solution with a convergence rate of O(T \u22121/4 ) for monotone continuous DR-submodular functions. Compared with previous offline algorithms, which require \u2126(T) space, our online algorithm only requires O( \u221a T) space. We extend our on- line algorithm to portfolio optimization for mono- tone submodular set functions under a matroid constraint. Experiments conducted on real-world datasets demonstrate that our algorithm can rapidly achieve CVaRs that are comparable to those obtained by existing offline algorithms."}}
{"id": "bcJIOVVulrs", "cdate": 1577836800000, "mdate": null, "content": {"title": "Information geometry of operator scaling", "abstract": "Matrix scaling is a classical problem with a wide range of applications. It is known that the Sinkhorn algorithm for matrix scaling is interpreted as alternating e-projections from the viewpoint of classical information geometry. Recently, a generalization of matrix scaling to completely positive maps called operator scaling has been found to appear in various fields of mathematics and computer science, and the Sinkhorn algorithm has been extended to operator scaling. In this study, the operator Sinkhorn algorithm is studied from the viewpoint of quantum information geometry through the Choi representation of completely positive maps. The operator Sinkhorn algorithm is shown to coincide with alternating e-projections with respect to the symmetric logarithmic derivative metric, which is a Riemannian metric on the space of quantum states relevant to quantum estimation theory. Other types of alternating e-projections algorithms are also provided by using different information geometric structures on the positive definite cone."}}
{"id": "Y2DRWZdKc0W", "cdate": 1577836800000, "mdate": null, "content": {"title": "Statistical Learning with Conditional Value at Risk", "abstract": "We propose a risk-averse statistical learning framework wherein the performance of a learning algorithm is evaluated by the conditional value-at-risk (CVaR) of losses rather than the expected loss. We devise algorithms based on stochastic gradient descent for this framework. While existing studies of CVaR optimization require direct access to the underlying distribution, our algorithms make a weaker assumption that only i.i.d.\\ samples are given. For convex and Lipschitz loss functions, we show that our algorithm has $O(1/\\sqrt{n})$-convergence to the optimal CVaR, where $n$ is the number of samples. For nonconvex and smooth loss functions, we show a generalization bound on CVaR. By conducting numerical experiments on various machine learning tasks, we demonstrate that our algorithms effectively minimize CVaR compared with other baseline algorithms."}}
{"id": "J2rEiP81DOJ", "cdate": 1577836800000, "mdate": null, "content": {"title": "Improved Algorithms for Online Submodular Maximization via First-order Regret Bounds", "abstract": "We consider the problem of nonnegative submodular maximization in the online setting. At time step t, an algorithm selects a set S<em>t \u2208 C \u2286 2^V where C is a feasible family of sets. An adversary then reveals a submodular function f</em>t. The goal is to design an efficient algorithm for minimizing the expected approximate regret. In this work, we give a general approach for improving regret bounds in online submodular maximization by exploiting \u201cfirst-order\u201d regret bounds for online linear optimization. - For monotone submodular maximization subject to a matroid, we give an efficient algorithm which achieves a (1 \u2212 c/e \u2212 \u03b5)-regret of O(\u221akT ln(n/k)) where n is the size of the ground set, k is the rank of the matroid, \u03b5 &gt; 0 is a constant, and c is the average curvature. Even without assuming any curvature (i.e., taking c = 1), this regret bound improves on previous results of Streeter et al. (2009) and Golovin et al. (2014). - For nonmonotone, unconstrained submodular functions, we give an algorithm with 1/2-regret O(\u221a nT), improving on the results of Roughgarden and Wang (2018). Our approach is based on Blackwell approachability; in particular, we give a novel first-order regret bound for the Blackwell instances that arise in this setting"}}
{"id": "45DApnNHWmW", "cdate": 1577836800000, "mdate": null, "content": {"title": "Tight First- and Second-Order Regret Bounds for Adversarial Linear Bandits", "abstract": "We propose novel algorithms with first- and second-order regret bounds for adversarial linear bandits. These regret bounds imply that our algorithms perform well when there is an action achieving a small cumulative loss or the loss has a small variance. In addition, we need only assumptions weaker than those of existing algorithms; our algorithms work on discrete action sets as well as continuous ones without a priori knowledge about losses, and they run efficiently if a linear optimization oracle for the action set is available. These results are obtained by combining optimistic online optimization, continuous multiplicative weight update methods, and a novel technique that we refer to as distribution truncation. We also show that the regret bounds of our algorithms are tight up to polylogarithmic factors."}}
