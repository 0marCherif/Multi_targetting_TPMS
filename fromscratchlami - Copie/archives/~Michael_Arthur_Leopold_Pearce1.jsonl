{"id": "WDBWgcqgsz0", "cdate": 1640995200000, "mdate": 1682540970060, "content": {"title": "Efficient computation of the Knowledge Gradient for Bayesian Optimization", "abstract": "Bayesian optimization is a powerful collection of methods for optimizing stochastic expensive black box functions. One key component of a Bayesian optimization algorithm is the acquisition function that determines which solution should be evaluated in every iteration. A popular and very effective choice is the Knowledge Gradient acquisition function, however there is no analytical way to compute it. Several different implementations make different approximations. In this paper, we review and compare the spectrum of Knowledge Gradient implementations and propose One-shot Hybrid KG, a new approach that combines several of the previously proposed ideas and is cheap to compute as well as powerful and efficient. We prove the new method preserves theoretical properties of previous methods and empirically show the drastically reduced computational overhead with equal or improved performance. All experiments are implemented in BOTorch and code is available on github."}}
{"id": "3X_Tp7qVnd", "cdate": 1640995200000, "mdate": 1682540970060, "content": {"title": "Multi-objective path planning for environmental monitoring using an autonomous surface vehicle", "abstract": ""}}
{"id": "0gd5szDyWF", "cdate": 1640995200000, "mdate": 1682540970048, "content": {"title": "Bayesian Optimisation vs. Input Uncertainty Reduction", "abstract": ""}}
{"id": "T1STW1NYYKW", "cdate": 1621629863313, "mdate": null, "content": {"title": "Practical Bayesian Optimization of Objectives with Conditioning Variables", "abstract": "Bayesian optimization is a class of data efficient model based algorithms typically\nfocused on global optimization. We consider the more general case where a user is\nfaced with multiple problems that each need to be optimized conditional on a state\nvariable, for example given a range of cities with different patient distributions,\nwe optimize the ambulance locations conditioned on patient distribution. Given partitions of Cifar10,\nwe optimize CNN hyperparameters for each partition. Similarity\nacross objectives boosts optimization of each objective in two ways: in modelling by\ndata sharing across objectives, and also in acquisition by quantifying how a single\npoint on one objective can provide benefit to all objectives. For this we propose a framework\nfor conditional optimization: ConBO. This can be built on top of a range of acquisition functions\nand we propose a new Hybrid Knowledge Gradient acquisition function. The resulting method is intuitive and\ntheoretically grounded, performs either similar to or significantly better than recently published works\non a range of problems, and is easily parallelized to collect a batch of points."}}
{"id": "BeZdG9lwxc", "cdate": 1609459200000, "mdate": 1645836815777, "content": {"title": "Scalable Gaussian Process Variational Autoencoders", "abstract": "Conventional variational autoencoders fail in modeling correlations between data points due to their use of factorized priors. Amortized Gaussian process inference through GP-VAEs has led to significant improvements in this regard, but is still inhibited by the intrinsic complexity of exact GP inference. We improve the scalability of these methods through principled sparse inference approaches. We propose a new scalable GP-VAE model that outperforms existing approaches in terms of runtime and memory footprint, is easy to implement, and allows for joint end-to-end optimization of all components."}}
{"id": "nnjxngksr4-", "cdate": 1606146132126, "mdate": null, "content": {"title": "Factorized Gaussian Process Variational Autoencoders", "abstract": "Variational autoencoders often assume isotropic Gaussian priors and mean-field posteriors, hence do not exploit structure in scenarios where we may expect similarity or consistency across latent variables. Gaussian process variational autoencoders alleviate this problem through the use of a latent Gaussian process, but lead to a cubic inference time complexity. We propose a more scalable extension of these models by leveraging the independence of the auxiliary features, which is present in many datasets. Our model factorizes the latent kernel across these features in different dimensions, leading to a significant speed-up (in theory and practice), while empirically performing comparably to existing non-scalable approaches. Moreover, our approach allows for additional modeling of global latent information and for more general extrapolation to unseen input combinations."}}
{"id": "czv8Ac3Kg7l", "cdate": 1601308355756, "mdate": null, "content": {"title": "Sparse Gaussian Process Variational Autoencoders", "abstract": "Large, multi-dimensional spatio-temporal datasets are omnipresent in modern science and engineering. An effective framework for handling such data are Gaussian process deep generative models (GP-DGMs), which employ GP priors over the latent variables of DGMs. Existing approaches for performing inference in GP-DGMs do not support sparse GP approximations based on inducing points, which are essential for the computational efficiency of GPs, nor do they handle missing data -- a natural occurrence in many spatio-temporal datasets -- in a principled manner. We address these shortcomings with the development of the sparse Gaussian process variational autoencoder (SGP-VAE), characterised by the use of partial inference networks for parameterising sparse GP approximations. Leveraging the benefits of amortised variational inference, the SGP-VAE enables inference in multi-output sparse GPs on previously unobserved data with no additional training. The SGP-VAE is evaluated in a variety of experiments where it outperforms alternative approaches including multi-output GPs and structured VAEs."}}
{"id": "rRxWuf9gweq", "cdate": 1577836800000, "mdate": 1645836815799, "content": {"title": "Scalable Gaussian Process Variational Autoencoders", "abstract": "Conventional variational autoencoders fail in modeling correlations between data points due to their use of factorized priors. Amortized Gaussian process inference through GP-VAEs has led to significant improvements in this regard, but is still inhibited by the intrinsic complexity of exact GP inference. We improve the scalability of these methods through principled sparse inference approaches. We propose a new scalable GP-VAE model that outperforms existing approaches in terms of runtime and memory footprint, is easy to implement, and allows for joint end-to-end optimization of all components."}}
{"id": "rDI7daJXNH", "cdate": 1577836800000, "mdate": 1682540970194, "content": {"title": "ConBO: Conditional Bayesian Optimization", "abstract": "Bayesian optimization is a class of data efficient model based algorithms typically focused on global optimization. We consider the more general case where a user is faced with multiple problems that each need to be optimized conditional on a state variable, for example given a range of cities with different patient distributions, we optimize the ambulance locations conditioned on patient distribution. Given partitions of CIFAR-10, we optimize CNN hyperparameters for each partition. Similarity across objectives boosts optimization of each objective in two ways: in modelling by data sharing across objectives, and also in acquisition by quantifying how a single point on one objective can provide benefit to all objectives. For this we propose a framework for conditional optimization: ConBO. This can be built on top of a range of acquisition functions and we propose a new Hybrid Knowledge Gradient acquisition function. The resulting method is intuitive and theoretically grounded, performs either similar to or significantly better than recently published works on a range of problems, and is easily parallelized to collect a batch of points."}}
{"id": "fsFHpFioVP", "cdate": 1577836800000, "mdate": 1682540970194, "content": {"title": "Bayesian Optimisation vs. Input Uncertainty Reduction", "abstract": "Simulators often require calibration inputs estimated from real world data and the quality of the estimate can significantly affect simulation output. Particularly when performing simulation optimisation to find an optimal solution, the uncertainty in the inputs significantly affects the quality of the found solution. One remedy is to search for the solution that has the best performance on average over the uncertain range of inputs yielding an optimal compromise solution. We consider the more general setting where a user may choose between either running simulations or instead collecting real world data. A user may choose an input and a solution and observe the simulation output, or instead query an external data source improving the input estimate enabling the search for a more focused, less compromised solution. We explicitly examine the trade-off between simulation and real data collection in order to find the optimal solution of the simulator with the true inputs. Using a value of information procedure, we propose a novel unified simulation optimisation procedure called Bayesian Information Collection and Optimisation (BICO) that, in each iteration, automatically determines which of the two actions (running simulations or data collection) is more beneficial. Numerical experiments demonstrate that the proposed algorithm is able to automatically determine an appropriate balance between optimisation and data collection."}}
