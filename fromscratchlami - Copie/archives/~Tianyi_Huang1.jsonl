{"id": "qGyE8pDQNWY", "cdate": 1640995200000, "mdate": 1683884083598, "content": {"title": "WDIBS: Wasserstein deterministic information bottleneck for state abstraction to balance state-compression and performance", "abstract": "As an important branch of reinforcement learning, Apprenticeship learning studies how an agent learns good behavioral decisions by observing an expert policy from the environment. It has made many encouraging breakthroughs in real-world applications. State abstraction is typically used to compress the state space of the environment to eliminate redundant information, thereby improving learning efficiency. However, excessive compression results in poor decision performance. Therefore, it is important to balance the compression degree and decision performance. Deterministic Information Bottleneck for State abstraction (DIBS) attempts to solve this problem. Specifically, DIBS uses the information rate to represent the compression degree at first. Then, decision performance after compression is measured using the Kullback-Leibler (KL) divergence of distributions between the policy after state compression and the expert policy. However, if the two distributions do not have exactly overlapping support sets, then the KL divergence is usually infinity, which leads to poor decision performance under the low information rate. In this paper, we propose the Wasserstein DIBS (WDIBS) algorithm to optimize the trade-off between the compression degree and decision performance. Specifically, we use the Wasserstein distance to calculate the difference of the distributions between the policy after state compression and the expert policy. Even if the two distributions do not have precisely overlapping support sets, the Wasserstein distance can still reflect their actual difference, thereby ensuring that WDIBS has good decision performance under the low information rate. Theoretical analyses and experiments demonstrate that our method provides a better trade-off between the compression degree and decision performance than DIBS."}}
{"id": "nShIueXH9LM", "cdate": 1640995200000, "mdate": 1683884082260, "content": {"title": "ACP based reinforcement learning for long-term recommender system", "abstract": "Recommender systems aim to suggest the items which can best fit the needs of the users and thus play an important role in online services. To get a satisfactory recommendation, some researchers model the recommendation procedure as a Markov decision process where the recommender is the agent and the users are the environment. Then, they use reinforcement learning to perform the recommendation by sharing the browsing histories of different users. However, when the number of users is large, there will be much noise in the sharing process, limiting the ability of reinforcement learning to generate a satisfactory recommendation. ACP approach is proposed to deal with social computing by learning a parallel system from the real system. There can be less noise in the parallel system than that in the real system with an effective learning process, thus the ACP approach has the potential to address the noise in the recommendation. In this paper, we combine the ACP approach into the reinforcement learning based recommender system to deal with the noise and thus improve the recommendation. Firstly, based on the ACP approach, we train a parallel environment of the real environment. Then we use the trained parallel environment to predict the future state in the Markov decision process of the recommender system. There will be less noise in the predicted states than that in the original states, since the output of our parallel environment is effectively learned by the expectation of the future state in the deep neural network. Finally, instead of the original states, we use the predicted states to generate the recommendation list in the reinforcement learning for the recommendation. In this way, the generated recommendation list can be better with less noise from the states. The theoretical analysis and the experiment illustrate that our recommender system can better perform the recommendation than existing recommender systems."}}
{"id": "WGFEtY-EYy", "cdate": 1640995200000, "mdate": 1683884082371, "content": {"title": "Consolidation of structure of high noise data by a new noise index and reinforcement learning", "abstract": ""}}
{"id": "KNFvfnAoqq", "cdate": 1640995200000, "mdate": 1683884083495, "content": {"title": "A CNN-based policy for optimizing continuous action control by learning state sequences", "abstract": ""}}
{"id": "EgFZOhB-wp", "cdate": 1640995200000, "mdate": 1683884082853, "content": {"title": "Clustering experience replay for the effective exploitation in reinforcement learning", "abstract": ""}}
{"id": "AxwkOfVyXy", "cdate": 1640995200000, "mdate": 1683884083719, "content": {"title": "Identifying the skeptics and the undecided through visual cluster analysis of local network geometry", "abstract": ""}}
{"id": "b5k1hhpbBR", "cdate": 1609459200000, "mdate": 1683884082035, "content": {"title": "Anchor: The achieved goal to replace the subgoal for hierarchical reinforcement learning", "abstract": ""}}
{"id": "1m0m3_DYuEH", "cdate": 1609459200000, "mdate": 1683884083278, "content": {"title": "Adaptive exploration policy for exploration-exploitation tradeoff in continuous action control optimization", "abstract": "The optimization of continuous action control is an important research field. It aims to find optimal decisions by the experience of making decisions in a continuous action control task. This process can be done via reinforcement learning to train an agent for learning a policy by maximizing cumulative rewards of making decisions in a dynamic environment. Exploration\u2013exploitation tradeoff is a key issue in learning this policy. The current solution called exploration policy addresses this issue by adding exploration noise to the policy in training for more efficient exploration while keeping exploitation. This noise is from a fixed distribution during the training process. However, in the dynamic environment, the stability of training is frequently changed in different training episodes, leading to the low adaptability for exploration policy to training stability. In this paper, we propose an adaptive exploration policy to address exploration\u2013exploitation tradeoff. The motivation is that the noise scale should be increased to enhance exploration when the stability of training is high, while it should be reduced to keep exploitation when the stability of training is low. Firstly, we regard the variance of cumulative rewards from decisions as an index of the training stability. Then, based on this index, we construct a tradeoff coefficient, which is negatively correlated to the training stability. Finally, we propose adaptive exploration policy by the tradeoff coefficient to adjust the added exploration noise for adapting to the training stability. By the theoretical analysis and the experiments, we illustrate the effectiveness of our adaptive exploration policy. The source code can be downloaded from https://github.com/grcai/AEP-algorithm ."}}
{"id": "M31KS9v98B4", "cdate": 1577836800000, "mdate": 1683884082917, "content": {"title": "A new similarity combining reconstruction coefficient with pairwise distance for agglomerative clustering", "abstract": ""}}
{"id": "5cCIbzDF5k_", "cdate": 1577836800000, "mdate": 1668764266071, "content": {"title": "An adaptive kernelized rank-order distance for clustering non-spherical data with high noise", "abstract": ""}}
