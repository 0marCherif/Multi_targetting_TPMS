{"id": "U30wC_mEVYu", "cdate": 1668513561685, "mdate": 1668513561685, "content": {"title": "Neural generators of sparse local linear models for achieving both accuracy and interpretability", "abstract": "For reliability, it is important for the predictions made by machine learning methods to be interpretable by humans. In general, deep neural networks (DNNs) can provide accurate predictions, although it is difficult to interpret why such predictions are obtained by the DNNs. On the other hand, interpretation of linear models is easy, although their predictive performance is low because real-world data are often intrinsically non-linear. To combine both the benefits of the high predictive performance of DNNs and the high interpretability of linear models into a single model, we propose neural generators of sparse local linear models (NGSLL). Sparse local linear models have high flexibility because they can approximate non-linear functions. NGSLL generates sparse linear weights for each sample using DNNs that take the original representations of each sample (e.g., word sequence) and their simplified representations (e.g., bag-of-words) as input. By extracting features from the original representations, the weights can contain rich information and achieve a high predictive performance. In addition, the prediction is interpretable because it is obtained through the inner product between the simplified representations and the sparse weights, where only a small number of weights are selected by our gate module in NGSLL. In experiments on image, text and tabular datasets, we demonstrate the effectiveness of NGSLL quantitatively and qualitatively by evaluating the prediction performance and visualizing generated weights."}}
{"id": "xMaaefyNNo", "cdate": 1668513498413, "mdate": 1668513498413, "content": {"title": "Gaussian Process Regression With Interpretable Sample-Wise Feature Weights", "abstract": "Gaussian process regression (GPR) is a fundamental model used in machine learning (ML). Due to its accurate prediction with uncertainty and versatility in handling various data structures via kernels, GPR has been successfully used in various applications. However, in GPR, how the features of an input contribute to its prediction cannot be interpreted. Here, we propose GPR with local explanation, which reveals the feature contributions to the prediction of each sample while maintaining the predictive performance of GPR. In the proposed model, both the prediction and explanation for each sample are performed using an easy-to-interpret locally linear model. The weight vector of the locally linear model is assumed to be generated from multivariate Gaussian process priors. The hyperparameters of the proposed models are estimated by maximizing the marginal likelihood. For a new test sample, the proposed model can predict the values of its target variable and weight vector, as well as their uncertainties, in a closed form. Experimental results on various benchmark datasets verify that the proposed model can achieve predictive performance comparable to those of GPR and superior to that of existing interpretable models and can achieve higher interpretability than them, both quantitatively and qualitatively."}}
{"id": "HyZNU2edbS", "cdate": 1483228800000, "mdate": null, "content": {"title": "STAIR Captions: Constructing a Large-Scale Japanese Image Caption Dataset", "abstract": "In recent years, automatic generation of image descriptions (captions), that is, image captioning, has attracted a great deal of attention. In this paper, we particularly consider generating Japanese captions for images. Since most available caption datasets have been constructed for English language, there are few datasets for Japanese. To tackle this problem, we construct a large-scale Japanese image caption dataset based on images from MS-COCO, which is called STAIR Captions. STAIR Captions consists of 820,310 Japanese captions for 164,062 images. In the experiment, we show that a neural network trained using STAIR Captions can generate more natural and better Japanese captions, compared to those generated using English-Japanese machine translation after generating English captions."}}
{"id": "HkZ0Jubd-r", "cdate": 1420070400000, "mdate": null, "content": {"title": "Cross-Domain Matching for Bag-of-Words Data via Kernel Embeddings of Latent Distributions", "abstract": "We propose a kernel-based method for finding matching between instances across different domains, such as multilingual documents and images with annotations. Each instance is assumed to be represented as a multiset of features, e.g., a bag-of-words representation for documents. The major difficulty in finding cross-domain relationships is that the similarity between instances in different domains cannot be directly measured. To overcome this difficulty, the proposed method embeds all the features of different domains in a shared latent space, and regards each instance as a distribution of its own features in the shared latent space. To represent the distributions efficiently and nonparametrically, we employ the framework of the kernel embeddings of distributions. The embedding is estimated so as to minimize the difference between distributions of paired instances while keeping unpaired instances apart. In our experiments, we show that the proposed method can achieve high performance on finding correspondence between multi-lingual Wikipedia articles, between documents and tags, and between images and tags."}}
{"id": "B1bZwxb_WH", "cdate": 1420070400000, "mdate": null, "content": {"title": "Non-Linear Regression for Bag-of-Words Data via Gaussian Process Latent Variable Set Model", "abstract": "Gaussian process (GP) regression is a widely used method for non-linear prediction. The performance of the GP regression depends on whether it can properly capture the covariance structure of target variables, which is represented by kernels between input data. However, when the input is represented as a set of features, e.g. bag-of-words, it is difficult to calculate desirable kernel values because the co-occurrence of different but relevant words cannot be reflected in the kernel calculation. To overcome this problem, we propose a Gaussian process latent variable set model (GP-LVSM), which is a non-linear regression model effective for bag-of-words data. With the GP-LVSM, a latent vector is associated with each word, and each document is represented as a distribution of the latent vectors for words appearing in the document. We efficiently represent the distributions by using the framework of kernel embed-dings of distributions that can hold high-order moment information of distributions without need for explicit density estimation. By learning latent vectors so as to maximize the posterior probability, kernels that reflect relations between words are obtained, and also words are visualized in a low-dimensional space. In experiments using 25 item review datasets, we demonstrate the effectiveness of the GP-LVSM in prediction and visualization."}}
{"id": "r1Nkmt-uZH", "cdate": 1388534400000, "mdate": null, "content": {"title": "Latent Support Measure Machines for Bag-of-Words Data Classification", "abstract": "In many classification problems, the input is represented as a set of features, e.g., the bag-of-words (BoW) representation of documents. Support vector machines (SVMs) are widely used tools for such classification problems. The performance of the SVMs is generally determined by whether kernel values between data points can be defined properly. However, SVMs for BoW representations have a major weakness in that the co-occurrence of different but semantically similar words cannot be reflected in the kernel calculation. To overcome the weakness, we propose a kernel-based discriminative classifier for BoW data, which we call the latent support measure machine (latent SMM). With the latent SMM, a latent vector is associated with each vocabulary term, and each document is represented as a distribution of the latent vectors for words appearing in the document. To represent the distributions efficiently, we use the kernel embeddings of distributions that hold high order moment information about distributions. Then the latent SMM finds a separating hyperplane that maximizes the margins between distributions of different classes while estimating latent vectors for words to improve the classification performance. In the experiments, we show that the latent SMM achieves state-of-the-art accuracy for BoW text classification, is robust with respect to its own hyper-parameters, and is useful to visualize words."}}
