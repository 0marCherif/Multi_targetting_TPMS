{"id": "Bvaekygzl2m", "cdate": 1663849948345, "mdate": null, "content": {"title": "Strength-Adaptive Adversarial Training", "abstract": "Adversarial training (AT) is proved to reliably improve network's robustness against adversarial data. However, current AT with a pre-specified perturbation budget has limitations in learning a robust network. Firstly, applying a pre-specified perturbation budget on networks of various model capacities will yield divergent degree of robustness disparity between natural and robust accuracies, which deviates from robust network's desideratum. Secondly, the attack strength of adversarial training data constrained by the pre-specified perturbation budget fails to upgrade as the growth of network robustness, which leads to robust overfitting and further degrades the adversarial robustness. To overcome these limitations, we propose Strength-Adaptive Adversarial Training (SAAT). Specifically, the adversary employs an adversarial loss constraint to generate adversarial training data. Under this constraint, the perturbation budget will be adaptively adjusted according to the training state of adversarial data, which can effectively avoid robust overfitting. Besides, SAAT explicitly constrains the attack strength of training data through the adversarial loss, which manipulates model capacity scheduling during training, and thereby can flexibly control the degree of robustness disparity and adjust the tradeoff between natural accuracy and robustness. Extensive experiments show that our proposal boosts the robustness of adversarial training. "}}
{"id": "ndtLe66Saq6", "cdate": 1649643443088, "mdate": 1649643443088, "content": {"title": "Towards Defending against Adversarial Examples via Attack-Invariant Features", "abstract": "Deep neural networks (DNNs) are vulnerable to\nadversarial noise. Their adversarial robustness\ncan be improved by exploiting adversarial exam\u0002ples. However, given the continuously evolving at\u0002tacks, models trained on seen types of adversarial\nexamples generally cannot generalize well to un\u0002seen types of adversarial examples. To solve this\nproblem, in this paper, we propose to remove ad\u0002versarial noise by learning generalizable invariant\nfeatures across attacks which maintain semantic\nclassification information. Specifically, we intro\u0002duce an adversarial feature learning mechanism\nto disentangle invariant features from adversarial\nnoise. A normalization term has been proposed in\nthe encoded space of the attack-invariant features\nto address the bias issue between the seen and\nunseen types of attacks. Empirical evaluations\ndemonstrate that our method could provide better\nprotection in comparison to previous state-of-the\u0002art approaches, especially against unseen types of\nattacks and adaptive attacks"}}
{"id": "yI6jLeIb8El", "cdate": 1640995200000, "mdate": 1668526018331, "content": {"title": "Visual Privacy Protection Based on Type-I Adversarial Attack", "abstract": "Growing leakage and misuse of visual information raise security and privacy concerns, which promotes the development of information protection. Existing adversarial perturbations-based methods mainly focus on the de-identification against deep learning models. However, the inherent visual information of the data has not been well protected. In this work, inspired by the Type-I adversarial attack, we propose an adversarial visual information hiding method to protect the visual privacy of data. Specifically, the method generates obfuscating adversarial perturbations to obscure the visual information of the data. Meanwhile, it maintains the hidden objectives to be correctly predicted by models. In addition, our method does not modify the parameters of the applied model, which makes it flexible for different scenarios. Experimental results on the recognition and classification tasks demonstrate that the proposed method can effectively hide visual information and hardly affect the performances of models. The code is available in the supplementary material."}}
{"id": "vLZI0SBpno", "cdate": 1640995200000, "mdate": 1668526018381, "content": {"title": "Improving Adversarial Robustness via Mutual Information Estimation", "abstract": "Deep neural networks (DNNs) are found to be vulnerable to adversarial noise. They are typically misled by adversarial samples to make wrong predictions. To alleviate this negative effect, in this p..."}}
{"id": "vIrMNgc_U1Z", "cdate": 1640995200000, "mdate": 1668526018331, "content": {"title": "Modeling Adversarial Noise for Adversarial Training", "abstract": "Deep neural networks have been demonstrated to be vulnerable to adversarial noise, promoting the development of defense against adversarial attacks. Motivated by the fact that adversarial noise con..."}}
{"id": "fU18whDz7J", "cdate": 1640995200000, "mdate": 1668526018345, "content": {"title": "Towards Multi-Domain Face Synthesis Via Domain-Invariant Representations and Multi-Level Feature Parts", "abstract": "Cross-domain face synthesis plays a positive role in the real world. It is challenging to synthesize high-quality faces across multiple domains based on limited paired data because the multiple mappings between different domains may interfere with each other. Cognitive science investigates that the brain can recognize the same person with multiple different expressions by extracting invariant information on the face and we humans perceive instances by decomposing them into parts. Motivated by these cognition, we propose a unified semi-supervised framework for multi-domain face synthesis by extracting a domain-invariant representation and exploiting parts of multi-level features. Specifically, realized by adversarial training with additional ability to utilize domain-specific information, a encoder is trained to remove domain-specific information and extract the domain-invariant representation from multiple inputs. Then, we utilize the multi-level feature parts extracted from inputs and reconstructed faces via a pre-trained recognition model to ensure that the domain-invariant representation contains enough useful semantic information. we also utilize the feature parts extracted from inputs and limited paired data to compose pseudo features in target domain for supervising the synthesis, which makes our framework suitable for large amounts of unpaired training data. By exploiting this framework, we can achieve face synthesis between multiple domains using some paired data together with a large training database without ground truth target faces. Experimental results demonstrate our framework achieves great performances on qualitative and quantitative evaluations under both artificial and uncontrolled environments, and our framework has competitive performances in single translation compared with specialized methods for translation between two specific domains."}}
{"id": "-A6jZNrGAuh", "cdate": 1640995200000, "mdate": 1668526018350, "content": {"title": "Strength-Adaptive Adversarial Training", "abstract": "Adversarial training (AT) is proved to reliably improve network's robustness against adversarial data. However, current AT with a pre-specified perturbation budget has limitations in learning a robust network. Firstly, applying a pre-specified perturbation budget on networks of various model capacities will yield divergent degree of robustness disparity between natural and robust accuracies, which deviates from robust network's desideratum. Secondly, the attack strength of adversarial training data constrained by the pre-specified perturbation budget fails to upgrade as the growth of network robustness, which leads to robust overfitting and further degrades the adversarial robustness. To overcome these limitations, we propose \\emph{Strength-Adaptive Adversarial Training} (SAAT). Specifically, the adversary employs an adversarial loss constraint to generate adversarial training data. Under this constraint, the perturbation budget will be adaptively adjusted according to the training state of adversarial data, which can effectively avoid robust overfitting. Besides, SAAT explicitly constrains the attack strength of training data through the adversarial loss, which manipulates model capacity scheduling during training, and thereby can flexibly control the degree of robustness disparity and adjust the tradeoff between natural accuracy and robustness. Extensive experiments show that our proposal boosts the robustness of adversarial training."}}
{"id": "anWCFENEc5H", "cdate": 1632875486011, "mdate": null, "content": {"title": "Modeling Adversarial Noise for Adversarial Defense", "abstract": "Deep neural networks have been demonstrated to be vulnerable to adversarial noise, promoting the development of defense against adversarial attacks. Motivated by the fact that adversarial noise contains well-generalizing features and that the relationship between adversarial data and natural data can help infer natural data and make reliable predictions, in this paper, we study to model adversarial noise by learning the transition relationship between adversarial labels (i.e. the flipped labels used to generate adversarial data) and natural labels (i.e. the ground truth labels of the natural data). Specifically, we introduce an instance-dependent transition matrix to relate adversarial labels and natural labels, which can be seamlessly embedded with the target model (enabling us to model stronger adaptive adversarial noise). Empirical evaluations demonstrate that our method could effectively improve adversarial accuracy."}}
{"id": "smZKtc9RyVN", "cdate": 1609459200000, "mdate": 1631667996408, "content": {"title": "Towards Defending against Adversarial Examples via Attack-Invariant Features", "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial noise. Their adversarial robustness can be improved by exploiting adversarial examples. However, given the continuously evolving attacks, m..."}}
{"id": "ag3VD5xpVmx", "cdate": 1609459200000, "mdate": 1668526018350, "content": {"title": "Removing Adversarial Noise in Class Activation Feature Space", "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial noise. Pre-processing based defenses could largely remove adversarial noise by processing inputs. However, they are typically affected by the error amplification effect, especially in the front of continuously evolving attacks. To solve this problem, in this paper, we propose to remove adversarial noise by implementing a self-supervised adversarial training mechanism in a class activation feature space. To be specific, we first maximize the disruptions to class activation features of natural examples to craft adversarial examples. Then, we train a denoising model to minimize the distances between the adversarial examples and the natural examples in the class activation feature space. Empirical evaluations demonstrate that our method could significantly enhance adversarial robustness in comparison to previous state-of-the-art approaches, especially against unseen adversarial attacks and adaptive attacks."}}
