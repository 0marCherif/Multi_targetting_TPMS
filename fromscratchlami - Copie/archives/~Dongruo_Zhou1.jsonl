{"id": "3qWdwBUWTM", "cdate": 1676827093036, "mdate": null, "content": {"title": "Provably Efficient Representation Selection in Low-rank Markov Decision Processes: From Online to Offline RL", "abstract": "The success of deep reinforcement learning (DRL) lies in its ability to learn a representation that is well-suited for the exploration and exploitation task. To understand how the choice of representation can improve the efficiency of reinforcement learning (RL), we study representation selection for a class of low-rank Markov Decision Processes (MDPs) where the transition kernel can be represented in a bilinear form. We propose an efficient algorithm, called ReLEX, for representation learning in both online and offline RL. Specifically, we show that the online version of ReLEX, calledReLEX-UCB, always performs no worse than the state-of-the-art algorithm without representation selection, and achieves a strictly better constant regret if the representation function class has a \"coverage\" property over the entire state-action space. For the offline counterpart, ReLEX-LCB, we show that the algorithm can find the optimal policy if the representation class can cover the state-action space and achieves gap-dependent sample complexity. This is the first result with constant sample complexity for representation learning in offline RL."}}
{"id": "fySLokohvj4", "cdate": 1663850214867, "mdate": null, "content": {"title": "Bandit Learning with General Function Classes: Heteroscedastic Noise and Variance-dependent Regret Bounds", "abstract": "We consider learning a stochastic bandit model, where the reward function belongs to a general class of uniformly bounded functions, and the additive noise can be heteroscedastic. Our model captures contextual linear bandits and generalized linear bandits as special cases. While previous works (Kirschner and Krause, 2018; Zhou et al., 2021) based on weighted ridge regression can deal with linear bandits with heteroscedastic noise, they are not directly applicable to our general model due to the curse of nonlinearity. In order to tackle this problem, we propose a \\emph{multi-level learning} framework for the general bandit model. The core idea of our framework is to partition the observed data into different levels according to the variance of their respective reward and perform online learning at each level collaboratively. Under our framework, we first design an algorithm that constructs the variance-aware confidence set based on empirical risk minimization and prove a variance-dependent regret bound. For generalized linear bandits, we further propose an algorithm based on follow-the-regularized-leader (FTRL) subroutine and online-to-confidence-set conversion, which can achieve a tighter variance-dependent regret under certain conditions."}}
{"id": "H4GmqyYMxFP", "cdate": 1652737662609, "mdate": null, "content": {"title": "Computationally Efficient Horizon-Free Reinforcement Learning for Linear Mixture MDPs", "abstract": "Recent studies have shown that episodic reinforcement learning (RL) is not more difficult than bandits, even with a long planning horizon and unknown state transitions. However, these results are limited to either tabular Markov decision processes (MDPs) or computationally inefficient algorithms for linear mixture MDPs. In this paper, we propose the first computationally efficient horizon-free algorithm for linear mixture MDPs, which achieves the optimal $\\tilde O(d\\sqrt{K} +d^2)$ regret up to logarithmic factors. Our algorithm adapts a weighted least square estimator for the unknown transitional dynamic, where the weight is both \\emph{variance-aware} and \\emph{uncertainty-aware}. When applying our weighted least square estimator to heterogeneous linear bandits, we can obtain an $\\tilde O(d\\sqrt{\\sum_{k=1}^K \\sigma_k^2} +d)$ regret in the first $K$ rounds, where $d$ is the dimension of the context and $\\sigma_k^2$ is the variance of the reward in the $k$-th round. This also improves upon the best known algorithms in this setting when $\\sigma_k^2$'s are known."}}
{"id": "3PAIKtWQsc", "cdate": 1652737589012, "mdate": null, "content": {"title": "Learning Two-Player Markov Games: Neural Function Approximation and Correlated Equilibrium", "abstract": "We consider learning Nash equilibria in two-player zero-sum Markov Games with nonlinear function approximation, where the action-value function is approximated by a function in a Reproducing Kernel Hilbert Space (RKHS). The key challenge is how to do exploration in the high-dimensional function space. We propose a novel online learning algorithm to find a Nash equilibrium by minimizing the duality gap. At the core of our algorithms are upper and lower confidence bounds that are derived based on the principle of optimism in the face of uncertainty. We prove that our algorithm is able to attain an $O(\\sqrt{T})$ regret with polynomial computational complexity, under very mild assumptions on the reward function and the underlying dynamic of the Markov Games. We also propose several extensions of our algorithm, including an algorithm with Bernstein-type bonus that can achieve a tighter regret bound, and another algorithm for model misspecification that can be applied to neural network function approximation."}}
{"id": "YeuBRKq_yZ-", "cdate": 1652737577668, "mdate": null, "content": {"title": "Nearly Optimal Algorithms for Linear Contextual Bandits with Adversarial Corruptions", "abstract": "We study the linear contextual bandit problem in the presence of adversarial corruption, where the reward at each round is corrupted by an adversary, and the corruption level (i.e., the sum of corruption magnitudes over the horizon) is $C\\geq 0$. The best-known algorithms in this setting are limited in that they either are computationally inefficient or require a strong assumption on the corruption, or their regret is at least $C$ times worse than the regret without corruption. In this paper, to overcome these limitations, we propose a new algorithm based on the principle of optimism in the face of uncertainty. At the core of our algorithm is a weighted ridge regression where the weight of each chosen action depends on its confidence up to some threshold. We show that for both known $C$ and unknown $C$ cases, our algorithm with proper choice of hyperparameter achieves a regret that nearly matches the lower bounds. Thus, our algorithm is nearly optimal up to logarithmic factors for both cases. Notably, our algorithm achieves the near-optimal regret for both corrupted and uncorrupted cases ($C=0$) simultaneously."}}
{"id": "g4NT2pNvq9h", "cdate": 1634067442427, "mdate": null, "content": {"title": "Learning Two-Player Mixture Markov Games: Kernel Function Approximation and Correlated Equilibrium", "abstract": "We consider learning Nash equilibrium in two-player zero-sum Markov games with nonlinear function approximation, where the action-value function is approximated by a function in the Reproducing Kernel Hilbert Space (RKHS). The key challenge is how to do exploration in the high-dimensional function space. We propose novel online learning algorithms to find an approximate Nash equilibrium by minimizing the duality gap. At the core of our algorithms are upper and lower confidence bounds that are derived based on the principle of optimism in the face of uncertainty. We prove that our algorithm is able to attain an $O(\\sqrt{T})$ regret with polynomial computational complexity, under very mild assumptions on the reward function and the underlying dynamic of the Markov Games. This work provides the first complexity results for learning two-player zero-sum Markov games with nonlinear function approximation in the mixture model settings, and its implications for function approximation via deep neural networks."}}
{"id": "7inCJ3MhXt3", "cdate": 1632875607943, "mdate": null, "content": {"title": "Learning Neural Contextual Bandits through Perturbed Rewards", "abstract": "Thanks to the power of representation learning, neural contextual bandit algorithms demonstrate remarkable performance improvement against their classical counterparts. But because their exploration has to be performed in the entire neural network parameter space to obtain nearly optimal regret, the resulting computational cost is prohibitively high.  \nWe propose to perturb the rewards when updating the neural network to eliminate the need of explicit exploration and the corresponding computational overhead. We prove that a $\\tilde{O}(\\tilde{d}\\sqrt{T})$ regret upper bound is still achievable under standard regularity conditions, where $T$ is the number of rounds of interactions and $\\tilde{d}$ is the effective dimension of a neural tangent kernel matrix. \nExtensive comparisons with several benchmark contextual bandit algorithms, including two recent neural contextual bandit models, demonstrate the effectiveness and computational efficiency of our proposed neural bandit algorithm."}}
{"id": "oRMRIR4qPC1", "cdate": 1621630279682, "mdate": null, "content": {"title": "Faster Perturbed Stochastic Gradient Methods for Finding Local Minima", "abstract": "Escaping from saddle points and finding local minimum is a central problem in nonconvex optimization.\nPerturbed gradient methods are perhaps the simplest approach for this problem. However, to find $(\\epsilon, \\sqrt{\\epsilon})$-approximate local minima, the existing best stochastic gradient complexity for this type of algorithms is $\\tilde O(\\epsilon^{-3.5})$, which is not optimal. In this paper, we propose \\texttt{Pullback}, a faster perturbed stochastic gradient framework for finding local minima.  We show that Pullback with stochastic gradient estimators such as SARAH/SPIDER and STORM can find $(\\epsilon, \\epsilon_{H})$-approximate local minima within $\\tilde O(\\epsilon^{-3} + \\epsilon_{H}^{-6})$ stochastic gradient evaluations (or $\\tilde O(\\epsilon^{-3})$ when $\\epsilon_H = \\sqrt{\\epsilon}$). The core idea of our framework is a step-size ``pullback'' scheme to control the average movement of the iterates, which leads to faster convergence to the local minima. "}}
{"id": "Wz-t1oOTWa", "cdate": 1621630177303, "mdate": null, "content": {"title": "Linear Contextual Bandits with Adversarial Corruptions", "abstract": "We study the linear contextual bandits problem in the presence of adversarial corruption, where the interaction between the player and a possibly infinite decision set is contaminated by an adversary that can corrupt the reward up to a corruption level $C$ measured by the sum of the largest alteration on rewards in each round. \nWe present a variance-aware algorithm that is adaptive to the level of adversarial contamination $C$. The key algorithmic design includes (1) a multi-level partition scheme of the observed data, (2) a cascade of confidence sets that are adaptive to the level of the corruption, and (3) a variance-aware confidence set construction that can take advantage of low-variance reward. We further prove that the regret of the proposed algorithm is $\\tilde{O}(C^2d\\sqrt{\\sum_{t = 1}^T \\sigma_t^2} + C^2 \\sqrt{dT} + CR\\sqrt{dT})$, where $d$ is the dimension of context vectors, $T$ is the number of rounds, $R$ is the range of noise and $\\sigma_t^2,t=1\\ldots,T$ are the variances of instantaneous reward. We also prove a gap-dependent regret bound for the proposed algorithm, which is instance-dependent and thus leads to better performance on good practical instances. To the best of our knowledge, this is the first variance-aware corruption robust algorithm for contextual bandits. "}}
{"id": "9lwprXiGdR4", "cdate": 1621630172366, "mdate": null, "content": {"title": "Nearly Minimax Optimal Reinforcement Learning for Discounted MDPs", "abstract": " We study the reinforcement learning problem for discounted Markov Decision Processes (MDPs) under the tabular setting. We propose a model-based algorithm named UCBVI-$\\gamma$, which is based on the \\emph{optimism in the face of uncertainty principle} and the Bernstein-type bonus. We show that UCBVI-$\\gamma$ achieves an $\\tilde{O}\\big({\\sqrt{SAT}}/{(1-\\gamma)^{1.5}}\\big)$ regret, where $S$ is the number of states, $A$ is the number of actions, $\\gamma$ is the discount factor and $T$ is the number of steps. In addition,  we construct a class of hard MDPs and show that for any algorithm, the expected regret is at least $\\tilde{\\Omega}\\big({\\sqrt{SAT}}/{(1-\\gamma)^{1.5}}\\big)$. Our upper bound matches the minimax lower bound up to logarithmic factors, which suggests that UCBVI-$\\gamma$ is nearly minimax optimal for discounted MDPs."}}
