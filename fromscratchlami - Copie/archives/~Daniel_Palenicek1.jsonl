{"id": "QeEMO4yI9Qi", "cdate": 1672531200000, "mdate": 1681653533300, "content": {"title": "Diminishing Return of Value Expansion Methods in Model-Based Reinforcement Learning", "abstract": ""}}
{"id": "H4Ncs5jhTCu", "cdate": 1663850119265, "mdate": null, "content": {"title": "Diminishing Return of Value Expansion Methods in Model-Based Reinforcement Learning", "abstract": "Model-based reinforcement learning is one approach to increase sample efficiency. However, the accuracy of the dynamics model and the resulting compounding error over modelled trajectories are commonly regarded as key limitations. A natural question to ask is: How much more sample efficiency can be gained by improving the learned dynamics models? Our paper empirically answers this question for the class of model-based value expansion methods in continuous control problems. Value expansion methods should benefit from increased model accuracy by enabling longer rollout horizons and better value function approximations. Our empirical study, which leverages oracle dynamics models to avoid compounding model errors, shows that (1) longer horizons increase sample efficiency, but the gain in improvement decreases with each additional expansion step, and (2) the increased model accuracy only marginally increases the sample efficiency compared to learned models with identical horizons. Therefore, longer horizons and increased model accuracy yield diminishing returns in terms of sample efficiency. These improvements in sample efficiency are particularly disappointing when compared to model-free value expansion methods. Even though they introduce no computational overhead, we find their performance to be on-par with model-based value expansion methods. Therefore, we conclude that the limitation of model-based value expansion methods is not the model accuracy of the learned models. While higher model accuracy is beneficial, our experiments show that even a perfect model will not provide an un-rivaled sample efficiency but that the bottleneck lies elsewhere."}}
{"id": "SNhXhN6Nkbq", "cdate": 1646378292298, "mdate": null, "content": {"title": "Revisiting Model-based Value Expansion", "abstract": "Model-based value expansion methods promise to improve the quality of value function targets and, thereby, the effectiveness of value function learning. However, to date, these methods are being outperformed by Dyna-style algorithms with conceptually simpler 1-step value function targets. This shows that in practice, the theoretical justification of value expansion does not seem to hold. We provide a thorough empirical study to shed light on the causes of failure of value expansion methods in practice which is believed to be the compounding model error. By leveraging GPU based physics simulators, we are able to efficiently use the true dynamics for analysis inside the model-based reinforcement learning loop. Performing extensive comparisons between true and learned dynamics sheds light into this black box. This paper provides a better understanding of the actual problems in value expansion. We provide future directions of research by empirically testing the maximum theoretical performance of current approaches."}}
{"id": "pZZE4Qx5IB", "cdate": 1640995200000, "mdate": 1681653533286, "content": {"title": "Revisiting Model-based Value Expansion", "abstract": ""}}
{"id": "le7DTnEgdG-", "cdate": 1640995200000, "mdate": 1681653533360, "content": {"title": "SAMBA: safe model-based & active reinforcement learning", "abstract": ""}}
{"id": "0hjoiBfpq9", "cdate": 1577836800000, "mdate": 1681653533320, "content": {"title": "SAMBA: Safe Model-Based & Active Reinforcement Learning", "abstract": ""}}
