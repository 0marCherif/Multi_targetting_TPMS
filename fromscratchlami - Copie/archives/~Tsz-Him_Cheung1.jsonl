{"id": "OM7doLjQbOQ", "cdate": 1663850053726, "mdate": null, "content": {"title": "ILA-DA: Improving Transferability of Intermediate Level Attack with Data Augmentation", "abstract": "Adversarial attack aims to generate deceptive inputs to fool a machine learning model. In deep learning, an adversarial input created for a specific neural network can also trick other neural networks. This intriguing property is known as black-box transferability of adversarial examples. To improve black-box transferability, a previously proposed method called Intermediate Level Attack (ILA) fine-tunes an adversarial example by maximizing its perturbation on an intermediate layer of the source model. Meanwhile, it has been shown that simple image transformations can also enhance attack transferability. Based on these two observations, we propose ILA-DA, which employs three novel augmentation techniques to enhance ILA. Specifically, we propose (1) an automated way to apply effective image transformations, (2) an efficient reverse adversarial update technique, and (3) an attack interpolation method to create more transferable adversarial examples. Shown by extensive experiments, ILA-DA greatly outperforms ILA and other state-of-the-art attacks by a large margin. On ImageNet, we attain an average attack success rate of 84.5%, which is 19.5% better than ILA and 4.7% better than the previous state-of-the-art across nine undefended models. For defended models, ILA-DA also leads existing attacks and provides further gains when incorporated into more advanced attack methods."}}
{"id": "-1vpxBUtP0B", "cdate": 1663849990130, "mdate": null, "content": {"title": "TransformMix: Learning Transformation and Mixing Strategies for Sample-mixing Data Augmentation", "abstract": "Data augmentation improves the generalization power of deep learning models by synthesizing more training samples. Sample-mixing is a popular data augmentation approach that creates additional training samples by combining existing images. Recent sample-mixing methods, like Mixup and Cutmix, adopt simple mixing operations to blend multiple input images. Although such a heuristic approach shows certain performance gains in some computer vision tasks, it mixes the images blindly and does not adapt to different datasets automatically. A mixing strategy that is effective for a particular dataset does not often generalize well to other datasets. If not properly configured, the methods may create misleading mixed images, which jeopardize the effectiveness of sample-mixing augmentations. In this work, we propose an automated approach, TransformMix, to learn better transformation and mixing augmentation strategies from data. In particular, TransformMix applies learned transformations and mixing masks to create compelling mixed images that contain correct and important information for the target tasks. We demonstrate the effectiveness of TransformMix in multiple datasets under the direct and transfer settings. Experimental results show that our method achieves better top-1 and top-5 accuracy as well as efficiency when compared with strong sample-mixing baselines."}}
{"id": "kmqj8kB7TgN", "cdate": 1640995200000, "mdate": 1681544314470, "content": {"title": "AdaAug: Learning Class- and Instance-adaptive Data Augmentation Policies", "abstract": ""}}
{"id": "rWXfFogxRJN", "cdate": 1632875611210, "mdate": null, "content": {"title": "AdaAug: Learning Class- and Instance-adaptive Data Augmentation Policies", "abstract": "Data augmentation is an effective way to improve the generalization capability of modern deep learning models. However, the underlying augmentation methods mostly rely on handcrafted operations. Moreover, an augmentation policy useful to one dataset may not transfer well to other datasets. Therefore, Automated Data Augmentation (AutoDA) methods, like \\textit{AutoAugment} and \\textit{Population-based Augmentation}, have been proposed recently to automate the process of searching for optimal augmentation policies. However, the augmentation policies found are not adaptive to the dataset used, hindering the effectiveness of these AutoDA methods. In this paper, we propose a novel AutoDA method called \\texttt{AdaAug} to efficiently learn adaptive augmentation policies in a class-dependent and potentially instance-dependent manner. Our experiments show that the adaptive augmentation policies learned by our method transfer well to unseen datasets such as the Oxford Flowers, Oxford-IIT Pets, FGVC Aircraft, and Stanford Cars datasets when compared with other AutoDA baselines. In addition, our method also achieves state-of-the-art performance on the CIFAR-10, CIFAR-100, and SVHN datasets."}}
{"id": "FIfWXfPe8Zh", "cdate": 1609459200000, "mdate": 1681544314079, "content": {"title": "MODALS: Modality-agnostic Automated Data Augmentation in the Latent Space", "abstract": ""}}
{"id": "XjYgR6gbCEc", "cdate": 1601308158979, "mdate": null, "content": {"title": "MODALS: Modality-agnostic Automated Data Augmentation in the Latent Space", "abstract": "Data augmentation is an efficient way to expand a training dataset by creating additional artificial data. While data augmentation is found to be effective in improving the generalization capabilities of models for various machine learning tasks, the underlying augmentation methods are usually manually designed and carefully evaluated for each data modality separately, like image processing functions for image data and word-replacing rules for text data. In this work, we propose an automated data augmentation approach called MODALS (Modality-agnostic Automated Data Augmentation in the Latent Space) to augment data for any modality in a generic way. MODALS exploits automated data augmentation to fine-tune four universal data transformation operations in the latent space to adapt the transform to data of different modalities. Through comprehensive experiments, we demonstrate the effectiveness of MODALS on multiple datasets for text, tabular, time-series and image modalities."}}
