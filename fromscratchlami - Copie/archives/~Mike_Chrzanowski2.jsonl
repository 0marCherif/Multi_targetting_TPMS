{"id": "CGFN_nV1ql", "cdate": 1601308109027, "mdate": null, "content": {"title": "Non-Attentive Tacotron: Robust and controllable neural TTS synthesis including unsupervised duration modeling", "abstract": "This paper presents Non-Attentive Tacotron based on the Tacotron 2 text-to-speech model, replacing the attention mechanism with an explicit duration predictor. This improves robustness significantly as measured by unaligned duration ratio and word deletion rate, two metrics introduced in this paper for large-scale robustness evaluation using a pre-trained speech recognition model. With the use of Gaussian upsampling, Non-Attentive Tacotron achieves a 5-scale mean opinion score for naturalness of 4.41, slightly outperforming Tacotron 2. The duration predictor enables both utterance-wide and per-phoneme control of duration at inference time. When accurate target durations are scarce or unavailable in the training data, we propose a method using a fine-grained variational auto-encoder to train the duration predictor in a semi-supervised or unsupervised manner, with results almost as good as supervised training."}}
{"id": "KolPNdO2Ls1", "cdate": 1598826596048, "mdate": null, "content": {"title": "Using Twitter to predict voting behavior", "abstract": "An increasing amount of research has emerged in the past few years using social media to either predict the outcomes of elections or, in the case of the United States, to classify users as Democrat or Republican. While predictions based on social media are not representative of the voting population, they have been shown to compete with surveys in accuracy and can be done in real time to provide instantaneous feedback to political events (Tweetminster, 2010). Twitter provides an excellent platform for solving political classification problems. The realtime availability of large amounts of data as well as built-in content sorting mechanisms (eg, hashtags, retweets, followers) are major advantages over using other forms of social media. If these advantages can be leveraged to predict how Twitter users will vote, this information could be used to predict an election outcome or predict the likely impact of real-time events on voting patterns."}}
{"id": "IqMwef87p4e", "cdate": 1577836800000, "mdate": null, "content": {"title": "Towards Robust Image Classification Using Sequential Attention Models", "abstract": "In this paper we propose to augment a modern neuralnetwork architecture with an attention model inspired by human perception. Specifically, we adversarially train and analyze a neural model incorporating a human inspired, visual attention component that is guided by a recurrent top-down sequential process. Our experimental evaluation uncovers several notable findings about the robustness and behavior of this new model. First, introducing attention to the model significantly improves adversarial robustness resulting in state-of-the-art ImageNet accuracies under a wide range of random targeted attack strengths. Second, we show that by varying the number of attention steps (glances/fixations) for which the model is unrolled, we are able to make its defense capabilities stronger, even in light of stronger attacks - resulting in a \u201ccomputational race\u201d between the attacker and the defender. Finally, we show that some of the adversarial examples generated by attacking our model are quite different from conventional adversarial examples - they contain global, salient and spatially coherent structures coming from the target class that would be recognizable even to a human, and work by distracting the attention of the model away from the main object in the original image."}}
{"id": "vFgHkcJPDI6", "cdate": 1546300800000, "mdate": null, "content": {"title": "Learning and Evaluating General Linguistic Intelligence.", "abstract": "We define general linguistic intelligence as the ability to reuse previously acquired knowledge about a language's lexicon, syntax, semantics, and pragmatic conventions to adapt to new tasks quickly. Using this definition, we analyze state-of-the-art natural language understanding models and conduct an extensive empirical investigation to evaluate them against these criteria through a series of experiments that assess the task-independence of the knowledge being acquired by the learning process. In addition to task performance, we propose a new evaluation metric based on an online encoding of the test data that quantifies how quickly an existing agent (model) learns a new task. Our results show that while the field has made impressive progress in terms of model architectures that generalize to many tasks, these models still require a lot of in-domain training examples (e.g., for fine tuning, training task-specific modules), and are prone to catastrophic forgetting. Moreover, we find that far from solving general tasks (e.g., document question answering), our models are overfitting to the quirks of particular datasets (e.g., SQuAD). We discuss missing components and conjecture on how to make progress toward general linguistic intelligence."}}
{"id": "8yuKuvLSRJBf", "cdate": 1546300800000, "mdate": null, "content": {"title": "Towards Robust Image Classification Using Sequential Attention Models", "abstract": "In this paper we propose to augment a modern neural-network architecture with an attention model inspired by human perception. Specifically, we adversarially train and analyze a neural model incorporating a human inspired, visual attention component that is guided by a recurrent top-down sequential process. Our experimental evaluation uncovers several notable findings about the robustness and behavior of this new model. First, introducing attention to the model significantly improves adversarial robustness resulting in state-of-the-art ImageNet accuracies under a wide range of random targeted attack strengths. Second, we show that by varying the number of attention steps (glances/fixations) for which the model is unrolled, we are able to make its defense capabilities stronger, even in light of stronger attacks --- resulting in a \"computational race\" between the attacker and the defender. Finally, we show that some of the adversarial examples generated by attacking our model are quite different from conventional adversarial examples --- they contain global, salient and spatially coherent structures coming from the target class that would be recognizable even to a human, and work by distracting the attention of the model away from the main object in the original image."}}
{"id": "5_KA4pUEqoW", "cdate": 1546300800000, "mdate": null, "content": {"title": "Towards Interpretable Reinforcement Learning Using Attention Augmented Agents", "abstract": "Inspired by recent work in attention models for image captioning and question answering, we present a soft attention model for the reinforcement learning domain. This model uses a soft, top-down attention mechanism to create a bottleneck in the agent, forcing it to focus on task-relevant information by sequentially querying its view of the environment. The output of the attention mechanism allows direct observation of the information used by the agent to select its actions, enabling easier interpretation of this model than of traditional models. We analyze different strategies that the agents learn and show that a handful of strategies arise repeatedly across different games. We also show that the model learns to query separately about space and content (`where' vs. `what'). We demonstrate that an agent using this mechanism can achieve performance competitive with state-of-the-art models on ATARI tasks while still being interpretable."}}
{"id": "-dgkkwecbW6", "cdate": 1546300800000, "mdate": null, "content": {"title": "Towards Interpretable Reinforcement Learning Using Attention Augmented Agents", "abstract": "Inspired by recent work in attention models for image captioning and question answering, we present a soft attention model for the reinforcement learning domain. This model bottlenecks the view of an agent by a soft, top-down attention mechanism, forcing the agent to focus on task-relevant information by sequentially querying its view of the environment. The output of the attention mechanism allows direct observation of the information used by the agent to select its actions, enabling easier interpretation of this model than of traditional models. We analyze the different strategies the agents learn and show that a handful of strategies arise repeatedly across different games. We also show that the model learns to query separately about space and content (<code>where'' vs.</code>what''). We demonstrate that an agent using this mechanism can achieve performance competitive with state-of-the-art models on ATARI tasks while still being interpretable."}}
{"id": "B1gJOoRcYQ", "cdate": 1538087782646, "mdate": null, "content": {"title": "S3TA: A Soft, Spatial, Sequential, Top-Down Attention Model", "abstract": "We present a soft, spatial, sequential, top-down attention model (S3TA). This model uses a soft attention mechanism to bottleneck its view of the input. A recurrent core is used to generate query vectors, which actively select information from the input by correlating the query with input- and space-dependent key maps at different spatial locations.\n\nWe demonstrate the power and interpretabilty of this model under two settings. First, we build an agent which uses this attention model in RL environments and show that we can achieve performance competitive with state-of-the-art models while producing attention maps that elucidate some of the strategies used to solve the task. Second, we use this model in supervised learning tasks and show that it also achieves competitive performance and provides interpretable attention maps that show some of the underlying logic in the model's decision making."}}
{"id": "S1WGwP-_WB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Relational recurrent neural networks", "abstract": "Memory-based neural networks model temporal data by leveraging an ability to remember information for long periods. It is unclear, however, whether they also have an ability to perform complex relational reasoning with the information they remember. Here, we first confirm our intuitions that standard memory architectures may struggle at tasks that heavily involve an understanding of the ways in which entities are connected -- i.e., tasks involving relational reasoning. We then improve upon these deficits by using a new memory module -- a Relational Memory Core (RMC) -- which employs multi-head dot product attention to allow memories to interact. Finally, we test the RMC on a suite of tasks that may profit from more capable relational reasoning across sequential information, and show large gains in RL domains (BoxWorld &amp; Mini PacMan), program evaluation, and language modeling, achieving state-of-the-art results on the WikiText-103, Project Gutenberg, and GigaWord datasets."}}
{"id": "S1-EAcWOZH", "cdate": 1483228800000, "mdate": null, "content": {"title": "Deep Voice: Real-time Neural Text-to-Speech", "abstract": "We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The sy..."}}
