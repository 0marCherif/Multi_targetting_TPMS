{"id": "2cq1DhynJvL", "cdate": 1672865453388, "mdate": 1672865453388, "content": {"title": "Accelerating large-scale inference with anisotropic vector quantization", "abstract": "Quantization based techniques are the current state-of-the-art for scaling maximum inner product search to massive databases. Traditional approaches to quantization aim to minimize the reconstruction error of the database points. Based on the observation that for a given query, the database points that have the largest inner products are more relevant, we develop a family of anisotropic quantization loss functions. Under natural statistical assumptions, we show that quantization with these loss functions leads to a new variant of vector quantization that more greatly penalizes the parallel component of a datapoint\u2019s residual relative to its orthogonal component. The proposed approach, whose implementation is open-source, achieves state-of-theart results on the public benchmarks available at ann-benchmarks.com."}}
{"id": "vZGPHN0Q_f", "cdate": 1623413377034, "mdate": null, "content": {"title": "Composing Normalizing Flows for Inverse Problems", "abstract": "Given an inverse problem with a normalizing flow prior, we wish to estimate the distribution of the underlying signal conditioned on the observations.  We approach this problem as a task of conditional inference on the pre-trained unconditional flow model.  We first establish that this is computationally hard for a large class of flow models.  Motivated by this, we propose a framework for approximate inference that estimates the target conditional as a composition of two flow models.  This formulation leads to a stable variational inference training procedure that avoids adversarial training.  Our method is evaluated on a variety of inverse problems and is shown to produce high quality samples with uncertainty quantification.  We further demonstrate that our approach can be amortized for zero-shot inference."}}
{"id": "824xC-SgWgU", "cdate": 1621630112424, "mdate": null, "content": {"title": "Efficient Training of Retrieval Models using Negative Cache", "abstract": "Factorized models, such as two tower neural network models, are widely used for scoring (query, document) pairs in information retrieval tasks. These models are typically trained by optimizing the model parameters to score relevant ``positive\" pairs higher than the irrelevant ``negative\" ones. While a large set of negatives typically improves the model performance, limited computation and memory budgets place constraints on the number of negatives used during training. In this paper, we develop a novel negative sampling technique for accelerating training with softmax cross-entropy loss. By using cached (possibly stale) item embeddings, our technique enables training with a large pool of negatives with reduced memory and computation. We also develop a streaming variant of our algorithm geared towards very large datasets. Furthermore, we establish a theoretical basis for our approach by showing that updating a very small fraction of the cache at each iteration can still ensure fast convergence. Finally, we experimentally validate our approach and show that it is efficient and compares favorably with more complex, state-of-the-art approaches."}}
{"id": "qMIW5nuCjwL", "cdate": 1603473990146, "mdate": null, "content": {"title": "Approximate Probabilistic Inference with Composed Flows", "abstract": "We study the problem of probabilistic inference on the joint distribution defined by a normalizing flow model. Given a pre-trained flow model $p(\\boldsymbol{x})$, we wish to estimate $p(\\boldsymbol{x}_2 \\mid \\boldsymbol{x}_1)$ for some partitioning of the variables $\\boldsymbol{x} = (\\boldsymbol{x}_1, \\boldsymbol{x}_2)$. We first show that this task is computationally hard for a large class of flow models.  Motivated by this, we propose a framework for \\textit{approximate} probabilistic inference. Specifically, our method trains a new flow model with the property that its composition with the given model approximates the target conditional distribution.  We describe how we can train this new model using variational inference and handle conditioning under arbitrary differentiable transformations.  Experimentally, our approach outperforms Langevin Dynamics in terms of sample quality, while requiring much fewer parameters and training time compared to regular variational inference. We further validate the flexibility of our method on a variety of inference tasks with applications to inverse problems."}}
{"id": "SQfqNwVoWu", "cdate": 1601308412292, "mdate": null, "content": {"title": "Approximate Probabilistic Inference with Composed Flows", "abstract": "We study the problem of probabilistic inference on the joint distribution defined by a normalizing flow model. Given a pre-trained flow model $p(\\boldsymbol{x})$, we wish to estimate $p(\\boldsymbol{x}_2 \\mid \\boldsymbol{x}_1)$ for some arbitrary partitioning of the variables $\\boldsymbol{x} = (\\boldsymbol{x}_1, \\boldsymbol{x}_2)$. We first show that this task is computationally hard for a large class of flow models.  Motivated by this hardness result, we propose a framework for $\\textit{approximate}$ probabilistic inference. Specifically, our method trains a new generative model with the property that its composition with the given model approximates the target conditional distribution. By parametrizing this new distribution as another flow model, we can efficiently train it using variational inference and also handle conditioning under arbitrary differentiable transformations. Since the resulting approximate posterior remains a flow, it offers exact likelihood evaluation, inversion, and efficient sampling. We provide an extensive empirical evidence showcasing the flexibility of our method on a variety of inference tasks with applications to inverse problems. We also experimentally demonstrate that our approach is comparable to simple MCMC baselines in terms of sample quality. Further, we explain the failure of naively applying variational inference and show that our method does not suffer from the same issue."}}
{"id": "UJMoq2mW_b5", "cdate": 1596139395136, "mdate": null, "content": {"title": "On Robust Learning of Ising Models", "abstract": "Ising Models are one of the most popular class of probability distributions withapplications in wide ranging fields such as physics, engineering and finance.  Inthis paper, we attempt to learn the underlying graphical model robustly in presenceof adversarial corruptions. In this work, we establish new lower and upper boundsfor robustly learning Ising models."}}
{"id": "Sk-sbKWdZH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Experimental Design for Cost-Aware Learning of Causal Graphs", "abstract": "We consider the minimum cost intervention design problem: Given the essential graph of a causal graph and a cost to intervene on a variable, identify the set of interventions with minimum total cost that can learn any causal graph with the given essential graph. We first show that this problem is NP-hard. We then prove that we can achieve a constant factor approximation to this problem with a greedy algorithm. We then constrain the sparsity of each intervention. We develop an algorithm that returns an intervention design that is nearly optimal in terms of size for sparse graphs with sparse interventions and we discuss how to use it when there are costs on the vertices."}}
{"id": "S1WdvsZ_bH", "cdate": 1483228800000, "mdate": null, "content": {"title": "Exact MAP Inference by Avoiding Fractional Vertices", "abstract": "Given a graphical model, one essential problem is MAP inference, that is, finding the most likely configuration of states according to the model. Although this problem is NP-hard, large instances c..."}}
{"id": "Bk4EguWuZr", "cdate": 1451606400000, "mdate": null, "content": {"title": "Leveraging Sparsity for Efficient Submodular Data Summarization", "abstract": "The facility location problem is widely used for summarizing large datasets and has additional applications in sensor placement, image retrieval, and clustering. One difficulty of this problem is that submodular optimization algorithms require the calculation of pairwise benefits for all items in the dataset. This is infeasible for large problems, so recent work proposed to only calculate nearest neighbor benefits. One limitation is that several strong assumptions were invoked to obtain provable approximation guarantees. In this paper we establish that these extra assumptions are not necessary\u2014solving the sparsified problem will be almost optimal under the standard assumptions of the problem. We then analyze a different method of sparsification that is a better model for methods such as Locality Sensitive Hashing to accelerate the nearest neighbor computations and extend the use of the problem to a broader family of similarities. We validate our approach by demonstrating that it rapidly generates interpretable summaries."}}
