{"id": "mjVmifxpKqS", "cdate": 1652737548752, "mdate": null, "content": {"title": "Near-Optimal Regret Bounds for Multi-batch Reinforcement Learning", "abstract": "\tIn this paper, we study the episodic reinforcement learning (RL) problem modeled by finite-horizon Markov Decision Processes (MDPs) with constraint on the number of batches. The multi-batch reinforcement learning framework, where the agent is required to provide a time schedule to update policy before everything, which is particularly suitable for the scenarios where the agent suffers extensively from changing the policy adaptively. Given a finite-horizon MDP with $S$ states, $A$ actions and planning horizon $H$, we design a computational efficient algorithm to achieve near-optimal regret of $\\tilde{O}(\\sqrt{SAH^3K\\ln(1/\\delta)})$\\footnote{$\\tilde{O}(\\cdot)$ hides logarithmic terms of $(S,A,H,K)$} in $K$ episodes using $O\\left(H+\\log_2\\log_2(K) \\right)$ batches with confidence parameter $\\delta$. \n\tTo our best of knowledge, it is the first $\\tilde{O}(\\sqrt{SAH^3K})$ regret bound with $O(H+\\log_2\\log_2(K))$ batch complexity. Meanwhile, we show that to achieve $\\tilde{O}(\\mathrm{poly}(S,A,H)\\sqrt{K})$ regret, the number of batches is at least $\\Omega\\left(H/\\log_A(K)+ \\log_2\\log_2(K) \\right)$, which matches our upper bound up to logarithmic terms.\n\tOur technical contribution are two-fold: 1) a near-optimal design scheme to explore over the unlearned states; 2) an computational efficient algorithm to explore certain directions with an approximated transition model.ion model."}}
{"id": "DMkdzO--w24", "cdate": 1621629793646, "mdate": null, "content": {"title": "Improved Variance-Aware Confidence Sets for Linear Bandits and Linear Mixture MDP", "abstract": "This paper presents new \\emph{variance-aware} confidence sets for linear bandits and linear mixture Markov Decision Processes (MDPs).\nWith the new confidence sets, we obtain the follow regret bounds:\n\nFor linear bandits, we obtain an $\\widetilde{O}(\\mathrm{poly}(d)\\sqrt{1 + \\sum_{k=1}^{K}\\sigma_k^2})$ data-dependent regret bound, where $d$ is the feature dimension, $K$ is the number of rounds, and $\\sigma_k^2$ is the \\emph{unknown} variance of the reward at the $k$-th round. This is the first regret bound that only scales with the variance and the dimension but \\emph{no explicit polynomial dependency on $K$}.\nWhen variances are small, this bound can be significantly smaller than the $\\widetilde{\\Theta}\\left(d\\sqrt{K}\\right)$ worst-case regret bound.\nFor linear mixture MDPs, we obtain an $\\widetilde{O}(\\mathrm{poly}(d, \\log H)\\sqrt{K})$ regret bound, where $d$ is the number of base models, $K$ is the number of episodes, and $H$ is the planning horizon. \nThis is the first regret bound that only scales \\emph{logarithmically} with $H$ in the reinforcement learning with linear function approximation setting, thus \\emph{exponentially improving} existing results, and resolving an open problem in \\citep{zhou2020nearly}.\n\nWe develop three technical ideas that may be of independent interest:\n1) applications of the peeling technique to both the input norm and the variance magnitude, 2) a recursion-based estimator for the variance, and 3) a new convex potential lemma that generalizes the seminal elliptical potential lemma."}}
{"id": "rkgCIESlLr", "cdate": 1567802453745, "mdate": null, "content": {"title": "Regret Minimization for Reinforcement Learning by Evaluating the Optimal Bias Function", "abstract": "We present an algorithm based on the \\emph{Optimism in the Face of Uncertainty} (OFU) principle which is able to learn Reinforcement Learning (RL) modeled by Markov decision process (MDP) with finite state-action space efficiently.  By evaluating the state-pair difference of the optimal bias function $h^{*}$, the proposed algorithm achieves a regret bound of $\\tilde{O}(\\sqrt{SATH})$\\footnote{The symbol $\\tilde{O}$ means $O$ with log factors ignored. } for MDP with S states and A actions, in the case that an upper bound $H$ on the span of $h^{*}$, i.e., $sp(h^{*})$ is known.  This result outperforms the best previous regret bounds $\\tilde{O}(HS\\sqrt{AT})$\\cite{bartlett2009regal} by a factor of $\\sqrt{SH}$.  Furthermore, this regret bound matches the lower bound of $\\Omega(\\sqrt{SATH})$\\cite{jaksch2010near} up to a logarithmic factor. As a consequence,  we show that there is a near optimal regret bound of $\\tilde{O}(\\sqrt{DSAT})$ for MDPs with finite diameter $D$ compared to the lower bound of $\\Omega(\\sqrt{DSAT})$\\cite{jaksch2010near}. "}}
