{"id": "qbIfsfeEf9l", "cdate": 1687343291209, "mdate": 1687343291209, "content": {"title": "XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages", "abstract": "Data scarcity is a crucial issue for the development of highly multilingual NLP systems. Yet\nfor many under-represented languages (ULs)\u2014\nlanguages for which NLP research is particularly far behind in meeting user needs\u2014it is\nfeasible to annotate small amounts of data. Motivated by this, we propose XTREME-UP, a\nbenchmark defined by: its focus on the scarcedata scenario rather than zero-shot; its focus on\nuser-centric tasks\u2014tasks with broad adoption\nby speakers of high-resource languages; and its\nfocus on under-represented languages where\nthis scarce-data scenario tends to be most realistic. XTREME-UP evaluates the capabilities of\nlanguage models across 88 under-represented\nlanguages over 9 key user-centric technologies\nincluding ASR, OCR, MT, and information access tasks that are of general utility. We create\nnew datasets for OCR, autocomplete, semantic\nparsing, and transliteration, and build on and refine existing datasets for other tasks. XTREMEUP provides methodology for evaluating many\nmodeling scenarios including text-only, multimodal (vision, audio, and text), supervised parameter tuning, and in-context learning.1 We\nevaluate commonly used models on the benchmark. We release all code and scripts to train\nand evaluate models.2\\"}}
{"id": "Y33JsvNrn1o", "cdate": 1676170031161, "mdate": null, "content": {"title": "CrysGNN : Distilling pre-trained knowledge to enhance property prediction for crystalline materials.", "abstract": "In recent years, graph neural network (GNN) based approaches have emerged as a\npowerful technique to encode complex topological structure of crystal materials in\nan enriched representation space. These models are often supervised in nature and\nusing the property-specific training data, learn relationship between crystal structure\nand different properties like formation energy, bandgap, bulk modulus, etc. Most\nof these methods require a huge amount of property-tagged data to train the system\nwhich may not be available for different properties. However, there is an availability\nof a huge amount of crystal data with its chemical composition and structural bonds.\nTo leverage these untapped data, this paper presents CrysGNN, a new pre-trained\nGNN framework for crystalline materials, which captures both node and graph\nlevel structural information of crystal graphs using a huge amount of unlabelled\nmaterial data. Further, we extract distilled knowledge from CrysGNN and inject\ninto different state of the art property predictors to enhance their property prediction\naccuracy. We conduct extensive experiments to show that with distilled knowledge\nfrom the pre-trained model, all the SOTA algorithms are able to outperform their\nown vanilla version with good margins. We also observe that the distillation process\nprovides a significant improvement over the conventional approach of finetuning\nthe pre-trained model."}}
{"id": "LSh1MP8eM7E", "cdate": 1672531200000, "mdate": 1685725365792, "content": {"title": "XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages", "abstract": "Data scarcity is a crucial issue for the development of highly multilingual NLP systems. Yet for many under-represented languages (ULs) -- languages for which NLP re-search is particularly far behind in meeting user needs -- it is feasible to annotate small amounts of data. Motivated by this, we propose XTREME-UP, a benchmark defined by: its focus on the scarce-data scenario rather than zero-shot; its focus on user-centric tasks -- tasks with broad adoption by speakers of high-resource languages; and its focus on under-represented languages where this scarce-data scenario tends to be most realistic. XTREME-UP evaluates the capabilities of language models across 88 under-represented languages over 9 key user-centric technologies including ASR, OCR, MT, and information access tasks that are of general utility. We create new datasets for OCR, autocomplete, semantic parsing, and transliteration, and build on and refine existing datasets for other tasks. XTREME-UP provides methodology for evaluating many modeling scenarios including text-only, multi-modal (vision, audio, and text),supervised parameter tuning, and in-context learning. We evaluate commonly used models on the benchmark. We release all code and scripts to train and evaluate models"}}
{"id": "r6wZ04Cidb9", "cdate": 1609459200000, "mdate": 1646997046128, "content": {"title": "Few-shot Controllable Style Transfer for Low-Resource Settings: A Study in Indian Languages", "abstract": "Style transfer is the task of rewriting a sentence into a target style while approximately preserving content. While most prior literature assumes access to a large style-labelled corpus, recent work (Riley et al. 2021) has attempted \"few-shot\" style transfer using only 3-10 sentences at inference for style extraction. In this work we study a relevant low-resource setting: style transfer for languages where no style-labelled corpora are available. We notice that existing few-shot methods perform this task poorly, often copying inputs verbatim. We push the state-of-the-art for few-shot style transfer with a new method modeling the stylistic difference between paraphrases. When compared to prior work, our model achieves 2-3x better performance in formality transfer and code-mixing addition across seven languages. Moreover, our method is better at controlling the style transfer magnitude using an input scalar knob. We report promising qualitative results for several attribute transfer tasks (sentiment transfer, simplification, gender neutralization, text anonymization) all without retraining the model. Finally, we find model evaluation to be difficult due to the lack of datasets and metrics for many languages. To facilitate future research we crowdsource formality annotations for 4000 sentence pairs in four Indic languages, and use this data to design our automatic evaluations."}}
{"id": "HduE0ECjdZ5", "cdate": 1609459200000, "mdate": 1646997046341, "content": {"title": "A Data Bootstrapping Recipe for Low Resource Multilingual Relation Classification", "abstract": "Relation classification (sometimes called 'extraction') requires trustworthy datasets for fine-tuning large language models, as well as for evaluation. Data collection is challenging for Indian languages, because they are syntactically and morphologically diverse, as well as different from resource-rich languages like English. Despite recent interest in deep generative models for Indian languages, relation classification is still not well served by public data sets. In response, we present IndoRE, a dataset with 21K entity and relation tagged gold sentences in three Indian languages, plus English. We start with a multilingual BERT (mBERT) based system that captures entity span positions and type information and provides competitive monolingual relation classification. Using this system, we explore and compare transfer mechanisms between languages. In particular, we study the accuracy efficiency tradeoff between expensive gold instances vs. translated and aligned 'silver' instances. We release the dataset for future research."}}
{"id": "Bu_bR4As_-q", "cdate": 1609459200000, "mdate": 1646997046129, "content": {"title": "A Hierarchical VAE for Calibrating Attributes while Generating Text using Normalizing Flow", "abstract": "Bidisha Samanta, Mohit Agrawal, NIloy Ganguly. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021."}}
{"id": "SAwHhu9w7HJ", "cdate": 1598675745101, "mdate": null, "content": {"title": "A deep generative model for code-switched text", "abstract": "Code-switching, the interleaving of two or more languages within a sentence or discourse is pervasive in multilingual societies. Accurate language models for code-switched text are critical for NLP tasks. State-of-the-art data-intensive neural language models are difficult to train well from scarce language-labeled code-switched text. A potential solution is to use deep generative models to synthesize large volumes of realistic code-switched text. Although generative adversarial networks and variational autoencoders can synthesize plausible monolingual text from continuous latent space, they cannot adequately address code-switched text, owing to their informal style and complex interplay between the constituent languages. We introduce VACS, a novel variational autoencoder architecture specifically tailored to code-switching phenomena. VACS encodes to and decodes from a two-level hierarchical representation, which models syntactic contextual signals in the lower level, and language switching signals in the upper layer. Sampling representations from the prior and decoding them produced well-formed, diverse code-switched sentences. Extensive experiments show that using synthetic code-switched text with natural monolingual data results in significant (33.32%) drop in perplexity."}}
{"id": "SRGbCECsO-c", "cdate": 1577836800000, "mdate": 1646997046119, "content": {"title": "NEVAE: A Deep Generative Model for Molecular Graphs", "abstract": "Deep generative models have been praised for their ability to learn smooth latent representations of images, text, and audio, which can then be used to generate new, plausible data. Motivated by these success stories, there has been a surge of interest in developing deep generative models for automated molecule design. However, these models face several difficulties due to the unique characteristics of molecular graphs\u2014their underlying structure is not Euclidean or grid-like, they remain isomorphic under permutation of the nodes\u2019 labels, and they come with a different number of nodes and edges. In this paper, we first propose a novel variational autoencoder for molecular graphs, whose encoder and decoder are specially designed to account for the above properties by means of several technical innovations. Moreover, in contrast with the state of the art, our decoder is able to provide the spatial coordinates of the atoms of the molecules it generates. Then, we develop a gradient-based algorithm to optimize the decoder of our model so that it learns to generate molecules that maximize the value of certain property of interest and, given any arbitrary molecule, it is able to optimize the spatial configuration of its atoms for greater stability. Experiments reveal that our variational autoencoder can discover plausible, diverse and novel molecules more effectively than several state of the art models. Moreover, for several properties of interest, our optimized decoder is able to identify molecules with property values 121% higher than those identified by several state of the art methods based on Bayesian optimization and reinforcement learning."}}
{"id": "rmYbvkzgOpS", "cdate": 1546300800000, "mdate": null, "content": {"title": "NeVAE: A Deep Generative Model for Molecular Graphs.", "abstract": "Deep generative models have been praised for their ability to learn smooth latent representation of images, text, and audio, which can then be used to generate new, plausible data. However, current generative models are unable to work with molecular graphs due to their unique characteristics\u2014their underlying structure is not Euclidean or grid-like, they remain isomorphic under permutation of the nodes labels, and they come with a different number of nodes and edges. In this paper, we propose NeVAE, a novel variational autoencoder for molecular graphs, whose encoder and decoder are specially designed to account for the above properties by means of several technical innovations. In addition, by using masking, the decoder is able to guarantee a set of valid properties in the generated molecules. Experiments reveal that our model can discover plausible, diverse and novel molecules more effectively than several state of the art methods. Moreover, by utilizing Bayesian optimization over the continuous latent representation of molecules our model finds, we can also find molecules that maximize certain desirable properties more effectively than alternatives."}}
{"id": "rVxZ0N0ou-5", "cdate": 1546300800000, "mdate": 1646997046132, "content": {"title": "A Deep Generative Model for Code-Switched Text", "abstract": "Code-switching, the interleaving of two or more languages within a sentence or discourse is pervasive in multilingual societies. Accurate language models for code-switched text are critical for NLP tasks. State-of-the-art data-intensive neural language models are difficult to train well from scarce language-labeled code-switched text. A potential solution is to use deep generative models to synthesize large volumes of realistic code-switched text. Although generative adversarial networks and variational autoencoders can synthesize plausible monolingual text from continuous latent space, they cannot adequately address code-switched text, owing to their informal style and complex interplay between the constituent languages. We introduce VACS, a novel variational autoencoder architecture specifically tailored to code-switching phenomena. VACS encodes to and decodes from a two-level hierarchical representation, which models syntactic contextual signals in the lower level, and language switching signals in the upper layer. Sampling representations from the prior and decoding them produced well-formed, diverse code-switched sentences. Extensive experiments show that using synthetic code-switched text with natural monolingual data results in significant (33.06%) drop in perplexity."}}
