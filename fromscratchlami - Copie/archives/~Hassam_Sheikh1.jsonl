{"id": "SUI7tllPw7q", "cdate": 1653438350552, "mdate": 1653438350552, "content": {"title": "DNS: Determinantal Point Process Based Neural Network Sampler for Ensemble Reinforcement Learning", "abstract": "The application of an ensemble of neural networks is becoming an imminent tool for advancing state-of-the-art deep reinforcement learning\nalgorithms. However, training these large numbers of neural networks in the ensemble has an\nexceedingly high computation cost which may become a hindrance in training large-scale systems.\nIn this paper, we propose DNS: a Determinantal\nPoint Process based Neural Network Sampler that\nspecifically uses k-DPP to sample a subset of neural networks for backpropagation at every training step thus significantly reducing the training\ntime and computation cost. We integrated DNS in\nREDQ for continuous control tasks and evaluated\non MuJoCo environments. Our experiments show\nthat DNS augmented REDQ matches the baseline\nREDQ in terms of average cumulative reward and\nachieves this using less than 50% computation\nwhen measured in FLOPS."}}
{"id": "hjd-kcpDpf2", "cdate": 1632875607273, "mdate": null, "content": {"title": "Maximizing Ensemble Diversity in Deep Reinforcement Learning", "abstract": "Modern deep reinforcement learning (DRL) has been successful in solving a range of challenging sequential decision-making problems. Most of these algorithms use an ensemble of neural networks as their backbone structure and benefit from the diversity among the neural networks to achieve optimal results. Unfortunately, the members of the ensemble can converge to the same point either the parametric space or representation space during the training phase, therefore, losing all the leverage of an ensemble. In this paper, we describe Maximize Ensemble Diversity in Reinforcement Learning (MED-RL), a set of regularization methods inspired from the economics and consensus optimization to improve diversity in the ensemble-based deep reinforcement learning methods by encouraging inequality between the networks during training. We integrated MED-RL in five of the most common ensemble-based deep RL algorithms for both continuous and discrete control tasks and evaluated on six Mujoco environments and six Atari games. Our results show that MED-RL augmented algorithms outperform their un-regularized counterparts significantly and in some cases achieved more than 300$\\%$ in performance gains."}}
{"id": "dN_iVr6iNuU", "cdate": 1601308066106, "mdate": null, "content": {"title": "Preventing Value Function Collapse in Ensemble  Q-Learning by Maximizing Representation Diversity", "abstract": "The first deep RL algorithm, DQN, was limited by the overestimation bias of the learned Q-function. Subsequent algorithms proposed techniques to reduce this problem, without fully eliminating it. Recently, the Maxmin and Ensemble Q-learning algorithms used the different estimates provided by ensembles of learners to reduce the bias. Unfortunately, these learners can converge to the same point in the parametric or representation space, falling back to the classic single neural network DQN. In this paper, we describe a regularization technique to maximize diversity in the representation space in these algorithms. We propose and compare five regularization functions inspired from economics theory and consensus optimization. We show that the resulting approach significantly outperforms the Maxmin and Ensemble Q-learning algorithms as well as non-ensemble baselines."}}
{"id": "4CxsUBDQJqv", "cdate": 1601308061412, "mdate": null, "content": {"title": "Learning Intrinsic Symbolic Rewards in Reinforcement Learning", "abstract": "Learning effective policies for sparse objectives is a key challenge in Deep Reinforcement Learning (RL). A common approach is to design task-related dense rewards to improve task learnability. While such rewards are easily interpreted, they rely on heuristics and domain expertise. Alternate approaches that train neural networks to discover dense surrogate rewards avoid heuristics, but are high-dimensional, black-box solutions offering little interpretability. In this paper, we present a method that discovers dense rewards in the form of low-dimensional symbolic trees - thus making them more tractable for analysis. The trees use simple functional operators to map an agent's observations to a scalar reward, which then supervises the policy gradient learning of a neural network policy. We test our method on continuous action spaces in Mujoco and discrete action spaces in Atari and Pygames environments. We show that the discovered dense rewards are an effective signal for an RL policy to solve the benchmark tasks. Notably, we significantly outperform a widely used, contemporary neural-network based reward-discovery algorithm in all environments considered."}}
