{"id": "phc0KisUnS", "cdate": 1669057834154, "mdate": null, "content": {"title": "Deep Generative Multimedia Children's Literature", "abstract": "Artistic work leveraging Machine Learning techniques is an increasingly popular endeavour for those with a creative lean. \nHowever, most work done is in a single domain: text, images, music, etc. In this work, we design a system for a machine learning created multimedia experience, specifically in the genre of children's literature. \nWe detail the process for exclusively using publicly available pretrained deep neural network based models, we present multiple examples of the work our system creates, and we explore the problems associated in this genre of creative work."}}
{"id": "Bk1hklAuZyh", "cdate": 1633790968480, "mdate": null, "content": {"title": "Unsupervised Attribute Alignment for Characterizing Distribution Shift", "abstract": "Detecting and addressing distribution shift is an important task in machine learning. However, most of the machine learning solutions to deal with distribution shift lack the capability to identify the key characteristics of such a shift and present it to humans in an interpretable way. In this work, we propose a novel framework to compare two datasets and identify distribution shifts between the datasets. The key challenge is to identify generative factors of variation, which we refer to as attributes, that characterize the similarities and differences between the datasets. Producing this characterization requires finding a set of attributes that can be aligned between the two datasets and sets that are unique. We address this challenge through a novel approach that performs both attribute discovery and attribute alignment across the two distributions. We evaluate our algorithm's effectiveness at accurately identifying these attributes in two separate experiments, one involving two variants of MNIST and a second experiment involving two versions of dSprites."}}
{"id": "_S7yM35SUCy", "cdate": 1632875757825, "mdate": null, "content": {"title": "Generalizing Cross Entropy Loss with a Beta Proper Composite Loss: An Improved Loss Function for Open Set Recognition", "abstract": "Open set recognition involves identifying data instances encountered during test time that do not belong to known classes in the training set. The majority of recent deep learning approaches to open set recognition use a cross entropy loss to train their networks. Surprisingly, other loss functions are seldom used. In our work, we explore generalizing cross entropy with a Beta loss. This Beta loss is a proper composite loss with a Beta weight function. This weight function adds the flexibility of putting more emphasis on different parts of the observation-conditioned class probability (i.e. $P(Y|X)$) range during training. We show that the flexibility gained through this is Beta loss function produces consistent improvements over cross entropy loss for open set recognition and produces state of the art results relative to recent methods."}}
{"id": "ryxW804FPH", "cdate": 1569439304880, "mdate": null, "content": {"title": "ADAPTING PRETRAINED LANGUAGE MODELS FOR LONG DOCUMENT CLASSIFICATION", "abstract": "Pretrained language models (LMs) have shown excellent results in achieving human like performance on many language tasks. However, the most powerful LMs have one significant drawback: a fixed-sized input. With this constraint, these LMs are unable to utilize the full input of long documents. In this paper, we introduce a new framework to handle documents of arbitrary lengths. We investigate the addition of a recurrent mechanism to extend the input size and utilizing attention to identify the most discriminating segment of the input. We perform extensive validating experiments on patent and Arxiv datasets, both of which have long text. We demonstrate our method significantly outperforms state-of-the-art results reported in recent literature."}}
{"id": "Sy0BfKkvM", "cdate": 1518469701714, "mdate": null, "content": {"title": "Open Set Recognition with Generated Data", "abstract": "In open set recognition tasks, a classifier must label instances of known classes while detecting unknown classes not encountered during training. We propose a framework in which a classifier jointly learns to classify instances of known classes, and to detect unknown classes as an additional \"open set\" class. Training examples for this open set class are synthesized by a generative adversarial network, itself trained only on the known classes. Augmenting the dataset with synthesized open set examples improves upon standard techniques like confidence thresholding."}}
{"id": "ryE5_cbObH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Open Set Learning with Counterfactual Images", "abstract": "In open set recognition, a classifier must label instances of known classes while detecting instances of unknown classes not encountered during training. To detect unknown classes while still generalizing to new instances of existing classes, we introduce a dataset augmentation technique that we call counterfactual image generation. Our approach, based on generative adversarial networks, generates examples that are close to training set examples yet do not belong to any training category. By augmenting training with examples generated by this optimization, we can reformulate open set recognition as classification with one additional class, which includes the set of novel and unknown examples. Our approach outperforms existing open set recognition algorithms on a selection of image classification tasks."}}
