{"id": "8uNe-pd3znj", "cdate": 1672531200000, "mdate": 1684274110776, "content": {"title": "Distilling Cognitive Backdoor Patterns within an Image", "abstract": "This paper proposes a simple method to distill and detect backdoor patterns within an image: \\emph{Cognitive Distillation} (CD). The idea is to extract the \"minimal essence\" from an input image responsible for the model's prediction. CD optimizes an input mask to extract a small pattern from the input image that can lead to the same model output (i.e., logits or deep features). The extracted pattern can help understand the cognitive mechanism of a model on clean vs. backdoor images and is thus called a \\emph{Cognitive Pattern} (CP). Using CD and the distilled CPs, we uncover an interesting phenomenon of backdoor attacks: despite the various forms and sizes of trigger patterns used by different attacks, the CPs of backdoor samples are all surprisingly and suspiciously small. One thus can leverage the learned mask to detect and remove backdoor examples from poisoned training datasets. We conduct extensive experiments to show that CD can robustly detect a wide range of advanced backdoor attacks. We also show that CD can potentially be applied to help detect potential biases from face datasets. Code is available at \\url{https://github.com/HanxunH/CognitiveDistillation}."}}
{"id": "S3D9NLzjnQ5", "cdate": 1663849853216, "mdate": null, "content": {"title": "Distilling Cognitive Backdoor Patterns within an Image", "abstract": "This paper proposes a simple method to distill and detect backdoor patterns within an image: \\emph{Cognitive Distillation} (CD). The idea is to extract the ``minimal essence\" from an input image responsible for the model's prediction. CD optimizes an input mask to extract a small pattern from the input image that can lead to the same model output (i.e., logits or deep features). The extracted pattern can help understand the cognitive mechanism of a model on clean vs. backdoor images and is thus called a \\emph{Cognitive Pattern} (CP). Using CD and the distilled CPs, we uncover an interesting phenomenon of backdoor attacks: despite the various forms and sizes of trigger patterns used by different attacks, the CPs of backdoor samples are all surprisingly and suspiciously small. \nOne thus can leverage the learned mask to detect and remove backdoor examples from poisoned training datasets. \nWe conduct extensive experiments to show that CD can robustly detect a wide range of advanced backdoor attacks.\nWe also show that CD can potentially be applied to help detect potential biases from face datasets.\nCode is available at https://github.com/HanxunH/CognitiveDistillation."}}
{"id": "OdklztJBBYH", "cdate": 1621629741824, "mdate": null, "content": {"title": "Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks", "abstract": "Deep neural networks (DNNs) are known to be vulnerable to adversarial attacks. A range of defense methods have been proposed to train adversarially robust DNNs, among which adversarial training has demonstrated promising results. However, despite preliminary understandings developed for adversarial training, it is still not clear, from the architectural perspective, what configurations can lead to more robust DNNs. In this paper, we address this gap via a comprehensive investigation on the impact of network width and depth on the robustness of adversarially trained DNNs. Specifically, we make the following key observations: 1) more parameters (higher model capacity) does not necessarily help adversarial robustness; 2) reducing capacity at the last stage (the last group of blocks) of the network can actually improve adversarial robustness; and 3) under the same parameter budget, there exists an optimal architectural configuration for adversarial robustness. We also provide a theoretical analysis explaning why such network configuration can help robustness. These architectural insights can help design adversarially robust DNNs. "}}
{"id": "opy2mzYmTvQ", "cdate": 1609459200000, "mdate": 1684274112316, "content": {"title": "Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks", "abstract": "Deep neural networks (DNNs) are known to be vulnerable to adversarial attacks. A range of defense methods have been proposed to train adversarially robust DNNs, among which adversarial training has demonstrated promising results. However, despite preliminary understandings developed for adversarial training, it is still not clear, from the architectural perspective, what configurations can lead to more robust DNNs. In this paper, we address this gap via a comprehensive investigation on the impact of network width and depth on the robustness of adversarially trained DNNs. Specifically, we make the following key observations: 1) more parameters (higher model capacity) does not necessarily help adversarial robustness; 2) reducing capacity at the last stage (the last group of blocks) of the network can actually improve adversarial robustness; and 3) under the same parameter budget, there exists an optimal architectural configuration for adversarial robustness. We also provide a theoretical analysis explaning why such network configuration can help robustness. These architectural insights can help design adversarially robust DNNs."}}
{"id": "jbaATiGjmy0", "cdate": 1609459200000, "mdate": 1652347385317, "content": {"title": "Unlearnable Examples: Making Personal Data Unexploitable", "abstract": "The volume of \"free\" data on the internet has been key to the current success of deep learning. However, it also raises privacy concerns about the unauthorized exploitation of personal data for training commercial models. It is thus crucial to develop methods to prevent unauthorized data exploitation. This paper raises the question: can data be made unlearnable for deep learning models? We present a type of error-minimizing noise that can indeed make training examples unlearnable. Error-minimizing noise is intentionally generated to reduce the error of one or more of the training example(s) close to zero, which can trick the model into believing there is \"nothing\" to learn from these example(s). The noise is restricted to be imperceptible to human eyes, and thus does not affect normal data utility. We empirically verify the effectiveness of error-minimizing noise in both sample-wise and class-wise forms. We also demonstrate its flexibility under extensive experimental settings and practicability in a case study of face recognition. Our work establishes an important \ufb01rst step towards making personal data unexploitable to deep learning models."}}
{"id": "M-CV5PNf37", "cdate": 1609459200000, "mdate": 1684274111425, "content": {"title": "Neural Architecture Search via Combinatorial Multi-Armed Bandit", "abstract": "Neural Architecture Search (NAS) has gained significant popularity as an effective tool for designing high performance deep neural networks (DNNs). NAS can be performed via reinforcement learning, evolutionary algorithms, differentiable architecture search or tree-search methods. While significant progress has been made for both reinforcement learning and differentiable architecture search, tree-search methods have so far failed to achieve comparable accuracy or search efficiency. In this paper, we formulate NAS as a Combinatorial Multi-Armed Bandit (CMAB) problem (CMAB-NAS). This allows the decomposition of a large search space into smaller blocks where tree-search methods can be applied more effectively and efficiently. We further leverage a tree-based method called Nested Monte-Carlo Search to tackle the CMAB-NAS problem. On CIFAR-10, our approach discovers a cell structure that achieves a low error rate that is comparable to the state-of-the-art, using only 0.58 GPU days, which is 20 times faster than current tree-search methods. Moreover, the discovered structure transfers well to large-scale datasets such as ImageNet."}}
{"id": "iAmZUo0DxC0", "cdate": 1601308131239, "mdate": null, "content": {"title": "Unlearnable Examples: Making Personal Data Unexploitable", "abstract": "The volume of \"free\" data on the internet has been key to the current success of deep learning. However, it also raises privacy concerns about the unauthorized exploitation of personal data for training commercial models. It is thus crucial to develop methods to prevent unauthorized data exploitation. This paper raises the question: can data be made unlearnable for deep learning models? We present a type of error-minimizing noise that can indeed make training examples unlearnable. Error-minimizing noise is intentionally generated to reduce the error of one or more of the training example(s) close to zero, which can trick the model into believing there is \"nothing\" to learn from these example(s). The noise is restricted to be imperceptible to human eyes, and thus does not affect normal data utility. We empirically verify the effectiveness of error-minimizing noise in both sample-wise and class-wise forms. We also demonstrate its flexibility under extensive experimental settings and practicability in a case study of face recognition. Our work establishes an important \ufb01rst step towards making personal data unexploitable to deep learning models."}}
{"id": "_S_v6NyK_dE", "cdate": 1577836800000, "mdate": 1684274111998, "content": {"title": "Normalized Loss Functions for Deep Learning with Noisy Labels", "abstract": "Robust loss functions are essential for training accurate deep neural networks (DNNs) in the presence of noisy (incorrect) labels. It has been shown that the commonly used Cross Entropy (CE) loss i..."}}
