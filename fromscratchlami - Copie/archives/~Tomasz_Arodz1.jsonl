{"id": "kXa2Xcl4WJ", "cdate": 1640995200000, "mdate": 1681653602783, "content": {"title": "Real-Valued Group Testing for Quantitative Molecular Assays", "abstract": ""}}
{"id": "YwQddZJ8Xuv", "cdate": 1640995200000, "mdate": 1681653602743, "content": {"title": "Group Testing Matrix Design for PCR Screening with Real-Valued Measurements", "abstract": ""}}
{"id": "ZjGr1tMVbjw", "cdate": 1621629924896, "mdate": null, "content": {"title": "Shapeshifter: a Parameter-efficient Transformer using Factorized Reshaped Matrices", "abstract": "Language models employ a very large number of trainable parameters. Despite being highly overparameterized, these networks often achieve good out-of-sample test performance on the original task and easily fine-tune to related tasks. Recent observations involving, for example, intrinsic dimension of the objective landscape and  the lottery ticket hypothesis, indicate that often training actively involves only a small fraction of the parameter space. Thus, a question remains how large a parameter space needs to be in the first place \u2013- the evidence from recent work on model compression, parameter sharing, factorized representations, and knowledge distillation increasingly shows that models can be made much smaller and still perform well. Here, we focus on factorized representations of matrices that underpin dense, embedding, and self-attention layers. We use low-rank factorized representation of a reshaped and rearranged original matrix to achieve space efficient and expressive linear layers. We prove that stacking such low-rank layers increases their expressiveness, providing theoretical understanding for their effectiveness in deep networks. In Transformer models, our approach leads to more than ten-fold reduction in the number of total trainable parameters, including embedding, attention, and feed-forward layers, with little degradation in on-task performance. The approach operates out-of-the-box,  replacing each parameter matrix with its compact equivalent while maintaining the architecture of the network."}}
{"id": "oZCaOclE-K", "cdate": 1609459200000, "mdate": 1681653602723, "content": {"title": "Quantum semi-supervised kernel learning", "abstract": ""}}
{"id": "a12-LQgmrIf", "cdate": 1609459200000, "mdate": 1681653602693, "content": {"title": "Shapeshifter: a Parameter-efficient Transformer using Factorized Reshaped Matrices", "abstract": ""}}
{"id": "M66wKKjz-Ks", "cdate": 1609459200000, "mdate": 1681653602832, "content": {"title": "Learning Invariance in Deep Neural Networks", "abstract": ""}}
{"id": "4fX26JHbuac", "cdate": 1609459200000, "mdate": 1681653602833, "content": {"title": "PathMEx: Pathway-Based Mutual Exclusivity for Discovering Rare Cancer Driver Mutations", "abstract": ""}}
{"id": "vlcVTDaufN", "cdate": 1601308196773, "mdate": null, "content": {"title": "Differentiable Combinatorial Losses through Generalized Gradients of Linear Programs", "abstract": "Combinatorial problems with linear objective function play a central role in many computer science applications, and efficient algorithms for solving them are well known. However, the solutions to these problems are not differentiable with respect to the parameters specifying the problem instance \u2013 for example, shortest distance between two nodes in a graph is not a differentiable function of graph edge weights. Recently, attempts to integrate combinatorial and, more broadly, convex optimization solvers into gradient-trained models resulted in several approaches for differentiating over the solution vector to the optimization problem. However, in many cases, the interest is in differentiating over only the objective value, not the solution vector, and using existing approaches introduces unnecessary overhead. Here, we show how to perform gradient descent directly over the objective value of the solution to combinatorial problems. We demonstrate advantage of the approach in examples involving sequence-to-sequence modeling using differentiable encoder-decoder architecture with softmax or Gumbel-softmax, and in weakly supervised learning involving a convolutional, residual feed-forward network for image classification.\n"}}
{"id": "XTAkyxELUW7", "cdate": 1577836800000, "mdate": null, "content": {"title": "QuaDMutNetEx: a method for detecting cancer driver genes with low mutation frequency", "abstract": "Background Cancer is caused by genetic mutations, but not all somatic mutations in human DNA drive the emergence or growth of cancers. While many frequently-mutated cancer driver genes have already been identified and are being utilized for diagnostic, prognostic, or therapeutic purposes, identifying driver genes that harbor mutations occurring with low frequency in human cancers is an ongoing endeavor. Typically, mutations that do not confer growth advantage to tumors \u2013 passenger mutations \u2013 dominate the mutation landscape of tumor cell genome, making identification of low-frequency driver mutations a challenge. The leading approach for discovering new putative driver genes involves analyzing patterns of mutations in large cohorts of patients and using statistical methods to discriminate driver from passenger mutations. Results We propose a novel cancer driver gene detection method, QuaDMutNetEx. QuaDMutNetEx discovers cancer drivers with low mutation frequency by giving preference to genes encoding proteins that are connected in human protein-protein interaction networks, and that at the same time show low deviation from the mutual exclusivity pattern that characterizes driver mutations occurring in the same pathway or functional gene group across a cohort of cancer samples. Conclusions Evaluation of QuaDMutNetEx on four different tumor sample datasets show that the proposed method finds biologically-connected sets of low-frequency driver genes, including many genes that are not found if the network connectivity information is not considered. Improved quality and interpretability of the discovered putative driver gene sets compared to existing methods shows that QuaDMutNetEx is a valuable new tool for detecting driver genes. QuaDMutNetEx is available for download from https://github.com/bokhariy/QuaDMutNetEx under the GNU GPLv3 license."}}
{"id": "RxIX0rTWTt", "cdate": 1577836800000, "mdate": 1681653602709, "content": {"title": "word2ket: Space-efficient Word Embeddings inspired by Quantum Entanglement", "abstract": ""}}
