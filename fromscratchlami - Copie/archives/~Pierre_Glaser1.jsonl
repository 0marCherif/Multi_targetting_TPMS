{"id": "bEKo5bsV77U", "cdate": 1676827098737, "mdate": null, "content": {"title": "Fast and Scalable Score-Based Kernel Calibration Tests", "abstract": "We introduce the Kernel Calibration Conditional Stein Discrepancy test (KCCSD test), a nonparametric, kernel-based test for assessing the calibration of probabilistic models with well-defined scores. In contrast to previous methods, our test avoids the need for possibly expensive expectation approximations while providing control over its type-I error. We achieve these improvements by using a new family of kernels for score-based probabilities that can be estimated without probability density samples, and by using a Conditional Goodness of Fit criterion for the KCCSD test's U-statistic. We demonstrate the properties of our test on various synthetic settings.\n"}}
{"id": "gL68u5UuWa", "cdate": 1663850432271, "mdate": null, "content": {"title": "Maximum Likelihood Learning of Energy-Based Models for Simulation-Based Inference", "abstract": "We introduce two Synthetic Likelihood methods for Simulation-Based Inference (SBI), to conduct either amortized or targeted inference from experimental observations when a high-fidelity simulator is available. Both methods learn a Conditional Energy-Based Model (EBM) of the likelihood using synthetic data generated by the simulator, conditioned on parameters drawn from a proposal distribution. The learned likelihood can then be combined with any prior to obtain a posterior estimate, from which samples can be drawn using MCMC. \nOur methods uniquely combine a flexible Energy-Based Model and the minimization of a KL loss: this is in contrast to other synthetic likelihood methods, which either rely on normalizing flows, or minimize score-based objectives; choices that come with known pitfalls. Our first method, Amortized Unnormalized Neural Likelihood Estimation (AUNLE), introduces a tilting trick during training that allows to perform inference using efficient MCMC techniques. Our second method, Sequential UNLE (SUNLE), employs a doubly intractable approach in order to re-use simulation data and improve posterior accuracy for a specific observation. \nWe demonstrate the properties of both methods on a range of synthetic datasets, and apply it to a neuroscience model of the pyloric network in the crab, matching the performance of other synthetic likelihood methods at a fraction of the simulation budget."}}
{"id": "ZBeCVICs1Ua", "cdate": 1621630350672, "mdate": null, "content": {"title": "KALE Flow: A Relaxed KL Gradient Flow for Probabilities with Disjoint Support", "abstract": "We study the gradient flow for a relaxed approximation to the Kullback-Leibler (KL) divergence\nbetween a moving source and a fixed target distribution.\nThis approximation, termed the\nKALE (KL approximate lower-bound estimator), solves a regularized version of\nthe Fenchel dual problem defining the KL over a restricted class of functions.\nWhen using a Reproducing Kernel Hilbert Space (RKHS) to define the function\nclass, we show that the KALE continuously interpolates between the KL and the\nMaximum Mean Discrepancy (MMD). Like the MMD and other Integral Probability\nMetrics, the KALE remains well defined for mutually singular\ndistributions. Nonetheless, the KALE inherits from the limiting KL a greater \nsensitivity to mismatch in the support of the distributions, compared with the MMD. These two properties make the\nKALE gradient flow particularly well suited when the target distribution is supported on a low-dimensional manifold. Under an assumption of sufficient smoothness of the trajectories, we show the global convergence of the KALE flow. We propose a particle implementation of the flow given initial samples from the source and the target distribution, which we use to empirically confirm the KALE's properties."}}
