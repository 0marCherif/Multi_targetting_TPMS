{"id": "7S0eNlTLyh0", "cdate": 1667349148277, "mdate": 1667349148277, "content": {"title": "Diagnosing glaucoma on imbalanced data with self-ensemble dual-curriculum learning", "abstract": "Glaucoma diagnosis often suffers from two types of data imbalances: 1) class imbalance, i.e., the non-glaucoma majority cases occupy most of the data; 2) rare cases, i.e., few cases present the uncommon retinopathy e.g., bayoneting or physiologic cupping. This dual-imbalances make glaucoma diagnosis model easy to be dominated by the majority cases but cannot correctly classify the minority and/or rare ones. In this paper, we propose an adaptive re-balancing strategy in the feature space, Self-Ensemble Dual-Curriculum learning (SEDC), to improve the glaucoma diagnosis on imbalanced data by augmenting feature distribution with feature distilling and feature re-weighting. Firstly, the self-ensembling (SEL) is developed to reinforce the discriminative ability of feature representations for the minority class and rare cases by distilling the features learned from the abundant majority cases. Secondly, the dual-curriculum (DCL) is designed to adaptively re-weight the imbalanced data in the feature space to learn a balanced decision function for accurate glaucoma diagnosis. Benefiting from feature distilling and re-weighting, the proposed SEDC fairly represents fundus images, regardless of the majority or rare cases, by augmenting the feature distribution to obtains the optimal decision boundary for accurate glaucoma diagnosis on the imbalanced dataset. Experimental results on three challenging glaucoma datasets show that our SEDC successfully delivers accurate glaucoma diagnosis by the adaptive re-balancing strategy, with the average mean value of Accuracy 0.9712, Sensitivity 0.9520, Specificity 0.9816, AUC 0.9928, F2-score 0.9547. Ablation and comparison studies demonstrate that our method outperforms state-of-the-art methods and traditional re-balancing strategies. The experiment also shows that the adaptive re-balancing strategy proposed in our method provides a more effective training approach with optimal convergence performance. It endows our SEDC a great advantage to handle the disease diagnosis on imbalanced data distribution."}}
{"id": "zLjAXGQVvU", "cdate": 1640995200000, "mdate": 1668126633002, "content": {"title": "Marginal samples for knowledge distillation", "abstract": ""}}
{"id": "Nih4oZJW2wS", "cdate": 1640995200000, "mdate": 1668126632935, "content": {"title": "Retinal microaneurysm detection based on transformation splicing and multi-context ensemble learning", "abstract": ""}}
{"id": "Iwu251m_k8", "cdate": 1640995200000, "mdate": 1668126632857, "content": {"title": "A Pair-Metamorphosis-Decouple Synthetic Data Scheme for Color Fundus Image Registration", "abstract": ""}}
{"id": "2RyPg10QvI", "cdate": 1640995200000, "mdate": 1668126632997, "content": {"title": "Diagnosing glaucoma on imbalanced data with self-ensemble dual-curriculum learning", "abstract": ""}}
{"id": "jlyRd5eGDEu", "cdate": 1609459200000, "mdate": 1631882676656, "content": {"title": "A refined equilibrium generative adversarial network for retinal vessel segmentation", "abstract": "Highlights \u2022 Enhanced adversarial training promotes segmenting elusive vessels in the fundus image. \u2022 Multi-scale feature refine blocks maintain the high-resolution information. \u2022 The top-to-down feature delivery strategy supplements the detailed representation. \u2022 Self-attention mechanism highlights the informative channels for vessel segmentation. \u2022 The proposed method outperforms state-of-the-arts in segmenting and generalisability. Abstract Objective Retinal vessel morphological parameters are vital indicator for early diagnosis of ophthalmological diseases and cardiovascular events. However, segmentation performance is highly influenced by elusive vessels, especially in low-contrast background and lesion regions. In this work, we present an end-to-end synthetic neural network to strengthen elusive vessels segmentation capability, containing a symmetric equilibrium generative adversarial network (SEGAN), multi-scale features refine blocks (MSFRB), and attention mechanism (AM). Method The proposed network is superior in detail information extraction by maximizing multi-scale features representation. First, SEGAN constructs a symmetric adversarial architecture in which generator is forced to produce more realistic images with local details. Second, MSFRB are devised to optimize the feature merging process, thereby maximally maintaining high resolution information. Finally, the AM is employed to encourage the network to concentrate on discriminative features. Results On public dataset DRIVE, STARE, CHASEDB1, and HRF, we evaluate our network quantitatively and compare it with state-of-the-art works. The ablation experiment shows that SEGAN, MSFRB, and AM both contribute to the desirable performance. Conclusion: The proposed network outperforms the existing methods and effectively functions in elusive vessels segmentation, achieving highest scores in Sensitivity, G-Mean, Precision, and F1-Score while maintaining the top level in other metrics. Significance: The satisfactory performance and computational efficiency offer great potential in clinical retinal vessel segmentation application. Meanwhile, the network could be utilized to extract detail information in other biomedical image computing."}}
{"id": "7FNM4ZDiO_U", "cdate": 1609459200000, "mdate": 1631882676656, "content": {"title": "Robust and discriminative zero-watermark scheme based on invariant features and similarity-based retrieval to protect large-scale DIBR 3D videos", "abstract": "Highlights \u2022 Fuse zero watermark and retrieval techniques to protect large-scale DIBR videos. \u2022 Design robust and discriminative features for precise and reliable protection. \u2022 Design attention-based fusion method to provide flexible copyright protection. Abstract Digital rights management (DRM) of depth-image-based rendering (DIBR) 3D video is an emerging area of research. Existing schemes for DIBR 3D video cause video distortions, are vulnerable to severe signal and geometric attacks, cannot protect 2D frames and depth maps independently, or have difficulty handling large-scale videos. To address these issues, a novel zero-watermark scheme based on invariant features and similarity-based retrieval to protect DIBR 3D video (RZW-SR) is proposed in this study. In RZW-SR, invariant features are extracted to generate master and ownership shares to provide distortion-free, robust and discriminative copyright identification under various attacks. Different from conventional zero-watermark schemes, our proposed scheme stores features and ownership shares correlatively and designs a similarity-based retrieval phase to provide effective solutions for large-scale videos. In addition, flexible mechanisms based on attention-based fusion are designed to protect 2D frames and depth maps, either independently or simultaneously. The experimental results demonstrate that RZW-SR has superior DRM performance compared to existing schemes. First, RZW-SR can obtain the ownership shares relevant to a particular 3D video precisely and reliably for effective copyright identification of large-scale videos. Second, RZW-SR ensures lossless, precise, reliable and flexible copyright identification for 2D frames and depth maps of 3D videos."}}
{"id": "oh0vXoPu3sM", "cdate": 1577836800000, "mdate": 1631882676449, "content": {"title": "Clinical Interpretable Deep Learning Model for Glaucoma Diagnosis", "abstract": "Despite the potential to revolutionise disease diagnosis by performing data-driven classification, clinical interpretability of ConvNet remains challenging. In this paper, a novel clinical interpretable ConvNet architecture is proposed not only for accurate glaucoma diagnosis but also for the more transparent interpretation by highlighting the distinct regions recognised by the network. To the best of our knowledge, this is the first work of providing the interpretable diagnosis of glaucoma with the popular deep learning model. We propose a novel scheme for aggregating features from different scales to promote the performance of glaucoma diagnosis, which we refer to as M-LAP. Moreover, by modelling the correspondence from binary diagnosis information to the spatial pixels, the proposed scheme generates glaucoma activations, which bridge the gap between global semantical diagnosis and precise location. In contrast to previous works, it can discover the distinguish local regions in fundus images as evidence for clinical interpretable glaucoma diagnosis. Experimental results, performed on the challenging ORIGA datasets, show that our method on glaucoma diagnosis outperforms state-of-the-art methods with the highest AUC (0.88). Remarkably, the extensive results, optic disc segmentation (dice of 0.9) and local disease focus localization based on the evidence map, demonstrate the effectiveness of our methods on clinical interpretability."}}
{"id": "kJDk_T3udMX", "cdate": 1577836800000, "mdate": 1631882676452, "content": {"title": "Direct Cup-to-Disc Ratio Estimation for Glaucoma Screening via Semi-Supervised Learning", "abstract": "Glaucoma is a chronic eye disease that leads to irreversible vision loss. The Cup-to-Disc Ratio (CDR) serves as the most important indicator for glaucoma screening and plays a significant role in clinical screening and early diagnosis of glaucoma. In general, obtaining CDR is subjected to measuring on manually or automatically segmented optic disc and cup. Despite great efforts have been devoted, obtaining CDR values automatically with high accuracy and robustness is still a great challenge due to the heavy overlap between optic cup and neuroretinal rim regions. In this paper, a direct CDR estimation method is proposed based on the well-designed semi-supervised learning scheme, in which CDR estimation is formulated as a general regression problem while optic disc/cup segmentation is cancelled. The method directly regresses CDR value based on the feature representation of optic nerve head via deep learning technique while bypassing intermediate segmentation. The scheme is a two-stage cascaded approach comprised of two phases: unsupervised feature representation of fundus image with a convolutional neural networks (MFPPNet) and CDR value regression by random forest regressor. The proposed scheme is validated on the challenging glaucoma dataset Direct-CSU and public ORIGA, and the experimental results demonstrate that our method can achieve a lower average CDR error of 0.0563 and a higher correlation of around 0.726 with measurement before manual segmentation of optic disc/cup by human experts. Our estimated CDR values are also tested for glaucoma screening, which achieves the areas under curve of 0.905 on dataset of 421 fundus images. The experiments show that the proposed method is capable of state-of-the-art CDR estimation and satisfactory glaucoma screening with calculated CDR value."}}
{"id": "gbsmxSFRHe", "cdate": 1577836800000, "mdate": 1631882676659, "content": {"title": "Non-rigid retinal image registration using an unsupervised structure-driven regression network", "abstract": "Retinal image registration is clinically significant to help clinicians obtain more complete details of the retinal structure by correlating the properties of the retina. However, existing methods suffer from great challenges due to time-consuming optimization and lack of ground truth. In this paper, we propose an unsupervised learning framework for non-rigid retinal image registration, which directly learns the mapping from a retinal image pair to their corresponding deformation field without any supervision such as ground truth registration fields. Specifically, we formulate the complex mapping as a parameterized deformation function, which can be represented and optimized by a deep neural network. Furthermore, the Structure-Driven Regression Network (SDRN) framework is applied to compute the multi-scale similarity combined with contextual structures (e.g., vessel distribution, optic disk appearance, and edge information) to guide the end-to-end learning procedure more effectively with unlabeled data. Given a new pair of images, our method can quickly register images by directly evaluating the parametric function using the learned parameters, which runs faster than traditional registration algorithms. Experimental results, performed on the public challenging dataset (FIRE), show that our method achieves an average Dice similarity coefficient (DSC) of 0.753 with short execution times (0.021\u00a0s), which is more accurate and robust than existing approaches and promises to significantly speed up retinal image analysis and processing."}}
