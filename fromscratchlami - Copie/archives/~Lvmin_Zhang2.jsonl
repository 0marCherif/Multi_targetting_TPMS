{"id": "-iG0CZgpn3", "cdate": 1672531200000, "mdate": 1683654120163, "content": {"title": "Adding Conditional Control to Text-to-Image Diffusion Models", "abstract": "We present a neural network structure, ControlNet, to control pretrained large diffusion models to support additional input conditions. The ControlNet learns task-specific conditions in an end-to-end way, and the learning is robust even when the training dataset is small (< 50k). Moreover, training a ControlNet is as fast as fine-tuning a diffusion model, and the model can be trained on a personal devices. Alternatively, if powerful computation clusters are available, the model can scale to large amounts (millions to billions) of data. We report that large diffusion models like Stable Diffusion can be augmented with ControlNets to enable conditional inputs like edge maps, segmentation maps, keypoints, etc. This may enrich the methods to control large diffusion models and further facilitate related applications."}}
{"id": "gDX8NGKfnAI", "cdate": 1640995200000, "mdate": 1683654120147, "content": {"title": "Sprite-from-Sprite: Cartoon Animation Decomposition with Self-supervised Sprite Estimation", "abstract": "We present an approach to decompose cartoon animation videos into a set of \"sprites\" --- the basic units of digital cartoons that depict the contents and transforms of each animated object. The sprites in real-world cartoons are unique: artists may draw arbitrary sprite animations for expressiveness, where the animated content is often complicated, irregular, and challenging; alternatively, artists may also reduce their workload by tweening and adjusting sprites, or even reuse static sprites, in which case the transformations are relatively regular and simple. Based on these observations, we propose a sprite decomposition framework using Pixel Multilayer Perceptrons (Pixel MLPs) where the estimation of each sprite is conditioned on and guided by all other sprites. In this way, once those relatively regular and simple sprites are resolved, the decomposition of the remaining \"challenging\" sprites can simplified and eased with the guidance of other sprites. We call this method \"sprite-from-sprite\" cartoon decomposition. We study ablative architectures of our framework, and the user study demonstrates that our results are the most preferred ones in 19/20 cases."}}
{"id": "dsDrfWrqZ9", "cdate": 1609459200000, "mdate": 1668639678601, "content": {"title": "Screenshots from Screen Photography", "abstract": "Screenshot is a frequently used tool in our daily life, while the screenshot capturing techniques are not much discussed in computer graphics and image processing researches. Capturing a screenshot is not always as easy as it seems. Firstly, the target devices for screenshot capturing must have screenshot software installed or featured in their operating systems. Secondly, the users must have input access to control the screenshot software within the target devices. Thirdly, the target devices must have Internet access or other hardware interfaces (such as USB ports) so that the users can take their screenshots out. When these requirements are not met, people often need to use their smartphones to take photographs in front of the screens as a substitute of screenshots. This allows direct sharing of the screen content, but the fidelity of the obtained content is apparently not as good as software screenshots. Might we be able to achieve a computer graphic solution to directly convert a screen photography to a screenshot, which looks like as if it was taken using software?"}}
{"id": "YEKAf74J5Z", "cdate": 1609459200000, "mdate": 1683654120125, "content": {"title": "SmartShadow: Artistic Shadow Drawing Tool for Line Drawings", "abstract": "SmartShadow is a deep learning application for digital painting artists to draw shadows on line drawings, with three proposed tools. (1) Shadow brush: artists can draw scribbles to coarsely indicate the areas inside or outside their wanted shadows, and the application will generate the shadows in real-time. (2) Shadow boundary brush: this brush can precisely control the boundary of any specific shadow. (3) Global shadow generator: this tool can estimate the global shadow direction from input brush scribbles, and then consistently propagate local shadows to the entire image. These three tools can not only speed up the shadow drawing process (by 3.1\u00d7 as experiments validate), but also allow for the flexibility to achieve various shadow effects and facilitate richer artistic creations. To this end, we train Convolutional Neural Networks (CNNs) with a collected large-scale dataset of both real and synthesized data, and especially, we collect 1670 shadow samples drawn by real artists. Both qualitative analysis and user study show that our approach can generate high-quality shadows that are practically usable in the daily works of digital painting artists. We present 30 additional results and 15 visual comparisons in the supplementary materiel."}}
{"id": "Q5lG4stEPE", "cdate": 1609459200000, "mdate": 1668639678745, "content": {"title": "User-Guided Line Art Flat Filling With Split Filling Mechanism", "abstract": "Flat filling is a critical step in digital artistic content creation with the objective of filling line arts with flat colors. We present a deep learning framework for user-guided line art flat filling that can compute the \"influence areas\" of the user color scribbles, i.e., the areas where the user scribbles should propagate and influence. This framework explicitly controls such scribble influence areas for artists to manipulate the colors of image details and avoid color leakage/contamination between scribbles, and simultaneously, leverages data-driven color generation to facilitate content creation. This framework is based on a Split Filling Mechanism (SFM), which first splits the user scribbles into individual groups and then independently processes the colors and influence areas of each group with a Convolutional Neural Network (CNN). Learned from more than a million illustrations, the framework can estimate the scribble influence areas in a content-aware manner, and can smartly generate visually pleasing colors to assist the daily works of artists. We show that our proposed framework is easy to use, allowing even amateurs to obtain professional-quality results on a wide variety of line arts."}}
{"id": "Hg-NK_e4py", "cdate": 1609459200000, "mdate": 1682350647360, "content": {"title": "Generating Manga From Illustrations via Mimicking Manga Creation Workflow", "abstract": "We present a framework to generate manga from digital illustrations. In professional mange studios, the manga create workflow consists of three key steps: (1) Artists use line drawings to delineate the structural outlines in manga storyboards. (2) Artists apply several types of regular screentones to render the shading, occlusion, and object materials. (3) Artists selectively paste irregular screen textures onto the canvas to achieve various background layouts or special effects. Motivated by this workflow, we propose a data-driven framework to convert a digital illustration into three corresponding components: manga line drawing, regular screentone, and irregular screen texture. These components can be directly composed into manga images and can be further retouched for more plentiful manga creations. To this end, we create a large-scale dataset with these three components annotated by artists in a human-in-the-loop manner. We conduct both perceptual user study and qualitative evaluation of the generated manga, and observe that our generated image layers for these three components are practically usable in the daily works of manga artists. We provide 60 qualitative results and 15 additional comparisons in the supplementary material. We will make our presented manga dataset publicly available to assist related applications."}}
{"id": "xP9wwReagZs", "cdate": 1577836800000, "mdate": 1668639678562, "content": {"title": "Erasing Appearance Preservation in Optimization-Based Smoothing", "abstract": "Optimization-based Image smoothing is routinely formulated as the game between a smoothing energy and an appearance preservation energy. Achieving adequate smoothing is a fundamental goal of these Image smoothing algorithms. We show that partially \u201cerasing\u201d the appearance preservation facilitate adequate Image smoothing. In this paper, we call this manipulation as Erasing Appearance Preservation (EAP). We conduct an user study, allowing users to indicate the \u201cerasing\u201d positions by drawing scribbles interactively, to verify the correctness and effectiveness of EAP. We observe the characteristics of human-indicated \u201cerasing\u201d positions, and then formulate a simple and effective 0-1 knapsack to automatically synthesize the \u201cerasing\u201d positions. We test our synthesized erasing positions in a majority of Image smoothing methods. Experimental results and large-scale perceptual human judgments show that the EAP solution tends to encourage the pattern separation or elimination capabilities of Image smoothing algorithms. We further study the performance of the EAP solution in many image decomposition problems to decompose textures, shadows, and the challenging specular reflections. We also present examinations of diversiform image manipulation applications like texture removal, retexturing, intrinsic decomposition, layer extraction, recoloring, material manipulation, etc. Due to the widespread applicability of Image smoothing, the EAP is also likely to be used in more image editing applications."}}
{"id": "x-Kh1b3R7Sk", "cdate": 1577836800000, "mdate": 1683654120127, "content": {"title": "DanbooRegion: An Illustration Region Dataset", "abstract": "Region is a fundamental element of various cartoon animation techniques and artistic painting applications. Achieving satisfactory region is essential to the success of these techniques. Motivated to assist diversiform region-based cartoon applications, we invite artists to annotate regions for in-the-wild cartoon images with several application-oriented goals: (1) To assist image-based cartoon rendering, relighting, and cartoon intrinsic decomposition literature, artists identify object outlines and eliminate lighting-and-shadow boundaries. (2) To assist cartoon inking tools, cartoon structure extraction applications, and cartoon texture processing techniques, artists clean-up texture or deformation patterns and emphasize cartoon structural boundary lines. (3) To assist region-based cartoon digitalization, clip-art vectorization, and animation tracking applications, artists inpaint and reconstruct broken or blurred regions in cartoon images. Given the typicality of these involved applications, this dataset is also likely to be used in other cartoon techniques. We detail the challenges in achieving this dataset and present a human-in-the-loop workflow namely Feasibility-based Assignment Recommendation (FAR) to enable large-scale annotating. The FAR tends to reduce artist trails-and-errors and encourage their enthusiasm during annotating. Finally, we present a dataset that contains a large number of artistic region compositions paired with corresponding cartoon illustrations. We also invite multiple professional artists to assure the quality of each annotation."}}
{"id": "Pv1M7VG08OM", "cdate": 1577836800000, "mdate": 1666688151638, "content": {"title": "Generating Digital Painting Lighting Effects via RGB-space Geometry", "abstract": "We present an algorithm to generate digital painting lighting effects from a single image. Our algorithm is based on a key observation: Artists use many overlapping strokes to paint lighting effects, i.e., pixels with dense stroke history tend to gather more illumination strokes. Based on this observation, we design an algorithm to both estimate the density of strokes in a digital painting using color geometry and then generate novel lighting effects by mimicking artists\u2019 coarse-to-fine workflow. Coarse lighting effects are first generated using a wave transform and then retouched according to the stroke density of the original illustrations into usable lighting effects. Our algorithm is content-aware, with generated lighting effects naturally adapting to image structures, and can be used as an interactive tool to simplify current labor-intensive workflows for generating lighting effects for digital and matte paintings. In addition, our algorithm can also produce usable lighting effects for photographs or three-dimensional rendered images. We evaluate our approach with both an in-depth qualitative and a quantitative analysis that includes a perceptual user study. Results show that our proposed approach is not only able to produce favorable lighting effects with respect to existing approaches, but also that it is able to significantly reduce the needed interaction time."}}
{"id": "BkXCS5sK-U0", "cdate": 1514764800000, "mdate": 1668639678715, "content": {"title": "Two-stage sketch colorization", "abstract": "Sketch or line art colorization is a research field with significant market demand. Different from photo colorization which strongly relies on texture information, sketch colorization is more challenging as sketches may not have texture. Even worse, color, texture, and gradient have to be generated from the abstract sketch lines. In this paper, we propose a semi-automatic learning-based framework to colorize sketches with proper color, texture as well as gradient. Our framework consists of two stages. In the first drafting stage, our model guesses color regions and splashes a rich variety of colors over the sketch to obtain a color draft. In the second refinement stage, it detects the unnatural colors and artifacts, and try to fix and refine the result. Comparing to existing approaches, this two-stage design effectively divides the complex colorization task into two simpler and goal-clearer subtasks. This eases the learning and raises the quality of colorization. Our model resolves the artifacts such as water-color blurring, color distortion, and dull textures. We build an interactive software based on our model for evaluation. Users can iteratively edit and refine the colorization. We evaluate our learning model and the interactive system through an extensive user study. Statistics shows that our method outperforms the state-of-art techniques and industrial applications in several aspects including, the visual quality, the ability of user control, user experience, and other metrics."}}
