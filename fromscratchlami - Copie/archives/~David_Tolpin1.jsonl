{"id": "611AnxlxcTg", "cdate": 1600140828195, "mdate": null, "content": {"title": "Deployable probabilistic programming", "abstract": "We propose design guidelines for a probabilistic programming\n  facility suitable for deployment as a part of a production\n  software system. As a reference implementation, we introduce\n  Infergo, a probabilistic programming facility for Go, a modern\n  programming language of choice for server-side software\n  development. We argue that a similar probabilistic\n  programming facility can be added to most modern general-purpose\n  programming languages.\n\n  Probabilistic programming enables automatic tuning of\n  program parameters and algorithmic decision making through\n  probabilistic inference based on the data. To facilitate\n  addition of probabilistic programming capabilities to other\n  programming languages, we share implementation choices and\n  techniques employed in development of Infergo. We illustrate\n  applicability of Infergo to various use cases on case\n  studies, and evaluate Infergo's performance on several\n  benchmarks, comparing Infergo to dedicated inference-centric\n  probabilistic programming frameworks."}}
{"id": "ts8kWqwBBZo", "cdate": 1577836800000, "mdate": null, "content": {"title": "Stochastically Differentiable Probabilistic Programs", "abstract": "Probabilistic programs with mixed support (both continuous and discrete latent random variables) commonly appear in many probabilistic programming systems (PPSs). However, the existence of the discrete random variables prohibits many basic gradient-based inference engines, which makes the inference procedure on such models particularly challenging. Existing PPSs either require the user to manually marginalize out the discrete variables or to perform a composing inference by running inference separately on discrete and continuous variables. The former is infeasible in most cases whereas the latter has some fundamental shortcomings. We present a novel approach to run inference efficiently and robustly in such programs using stochastic gradient Markov Chain Monte Carlo family of algorithms. We compare our stochastic gradient-based inference algorithm against conventional baselines in several important cases of probabilistic programs with mixed support, and demonstrate that it outperforms existing composing inference baselines and works almost as well as inference in marginalized versions of the programs, but with less programming effort and at a lower computation cost."}}
{"id": "Mn78C0dR8pk", "cdate": 1577836800000, "mdate": null, "content": {"title": "Stochastic Probabilistic Programs", "abstract": "We introduce the notion of a stochastic probabilistic program and present a reference implementation of a probabilistic programming facility supporting specification of stochastic probabilistic programs and inference in them. Stochastic probabilistic programs allow straightforward specification and efficient inference in models with nuisance parameters, noise, and nondeterminism. We give several examples of stochastic probabilistic programs, and compare the programs with corresponding deterministic probabilistic programs in terms of model specification and inference. We conclude with discussion of open research topics and related work."}}
{"id": "VzWlysbq-g", "cdate": 1546300800000, "mdate": null, "content": {"title": "Population anomaly detection through deep gaussianization", "abstract": "We introduce an algorithmic method for population anomaly detection based on gaussianization through an adversarial autoencoder. This method is applicable to detection of 'soft' anomalies in arbitrarily distributed highly-dimensional data. A soft, or population, anomaly is characterized by a shift in the distribution of the data set, where certain elements appear with higher probability than anticipated. Such anomalies must be detected by considering a sufficiently large sample set rather than a single sample. Applications include, but not limited to, payment fraud trends, data exfiltration, disease clusters and epidemics, and social unrests. We evaluate the method on several domains and obtain both quantitative results and qualitative insights."}}
{"id": "95dakaZ2aPM", "cdate": 1546300800000, "mdate": null, "content": {"title": "Warped Input Gaussian Processes for Time Series Forecasting", "abstract": "We introduce a Gaussian process-based model for handling of non-stationarity. The warping is achieved non-parametrically, through imposing a prior on the relative change of distance between subsequent observation inputs. The model allows the use of general gradient optimization algorithms for training and incurs only a small computational overhead on training and prediction. The model finds its applications in forecasting in non-stationary time series with either gradually varying volatility, presence of change points, or a combination thereof. We evaluate the model on synthetic and real-world time series data comparing against both baseline and known state-of-the-art approaches and show that the model exhibits state-of-the-art forecasting performance at a lower implementation and computation cost."}}
{"id": "dk03Q6UTuhM", "cdate": 1514764800000, "mdate": null, "content": {"title": "Population Anomaly Detection through Deep Gaussianization", "abstract": "We introduce an algorithmic method for population anomaly detection based on gaussianization through an adversarial autoencoder. This method is applicable to detection of `soft' anomalies in arbitrarily distributed highly-dimensional data.   A soft, or population, anomaly is characterized by a shift in the distribution of the data set, where certain elements appear with higher probability than anticipated. Such anomalies must be detected by considering a sufficiently large sample set rather than a single sample.   Applications include, but not limited to, payment fraud trends, data exfiltration, disease clusters and epidemics, and social unrests. We evaluate the method on several domains and obtain both quantitative results and qualitative insights."}}
{"id": "ZNSyoKa4M_4", "cdate": 1514764800000, "mdate": null, "content": {"title": "A Planning Approach to Monitoring Computer Programs' Behavior", "abstract": "We describe a novel approach to monitoring high level behaviors using concepts from AI planning. Our goal is to understand what a program is doing based on its system call trace. This ability is particularly important for detecting malware. We approach this problem by building an abstract model of the operating system using the STRIPS planning language, casting system calls as planning operators. Given a system call trace, we simulate the corresponding operators on our model and by observing the properties of the state reached, we learn about the nature of the original program and its behavior. Thus, unlike most statistical detection methods that focus on syntactic features, our approach is semantic in nature. Therefore, it is more robust against obfuscation techniques used by malware that change the outward appearance of the trace but not its effect. We demonstrate the efficacy of our approach by evaluating it on actual system call traces."}}
{"id": "3Cf3jlCGXfk", "cdate": 1514764800000, "mdate": null, "content": {"title": "Rational deployment of multiple heuristics in optimal state-space search", "abstract": "The obvious way to use several admissible heuristics in searching for an optimal solution is to take their maximum. In this paper, we aim to reduce the time spent on computing heuristics within the context of A \u204e and I D A \u204e . We discuss Lazy A \u204e and Lazy I D A \u204e , variants of A \u204e and I D A \u204e , respectively, where heuristics are evaluated lazily: only when they are essential to a decision to be made in the search process. While these lazy algorithms outperform naive maximization, we can do even better by intelligently deciding when to compute the more expensive heuristic. We present a new rational metareasoning based scheme which decides whether to compute the more expensive heuristics at all, based on a myopic regret estimate. This scheme is used to create rational lazy A \u204e and rational lazy I D A \u204e . We also present different methods for estimating the parameters necessary for making such decisions. An empirical evaluation in several domains supports the theoretical results, and shows that the rational variants, rational lazy A \u204e and rational lazy I D A \u204e , are better than their non-rational counterparts."}}
{"id": "j_-sOSLAYVg", "cdate": 1483228800000, "mdate": null, "content": {"title": "Process monitoring on sequences of system call count vectors", "abstract": "We introduce a methodology for efficient monitoring of processes running on hosts in a corporate network. The methodology is based on collecting streams of system calls produced by all or selected processes on the hosts, and sending them over the network to a monitoring server, where machine learning algorithms are used to identify changes in process behavior due to malicious activity, hardware failures, or software errors. The methodology uses a sequence of system call count vectors as the data format which can handle large and varying volumes of data. Unlike previous approaches, the methodology introduced in this paper is suitable for distributed collection and processing of data in large corporate networks. We evaluate the methodology both in a laboratory setting on a real-life setup and provide statistics characterizing performance and accuracy of the methodology."}}
{"id": "cheI3yH2W0", "cdate": 1483228800000, "mdate": null, "content": {"title": "Session Analysis using Plan Recognition", "abstract": "This paper presents preliminary results of our work with a major financial company, where we try to use methods of plan recognition in order to investigate the interactions of a costumer with the company's online interface. In this paper, we present the first steps of integrating a plan recognition algorithm in a real-world application for detecting and analyzing the interactions of a costumer. It uses a novel approach for plan recognition from bare-bone UI data, which reasons about the plan library at the lowest recognition level in order to define the relevancy of actions in our domain, and then uses it to perform plan recognition. We present preliminary results of inference on three different use-cases modeled by domain experts from the company, and show that this approach manages to decrease the overload of information required from an analyst to evaluate a costumer's session - whether this is a malicious or benign session, whether the intended tasks were completed, and if not - what actions are expected next."}}
