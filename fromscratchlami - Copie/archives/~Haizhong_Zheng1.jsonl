{"id": "QwKvL6wC8Yi", "cdate": 1663850054326, "mdate": null, "content": {"title": "Coverage-centric Coreset Selection for High Pruning Rates", "abstract": "One-shot coreset selection aims to select a representative subset of the training data, given a pruning rate, that can later be used to train future models while retaining high accuracy. State-of-the-art coreset selection methods pick the highest importance examples based on an importance metric and are found to perform well at low pruning rates.  However, at high pruning rates, they suffer from a catastrophic accuracy drop, performing worse than even random sampling. This paper explores the reasons behind this accuracy drop both theoretically and empirically. We first propose a novel metric to measure the coverage of a dataset on a specific distribution by extending the classical geometric set cover problem to a distribution cover problem. This metric helps explain why coresets selected by SOTA methods at high pruning rates perform poorly compared to random sampling because of worse data coverage. We then propose a novel one-shot coreset selection method, Coverage-centric Coreset Selection (CCS), that jointly considers overall data coverage upon a distribution as well as the importance of each example. We evaluate CCS on five datasets and show that, at high pruning rates (e.g., 90%), it achieves significantly better accuracy than previous SOTA methods (e.g., at least 19.56% higher on CIFAR10) as well as random selection (e.g., 7.04% higher on CIFAR10) and comparable accuracy at low pruning rates.  We make our code publicly available at https://github.com/haizhongzheng/Coverage-centric-coreset-selection."}}
{"id": "FWKLTSGcU_W", "cdate": 1577836800000, "mdate": 1668633851653, "content": {"title": "Efficient Adversarial Training With Transferable Adversarial Examples", "abstract": "Adversarial training is an effective defense method to protect classification models against adversarial attacks. However, one limitation of this approach is that it can require orders of magnitude additional training time due to high cost of generating strong adversarial examples during training. In this paper, we first show that there is high transferability between models from neighboring epochs in the same training process, i.e., adversarial examples from one epoch continue to be adversarial in subsequent epochs. Leveraging this property, we propose a novel method, Adversarial Training with Transferable Adversarial Examples (ATTA), that can enhance the robustness of trained models and greatly improve the training efficiency by accumulating adversarial perturbations through epochs. Compared to state-of-the-art adversarial training methods, ATTA enhances adversarial accuracy by up to 7.2% on CIFAR10 and requires 12 14x less training time on MNIST and CIFAR10 datasets with comparable model robustness."}}
{"id": "D9GhThylt6y", "cdate": 1577836800000, "mdate": 1631882029600, "content": {"title": "Understanding and Diagnosing Vulnerability under Adversarial Attacks", "abstract": "Deep Neural Networks (DNNs) are known to be vulnerable to adversarial attacks. Currently, there is no clear insight into how slight perturbations cause such a large difference in classification results and how we can design a more robust model architecture. In this work, we propose a novel interpretability method, InterpretGAN, to generate explanations for features used for classification in latent variables. Interpreting the classification process of adversarial examples exposes how adversarial perturbations influence features layer by layer as well as which features are modified by perturbations. Moreover, we design the first diagnostic method to quantify the vulnerability contributed by each layer, which can be used to identify vulnerable parts of model architectures. The diagnostic results show that the layers introducing more information loss tend to be more vulnerable than other layers. Based on the findings, our evaluation results on MNIST and CIFAR10 datasets suggest that average pooling layers, with lower information loss, are more robust than max pooling layers for the network architectures studied in this paper."}}
{"id": "jVGXaR1dkW2", "cdate": 1546300800000, "mdate": 1631882029607, "content": {"title": "Analyzing the Interpretability Robustness of Self-Explaining Models", "abstract": "Recently, interpretable models called self-explaining models (SEMs) have been proposed with the goal of providing interpretability robustness. We evaluate the interpretability robustness of SEMs and show that explanations provided by SEMs as currently proposed are not robust to adversarial inputs. Specifically, we successfully created adversarial inputs that do not change the model outputs but cause significant changes in the explanations. We find that even though current SEMs use stable co-efficients for mapping explanations to output labels, they do not consider the robustness of the first stage of the model that creates interpretable basis concepts from the input, leading to non-robust explanations. Our work makes a case for future work to start examining how to generate interpretable basis concepts in a robust way."}}
{"id": "JKoEXRGOBmJ", "cdate": 1546300800000, "mdate": null, "content": {"title": "Robust Classification using Robust Feature Augmentation", "abstract": "Existing deep neural networks, say for image classification, have been shown to be vulnerable to adversarial images that can cause a DNN misclassification, without any perceptible change to an image. In this work, we propose shock absorbing robust features such as binarization, e.g., rounding, and group extraction, e.g., color or shape, to augment the classification pipeline, resulting in more robust classifiers. Experimentally, we show that augmenting ML models with these techniques leads to improved overall robustness on adversarial inputs as well as significant improvements in training time. On the MNIST dataset, we achieved 14x speedup in training time to obtain 90% adversarial accuracy com-pared to the state-of-the-art adversarial training method of Madry et al., as well as retained higher adversarial accuracy over a broader range of attacks. We also find robustness improvements on traffic sign classification using robust feature augmentation. Finally, we give theoretical insights for why one can expect robust feature augmentation to reduce adversarial input space"}}
{"id": "cYUMrKGj3QP", "cdate": 1514764800000, "mdate": 1631882029610, "content": {"title": "Smoke Screener or Straight Shooter: Detecting Elite Sybil Attacks in User-Review Social Networks", "abstract": ""}}
{"id": "k20vMR3V3y2", "cdate": 1483228800000, "mdate": 1631882029599, "content": {"title": "Fake reviews tell no tales? Dissecting click farming in content-generated social networks", "abstract": "Recently, there has been a radial shift from traditional online social networks to content-generated social networks (CGSNs). Contemporary CGSNs, such as Dianping and TripAdvisor, are often the targets of click farming in which fake reviews are posted in order to boost or diminish the ratings of listed products and services simply through clicking. Click farming often emanates from a collection of multiple fake or compromised accounts, which we call click farmers. In this paper, we conduct a three-phase methodology to detect click farming. We begin by clustering communities based on newly-defined collusion networks. We then apply the Louvain community detection method to detecting communities. We finally perform a binary classification on detected-communities. Our results of over a year-long study show that (1) the prevalence of click farming is different across CGSNs; (2) most click farmers are lowly-rated; (3) click-farming communities have relatively tight relations between users; (4) more highly-ranked stores have a greater portion of fake reviews."}}
{"id": "RHKlP0hjqeG", "cdate": 1388534400000, "mdate": 1631882029601, "content": {"title": "You are where you have been: Sybil detection via geo-location analysis in OSNs", "abstract": "Online Social Networks (OSNs) are facing an increasing threat of sybil attacks. Sybil detection is regarded as one of major challenges for OSN security. The existing sybil detection proposals that leverage graph theory or exploit the unique clickstream patterns are either based on unrealistic assumptions or limited to the service providers. In this study, we introduce a novel sybil detection approach by exploiting the fundamental mobility patterns that separate real users from sybil ones. The proposed approach is motivated as follows. On the one hand, OSNs including Yelp and Dianping allow us to obtain the users' mobility trajectories based on their online reviews and the locations of their visited shops/restaurants. On the other side, a real user's mobility is generally predictable and confined to a limited neighborhood while the sybils' mobility is forged based on the paid review missions. To exploit the mobility differences between the real and sybil users, we introduce an entropy based definition to capture users' mobility patterns. Then we design a new sybil detection model by incorporating the newly defined location entropy based metrics into other traditional feature sets. The proposed sybil detection model can significantly improve the performance of sybil detections, which is well demonstrated by extensive evaluations based on the data set from Dianping."}}
