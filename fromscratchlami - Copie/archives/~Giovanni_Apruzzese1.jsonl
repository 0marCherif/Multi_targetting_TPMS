{"id": "QJDqkt2PPa", "cdate": 1696118400000, "mdate": 1696545214957, "content": {"title": "Mitigating Adversarial Gray-Box Attacks Against Phishing Detectors", "abstract": "Although machine learning based algorithms have been extensively used for detecting phishing websites, there has been relatively little work on how adversaries may attack such \u201cphishing detectors\u201d (PDs for short). In this paper, we propose a set of Gray-Box attacks on PDs that an adversary may use which vary depending on the knowledge that he has about the PD. We show that these attacks severely degrade the effectiveness of several existing PDs. We then propose the concept of <i>operation chains</i> that iteratively map an original set of features to a new set of features and develop the \u201cProtective Operation Chain\u201d ( <inline-formula><tex-math notation=\"LaTeX\">${{\\sf POC}}$</tex-math></inline-formula> for short) algorithm. <inline-formula><tex-math notation=\"LaTeX\">${{\\sf POC}}$</tex-math></inline-formula> leverages the combination of random feature selection and feature mappings in order to increase the attacker's uncertainty about the target PD. Using 3 existing publicly available datasets plus a fourth that we have created and will release upon the publication of this article <sup>1</sup> , we show that <inline-formula><tex-math notation=\"LaTeX\">${{\\sf POC}}$</tex-math></inline-formula> is more robust to these attacks than past competing work, while preserving predictive performance when no adversarial attacks are present. Moreover, <inline-formula><tex-math notation=\"LaTeX\">${{\\sf POC}}$</tex-math></inline-formula> is robust to attacks on 13 different classifiers, not just one. These results are shown to be statistically significant at the <inline-formula><tex-math notation=\"LaTeX\">$p &lt; 0.001$</tex-math></inline-formula> level."}}
{"id": "WZ2UbVTi5d", "cdate": 1685577600000, "mdate": 1696545214956, "content": {"title": "Dual adversarial attacks: Fooling humans and classifiers", "abstract": ""}}
{"id": "ZV72FM4hi9", "cdate": 1672531200000, "mdate": 1696545215013, "content": {"title": "Attribute Inference Attacks in Online Multiplayer Video Games: A Case Study on DOTA2", "abstract": ""}}
{"id": "MQam4GW95J7", "cdate": 1672531200000, "mdate": 1696545215023, "content": {"title": "SoK: Pragmatic Assessment of Machine Learning for Network Intrusion Detection", "abstract": "Machine Learning (ML) has become a valuable asset to solve many real-world tasks. For Network Intrusion Detection (NID), however, scientific advances in ML are still seen with skepticism by practitioners. This disconnection is due to the intrinsically limited scope of research papers, many of which primarily aim to demonstrate new methods \"outperforming\" prior work\u2014oftentimes overlooking the practical implications for deploying the proposed solutions in real systems. Unfortunately, the value of ML for NID depends on a plethora of factors, such as hardware, that are often neglected in scientific literature.This paper aims to reduce the practitioners\u2019 skepticism towards ML for NID by changing the evaluation methodology adopted in research. After elucidating which factors influence the operational deployment of ML in NID, we propose the notion of pragmatic assessment, which enable practitioners to gauge the real value of ML methods for NID. Then, we show that the state-of-research hardly allows one to estimate the value of ML for NID. As a constructive step forward, we carry out a pragmatic assessment. We re-assess existing ML methods for NID, focusing on the classification of malicious network traffic, and consider: hundreds of configuration settings; diverse adversarial scenarios; and four hardware platforms. Our large and reproducible evaluations enable estimating the quality of ML for NID. We also validate our claims through a user-study with security practitioners."}}
{"id": "-5bAFR7Kxl", "cdate": 1672531200000, "mdate": 1696545215003, "content": {"title": "Attacking logo-based phishing website detectors with adversarial perturbations", "abstract": "Recent times have witnessed the rise of anti-phishing schemes powered by deep learning (DL). In particular, logo-based phishing detectors rely on DL models from Computer Vision to identify logos of well-known brands on webpages, to detect malicious webpages that imitate a given brand. For instance, Siamese networks have demonstrated notable performance for these tasks, enabling the corresponding anti-phishing solutions to detect even \"zero-day\" phishing webpages. In this work, we take the next step of studying the robustness of logo-based phishing detectors against adversarial ML attacks. We propose a novel attack exploiting generative adversarial perturbations to craft \"adversarial logos\" that evade phishing detectors. We evaluate our attacks through: (i) experiments on datasets containing real logos, to evaluate the robustness of state-of-the-art phishing detectors; and (ii) user studies to gauge whether our adversarial logos can deceive human eyes. The results show that our proposed attack is capable of crafting perturbed logos subtle enough to evade various DL models-achieving an evasion rate of up to 95%. Moreover, users are not able to spot significant differences between generated adversarial logos and original ones."}}
{"id": "_5PRbbsiyhp", "cdate": 1670514590806, "mdate": 1670514590806, "content": {"title": "Attribute Inference Attacks in Online Multiplayer Video Games: a Case Study on Dota2", "abstract": "Did you know that over 70 million of Dota2 players have their in-game data freely accessible? What if such data is used in malicious ways? This paper is the first to investigate such a problem.\nMotivated by the widespread popularity of video games, we propose the first threat model for Attribute Inference Attacks (AIA) in the Dota2 context. We explain how (and why) attackers can exploit the abundant public data in the Dota2 ecosystem to infer private information about its players. Due to lack of concrete evidence on the efficacy of our AIA, we empirically prove and assess their impact in reality. By conducting an extensive survey on \u223c500 Dota2 players spanning over 26k matches, we verify whether a correlation exists between a player's Dota2 activity and their real-life. Then, after finding such a link (p<0.01 and \u03c1>0.3), we ethically perform diverse AIA. We leverage the capabilities of machine learning to infer real-life attributes of the respondents of our survey by using their publicly available in-game data. Our results show that, by applying domain expertise, some AIA can reach up to 98% precision and over 90% accuracy. This paper hence raises the alarm on a subtle, but concrete threat that can potentially affect the entire competitive gaming landscape. We alerted the developers of Dota2."}}
{"id": "ZSLZom4KR5", "cdate": 1670514461685, "mdate": 1670514461685, "content": {"title": "Cybersecurity in the Smart Grid: Practitioners` Perspective", "abstract": "The Smart Grid (SG) is a cornerstone of modern society, providing the energy required to sustain billions of lives and thousands of industries. Unfortunately, as one of the most critical infrastructures of our World, the SG is an attractive target for attackers. The problem is aggravated by the increasing adoption of digitalisation, which further increases the SG\u2019s exposure to cyberthreats. Successful exploitation of such exposure leads to entire countries being paralysed, which is an unacceptable \u2013 but ultimately inescapable \u2013 risk. This paper aims to mitigate this risk by elucidating the perspective of real practitioners on the cybersecurity of the SG. We interviewed 18 entities, operating in diverse countries in Europe and covering all domains of the SG \u2013 from energy generation, to its delivery. Our analysis highlights a stark contrast between (a) research and practice, but also between (b) public and private entities. For instance: some threats appear to be much less dangerous than what is claimed in related papers; some technological paradigms have dubious utility for practitioners, but are actively promoted by literature; finally, practitioners may either under- or over-estimate their own cybersecurity capabilities. We derive four takeaways that enable future endeavours to improve the overall cybersecurity in the SG. We conjecture that most of the problems are due to an improper communication between researchers, practitioners and regulatory bodies \u2013 which, despite sharing a common goal, tend to neglect the viewpoint of the other \u2018spheres\u2019."}}
{"id": "QMOLuGnS5i", "cdate": 1670514321216, "mdate": null, "content": {"title": "Wild Networks: Exposure of 5G Network Infrastructures to Adversarial Examples", "abstract": "Fifth Generation (5G) networks must support billions of heterogeneous devices while guaranteeing optimal Quality of Service (QoS). Such requirements are impossible to meet with human effort alone, and Machine Learning (ML) represents a core asset in 5G. ML, however, is known to be vulnerable to adversarial examples; moreover, as our paper will show, the 5G context is exposed to a yet another type of adversarial ML attacks that cannot be formalized with existing threat models. Proactive assessment of such risks is also challenging due to the lack of MLpowered 5G equipment available for adversarial ML research. To tackle these problems, we propose a novel adversarial ML threat model that is particularly suited to 5G scenarios, and is agnostic to the precise function solved by ML. In contrast to existing ML threat models, our attacks do not require any compromise of the target 5G system while still being viable due to the QoS guarantees and the open nature of 5G networks. Furthermore, we propose an original framework for realistic ML security assessments based on public data. We proactively evaluate our threat model on 6 applications of ML envisioned in 5G. Our attacks affect both the training and the inference stages, can degrade the performance of state-of-the-art ML systems, and have a lower entry barrier than previous attacks."}}
{"id": "yEBALCbjksB", "cdate": 1669852800000, "mdate": 1696545215012, "content": {"title": "The Cross-Evaluation of Machine Learning-Based Network Intrusion Detection Systems", "abstract": "Enhancing Network Intrusion Detection Systems (NIDS) with supervised Machine Learning (ML) is tough. ML-NIDS must be trained and evaluated, operations requiring data where benign and malicious samples are clearly labeled. Such labels demand costly expert knowledge, resulting in a lack of real deployments, as well as on papers always relying on the same outdated data. The situation improved recently, as some efforts disclosed their labeled datasets. However, most past works used such datasets just as a \u2018yet another\u2019 testbed, overlooking the added potential provided by such availability. In contrast, we promote using such existing labeled data to cross-evaluate ML-NIDS. Such approach received only limited attention and, due to its complexity, requires a dedicated treatment. We hence propose the first cross-evaluation model. Our model highlights the broader range of realistic use-cases that can be assessed via cross-evaluations, allowing the discovery of still unknown qualities of state-of-the-art ML-NIDS. For instance, their detection surface can be extended\u2014at no additional labeling cost. However, conducting such cross-evaluations is challenging. Hence, we propose the first framework, XeNIDS, for reliable cross-evaluations based on Network Flows. By using XeNIDS on six well-known datasets, we demonstrate the concealed potential, but also the risks, of cross-evaluations of ML-NIDS."}}
{"id": "BblteKrmqk", "cdate": 1669852800000, "mdate": 1696545215023, "content": {"title": "Wild Networks: Exposure of 5G Network Infrastructures to Adversarial Examples", "abstract": "Fifth Generation (5G) networks must support billions of heterogeneous devices while guaranteeing optimal Quality of Service (QoS). Such requirements are impossible to meet with human effort alone, and Machine Learning (ML) represents a core asset in 5G. ML, however, is known to be vulnerable to adversarial examples; moreover, as our paper will show, the 5G context is exposed to a yet another type of adversarial ML attacks that cannot be formalized with existing threat models. Proactive assessment of such risks is also challenging due to the lack of ML-powered 5G equipment available for adversarial ML research. To tackle these problems, we propose a novel adversarial ML threat model that is particularly suited to 5G scenarios, and is agnostic to the precise function solved by ML. In contrast to existing ML threat models, our attacks do not require any compromise of the target 5G system while still being viable due to the QoS guarantees and the open nature of 5G networks. Furthermore, we propose an original framework for realistic ML security assessments based on public data. We proactively evaluate our threat model on 6 applications of ML envisioned in 5G. Our attacks affect both the training and the inference stages, can degrade the performance of state-of-the-art ML systems, and have a lower entry barrier than previous attacks."}}
