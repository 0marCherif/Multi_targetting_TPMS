{"id": "baSZjlNC3J", "cdate": 1672531200000, "mdate": 1681774128885, "content": {"title": "PRN: Panoptic Refinement Network", "abstract": "Panoptic segmentation is the task of uniquely assigning every pixel in an image to either a semantic label or an individual object instance, generating a coherent and complete scene description. Many current panoptic segmentation methods, however, predict masks of semantic classes and object instances in separate branches, yielding inconsistent predictions. Moreover, because state-of-the-art panoptic segmentation models rely on box proposals, the instance masks predicted are often of low-resolution. To overcome these limitations, we propose the Panoptic Refinement Network (PRN), which takes masks from base panoptic segmentation models and refines them jointly to produce coherent results. PRN extends the offset map-based architecture of Panoptic-Deeplab with several novel ideas including a foreground mask and instance bounding box offsets, as well as coordinate convolutions for improved spatial prediction. Experimental results on COCO and Cityscapes show that PRN can significantly improve already accurate results from a variety of panoptic segmentation networks."}}
{"id": "ZpQ266SPjk", "cdate": 1672531200000, "mdate": 1681774128885, "content": {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "abstract": "In this paper, we present an approach for navigating a robotic wheelchair that provides users with multiple levels of autonomy and navigation capabilities to fit their individual needs and preferences. We focus on three main aspects: (i) egocentric computer vision based motion control to provide a natural human-robot interface to wheelchair users with impaired hand usage; (ii) techniques that enable user to initiate autonomous navigation to a location, object or person without use of the hands; and (iii) a framework that learns to navigate the wheelchair according to its user\u2019s, often subjective, criteria and preferences. These contributions are evaluated qualitatively and quantitatively in user studies with several subjects demonstrating their effectiveness. These studies have been conducted with healthy subjects, but they still indicate that clinical tests of the proposed technology can be initiated."}}
{"id": "8-jRtmNqKx7", "cdate": 1672531200000, "mdate": 1681774128890, "content": {"title": "Learning the Distribution of Errors in Stereo Matching for Joint Disparity and Uncertainty Estimation", "abstract": "We present a new loss function for joint disparity and uncertainty estimation in deep stereo matching. Our work is motivated by the need for precise uncertainty estimates and the observation that multi-task learning often leads to improved performance in all tasks. We show that this can be achieved by requiring the distribution of uncertainty to match the distribution of disparity errors via a KL divergence term in the network's loss function. A differentiable soft-histogramming technique is used to approximate the distributions so that they can be used in the loss. We experimentally assess the effectiveness of our approach and observe significant improvements in both disparity and uncertainty prediction on large datasets."}}
{"id": "biQWVByWWv", "cdate": 1668784076236, "mdate": 1668784076236, "content": {"title": "Do End-to-end Stereo Algorithms Under-utilize Information?", "abstract": "Deep networks for stereo matching typically leverage 2D or 3D convolutional encoder-decoder architectures to aggregate cost and regularize the cost volume for accurate disparity estimation. Due to content-insensitive convolutions and down-sampling and up-sampling operations, these cost aggregation mechanisms do not take full advantage of the information available in the images. Disparity maps suffer from over-smoothing near occlusion boundaries, and erroneous predictions in thin structures. In this paper, we show how deep adaptive filtering and differentiable semiglobal aggregation can be integrated in existing 2D and 3D convolutional networks for end-to-end stereo matching, leading to improved accuracy. The improvements are due to utilizing RGB information from the images as a signal to dynamically guide the matching process, in addition to being the signal we attempt to match across the images. We show extensive experimental results on the KITTI 2015 and Virtual KITTI 2 datasets comparing four stereo networks (DispNetC, GCNet, PSMNet and GANet) after integrating four adaptive filters (segmentation-aware bilateral filtering, dynamic filtering networks, pixel adaptive convolution and semi-global aggregation) into their architectures. Our code is available at https://github.com/ccj5351/DAFStereoNets."}}
{"id": "UekB4T532yv", "cdate": 1668783737498, "mdate": null, "content": {"title": "Matching-space stereo networks for cross-domain generalization", "abstract": "End-to-end deep networks represent the state of the art for stereo matching. While excelling on images framing environments similar to the training set, major drops in accuracy occur in unseen domains (e.g., when moving from synthetic to real scenes). In this paper we introduce a novel family of architectures, namely Matching-Space Networks (MS-Nets), with improved generalization properties. By replacing learning-based feature extraction from image RGB values with matching functions and confidence measures from conventional wisdom, we move the learning process from the color space to the Matching Space, avoiding over-specialization to domain specific features.\nExtensive experimental results on four real datasets highlight that our proposal leads to superior generalization to unseen environments over conventional deep architectures, keeping accuracy on the source domain almost unaltered. Our code is available at https://github.com/ccj5351/MS-Nets."}}
{"id": "d_Q7YiGpkoz", "cdate": 1668539657960, "mdate": 1668539657960, "content": {"title": "Glissando-Net: Deep sinGLe vIew category level poSe eStimation ANd 3D recOnstruction", "abstract": "We present a deep learning model, dubbed Glissando-Net, to simultaneously estimate the pose and reconstruct the 3D\nshape of objects at the category level from a single RGB image. Previous works predominantly focused on either estimating poses\n(often at the instance level), or reconstructing shapes, but not both. Glissando-Net is composed of two auto-encoders that are jointly\ntrained, one for RGB images and the other for point clouds. We embrace two key design choices in Glissando-Net to achieve a more\naccurate prediction of the 3D shape and pose of the object given a single RGB image as input. First, we augment the feature maps of\nthe point cloud encoder and decoder with transformed feature maps from the image decoder, enabling effective 2D-3D interaction in\nboth training and prediction. Second, we predict both the 3D shape and pose of the object in the decoder stage. This way, we better\nutilize the information in the 3D point clouds presented only in the training stage to train the network for more accurate prediction. We\njointly train the two encoder-decoders for RGB and point cloud data to learn how to pass latent features to the point cloud decoder\nduring inference. In testing, the encoder of the 3D point cloud is discarded. The design of Glissando-Net is inspired by codeSLAM.\nUnlike codeSLAM, which targets 3D reconstruction of scenes, we focus on pose estimation and shape reconstruction of objects, and\ndirectly predict the object pose and a pose invariant 3D reconstruction without the need of the code optimization step. Extensive\nexperiments, involving both ablation studies and comparison with competing methods, demonstrate the efficacy of our proposed\nmethod, and compare favorably with the state-of-the-art."}}
{"id": "nfx93zELd_", "cdate": 1668536857438, "mdate": null, "content": {"title": "PRN: Panoptic Refinement Network", "abstract": "Panoptic segmentation is the task of uniquely assigning every pixel in an image to either a semantic label or an individual\nobject instance, generating a coherent and complete scene description. Many current panoptic segmentation methods, however, predict masks of semantic classes and object instances in separate branches, yielding inconsistent predictions. Moreover, because state-of-the-art panoptic segmentation models rely on box proposals, the instance masks predicted are often of low-resolution. To overcome these limitations, we propose the Panoptic Refinement Network (PRN), which takes masks from base panoptic segmentation models and refines them jointly to produce coherent results. PRN extends the offset map-based architecture of Panoptic-Deeplab with several novel ideas including a foreground mask and instance bounding box offsets, as well as coordinate convolutions for improved spatial prediction. Experimental results on COCO and Cityscapes show that PRN can significantly improve already accurate results from a variety of panoptic segmentation networks."}}
{"id": "ezij4jGjD2r", "cdate": 1668528272944, "mdate": 1668528272944, "content": {"title": "Usability studies of an egocentric vision-based robotic wheelchair", "abstract": "Motivated by the need to improve the quality of life for the elderly and disabled individuals who rely on wheelchairs for mobility, and who may have limited or no hand functionality at all, we propose an egocentric computer vision based co-robot wheelchair to enhance their mobility without hand usage. The robot is built using a commercially available powered wheelchair modified to be controlled by head motion. Head motion is measured by tracking an egocentric camera mounted on the user\u2019s head and faces outward. Compared with previous approaches to hands-free mobility, our system provides a more natural human robot interface because it enables the user to control the speed and direction of motion in a continuous fashion, as opposed to providing a small number of discrete commands. This article presents three usability studies, which were conducted on 37 subjects. The first two usability studies focus on comparing the proposed control method with existing solutions while the third study was conducted to assess the effectiveness of training subjects to operate the wheelchair over several sessions. A limitation of our studies is that they have been conducted with healthy participants. Our findings, however, pave the way for further studies with subjects with disabilities."}}
{"id": "bqJ9ufqa9o", "cdate": 1668527868884, "mdate": 1668527868884, "content": {"title": "Learning to Navigate Robotic Wheelchairs from Demonstration: Is Training in Simulation Viable?", "abstract": "Learning from demonstration (LfD) enables robots to learn complex relationships between their state, perception and actions that are hard to express in an optimization framework. While people intuitively know what they would like to do in a given situation, they often have difficulty representing their decision process precisely enough to enable an implementation. Here, we are interested in robots that carry passengers, such as robotic wheelchairs, where user preferences, comfort and the feeling of safety are important for autonomous navigation. Balancing these requirements is not straightforward. While robots can be trained in an LfD framework in which users drive the robot according to their preferences, performing these demonstrations can be time-consuming, expensive, and possibly dangerous. Inspired by recent efforts for generating synthetic data for training autonomous driving systems, we investigate whether it is possible to train a robot based on simulations to reduce the time requirements, cost and potential risk. A key characteristic of our approach is that the input is not images, but the locations of people and obstacles relative to the robot. We argue that this allows us to transfer the classifier from the simulator to the physical world and to previously unseen environments that do not match the appearance of the training set. Experiments with 14 subjects providing physical and simulated demonstrations validate our claim."}}
{"id": "MnDEbF8R4Fw", "cdate": 1668527725939, "mdate": 1668527725939, "content": {"title": "A shared autonomy approach for wheelchair navigation based on learned user preferences", "abstract": "Research on robotic wheelchairs covers a broad range from complete autonomy to shared autonomy to manual navigation by a joystick or other means. Shared autonomy is valuable because it allows the user and the robot to complement each other, to correct each other's mistakes and to avoid collisions. In this paper, we present an approach that can learn to replicate path selection according to the wheelchair user's individual, often subjective, criteria in order to reduce the number of times the user has to intervene during automatic navigation. This is achieved by learning to rank paths using a support vector machine trained on selections made by the user in a simulator. If the classifier's confidence in the top ranked path is high, it is executed without requesting confirmation from the user. Otherwise, the choice is deferred to the user. Simulations and laboratory experiments using two path generation strategies demonstrate the effectiveness of our approach."}}
