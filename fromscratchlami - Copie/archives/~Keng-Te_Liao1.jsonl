{"id": "FDmIo6o09H", "cdate": 1652737797024, "mdate": null, "content": {"title": "Environment Diversification with Multi-head Neural Network for Invariant Learning", "abstract": "Neural networks are often trained with empirical risk minimization; however, it has been shown that a shift between training and testing distributions can cause unpredictable performance degradation. On this issue, a research direction, invariant learning, has been proposed to extract causal features insensitive to the distributional changes. This work proposes EDNIL, an invariant learning framework containing a multi-head neural network to absorb data biases. We show that this framework does not require prior knowledge about environments or strong assumptions about the pre-trained model. We also reveal that the proposed algorithm has theoretical connections to recent studies discussing properties of variant and invariant features. Finally, we demonstrate that models trained with EDNIL are empirically more robust against distributional shifts. "}}
{"id": "j2C7tVxM-c", "cdate": 1640995200000, "mdate": 1683945302381, "content": {"title": "Bayesian mixture variational autoencoders for multi-modal learning", "abstract": "This paper provides an in-depth analysis on how to effectively acquire and generalize cross-modal knowledge for multi-modal learning. Mixture-of-Expert (MoE) and Product-of-Expert (PoE) are two popular directions in generalizing multi-modal information. Existing works based on MoE or PoE have shown notable improvement on data generation, while new challenges such as high training cost, overconfident experts, and encoding modal-specific features also emerge. In this work, we propose Bayesian mixture variational autoencoder (BMVAE) which learns to select or combine experts via Bayesian inference. We show that the proposed idea can naturally encourage models to learn modal-specific knowledge and avoid overconfident experts. Also, we show that the idea is compatible with both MoE and PoE frameworks. When being a MoE model, BMVAE can be optimized by a tight lower bound and is efficient to train. The PoE BMVAE has the same advantages and a theoretical connection to existing works. In the experiments, we show that BMVAE achieves state-of-the-art performance."}}
{"id": "14ggkeOeQD", "cdate": 1640995200000, "mdate": 1683945302380, "content": {"title": "Environment Diversification with Multi-head Neural Network for Invariant Learning", "abstract": "Neural networks are often trained with empirical risk minimization; however, it has been shown that a shift between training and testing distributions can cause unpredictable performance degradation. On this issue, a research direction, invariant learning, has been proposed to extract causal features insensitive to the distributional changes. This work proposes EDNIL, an invariant learning framework containing a multi-head neural network to absorb data biases. We show that this framework does not require prior knowledge about environments or strong assumptions about the pre-trained model. We also reveal that the proposed algorithm has theoretical connections to recent studies discussing properties of variant and invariant features. Finally, we demonstrate that models trained with EDNIL are empirically more robust against distributional shifts."}}
{"id": "2C-rgZr84_a", "cdate": 1577836800000, "mdate": 1683945302383, "content": {"title": "Explainable and Sparse Representations of Academic Articles for Knowledge Exploration", "abstract": ""}}
{"id": "17mB4Yi2I1W", "cdate": 1577836800000, "mdate": 1683945302381, "content": {"title": "Explaining Word Embeddings via Disentangled Representation", "abstract": "Keng-Te Liao, Cheng-Syuan Lee, Zhong-Yu Huang, Shou-de Lin. Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing. 2020."}}
{"id": "HyWrA-fOWB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Word Relation Autoencoder for Unseen Hypernym Extraction Using Word Embeddings", "abstract": ""}}
{"id": "1e4hTfT1Foz", "cdate": 1483228800000, "mdate": 1683945302381, "content": {"title": "A Cross-Domain Recommender System Based on Common-Sense Knowledge Bases", "abstract": "A system able to extract and recommend technical terms from various domains is proposed in this paper. The motivation is to provide keywords that users may not be familiar with in the beginning but will be interested in after studying. To acquire domain knowledge, we collect documents from various sources, and the words in the documents are then represented as semantic word vectors. Given queries from users, the system first extracts important terms from given documents and computes the semantic similarity between those terms. Next, we utilize third party common-sense knowledge bases such as ConceptNet and Wikipedia to connect the queries to those extracted keywords through the network structures. Finally, the system will collect all keywords traversed and recommend the top-n of them. We propose and compare four models for the recommendation, and the differences between using ConceptNet and Wikipedia for discovering related knowledge are also investigated in this work."}}
