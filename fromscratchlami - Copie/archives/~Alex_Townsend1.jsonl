{"id": "OiuKi9cokg_", "cdate": 1692869354394, "mdate": 1692869354394, "content": {"title": "Elliptic PDE learning is provably data-efficient", "abstract": "PDE learning is an emerging field that combines physics and machine learning to recover unknown physical systems from experimental data. While deep learning models traditionally require copious amounts of training data, recent PDE learning techniques achieve spectacular results with limited data availability. Still, these results are empirical. Our work provides theoretical guarantees on the number of input-output training pairs required in PDE learning, explaining why these methods can be data-efficient. Specifically, we exploit randomized numerical linear algebra and PDE theory to derive a provably data-efficient algorithm that recovers solution operators of 3D elliptic PDEs from input-output data and achieves an exponential convergence rate with respect to the size of the training dataset with an exceptionally high probability of success."}}
{"id": "-Ea9X95HOGv", "cdate": 1692869169698, "mdate": 1692869169698, "content": {"title": "Data-driven discovery of Green\u2019s functions with human-understandable deep learning", "abstract": "There is an opportunity for deep learning to revolutionize science and technology by revealing its findings in a human interpretable manner. To do this, we develop a novel data-driven approach for creating a human\u2013machine partnership to accelerate scientific discovery. By collecting physical system responses under excitations drawn from a Gaussian process, we train rational neural networks to learn Green\u2019s functions of hidden linear partial differential equations. These functions reveal human-understandable properties and features, such as linear conservation laws and symmetries, along with shock and singularity locations, boundary effects, and dominant modes. We illustrate the technique on several examples and capture a range of physics, including advection\u2013diffusion, viscous shocks, and Stokes flow in a lid-driven cavity."}}
{"id": "DkJ9dlTFBm", "cdate": 1692869068634, "mdate": 1692869068634, "content": {"title": "Learning Elliptic Partial Differential Equations with Randomized Linear Algebra", "abstract": "Given input\u2013output pairs of an elliptic partial differential equation (PDE) in three dimensions, we derive the first theoretically rigorous scheme for learning the associated Green\u2019s function G. By exploiting the hierarchical low-rank structure of G, we show that one can construct an approximant to G that converges almost surely and achieves a relative error of O(\u0393\u22121/2\u03f5log3(1/\u03f5)\u03f5) using at most O(\u03f5\u22126log4(1/\u03f5)) input\u2013output training pairs with high probability, for any 0<\u03f5<1. The quantity 0<\u0393\u03f5\u22641 characterizes the quality of the training dataset. Along the way, we extend the randomized singular value decomposition algorithm for learning matrices to Hilbert\u2013Schmidt operators and characterize the quality of covariance kernels for PDE learning."}}
{"id": "UzNQFA1GOJ", "cdate": 1692868689525, "mdate": 1692868689525, "content": {"title": "Learning Green's functions associated with time-dependent partial differential equations", "abstract": "Neural operators are a popular technique in scientific machine learning to learn a mathematical model of the behavior of unknown physical systems from data. Neural operators are especially useful to learn solution operators associated with partial differential equations (PDEs) from pairs of forcing functions and solutions when numerical solvers are not available or the underlying physics is poorly understood. In this work, we attempt to provide theoretical foundations to understand the amount of training data needed to learn time-dependent PDEs. Given input-output pairs from a parabolic PDE in any spatial dimension n \u2265 1, we derive the first theoretically rigorous scheme for learning the associated solution operator, which takes the form of a convolution with a Green's function G. Until now, rigorously learning Green's functions associated with time-dependent PDEs has been a major challenge in the field of scientific machine learning because G may not be square-integrable when n > 1, and time-dependent PDEs have transient dynamics. By combining the hierarchical low-rank structure of G together with randomized numerical linear algebra, we construct an approximant to G that achieves a relative error of $\\smash{\\mathcal{O}(\\Gamma_\\epsilon^{-1/2}\\epsilon)}$ in the L1-norm with high probability by using at most $\\smash{\\mathcal{O}(\\epsilon^{-\\frac{n+2}{2}}\\log(1/\\epsilon))}$ input-output training pairs, where \u0393\u03b5 is a measure of the quality of the training dataset for learning G, and \u03b5 > 0 is sufficiently small."}}
{"id": "xY7uLfhwD3M", "cdate": 1680307200000, "mdate": 1683618480679, "content": {"title": "Learning Elliptic Partial Differential Equations with Randomized Linear Algebra", "abstract": "Given input\u2013output pairs of an elliptic partial differential equation (PDE) in three dimensions, we derive the first theoretically rigorous scheme for learning the associated Green\u2019s function G. By exploiting the hierarchical low-rank structure of G, we show that one can construct an approximant to G that converges almost surely and achieves a relative error of $$\\mathcal {O}(\\varGamma _\\epsilon ^{-1/2}\\log ^3(1/\\epsilon )\\epsilon )$$ O ( \u0393 \u03f5 - 1 / 2 log 3 ( 1 / \u03f5 ) \u03f5 ) using at most $$\\mathcal {O}(\\epsilon ^{-6}\\log ^4(1/\\epsilon ))$$ O ( \u03f5 - 6 log 4 ( 1 / \u03f5 ) ) input\u2013output training pairs with high probability, for any $$0<\\epsilon <1$$ 0 < \u03f5 < 1 . The quantity $$0<\\varGamma _\\epsilon \\le 1$$ 0 < \u0393 \u03f5 \u2264 1 characterizes the quality of the training dataset. Along the way, we extend the randomized singular value decomposition algorithm for learning matrices to Hilbert\u2013Schmidt operators and characterize the quality of covariance kernels for PDE learning."}}
{"id": "wd7DOttG-w", "cdate": 1672531200000, "mdate": 1683618481652, "content": {"title": "Elliptic PDE learning is provably data-efficient", "abstract": "PDE learning is an emerging field that combines physics and machine learning to recover unknown physical systems from experimental data. While deep learning models traditionally require copious amounts of training data, recent PDE learning techniques achieve spectacular results with limited data availability. Still, these results are empirical. Our work provides theoretical guarantees on the number of input-output training pairs required in PDE learning, explaining why these methods can be data-efficient. Specifically, we exploit randomized numerical linear algebra and PDE theory to derive a provably data-efficient algorithm that recovers solution operators of 3D elliptic PDEs from input-output data and achieves an exponential convergence rate with respect to the size of the training dataset with an exceptionally high probability of success."}}
{"id": "qzUZ9x0wyqB", "cdate": 1672531200000, "mdate": 1683618482113, "content": {"title": "Avoiding discretization issues for nonlinear eigenvalue problems", "abstract": "The first step when solving an infinite-dimensional eigenvalue problem is often to discretize it. We show that one must be extremely careful when discretizing nonlinear eigenvalue problems. Using examples, we show that discretization can: (1) introduce spurious eigenvalues, (2) entirely miss spectra, and (3) bring in severe ill-conditioning. While there are many eigensolvers for solving matrix nonlinear eigenvalue problems, we propose a solver for general holomorphic infinite-dimensional nonlinear eigenvalue problems that avoids discretization issues, which we prove is stable and converges. Moreover, we provide an algorithm that computes the problem's pseudospectra with explicit error control, allowing verification of computed spectra. The algorithm and numerical examples are publicly available in $\\texttt{infNEP}$, which is a software package written in MATLAB."}}
{"id": "Wel2EF6_PDE", "cdate": 1672531200000, "mdate": 1683618480765, "content": {"title": "Are sketch-and-precondition least squares solvers numerically stable?", "abstract": "Sketch-and-precondition techniques are popular for solving large least squares (LS) problems of the form $Ax=b$ with $A\\in\\mathbb{R}^{m\\times n}$ and $m\\gg n$. This is where $A$ is ``sketched\" to a smaller matrix $SA$ with $S\\in\\mathbb{R}^{\\lceil cn\\rceil\\times m}$ for some constant $c>1$ before an iterative LS solver computes the solution to $Ax=b$ with a right preconditioner $P$, where $P$ is constructed from $SA$. Popular sketch-and-precondition LS solvers are Blendenpik and LSRN. We show that the sketch-and-precondition technique is not numerically stable for ill-conditioned LS problems. Instead, we propose using an unpreconditioned iterative LS solver on $(AP)y=b$ with $x=Py$ when accuracy is a concern. Provided the condition number of $A$ is smaller than the reciprocal of the unit round-off, we show that this modification ensures that the computed solution has a comparable backward error to the iterative LS solver applied to a well-conditioned matrix. Using smoothed analysis, we model floating-point rounding errors to provide a convincing argument that our modification is expected to compute a backward stable solution even for arbitrarily ill-conditioned LS problems."}}
{"id": "BWyKAvAlbcA", "cdate": 1672531200000, "mdate": 1683618481836, "content": {"title": "Leveraging the Hankel norm approximation and block-AAA algorithms in reduced order modeling", "abstract": "Large-scale linear, time-invariant (LTI) dynamical systems are widely used to characterize complicated physical phenomena. We propose a two-stage algorithm to reduce the order of a large-scale LTI system given samples of its transfer function for a target degree $k$ of the reduced system. In the first stage, a modified adaptive Antoulas--Anderson (AAA) algorithm is used to construct a degree $d$ rational approximation of the transfer function that corresponds to an intermediate system, which can be numerically stably reduced in the second stage using ideas from the theory on Hankel norm approximation (HNA). We also study the numerical issues of Glover's HNA algorithm and provide a remedy for its numerical instabilities. A carefully computed rational approximation of degree $d$ gives us a numerically stable algorithm for reducing an LTI system, which is more efficient than SVD-based algorithms and more accurate than moment-matching algorithms."}}
{"id": "oLIZ2jGTiv", "cdate": 1663850175353, "mdate": null, "content": {"title": "Tuning Frequency Bias in Neural Network Training with Nonuniform Data", "abstract": "Small generalization errors of over-parameterized neural networks (NNs) can be partially explained by the frequency biasing phenomenon, where gradient-based algorithms minimize the low-frequency misfit before reducing the high-frequency residuals. Using the Neural Tangent Kernel (NTK), one can provide a theoretically rigorous analysis for training where data are drawn from constant or piecewise-constant probability densities. Since most training data sets are not drawn from such distributions, we use the NTK model and a data-dependent quadrature rule to theoretically quantify the frequency biasing of NN training given fully nonuniform data. By replacing the loss function with a carefully selected Sobolev norm, we can further amplify, dampen, counterbalance, or reverse the intrinsic frequency biasing in NN training."}}
