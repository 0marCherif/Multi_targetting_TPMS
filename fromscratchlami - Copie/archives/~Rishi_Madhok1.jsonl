{"id": "eD8c3rjn0v", "cdate": 1684061277894, "mdate": 1684061277894, "content": {"title": "Content Driven Enrichment of Formal Text using Concept Definitions and Applications", "abstract": "Formal text is objective, unambiguous and tends to have complex sentence construction intended to be understood by the target demographic. However, in the absence of domain knowledge it is imperative to define key concepts and their relationship in the text for correct interpretation for general readers. To address this, we propose a text enrichment framework that identifies the key concepts from input text, highlights definitions and fetches the definition from external data sources in case the concept is undefined. Beyond concept definitions, the system enriches the input text with concept applications and a pre-requisite concept graph that showcases the inter-dependency within the extracted concepts. While the problem of learning definition statements is attempted in literature, the task of learning application statements is novel. We manually annotated a dataset for training a deep learning network for identifying application statements in text. We quantitatively compared the results of both application and definition identification models with standard baselines. To validate the utility of the proposed framework for general readers, we report enrichment accuracy and show promising results."}}
{"id": "jUk-XRBskheI", "cdate": 1577836800000, "mdate": 1671732624453, "content": {"title": "Towards Latency-aware DNN Optimization with GPU Runtime Analysis and Tail Effect Elimination", "abstract": "Despite the superb performance of State-Of-The-Art (SOTA) DNNs, the increasing computational cost makes them very challenging to meet real-time latency and accuracy requirements. Although DNN runtime latency is dictated by model property (e.g., architecture, operations), hardware property (e.g., utilization, throughput), and more importantly, the effective mapping between these two, many existing approaches focus only on optimizing model property such as FLOPS reduction and overlook the mismatch between DNN model and hardware properties. In this work, we show that the mismatch between the varied DNN computation workloads and GPU capacity can cause the idle GPU tail effect, leading to GPU under-utilization and low throughput. As a result, the FLOPs reduction cannot bring effective latency reduction, which causes sub-optimal accuracy versus latency trade-offs. Motivated by this, we propose a GPU runtime-aware DNN optimization methodology to eliminate such GPU tail effect adaptively on GPU platforms. Our methodology can be applied on top of existing SOTA DNN optimization approaches to achieve better latency and accuracy trade-offs. Experiments show 11%-27% latency reduction and 2.5%-4.0% accuracy improvement over several SOTA DNN pruning and NAS methods, respectively"}}
{"id": "dS7e2Ruez6", "cdate": 1577836800000, "mdate": 1682381959527, "content": {"title": "An Extensible Multi-Sensor Fusion Framework for 3D Imaging", "abstract": "Many autonomous vehicles rely on an array of sensors for safe navigation, where each sensor captures different visual attributes from the surrounding environment. For example, a single conventional camera captures high-resolution images but no 3D information; a LiDAR provides excellent range information but poor spatial resolution; and a prototype single-photon LiDAR (SP-LiDAR) can provide a dense but noisy representation of the 3D scene. Although the outputs of these sensors vary dramatically (e.g., 2D images, point clouds, 3D volumes), they all derive from the same 3D scene. We propose an extensible sensor fusion framework that (1) lifts the sensor output to volumetric representations of the 3D scene, (2) fuses these volumes together, and (3) processes the resulting volume with a deep neural network to generate a depth (or disparity) map. Although our framework can potentially extend to many types of sensors, we focus on fusing combinations of three imaging systems: monocular/stereo cameras, regular LiDARs, and SP-LiDARs. To train our neural network, we generate a synthetic dataset through CARLA that contains the individual measurements. We also conduct various fusion ablation experiments and evaluate the results of different sensor combinations."}}
{"id": "hDgreBgMVrZ", "cdate": 1546300800000, "mdate": 1633618916297, "content": {"title": "Video Jigsaw: Unsupervised Learning of Spatiotemporal Context for Video Action Recognition", "abstract": "We propose a self-supervised learning method to jointly reason about spatial and temporal context for video recognition. Recent self-supervised approaches have used spatial context [9, 34] as well as temporal coherency [32] but a combination of the two requires extensive preprocessing such as tracking objects through millions of video frames [59] or computing optical flow to determine frame regions with high motion [30]. We propose to combine spatial and temporal context in one self-supervised framework without any heavy preprocessing. We divide multiple video frames into grids of patches and train a network to solve jigsaw puzzles on these patches from multiple frames. So the network is trained to correctly identify the position of a patch within a video frame as well as the position of a patch over time. We also propose a novel permutation strategy that outperforms random permutations while significantly reducing computational and memory constraints. We use our trained network for transfer learning tasks such as video activity recognition and demonstrate the strength of our approach on two benchmark video action recognition datasets without using a single frame from these datasets for unsupervised pretraining of our proposed video jigsaw network."}}
{"id": "tiwd77cJN0Q", "cdate": 1514764800000, "mdate": 1682381959524, "content": {"title": "Proposing Contextually Relevant Quotes for Images", "abstract": "Due to the rise in deep learning techniques used for the task of automatic image captioning, it is now possible to generate natural language descriptions of images and their regions. However, these captions are often too plain and simple. Most users on social media and other micro blogging websites use flowery language and quote like captions to describe the pictures they post online. We propose an algorithm that uses a combination of deep learning and natural language processing techniques to provide contextually relevant quotes for any given input image. We also present a new dataset, QUOTES500K, with the goal of advancing research requiring large dataset of quotes. Our dataset contains five hundred thousand (500K) quotes along with the author name and their category tags."}}
{"id": "Pl-znTrw47", "cdate": 1514764800000, "mdate": 1633618916297, "content": {"title": "Video Jigsaw: Unsupervised Learning of Spatiotemporal Context for Video Action Recognition", "abstract": "We propose a self-supervised learning method to jointly reason about spatial and temporal context for video recognition. Recent self-supervised approaches have used spatial context [9, 34] as well as temporal coherency [32] but a combination of the two requires extensive preprocessing such as tracking objects through millions of video frames [59] or computing optical flow to determine frame regions with high motion [30]. We propose to combine spatial and temporal context in one self-supervised framework without any heavy preprocessing. We divide multiple video frames into grids of patches and train a network to solve jigsaw puzzles on these patches from multiple frames. So the network is trained to correctly identify the position of a patch within a video frame as well as the position of a patch over time. We also propose a novel permutation strategy that outperforms random permutations while significantly reducing computational and memory constraints. We use our trained network for transfer learning tasks such as video activity recognition and demonstrate the strength of our approach on two benchmark video action recognition datasets without using a single frame from these datasets for unsupervised pretraining of our proposed video jigsaw network."}}
{"id": "I99Jaduts3u", "cdate": 1514764800000, "mdate": 1682381959527, "content": {"title": "Content Driven Enrichment of Formal Text using Concept Definitions and Applications", "abstract": "Formal text is objective, unambiguous and tends to have complex sentence construction intended to be understood by the target demographic. However, in the absence of domain knowledge it is imperative to define key concepts and their relationship in the text for correct interpretation for general readers. To address this, we propose a text enrichment framework that identifies the key concepts from input text, highlights definitions and fetches the definition from external data sources in case the concept is undefined. Beyond concept definitions, the system enriches the input text with concept applications and a pre-requisite concept graph that showcases the inter-dependency within the extracted concepts. While the problem of learning definition statements is attempted in literature, the task of learning application statements is novel. We manually annotated a dataset for training a deep learning network for identifying application statements in text. We quantitatively compared the results of both application and definition identification models with standard baselines. To validate the utility of the proposed framework for general readers, we report enrichment accuracy and show promising results."}}
{"id": "BJ-abCx_bB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Semantic Understanding for Contextual In-Video Advertising", "abstract": "With the increasing consumer base of online video content, it is important for advertisers to understand the video context when targeting video ads to consumers. To improve the consumer experience and quality of ads, key factors need to be considered such as (i) ad relevance to video content (ii) where and how video ads are placed, and (iii) non-intrusive user experience. We propose a framework to semantically understand the video content for better ad recommendation that ensure these criteria."}}
{"id": "2IVsmlsrb0", "cdate": 1514764800000, "mdate": 1682381959525, "content": {"title": "SentiMozart: Music Generation based on Emotions", "abstract": ""}}
