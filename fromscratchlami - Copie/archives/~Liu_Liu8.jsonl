{"id": "aPc-R01WvJV", "cdate": 1663850284099, "mdate": null, "content": {"title": "Prescribed Safety Performance Imitation Learning from A Single Expert Dataset", "abstract": "Existing safe imitation learning (safe IL) methods mainly focus on learning safe policies that are similar to expert ones, but may fail in applications requiring different safety constraints. In this paper, we propose the Lagrangian Generative Adversarial Imitation Learning (LGAIL) algorithm, which can adaptively learn safe policies from a single expert dataset under diverse prescribed safety constraints. To achieve this, we augment GAIL with safety constraints and then relax it as an unconstrained optimization problem by utilizing a Lagrange multiplier. The Lagrange multiplier enables explicit consideration of the safety and is dynamically adjusted to balance the imitation and safety performance during training. Then, we apply a two-stage optimization framework to solve LGAIL: (1) a discriminator is optimized to measure the similarity between the agent-generated data and the expert ones; (2) forward reinforcement learning is employed to improve the similarity while considering safety concerns enabled by a Lagrange multiplier. Furthermore, theoretical analyses on the convergence and safety of LGAIL demonstrate its capability of adaptively learning a safe policy given prescribed safety constraints. At last, extensive experiments in OpenAI Safety Gym conclude the effectiveness of our approach."}}
{"id": "jXgbJdQ2YIy", "cdate": 1652737826017, "mdate": null, "content": {"title": "Escaping from the Barren Plateau via Gaussian Initializations in Deep Variational Quantum Circuits", "abstract": "Variational quantum circuits have been widely employed in quantum simulation and quantum machine learning in recent years. However, quantum circuits with random structures have poor trainability due to the exponentially vanishing gradient with respect to the circuit depth and the qubit number. This result leads to a general standpoint that deep quantum circuits would not be feasible for practical tasks. In this work, we propose an initialization strategy with theoretical guarantees for the vanishing gradient problem in general deep quantum circuits. Specifically, we prove that under proper Gaussian initialized parameters, the norm of the gradient decays at most polynomially when the qubit number and the circuit depth increase. Our theoretical results hold for both the local and the global observable cases, where the latter was believed to have vanishing gradients even for very shallow circuits. Experimental results verify our theoretical findings in quantum simulation and quantum chemistry."}}
{"id": "tF7LbpcAaS", "cdate": 1640995200000, "mdate": 1668231119563, "content": {"title": "Continual Learning with Lifelong Vision Transformer", "abstract": "Continual learning methods aim at training a neural network from sequential data with streaming labels, relieving catastrophic forgetting. However, existing methods are based on and designed for convolutional neural networks (CNNs), which have not utilized the full potential of newly emerged powerful vision transformers. In this paper, we propose a novel attention-based framework Lifelong Vision Transformer (LVT), to achieve a better stability-plasticity trade-off for continual learning. Specifically, an inter-task attention mechanism is presented in LVT, which implicitly absorbs the previous tasks' information and slows down the drift of important attention between previous tasks and the current task. LVT designs a dual-classifier structure that independently injects new representation to avoid catas-trophic interference and accumulates the new and previous knowledge in a balanced manner to improve the overall performance. Moreover, we develop a confidence-aware memory update strategy to deepen the impression of the previous tasks. The extensive experimental results show that our approach achieves state-of-the-art performance with even fewer parameters on continual learning benchmarks."}}
{"id": "nLxYFDDwyy", "cdate": 1640995200000, "mdate": 1668231119557, "content": {"title": "Variance Reduced Methods for Non-Convex Composition Optimization", "abstract": "This paper explores the non-convex composition optimization consisting of inner and outer finite-sum functions with a large number of component functions. This problem arises in important applications such as nonlinear embedding and reinforcement learning. Although existing approaches such as stochastic gradient descent (SGD) and stochastic variance reduced gradient (SVRG) descent can be applied to solve this problem, their query complexities tend to be high, especially when the number of inner component functions is large. Therefore, to significantly improve the query complexity of current approaches, we have devised the stochastic composition via variance reduction (SCVR). What's more, we analyze the query complexity under different numbers of inner function and outer function. Based on different kinds of estimation of inner component function, we also present the SCVRII algorithm, though the order of query complexities are the same with SCVR. Additionally, we propose an extension to handle the mini-batch cases, which improve the query complexity under the optimal mini-batch size. The experimental results validate our proposed algorithms and theoretical analyses."}}
{"id": "XJMaqB7dv0", "cdate": 1640995200000, "mdate": 1668231119559, "content": {"title": "Continual Learning through Retrieval and Imagination", "abstract": "Continual learning is an intellectual ability of artificial agents to learn new streaming labels from sequential data. The main impediment to continual learning is catastrophic forgetting, a severe performance degradation on previously learned tasks. Although simply replaying all previous data or continuously adding the model parameters could alleviate the issue, it is impractical in real-world applications due to the limited available resources. Inspired by the mechanism of the human brain to deepen its past impression, we propose a novel framework, Deep Retrieval and Imagination (DRI), which consists of two components: 1) an embedding network that constructs a unified embedding space without adding model parameters on the arrival of new tasks; and 2) a generative model to produce additional (imaginary) data based on the limited memory. By retrieving the past experiences and corresponding imaginary data, DRI distills knowledge and rebalances the embedding space to further mitigate forgetting. Theoretical analysis demonstrates that DRI can reduce the loss approximation error and improve the robustness through retrieval and imagination, bringing better generalizability to the network. Extensive experiments show that DRI performs significantly better than the existing state-of-the-art continual learning methods and effectively alleviates catastrophic forgetting."}}
{"id": "Wo0GRfCAE5N", "cdate": 1640995200000, "mdate": 1668231119558, "content": {"title": "Balancing Stability and Plasticity Through Advanced Null Space in Continual Learning", "abstract": "Continual learning is a learning paradigm that learns tasks sequentially with resources constraints, in which the key challenge is stability-plasticity dilemma, i.e., it is uneasy to simultaneously have the stability to prevent catastrophic forgetting of old tasks and the plasticity to learn new tasks well. In this paper, we propose a new continual learning approach, Advanced Null Space (AdNS), to balance the stability and plasticity without storing any old data of previous tasks. Specifically, to obtain better stability, AdNS makes use of low-rank approximation to obtain a novel null space and projects the gradient onto the null space to prevent the interference on the past tasks. To control the generation of the null space, we introduce a non-uniform constraint strength to further reduce forgetting. Furthermore, we present a simple but effective method, intra-task distillation, to improve the performance of the current task. Finally, we theoretically find that null space plays a key role in plasticity and stability, respectively. Experimental results show that the proposed method can achieve better performance compared to state-of-the-art continual learning approaches."}}
{"id": "zc0YnpS90ug", "cdate": 1632875668071, "mdate": null, "content": {"title": "On Exploring Node-feature and Graph-structure Diversities for Node Drop Graph Pooling", "abstract": "Graph pooling is essential in learning effective graph-level representations. One mainstream type of graph pooling is node drop pooling, which preserves the nodes in graphs with top-k calculated significance scores. However, despite being commonly adopted, current node drop pooling methods generally ignore node diversity from the perspectives of node features and graph structures. Therefore, they still obtain graph-level representations suboptimally. To address the issue mentioned above, we propose a novel plug-and-play scheme, termed MID, using a \\textbf{M}ultidimensional score space with two score operations, \\textit{i.e.}, fl\\textbf{I}pscore and \\textbf{D}ropscore, to explore the node-feature and graph-structure diversities in graphs. Specifically, the multidimensional score space depicts the significance of nodes through multiple criteria; the flipsscore encourages the maintenance of dissimilar features, thus preserving the node-feature diversity; and the dropscore forces the model to notice diverse graph structures instead of being stuck in significant local structures. What is more, we evaluate our proposed MID by applying it to a variety of popular node drop pooling methods, including TopKPool, SAGPool, GSAPool, and ASAP.  Extensive experiments on seventeen real-world graph classification datasets demonstrate that our proposed scheme efficiently and consistently brings over 2.8\\% improvements in average when using different backbone models and datasets. The datasets include FRANKENSTEIN, IMDB-B, IMDB-M, REDDIT-B, COLLAB from the social domain and  D\\&D, PROTEINS, NCI1, MUTAG, PTC-MR, NCI109, ENZYMES, MUTAGENICITY, HIV, BBBP, TOXCAST, TOX21 from the biochemical domain.\\footnote{Code will be made publicly available at~\\url{http://github.com/xxx/xxx}."}}
{"id": "11PMuvv3tEO", "cdate": 1632875499330, "mdate": null, "content": {"title": "Lagrangian Generative Adversarial Imitation Learning with Safety", "abstract": "Imitation Learning (IL) merely concentrates on reproducing expert behaviors and could take dangerous actions, which is unbearable in safety-critical scenarios. In this work, we first formalize a practical task of safe imitation learning (Safe IL), which has been long neglected. Taking safety into consideration, we augment Generative Adversarial Imitation Learning (GAIL) with safety constraints and then relax it as an unconstrained saddle point problem by utilizing a Lagrange multiplier, dubbed LGAIL. Then, we apply a two-stage optimization framework to solve LGAIL. Specifically, a discriminator is firstly optimized to measure the similarity between the agent-generated state-action pairs and the expert ones, and then forward reinforcement learning is employed to improve the similarity while considering safety concerns via a Lagrange multiplier. Besides, we provide a theoretical interpretation of LGAIL, which indicates that the proposed LGAIL can be guaranteed to learn a safe policy from unsafe expert data. At last, extensive experiments in OpenAI Safety Gym conclude the effectiveness of our approach."}}
{"id": "ek0RuhPoGiD", "cdate": 1621629883193, "mdate": null, "content": {"title": "Contrastive Graph Poisson Networks: Semi-Supervised Learning with Extremely Limited Labels", "abstract": "Graph Neural Networks (GNNs) have achieved remarkable performance in the task of semi-supervised node classification. However, most existing GNN models require sufficient labeled data for effective network training. Their performance can be seriously degraded when labels are extremely limited. To address this issue, we propose a new framework termed Contrastive Graph Poisson Networks (CGPN) for node classification under extremely limited labeled data. Specifically, our CGPN derives from variational inference; integrates a newly designed Graph Poisson Network (GPN) to effectively propagate the limited labels to the entire graph and a normal GNN, such as Graph Attention Network, that flexibly guides the propagation of GPN; applies a contrastive objective to further exploit the supervision information from the learning process of GPN and GNN models. Essentially, our CGPN can enhance the learning performance of GNNs under extremely limited labels by contrastively propagating the limited labels to the entire graph. We conducted extensive experiments on different types of datasets to demonstrate the superiority of CGPN. "}}
{"id": "gfX1SrAezyC", "cdate": 1609459200000, "mdate": 1668231119591, "content": {"title": "CAA : Channelized Axial Attention for Semantic Segmentation", "abstract": "Spatial and channel attentions, modelling the semantic interdependencies in spatial and channel dimensions respectively, have recently been widely used for semantic segmentation. However, computing spatial and channel attentions separately sometimes causes errors, especially for those difficult cases. In this paper, we propose Channelized Axial Attention (CAA) to seamlessly integrate channel attention and spatial attention into a single operation with negligible computation overhead. Specifically, we break down the dot-product operation of the spatial attention into two parts and insert channel relation in between, allowing for independently optimized channel attention on each spatial location. We further develop grouped vectorization, which allows our model to run with very little memory consumption without slowing down the running speed. Comparative experiments conducted on multiple benchmark datasets, including Cityscapes, PASCAL Context, and COCO-Stuff, demonstrate that our CAA outperforms many state-of-the-art segmentation models (including dual attention) on all tested datasets."}}
