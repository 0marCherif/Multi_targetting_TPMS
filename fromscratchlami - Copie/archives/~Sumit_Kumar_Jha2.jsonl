{"id": "K8Zgd3hoea", "cdate": 1665069644487, "mdate": null, "content": {"title": "Responsible Reasoning with Large Language Models and The Impact of Proper Nouns", "abstract": "Language models with billions of parameters have shown remarkable emergent properties, including the ability to reason on unstructured data. We show that open-science multi-lingual large language models can perform the task of spatial reasoning on two or more entities with significant accuracy. A responsible large language model would perform this spatial reasoning task with the same accuracy regardless of the choice of the names of the entities over which the spatial relationships are defined. However, we show that the accuracies of contemporary large language models are impacted by the choice of proper nouns even when the underlying task ought to be independent of the choice of proper nouns. In this context, we also observe that the conditional log probabilities or beam scores of open-science multi-lingual large language model predictions are not well-calibrated, and the scores do not discriminate between correct and wrong responses in this context."}}
{"id": "hZ2H2Ps5dp6", "cdate": 1663850552909, "mdate": null, "content": {"title": "What's in a name? The Influence of Personal Names on Spatial Reasoning in BLOOM Large Language Models", "abstract": "Large language models have been shown to exhibit reasoning capability. But the ability of these models to truly comprehend the reasoning task is not yet clear. An ideal model capable of reasoning would not be affected by the names of the entities over which the relations are defined. In this paper, we consider an algorithmically generated spatial reasoning task over the names of persons. We show that the choice of names has a significant impact on the reasoning accuracy of BLOOM large language models. Using popular names from different countries of the world, we show that BLOOM large language models are susceptible to undesirable variations in reasoning ability even though the underlying logical reasoning challenge does not depend on these names. We further identify that the conditional log probability scores characterizing the uncertainty in prediction produced by BLOOM models are not well-calibrated and cannot be used to detect such reasoning errors. We then suggest a new approach based on model self-explanations and iterative model introspection that performs better than BLOOM conditional log probability scores in detecting such errors and may help alleviate the bias exhibited by these models."}}
{"id": "jczReTpeJ0N", "cdate": 1663850513124, "mdate": null, "content": {"title": "BLOOM Large Language Models and the Chomsky Hierarchy", "abstract": "We study the performance of BLOOM large language models of different sizes on understanding 12 different languages from the Chomsky hierarchy using few-shot prompts. We investigate whether an increase in the complexity of the languages learned by the larger models can be characterized using the Chomsky hierarchy. We first show that prompting in BLOOM models enables reasoning with a good accuracy on language tasks as diverse as stack manipulation, string reversal, odds first, and interlocked pairing, when the queries are over short strings, that is, small bitwidth bit-vectors from the language. Second, we discover that the two largest models have the highest accuracy on such tasks for prompts with a fixed length, but smaller models are able to achieve similar accuracies with longer prompts. Unlike classical automata or grammar based approaches where algorithms for more complex languages in the Chomsky hierarchy can also recognize simpler languages, we find that the performance of the BLOOM large language models cannot be explained by the complexity of the languages in the Chomsky hierarchy."}}
{"id": "C6CEY8xiA7v", "cdate": 1663850019186, "mdate": null, "content": {"title": "Automaton Distillation: A Neuro-Symbolic Transfer Learning Approach for Deep RL", "abstract": "Reinforcement learning is a powerful tool for finding optimal policies in sequential decision processes. However, deep learning methods suffer from two weaknesses: collecting the amount of agent experience required for practical RL problems is prohibitively expensive, and the learned policies exhibit poor generalization on tasks outside the training distribution. To mitigate these issues, we introduce automaton distillation, a form of neuro-symbolic transfer learning in which Q-value estimates from a teacher are distilled into a low-dimensional representation in the form of an automaton. We then propose two methods for generating Q-value estimates: static transfer, which reasons over an abstract MDP constructed based on prior knowledge, and dynamic transfer, where symbolic information is extracted from a DQN teacher. The resulting Q-value estimates from either method are used to bootstrap learning in the target environment via a modified DQN loss function. We list several failure modes of existing automaton-based transfer methods and demonstrate that both static and dynamic automaton distillation decrease the time required to find optimal policies for various decision tasks."}}
{"id": "yTg8IXnz4sm", "cdate": 1640995200000, "mdate": 1667335997416, "content": {"title": "Deep Learning Toolkit-Driven Equivalence Checking of Flow-Based Computing Systems", "abstract": "The processor-memory bottleneck inherent to von Neumann architectures has encouraged the development of alternative computing paradigms. One such paradigm is flow-based in-memory computing using nanoscale crossbars. Recent improvements to design automation tools has scaled the crossbar designs to over one million memristors and over one hundred input variables. However, the state-of-the-art verification method using graph reachability cannot verify the correctness of crossbar designs with more than twenty input variables. In this paper, we propose an equivalence checking technique that natively leverages existing deep learning infrastructure. We achieve this by observing an analogy between a memristor crossbar and recurrent neural networks (RNNs), which allows equivalence checking to be efficiently performed using neural network inference. Using benchmark circuits from the RevLib and MCNC suites, we show that our proposed method can, on average, verify the correctness of a design 166x faster than the state-of-the-art method."}}
{"id": "yRA7VUwjoU6", "cdate": 1640995200000, "mdate": 1667335997437, "content": {"title": "A Game-theoretic Understanding of Repeated Explanations in ML Models", "abstract": "This paper formally models the strategic repeated interactions between a system, comprising of a machine learning (ML) model and associated explanation method, and an end-user who is seeking a prediction/label and its explanation for a query/input, by means of game theory. In this game, a malicious end-user must strategically decide when to stop querying and attempt to compromise the system, while the system must strategically decide how much information (in the form of noisy explanations) it should share with the end-user and when to stop sharing, all without knowing the type (honest/malicious) of the end-user. This paper formally models this trade-off using a continuous-time stochastic Signaling game framework and characterizes the Markov perfect equilibrium state within such a framework."}}
{"id": "x_j9nq_JI9", "cdate": 1640995200000, "mdate": 1667335997455, "content": {"title": "PATH: evaluation of boolean logic using path-based in-memory computing", "abstract": "Processing in-memory breaks von Neumann-based constructs to accelerate data-intensive applications. Noteworthy efforts have been devoted to executing Boolean logic using digital in-memory computing. The limitation of state-of-the-art paradigms is that they heavily rely on repeatedly switching the state of the non-volatile resistive devices using expensive WRITE operations. In this paper, we propose a new in-memory computing paradigm called path-based computing for evaluating Boolean logic. Computation within the paradigm is performed using a one-time expensive compile phase and a fast and efficient evaluation phase. The key property of the paradigm is that the execution phase only involves cheap READ operations. Moreover, a synthesis tool called PATH is proposed to automatically map computation to a single crossbar design. The PATH tool also supports the synthesis of path-based computing systems where the total number of crossbars and the number of inter-crossbar connections are minimized. We evaluate the proposed paradigm using 10 circuits from the RevLib benchmark suite. Compared with state-of-the-art digital in-memory computing paradigms, path-based computing improves energy and latency up to 4.7X and 8.5X, respectively."}}
{"id": "uKr4Y1yagV", "cdate": 1640995200000, "mdate": 1667335997401, "content": {"title": "Towards resilient analog in-memory deep learning via data layout re-organization", "abstract": "Processing in-memory paves the way for neural network inference engines. An arising challenge is to develop the software/hardware interface to automatically compile deep learning models onto in-memory computing platforms. In this paper, we observe that the data layout organization of a deep neural network (DNN) model directly impacts the model's classification accuracy. This stems from that the resistive parasitics within a crossbar introduces a dependency between the matrix data and the precision of the analog computation. To minimize the impact of the parasitics, we first perform a case study to understand the underlying matrix properties that result in computation with low and high precision, respectively. Next, we propose the XORG framework that performs data layout organization for DNNs deployed on in-memory computing platforms. The data layout organization improves precision by optimizing the weight matrix to crossbar assignments at compile time. The experimental results show that the XORG framework improves precision with up to 3.2X and 31% on the average. When accelerating DNNs using XORG, the write bit-accuracy requirements are relaxed with 1-bit and the robustness to random telegraph noise (RTN) is improved."}}
{"id": "l3HgUD0BQNq", "cdate": 1640995200000, "mdate": 1667335997412, "content": {"title": "A Game-theoretic Understanding of Repeated Explanations in ML Models", "abstract": "This paper formally models the strategic repeated interactions between a system, comprising of a machine learning (ML) model and associated explanation method, and an end-user who is seeking a prediction/label and its explanation for a query/input, by means of game theory. In this game, a malicious end-user must strategically decide when to stop querying and attempt to compromise the system, while the system must strategically decide how much information (in the form of noisy explanations) it should share with the end-user and when to stop sharing, all without knowing the type (honest/malicious) of the end-user. This paper formally models this trade-off using a continuous-time stochastic Signaling game framework and characterizes the Markov perfect equilibrium state within such a framework."}}
{"id": "iEJcU2jNViT", "cdate": 1640995200000, "mdate": 1667335997397, "content": {"title": "Shaping Noise for Robust Attributions in Neural Stochastic Differential Equations", "abstract": "Neural SDEs with Brownian motion as noise lead to smoother attributions than traditional ResNets. Various attribution methods such as saliency maps, integrated gradients, DeepSHAP and DeepLIFT have been shown to be more robust for neural SDEs than for ResNets using the recently proposed sensitivity metric. In this paper, we show that neural SDEs with adaptive attribution-driven noise lead to even more robust attributions and smaller sensitivity metrics than traditional neural SDEs with Brownian motion as noise. In particular, attribution-driven shaping of noise leads to 6.7%, 6.9% and 19.4% smaller sensitivity metric for integrated gradients computed on three discrete approximations of neural SDEs with standard Brownian motion noise: stochastic ResNet-50, WideResNet-101 and ResNeXt-101 models respectively. The neural SDE model with adaptive attribution-driven noise leads to 25.7% and 4.8% improvement in the SIC metric over traditional ResNets and Neural SDEs with Brownian motion as noise. To the best of our knowledge, we are the first to propose the use of attributions for shaping the noise injected in neural SDEs, and demonstrate that this process leads to more robust attributions than traditional neural SDEs with standard Brownian motion as noise."}}
