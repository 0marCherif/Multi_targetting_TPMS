{"id": "_DOaUiLGUO", "cdate": 1640995200000, "mdate": 1667334160057, "content": {"title": "Neural Inertial Localization", "abstract": "This paper proposes the inertial localization problem, the task of estimating the absolute location from a sequence of inertial sensor measurements. This is an exciting and unexplored area of indoor localization research, where we present a rich dataset with 53 hours of inertial sensor data and the associated ground truth locations. We developed a solution, dubbed neural inertial localization (NILoc) which 1) uses a neural inertial navigation technique to turn inertial sensor history to a sequence of velocity vectors; then 2) employs a transformer-based neural architecture to find the device location from the sequence of velocities. We only use an IMU sensor, which is energy efficient and privacy preserving compared to WiFi, cameras, and other data sources. Our approach is significantly faster and achieves competitive results even compared with state-of-the-art methods that require a floorplan and run 20 to 30 times slower. We share our code, model and data at https://sachini.github.io/niloc."}}
{"id": "PU0ZYewt5Lm", "cdate": 1640995200000, "mdate": 1667334160055, "content": {"title": "Single User WiFi Structure from Motion in the Wild", "abstract": "This paper proposes a novel motion estimation algorithm using WiFi networks and IMU sensor data in large uncontrolled environments, dubbed \u201cWiFi Structure-from-Motion\u201d (WiFi SfM). Given smartphone sensor data through day-to-day activities from a single user over a month, our WiFi SfM algorithm estimates smartphone motion tra-jectories and the structure of the environment represented as a WiFi radio map. The approach 1) establishes frame-to-frame correspondences based on WiFi fingerprints while exploiting our repetitive behavior patterns; 2) aligns trajectories via bundle adjustment; and 3) trains a self-supervised neural network to extract further motion constraints. We have col-lected 235 hours of smartphone data, spanning 38 days of daily activities in a university campus. Our experiments demonstrate the effectiveness of our approach over the competing methods with qualitative evaluations of the estimated motions and quantitative evaluations of indoor localization accuracy based on the reconstructed WiFi radio map. The WiFi SfM technology will potentially allow digital mapping companies to build better radio maps automatically by asking users to share WiFi/IMU sensor data in their daily activities."}}
{"id": "7wUY58jXKv4", "cdate": 1640995200000, "mdate": 1667334160002, "content": {"title": "Neural Inertial Localization", "abstract": "This paper proposes the inertial localization problem, the task of estimating the absolute location from a sequence of inertial sensor measurements. This is an exciting and unexplored area of indoor localization research, where we present a rich dataset with 53 hours of inertial sensor data and the associated ground truth locations. We developed a solution, dubbed neural inertial localization (NILoc) which 1) uses a neural inertial navigation technique to turn inertial sensor history to a sequence of velocity vectors; then 2) employs a transformer-based neural architecture to find the device location from the sequence of velocities. We only use an IMU sensor, which is energy efficient and privacy preserving compared to WiFi, cameras, and other data sources. Our approach is significantly faster and achieves competitive results even compared with state-of-the-art methods that require a floorplan and run 20 to 30 times slower. We share our code, model and data at https://sachini.github.io/niloc."}}
{"id": "qqa4G5ZOrGO", "cdate": 1609459200000, "mdate": 1667334160056, "content": {"title": "Fusion-DHL: WiFi, IMU, and Floorplan Fusion for Dense History of Locations in Indoor Environments", "abstract": "The paper proposes a multi-modal sensor fusion algorithm that fuses WiFi, IMU, and floorplan information to infer an accurate and dense location history in indoor environments. The algorithm uses 1) an inertial navigation algorithm to estimate a relative motion trajectory from IMU sensor data; 2) a WiFi-based localization API in industry to obtain positional constraints and geo-localize the trajectory; and 3) a convolutional neural network to refine the location history to be consistent with the floorplan. We have developed a data acquisition app to build a new dataset with WiFi, IMU, and floorplan data with ground-truth positions at 4 university buildings and 3 shopping malls. Our qualitative and quantitative evaluations demonstrate that the proposed system is able to produce twice as accurate and a few orders of magnitude denser location history than the current standard, while requiring minimal additional energy consumption. We will publicly share our code, data and models."}}
{"id": "F7HvAJlBwGk", "cdate": 1609459200000, "mdate": 1667334160060, "content": {"title": "Fusion-DHL: WiFi, IMU, and Floorplan Fusion for Dense History of Locations in Indoor Environments", "abstract": "The paper proposes a multi-modal sensor fusion algorithm that fuses WiFi, IMU, and floorplan information to infer an accurate and dense location history in indoor environments. The algorithm uses 1) an inertial navigation algorithm to estimate a relative motion trajectory from IMU sensor data; 2) a WiFi-based localization API in industry to obtain positional constraints and geo-localize the trajectory; and 3) a convolutional neural network to refine the location history to be consistent with the floorplan. We have developed a data acquisition app to build a new dataset with WiFi, IMU, and floorplan data with ground-truth positions at 4 university buildings and 3 shopping malls. Our qualitative and quantitative evaluations demonstrate that the proposed system is able to produce twice as accurate and a few orders of magnitude denser location history than the current standard, while requiring minimal additional energy consumption. We will publicly share our code and models."}}
{"id": "kkNq_AvzR2", "cdate": 1577836800000, "mdate": 1667334160057, "content": {"title": "RoNIN: Robust Neural Inertial Navigation in the Wild: Benchmark, Evaluations, & New Methods", "abstract": "This paper sets a new foundation for data-driven inertial navigation research, where the task is the estimation of horizontal positions and heading direction of a moving subject from a sequence of IMU sensor measurements from a phone. In contrast to existing methods, our method can handle varying phone orientations and placements.More concretely, the paper presents 1) a new benchmark containing more than 40 hours of IMU sensor data from 100 human subjects with ground-truth 3D trajectories under natural human motions; 2) novel neural inertial navigation architectures, making significant improvements for challenging motion cases; and 3) qualitative and quantitative evaluations of the competing methods over three inertial navigation benchmarks. We share the code and data to promote further research. (http://ronin.cs.sfu.ca)."}}
{"id": "VT-jPXcYHR", "cdate": 1546300800000, "mdate": 1667334160056, "content": {"title": "Deep Attention Models for Human Tracking Using RGBD", "abstract": "Visual tracking performance has long been limited by the lack of better appearance models. These models fail either where they tend to change rapidly, like in motion-based tracking, or where accurate information of the object may not be available, like in color camouflage (where background and foreground colors are similar). This paper proposes a robust, adaptive appearance model which works accurately in situations of color camouflage, even in the presence of complex natural objects. The proposed model includes depth as an additional feature in a hierarchical modular neural framework for online object tracking. The model adapts to the confusing appearance by identifying the stable property of depth between the target and the surrounding object(s). The depth complements the existing RGB features in scenarios when RGB features fail to adapt, hence becoming unstable over a long duration of time. The parameters of the model are learned efficiently in the Deep network, which consists of three modules: (1) The spatial attention layer, which discards the majority of the background by selecting a region containing the object of interest; (2) the appearance attention layer, which extracts appearance and spatial information about the tracked object; and (3) the state estimation layer, which enables the framework to predict future object appearance and location. Three different models were trained and tested to analyze the effect of depth along with RGB information. Also, a model is proposed to utilize only depth as a standalone input for tracking purposes. The proposed models were also evaluated in real-time using KinectV2 and showed very promising results. The results of our proposed network structures and their comparison with the state-of-the-art RGB tracking model demonstrate that adding depth significantly improves the accuracy of tracking in a more challenging environment (i.e., cluttered and camouflaged environments). Furthermore, the results of depth-based models showed that depth data can provide enough information for accurate tracking, even without RGB information."}}
{"id": "M49M_7NUs_", "cdate": 1546300800000, "mdate": 1667334160055, "content": {"title": "RoNIN: Robust Neural Inertial Navigation in the Wild: Benchmark, Evaluations, and New Methods", "abstract": "This paper sets a new foundation for data-driven inertial navigation research, where the task is the estimation of positions and orientations of a moving subject from a sequence of IMU sensor measurements. More concretely, the paper presents 1) a new benchmark containing more than 40 hours of IMU sensor data from 100 human subjects with ground-truth 3D trajectories under natural human motions; 2) novel neural inertial navigation architectures, making significant improvements for challenging motion cases; and 3) qualitative and quantitative evaluations of the competing methods over three inertial navigation benchmarks. We will share the code and data to promote further research."}}
{"id": "FQU1fUdwLYQ", "cdate": 1514764800000, "mdate": 1667334160055, "content": {"title": "Effective Learning Content Offering in MOOCs with Virtual Reality - An Exploratory Study on Learner Experience", "abstract": ""}}
{"id": "lKMVquanNI", "cdate": 1483228800000, "mdate": 1667334160055, "content": {"title": "Generation of intermediate viewpoints for scalable adaptation of real world environments for virtual reality", "abstract": "Virtual Reality (VR) is a rapidly growing field with innovative applications. Two main forms of VR content can be identified in current applications: Computer generated 3D environments and adaptations of real world scenarios through means of digital imagery and video. While the latter provides a more realistic experience, scalable content generation methods for adaptations of real world environments to virtual space is still unavailable. Needless to say achieving unrestricted navigability in such environments, on a mobile platform with near real time response is quite challenging. The paper presents the algorithm for scalable intermediate view generation, devised as a part of a research to develop an universal content generation platform for real world environments in virtual reality. The algorithm uses adjacent equirectangular images to recreate the 360\u00b0 \u00d7 180\u00b0 view of any point in-between, taking directional deformation into account. Input can be equirectangular images captured using any device, including 360 cameras and panoramas captured through mobile phones. The output can be rendered on specialized VR headsets or mobile phones. The algorithm is implemented with mobile platform as the target, thus rendering is optimized for limited resource utilization. The paper presents the concept, design and optimization used in this method."}}
