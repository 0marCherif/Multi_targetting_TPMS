{"id": "0OtYm1czQH", "cdate": 1680307200000, "mdate": 1682494887643, "content": {"title": "Efficient and stable circular cartograms for time-varying data by using improved elastic beam algorithm and hierarchical optimization", "abstract": "The circular cartogram, also known as the famous DorlingMap, is widely used to visualize geographical statistics by representing geographical regions as circles. However, all existing approaches for circular cartograms are only designed for static data. While applying these approaches for time-varying data, the circle locations in each circular cartogram are recomputed separately and will result in low efficiency and low visual stability between sequential circle cartograms. To generate visually stable circular cartograms for time-varying data efficiently, we propose a novel approach by improving the elastic beam algorithm with a hierarchical optimization strategy. First, the time-varying data at different time points are grouped using a hierarchical clustering method based on their similarity, and a hierarchy is then built for their corresponding circular cartograms. Second, we generate intermediate circle locations level by level for clusters of circular cartograms according to the built hierarchy with an improved elastic beam algorithm iteratively. The elastic beam algorithm is improved in its proximity graph construction and force computation by considering that the algorithm will be applied to displace circles in a cluster of circular cartograms. The iterative process stops until we obtain satisfactory circular cartograms for each time point. The evaluation results indicate that the proposed approach can achieve a higher quality (184.85%\u2191 and 265.69%\u2191) on visual stability, and a higher efficiency (58.54%\u2191 and 73.96%\u2191) with almost the same quality on overlap avoidance and relation maintenance by comparing to the existing approaches. Project website: https://github.com/TrentonWei/DorlingMap . Graphical abstract"}}
{"id": "pVZY3dpn-D", "cdate": 1672531200000, "mdate": 1681725133480, "content": {"title": "On Distinctive Image Captioning via Comparing and Reweighting", "abstract": "Recent image captioning models are achieving impressive results based on popular metrics, i.e., BLEU, CIDEr, and SPICE. However, focusing on the most popular metrics that only consider the overlap between the generated captions and human annotation could result in using common words and phrases, which lacks distinctiveness, i.e., many similar images have the same caption. In this paper, we aim to improve the distinctiveness of image captions via comparing and reweighting with a set of similar images. First, we propose a distinctiveness metric\u2014between-set CIDEr (CIDErBtw) to evaluate the distinctiveness of a caption with respect to those of similar images. Our metric reveals that the human annotations of each image in the MSCOCO dataset are not equivalent based on distinctiveness; however, previous works normally treat the human annotations equally during training, which could be a reason for generating less distinctive captions. In contrast, we reweight each ground-truth caption according to its distinctiveness during training. We further integrate a long-tailed weight strategy to highlight the rare words that contain more information, and captions from the similar image set are sampled as negative examples to encourage the generated sentence to be unique. Finally, extensive experiments are conducted, showing that our proposed approach significantly improves both distinctiveness (as measured by CIDErBtw and retrieval metrics) and accuracy (e.g., as measured by CIDEr) for a wide variety of image captioning baselines. These results are further confirmed through a user study."}}
{"id": "TdHoJ-Z1SZ", "cdate": 1672531200000, "mdate": 1682494887668, "content": {"title": "Inferring High-level Geographical Concepts via Knowledge Graph and Multi-scale Data Integration: A Case Study of C-shaped Building Pattern Recognition", "abstract": "Effective building pattern recognition is critical for understanding urban form, automating map generalization, and visualizing 3D city models. Most existing studies use object-independent methods based on visual perception rules and proximity graph models to extract patterns. However, because human vision is a part-based system, pattern recognition may require decomposing shapes into parts or grouping them into clusters. Existing methods may not recognize all visually aware patterns, and the proximity graph model can be inefficient. To improve efficiency and effectiveness, we integrate multi-scale data using a knowledge graph, focusing on the recognition of C-shaped building patterns. First, we use a property graph to represent the relationships between buildings within and across different scales involved in C-shaped building pattern recognition. Next, we store this knowledge graph in a graph database and convert the rules for C-shaped pattern recognition and enrichment into query conditions. Finally, we recognize and enrich C-shaped building patterns using rule-based reasoning in the built knowledge graph. We verify the effectiveness of our method using multi-scale data with three levels of detail (LODs) collected from the Gaode Map. Our results show that our method achieves a higher recall rate of 26.4% for LOD1, 20.0% for LOD2, and 9.1% for LOD3 compared to existing approaches. We also achieve recognition efficiency improvements of 0.91, 1.37, and 9.35 times, respectively."}}
{"id": "oilgbNOWmEu", "cdate": 1640995200000, "mdate": 1668419348279, "content": {"title": "Distinctive Image Captioning via CLIP Guided Group Optimization", "abstract": "Image captioning models are usually trained according to human annotated ground-truth captions, which could generate accurate but generic captions. In this paper, we focus on generating distinctive captions that can distinguish the target image from other similar images. To evaluate the distinctiveness of captions, we introduce a series of metrics that use large-scale vision-language pre-training model CLIP to quantify the distinctiveness. To further improve the distinctiveness of captioning models, we propose a simple and effective training strategy that trains the model by comparing target image with similar image group and optimizing the group embedding gap. Extensive experiments are conducted on various baseline models to demonstrate the wide applicability of our strategy and the consistency of metric results with human evaluation. By comparing the performance of our best model with existing state-of-the-art models, we claim that our model achieves new state-of-the-art towards distinctiveness objective."}}
{"id": "hum1fcOKQ-", "cdate": 1640995200000, "mdate": 1668419348317, "content": {"title": "Attribute Prototype Network for Any-Shot Learning", "abstract": "Any-shot image classification allows to recognize novel classes with only a few or even zero samples. For the task of zero-shot learning, visual attributes have been shown to play an important role, while in the few-shot regime, the effect of attributes is under-explored. To better transfer attribute-based knowledge from seen to unseen classes, we argue that an image representation with integrated attribute localization ability would be beneficial for any-shot, i.e. zero-shot and few-shot, image classification tasks. To this end, we propose a novel representation learning framework that jointly learns discriminative global and local features using only class-level attributes. While a visual-semantic embedding layer learns global features, local features are learned through an attribute prototype network that simultaneously regresses and decorrelates attributes from intermediate features. Furthermore, we introduce a zoom-in module that localizes and crops the informative regions to encourage the network to learn informative features explicitly. We show that our locality augmented image representations achieve a new state-of-the-art on challenging benchmarks, i.e. CUB, AWA2, and SUN. As an additional benefit, our model points to the visual evidence of the attributes in an image, confirming the improved attribute localization ability of our image representation. The attribute localization is evaluated quantitatively with ground truth part annotations, qualitatively with visualizations, and through well-designed user studies."}}
{"id": "flKYEv7FJ10", "cdate": 1640995200000, "mdate": 1668524817370, "content": {"title": "Learning Prototype via Placeholder for Zero-shot Recognition", "abstract": "Zero-shot learning (ZSL) aims to recognize unseen classes by exploiting semantic descriptions shared between seen classes and unseen classes. Current methods show that it is effective to learn visual-semantic alignment by projecting semantic embeddings into the visual space as class prototypes. However, such a projection function is only concerned with seen classes. When applied to unseen classes, the prototypes often perform suboptimally due to domain shift. In this paper, we propose to learn prototypes via placeholders, termed LPL, to eliminate the domain shift between seen and unseen classes. Specifically, we combine seen classes to hallucinate new classes which play as placeholders of the unseen classes in the visual and semantic space. Placed between seen classes, the placeholders encourage prototypes of seen classes to be highly dispersed. And more space is spared for the insertion of well-separated unseen ones. Empirically, well-separated prototypes help counteract visual-semantic misalignment caused by domain shift. Furthermore, we exploit a novel semantic-oriented fine-tuning to guarantee the semantic reliability of placeholders. Extensive experiments on five benchmark datasets demonstrate the significant performance gain of LPL over the state-of-the-art methods. Code is available at https://github.com/zaiquanyang/LPL."}}
{"id": "WJJzuAMvjw", "cdate": 1640995200000, "mdate": 1668419348383, "content": {"title": "Learning Prototype via Placeholder for Zero-shot Recognition", "abstract": "Zero-shot learning (ZSL) aims to recognize unseen classes by exploiting semantic descriptions shared between seen classes and unseen classes. Current methods show that it is effective to learn visual-semantic alignment by projecting semantic embeddings into the visual space as class prototypes. However, such a projection function is only concerned with seen classes. When applied to unseen classes, the prototypes often perform suboptimally due to domain shift. In this paper, we propose to learn prototypes via placeholders, termed LPL, to eliminate the domain shift between seen and unseen classes. Specifically, we combine seen classes to hallucinate new classes which play as placeholders of the unseen classes in the visual and semantic space. Placed between seen classes, the placeholders encourage prototypes of seen classes to be highly dispersed. And more space is spared for the insertion of well-separated unseen ones. Empirically, well-separated prototypes help counteract visual-semantic misalignment caused by domain shift. Furthermore, we exploit a novel semantic-oriented fine-tuning method to guarantee the semantic reliability of placeholders. Extensive experiments on five benchmark datasets demonstrate the significant performance gain of LPL over the state-of-the-art methods."}}
{"id": "PYfykj6cIn3", "cdate": 1640995200000, "mdate": 1668419348278, "content": {"title": "Attribute Prototype Network for Any-Shot Learning", "abstract": "Any-shot image classification allows to recognize novel classes with only a few or even zero samples. For the task of zero-shot learning, visual attributes have been shown to play an important role, while in the few-shot regime, the effect of attributes is under-explored. To better transfer attribute-based knowledge from seen to unseen classes, we argue that an image representation with integrated attribute localization ability would be beneficial for any-shot, i.e. zero-shot and few-shot, image classification tasks. To this end, we propose a novel representation learning framework that jointly learns discriminative global and local features using only class-level attributes. While a visual-semantic embedding layer learns global features, local features are learned through an attribute prototype network that simultaneously regresses and decorrelates attributes from intermediate features. Furthermore, we introduce a zoom-in module that localizes and crops the informative regions to encourage the network to learn informative features explicitly. We show that our locality augmented image representations achieve a new state-of-the-art on challenging benchmarks, i.e. CUB, AWA2, and SUN. As an additional benefit, our model points to the visual evidence of the attributes in an image, confirming the improved attribute localization ability of our image representation. The attribute localization is evaluated quantitatively with ground truth part annotations, qualitatively with visualizations, and through well-designed user studies."}}
{"id": "Kf3nV3C6Jwp", "cdate": 1640995200000, "mdate": 1668524817308, "content": {"title": "On Distinctive Image Captioning via Comparing and Reweighting", "abstract": "Recent image captioning models are achieving impressive results based on popular metrics, i.e., BLEU, CIDEr, and SPICE. However, focusing on the most popular metrics that only consider the overlap between the generated captions and human annotation could result in using common words and phrases, which lacks distinctiveness, i.e., many similar images have the same caption. In this paper, we aim to improve the distinctiveness of image captions via comparing and reweighting with a set of similar images. First, we propose a distinctiveness metric -- between-set CIDEr (CIDErBtw) to evaluate the distinctiveness of a caption with respect to those of similar images. Our metric reveals that the human annotations of each image in the MSCOCO dataset are not equivalent based on distinctiveness; however, previous works normally treat the human annotations equally during training, which could be a reason for generating less distinctive captions. In contrast, we reweight each ground-truth caption according to its distinctiveness during training. We further integrate a long-tailed weight strategy to highlight the rare words that contain more information, and captions from the similar image set are sampled as negative examples to encourage the generated sentence to be unique. Finally, extensive experiments are conducted, showing that our proposed approach significantly improves both distinctiveness (as measured by CIDErBtw and retrieval metrics) and accuracy (e.g., as measured by CIDEr) for a wide variety of image captioning baselines. These results are further confirmed through a user study."}}
{"id": "HNA3g7xnlqS", "cdate": 1640995200000, "mdate": 1668524817502, "content": {"title": "Circular cartograms via the elastic beam algorithm originated from cartographic generalization", "abstract": "The circular cartogram, also known as the Dorling map, is a widely used tool for visualizing statistical data. It represents regions as circles with their areas in proportion to the statistical values and requires circle displacement to avoid overlap and maintain spatial relationships. In this paper, we propose a new approach for circular cartogram production that utilizes the elastic beam displacement algorithm in cartographic generalization. First, the initial circles are generated with their areas in proportion to the statistical values. Second, an elastic beam structure is built as a proximity graph based on the spatial relations between the circles. Third, the circles violating the quality requirements are considered to have a force on the nodes of a beam. Fourth, the elastic beam algorithm is applied to assign forces for each node to determine the new positions of the circles. Steps two through four are repeated until a circular cartogram that meets the defined quality requirements is obtained. The experiments indicate that the proposed approach can successfully generate circular cartograms without overlaps while maintaining topology and relative relationships with higher quality than existing approaches."}}
