{"id": "RwfHI3-qda", "cdate": 1672531200000, "mdate": 1682344972598, "content": {"title": "NeuroBench: Advancing Neuromorphic Computing through Collaborative, Fair and Representative Benchmarking", "abstract": "The field of neuromorphic computing holds great promise in terms of advancing computing efficiency and capabilities by following brain-inspired principles. However, the rich diversity of techniques employed in neuromorphic research has resulted in a lack of clear standards for benchmarking, hindering effective evaluation of the advantages and strengths of neuromorphic methods compared to traditional deep-learning-based methods. This paper presents a collaborative effort, bringing together members from academia and the industry, to define benchmarks for neuromorphic computing: NeuroBench. The goals of NeuroBench are to be a collaborative, fair, and representative benchmark suite developed by the community, for the community. In this paper, we discuss the challenges associated with benchmarking neuromorphic solutions, and outline the key features of NeuroBench. We believe that NeuroBench will be a significant step towards defining standards that can unify the goals of neuromorphic computing and drive its technological progress. Please visit neurobench.ai for the latest updates on the benchmark tasks and metrics."}}
{"id": "LwaoSniDVt", "cdate": 1672531200000, "mdate": 1683917368997, "content": {"title": "The Hardware Impact of Quantization and Pruning for Weights in Spiking Neural Networks", "abstract": "Energy efficient implementations and deployments of Spiking neural networks (SNNs) have been of great interest due to the possibility of developing artificial systems that can achieve the computational powers and energy efficiency of the biological brain. Efficient implementations of SNNs on modern digital hardware are also inspired by advances in machine learning and deep neural networks (DNNs). Two techniques widely employed in the efficient deployment of DNNs -- the quantization and pruning of parameters, can both compress the model size, reduce memory footprints, and facilitate low-latency execution. The interaction between quantization and pruning and how they might impact model performance on SNN accelerators is currently unknown. We study various combinations of pruning and quantization in isolation, cumulatively, and simultaneously (jointly) to a state-of-the-art SNN targeting gesture recognition for dynamic vision sensor cameras (DVS). We show that this state-of-the-art model is amenable to aggressive parameter quantization, not suffering from any loss in accuracy down to ternary weights. However, pruning only maintains iso-accuracy up to 80% sparsity, which results in 45% more energy than the best quantization on our architectural model. Applying both pruning and quantization can result in an accuracy loss to offer a favourable trade-off on the energy-accuracy Pareto-frontier for the given hardware configuration."}}
{"id": "1KOlEbtZL-", "cdate": 1672531200000, "mdate": 1683898650352, "content": {"title": "Mixed Precision Post Training Quantization of Neural Networks with Sensitivity Guided Search", "abstract": "Serving large-scale machine learning (ML) models efficiently and with low latency has become challenging owing to increasing model size and complexity. Quantizing models can simultaneously reduce memory and compute requirements, facilitating their widespread access. However, for large models not all layers are equally amenable to the same numerical precision and aggressive quantization can lead to unacceptable loss in model accuracy. One approach to prevent this accuracy degradation is mixed-precision quantization, which allows different tensors to be quantized to varying levels of numerical precision, leveraging the capabilities of modern hardware. Such mixed-precision quantiztaion can more effectively allocate numerical precision to different tensors `as needed' to preserve model accuracy while reducing footprint and compute latency. In this paper, we propose a method to efficiently determine quantization configurations of different tensors in ML models using post-training mixed precision quantization. We analyze three sensitivity metrics and evaluate them for guiding configuration search of two algorithms. We evaluate our method for computer vision and natural language processing and demonstrate latency reductions of up to 27.59% and 34.31% compared to the baseline 16-bit floating point model while guaranteeing no more than 1% accuracy degradation."}}
{"id": "fULme51bMP", "cdate": 1640995200000, "mdate": 1683917369388, "content": {"title": "Ruby: Improving Hardware Efficiency for Tensor Algebra Accelerators Through Imperfect Factorization", "abstract": "Finding high-quality mappings of Deep Neural Network (DNN) models onto tensor accelerators is critical for efficiency. State-of-the-art mapping exploration tools use remainderless (i.e., perfect) factorization to allocate hardware resources, through tiling the tensors, based on factors of tensor dimensions. This limits the size of the search space, (i.e., mapspace), but can lead to low resource utilization. We introduce a new mapspace, Ruby, that adds remainders (i.e., imperfect factorization) to expand the mapspace with high-quality mappings for user-defined architectures. This expansion allows us to allocate resources more precisely by generating tile sizes that better conform to hardware resources. However, this mapspace expansion also incurs an increase in the number of unique mappings. Consequently, this paper studies the trade-off between Ruby\u2019s mapspace expansion and mapping quality. We propose Ruby-S (Spatial) to only employ imperfect factorization towards improved parallelism. Ruby-S incurs a moderate mapspace expansion while reducing energy-delay product (EDP) up to 50% when implementing ResNet-50 on an Eyeriss-like architecture with an average improvement of 20%. For the most part, this improvement can be attributed to higher compute utilization. EDP on a Simba-like architecture improves up to 40% with an average of 10%. For DeepBench workloads Ruby-S yields improvements of up to 45% with an average improvement of 10% on an Eyeriss-like architecture. Ruby-S is robust to accelerator configurations and improves EDP by 20% on average, with a maximum improvement of 55% when implementing ResNet-50 on different accelerator configurations. Ruby-S mappings form a new Pareto frontier, improving the performance of previous configurations by an average of 30% and 20% for ResNet-50 and DeepBench workloads respectively."}}
{"id": "W_taYkcQJq", "cdate": 1640995200000, "mdate": 1683917369326, "content": {"title": "A compute-in-memory chip based on resistive random-access memory", "abstract": "A compute-in-memory neural-network inference accelerator based on resistive random-access memory simultaneously improves energy efficiency, flexibility and accuracy compared with existing hardware by co-optimizing across all hierarchies of the design."}}
{"id": "JgMnQvMoyM", "cdate": 1640995200000, "mdate": 1683917369110, "content": {"title": "Edge Inference with Fully Differentiable Quantized Mixed Precision Neural Networks", "abstract": "The large computing and memory cost of deep neural networks (DNNs) often precludes their use in resource-constrained devices. Quantizing the parameters and operations to lower bit-precision offers substantial memory and energy savings for neural network inference, facilitating the use of DNNs on edge computing platforms. Recent efforts at quantizing DNNs have employed a range of techniques encompassing progressive quantization, step-size adaptation, and gradient scaling. This paper proposes a new quantization approach for mixed precision convolutional neural networks (CNNs) targeting edge-computing. Our method establishes a new pareto frontier in model accuracy and memory footprint demonstrating a range of quantized models, delivering best-in-class accuracy below 4.3 MB of weights (wgts.) and activations (acts.). Our main contributions are: (i) hardware-aware heterogeneous differentiable quantization with tensor-sliced learned precision, (ii) targeted gradient modification for wgts. and acts. to mitigate quantization errors, and (iii) a multi-phase learning schedule to address instability in learning arising from updates to the learned quantizer and model parameters. We demonstrate the effectiveness of our techniques on the ImageNet dataset across a range of models including EfficientNet-Lite0 (e.g., 4.14MB of wgts. and acts. at 67.66% accuracy) and MobileNetV2 (e.g., 3.51MB wgts. and acts. at 65.39% accuracy)."}}
{"id": "mpkg1c7X3RX", "cdate": 1609459200000, "mdate": 1683917369056, "content": {"title": "Graph Coloring Using Coupled Oscillator-Based Dynamical Systems", "abstract": "Graph coloring is a NP-hard problem, and computing the solution on a digital computer entails an exponential increase in the computing resources (time, memory) with increasing problem size. This has motivated the search for alternate and more efficient non-Boolean approaches. Here, we experimentally demonstrate the solution to this problem using the phase dynamics of coupled oscillators. Using a 30-oscillator IC platform with reconfigurable all-to-all coupling and minimal post-processing, our approach achieves 98% accuracy in detecting (near-) optimal solutions within 1 color of the optimal solution in comparison to the 77% accuracy achieved with the heuristic Johnson algorithm. Additionally, we propose a new local search-based post-processing scheme to improve the quality of the coloring solution. Finally, using circuit simulations, we demonstrate the scalability and speed up (~ 100\u00d7) achievable with the above approach in larger graphs."}}
{"id": "GPzH6g4hWz-", "cdate": 1609459200000, "mdate": 1683917369127, "content": {"title": "Edge AI without Compromise: Efficient, Versatile and Accurate Neurocomputing in Resistive Random-Access Memory", "abstract": "Realizing today's cloud-level artificial intelligence functionalities directly on devices distributed at the edge of the internet calls for edge hardware capable of processing multiple modalities of sensory data (e.g. video, audio) at unprecedented energy-efficiency. AI hardware architectures today cannot meet the demand due to a fundamental \"memory wall\": data movement between separate compute and memory units consumes large energy and incurs long latency. Resistive random-access memory (RRAM) based compute-in-memory (CIM) architectures promise to bring orders of magnitude energy-efficiency improvement by performing computation directly within memory. However, conventional approaches to CIM hardware design limit its functional flexibility necessary for processing diverse AI workloads, and must overcome hardware imperfections that degrade inference accuracy. Such trade-offs between efficiency, versatility and accuracy cannot be addressed by isolated improvements on any single level of the design. By co-optimizing across all hierarchies of the design from algorithms and architecture to circuits and devices, we present NeuRRAM - the first multimodal edge AI chip using RRAM CIM to simultaneously deliver a high degree of versatility for diverse model architectures, record energy-efficiency $5\\times$ - $8\\times$ better than prior art across various computational bit-precisions, and inference accuracy comparable to software models with 4-bit weights on all measured standard AI benchmarks including accuracy of 99.0% on MNIST and 85.7% on CIFAR-10 image classification, 84.7% accuracy on Google speech command recognition, and a 70% reduction in image reconstruction error on a Bayesian image recovery task. This work paves a way towards building highly efficient and reconfigurable edge AI hardware platforms for the more demanding and heterogeneous AI applications of the future."}}
{"id": "CWw_Ef6k56", "cdate": 1609459200000, "mdate": 1683917369114, "content": {"title": "LSTMs for Keyword Spotting with ReRAM-Based Compute-In-Memory Architectures", "abstract": "The increasingly central role of speech based human computer interaction necessitates on-device, low-latency, low- power, high-accuracy key word spotting (KWS). State-of-the- art accuracies on speech-related tasks have been achieved by long short-term memory (LSTM) neural network (NN) models. Such models are typically computationally intensive because of their heavy use of Matrix vector multiplication (MVM) operations. Compute-in-Memory (CIM) architectures, while well suited to MVM operations, have not seen widespread adoption for LSTMs. In this paper we adapt resistive random access memory based CIM architectures for KWS using LSTMs. We find that a hybrid system composed of CIM cores and digital cores achieves 90% test accuracy on the google speech data set at the cost of 25 uJ/decision. Our optimized architecture uses 5-bit inputs, and analog weights to produce 6-bit outputs. All digital computation are performed with 8-bit precision leading to a 3.7\u00d7 improvement in computational efficiency compared to equivalent digital systems at that accuracy."}}
{"id": "wCx2VU-Ysf9", "cdate": 1577836800000, "mdate": 1683917369253, "content": {"title": "Memory Organization and Structures for On-Chip Learning in Spiking Neural Networks", "abstract": "Spiking Neural Networks (SNNs) offer a promising avenue for developing adaptive intelligence that can operate effectively, when subject to energy constraints. This is especially true for emerging gradient-based SNNs that are designed for continuous learning. However, hardware efficiency and complex SNN functionality are at odds with each other, leading to large design space, with many possible trade-offs. This paper discusses the co-optimization of SNNs and hardware architectures. We compare digital designs and emerging compute-in-memory architectures, in terms of operation. We discuss the effect of different layer-types on memory organization and outline how different memory organization schemes are impacted by the SNN layertype. Finally, we outline some open problems that must be solved to design the next-generation of intelligent hardware systems."}}
