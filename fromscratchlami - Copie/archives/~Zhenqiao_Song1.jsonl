{"id": "022cdH8brR", "cdate": 1686249433720, "mdate": 1686249433720, "content": {"title": "Importance Weighted Expectation-Maximization for Protein Sequence Design", "abstract": "Designing protein sequences with desired biological function is crucial in biology and chemistry. Recent machine learning methods use a surrogate sequence-function model to replace the expensive wet-lab validation. How can we efficiently generate diverse and novel protein sequences with high fitness? In this paper, we propose IsEM-Pro, an approach to generate protein sequences towards a given fitness criterion. At its core, IsEM-Pro is a latent generative model, augmented by combinatorial structure features from a separately learned Markov random fields (MRFs). We develop an Monte Carlo Expectation-Maximization method (MCEM) to learn the model. During inference, sampling from its latent space enhances diversity while its MRFs features guide the exploration in high fitness regions. Experiments on eight protein sequence design tasks show that our IsEM-Pro outperforms the previous best methods by at least 55% on average fitness score and generates more diverse and novel protein sequences."}}
{"id": "wI57D_F7t", "cdate": 1672531200000, "mdate": 1695411282980, "content": {"title": "INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback", "abstract": "The field of automatic evaluation of text generation made tremendous progress in the last few years. In particular, since the advent of neural metrics, like COMET, BLEURT, and SEScore2, the newest generation of metrics show a high correlation with human judgment. Unfortunately, quality scores generated with neural metrics are not interpretable, and it is unclear which part of the generation output is criticized by the metrics. To address this limitation, we present INSTRUCTSCORE, an open-source, explainable evaluation metric for text generation. By harnessing both explicit human instruction and the implicit knowledge of GPT4, we fine-tune a LLAMA model to create an evaluative metric that can produce a diagnostic report aligned with human judgment. We evaluate INSTRUCTSCORE on the WMT22 Zh-En translation task, where our 7B model surpasses other LLM-based baselines, including those based on 175B GPT3. Impressively, our INSTRUCTSCORE, even without direct supervision from human-rated data, achieves performance levels on par with state-of-the-art metrics like COMET22, which was fine-tuned on human ratings."}}
{"id": "QLD9ivPC9xn", "cdate": 1672531200000, "mdate": 1696955440863, "content": {"title": "Importance Weighted Expectation-Maximization for Protein Sequence Design", "abstract": "Designing protein sequences with desired biological function is crucial in biology and chemistry. Recent machine learning methods use a surrogate sequence-function model to replace the expensive we..."}}
{"id": "t0kDJ2h6PL", "cdate": 1640995200000, "mdate": 1681667752348, "content": {"title": "switch-GLAT: Multilingual Parallel Machine Translation Via Code-Switch Decoder", "abstract": "Multilingual machine translation aims to develop a single model for multiple language directions. However, existing multilingual models based on Transformer are limited in terms of both translation..."}}
{"id": "WnZNB3QiTf", "cdate": 1640995200000, "mdate": 1681667752356, "content": {"title": "MTG: A Benchmark Suite for Multilingual Text Generation", "abstract": ""}}
{"id": "5HvpvYd68b", "cdate": 1632875596238, "mdate": null, "content": {"title": "switch-GLAT: Multilingual Parallel Machine Translation Via Code-Switch Decoder", "abstract": "Multilingual machine translation aims to develop a single model for multiple language directions. However, existing multilingual models based on Transformer are limited in terms of both translation performance and inference speed. In this paper, we propose switch-GLAT, a non-autoregressive multilingual machine translation model with a code-switch decoder. It can generate contextual code-switched translations for a given source sentence, and perform code-switch back-translation, greatly boosting multilingual translation performance. In addition, its inference is highly efficient thanks to its parallel decoder. Experiments show that our proposed switch-GLAT outperform the multilingual Transformer with as much as 0.74 BLEU improvement and 6.2x faster decoding speed in inference.\n"}}
{"id": "rz5czuWp-RL", "cdate": 1609459200000, "mdate": 1651067685195, "content": {"title": "MTG: A Benchmarking Suite for Multilingual Text Generation", "abstract": "We introduce MTG, a new benchmark suite for training and evaluating multilingual text generation. It is the first-proposed multilingual multiway text generation dataset with the largest human-annotated data (400k). It includes four generation tasks (story generation, question generation, title generation and text summarization) across five languages (English, German, French, Spanish and Chinese). The multiway setup enables testing knowledge transfer capabilities for a model across languages and tasks. Using MTG, we train and analyze several popular multilingual generation models from different aspects. Our benchmark suite fosters model performance enhancement with more human-annotated parallel data. It provides comprehensive evaluations with diverse generation scenarios. Code and data are available at \\url{https://github.com/zide05/MTG}."}}
{"id": "MgAF9yyHsiW", "cdate": 1609459200000, "mdate": 1651067687386, "content": {"title": "Triangular Bidword Generation for Sponsored Search Auction", "abstract": "Sponsored search auction is a crucial component of modern search engines. It requires a set of candidate bidwords that advertisers can place bids on. Existing methods generate bidwords from search queries or advertisement content. However, they suffer from the data noise in (query, bidword) and (advertisement, bidword) pairs. In this paper, we propose a triangular bidword generation model (TRIDENT), which takes the high-quality data of paired (query, advertisement) as a supervision signal to indirectly guide the bidword generation process. Our proposed model is simple yet effective: by using bidword as the bridge between search query and advertisement, the generation of search query, advertisement and bidword can be jointly learned in the triangular training framework. This alleviates the problem that the training data of bidword may be noisy. Experimental results, including automatic and human evaluations, show that our proposed TRIDENT can generate relevant and diverse bidwords for both search queries and advertisements. Our evaluation on online real data validates the effectiveness of the TRIDENT's generated bidwords for product search."}}
{"id": "KPvo-A3mMT7", "cdate": 1609459200000, "mdate": 1681667752367, "content": {"title": "Generating Personalized Titles Incorporating Advertisement Profile", "abstract": ""}}
{"id": "BJYtqPQOt1S", "cdate": 1609459200000, "mdate": 1651067685594, "content": {"title": "Triangular Bidword Generation for Sponsored Search Auction", "abstract": "Sponsored search auction is a crucial component of modern search engines. It requires a set of candidate bidwords that advertisers can place bids on. Existing methods generate bidwords from search queries or advertisement content. However, they suffer from the data noise in <query, bidword> and <advertisement, bidword> pairs. In this paper, we propose a triangular bidword generation model (TRIDENT), which takes the high-quality data of paired <query, advertisement> as a supervision signal to indirectly guide the bidword generation process. Our proposed model is simple yet effective: by using bidword as the bridge between search query and advertisement, the generation of search query, advertisement and bidword can be jointly learned in the triangular training framework. This alleviates the problem that the training data of bidword may be noisy. Experimental results, including automatic and human evaluations, show that our proposed TRIDENT can generate relevant and diverse bidwords for both search queries and advertisements. Our evaluation on online real data validates the effectiveness of the TRIDENT's generated bidwords for product search."}}
