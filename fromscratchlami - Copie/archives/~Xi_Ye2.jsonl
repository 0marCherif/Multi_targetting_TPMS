{"id": "vU50O03Qcjd", "cdate": 1672531200000, "mdate": 1684261266067, "content": {"title": "Assessing Out-of-Domain Language Model Performance from Few Examples", "abstract": ""}}
{"id": "B0VZZOVoDf", "cdate": 1672531200000, "mdate": 1681502594286, "content": {"title": "Explanation Selection Using Unlabeled Data for In-Context Learning", "abstract": ""}}
{"id": "Bct2f8fRd8S", "cdate": 1652737426309, "mdate": null, "content": {"title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning", "abstract": "Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning? We study this question on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks, we find that including explanations in the prompts for OPT, GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However, text-davinci-002 is able to benefit more substantially.\n\nWe further show that explanations generated by the LLMs may not entail the models\u2019 predictions nor be factually grounded in the input, even on simple tasks with extractive explanations. However, these flawed explanations can still be useful as a way to verify LLMs\u2019 predictions post-hoc. Through analysis in our three settings, we show that explanations judged by humans to be good\u2014logically consistent with the input and the prediction\u2014more likely cooccur with accurate predictions. Following these observations, we train calibrators using automatically extracted scores that assess the reliability of explanations, allowing us to improve performance post-hoc across all of our datasets."}}
{"id": "s_tbKY4cXL", "cdate": 1640995200000, "mdate": 1684261266143, "content": {"title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning", "abstract": "Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning? We study this question on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks, we find that including explanations in the prompts for OPT, GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However, text-davinci-002 is able to benefit more substantially.We further show that explanations generated by the LLMs may not entail the models\u2019 predictions nor be factually grounded in the input, even on simple tasks with extractive explanations. However, these flawed explanations can still be useful as a way to verify LLMs\u2019 predictions post-hoc. Through analysis in our three settings, we show that explanations judged by humans to be good\u2014logically consistent with the input and the prediction\u2014more likely cooccur with accurate predictions. Following these observations, we train calibrators using automatically extracted scores that assess the reliability of explanations, allowing us to improve performance post-hoc across all of our datasets."}}
{"id": "jdAa_amy3EL", "cdate": 1640995200000, "mdate": 1675174135879, "content": {"title": "Complementary Explanations for Effective In-Context Learning", "abstract": "Large language models (LLMs) have exhibited remarkable capabilities in learning from explanations in prompts, but there has been limited understanding of exactly how these explanations function or why they are effective. This work aims to better understand the mechanisms by which explanations are used for in-context learning. We first study the impact of two different factors on the performance of prompts with explanations: the computation trace (the way the solution is decomposed) and the natural language used to express the prompt. By perturbing explanations on three controlled tasks, we show that both factors contribute to the effectiveness of explanations. We further study how to form maximally effective sets of explanations for solving a given test query. We find that LLMs can benefit from the complementarity of the explanation set: diverse reasoning skills shown by different exemplars can lead to better performance. Therefore, we propose a maximal marginal relevance-based exemplar selection approach for constructing exemplar sets that are both relevant as well as complementary, which successfully improves the in-context learning performance across three real-world tasks on multiple LLMs."}}
{"id": "gqtSN5Ztovi", "cdate": 1640995200000, "mdate": 1675174135914, "content": {"title": "RNG-KBQA: Generation Augmented Iterative Ranking for Knowledge Base Question Answering", "abstract": ""}}
{"id": "VkHFixN7zR", "cdate": 1640995200000, "mdate": 1675174135982, "content": {"title": "Can Explanations Be Useful for Calibrating Black Box Models?", "abstract": ""}}
{"id": "QLrYGYVkP9", "cdate": 1640995200000, "mdate": 1683879675973, "content": {"title": "Diagnosing Ensemble Few-Shot Classifiers", "abstract": "The base learners and labeled samples (shots) in an ensemble few-shot classifier greatly affect the model performance. When the performance is not satisfactory, it is usually difficult to understand the underlying causes and make improvements. To tackle this issue, we propose a visual analysis method, FSLDiagnotor. Given a set of base learners and a collection of samples with a few shots, we consider two problems: 1) finding a subset of base learners that well predict the sample collections; and 2) replacing the low-quality shots with more representative ones to adequately represent the sample collections. We formulate both problems as sparse subset selection and develop two selection algorithms to recommend appropriate learners and shots, respectively. A matrix visualization and a scatterplot are combined to explain the recommended learners and shots in context and facilitate users in adjusting them. Based on the adjustment, the algorithm updates the recommendation results for another round of improvement. Two case studies are conducted to demonstrate that FSLDiagnotor helps build a few-shot classifier efficiently and increases the accuracy by 12% and 21%, respectively."}}
{"id": "KNSA0ZcAB2", "cdate": 1640995200000, "mdate": 1683879676009, "content": {"title": "Diagnosing Ensemble Few-Shot Classifiers", "abstract": "The base learners and labeled samples (shots) in an ensemble few-shot classifier greatly affect the model performance. When the performance is not satisfactory, it is usually difficult to understand the underlying causes and make improvements. To tackle this issue, we propose a visual analysis method, FSLDiagnotor. Given a set of base learners and a collection of samples with a few shots, we consider two problems: 1) finding a subset of base learners that well predict the sample collections; and 2) replacing the low-quality shots with more representative ones to adequately represent the sample collections. We formulate both problems as sparse subset selection and develop two selection algorithms to recommend appropriate learners and shots, respectively. A matrix visualization and a scatterplot are combined to explain the recommended learners and shots in context and facilitate users in adjusting them. Based on the adjustment, the algorithm updates the recommendation results for another round of improvement. Two case studies are conducted to demonstrate that FSLDiagnotor helps build a few-shot classifier efficiently and increases the accuracy by 12% and 21%, respectively."}}
{"id": "99YFZ5Fwlq4", "cdate": 1640995200000, "mdate": 1664911710212, "content": {"title": "The Unreliability of Explanations in Few-Shot In-Context Learning", "abstract": "How can prompting a large language model like GPT-3 with explanations improve in-context learning? We focus specifically on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. Including explanations in the prompt and having the model generate them does not consistently improve performance in the settings we study, contrary to recent results on symbolic reasoning tasks (Nye et al., 2021; Wei et al., 2022). Despite careful prompting, explanations generated by GPT-3 may not even be factually grounded in the input, even on simple tasks with straightforward extractive explanations. However, these flawed explanations can still be useful as a way to verify GPT-3's predictions post-hoc. Through analysis in three settings, we show that explanations judged as good by humans--those that are logically consistent with the input and the prediction--usually indicate more accurate predictions. Following these observations, we present a framework for calibrating model predictions based on the reliability of the explanations. Our framework trains calibrators using automatically extracted scores that approximately assess the reliability of explanations, which helps improve performance across three different datasets."}}
