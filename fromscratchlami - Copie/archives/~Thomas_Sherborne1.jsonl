{"id": "s0ASAbK9_i", "cdate": 1672531200000, "mdate": 1690550168628, "content": {"title": "Extrinsic Evaluation of Machine Translation Metrics", "abstract": ""}}
{"id": "r1BYfNXKo-", "cdate": 1672531200000, "mdate": 1690550168632, "content": {"title": "Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation", "abstract": "Rising computational demands of modern natural language processing (NLP) systems have increased the barrier to entry for cutting-edge research while posing serious environmental concerns. Yet, progress on model efficiency has been impeded by practical challenges in model evaluation and comparison. For example, hardware is challenging to control due to disparate levels of accessibility across different institutions. Moreover, improvements in metrics such as FLOPs often fail to translate to progress in real-world applications. In response, we introduce Pentathlon, a benchmark for holistic and realistic evaluation of model efficiency. Pentathlon focuses on inference, which accounts for a majority of the compute in a model's lifecycle. It offers a strictly-controlled hardware platform, and is designed to mirror real-world applications scenarios. It incorporates a suite of metrics that target different aspects of efficiency, including latency, throughput, memory overhead, and energy consumption. Pentathlon also comes with a software library that can be seamlessly integrated into any codebase and enable evaluation. As a standardized and centralized evaluation platform, Pentathlon can drastically reduce the workload to make fair and reproducible efficiency comparisons. While initially focused on natural language processing (NLP) models, Pentathlon is designed to allow flexible extension to other fields. We envision Pentathlon will stimulate algorithmic innovations in building efficient models, and foster an increased awareness of the social and environmental implications in the development of future-generation NLP models."}}
{"id": "Z_bXjO3wED", "cdate": 1672531200000, "mdate": 1681921461633, "content": {"title": "Meta-Learning a Cross-lingual Manifold for Semantic Parsing", "abstract": "Localizing a semantic parser to support new languages requires effective cross-lingual generalization.\u00a0 Recent work has found success with machine-translation or zero-shot methods although these approaches can struggle to model how native speakers ask questions. We consider how to effectively leverage minimal annotated examples in new languages for few-shot cross-lingual semantic parsing. We introduce a first-order meta-learning algorithm to train a semantic parser with maximal sample efficiency during cross-lingual transfer. Our algorithm uses high-resource languages to train the parser and simultaneously optimizes for cross-lingual generalization for lower-resource languages. Results across six languages on ATIS demonstrate that our combination of generalization steps yields accurate semantic parsers sampling \u226410% of source training data in each new language. Our approach also trains a competitive model on Spider using English with generalization to Chinese similarly sampling \u226410% of training data."}}
{"id": "Z93bVHe42R0", "cdate": 1672531200000, "mdate": 1690550168632, "content": {"title": "Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing", "abstract": "Cross-lingual semantic parsing transfers parsing capability from a high-resource language (e.g., English) to low-resource languages with scarce training data. Previous work has primarily considered silver-standard data augmentation or zero-shot methods, however, exploiting few-shot gold data is comparatively unexplored. We propose a new approach to cross-lingual semantic parsing by explicitly minimizing cross-lingual divergence between probabilistic latent variables using Optimal Transport. We demonstrate how this direct guidance improves parsing from natural languages using fewer examples and less training. We evaluate our method on two datasets, MTOP and MultiATIS++SQL, establishing state-of-the-art results under a few-shot cross-lingual regime. Ablation studies further reveal that our method improves performance even without parallel input translations. In addition, we show that our model better captures cross-lingual structure in the latent space to improve semantic representation similarity."}}
{"id": "pvitBwn8Mz", "cdate": 1640995200000, "mdate": 1673169222050, "content": {"title": "Extrinsic Evaluation of Machine Translation Metrics", "abstract": ""}}
{"id": "oYQQtJ7tZd", "cdate": 1640995200000, "mdate": 1665823270721, "content": {"title": "Zero-Shot Cross-lingual Semantic Parsing", "abstract": ""}}
{"id": "8V5rLRnt1u", "cdate": 1640995200000, "mdate": 1665823270737, "content": {"title": "Meta-Learning a Cross-lingual Manifold for Semantic Parsing", "abstract": "Localizing a semantic parser to support new languages requires effective cross-lingual generalization. Recent work has found success with machine-translation or zero-shot methods although these approaches can struggle to model how native speakers ask questions. We consider how to effectively leverage minimal annotated examples in new languages for few-shot cross-lingual semantic parsing. We introduce a first-order meta-learning algorithm to train a semantic parser with maximal sample efficiency during cross-lingual transfer. Our algorithm uses high-resource languages to train the parser and simultaneously optimizes for cross-lingual generalization for lower-resource languages. Results across six languages on ATIS demonstrate that our combination of generalization steps yields accurate semantic parsers sampling $\\le$10% of source training data in each new language. Our approach also trains a competitive model on Spider using English with generalization to Chinese similarly sampling $\\le$10% of training data."}}
{"id": "TBaGA3qp6tF", "cdate": 1609459200000, "mdate": 1665823270745, "content": {"title": "Zero-Shot Cross-lingual Semantic Parsing", "abstract": "Recent work in cross-lingual semantic parsing has successfully applied machine translation to localize parsers to new languages. However, these advances assume access to high-quality machine translation systems and word alignment tools. We remove these assumptions and study cross-lingual semantic parsing as a zero-shot problem, without parallel data (i.e., utterance-logical form pairs) for new languages. We propose a multi-task encoder-decoder model to transfer parsing knowledge to additional languages using only English-logical form paired data and in-domain natural language corpora in each new language. Our model encourages language-agnostic encodings by jointly optimizing for logical-form generation with auxiliary objectives designed for cross-lingual latent representation alignment. Our parser performs significantly above translation-based baselines and, in some cases, competes with the supervised upper-bound."}}
{"id": "UK8dZs0PVpt", "cdate": 1577836800000, "mdate": 1665823270840, "content": {"title": "Bootstrapping a Crosslingual Semantic Parser", "abstract": ""}}
{"id": "OGZ8V7UXRWn", "cdate": 1577836800000, "mdate": 1631218400287, "content": {"title": "Bootstrapping a Crosslingual Semantic Parser", "abstract": "Recent progress in semantic parsing scarcely considers languages other than English but professional translation can be prohibitively expensive. We adapt a semantic parser trained on a single language, such as English, to new languages and multiple domains with minimal annotation. We query if machine translation is an adequate substitute for training data, and extend this to investigate bootstrapping using joint training with English, paraphrasing, and multilingual pre-trained models. We develop a Transformer-based parser combining paraphrases by ensembling attention over multiple encoders and present new versions of ATIS and Overnight in German and Chinese for evaluation. Experimental results indicate that MT can approximate training data in a new language for accurate parsing when augmented with paraphrasing through multiple MT engines. Considering when MT is inadequate, we also find that using our approach achieves parsing accuracy within 2% of complete translation using only 50% of training data."}}
