{"id": "hvcWwAqdGs", "cdate": 1676827093287, "mdate": null, "content": {"title": "Gaussian Process Surrogate Models for Neural Networks", "abstract": "Not being able to understand and predict the behavior of deep learning systems makes it hard to decide what architecture and algorithm to use for a given problem. In science and engineering, modeling is a methodology used to understand complex systems whose internal processes are opaque. Modeling replaces a complex system with a simpler, more interpretable surrogate. Drawing inspiration from this, we construct a class of surrogate models for neural networks using Gaussian processes. Rather than deriving kernels for infinite neural networks, we learn kernels empirically from the naturalistic behavior of finite neural networks. We demonstrate our approach captures existing phenomena related to the spectral bias of neural networks, and then show that our surrogate models can be used to solve practical problems such as identifying which points most influence the behavior of specific neural networks and predicting which architectures and algorithms will generalize well for specific datasets."}}
{"id": "qUj5PXa_qMq", "cdate": 1672531200000, "mdate": 1705340540295, "content": {"title": "Bayes in the age of intelligent machines", "abstract": "The success of methods based on artificial neural networks in creating intelligent machines seems like it might pose a challenge to explanations of human cognition in terms of Bayesian inference. We argue that this is not the case, and that in fact these systems offer new opportunities for Bayesian modeling. Specifically, we argue that Bayesian models of cognition and artificial neural networks lie at different levels of analysis and are complementary modeling approaches, together offering a way to understand human cognition that spans these levels. We also argue that the same perspective can be applied to intelligent machines, where a Bayesian approach may be uniquely valuable in understanding the behavior of large, opaque artificial neural networks that are trained on proprietary data."}}
{"id": "M2EXB2DHgE", "cdate": 1672531200000, "mdate": 1705340540300, "content": {"title": "The Transient Nature of Emergent In-Context Learning in Transformers", "abstract": "Transformer neural networks can exhibit a surprising capacity for in-context learning (ICL) despite not being explicitly trained for it. Prior work has provided a deeper understanding of how ICL emerges in transformers, e.g. through the lens of mechanistic interpretability, Bayesian inference, or by examining the distributional properties of training data. However, in each of these cases, ICL is treated largely as a persistent phenomenon; namely, once ICL emerges, it is assumed to persist asymptotically. Here, we show that the emergence of ICL during transformer training is, in fact, often transient. We train transformers on synthetic data designed so that both ICL and in-weights learning (IWL) strategies can lead to correct predictions. We find that ICL first emerges, then disappears and gives way to IWL, all while the training loss decreases, indicating an asymptotic preference for IWL. The transient nature of ICL is observed in transformers across a range of model sizes and datasets, raising the question of how much to \"overtrain\" transformers when seeking compact, cheaper-to-run models. We find that L2 regularization may offer a path to more persistent ICL that removes the need for early stopping based on ICL-style validation tasks. Finally, we present initial evidence that ICL transience may be caused by competition between ICL and IWL circuits."}}
{"id": "1VuI-k-FwY", "cdate": 1672531200000, "mdate": 1699202054151, "content": {"title": "Getting aligned on representational alignment", "abstract": "Biological and artificial information processing systems form representations that they can use to categorize, reason, plan, navigate, and make decisions. How can we measure the extent to which the representations formed by these diverse systems agree? Do similarities in representations then translate into similar behavior? How can a system's representations be modified to better match those of another system? These questions pertaining to the study of representational alignment are at the heart of some of the most active research areas in cognitive science, neuroscience, and machine learning. For example, cognitive scientists measure the representational alignment of multiple individuals to identify shared cognitive priors, neuroscientists align fMRI responses from multiple individuals into a shared representational space for group-level analyses, and ML researchers distill knowledge from teacher models into student models by increasing their alignment. Unfortunately, there is limited knowledge transfer between research communities interested in representational alignment, so progress in one field often ends up being rediscovered independently in another. Thus, greater cross-field communication would be advantageous. To improve communication between these fields, we propose a unifying framework that can serve as a common language between researchers studying representational alignment. We survey the literature from all three fields and demonstrate how prior work fits into this framework. Finally, we lay out open problems in representational alignment where progress can benefit all three of these fields. We hope that our work can catalyze cross-disciplinary collaboration and accelerate progress for all communities studying and developing information processing systems. We note that this is a working paper and encourage readers to reach out with their suggestions for future revisions."}}
{"id": "_Qaz9ZZSIHc", "cdate": 1653100932267, "mdate": null, "content": {"title": "Predicting generalization with degrees of freedom in neural networks", "abstract": "Model complexity is fundamentally tied to predictive power in the sciences as well as in applications. However, there is a divergence between naive measures of complexity such as parameter count and the generalization performance of over-parameterized machine learning models. Prior empirical approaches to capturing intrinsic complexity in a more sophisticated manner than parameter count are computationally intractable, do not capture the implicitly regularizing effects of the entire machine-learning pipeline, or do not provide a quantitative fit to the double descent behavior of overparameterized models. In this work, we introduce an empirical complexity measure inspired by the classical notion of generalized degrees of freedom in statistics. This measure can be approximated efficiently and is a function of the entire machine learning training pipeline. We demonstrate that this measure correlates with generalization performance in the double-descent regime."}}
{"id": "qPmNg7LY4Z", "cdate": 1640995200000, "mdate": 1705340540291, "content": {"title": "Cognitive analyses of machine learning systems", "abstract": "Author(s): Grant, Erin Marie | Advisor(s): Jordan, Michael I | Abstract: Machine learning systems are increasingly a part of human lives, and so it is increasingly important to understand the similarities and differences between human intelligence and machine intelligence. However, as machine learning systems are applied to more complex problem settings, understanding them becomes more challenging, and their performance, correctness, and reliability become increasingly difficult to guarantee. Moreover, \"human-level performance\" in such settings is often itself not well-defined, as many of the cognitive mechanisms underlying human behavior remain opaque. This dissertation bridges gaps in our understanding of human and machine intelligence using cross-disciplinary insights from cognitive science and machine learning.First, I develop two frameworks that borrow methodologically from cognitive science to identify deviations in the expected behavior of machine learning systems. Second, I forge a connection between a classical approach to building computational models of human cognition, hierarchical modeling, and a recent technique for small-sample learning in machine learning, meta-learning. I use this connection to develop algorithmic improvements to machine learning systems on established benchmarks and in new settings that highlight their inability to come close to human standards. Finally, I argue that machine learning should borrow methodologically from cognitive science, as both are now tasked with studying opaque learning and decision-making systems. I use this perspective to construct a computational model of machine learning systems that allows us to formalize and test hypotheses about how these systems operate."}}
{"id": "nqtL9tMyrr", "cdate": 1640995200000, "mdate": 1682996381750, "content": {"title": "Distinguishing rule and exemplar-based generalization in learning systems", "abstract": "Machine learning systems often do not share the same inductive biases as humans and, as a result, extrapolate or generalize in ways that are inconsistent with our expectations. The trade-off betwee..."}}
{"id": "aPSShg9GduT", "cdate": 1640995200000, "mdate": 1682344423091, "content": {"title": "The Emergence of Gender Associations in Child Language Development", "abstract": "Gender associations have been a long-standing research topic in psychological and social sciences. Although it is known that children learn aspects of gender associations at a young age, it is not we..."}}
{"id": "VQRWeFKJZEi", "cdate": 1640995200000, "mdate": 1683913807220, "content": {"title": "Gaussian process surrogate models for neural networks", "abstract": "The lack of insight into deep learning systems hinders their systematic design. In science and engineering, modeling is a methodology used to understand complex systems whose internal processes are opaque. Modeling replaces a complex system with a simpler surrogate that is more amenable to interpretation. Drawing inspiration from this, we construct a class of surrogate models for neural networks using Gaussian processes. Rather than deriving the kernels for certain limiting cases of neural networks, we learn the kernels of the Gaussian process empirically from the naturalistic behavior of neural networks. We first evaluate our approach with two case studies inspired by previous theoretical studies of neural network behavior in which we capture neural network preferences for learning low frequencies and identify pathological behavior in deep neural networks. In two further practical case studies, we use the learned kernel to predict the generalization properties of neural networks."}}
{"id": "YPeusrnp28q", "cdate": 1633015336348, "mdate": null, "content": {"title": "Meta-learning inductive biases of learning systems with Gaussian processes", "abstract": "Many advances in machine learning can be attributed to designing systems with inductive biases well-suited for particular tasks.  However, it can be challenging to ascertain the inductive biases of a learning system, much less control them in the design process. We propose a framework to capture the inductive biases in a learning system by meta-learning Gaussian process kernel hyperparameters from its predictions. We illustrate the potential of this framework across several case studies, including investigating the inductive biases of both untrained and trained neural networks, and assessing whether a given neural network family is well-suited for a task family."}}
