{"id": "PBQAoJawDw", "cdate": 1672531200000, "mdate": 1680985332033, "content": {"title": "Very fast, approximate counterfactual explanations for decision forests", "abstract": ""}}
{"id": "qhCRKV4L_Kr", "cdate": 1640995200000, "mdate": 1680985332037, "content": {"title": "Sparse Oblique Decision Trees: A Tool to Interpret Natural Language Processing Datasets", "abstract": ""}}
{"id": "dKkK8SePtY", "cdate": 1640995200000, "mdate": 1680985332035, "content": {"title": "Interpretable Image Classification Using Sparse Oblique Decision Trees", "abstract": ""}}
{"id": "PvbkA-dUU5i", "cdate": 1640995200000, "mdate": 1683613242948, "content": {"title": "Approaches to Interpret Deep Neural Networks", "abstract": "Author(s): Hada, Suryabhan Singh | Advisor(s): Carreira-Perpinan, Miguel A. | Abstract: Practical deployment of deep neural networks has become widespread in the last decade due to their ability to provide simple, intelligent, and automated processing of the tasks that up to now were hard for other machine learning models. There is an enormous financial and societal interest in deep neural networks as a viable solution for many practical problems such as computer vision, language processing, financial fraud detection, and many more. At the same time, some concerns regarding the safety and ethical use of these models have arisen as well. One of the main concerns is interpretability, i.e., explaining how the model makes a decision for an input. Interpretability is one of the most important problems to address for building trust and accountability as the adoption of deep neural networks has increased significantly in sensitive areas like medicine, security, and finance.This dissertation proposes two novel approaches to interpreting deep neural networks. The first approach focuses on understanding what information is retained by the neurons of a deep net. We propose an approach to characterize the region of input space that excites a given neuron to a certain level. Inspection of these regions by a human can reveal regularities that help to understand the neuron. In the second approach, we provide a systematic way to understand what group of neurons in a deep net are responsible for a particular class. This allows us to study the relation between deep net features (neuron\u2019s activation) and output classes; and how different classes are distributed in the latent space. We also show that out of thousands of neurons in the deep net, only a small subset of neurons is associated with a specific class. Finally, we demonstrate that the latter approach can also be used to interpret large datasets. This is achieved by applying the second approach directly over the input features. This allows us to understand what input features are related to a specific class and what set of features differentiates between a group of classes or even sub-groups within a given class."}}
{"id": "pzwBwjqvwKB", "cdate": 1609459200000, "mdate": 1680985332078, "content": {"title": "Counterfactual Explanations for Oblique Decision Trees: Exact, Efficient Algorithms", "abstract": ""}}
{"id": "k3gjiqKx-m", "cdate": 1609459200000, "mdate": 1680985332038, "content": {"title": "Counterfactual Explanations for Oblique Decision Trees: Exact, Efficient Algorithms", "abstract": ""}}
{"id": "amK2ryhax1", "cdate": 1609459200000, "mdate": 1669142146025, "content": {"title": "Non-Greedy Algorithms for Decision Tree Optimization: An Experimental Comparison", "abstract": "Learning decision trees is a difficult optimization problem: nonconvex, nondifferentiable and over a huge number of tree structures. The dominant paradigm in practice, established in the 1980s, are axis-aligned trees trained with a greedy recursive partitioning algorithm such as CART or C5.0. Several non-greedy optimization algorithms have been proposed recently, which optimize all the nodes' parameters jointly, and we compare experimentally some of them in a range of classification and regression datasets, in terms of accuracy, training time and tree size. The non-greedy algorithms do not improve over CART significantly with one exception, tree alternating optimization (TAO). TAO scales to large datasets and produces axis-aligned and especially oblique trees that consistently outperform all other algorithms, often by a large margin. TAO makes oblique trees preferable to axis-aligned ones in many cases, since they are much more accurate while remaining small and interpretable. This suggests a change in paradigm in practical applications of decision trees."}}
{"id": "IL79Z_rWFMf", "cdate": 1609459200000, "mdate": 1680985332140, "content": {"title": "Sampling The \"Inverse Set\" of a Neuron", "abstract": ""}}
{"id": "FzNOHmaeU2", "cdate": 1609459200000, "mdate": 1680985332064, "content": {"title": "Understanding And Manipulating Neural Net Features Using Sparse Oblique Classification Trees", "abstract": ""}}
{"id": "AQsPaGX14K", "cdate": 1609459200000, "mdate": 1680985332048, "content": {"title": "Exploring Counterfactual Explanations for Classification and Regression Trees", "abstract": ""}}
