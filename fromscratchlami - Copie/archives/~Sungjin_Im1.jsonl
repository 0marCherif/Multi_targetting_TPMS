{"id": "WifsRTGono", "cdate": 1673540025120, "mdate": 1673540025120, "content": {"title": "Min-max Submodular Ranking for Multiple Agents", "abstract": "In the submodular ranking (SR) problem, the input consists of a set of submodular functions defined on a ground set of elements. The goal is to order elements for all the functions to have value above a certain threshold as soon on average as possible, assuming we choose one element per time. The problem is flexible enough to capture various applications in machine learning, including decision trees.\nThis paper considers the min-max version of SR where multiple instances share the ground set. With the view of each instance being associated with an agent, the min-max problem is to order the common elements to minimize the maximum objective of all agents -- thus, finding a fair solution for all agents. We give approximation algorithms for this problem and demonstrate their effectiveness in the application of finding a decision tree for multiple agents."}}
{"id": "wwWCZ7sER_C", "cdate": 1652737814409, "mdate": null, "content": {"title": "Algorithms with Prediction Portfolios", "abstract": "The research area of algorithms with predictions has seen recent success showing how to incorporate machine learning into algorithm design to improve performance when the predictions are correct, while retaining worst-case guarantees when they are not.  Most previous work has assumed that the algorithm has access to a single predictor. However, in practice, there are many machine learning methods available, often with incomparable generalization guarantees, making it hard to pick a best method a priori. In this work we consider scenarios where multiple predictors are available to the algorithm and the question is how to best utilize them. \n\nIdeally, we would like the algorithm's performance to depend on the quality of the {\\em best} predictor.  However, utilizing more predictions comes with a cost, since we now have to identify which prediction is best.  We study the use of multiple predictors for a number of fundamental problems, including matching, load balancing, and non-clairvoyant scheduling, which have been well-studied in the single predictor setting. For each of these problems we introduce new algorithms that take advantage of multiple predictors, and prove bounds on the resulting performance. "}}
{"id": "kB8eks2Edt8", "cdate": 1621630240292, "mdate": null, "content": {"title": "Faster Matchings via Learned Duals", "abstract": "A recent line of research investigates how algorithms can be augmented with machine-learned predictions to overcome worst case lower bounds.  This area has revealed interesting algorithmic insights into problems, with particular success in the design of competitive online algorithms.  However, the question of improving algorithm running times with predictions has largely been unexplored.\n  \nWe take a first step in this direction by combining the idea of machine-learned predictions with the idea of ``warm-starting\" primal-dual algorithms. We consider one of the most important primitives in combinatorial optimization: weighted bipartite matching and its generalization to $b$-matching. We identify three key challenges when using learned dual variables in a primal-dual algorithm.  First, predicted duals may be infeasible, so we give an algorithm that efficiently maps predicted infeasible duals to nearby feasible solutions.  Second, once the duals are feasible, they may not be optimal, so we show that they can be used to quickly find an optimal solution. Finally, such predictions are useful only if they can be learned, so we show that the problem of learning duals for matching has low sample complexity.  We validate our theoretical findings through experiments on both real and synthetic data.  As a result we give a rigorous, practical, and empirically effective method to compute bipartite matchings."}}
{"id": "rMm9d_aDtOa", "cdate": 1621630105827, "mdate": null, "content": {"title": "Online Knapsack with Frequency Predictions", "abstract": "There has been recent interest in using machine-learned predictions to improve the worst-case guarantees of online algorithms.  In this paper we continue this line of work by studying the online knapsack problem, but with very weak predictions: in the form of knowing an upper and lower bound for the number of items of each value.  We systematically derive online algorithms that attain the best possible competitive ratio for any fixed prediction; we also extend the results to more general settings such as generalized one-way trading and two-stage online knapsack. Our work shows that even seemingly weak predictions can be utilized effectively to provably improve the performance of online algorithms."}}
{"id": "1XxaUEa3Yz", "cdate": 1621630105827, "mdate": null, "content": {"title": "Online Knapsack with Frequency Predictions", "abstract": "There has been recent interest in using machine-learned predictions to improve the worst-case guarantees of online algorithms.  In this paper we continue this line of work by studying the online knapsack problem, but with very weak predictions: in the form of knowing an upper and lower bound for the number of items of each value.  We systematically derive online algorithms that attain the best possible competitive ratio for any fixed prediction; we also extend the results to more general settings such as generalized one-way trading and two-stage online knapsack. Our work shows that even seemingly weak predictions can be utilized effectively to provably improve the performance of online algorithms."}}
