{"id": "1tfhQMOSmx", "cdate": 1667338229034, "mdate": 1667338229034, "content": {"title": "Unsupervised Joint 3D Object Model Learning and 6D Pose Estimation for Depth-Based Instance Segmentation", "abstract": "In this work, we propose a novel unsupervised approach\nto jointly learn the 3D object model and estimate the 6D\nposes of multiple instances of a previously unknown object, with applications to depth-based instance segmentation.\nThe inputs are depth images, and the learned object model\nis represented by a 3D point cloud. Traditional 6D pose\nestimation approaches are not sufficient to address this unsupervised problem, in which neither a CAD model of the\nobject nor the ground-truth 6D poses of its instances are\navailable during training. To solve this problem, we propose\nto jointly optimize the model learning and pose estimation\nin an end-to-end deep learning framework. Specifically, our\nnetwork produces a 3D object model and a list of rigid transformations of this model to generate instances, which when\nrendered must match the observed 3D point cloud to minimize the Chamfer distance. To render the set of instance\npoint clouds with occlusions, the network automatically removes the occluded points in a given camera view. Extensive\nexperiments evaluate our technique on several object models and varying numbers of instances. We demonstrate the\napplication of our method to instance segmentation of depth\nimages of small bins of industrial parts. Compared with\npopular baselines for instance segmentation, our model not\nonly demonstrates competitive performance, but also learns\na 3D object model that is represented as a 3D point cloud"}}
{"id": "JRXgTMqESS", "cdate": 1652737820907, "mdate": null, "content": {"title": "Learning Audio-Visual Dynamics Using Scene Graphs for Audio Source Separation", "abstract": "There exists an unequivocal distinction between the sound produced by a static source and that produced by a moving one, especially when the source moves towards or away from the microphone. In this paper, we propose to use this connection between audio and visual dynamics for solving two challenging tasks simultaneously, namely: (i) separating audio sources from a mixture using visual cues, and (ii) predicting the 3D visual motion of a sounding source using its separated audio. Towards this end, we present Audio Separator and Motion Predictor (ASMP) -- a deep learning framework that leverages the 3D structure of the scene and the motion of sound sources for better audio source separation. At the heart of ASMP is a 2.5D scene graph capturing various objects in the video and their pseudo-3D spatial proximities. This graph is constructed by registering together 2.5D monocular depth predictions from the 2D video frames and associating the 2.5D scene regions with the outputs of an object detector applied on those frames. The ASMP task is then mathematically modeled as the joint problem of: (i) recursively segmenting the 2.5D scene graph into several sub-graphs, each associated with a constituent sound in the input audio mixture (which is then separated) and (ii) predicting the 3D motions of the corresponding sound sources from the separated audio. To empirically evaluate ASMP, we present experiments on two challenging audio-visual datasets, viz. Audio Separation in the Wild (ASIW) and Audio Visual Event (AVE). Our results demonstrate that ASMP achieves a clear improvement in source separation quality, outperforming prior works on both datasets, while also estimating the direction of motion of the sound sources better than other methods."}}
{"id": "xpdaDM_B4D", "cdate": 1652737699755, "mdate": null, "content": {"title": "FeLMi : Few shot Learning with hard Mixup", "abstract": "Learning from a few examples is a challenging computer vision task. Traditionally,\nmeta-learning-based methods have shown promise towards solving this problem.\nRecent approaches show benefits by learning a feature extractor on the abundant\nbase examples and transferring these to the fewer novel examples. However, the\nfinetuning stage is often prone to overfitting due to the small size of the novel\ndataset. To this end, we propose Few shot Learning with hard Mixup (FeLMi)\nusing manifold mixup to synthetically generate samples that helps in mitigating\nthe data scarcity issue. Different from a na\u00efve mixup, our approach selects the hard\nmixup samples using an uncertainty-based criteria. To the best of our knowledge,\nwe are the first to use hard-mixup for the few-shot learning problem. Our approach\nallows better use of the pseudo-labeled base examples through base-novel mixup\nand entropy-based filtering. We evaluate our approach on several common few-shot\nbenchmarks - FC-100, CIFAR-FS, miniImageNet and tieredImageNet and obtain\nimprovements in both 1-shot and 5-shot settings. Additionally, we experimented on\nthe cross-domain few-shot setting (miniImageNet \u2192 CUB) and obtain significant\nimprovements."}}
{"id": "1Re5RKwpieG", "cdate": 1652737574758, "mdate": null, "content": {"title": "AVLEN: Audio-Visual-Language Embodied Navigation in 3D Environments", "abstract": "Recent years have seen embodied visual navigation advance in two distinct directions: (i) in equipping the AI agent to follow natural language instructions, and (ii) in making the navigable world multimodal, e.g., audio-visual navigation. However, the real world is not only multimodal, but also often complex, and thus in spite of these advances, agents still need to understand the uncertainty in their actions and seek instructions to navigate. To this end, we present AVLEN -- an interactive agent for Audio-Visual-Language Embodied Navigation. Similar to audio-visual navigation tasks, the goal of our embodied agent is to localize an audio event via navigating the 3D visual world; however, the agent may also seek help from a human (oracle), where the assistance is provided in free-form natural language. To realize these abilities, AVLEN uses a multimodal hierarchical reinforcement learning backbone that learns: (a) high-level policies to choose either audio-cues for navigation or to query the oracle, and (b) lower-level policies to select navigation actions based on its audio-visual and language inputs. The policies are trained via rewarding for the success on the navigation task while minimizing the number of queries to the oracle. To empirically evaluate AVLEN, we present experiments on the SoundSpaces framework for semantic audio-visual navigation tasks. Our results show that equipping the agent to ask for help leads to a clear improvement in performances, especially in challenging cases, e.g., when the sound is unheard during training or in the presence of distractor sounds."}}
{"id": "slbanyT31mz", "cdate": 1640995200000, "mdate": 1668029526146, "content": {"title": "Learning Log-Determinant Divergences for Positive Definite Matrices", "abstract": "Representations in the form of Symmetric Positive Definite (SPD) matrices have been popularized in a variety of visual learning applications due to their demonstrated ability to capture rich second-order statistics of visual data. There exist several similarity measures for comparing SPD matrices with documented benefits. However, selecting an appropriate measure for a given problem remains a challenge and in most cases, is the result of a trial-and-error process. In this paper, we propose to learn similarity measures in a data-driven manner. To this end, we capitalize on the <inline-formula><tex-math notation=\"LaTeX\">$\\alpha \\beta$</tex-math></inline-formula> -log-det divergence, which is a meta-divergence parametrized by scalars <inline-formula><tex-math notation=\"LaTeX\">$\\alpha$</tex-math></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">$\\beta$</tex-math></inline-formula> , subsuming a wide family of popular information divergences on SPD matrices for distinct and discrete values of these parameters. Our key idea is to cast these parameters in a continuum and learn them from data. We systematically extend this idea to learn vector-valued parameters, thereby increasing the expressiveness of the underlying non-linear measure. We conjoin the divergence learning problem with several standard tasks in machine learning, including supervised discriminative dictionary learning and unsupervised SPD matrix clustering. We present Riemannian gradient descent schemes for optimizing our formulations efficiently, and show the usefulness of our method on eight standard computer vision tasks."}}
{"id": "YQkQM2SXlP", "cdate": 1640995200000, "mdate": 1680460220804, "content": {"title": "Learning Audio-Visual Dynamics Using Scene Graphs for Audio Source Separation", "abstract": ""}}
{"id": "VtjvQAb463", "cdate": 1640995200000, "mdate": 1680460220747, "content": {"title": "H-SAUR: Hypothesize, Simulate, Act, Update, and Repeat for Understanding Object Articulations from Interactions", "abstract": ""}}
{"id": "NUGHXMk8Gf3", "cdate": 1640995200000, "mdate": 1665944319255, "content": {"title": "Generalized One-Class Learning Using Pairs of Complementary Classifiers", "abstract": "One-class learning is the classic problem of fitting a model to the data for which annotations are available only for a single class. In this paper, we explore novel objectives for one-class learning, which we collectively refer to as <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Generalized One-class Discriminative Subspaces</i> (GODS). Our key idea is to learn a pair of complementary classifiers to flexibly bound the one-class data distribution, where the data belongs to the positive half-space of one of the classifiers in the complementary pair and to the negative half-space of the other. To avoid redundancy while allowing non-linearity in the classifier decision surfaces, we propose to design each classifier as an orthonormal frame and seek to learn these frames via jointly optimizing for two conflicting objectives, namely: i) to minimize the distance between the two frames, and ii) to maximize the margin between the frames and the data. The learned orthonormal frames will thus characterize a piecewise linear decision surface that allows for efficient inference, while our objectives seek to bound the data within a minimal volume that maximizes the decision margin, thereby robustly capturing the data distribution. We explore several variants of our formulation under different constraints on the constituent classifiers, including kernelized feature maps. We demonstrate the empirical benefits of our approach via experiments on data from several applications in computer vision, such as anomaly detection in video sequences, human poses, and human activities. We also explore the generality and effectiveness of GODS for non-vision tasks via experiments on several UCI datasets, demonstrating state-of-the-art results."}}
{"id": "M3c5T2DconF", "cdate": 1640995200000, "mdate": 1680460220752, "content": {"title": "(2.5+1)D Spatio-Temporal Scene Graphs for Video Question Answering", "abstract": ""}}
{"id": "LVy5AR3ySJK", "cdate": 1640995200000, "mdate": 1680460220722, "content": {"title": "Are Deep Neural Networks SMARTer than Second Graders?", "abstract": ""}}
