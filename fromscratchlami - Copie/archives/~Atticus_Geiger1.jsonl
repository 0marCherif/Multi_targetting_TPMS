{"id": "53eo778jdtX", "cdate": 1667393653165, "mdate": null, "content": {"title": "Causal Abstraction with Soft Interventions", "abstract": "Causal abstraction provides a theory describing how several causal models can represent the same system at different levels of detail. Existing theoretical proposals limit the analysis of abstract models to \"hard\" interventions fixing causal variables to be constant values. In this work, we extend causal abstraction to \"soft\" interventions, which assign possibly non-constant functions to variables without adding new causal connections. Specifically, (i) we generalize $\\tau$-abstraction from Beckers and Halpern (2019) to soft interventions, (ii) we propose a further definition of soft abstraction to ensure a unique map $\\omega$ between soft interventions, and (iii) we prove that our constructive definition of soft abstraction guarantees the intervention map $\\omega$ has a specific and necessary explicit form."}}
{"id": "oHBgj83w1MB", "cdate": 1663850041801, "mdate": null, "content": {"title": "Causal Proxy Models For Concept-Based Model Explanations", "abstract": "Explainability methods for NLP systems encounter a version of the fundamental problem of causal inference: for a given ground-truth input text, we never truly observe the counterfactual texts necessary for isolating the causal effects of model representations on outputs. In response, many explainability methods make no use of counterfactual texts, assuming they will be unavailable. In this paper, we show that robust causal explainability methods can be created using approximate counterfactuals, which can be written by humans to approximate a specific counterfactual or simply sampled using metadata-guided heuristics. The core of our proposal is the Causal Proxy Model (CPM). A CPM explains a black-box model $\\mathcal{N}$ because it is trained to have the same \\emph{actual} input/output behavior as $\\mathcal{N}$ while creating neural representations that can be intervened upon to simulate the \\emph{counterfactual} input/output behavior of $\\mathcal{N}$.  Furthermore, we show that the best CPM for $\\mathcal{N}$ performs comparably to $\\mathcal{N}$ in making factual predictions, which means that the CPM can simply replace $\\mathcal{N}$, leading to more explainable deployed models."}}
{"id": "3AbigH4s-ml", "cdate": 1652737356101, "mdate": null, "content": {"title": "CEBaB: Estimating the Causal Effects of Real-World Concepts on NLP Model Behavior", "abstract": "The increasing size and complexity of modern ML systems has improved their predictive capabilities but made their behavior harder to explain. Many techniques for model explanation have been developed in response, but we lack clear criteria for assessing these techniques. In this paper, we cast model explanation as the causal inference problem of estimating causal effects of real-world concepts on the output behavior of ML models given actual input data. We introduce CEBaB, a new benchmark dataset for assessing concept-based explanation methods in Natural Language Processing (NLP). CEBaB consists of short restaurant reviews with human-generated counterfactual reviews in which an aspect (food, noise, ambiance, service) of the dining experience was modified. Original and counterfactual reviews are annotated with multiply-validated sentiment ratings at the aspect-level and review-level. The rich structure of CEBaB allows us to go beyond input features to study the effects of abstract, real-world concepts on model behavior. We use CEBaB to compare the quality of a range of concept-based explanation methods covering different assumptions and conceptions of the problem, and we seek to establish natural metrics for comparative assessments of these methods."}}
{"id": "RmuXDtjDhG", "cdate": 1621629933016, "mdate": null, "content": {"title": "Causal Abstractions of Neural Networks", "abstract": "Structural analysis methods (e.g., probing and feature attribution) are increasingly important tools for neural network analysis. We propose a new structural analysis method grounded in a formal theory of causal abstraction that provides rich characterizations of model-internal representations and their roles in input/output behavior. In this method, neural representations are aligned with variables in interpretable causal models, and then interchange interventions are used to experimentally verify that the neural representations have the causal properties of their aligned variables. We apply this method in a case study to analyze neural models trained on Multiply Quantified Natural Language Inference (MQNLI) corpus, a highly complex NLI dataset that was constructed with a tree-structured natural logic causal model. We discover that a BERT-based model with state-of-the-art performance successfully realizes parts of the natural logic model\u2019s causal structure, whereas a simpler baseline model fails to show any such structure, demonstrating that neural representations encode the compositional structure of MQNLI examples."}}
{"id": "S1WvnfbdZS", "cdate": 1546300800000, "mdate": null, "content": {"title": "Recursive Routing Networks: Learning to Compose Modules for Language Understanding", "abstract": "Ignacio Cases, Clemens Rosenbaum, Matthew Riemer, Atticus Geiger, Tim Klinger, Alex Tamkin, Olivia Li, Sandhini Agarwal, Joshua D. Greene, Dan Jurafsky, Christopher Potts, Lauri Karttunen. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019."}}
