{"id": "Ul1legCUGIV", "cdate": 1652737565136, "mdate": null, "content": {"title": "Constraining Gaussian Processes to Systems of Linear Ordinary Differential Equations", "abstract": "Data in many applications follows systems of Ordinary Differential Equations (ODEs).This paper presents a novel algorithmic and symbolic construction for covariance functions of Gaussian Processes (GPs) with realizations strictly following a system of linear homogeneous ODEs with constant coefficients, which we call LODE-GPs. Introducing this strong inductive bias into a GP improves modelling of such data. Using smith normal form algorithms, a symbolic technique, we overcome two current restrictions in the state of the art: (1) the need for certain uniqueness conditions in the set of solutions, typically assumed in classical ODE solvers and their probabilistic counterparts, and (2) the restriction to controllable systems, typically assumed when encoding differential equations in covariance functions. We show the effectiveness of LODE-GPs in a number of experiments, for example learning physically interpretable parameters by maximizing the likelihood."}}
{"id": "BaM-aStnwyc", "cdate": 1609459200000, "mdate": 1644837188752, "content": {"title": "Linearly Constrained Gaussian Processes with Boundary Conditions", "abstract": "One goal in Bayesian machine learning is to encode prior knowledge into prior distributions, to model data efficiently. We consider prior knowledge from systems of linear partial differential equations together with their boundary conditions. We construct multi-output Gaussian process priors with realizations in the solution set of such systems, in particular only such solutions can be represented by Gaussian process regression. The construction is fully algorithmic via Gr\u00f6bner bases and it does not employ any approximation. It builds these priors combining two parametrizations via a pullback: the first parametrizes the solutions for the system of differential equations and the second parametrizes all functions adhering to the boundary conditions."}}
{"id": "BHl-6rKhvk5", "cdate": 1609459200000, "mdate": 1644837188766, "content": {"title": "Including Sparse Production Knowledge into Variational Autoencoders to Increase Anomaly Detection Reliability", "abstract": "Digitalization leads to data transparency for production systems that we can benefit from with data-driven analysis methods like neural networks. For example, automated anomaly detection enables saving resources and optimizing the production. We study using rarely occurring information about labeled anomalies into Variational Autoencoder neural network structures to overcome information deficits of supervised and unsupervised approaches. This method outperforms all other models in terms of accuracy, precision, and recall. We evaluate the following methods: Principal Component Analysis, Isolation Forest, Classifying Neural Networks, and Variational Autoencoders on seven time series datasets to find the best performing detection methods. We extend this idea to include more infrequently occurring meta information about production processes. This use of sparse labels, both of anomalies or production data, allows to harness any additional information available for increasing anomaly detection performance."}}
{"id": "Su-b6rYhPy5", "cdate": 1577836800000, "mdate": 1644837188766, "content": {"title": "Towards Gaussian Processes for Automatic and Interpretable Anomaly Detection in Industry 4.0", "abstract": ""}}
{"id": "H8cZ6HY2wk9", "cdate": 1546300800000, "mdate": 1644837188753, "content": {"title": "Priors for Linear Differential Equations", "abstract": "We algorithmically construct multi-output Gaussian process priors which satisfy linear differential equations. We parametrize all solutions of the differential equations using Gr\u00f6bner bases for controllable systems. If successful, a push forward along the parametrization is the desired prior. This prior yields an interpretable machine learning model, which can combine linear differential equations with noisy data points."}}
{"id": "rkbx6ububS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Algorithmic Linearly Constrained Gaussian Processes", "abstract": "We algorithmically construct multi-output Gaussian process priors which satisfy linear differential equations. Our approach attempts to parametrize all solutions of the equations using Gr\u00f6bner bases. If successful, a push forward Gaussian process along the paramerization is the desired prior. We consider several examples from physics, geomathmatics and control, among them the full inhomogeneous system of Maxwell's equations. By bringing together stochastic learning and computeralgebra in a novel way, we combine noisy observations with precise algebraic computations."}}
