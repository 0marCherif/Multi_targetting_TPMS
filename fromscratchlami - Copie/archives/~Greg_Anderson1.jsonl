{"id": "IS-ewo8nLSO", "cdate": 1672599550371, "mdate": 1672599550371, "content": {"title": "Optimization and Abstraction: A Synergistic Approach for Analyzing Neural Network Robustness", "abstract": "In recent years, the notion of local robustness (or robustness\nfor short) has emerged as a desirable property of deep neural\nnetworks. Intuitively, robustness means that small perturba-\ntions to an input do not cause the network to perform misclas-\nsifications. In this paper, we present a novel algorithm for ver-\nifying robustness properties of neural networks. Our method\nsynergistically combines gradient-based optimization meth-\nods for counterexample search with abstraction-based proof\nsearch to obtain a sound and (\u03b4 -)complete decision proce-\ndure. Our method also employs a data-driven approach to\nlearn a verification policy that guides abstract interpretation\nduring proof search. We have implemented the proposed\napproach in a tool called Charon and experimentally evalu-\nated it on hundreds of benchmarks. Our experiments show\nthat the proposed approach significantly outperforms three\nstate-of-the-art tools, namely AI2, Reluplex, and Reluval."}}
{"id": "ZtTqF6iytkL", "cdate": 1672599438313, "mdate": 1672599438313, "content": {"title": "Neurosymbolic Reinforcement Learning with Formally Verified Exploration", "abstract": "We present REVEL, a partially neural reinforcement learning (RL) framework for\nprovably safe exploration in continuous state and action spaces. A key challenge\nfor provably safe deep RL is that repeatedly verifying neural networks within a\nlearning loop is computationally infeasible. We address this challenge using two\npolicy classes: a general, neurosymbolic class with approximate gradients and a\nmore restricted class of symbolic policies that allows efficient verification. Our\nlearning algorithm is a mirror descent over policies: in each iteration, it safely lifts\na symbolic policy into the neurosymbolic space, performs safe gradient updates to\nthe resulting policy, and projects the updated policy into the safe symbolic subset,\nall without requiring explicit verification of neural networks. Our empirical results\nshow that REVEL enforces safe exploration in many scenarios in which Constrained\nPolicy Optimization does not, and that it can discover policies that outperform\nthose learned through prior approaches to verified exploration."}}
{"id": "c8yRxKKvjN", "cdate": 1672531200000, "mdate": 1682002083174, "content": {"title": "Policy Optimization with Robustness Certificates", "abstract": "We present a reinforcement learning (RL) framework in which the learned policy comes with a machine-checkable certificate of provable adversarial robustness. Our approach, called CAROL, learns a model of the environment. In each learning iteration, it uses the current version of this model and an external abstract interpreter to construct a differentiable signal for provable robustness. This signal is used to guide learning, and the abstract interpretation used to construct it directly leads to the robustness certificate returned at convergence. We give a theoretical analysis that bounds the worst-case accumulative reward of CAROL. We also experimentally evaluate CAROL on four MuJoCo environments with continuous state and action spaces. On these tasks, CAROL learns policies that, when contrasted with policies from the state-of-the-art robust RL algorithms, exhibit: (i) markedly enhanced certified performance lower bounds; and (ii) comparable performance under empirical adversarial attacks."}}
{"id": "zzqBoIFOQ1", "cdate": 1663850184200, "mdate": null, "content": {"title": "Guiding Safe Exploration with Weakest Preconditions", "abstract": "In reinforcement learning for safety-critical settings, it is often desirable for the agent to obey safety constraints at all points in time, including during training. We present a novel neurosymbolic approach called SPICE to solve this safe exploration problem. SPICE uses an online shielding layer based on symbolic weakest preconditions to achieve a more precise safety analysis than existing tools without unduly impacting the training process. We evaluate the approach on a suite of continuous control benchmarks and show that it can achieve comparable performance to existing safe learning techniques while incurring fewer safety violations. Additionally, we present theoretical results showing that SPICE converges to the optimal safe policy under reasonable assumptions."}}
{"id": "w3lx0vvMna", "cdate": 1640995200000, "mdate": 1682002083183, "content": {"title": "Guiding Safe Exploration with Weakest Preconditions", "abstract": "In reinforcement learning for safety-critical settings, it is often desirable for the agent to obey safety constraints at all points in time, including during training. We present a novel neurosymbolic approach called SPICE to solve this safe exploration problem. SPICE uses an online shielding layer based on symbolic weakest preconditions to achieve a more precise safety analysis than existing tools without unduly impacting the training process. We evaluate the approach on a suite of continuous control benchmarks and show that it can achieve comparable performance to existing safe learning techniques while incurring fewer safety violations. Additionally, we present theoretical results showing that SPICE converges to the optimal safe policy under reasonable assumptions."}}
