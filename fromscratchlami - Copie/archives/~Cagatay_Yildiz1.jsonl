{"id": "kaVwKMbF7T", "cdate": 1672531200000, "mdate": 1699198608753, "content": {"title": "Latent Neural ODEs with Sparse Bayesian Multiple Shooting", "abstract": ""}}
{"id": "WnFQioG-LT2", "cdate": 1672531200000, "mdate": 1683145290138, "content": {"title": "Invariant Neural Ordinary Differential Equations", "abstract": "Neural ordinary differential equations (NODEs) have been proven useful for learning non-linear dynamics of arbitrary trajectories. However, current NODE methods capture variations across trajectories only via the initial state value or by auto-regressive encoder updates. In this work, we introduce Modulated Neural ODEs (MoNODEs), a novel framework that sets apart dynamics states from underlying static factors of variation and improves the existing NODE methods. In particular, we introduce $\\textit{time-invariant modulator variables}$ that are learned from the data. We incorporate our proposed framework into four existing NODE variants. We test MoNODE on oscillating systems, videos and human walking trajectories, where each trajectory has trajectory-specific modulation. Our framework consistently improves the existing model ability to generalize to new dynamic parameterizations and to perform far-horizon forecasting. In addition, we verify that the proposed modulator variables are informative of the true unknown factors of variation as measured by $R^2$ scores."}}
{"id": "vj9vS27Gq6P", "cdate": 1664300345323, "mdate": null, "content": {"title": "Latent GP-ODEs with Informative Priors", "abstract": "  For many complex systems the parametric form of the differential equation might be \n  unknown or infeasible to determine. Earlier works have explored to model the unknown\n  ODE system with a Gaussian Process model, however, the application has been limited to\n  a low dimensional data setting. We propose a novel framework by combining a generative and a\n  Bayesian nonparametric model. Our model learns a physically meaningful latent representation (position, \n  momentum) and solves in the latent space an ODE system. The use of GP allows us to account \n  for uncertainty as well as to extend our work with informative priors. We demonstrate our \n  framework on an image rotation dataset. The method demonstrates its ability to learn \n  dynamics from high dimensional data and we obtain state-of-the-art performance compared to \n  earlier GP-based ODEs models on dynamic forecasting."}}
{"id": "moIlFZfj_1b", "cdate": 1663850340451, "mdate": null, "content": {"title": "Latent Neural ODEs with Sparse Bayesian Multiple Shooting", "abstract": "Training dynamic models, such as neural ODEs, on long trajectories is a hard problem that requires using various tricks, such as trajectory splitting, to make model training work in practice. These methods are often heuristics with poor theoretical justifications, and require iterative manual tuning. We propose a principled multiple shooting technique for neural ODEs that splits the trajectories into manageable short segments, which are optimized in parallel, while ensuring probabilistic control on continuity over consecutive segments. We derive variational inference for our shooting-based latent neural ODE models and propose amortized encodings of irregularly sampled trajectories with a transformer-based recognition network with temporal attention and relative positional encoding. We demonstrate efficient and stable training, and state-of-the-art performance on multiple large-scale benchmark datasets."}}
{"id": "v1bxRZJ9c8V", "cdate": 1652737696053, "mdate": null, "content": {"title": "Learning interacting dynamical systems with latent Gaussian process ODEs", "abstract": "We study uncertainty-aware modeling of continuous-time dynamics of interacting objects. We introduce a new model that decomposes independent dynamics of single objects accurately from their interactions. By employing latent Gaussian process ordinary differential equations, our model infers both independent dynamics and their interactions with reliable uncertainty estimates. In our formulation, each object is represented as a graph node and interactions are modeled by accumulating the messages coming from neighboring objects. We show that efficient inference of such a complex network of variables is possible with modern variational sparse Gaussian process inference techniques. We empirically demonstrate that our model improves the reliability of long-term predictions over neural network based alternatives and it successfully handles missing dynamic or static information. Furthermore, we observe that only our model can successfully encapsulate independent dynamics and interaction information in distinct functions and show the benefit from this disentanglement in extrapolation scenarios."}}
{"id": "r2NuhIUoceq", "cdate": 1646077524243, "mdate": null, "content": {"title": "Variational multiple shooting for Bayesian ODEs with Gaussian processes", "abstract": "Recent machine learning advances have proposed black-box estimation of $\\textit{unknown continuous-time system dynamics}$ directly from data. However, earlier works are based on approximative solutions or point estimates. We propose a novel Bayesian nonparametric model that uses Gaussian processes to infer posteriors of unknown ODE systems directly from data. We derive sparse variational inference with decoupled functional sampling to represent vector field posteriors. We also introduce a probabilistic shooting augmentation to enable efficient inference from arbitrarily long trajectories. The method demonstrates the benefit of computing vector field posteriors, with predictive uncertainty scores outperforming alternative methods on multiple ODE learning tasks."}}
{"id": "wG7jsxFZeu", "cdate": 1640995200000, "mdate": 1682577613518, "content": {"title": "Latent Neural ODEs with Sparse Bayesian Multiple Shooting", "abstract": "Training dynamic models, such as neural ODEs, on long trajectories is a hard problem that requires using various tricks, such as trajectory splitting, to make model training work in practice. These methods are often heuristics with poor theoretical justifications, and require iterative manual tuning. We propose a principled multiple shooting technique for neural ODEs that splits the trajectories into manageable short segments, which are optimised in parallel, while ensuring probabilistic control on continuity over consecutive segments. We derive variational inference for our shooting-based latent neural ODE models and propose amortized encodings of irregularly sampled trajectories with a transformer-based recognition network with temporal attention and relative positional encoding. We demonstrate efficient and stable training, and state-of-the-art performance on multiple large-scale benchmark datasets."}}
{"id": "aNW90OqBoul", "cdate": 1640995200000, "mdate": 1699198608716, "content": {"title": "Learning interacting dynamical systems with latent Gaussian process ODEs", "abstract": "We study uncertainty-aware modeling of continuous-time dynamics of interacting objects. We introduce a new model that decomposes independent dynamics of single objects accurately from their interactions. By employing latent Gaussian process ordinary differential equations, our model infers both independent dynamics and their interactions with reliable uncertainty estimates. In our formulation, each object is represented as a graph node and interactions are modeled by accumulating the messages coming from neighboring objects. We show that efficient inference of such a complex network of variables is possible with modern variational sparse Gaussian process inference techniques. We empirically demonstrate that our model improves the reliability of long-term predictions over neural network based alternatives and it successfully handles missing dynamic or static information. Furthermore, we observe that only our model can successfully encapsulate independent dynamics and interaction information in distinct functions and show the benefit from this disentanglement in extrapolation scenarios."}}
{"id": "YJNze3flk8A", "cdate": 1640995200000, "mdate": 1699198608707, "content": {"title": "Differential Equations for Machine Learning", "abstract": "Mechanistic models express novel hypotheses for an observed phenomenon by constructing mathematical formulations of causal mechanisms. As opposed to this modeling paradigm, machine learning approaches learn input-output mappings by complicated and often non-interpretable models. While requiring large chunks of data for successful training and downstream performance,the resulting models can come with universal approximation guarantees. Historically, differential equations (DEs) developed in physics, economics, engineering, and numerous other fields have relied on the principles of mechanistic modeling. Despite providing causality and interpretability that machine learning approaches usually lack, mechanistic differential equation models tend tocarry oversimplified assumptions. In this dissertation, we aim to bring these two worlds together by demonstrating how machine learning problems can be tackled by means of differential equations, and how differential equation models can benefit from modern machine learning tools. First, we examine the problems in which mechanistic modeling becomes too difficult, which include the cases with partial knowledge about the observed system and with an excessive number of interactions. Such limitations complicate the process of constructing mathematical descriptions of the phenomenon of interest. To bypass this, we propose to place Gaussian process priors to the time differential and diffusion functions of unknown ordinary (ODEs) and stochastic differential equations (SDEs), and approximate the resulting intractable posterior distribution. We demonstrate that the model can estimate unknown dynamics from sparse and noisy observations. Motivated by the fact that our proposed approach is unable to learn sequences obtained by transforming the ODE states, we develop a new technique that can simultaneously embed the observations into a latent space, and learn an ODE system in the embedding space. Our new model infers the dynamics using Bayesian neural networks for uncertainty handling and more expressive power. We furthermore explicitly decompose the latent space into momentum and position components, which leads to increased predictive performance on a variety of physical tasks.Our next task concerns another problem involving DEs, namely, non-convex optimization. By carefully crafting the drift and diffusion functions of an SDE, we first obtain a stochastic gradient MCMC algorithm. Tuning a temperature variable in the proposed algorithm allows the chain to converge to the global minimum of a non-convex loss surface. We significantly speed up the convergence by using second-order Hessian information in an asynchronous parallel framework. Lastly, we explore how reinforcement learning problems can benefit from neural network based ODE models. In particular, we propose to learn dynamical systems controlled by external actions by a novel, uncertainty-aware neural ODE model. The inferred model, in turn, is utilized for learning optimal policy functions. We illustrate that our method is robust to both noisy and irregularly sampled data sequences, which poses major challenges to traditional methods."}}
{"id": "Wr2BV1Vyf2", "cdate": 1640995200000, "mdate": 1682577613519, "content": {"title": "Variational multiple shooting for Bayesian ODEs with Gaussian processes", "abstract": "Recent machine learning advances have proposed black-box estimation of \\textit{unknown continuous-time system dynamics} directly from data. However, earlier works are based on approximative solutio..."}}
