{"id": "t4VNXOgRNVf", "cdate": 1672531200000, "mdate": 1681490134628, "content": {"title": "Learning Object-Centric Neural Scattering Functions for Free-viewpoint Relighting and Scene Composition", "abstract": ""}}
{"id": "IF6rm7HE4S", "cdate": 1672531200000, "mdate": 1681490135099, "content": {"title": "Accidental Light Probes", "abstract": ""}}
{"id": "5OvsWmzoF6V", "cdate": 1672531200000, "mdate": 1681490133828, "content": {"title": "Learning Vortex Dynamics for Fluid Inference and Prediction", "abstract": ""}}
{"id": "BgoOPulkznY", "cdate": 1663850492218, "mdate": null, "content": {"title": "Unsupervised 3D Scene Representation Learning via Movable Object Inference", "abstract": "Unsupervised, category-agnostic, object-centric 3D representation learning for complex scenes remains an open problem in computer vision. While a few recent methods can now discover 3D object radiance fields from a single image without supervision, they are limited to simplistic scenes with objects of a single category, often with a uniform color. This is because they discover objects purely based on appearance cues\u2014objects are made of pixels that look alike. In this work, we propose Movable Object Radiance Fields (MORF), aiming at scaling to complex scenes with diverse categories of objects. Inspired by cognitive science of object learning in babies, MORF learns 3D object representations via movable object inference. During training, MORF first obtains 2D masks of movable objects via a self-supervised movable object segmentation method; it then bridges the gap to 3D object representations via conditional neural rendering in multiple views. During testing, MORF can discover, reconstruct, and move unseen objects from novel categories, all from a single image. Experiments show that MORF extracts accurate object geometry and supports realistic object and scene reconstruction and editing, significantly outperforming the state-of-the-art."}}
{"id": "nYWqxUwFc3x", "cdate": 1663850263251, "mdate": null, "content": {"title": "Learning Vortex Dynamics for Fluid Inference and Prediction", "abstract": "We propose a novel differentiable vortex particle (DVP) method to infer and predict fluid dynamics from a single video. Lying at its core is a particle-based latent space to encapsulate the hidden, Lagrangian vortical evolution underpinning the observable, Eulerian flow phenomena. Our differentiable vortex particles are coupled with a learnable, vortex-to-velocity dynamics mapping to effectively capture the complex flow features in a physically-constrained, low-dimensional space. This representation facilitates the learning of a fluid simulator tailored to the input video that can deliver robust, long-term future predictions. The value of our method is twofold: first, our learned simulator enables the inference of hidden physics quantities (e.g., velocity field) purely from visual observation; secondly, it also supports future prediction, constructing the input video's sequel along with its future dynamics evolution. We compare our method with a range of existing methods on both synthetic and real-world videos, demonstrating improved reconstruction quality, visual plausibility, and physical integrity."}}
{"id": "hkdqktIzBK", "cdate": 1640995200000, "mdate": 1681490136126, "content": {"title": "Rotationally Equivariant 3D Object Detection", "abstract": ""}}
{"id": "Q3mSqD1Jsk", "cdate": 1640995200000, "mdate": 1668232745862, "content": {"title": "Unsupervised Discovery and Composition of Object Light Fields", "abstract": "Neural scene representations, both continuous and discrete, have recently emerged as a powerful new paradigm for 3D scene understanding. Recent efforts have tackled unsupervised discovery of object-centric neural scene representations. However, the high cost of ray-marching, exacerbated by the fact that each object representation has to be ray-marched separately, leads to insufficiently sampled radiance fields and thus, noisy renderings, poor framerates, and high memory and time complexity during training and rendering. Here, we propose to represent objects in an object-centric, compositional scene representation as light fields. We propose a novel light field compositor module that enables reconstructing the global light field from a set of object-centric light fields. Dubbed Compositional Object Light Fields (COLF), our method enables unsupervised learning of object-centric neural scene representations, state-of-the-art reconstruction and novel view synthesis performance on standard datasets, and rendering and training speeds at orders of magnitude faster than existing 3D approaches."}}
{"id": "MPUbPrsZfV", "cdate": 1640995200000, "mdate": 1681490137103, "content": {"title": "Unsupervised Discovery of Object Radiance Fields", "abstract": ""}}
{"id": "E--742ESYKC", "cdate": 1640995200000, "mdate": 1681490136493, "content": {"title": "Differentiable Physics Simulation of Dynamics-Augmented Neural Objects", "abstract": ""}}
{"id": "rwE8SshAlxw", "cdate": 1632875449521, "mdate": null, "content": {"title": "Unsupervised Discovery of Object Radiance Fields", "abstract": "We study the problem of inferring an object-centric scene representation from a single image, aiming to derive a representation that explains the image formation process, captures the scene's 3D nature, and is learned without supervision. Most existing methods on scene decomposition lack one or more of these characteristics, due to the fundamental challenge in integrating the complex 3D-to-2D image formation process into powerful inference schemes like deep networks. In this paper, we propose unsupervised discovery of Object Radiance Fields (uORF), integrating recent progresses in neural 3D scene representations and rendering with deep inference networks for unsupervised 3D scene decomposition. Trained on multi-view RGB images without annotations, uORF learns to decompose complex scenes with diverse, textured background from a single image. We show that uORF enables novel tasks, such as scene segmentation and editing in 3D, and it performs well on these tasks and on novel view synthesis on three datasets."}}
