{"id": "IilnB8jfoP9", "cdate": 1675970198416, "mdate": null, "content": {"title": "Learning protein family manifolds with smoothed energy-based models", "abstract": "We resolve difficulties in training and sampling from discrete energy-based models (EBMs) by learning a smoothed energy landscape, sampling the smoothed data manifold with Langevin Markov chain Monte Carlo, and projecting back to the true data manifold with one-step denoising. Our formalism combines the attractive properties of EBMs and improved sample quality of score-based models, while simplifying training and sampling by requiring only a single noise scale. We demonstrate the robustness of our approach on generative modeling of antibody proteins."}}
{"id": "fQbB0tr3-f0", "cdate": 1675965657349, "mdate": null, "content": {"title": "LEARNING PROTEIN FAMILY MANIFOLDS WITH SMOOTHED ENERGY-BASED MODELS", "abstract": "We resolve difficulties in training and sampling from discrete energy-based models (EBMs) by learning a smoothed energy landscape, sampling the smoothed data manifold with Langevin Markov chain Monte Carlo, and projecting back to the true data manifold with one-step denoising. Our Smoothed Discrete Sampling formalism combines the attractive properties of EBMs and improved sample quality of score-based models, while simplifying training and sampling by requiring only a single noise scale. We demonstrate the robustness of our approach on generative modeling of antibody proteins and successfully express and purify 97% of generated designs in a single round of laboratory experiments."}}
{"id": "dkSlMAOxlEQ", "cdate": 1675116391687, "mdate": 1675116391687, "content": {"title": "OpenFold: Retraining AlphaFold2 yields new insights into its learning mechanisms and capacity for generalization", "abstract": "AlphaFold2 revolutionized structural biology with the ability to predict protein structures with exceptionally high accuracy. Its implementation, however, lacks the code and data required to train new models. These are necessary to (i) tackle new tasks, like protein-ligand complex structure prediction, (ii) investigate the process by which the model learns, which remains poorly understood, and (iii) assess the model\u2019s generalization capacity to unseen regions of fold space. Here we report OpenFold, a fast, memory-efficient, and trainable implementation of AlphaFold2, and OpenProtein-Set, the largest public database of protein multiple sequence alignments. We use OpenProteinSet to train OpenFold from scratch, fully matching the accuracy of AlphaFold2. Having established parity, we assess OpenFold\u2019s capacity to generalize across fold space by retraining it using carefully designed datasets. We find that OpenFold is remarkably robust at generalizing despite extreme reductions in training set size and diversity, including near-complete elisions of classes of secondary structure elements. By analyzing intermediate structures produced by OpenFold during training, we also gain surprising insights into the manner in which the model learns to fold proteins, discovering that spatial dimensions are learned sequentially. Taken together, our studies demonstrate the power and utility of OpenFold, which we believe will prove to be a crucial new resource for the protein modeling community."}}
{"id": "1ole6zX6USA", "cdate": 1648731966261, "mdate": null, "content": {"title": "Multi-Segment Preserving Sampling for Deep Manifold Sampler", "abstract": "Deep generative modeling for biological sequences presents a unique challenge in reconciling the bias-variance trade-off between explicit biological insight and model flexibility.\nThe deep manifold sampler was recently proposed as a means to iteratively sample variable-length protein sequences. \nSampling was done by exploiting the gradients from a function predictor trained on top of the manifold sampler.\nIn this work, we introduce an alternative approach to guided sampling that enables the direct inclusion of domain-specific knowledge by designating preserved and non-preserved segments along the input sequence, thereby restricting variation to only select regions.\nWe call this method ``multi-segment preserving sampling\" and present its effectiveness in the context of antibody design.\nWe train two models: a deep manifold sampler and a GPT-2 language model on nearly six million heavy chain sequences annotated with the \\textit{IGHV1-18} gene.\nDuring sampling, we restrict variation to only the complementarity-determining region 3 (CDR3) of the input. We obtain log probability scores from a GPT-2 model for each sampled CDR3 and demonstrate that multi-segment preserving sampling generates reasonable designs while maintaining the desired, preserved regions."}}
