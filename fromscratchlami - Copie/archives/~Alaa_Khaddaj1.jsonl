{"id": "4NT3umNU3D0", "cdate": 1663850116789, "mdate": null, "content": {"title": "Backdoor or Feature? A New Perspective on Data Poisoning", "abstract": "In a backdoor attack, an adversary adds maliciously constructed (\"backdoor\") examples into a training set to make the resulting model\nvulnerable to manipulation. Defending against such attacks---that is, finding and removing the backdoor examples---typically involves viewing these examples as outliers and using techniques from robust statistics to detect and remove them.\n\nIn this work, we present a new perspective on backdoor attacks. We argue that without structural information on the training data distribution, backdoor attacks are indistinguishable from naturally-occuring features in the data (and thus impossible to ``detect'' in a general sense). To circumvent this impossibility, we assume that a backdoor attack corresponds to the strongest feature in the training data. Under this assumption---which we make formal---we develop a new framework for detecting backdoor attacks. Our framework naturally gives rise to a corresponding algorithm whose efficacy we show both theoretically and experimentally."}}
{"id": "IrUFsuTxVfY", "cdate": 1663850045187, "mdate": null, "content": {"title": "A Data-Based Perspective on Transfer Learning", "abstract": "It is commonly believed that more pre-training data leads to better transfer learning performance. However, recent evidence suggests that removing data from the source dataset can actually help too. In this work, we present a framework for probing the impact of the source dataset's composition on transfer learning performance. Our framework facilitates new capabilities such as identifying transfer learning brittleness and detecting pathologies such as data-leakage and the presence of misleading examples in the source dataset. In particular, we demonstrate that removing detrimental datapoints identified by our framework improves transfer performance from ImageNet on a variety of transfer tasks."}}
