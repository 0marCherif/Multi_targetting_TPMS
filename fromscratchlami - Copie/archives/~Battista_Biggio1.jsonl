{"id": "AWjwoHV4jpM", "cdate": 1701798822723, "mdate": 1701798822723, "content": {"title": "Rethinking Data Augmentation for Adversarial Robustness", "abstract": "Recent work has proposed novel data augmentation methods to improve the adversarial robustness of deep neural networks. In this paper, we re-evaluate such methods through the lens of different metrics that characterize the augmented manifold, finding contradictory evidence. Our extensive empirical analysis involving 5 data augmentation methods, all tested with an increasing probability of augmentation, shows that: (i) novel data augmentation methods proposed to improve adversarial robustness only improve it when combined with classical augmentations (like image flipping and rotation), and even worsen adversarial robustness if used in isolation; and (ii) adversarial robustness is significantly affected by the augmentation probability, conversely to what is claimed in recent work. We conclude by discussing how to rethink the development and evaluation of novel data augmentation methods for adversarial robustness."}}
{"id": "6vdDp42oLzC", "cdate": 1654348671601, "mdate": null, "content": {"title": "ImageNet-Patch: A Dataset for Benchmarking Machine Learning Robustness against Adversarial Patches", "abstract": "Adversarial patches are optimized contiguous pixel blocks in an input image that cause a  machine-learning model to misclassify it.\nHowever, their optimization is computationally demanding and requires careful hyperparameter tuning.\nTo overcome these issues, we propose ImageNet-Patch, a dataset to benchmark machine-learning models against adversarial patches.\nIt consists of a set of patches optimized to generalize across different models and applied to ImageNet data after preprocessing them with affine transformations.\nThis process enables an approximate yet faster robustness evaluation, leveraging the transferability of adversarial perturbations."}}
{"id": "Y1sWzKW0k4L", "cdate": 1652737772792, "mdate": null, "content": {"title": "Indicators of Attack Failure: Debugging and Improving Optimization of Adversarial Examples", "abstract": "Evaluating robustness of machine-learning models to adversarial examples is a challenging problem. Many defenses have been shown to provide a false sense of robustness by causing gradient-based attacks to fail, and they have been broken under more rigorous evaluations.\nAlthough guidelines and best practices have been suggested to improve current adversarial robustness evaluations, the lack of automatic testing and debugging tools makes it difficult to apply these recommendations in a systematic manner.\nIn this work, we overcome these limitations by: (i) categorizing   attack failures based on how they affect the optimization of gradient-based attacks, while also  unveiling two novel failures affecting many popular attack implementations and past evaluations;\n (ii) proposing six novel \\emph{indicators of failure}, to automatically detect the presence of such failures in the attack optimization process; and (iii) suggesting a systematic protocol to apply the corresponding fixes. \nOur extensive experimental analysis, involving more than 15 models in 3 distinct application domains, shows that our indicators of failure can be used to debug and improve current adversarial robustness evaluations, thereby providing a first concrete step towards automatizing and systematizing them. Our open-source code is available at: https://github.com/pralab/IndicatorsOfAttackFailure."}}
{"id": "DMyz-FVPm7_", "cdate": 1624022584960, "mdate": null, "content": {"title": "Adversarial EXEmples: Functionality-preserving Optimization of Adversarial Windows Malware", "abstract": "Windows malware classifiers that rely on static analysis have been proven vulnerable to adversarial EXEmples, i.e., malware samples carefully manipulated to evade detection.\nHowever, such attacks are typically optimized via query-inefficient algorithms that iteratively apply random manipulations on the input malware, and require checking that the malicious functionality is preserved after manipulation through computationally-expensive validations.\nTo overcome these limitations, we propose RAMEn, a general framework for creating adversarial EXEmples via  functionality-preserving manipulations.\nRAMEn optimizes their parameters of such manipulations via gradient-based (white-box) and gradient-free (black-box) attacks, implementing many state-of-the-art attacks for crafting adversarial Windows malware. It also includes a family of black-box attacks, called GAMMA, which optimize the injection of benign content to facilitate evasion. Our experiments show that gradient-based and gradient-free attacks can bypass  malware detectors based on deep learning, non-differentiable models trained on hand-crafted features, and even some renowned commercial products."}}
{"id": "ahGBBZ7bHKr", "cdate": 1624022580168, "mdate": null, "content": {"title": "Fast Minimum-norm Adversarial Attacks through Adaptive Norm Constraints", "abstract": "Evaluating adversarial robustness amounts to finding the minimum perturbation needed to have an input sample misclassified. \nThe inherent complexity of the underlying optimization requires current gradient-based attacks to be carefully tuned, initialized, and possibly executed for many computationally-demanding iterations, even if specialized to a given perturbation model.\nIn this work, we overcome these limitations by proposing a fast minimum-norm (FMN) attack that works with different $\\ell_p$-norm perturbation models ($p=0, 1, 2, \\infty$), is robust to hyperparameter choices, does not require adversarial starting points, and converges within few lightweight steps. \nIt works by iteratively finding the sample misclassified with maximum confidence within an $\\ell_p$-norm constraint of size $\\epsilon$, while adapting $\\epsilon$ to minimize the distance of the current sample to the decision boundary.\nExtensive experiments show that FMN significantly outperforms existing attacks in terms of convergence speed and computation time, while reporting comparable or even smaller perturbation sizes."}}
{"id": "Pvhk4ZFnTR", "cdate": 1624022580034, "mdate": null, "content": {"title": "Indicators of Attack Failure: Debugging and Improving Optimization of Adversarial Examples", "abstract": "Evaluating robustness of machine-learning models to adversarial examples is a challenging problem. Many defenses have been shown to provide a false sense of security by causing gradient-based attacks to fail, and they have been broken under more rigorous evaluations.\nAlthough guidelines and best practices have been suggested to improve current adversarial robustness evaluations, the lack of automatic testing and debugging tools makes it difficult to apply these recommendations in a systematic manner.\nIn this work, we overcome these limitations by (i) defining a set of quantitative indicators which unveil common failures in the optimization of gradient-based attacks, and (ii) proposing specific mitigation strategies within a systematic evaluation protocol. \nOur extensive experimental analysis shows that the proposed indicators of failure can be used to visualize, debug and improve current adversarial robustness evaluations, providing a first concrete step towards automatizing and systematizing current adversarial robustness evaluations."}}
{"id": "JdQ2-DTaGF", "cdate": 1621630338976, "mdate": null, "content": {"title": "Indicators of Attack Failure: Debugging and Improving Optimization of Adversarial Examples", "abstract": "Evaluating robustness of machine-learning models to adversarial examples is a challenging problem. Many defenses have been shown to provide a false sense of security by causing gradient-based attacks to fail, and they have been broken under more rigorous evaluations. Although guidelines and best practices have been suggested to improve current adversarial robustness evaluations, the lack of automatic testing and debugging tools makes it difficult to apply these recommendations in a systematic manner. In this work, we overcome these limitations by (i) defining a set of quantitative indicators which unveil common failures in the optimization of gradient-based attacks, and (ii) proposing specific mitigation strategies within a systematic evaluation protocol. Our extensive experimental analysis shows that the proposed indicators of failure can be used to visualize, debug and improve current adversarial robustness evaluations, providing a first concrete step towards automatizing and systematizing current adversarial robustness evaluations."}}
{"id": "jfDaBf8PAE", "cdate": 1621630165580, "mdate": null, "content": {"title": "Fast Minimum-norm Adversarial Attacks through Adaptive Norm Constraints", "abstract": "Evaluating adversarial robustness amounts to finding the minimum perturbation needed to have an input sample misclassified. \nThe inherent complexity of the underlying optimization requires current gradient-based attacks to be carefully tuned, initialized, and possibly executed for many computationally-demanding iterations, even if specialized to a given perturbation model.\nIn this work, we overcome these limitations by proposing a fast minimum-norm (FMN) attack that works with different $\\ell_p$-norm perturbation models ($p=0, 1, 2, \\infty$), is robust to hyperparameter choices, does not require adversarial starting points, and converges within few lightweight steps. \nIt works by iteratively finding the sample misclassified with maximum confidence within an $\\ell_p$-norm constraint of size $\\epsilon$, while adapting $\\epsilon$ to minimize the distance of the current sample to the decision boundary.\nExtensive experiments show that FMN significantly outperforms existing $\\ell_0$, $\\ell_1$, and $\\ell_\\infty$-norm attacks in terms of perturbation size, convergence speed and computation time, while reporting comparable performances with state-of-the-art $\\ell_2$-norm attacks. Our open-source code is available at: https://github.com/pralab/Fast-Minimum-Norm-FMN-Attack."}}
{"id": "xsEIKVufWYHu", "cdate": 1577836800000, "mdate": null, "content": {"title": "Poisoning Attacks on Algorithmic Fairness.", "abstract": "Research in adversarial machine learning has shown how the performance of machine learning models can be seriously compromised by injecting even a small fraction of poisoning points into the training data. While the effects on model accuracy of such poisoning attacks have been widely studied, their potential effects on other model performance metrics remain to be evaluated. In this work, we introduce an optimization framework for poisoning attacks against algorithmic fairness, and develop a gradient-based poisoning attack aimed at introducing classification disparities among different groups in the data. We empirically show that our attack is effective not only in the white-box setting, in which the attacker has full access to the target model, but also in a more challenging black-box scenario in which the attacks are optimized against a substitute model and then transferred to the target model. We believe that our findings pave the way towards the definition of an entirely novel set of adversarial attacks targeting algorithmic fairness in different scenarios, and that investigating such vulnerabilities will help design more robust algorithms and countermeasures in the future."}}
{"id": "VGAV9dGU631s", "cdate": 1577836800000, "mdate": null, "content": {"title": "Deep neural rejection against adversarial examples.", "abstract": "Despite the impressive performances reported by deep neural networks in different application domains, they remain largely vulnerable to adversarial examples, i.e., input samples that are carefully perturbed to cause misclassification at test time. In this work, we propose a deep neural rejection mechanism to detect adversarial examples, based on the idea of rejecting samples that exhibit anomalous feature representations at different network layers. With respect to competing approaches, our method does not require generating adversarial examples at training time, and it is less computationally demanding. To properly evaluate our method, we define an adaptive white-box attack that is aware of the defense mechanism and aims to bypass it. Under this worst-case setting, we empirically show that our approach outperforms previously proposed methods that detect adversarial examples by only analyzing the feature representation provided by the output network layer."}}
