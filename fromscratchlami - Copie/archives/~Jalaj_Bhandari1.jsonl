{"id": "0bkuZpf0Itw", "cdate": 1664288563350, "mdate": 1664288563350, "content": {"title": "On the Linear Convergence of Policy Gradient Methods for Finite MDPs", "abstract": "We revisit the finite time analysis of policy gradient methods in the one of the simplest settings: finite state and action MDPs with a policy class consisting of all stochastic policies and with exact gradient evaluations. There has been some recent work viewing this setting as an instance of smooth non-linear optimization problems, to show sub-linear convergence rates with small step-sizes. Here, we take a completely different perspective based on illuminating connections with policy iteration, to show how many variants of policy gradient algorithms succeed with large step-sizes and attain a linear rate of convergence.\n"}}
{"id": "CWqXojCHRqq", "cdate": 1577836800000, "mdate": null, "content": {"title": "A Note on the Linear Convergence of Policy Gradient Methods", "abstract": "We revisit the finite time analysis of policy gradient methods in the one of the simplest settings: finite state and action MDPs with a policy class consisting of all stochastic policies and with exact gradient evaluations. There has been some recent work viewing this setting as an instance of smooth non-linear optimization problems and showing sub-linear convergence rates with small step-sizes. Here, we take a different perspective based on connections with policy iteration and show that many variants of policy gradient methods succeed with large step-sizes and attain a linear rate of convergence."}}
{"id": "5bQx7wgnHi", "cdate": 1546300800000, "mdate": null, "content": {"title": "Global Optimality Guarantees For Policy Gradient Methods.", "abstract": "Policy gradients methods apply to complex, poorly understood, control problems by performing stochastic gradient descent over a parameterized class of polices. Unfortunately, even for simple control problems solvable by standard dynamic programming techniques, policy gradient algorithms face non-convex optimization problems and are widely understood to converge only to a stationary point. This work identifies structural properties -- shared by several classic control problems -- that ensure the policy gradient objective function has no suboptimal stationary points despite being non-convex. When these conditions are strengthened, this objective satisfies a Polyak-lojasiewicz (gradient dominance) condition that yields convergence rates. We also provide bounds on the optimality gap of any stationary point when some of these conditions are relaxed."}}
{"id": "uZbuwSdrzZv", "cdate": 1514764800000, "mdate": null, "content": {"title": "A Finite Time Analysis of Temporal Difference Learning With Linear Function Approximation.", "abstract": "Temporal difference learning (TD) is a simple iterative algorithm used to estimate the value function corresponding to a given policy in a Markov decision process. Although TD is one of the most wi..."}}
{"id": "SFS-6Sbr2uG", "cdate": 1514764800000, "mdate": null, "content": {"title": "A Finite Time Analysis of Temporal Difference Learning With Linear Function Approximation.", "abstract": "Temporal difference learning (TD) is a simple iterative algorithm used to estimate the value function corresponding to a given policy in a Markov decision process. Although TD is one of the most widely used algorithms in reinforcement learning, its theoretical analysis has proved challenging and few guarantees on its statistical efficiency are available. In this work, we provide a simple and explicit finite time analysis of temporal difference learning with linear function approximation. Except for a few key insights, our analysis mirrors standard techniques for analyzing stochastic gradient descent algorithms, and therefore inherits the simplicity and elegance of that literature. Final sections of the paper show how all of our main results extend to the study of TD learning with eligibility traces, known as TD($\\lambda$), and to Q-learning applied in high-dimensional optimal stopping problems."}}
{"id": "J609_A3yLnQ", "cdate": 1483228800000, "mdate": null, "content": {"title": "Annular Augmentation Sampling.", "abstract": "The exponentially large sample space of general binary probabilistic models renders intractable standard operations such as exact marginalization, inference, and normalization. Typically, researchers deal with these distributions via deterministic approximations, the class of belief propagation methods being a prominent example. Comparatively, Markov Chain Monte Carlo methods have been significantly less used in this domain. In this work, we introduce an auxiliary variable MCMC scheme that samples from an annular augmented space, translating to a great circle path around the hypercube of the binary sample space. This annular augmentation sampler explores the sample space more effectively than coordinate-wise samplers and has no tunable parameters, leading to substantial performance gains in estimating quantities of interest in large binary models. We extend the method to incorporate into the sampler any existing mean-field approximation (such as from belief propagation), leading to further performance improvements. Empirically, we consider a range of large Ising models and an application to risk factors for heart disease."}}
{"id": "ovXyJ7eVe3-", "cdate": 1451606400000, "mdate": null, "content": {"title": "On the tightness of an LP relaxation for rational optimization and its applications.", "abstract": "We consider the problem of optimizing a linear rational function subject to totally unimodular (TU) constraints over { 0 , 1 } variables. Such formulations arise in many applications including assortment optimization. We show that a natural extended LP relaxation of the problem is \u201ctight\u201d. In other words, any extreme point corresponds to an integral solution. We also consider more general constraints that are not TU but obtained by adding an arbitrary constraint to the set of TU constraints. Using structural insights about extreme points, we present a polynomial time approximation scheme (PTAS) for the general problem."}}
{"id": "2dhuUlXEhBe", "cdate": 1451606400000, "mdate": null, "content": {"title": "Elliptical Slice Sampling with Expectation Propagation.", "abstract": ""}}
