{"id": "Hx9BmydEYV", "cdate": 1691265994782, "mdate": 1691265994782, "content": {"title": "Sharp-MAML: Sharpness-Aware Model-Agnostic Meta Learning", "abstract": "Model-agnostic meta learning (MAML) is currently one of the dominating approaches for few-shot meta-learning. Albeit its effectiveness, the training of MAML can be challenging due to the innate bilevel problem structure. Specifically, the loss landscape of MAML is much complex with possibly many more saddle points and local minima than its empirical risk minimization counterpart. To address this challenge, we leverage the recently invented sharpness-aware minimization and develop a sharpness-aware MAML approach that we term Sharp-MAML. We empirically demonstrate that Sharp-MAML and its computation-efficient variant can outperform popular existing MAML baselines (eg,+ 12% accuracy on Mini-Imagenet). We complement the empirical study with the convergence analysis and the generalization bound of Sharp-MAML. To the best of our knowledge, this is the first empirical and theoretical study on sharpness-aware minimization in the context of bilevel optimization."}}
{"id": "Z48v8wmPFQ", "cdate": 1640995200000, "mdate": 1681486394605, "content": {"title": "Sharp-MAML: Sharpness-Aware Model-Agnostic Meta Learning", "abstract": ""}}
{"id": "KayFBhbufe", "cdate": 1640995200000, "mdate": 1675209991094, "content": {"title": "Alternating Implicit Projected SGD and Its Efficient Variants for Equality-constrained Bilevel Optimization", "abstract": "Stochastic bilevel optimization, which captures the inherent nested structure of machine learning problems, is gaining popularity in many recent applications. Existing works on bilevel optimization mostly consider either unconstrained problems or constrained upper-level problems. This paper considers the stochastic bilevel optimization problems with equality constraints both in the upper and lower levels. By leveraging the special structure of the equality constraints problem, the paper first presents an alternating implicit projected SGD approach and establishes the $\\tilde{\\cal O}(\\epsilon^{-2})$ sample complexity that matches the state-of-the-art complexity of ALSET \\citep{chen2021closing} for unconstrained bilevel problems. To further save the cost of projection, the paper presents two alternating implicit projection-efficient SGD approaches, where one algorithm enjoys the $\\tilde{\\cal O}(\\epsilon^{-2}/T)$ upper-level and $\\tilde{\\cal O}(\\epsilon^{-1.5}/T^{\\frac{3}{4}})$ lower-level projection complexity with ${\\cal O}(T)$ lower-level batch size, and the other one enjoys $\\tilde{\\cal O}(\\epsilon^{-1.5})$ upper-level and lower-level projection complexity with ${\\cal O}(1)$ batch size. Application to federated bilevel optimization has been presented to showcase the empirical performance of our algorithms. Our results demonstrate that equality-constrained bilevel optimization with strongly-convex lower-level problems can be solved as efficiently as stochastic single-level optimization problems."}}
{"id": "J-eRCE7cgkn", "cdate": 1640995200000, "mdate": 1683900523544, "content": {"title": "Lazy Queries Can Reduce Variance in Zeroth-order Optimization", "abstract": "A major challenge of applying zeroth-order (ZO) methods is the high query complexity, especially when queries are costly. We propose a novel gradient estimation technique for ZO methods based on adaptive lazy queries that we term as LAZO. Different from the classic one-point or two-point gradient estimation methods, LAZO develops two alternative ways to check the usefulness of old queries from previous iterations, and then adaptively reuses them to construct the low-variance gradient estimates. We rigorously establish that through judiciously reusing the old queries, LAZO can reduce the variance of stochastic gradient estimates so that it not only saves queries per iteration but also achieves the regret bound for the symmetric two-point method. We evaluate the numerical performance of LAZO, and demonstrate the low-variance property and the performance gain of LAZO in both regret and query complexity relative to several existing ZO methods. The idea of LAZO is general, and can be applied to other variants of ZO methods."}}
{"id": "F8cJ_SYFvu", "cdate": 1640995200000, "mdate": 1683900523545, "content": {"title": "Federated Multi-Armed Bandit Via Uncoordinated Exploration", "abstract": "A wide range of multi-agent decision-making problems can be abstracted as a federated multi-armed bandit (FMAB) problem. A key challenge of the FMAB problem is that the exploration-exploitation dichotomy inherited from the multi-armed bandit aspect is compounded with data heterogeneity in federated learning. This renders the exploration and exploitation of different agents inherently entangled. This paper focuses on overcoming the difficulty of exploration in FMAB problems, and it proposes a novel federated upper confidence bound (UCB) algorithm that requires uncoordinated exploration (UE) decisions by the agents. The major distinction of this algorithm, referred to as FedUCB-UE, with the existing FMAB algorithms is that it allows the agents to explore the non-optimal arms and make personalized arm-selection decisions without coordination. While such uncoordinated exploration makes the regret analysis non-trivial, it comes with both the theoretical and empirical benefit of diversity in explorations. Under certain mild assumptions, this paper establishes that FedUCB-UE has a $\\mathcal{O}(\\log T)$ regret bound. Furthermore, experiments performed on synthetic datasets show that FedUCB-UE outperforms the state-of-the-art algorithms."}}
{"id": "7qKecBhMII", "cdate": 1640995200000, "mdate": 1683900523526, "content": {"title": "A Single-Timescale Method for Stochastic Bilevel Optimization", "abstract": "Stochastic bilevel optimization generalizes the classic stochastic optimization from the minimization of a single objective to the minimization of an objective function that depends on the solution of another optimization problem. Recently, bilevel optimization is regaining popularity in emerging machine learning applications such as hyper-parameter optimization and model-agnostic meta learning. To solve this class of optimization problems, existing methods require either double-loop or two-timescale updates, which are sometimes less efficient. This paper develops a new optimization method for a class of stochastic bilevel problems that we term Single-Timescale stochAstic BiLevEl optimization (STABLE) method. STABLE runs in a single loop fashion, and uses a single-timescale update with a fixed batch size. To achieve an $\\epsilon$-stationary point of the bilevel problem, STABLE requires ${\\cal O}(\\epsilon^{-2})$ samples in total; and to achieve an $\\epsilon$-optimal solution in the strongly convex case, STABLE requires ${\\cal O}(\\epsilon^{-1})$ samples. To the best of our knowledge, when STABLE was proposed, it is the first bilevel optimization algorithm achieving the same order of sample complexity as SGD for single-level stochastic optimization."}}
