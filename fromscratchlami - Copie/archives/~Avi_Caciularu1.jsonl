{"id": "2FKO7AYIz5", "cdate": 1704067200000, "mdate": 1707829971593, "content": {"title": "Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models", "abstract": "Inspecting the information encoded in hidden representations of large language models (LLMs) can explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of questions about an LLM's computation. We show that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation can be viewed as instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by Patchscopes. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model, and unlocks new applications such as self-correction in multi-hop reasoning."}}
{"id": "SbSh5h87uM", "cdate": 1680307200000, "mdate": 1683036560415, "content": {"title": "An entangled mixture of variational autoencoders approach to deep clustering", "abstract": ""}}
{"id": "vC7Htuh7uMZ", "cdate": 1672531200000, "mdate": 1686244820511, "content": {"title": "Revisiting Sentence Union Generation as a Testbed for Text Consolidation", "abstract": "Tasks involving text generation based on multiple input texts, such as multi-document summarization, long-form question answering and contemporary dialogue applications, challenge models for their ability to properly consolidate partly-overlapping multi-text information. However, these tasks entangle the consolidation phase with the often subjective and ill-defined content selection requirement, impeding proper assessment of models' consolidation capabilities. In this paper, we suggest revisiting the sentence union generation task as an effective well-defined testbed for assessing text consolidation capabilities, decoupling the consolidation challenge from subjective content selection. To support research on this task, we present refined annotation methodology and tools for crowdsourcing sentence union, create the largest union dataset to date and provide an analysis of its rich coverage of various consolidation aspects. We then propose a comprehensive evaluation protocol for union generation, including both human and automatic evaluation. Finally, as baselines, we evaluate state-of-the-art language models on the task, along with a detailed analysis of their capacity to address multi-text consolidation challenges and their limitations."}}
{"id": "uo_ADyaGiU", "cdate": 1672531200000, "mdate": 1703284893919, "content": {"title": "A Comprehensive Evaluation of Tool-Assisted Generation Strategies", "abstract": ""}}
{"id": "ujx8bFM9SA", "cdate": 1672531200000, "mdate": 1703284893871, "content": {"title": "Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering", "abstract": ""}}
{"id": "iRzxgMnX7Q", "cdate": 1672531200000, "mdate": 1702920553359, "content": {"title": "The Curious Case of Hallucinatory Unanswerablity: Finding Truths in the Hidden States of Over-Confident Large Language Models", "abstract": "Large language models (LLMs) have been shown to possess impressive capabilities, while also raising crucial concerns about the faithfulness of their responses. A primary issue arising in this context is the management of (un)answerable queries by LLMs, which often results in hallucinatory behavior due to overconfidence. In this paper, we explore the behavior of LLMs when presented with (un)answerable queries. We ask: do models represent the fact that the question is (un)answerable when generating a hallucinatory answer? Our results show strong indications that such models encode the answerability of an input query, with the representation of the first decoded token often being a strong indicator. These findings shed new light on the spatial organization within the latent representations of LLMs, unveiling previously unexplored facets of these models. Moreover, they pave the way for the development of improved decoding techniques with better adherence to factual generation, particularly in scenarios where query (un)answerability is a concern."}}
{"id": "gRR5DfYC86", "cdate": 1672531200000, "mdate": 1703284893856, "content": {"title": "Utilizing Perturbation of Atoms' Positions for Equivariant Pre-Training in 3D Molecular Analysis", "abstract": "Over the past few years, a number of Graph Neural Network (GNN) architectures have been effectively employed for molecular analysis. However, generating annotated molecular data usually requires molecular dynamics or quantum chemistry calculations, which can be extremely time-consuming. To address this challenge, we introduce a predictive equivariant self-supervision technique that is founded on perturbing the 3D positions of the atoms. This method is ideal for 3D molecular data and allows the network to initially learn general structural information before fine-tuning it for specific tasks. We demonstrate that these pre-training procedures can also be utilized to fine-tune the network for learning molecular properties on a different dataset. Our pre-training method is demonstrated to surpass previously proposed solutions via extensive experiments on different standard molecular datasets."}}
{"id": "bmw90IgVlI", "cdate": 1672531200000, "mdate": 1697826992187, "content": {"title": "Revisiting Sentence Union Generation as a Testbed for Text Consolidation", "abstract": ""}}
{"id": "WGjADOj5kI", "cdate": 1672531200000, "mdate": 1686748695848, "content": {"title": "Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks", "abstract": "Data contamination has become especially prevalent and challenging with the rise of models pretrained on very large, automatically-crawled corpora. For closed models, the training data becomes a trade secret, and even for open models, it is not trivial to ascertain whether a particular test instance has been compromised. Strategies such as live leaderboards with hidden answers, or using test data which is guaranteed to be unseen, are expensive and become fragile with time. Assuming that all relevant actors value clean test data and will cooperate to mitigate data contamination, what can be done? We propose three strategies that can make a difference: (1) Test data made public should be encrypted with a public key and licensed to disallow derivative distribution; (2) demand training exclusion controls from closed API holders, and protect your test data by refusing to evaluate until demands are met; (3) in case of test data based on internet text, avoid data which appears with its solution on the internet, and release the context of internet-derived data along with the data. These strategies are practical and can be effective in preventing data contamination and allowing trustworthy evaluation of models' capabilities."}}
{"id": "V2OUi8b3p9", "cdate": 1672531200000, "mdate": 1686970342107, "content": {"title": "Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering", "abstract": "The integration of multi-document pre-training objectives into language models has resulted in remarkable improvements in multi-document downstream tasks. In this work, we propose extending this idea by pre-training a generic multi-document model from a novel cross-document question answering pre-training objective. To that end, given a set (or cluster) of topically-related documents, we systematically generate semantically-oriented questions from a salient sentence in one document and challenge the model, during pre-training, to answer these questions while \"peeking\" into other topically-related documents. In a similar manner, the model is also challenged to recover the sentence from which the question was generated, again while leveraging cross-document information. This novel multi-document QA formulation directs the model to better recover cross-text informational relations, and introduces a natural augmentation that artificially increases the pre-training data. Further, unlike prior multi-document models that focus on either classification or summarization tasks, our pre-training objective formulation enables the model to perform tasks that involve both short text generation (e.g., QA) and long text generation (e.g., summarization). Following this scheme, we pre-train our model -- termed QAmden -- and evaluate its performance across several multi-document tasks, including multi-document QA, summarization, and query-focused summarization, yielding improvements of up to 7%, and significantly outperforms zero-shot GPT-3.5 and GPT-4."}}
