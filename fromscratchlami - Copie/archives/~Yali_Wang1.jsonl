{"id": "1NZmJYjesf", "cdate": 1699691170158, "mdate": 1699691170158, "content": {"title": "HTML: Hybrid Temporal-scale Multimodal Learning framework for Referring Video Object Segmentation", "abstract": "Referring Video Object Segmentation (RVOS) is to segment the object instance from a given video, according to the textual description of this object. However, in the open world, the object descriptions are often diversified in contents and flexible in lengths. This leads to the key difficulty in RVOS, i.e., various descriptions of different objects are corresponding to different temporal scales in the video, which is ignored by most existing approaches with single stride of frame sampling. To tackle this problem, we propose a concise Hybrid Temporal-scale Multimodal Learning (HTML). framework, which can effectively align lingual and visual features to discover core object semantics in the video, by learning multimodal interaction hierarchically from different temporal scales. More specifically, we introduce a novel inter-scale multimodal perception module, where the language queries dynamically interact with visual features across temporal scales. It can effectively reduce complex object confusion by passing video context among different scales. Finally, we conduct extensive experiments on the widely used benchmarks, including Ref-Youtube-VOS, Ref-DAVIS17, A2D-Sentences and JHMDB-Sentences, where our HTML achieves state-of-the-art performance on all these datasets."}}
{"id": "mkNRGk-w96", "cdate": 1698556966661, "mdate": 1698556966661, "content": {"title": "Learning dynamical human-joint affinity for 3d pose estimation in videos", "abstract": "Graph Convolution Network (GCN) has been successfully used for 3D human pose estimation in videos. However, it is often built on the fixed human-joint affinity, according to human skeleton. This may reduce adaptation capacity of GCN to tackle complex spatio-temporal pose variations in videos. To alleviate this problem, we propose a novel Dynamical Graph Network (DG-Net), which can dynamically identify human-joint affinity, and estimate 3D pose by adaptively learning spatial/temporal joint relations from videos. Different from traditional graph convolution, we introduce Dynamical Spatial/Temporal Graph convolution (DSG/DTG) to discover spatial/temporal human-joint affinity for each video exemplar, depending on spatial distance/temporal movement similarity between human joints in this video. Hence, they can effectively understand which joints are spatially closer and/or have consistent motion."}}
{"id": "kzC-_4hE4P", "cdate": 1688169600000, "mdate": 1682317933639, "content": {"title": "Hybrid token transformer for deep face recognition", "abstract": ""}}
{"id": "UTsTisA3OX", "cdate": 1672531200000, "mdate": 1682317933574, "content": {"title": "Unmasked Teacher: Towards Training-Efficient Video Foundation Models", "abstract": "Video Foundation Models (VFMs) have received limited exploration due to high computational costs and data scarcity. Previous VFMs rely on Image Foundation Models (IFMs), which face challenges in transferring to the video domain. Although VideoMAE has trained a robust ViT from limited data, its low-level reconstruction poses convergence difficulties and conflicts with high-level cross-modal alignment. This paper proposes a training-efficient method for temporal-sensitive VFMs that integrates the benefits of existing methods. To increase data efficiency, we mask out most of the low-semantics video tokens, but selectively align the unmasked tokens with IFM, which serves as the UnMasked Teacher (UMT). By providing semantic guidance, our method enables faster convergence and multimodal friendliness. With a progressive pre-training framework, our model can handle various tasks including scene-related, temporal-related, and complex video-language understanding. Using only public sources for pre-training in 6 days on 32 A100 GPUs, our scratch-built ViT-L/16 achieves state-of-the-art performances on various video tasks. The code and models will be released at https://github.com/OpenGVLab/unmasked_teacher."}}
{"id": "Hb0sFE0RhxK", "cdate": 1668975712975, "mdate": 1668975712975, "content": {"title": "Temporal hallucinating for action recognition with few still images", "abstract": "Action recognition in still images has been recently promoted by deep learning. However, the success of these deep\nmodels heavily depends on huge amount of training images\nfor various action categories, which may not be available\nin practice. Alternatively, humans can classify new action\ncategories after seeing few images, since we may not only\ncompare appearance similarities between images on hand,\nbut also attempt to recall importance motion cues from relevant action videos in our memory. To mimic this capacity,\nwe propose a novel Hybrid Video Memory (HVM) machine,\nwhich can hallucinate temporal features of still images from\nvideo memory, in order to boost action recognition with few\nstill images. First, we design a temporal memory module\nconsisting of temporal hallucinating and predicting. Temporal hallucinating can generate temporal features of still\nimages in an unsupervised manner. Hence, it can be flexibly used in realistic scenarios, where image and video categories may not be consistent. Temporal predicting can\neffectively infer action categories for query image, by integrating temporal features of training images and videos\nwithin a domain-adaptation manner. Second, we design a\nspatial memory module for spatial predicting. As spatial\nand temporal features are complementary to represent different actions, we apply spatial-temporal prediction fusion\nto further boost performance. Finally, we design a video\nselection module to select strongly-relevant videos as memory. In this case, we can balance the number of images and\nvideos to reduce prediction bias as well as preserve computation efficiency. To show the effectiveness, we conduct\nextensive experiments on three challenging data sets, where\nour HVM outperforms a number of recent approaches by\ntemporal hallucinating from video memory."}}
{"id": "OdqFfrk0xrv", "cdate": 1668975618195, "mdate": 1668975618195, "content": {"title": "Adaptive pyramid context network for semantic segmentation", "abstract": "Recent studies witnessed that context features can significantly improve the performance of deep semantic segmentation networks. Current context based segmentation methods differ with each other in how to construct context features and perform differently in practice. This paper firstly\nintroduces three desirable properties of context features in\nsegmentation task. Specially, we find that Global-guided\nLocal Affinity (GLA) can play a vital role in constructing effective context features, while this property has been largely\nignored in previous works. Based on this analysis, this paper proposes Adaptive Pyramid Context Network (APCNet)\nfor semantic segmentation. APCNet adaptively constructs\nmulti-scale contextual representations with multiple welldesigned Adaptive Context Modules (ACMs). Specifically,\neach ACM leverages a global image representation as a\nguidance to estimate the local affinity coefficients for each\nsub-region, and then calculates a context vector with these\naffinities. We empirically evaluate our APCNet on three semantic segmentation and scene parsing datasets, including\nPASCAL VOC 2012, Pascal-Context, and ADE20K dataset.\nExperimental results show that APCNet achieves state-ofthe-art performance on all three benchmarks, and obtains\na new record 84.2% on PASCAL VOC 2012 test set without\nMS COCO pre-trained and any post-processing."}}
{"id": "H57TZVM1hnZ", "cdate": 1668101474723, "mdate": 1668101474723, "content": {"title": "DID: Disentangling-Imprinting-Distilling for Continuous Low-Shot Detection", "abstract": "Practical applications often face a challenging continuous low-shot detection scenario, where a target detection task only has a few annotated training images, and a number of such new tasks come in sequence. To address this challenge, we propose a generic detection scheme via Disentangling-Imprinting-Distilling (DID). DID can leverage delicate transfer insights into the main development flow of deep learning, i.e., architecture design (Disentangling), model initialization (Imprinting), and training methodology (Distilling). This allows DID to be a simple but effective solution for continuous low-shot detection. In addition, DID can integrate the supervision from different detection tasks into a progressive learning procedure. As a result, one can efficiently adapt the previous detector for a new low-shot task, while maintaining the learned detection knowledge in the history. Finally, we evaluate our DID on a number of challenging settings in continuous/incremental low-shot detection. All the results demonstrate that our DID outperforms the recent state-of-the-art approaches. The code and models are available at https://github.com/chenxy99/DID."}}
{"id": "eUjbwGCI6C", "cdate": 1668101327782, "mdate": 1668101327782, "content": {"title": "Context-transformer: tackling object confusion for few-shot detection", "abstract": "Few-shot object detection is a challenging but realistic scenario, where only a few annotated training images are available for training detectors. A popular approach to handle this problem is transfer learning, ie, fine-tuning a detector pretrained on a source-domain benchmark. However, such transferred detector often fails to recognize new objects in the target domain, due to low data diversity of training samples. To tackle this problem, we propose a novel Context-Transformer within a concise deep transfer framework. Specifically, Context-Transformer can effectively leverage source-domain object knowledge as guidance, and automatically exploit contexts from only a few training images in the target domain. Subsequently, it can adaptively integrate these relational clues to enhance the discriminative power of detector, in order to reduce object confusion in few-shot scenarios. Moreover, Context-Transformer is flexibly embedded in the popular SSD-style detectors, which makes it a plug-and-play module for end-to-end few-shot learning. Finally, we evaluate Context-Transformer on the challenging settings of few-shot detection and incremental few-shot detection. The experimental results show that, our framework outperforms the recent state-of-the-art approaches."}}
{"id": "gj34vad3-mo", "cdate": 1668024466435, "mdate": 1668024466435, "content": {"title": "PC-HMR: Pose Calibration for 3D Human Mesh Recovery from 2D Images/Videos", "abstract": "The end-to-end Human Mesh Recovery (HMR) approach has been successfully used for 3D body reconstruction. However, most HMR-based frameworks reconstruct human body by directly learning mesh parameters from images or videos, while lacking explicit guidance of 3D human pose in visual data. As a result, the generated mesh often exhibits incorrect pose for complex activities. To tackle this problem, we propose to exploit 3D pose to calibrate human mesh. Specifically, we develop two novel Pose Calibration frameworks, i.e., Serial PC-HMR and Parallel PC-HMR. By coupling advanced 3D pose estimators and HMR in a serial or parallel manner, these two frameworks can effectively correct human mesh with guidance of a concise pose calibration module. Furthermore, since the calibration module is designed via non-rigid pose transformation, our PC-HMR frameworks can flexibly tackle bone length variations to alleviate misplacement in the calibrated mesh. Finally, our frameworks are based on generic and complementary integration of data-driven learning and geometrical modeling. Via plug-and-play modules, they can be efficiently adapted for both image/video-based human mesh recovery. Additionally, they have no requirement of extra 3D pose annotations in the testing phase, which releases inference difficulties in practice. We perform extensive experiments on the popular bench-marks, i.e., Human3.6M, 3DPW and SURREAL, where our PC-HMR frameworks achieve the SOTA results"}}
{"id": "d77RVuVg-Mf", "cdate": 1663850099744, "mdate": null, "content": {"title": "UniFormerV2: Spatiotemporal Learning by Arming Image ViTs with Video UniFormer", "abstract": "Learning discriminative spatiotemporal representation is the key problem of video understanding. Recently, Vision Transformers (ViTs) have shown their power in learning long-term video dependency with self-attention. Unfortunately, they exhibit limitations in tackling local video redundancy, due to the blind global comparison among tokens. UniFormer has successfully alleviated this issue, by unifying convolution and self-attention as a relation aggregator in the transformer format. However, this model has to require a tiresome and complicated image-pretraining phrase, before being finetuned on videos. This blocks its wide usage in practice. On the contrary, open-sourced ViTs are readily available and well-pretrained with rich image supervision. Based on these observations, we propose a generic paradigm to build a powerful family of video networks, by arming the pretrained ViTs with efficient UniFormer designs. We call this family UniFormerV2, since it inherits the concise style of the UniFormer block. But it contains brand-new local and global relation aggregators, which allow for preferable accuracy-computation balance by seamlessly integrating advantages from both ViTs and UniFormer. Without any bells and whistles, our UniFormerV2 gets the state-of-the-art recognition performance on 8 popular video benchmarks, including scene-related Kinetics-400/600/700 and Moments in Time, temporal-related Something-Something V1/V2, untrimmed ActivityNet and HACS. In particular, it is the first model to achieve 90% top-1 accuracy on Kinetics-400, to our best knowledge. The models will be released afterward."}}
