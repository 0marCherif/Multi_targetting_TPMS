{"id": "UoIqUZfbbq", "cdate": 1696328529220, "mdate": 1696328529220, "content": {"title": "Weisfeiler and Leman go Hyperbolic: Learning Distance Preserving Node Representations", "abstract": "In recent years, graph neural networks (GNNs)\nhave emerged as a promising tool for solving\nmachine learning problems on graphs. Most\nGNNs are members of the family of message\npassing neural networks (MPNNs). There is a\nclose connection between these models and the\nWeisfeiler-Leman (WL) test of isomorphism, an\nalgorithm that can successfully test isomorphism\nfor a broad class of graphs. Recently, much research has focused on measuring the expressive\npower of GNNs. For instance, it has been shown\nthat standard MPNNs are at most as powerful as\nWL in terms of distinguishing non-isomorphic\ngraphs. However, these studies have largely ignored the distances between the representations\nof nodes/graphs which are of paramount importance for learning tasks. In this paper, we define\na distance function between nodes that is based\non the hierarchy produced by the WL algorithm\nand propose a model that learns representations\nwhich preserve those distances between nodes.\nSince the emerging hierarchy corresponds to a\ntree, to learn these representations, we capitalize\non recent advances in the field of hyperbolic neural networks. We empirically evaluate the proposed model on standard node and graph classification datasets where it achieves competitive\nperformance with state-of-the-art models."}}
{"id": "JTGgwGU00R2", "cdate": 1683891044632, "mdate": 1683891044632, "content": {"title": "Time Series Forecasting Models Copy the Past: How to Mitigate", "abstract": "Time series forecasting is at the core of important application domains posing significant challenges to machine learning algorithms.\nRecently neural network architectures have been widely applied to the problem of time series forecasting. Most of these models are trained by minimizing a loss function that measures predictions' deviation from the real values. Typical loss functions include mean squared error (MSE) and mean absolute error (MAE). In the presence of noise and uncertainty, neural network models tend to replicate the last observed value of the time series, thus limiting their applicability to real-world data. In this paper, we provide a formal definition of the above problem and we also give some examples of forecasts where the problem is observed. We also propose a regularization term penalizing the replication of previously seen values. We evaluate the proposed regularization term both on synthetic and real-world datasets. Our results indicate that the regularization term mitigates to some extent the aforementioned problem and gives rise to more robust models."}}
{"id": "YAX065LUF6K", "cdate": 1680307200000, "mdate": 1681724232762, "content": {"title": "Permute Me Softly: Learning Soft Permutations for Graph Representations", "abstract": "Graph neural networks (GNNs) have recently emerged as a dominant paradigm for machine learning with graphs. Research on GNNs has mainly focused on the family of message passing neural networks (MPNNs). Similar to the Weisfeiler-Leman (WL) test of isomorphism, these models follow an iterative neighborhood aggregation procedure to update vertex representations, and they next compute graph representations by aggregating the representations of the vertices. Although very successful, MPNNs have been studied intensively in the past few years. Thus, there is a need for novel architectures which will allow research in the field to break away from MPNNs. In this paper, we propose a new graph neural network model, so-called <inline-formula><tex-math notation=\"LaTeX\">$\\pi$</tex-math></inline-formula> -GNN which learns a \u201csoft\u201d permutation (i. e., doubly stochastic) matrix for each graph, and thus projects all graphs into a common vector space. The learned matrices impose a \u201csoft\u201d ordering on the vertices of the input graphs, and based on this ordering, the adjacency matrices are mapped into vectors. These vectors can be fed into fully-connected or convolutional layers to deal with supervised learning tasks. In case of large graphs, to make the model more efficient in terms of running time and memory, we further relax the doubly stochastic matrices to row stochastic matrices. We empirically evaluate the model on graph classification and graph regression datasets and show that it achieves performance competitive with state-of-the-art models."}}
{"id": "nA1jpyRgqp6", "cdate": 1672531200000, "mdate": 1687425918820, "content": {"title": "Graph Alignment Kernels using Weisfeiler and Leman Hierarchies", "abstract": "Graph kernels have become a standard approach for tackling the graph similarity and learning tasks at the same time. Most graph kernels proposed so far are instances of the R-convolution framework...."}}
{"id": "bClcw79mIHS", "cdate": 1672531200000, "mdate": 1687425918831, "content": {"title": "Geometric Random Walk Graph Neural Networks via Implicit Layers", "abstract": "Graph neural networks have recently attracted a lot of attention and have been applied with great success to several important graph problems. The Random Walk Graph Neural Network model was recentl..."}}
{"id": "Rar9fRmrNOj", "cdate": 1672531200000, "mdate": 1687425918802, "content": {"title": "Weisfeiler and Leman go Hyperbolic: Learning Distance Preserving Node Representations", "abstract": "In recent years, graph neural networks (GNNs) have emerged as a promising tool for solving machine learning problems on graphs. Most GNNs are members of the family of message passing neural network..."}}
{"id": "_BjtIlib8N9", "cdate": 1664248834668, "mdate": null, "content": {"title": "Structure-Aware Antibiotic Resistance Classification using Graph Neural Networks", "abstract": "Antibiotics are traditionally used to treat bacterial infections. However, bacteria can develop immunity to drugs, making them ineffective and thus posing a serious threat to global health. Identifying and classifying the genes responsible for this resistance is critical for the prevention, diagnosis, and treatment of infections as well as the understanding of its mechanisms. Previous methods developed for this purpose have mostly been sequence-based, relying on comparisons to existing databases or machine learning models trained on sequence features. However, genes with comparable functions may not always have similar sequences. As a result, in this paper, we develop a deep learning model that uses the protein structure as a complement to the sequence to classify novel ARGs (antibiotic resistant genes), which we expect to provide more useful information than the sequence alone. The proposed approach consists of two steps. First, we capitalize on the celebrated AlphaFold model to predict the 3D structure of a protein from its amino acid sequence. Then, we process the sequence using a transformers-based language model while we also apply a graph neural network to the graph extracted from the structure. We evaluate the proposed architecture on a standard benchmark dataset where it outperforms state-of-the-art methods. "}}
{"id": "gkmSDSwzDI", "cdate": 1640995200000, "mdate": 1687425918910, "content": {"title": "Mass Enhanced Node Embeddings for Drug Repurposing", "abstract": "Graph representation learning has recently emerged as a promising approach to solve pharmacological tasks by modeling biological networks. Among the different tasks, drug repurposing, the task of identifying new uses for approved or investigational drugs, has attracted a lot of attention recently. In this work, we propose a node embedding algorithm for the problem of drug repurposing. The proposed algorithm learns node representations that capture the influence of nodes in the biological network by learning a mass term for each node along with its embedding. We apply the proposed algorithm to a multiscale interactome network and embed its nodes (i.\u00a0e., proteins, drugs, diseases and biological functions) into a low-dimensional space. We evaluate the generated embeddings in the drug repurposing task. Our experiments show that the proposed approach outperforms the baselines and offers an improvement of 53.33% in average precision over typical walk-based embedding approaches."}}
{"id": "R6fURZ4hn_6", "cdate": 1640995200000, "mdate": 1687425918845, "content": {"title": "Time Series Forecasting Models Copy the Past: How to Mitigate", "abstract": "Time series forecasting is at the core of important application domains posing significant challenges to machine learning algorithms. Recently neural network architectures have been widely applied to the problem of time series forecasting. Most of these models are trained by minimizing a loss function that measures predictions\u2019 deviation from the real values. Typical loss functions include mean squared error (MSE) and mean absolute error (MAE). In the presence of noise and uncertainty, neural network models tend to replicate the last observed value of the time series, thus limiting their applicability to real-world data. In this paper, we provide a formal definition of the above problem and we also give some examples of forecasts where the problem is observed. We also propose a regularization term penalizing the replication of previously seen values. We evaluate the proposed regularization term both on synthetic and real-world datasets. Our results indicate that the regularization term mitigates to some extent the aforementioned problem and gives rise to more robust models."}}
{"id": "eV5d4I3eso", "cdate": 1632875754291, "mdate": null, "content": {"title": "Geometric Random Walk Graph Neural Networks via Implicit Layers", "abstract": "Graph neural networks have recently attracted a lot of attention and have been applied with great success to several important graph problems. The Random Walk Graph Neural Network model was recently proposed as a more intuitive alternative to the well-studied family of message passing neural networks. This model compares each input graph against a set of latent ``hidden graphs'' using a kernel that counts common random walks up to some length. In this paper, we propose a new architecture, called Geometric Random Walk Graph Neural Network (GRWNN), that generalizes the above model such that it can count common walks of infinite length in two graphs. The proposed model retains the transparency of Random Walk Graph Neural Networks since its first layer also consists of a number of trainable ``hidden graphs'' which are compared against the input graphs using the geometric random walk kernel. To compute the kernel, we employ a fixed-point iteration approach involving implicitly defined operations. Then, we capitalize on implicit differentiation to derive an efficient training scheme which requires only constant memory, regardless of the number of fixed-point iterations. The employed random walk kernel is differentiable, and therefore, the proposed model is end-to-end trainable. Experiments on standard graph classification datasets demonstrate the effectiveness of the proposed approach in comparison with state-of-the-art methods."}}
