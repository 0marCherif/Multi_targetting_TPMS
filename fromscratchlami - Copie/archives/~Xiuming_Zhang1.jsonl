{"id": "-czfPxSrhrS", "cdate": 1699224915145, "mdate": null, "content": {"title": "SunStage: Portrait Reconstruction and Relighting using the Sun as a Light Stage", "abstract": "A light stage uses a series of calibrated cameras and lights to capture a subject's facial appearance under varying illumination and viewpoint. This captured information is crucial for facial reconstruction and relighting. Unfortunately, light stages are often inaccessible: they are expensive and require significant technical expertise for construction and operation. In this paper, we present SunStage: a lightweight alternative to a light stage that captures comparable data using only a smartphone camera and the sun. Our method only requires the user to capture a selfie video outdoors, rotating in place, and uses the varying angles between the sun and the face as guidance in joint reconstruction of facial geometry, reflectance, camera pose, and lighting parameters. Despite the in-the-wild un-calibrated setting, our approach is able to reconstruct detailed facial appearance and geometry, enabling compelling effects such as relighting, novel view synthesis, and reflectance editing."}}
{"id": "eJEu3pCNnz", "cdate": 1699224822137, "mdate": null, "content": {"title": "DiffusionRig: Learning Personalized Priors for Facial Appearance Editing", "abstract": "We address the problem of learning person-specific facial priors from a small number (e.g., 20) of portrait photos of the same person. This enables us to edit this specific person's facial appearance, such as expression and lighting, while preserving their identity and high-frequency facial details. Key to our approach, which we dub DiffusionRig, is a diffusion model conditioned on, or \"rigged by,\" crude 3D face models estimated from single in-the-wild images by an off-the-shelf estimator. On a high level, DiffusionRig learns to map simplistic renderings of 3D face models to realistic photos of a given person. Specifically, DiffusionRig is trained in two stages: It first learns generic facial priors from a large-scale face dataset and then person-specific priors from a small portrait photo collection of the person of interest. By learning the CGI-to-photo mapping with such personalized priors, DiffusionRig can \"rig\" the lighting, facial expression, head pose, etc. of a portrait photo, conditioned only on coarse 3D models while preserving this person's identity and other high-frequency characteristics. Qualitative and quantitative experiments show that DiffusionRig outperforms existing approaches in both identity preservation and photorealism."}}
{"id": "Gq_UWk9uX0_", "cdate": 1672531200000, "mdate": 1682434061360, "content": {"title": "DiffusionRig: Learning Personalized Priors for Facial Appearance Editing", "abstract": "We address the problem of learning person-specific facial priors from a small number (e.g., 20) of portrait photos of the same person. This enables us to edit this specific person's facial appearance, such as expression and lighting, while preserving their identity and high-frequency facial details. Key to our approach, which we dub DiffusionRig, is a diffusion model conditioned on, or \"rigged by,\" crude 3D face models estimated from single in-the-wild images by an off-the-shelf estimator. On a high level, DiffusionRig learns to map simplistic renderings of 3D face models to realistic photos of a given person. Specifically, DiffusionRig is trained in two stages: It first learns generic facial priors from a large-scale face dataset and then person-specific priors from a small portrait photo collection of the person of interest. By learning the CGI-to-photo mapping with such personalized priors, DiffusionRig can \"rig\" the lighting, facial expression, head pose, etc. of a portrait photo, conditioned only on coarse 3D models while preserving this person's identity and other high-frequency characteristics. Qualitative and quantitative experiments show that DiffusionRig outperforms existing approaches in both identity preservation and photorealism. Please see the project website: https://diffusionrig.github.io for the supplemental material, video, code, and data."}}
{"id": "OsNsL5XO_ZP", "cdate": 1667355002876, "mdate": 1667355002876, "content": {"title": "Portrait Shadow Manipulation", "abstract": "Casually-taken portrait photographs often suffer from unflattering lighting and shadowing because of suboptimal conditions in the environment. Aesthetic qualities such as the position and softness of shadows and the lighting ratio between the bright and dark parts of the face are frequently determined by the constraints of the environment rather than by the photographer. Professionals address this issue by adding light shaping tools such as scrims, bounce cards, and flashes. In this paper, we present a computational approach that gives casual photographers some of this control, thereby allowing poorly-lit portraits to be relit post-capture in a realistic and easily controllable way. Our approach relies on a pair of neural networks\u2014one to remove foreign shadows cast by external objects, and another to soften facial shadows cast by the features of the subject and to add a synthetic fill light to improve the lighting ratio. To train our first network we construct a dataset of real-world portraits wherein synthetic foreign shadows are rendered onto the face, and we show that our network learns to remove those unwanted shadows. To train our second network we use a dataset of Light Stage scans of human subjects to construct input/output pairs of input images harshly lit by a small light source, and variably softened and fill-lit output images of each face. We propose a way to explicitly encode facial symmetry and show that our dataset and training procedure enable the model to generalize\nto images taken in the wild. Together, these networks enable the realistic and aesthetically pleasing enhancement of shadows and lights in real-world portrait images."}}
{"id": "vc2F6yH93d", "cdate": 1640995200000, "mdate": 1666597559106, "content": {"title": "SunStage: Portrait Reconstruction and Relighting using the Sun as a Light Stage", "abstract": "Outdoor portrait photographs are often marred by the harsh shadows cast under direct sunlight. To resolve this, one can use post-capture lighting manipulation techniques, but these methods either require complex hardware (e.g., a light stage) to capture each individual, or rely on image-based priors and thus fail to reconstruct many of the subtle facial details that vary from person to person. In this paper, we present SunStage, a system for accurate, individually-tailored, and lightweight reconstruction of facial geometry and reflectance that can be used for general portrait relighting with cast shadows. Our method only requires the user to capture a selfie video outdoors, rotating in place, and uses the varying angles between the sun and the face as constraints in the joint reconstruction of facial geometry, reflectance properties, and lighting parameters. Aside from relighting, we show that our reconstruction can be used for applications like reflectance editing and view synthesis. Results and interactive demos are available at https://grail.cs.washington.edu/projects/sunstage/."}}
{"id": "iUILcL65UQ_", "cdate": 1609459200000, "mdate": 1663098207237, "content": {"title": "NeRFactor: Neural Factorization of Shape and Reflectance Under an Unknown Illumination", "abstract": "We address the problem of recovering the shape and spatially-varying reflectance of an object from multi-view images (and their camera poses) of an object illuminated by one unknown lighting condition. This enables the rendering of novel views of the object under arbitrary environment lighting and editing of the object's material properties. The key to our approach, which we call Neural Radiance Factorization (NeRFactor), is to distill the volumetric geometry of a Neural Radiance Field (NeRF) [Mildenhall et al. 2020] representation of the object into a surface representation and then jointly refine the geometry while solving for the spatially-varying reflectance and environment lighting. Specifically, NeRFactor recovers 3D neural fields of surface normals, light visibility, albedo, and Bidirectional Reflectance Distribution Functions (BRDFs) without any supervision, using only a re-rendering loss, simple smoothness priors, and a data-driven BRDF prior learned from real-world BRDF measurements. By explicitly modeling light visibility, NeRFactor is able to separate shadows from albedo and synthesize realistic soft or hard shadows under arbitrary lighting conditions. NeRFactor is able to recover convincing 3D models for free-viewpoint relighting in this challenging and underconstrained capture setup for both synthetic and real scenes. Qualitative and quantitative experiments show that NeRFactor outperforms classic and deep learning-based state of the art across various tasks. Our videos, code, and data are available at people.csail.mit.edu/xiuming/projects/nerfactor/."}}
{"id": "hH13uXmPnC_", "cdate": 1609459200000, "mdate": 1663188949736, "content": {"title": "Editing Conditional Radiance Fields", "abstract": "A neural radiance field (NeRF) is a scene model supporting high-quality view synthesis, optimized per scene. In this paper, we explore enabling user editing of a category-level NeRF \u2013 also known as a conditional radiance field \u2013 trained on a shape category. Specifically, we introduce a method for propagating coarse 2D user scribbles to the 3D space, to modify the color or shape of a local region. First, we propose a conditional radiance field that incorporates new modular network components, including a shape branch that is shared across object instances. Observing multiple instances of the same category, our model learns underlying part semantics without any supervision, thereby allowing the propagation of coarse 2D user scribbles to the entire 3D region (e.g., chair seat). Next, we propose a hybrid network update strategy that targets specific network components, which balances efficiency and accuracy. During user interaction, we formulate an optimization problem that both satisfies the user\u2019s constraints and preserves the original object structure. We demonstrate our editing approach on rendered views of three shape datasets and show that it outperforms prior neural editing approaches. Finally, we edit the appearance and shape of a single-view real photograph and show that the edit propagates to extrapolated novel views."}}
{"id": "avP7XdPAqNH", "cdate": 1609459200000, "mdate": 1668510921538, "content": {"title": "NeRFactor: neural factorization of shape and reflectance under an unknown illumination", "abstract": "We address the problem of recovering the shape and spatially-varying reflectance of an object from multi-view images (and their camera poses) of an object illuminated by one unknown lighting condition. This enables the rendering of novel views of the object under arbitrary environment lighting and editing of the object's material properties. The key to our approach, which we call Neural Radiance Factorization (NeRFactor), is to distill the volumetric geometry of a Neural Radiance Field (NeRF) [Mildenhall et al. 2020] representation of the object into a surface representation and then jointly refine the geometry while solving for the spatially-varying reflectance and environment lighting. Specifically, NeRFactor recovers 3D neural fields of surface normals, light visibility, albedo, and Bidirectional Reflectance Distribution Functions (BRDFs) without any supervision, using only a re-rendering loss, simple smoothness priors, and a data-driven BRDF prior learned from real-world BRDF measurements. By explicitly modeling light visibility, NeRFactor is able to separate shadows from albedo and synthesize realistic soft or hard shadows under arbitrary lighting conditions. NeRFactor is able to recover convincing 3D models for free-viewpoint relighting in this challenging and underconstrained capture setup for both synthetic and real scenes. Qualitative and quantitative experiments show that NeRFactor outperforms classic and deep learning-based state of the art across various tasks. Our videos, code, and data are available at people.csail.mit.edu/xiuming/projects/nerfactor/."}}
{"id": "a_cCExetE6", "cdate": 1609459200000, "mdate": 1671993567317, "content": {"title": "Shape, Reflectance, and Illumination From Appearance", "abstract": "The image formation process describes how light interacts with the objects in a scene and eventually reaches the camera, forming an image that we observe. Inverting this process is a long-standing, ill-posed problem in computer vision, which involves estimating shape, material properties, and/or illumination passively from the object\u2019s appearance. Such \u201cinverse rendering\u201d capabilities enable 3D understanding of our world (as desired in autonomous driving, robotics, etc.) and computer graphics applications such as relighting, view synthesis, and object capture (as desired in extended reality [XR], etc.). In this dissertation, we study inverse rendering by recovering three-dimensional (3D) shape, reflectance, illumination, or everything jointly under different setups. The input in these different setups varies from single images to multi-view images lit by multiple known lighting conditions, then to multi-view images under one unknown illumination. Across the setups, we explore optimization-based recovery that exploits multiple observations of the same object, learning-based reconstruction that heavily relies on data-driven priors, and a mixture of both. Depending on the target application, we perform inverse rendering at three different levels of decomposition: I) At a low level of abstraction, we develop physically-based models that explicitly solve for every term in the rendering equation, II) at a middle level, we utilize the light transport function to abstract away intermediate light bounces and model only the final \u201cnet effect\u201d, and III) at a high level, we treat rendering as a black box and directly invert it with learned data-driven priors. We also demonstrate how higherlevel abstraction leads to models that are simple and applicable to single images but also possess fewer capabilities. This dissertation discusses four instances of inverse rendering, gradually ascending in the level of abstraction. In the first instance, we focus on the low-level abstraction where we decompose appearance explicitly into shape, reflectance, and illumination. To this end, we present a physically-based model capable of such full factorization under one unknown illumination and another that handles one-bounce indirect illumination. In the second instance, we ascend to the middle level of abstraction, at which we model appearance with the light transport function, demonstrating how this level of modeling easily supports relighting with global illumination, view synthesis, and both tasks simultaneously. Finally, at the high level of abstraction, we employ deep learning to directly invert the rendering black box in a data-driven fashion. Specifically, in the third instance, we recover 3D shapes from single images by learning data-driven shape priors and further make our reconstruction generalizable to novel shape classes unseen during training. Also relying on data-driven priors, the fourth instance concerns how to recover lighting from the appearance of the illuminated object, without explicitly modeling the image formation process."}}
{"id": "SrnxoSXzdSr", "cdate": 1609459200000, "mdate": 1663188949742, "content": {"title": "Editing Conditional Radiance Fields", "abstract": "A neural radiance field (NeRF) is a scene model supporting high-quality view synthesis, optimized per scene. In this paper, we explore enabling user editing of a category-level NeRF - also known as a conditional radiance field - trained on a shape category. Specifically, we introduce a method for propagating coarse 2D user scribbles to the 3D space, to modify the color or shape of a local region. First, we propose a conditional radiance field that incorporates new modular network components, including a shape branch that is shared across object instances. Observing multiple instances of the same category, our model learns underlying part semantics without any supervision, thereby allowing the propagation of coarse 2D user scribbles to the entire 3D region (e.g., chair seat). Next, we propose a hybrid network update strategy that targets specific network components, which balances efficiency and accuracy. During user interaction, we formulate an optimization problem that both satisfies the user's constraints and preserves the original object structure. We demonstrate our approach on various editing tasks over three shape datasets and show that it outperforms prior neural editing approaches. Finally, we edit the appearance and shape of a real photograph and show that the edit propagates to extrapolated novel views."}}
