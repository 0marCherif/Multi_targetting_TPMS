{"id": "Mcf9q1pCHx", "cdate": 1676827107187, "mdate": null, "content": {"title": "Greed is good: correspondence recovery for unlabeled linear regression", "abstract": "We consider the unlabeled linear regression reading as $\\mathbf{Y} = \\mathbf{\\Pi}^{*}\\mathbf{X}\\mathbf{B}^* + \\mathbf{W}$, where $\\mathbf{\\Pi}^{*}, \\mathbf{B}^*$ and $\\mathbf{W}$ represents missing (or incomplete) correspondence information, signals, and additive noise, respectively. Our goal is to perform data alignment between $\\mathbf{Y}$ and $\\mathbf{X}$, or equivalently, reconstruct the correspondence information encoded by $\\mathbf{\\Pi}^*$. Based on whether signal $\\mathbf{B}^*$ is given a prior, we separately propose two greedy-selection based estimators, which both reach the mini-max optimality. Compared with previous works, our work $(i)$ supports partial recovery of the correspondence information; and $(ii)$ applies to a general matrix family rather than the permutation matrices, to put more specifically, selection matrices, where multiple rows of $\\mathbf{X}$ can correspond to the same row in $\\mathbf{Y}$. Moreover, numerical experiments are provided to corroborate our claims."}}
{"id": "Q06wKxnHRv", "cdate": 1676827084059, "mdate": null, "content": {"title": "Fed-LAMB: Layer-wise and Dimension-wise Locally Adaptive Federated Learning", "abstract": "In the emerging paradigm of Federated Learning (FL), large amount of clients such as mobile devices are used to train possibly high-dimensional models on their respective data. Combining (dimension-wise) adaptive gradient methods (e.g., Adam, AMSGrad)\nwith FL has been an active direction, which is shown to outperform traditional SGD based FL in many cases. In this paper, we focus on the problem of training federated deep neural networks, and propose a novel FL framework which further introduces layer-wise adaptivity to the local model updates to accelerate the convergence of adaptive FL methods. Our framework includes two variants based on two recent locally adaptive federated learning algorithms. Theoretically, we provide a convergence analysis of our layer-wise FL methods, coined Fed-LAMB and Mime-LAMB, which match the convergence rate of state-of-the-art results in adaptive FL and exhibits linear speedup in terms of the number of workers. Experimental results on various datasets and models, under both IID and non-IID local data settings, show that both Fed-LAMB and Mime-LAMB achieve faster convergence speed and better generalization performance, compared to various recent adaptive FL methods."}}
{"id": "2niSExZenB", "cdate": 1676827073353, "mdate": null, "content": {"title": "Copula for Instance-wise Feature Selection and Rank", "abstract": "Instance-wise feature selection and ranking methods can achieve a good selection of task-friendly features for each sample in the context of neural networks. However, existing approaches that assume feature subsets to be independent are imperfect when considering the dependency between features. To address this limitation, we propose to incorporate the Gaussian copula, a powerful mathematical technique for capturing correlations between variables, into the current feature selection framework with no additional changes needed. Experimental results on both synthetic and real datasets, in terms of performance comparison and interpretability, demonstrate that our method is capable of capturing meaningful correlations."}}
{"id": "-4DiyBMgv9m", "cdate": 1663850541741, "mdate": null, "content": {"title": "Identifying Phase Transition Thresholds of Permuted Linear Regression via Message Passing", "abstract": "This paper considers the permuted linear regression, i.e., ${\\mathbf{Y}} = {\\mathbf{\\Pi}}^{\\natural}{\\mathbf{X}}{\\mathbf{B}}^{\\natural} + {\\mathbf{W}}$, where ${\\mathbf{Y}} \\in \\mathbb{R}^{n\\times m}, {\\mathbf{\\Pi}}^{\\natural}\\in\\mathbb{R}^{n\\times n}, {\\mathbf{X}} \\in \\mathbb{R}^{n\\times p}, {\\mathbf{B}}^{\\natural}\\in \\mathbb{R}^{p\\times m}$, and ${\\mathbf{W}}\\in \\mathbb{R}^{n\\times m}$ represent the observations, missing (or incomplete) information about ordering, sensing matrix, signal of interests, and additive sensing noise, respectively. As is shown in the previous work, there exists phase transition phenomena in terms of the \\emph{signal-to-noise ratio} ($\\mathsf{snr}$), number of permuted rows, etc. While all existing works only concern the convergence rates without specifying the associate constants in front of them, we give a precise identification of the phase transition thresholds via the message passing algorithm. Depending on whether the signal ${\\mathbf{B}}^{\\natural}$ is known or not, we separately identify the corresponding critical points around the phase transition regimes. Moreover, we provide numerical experiments and show the empirical phase transition points are well aligned with theoretical predictions."}}
{"id": "l2vPa8gwBuA", "cdate": 1663850540503, "mdate": null, "content": {"title": "One-Step Estimator for Permuted Sparse Recovery", "abstract": "This paper considers the unlabeled sparse recovery under multiple measurements, i.e., ${\\mathbf{Y}} = {\\mathbf{\\Pi}}^{\\natural}{\\mathbf{X}} {\\mathbf{B}}^{\\natural} + {\\mathbf{W}}$, where ${\\mathbf{Y}} \\in \\mathbb{R}^{n\\times m}, {\\mathbf{\\Pi}}^{\\natural}\\in \\mathbb{R}^{n\\times n}, {\\mathbf{X}} \\in \\mathbb{R}^{n\\times p}, {\\mathbf{B}}^{\\natural}\\in \\mathbb{R}^{p\\times m}, {\\mathbf{W}}\\in \\mathbb{R}^{n\\times m}$ represents the observations, missing (or incomplete) correspondence information, sensing matrix, sparse signals, and additive sensing noise, respectively. Different from the previous works on multiple measurements ($m > 1$) which all focus on the sufficient samples regime, namely, $n > p$, we consider a sparse matrix $\\mathbf{B}^{\\natural}$ and investigate the insufficient samples regime (i.e., $n \\ll p$) for the first time. To begin with, we establish the lower bound on the sample number and \\emph{signal-to-noise ratio} (${\\mathsf{SNR}}$) for the correct permutation recovery. Moreover, we present a simple yet effective estimator. Under mild conditions, we show that our estimator can restore the correct correspondence information with high probability.  Numerical experiments are presented to corroborate our theoretical claims."}}
{"id": "yEsj8pGNl1", "cdate": 1663850431912, "mdate": null, "content": {"title": "Projective Proximal Gradient Descent for Nonconvex Nonsmooth Optimization: Fast Convergence Without Kurdyka-Lojasiewicz (KL) Property", "abstract": "Nonconvex and nonsmooth optimization problems are important and challenging for statistics and machine learning. In this paper, we propose Projected Proximal Gradient Descent (PPGD) which solves a class of nonconvex and nonsmooth optimization problems, where the nonconvexity and nonsmoothness come from a nonsmooth regularization term which is nonconvex but piecewise convex. In contrast with existing convergence analysis of accelerated PGD methods for nonconvex and nonsmooth problems based on the Kurdyka-\\L{}ojasiewicz (K\\L{}) property, we provide a new theoretical analysis showing local fast convergence of PPGD. It is proved that PPGD achieves a fast convergence rate of $O(1/k^2)$ when the iteration number $k \\ge k_0$ for a finite $k_0$ on a class of nonconvex and nonsmooth problems under mild assumptions, which is locally the Nesterov's optimal convergence rate of first-order methods on smooth and convex objective function with Lipschitz continuous gradient. Experimental results demonstrate the effectiveness of PPGD."}}
{"id": "ZHyTtEd4lXz", "cdate": 1663850431079, "mdate": null, "content": {"title": "Differentiable Channel Selection for Self-Attention", "abstract": "Self-attention has been widely used in deep learning, and recent efforts have been devoted to incorporating self-attention modules into convolutional neural networks for computer vision. In this paper, we propose a novel attention module termed Differentiable Channel Selection (DCS). In contrast with conventional self-attention, DCS searches for the locations and key dimension of channels in a continuous space by a novel differentiable searching method. Our DCS module is compatible with either fixed neural network backbone or learnable backbone with Differentiable Neural Architecture Search (DNAS), leading to DCS with Fixed Backbone (DCS-FB) and DCS-DNAS respectively. We apply DCS-FB and DCS-DNAS to three computer vision tasks, person Re-IDentification methods (Re-ID), object detection, and image classification, with state-of-the-art results on standard benchmarks and compact architecture compared to competing methods, revealing the advantage of DCS."}}
{"id": "1_jtWjhSSkr", "cdate": 1663850134757, "mdate": null, "content": {"title": "Exponential Generalization Bounds with Near-Optimal Rates for $L_q$-Stable Algorithms", "abstract": "The \\emph{stability} of learning algorithms to changes in the training sample has been actively studied as a powerful proxy for reasoning about generalization. Recently, exponential  generalization and excess risk bounds with near-optimal rates have been obtained under the stringent and distribution-free notion of uniform stability~\\citep{bousquet2020sharper,klochkov2021stability}. In the meanwhile, under the notion of $L_q$-stability, which is weaker and distribution dependent, exponential generalization bounds are also available yet so far only with sub-optimal rates. Therefore, a fundamental question we would like to address in this paper is whether it is possible to derive near-optimal exponential generalization bounds for $L_q$-stable learning algorithms. As the core contribution of the present work, we give an affirmative answer to this question by developing strict analogues of the near-optimal generalization and risk bounds of uniformly stable algorithms for $L_q$-stable algorithms. Further, we demonstrate the power of our improved $L_q$-stability and generalization theory by applying it to derive strong sparse excess risk bounds, under mild conditions, for computationally tractable sparsity estimation algorithms such as Iterative Hard Thresholding (IHT)."}}
{"id": "TkSRbrUjQf3", "cdate": 1663850057438, "mdate": null, "content": {"title": "k-Median Clustering via Metric Embedding: Towards Better Initialization with Differential Privacy", "abstract": "In clustering algorithms, the choice of initial centers is crucial for the quality of the learned clusters. We propose a new initialization scheme for the $k$-median problem in the general metric space (e.g., discrete space induced by graphs), based on the construction of metric embedding tree structure of the data. From the tree, we propose a novel and efficient search algorithm, for good initial centers that can be used subsequently for the local search algorithm. The so-called HST initialization method can produce initial centers achieving lower errors than those from another popular initialization method, $k$-median++, with comparable efficiency. Our HST initialization can also be easily extended to the setting of differential privacy (DP) to generate private initial centers. We show that the error of applying DP local search followed by our private HST initialization improves previous results on the approximation error, and approaches the lower bound within a small factor. Experiments demonstrate the effectiveness of our proposed methods."}}
{"id": "lbzA07xjED", "cdate": 1663850057320, "mdate": null, "content": {"title": "Analysis of Error Feedback in Compressed Federated Non-Convex Optimization", "abstract": "Communication cost between the clients and the central server could be a bottleneck in real-world Federated Learning (FL) systems. In classical distributed learning, the method of Error Feedback (EF) has been a popular technique to remedy the downsides of biased gradient compression, but literature on applying EF to FL is still very limited. In this work, we propose a compressed FL scheme equipped with error feedback, named Fed-EF, with two variants depending on the global optimizer. We provide theoretical analysis showing that Fed-EF matches the convergence rate of the full-precision FL counterparts in non-convex optimization under data heterogeneity. Moreover, we initiate the first analysis of EF under partial client participation, which is an important scenario in FL, and demonstrate that the convergence rate of Fed-EF exhibits an extra slow down factor due to the ``stale error compensation'' effect. Experiments are conducted to validate the efficacy of Fed-EF in practical FL tasks and justify our theoretical findings."}}
