{"id": "1YW0u34ubE", "cdate": 1702896349540, "mdate": 1702896349540, "content": {"title": "Should you marginalize over possible tokenizations?", "abstract": "Autoregressive language models (LMs) map token sequences to probabilities. The usual practice for computing the probability of any character string (e.g. English sentences) is to first transform it into a sequence of tokens that is scored by the model. However, there are exponentially many token sequences that represent any given string. To truly compute the probability of a string one should marginalize over all tokenizations, which is typically intractable. Here, we analyze whether the practice of ignoring the marginalization is justified. To this end, we devise an importance-sampling-based algorithm that allows us to compute estimates of the marginal probabilities and compare them to the default procedure in a range of state-of-the-art models and datasets. Our results show that the gap in log-likelihood is no larger than 0.5% in most cases, but that it becomes more pronounced for data with long complex words."}}
{"id": "0QOgRaf8fBZ", "cdate": 1675827734297, "mdate": null, "content": {"title": "Aligning Foundation Models for Language with Preferences through  $f$-divergence Minimization", "abstract": "Aligning language models with preferences can be posed as approximating a target distribution representing some desired behavior. Existing approaches differ both in the functional form of the target distribution and the algorithm used to approximate it. For instance, Reinforcement Learning from Human Feedback (RLHF) corresponds to minimizing a reverse KL from an implicit target distribution arising from a KL penalty in the objective. On the other hand, Generative Distributional Control (GDC) has an explicit  target distribution and minimizes a forward KL from it using the Distributional Policy Gradient (DPG) algorithm. In this paper, we propose a new approach, $f$-DPG, which allows the use of any $f$-divergence to approximate any target distribution. $f$-DPG unifies both frameworks (RLHF, GDC) and the approximation methods (DPG, RL with KL penalties). We show the practical benefits of various choices of divergence objectives and demonstrate that there is no universally optimal objective but that  different divergences are good for approximating different targets."}}
{"id": "9GirHDN1mFC", "cdate": 1674127545117, "mdate": 1674127545117, "content": {"title": "Controlling Conditional Language Models without Catastrophic Forgetting", "abstract": "Machine learning is shifting towards general-purpose pretrained generative models, trained in a self-supervised manner on large amounts of data, which can then be applied to solve a large number of tasks. However, due to their generic training methodology, these models often fail to meet some of the downstream requirements (e.g., hallucinations in abstractive summarization or style violations in code generation). This raises the important question of how to adapt pre-trained generative models to meet all requirements without destroying their general capabilities (\"catastrophic forgetting\"). Recent work has proposed to solve this problem by representing task-specific requirements through energy-based models (EBMs) and approximating these EBMs using distributional policy gradients (DPG). Despite its effectiveness, this approach is however limited to unconditional distributions. In this paper, we extend DPG to conditional tasks by proposing Conditional DPG (CDPG). We evaluate CDPG on four different control objectives across three tasks (translation, summarization and code generation) and two pretrained models (T5 and GPT-Neo). Our results show that fine-tuning using CDPG robustly moves these pretrained models closer towards meeting control objectives and -- in contrast with baseline approaches -- does not result in catastrophic forgetting.\n"}}
{"id": "XvI6h-s4un", "cdate": 1652737869667, "mdate": null, "content": {"title": "On Reinforcement Learning and Distribution Matching for Fine-Tuning Language Models with no Catastrophic Forgetting", "abstract": "The availability of large pre-trained models is changing the landscape of Machine Learning research and practice, moving from a \"training from scratch\" to a \"fine-tuning'' paradigm. While in some applications the goal is to \"nudge'' the pre-trained distribution towards preferred outputs, in others it is to steer it towards a different distribution over the sample space. Two main paradigms have emerged to tackle this challenge: Reward Maximization (RM) and, more recently, Distribution Matching (DM). RM applies standard Reinforcement Learning (RL) techniques, such as Policy Gradients, to gradually increase the reward signal. DM prescribes to first make explicit the target distribution that the model is fine-tuned to approximate. Here we explore the theoretical connections between the two paradigms and show that methods such as KL-control developed in the RM paradigm can also be construed as belonging to DM. We further observe that while DM differs from RM, it can suffer from similar training difficulties, such as high gradient variance. We leverage connections between the two paradigms to import the concept of baseline into DM methods. We empirically validate the benefits of adding a baseline on an array of controllable language generation tasks such as constraining topic, sentiment, and gender distributions in texts sampled from a language model. We observe superior performance in terms of constraint satisfaction, stability, and sample efficiency."}}
{"id": "H42GVHMyabq", "cdate": 1647272507899, "mdate": null, "content": {"title": "Open-Ended Evolution as an Emergent Self-Organizing Search Process", "abstract": "The diversity and complexity of living systems on Earth have presumably emerged from a single common ancestor, and before that, from the inorganic components present on the surface of Earth.  So far, it is unclear what are the _algorithmic_ properties of a process that would display a similar trajectory in its state space. Describing such a process entails characterizing both the state space itself, the possible emergent forms, and the evolutionary process behind the diversification and complexification of forms. Because living systems are hypothesized to correspond to attractors in chemical networks, Artificial Chemistries (AC) are well suited to explore this question because they can simulate the evolution of these networks. Combinatory Chemistry is an AC in which self-reproducing metabolisms emerge from its dynamics. Here, I extend it with a set of mutation reactions, showing that said reactions coupled with the emergent structures in the system enable a more efficient search of complex structures. I conclude that the resulting dynamics constitute an emergent self-organizing search process that could capture the properties of open-ended evolutionary processes."}}
{"id": "8f95ajHrIFc", "cdate": 1632875659342, "mdate": null, "content": {"title": "On Reward Maximization and Distribution Matching for Fine-Tuning Language Models", "abstract": "The availability of large pre-trained models is changing the landscape of Machine Learning research and practice, moving from a \"training from scratch\" to a \"fine-tuning'' paradigm. While in some applications the goal is to \"nudge'' the pre-trained distribution towards preferred outputs, in others it is to steer it towards a different distribution over the sample space. Two main paradigms have emerged to tackle this challenge: Reward Maximization (RM) and, more recently, Distribution Matching (DM). RM applies standard Reinforcement Learning (RL) techniques, such as Policy Gradients, to gradually increase the reward signal. DM prescribes to first make explicit the target distribution that the model is fine-tuned to approximate. Here we explore the intimate connections between the two paradigms and show that methods such as KL-control developed in the RM paradigm can also be construed as belonging to DM. We further observe that while DM differs from RM, it can suffer from similar training difficulties, such as high gradient variance. We leverage connections between the two paradigms to import the concept of baseline into DM methods. We empirically validate the benefits of adding a baseline on an array of controllable language generation tasks such as constraining topic, sentiment, and gender distributions in texts sampled from a language model. We observe superior performance in terms of constraint satisfaction, stability, and sample efficiency."}}
{"id": "9zcjXdavnX", "cdate": 1632875646911, "mdate": null, "content": {"title": "Sampling from Discrete Energy-Based Models with Quality/Efficiency Trade-offs", "abstract": "Energy-Based Models (EBMs) allow for extremely flexible specifications of probability distributions. However, they do not provide a mechanism for obtaining exact samples from these distributions. Monte Carlo techniques can aid us in obtaining samples if some proposal distribution that we can easily sample from is available. For instance, rejection sampling can provide exact samples but is often difficult or impossible to apply due to the need to find a proposal distribution that upper-bounds the target distribution everywhere. Approximate Markov chain Monte Carlo sampling techniques like Metropolis-Hastings are usually easier to design, exploiting a local proposal distribution that performs local edits on an evolving sample.  However, these techniques can be inefficient due to the local nature of the proposal distribution and do not provide an estimate of the quality of their samples. In this work, we propose a new approximate sampling technique, Quasi Rejection Sampling (QRS), that allows for a trade-off between sampling efficiency and sampling quality, while providing explicit convergence bounds and diagnostics. QRS capitalizes on the availability of high-quality global proposal distributions obtained from deep learning models.  We demonstrate the effectiveness of QRS sampling for discrete EBMs over text for the tasks of controlled text generation with distributional constraints and paraphrase generation.  We show that we can sample from such EBMs with arbitrary precision at the cost of sampling efficiency."}}
{"id": "qJqDCR7ZxqK", "cdate": 1623708298347, "mdate": null, "content": {"title": "Energy-Based Models for Code Generation under Compilability Constraints", "abstract": "Neural language models can be successfully trained on source code, leading to applications such as code completion. However, their versatile autoregressive self-supervision objective overlooks important global sequence-level features that are present in the data such as syntactic correctness or compilability. In this work, we pose the problem of learning to generate compilable code as constraint satisfaction. We define an Energy-Based Model (EBM) representing a pre-trained generative model with an imposed constraint of generating only compilable sequences. We then use the KL-Adaptive Distributional Policy Gradient algorithm (Khalifa et al., 2021) to train a generative model approximating the EBM. We conduct experiments showing that our proposed approach is able to improve compilability rates without sacrificing diversity and complexity of the generated samples."}}
{"id": "vC8hNRk9dOR", "cdate": 1601308098540, "mdate": null, "content": {"title": "Evaluating Online Continual Learning with CALM", "abstract": "Online Continual Learning (OCL) studies learning over a continuous data stream without observing any single example more than once, a setting that is closer to the experience of humans and systems that must learn \u201con-the-wild\u201d. Yet, commonly available benchmarks are far from these real world conditions, because they explicitly signal different tasks, lack latent similarity structure or assume temporal independence between different examples. Here, we propose a new benchmark for OCL based on language modelling in which input alternates between different languages and domains without any explicit delimitation. Additionally, we propose new metrics to study catastrophic forgetting in this setting and evaluate multiple baseline models based on compositions of experts. Finally, we introduce a simple gating technique that learns the latent similarities between different inputs, improving the performance of a Products of Experts model."}}
{"id": "rJxoi1HtPr", "cdate": 1569439650986, "mdate": null, "content": {"title": "Task-agnostic Continual Learning via Growing Long-Term Memory Networks", "abstract": "As our experience shows, humans can learn and deploy a myriad of different skills to tackle the situations they encounter daily. Neural networks, in contrast, have a fixed memory capacity that prevents them from learning more than a few sets of skills before starting to forget them. \nIn this work, we make a step to bridge neural networks with human-like learning capabilities. For this, we propose a model with a growing and open-bounded memory capacity that can be accessed based on the model\u2019s current demands. To test this system, we introduce a continual learning task based on language modelling where the model is exposed to multiple languages and domains in sequence, without providing any explicit signal on the type of input it is currently dealing with. The proposed system exhibits improved adaptation skills in that it can recover faster than comparable baselines after a switch in the input language or domain."}}
