{"id": "Gu7XtUSMoc", "cdate": 1695440963366, "mdate": 1695440963366, "content": {"title": "Kubric: A scalable dataset generator", "abstract": "Data is the driving force of machine learning, with the amount and quality of training data often being more important for the performance of a system than architecture and training details. But collecting, processing and annotating real data at scale is difficult, expensive, and frequently raises additional privacy, fairness and legal concerns. Synthetic data is a powerful tool with the potential to address these shortcomings: 1) it is cheap 2) supports rich ground-truth annotations 3) offers full control over data and 4) can circumvent or mitigate problems regarding bias, privacy and licensing. Unfortunately, software tools for effective data generation are less mature than those for architecture design and training, which leads to fragmented generation efforts. To address these problems we introduce Kubric, an open-source Python framework that interfaces with PyBullet and Blender to generate photo-realistic scenes, with rich annotations, and seamlessly scales to large jobs distributed over thousands of machines, and generating TBs of data. We demonstrate the effectiveness of Kubric by presenting a series of 11 different generated datasets for tasks ranging from studying 3D NeRF models to optical flow estimation. We release Kubric, the used assets, all of the generation code, as well as the rendered datasets for reuse and modification"}}
{"id": "Ouf9ld6YXL", "cdate": 1668046931454, "mdate": 1668046931454, "content": {"title": "Controllable Shadow Generation Using Pixel Height Maps", "abstract": "Shadows are essential for realistic image compositing from\n2D image cutouts. Physics-based shadow rendering methods require 3D\ngeometries, which are not always available. Deep learning-based shadow\nsynthesis methods learn a mapping from the light information to an\nobject\u2019s shadow without explicitly modeling the shadow geometry. Still,\nthey lack control and are prone to visual artifacts. We introduce \u201cPixel\nHeight\u201d, a novel geometry representation that encodes the correlations\nbetween objects, ground, and camera pose. The Pixel Height can be\ncalculated from 3D geometries, manually annotated on 2D images, and\ncan also be predicted from a single-view RGB image by a supervised\napproach. It can be used to calculate hard shadows in a 2D image\nbased on the projective geometry, providing precise control of the shadows\u2019\ndirection and shape. Furthermore, we propose a data-driven soft\nshadow generator to apply softness to a hard shadow based on a softness\ninput parameter. Qualitative and quantitative evaluations demonstrate\nthat the proposed Pixel Height significantly improves the quality of the\nshadow generation while allowing for controllability."}}
{"id": "9OEW_t2uO4u", "cdate": 1663849844230, "mdate": null, "content": {"title": "CLIP-PAE: Projection-Augmentation Embedding to Extract Relevant Features for a Disentangled, Interpretable and Controllable Text-Guided Image Manipulation", "abstract": "Recently introduced Contrastive Language-Image Pre-Training (CLIP) bridges images and text by embedding them into a joint latent space. This opens the door to ample literature that aims to manipulate an input image by providing a textual explanation. However, due to the discrepancy between image and text embeddings in the joint space, using text embeddings as the optimization target often introduces undesired artifacts in the resulting images. Disentanglement, interpretability, and controllability are also hard to guarantee for manipulation. To alleviate these problems, we propose to define corpus subspaces spanned by prompts to capture specific image characteristics. We introduce CLIP projection-augmentation embedding (PAE) as an optimization target to improve the performance of text-guided image manipulation. Our method is a simple and general paradigm that can be easily computed and adapted, and smoothly incorporated into any CLIP-based latent manipulation algorithm to improve performance. To demonstrate the effectiveness of our method, we conduct several theoretical and empirical system studies. As a case study, we utilize the method for text-guided semantic face editing. We quantitatively and qualitatively demonstrate that PAE facilitates a more disentangled, interpretable, and controllable image manipulation method with state of the art quality and accuracy."}}
{"id": "rG7HZZtIc-", "cdate": 1652737418732, "mdate": null, "content": {"title": "D^2NeRF: Self-Supervised Decoupling of Dynamic and Static Objects from a Monocular Video", "abstract": "Given a monocular video, segmenting and decoupling dynamic objects while recovering the static environment is a widely studied problem in machine intelligence. Existing solutions usually approach this problem in the image domain, limiting their performance and understanding of the environment. We introduce Decoupled Dynamic Neural Radiance Field (D^2NeRF), a self-supervised approach that takes a monocular video and learns a 3D scene representation which decouples moving objects, including their shadows, from the static background. Our method represents the moving objects and the static background by two separate neural radiance fields with only one allowing for temporal changes. A naive implementation of this approach leads to the dynamic component taking over the static one as the representation of the former is inherently more general and prone to overfitting. To this end, we propose a novel loss to promote correct separation of phenomena. We further propose a shadow field network to detect and decouple dynamically moving shadows. We introduce a new dataset containing various dynamic objects and shadows and demonstrate that our method can achieve better performance than state-of-the-art approaches in decoupling dynamic and static 3D objects, occlusion and shadow removal, and image segmentation for moving objects. Project page: https://d2nerf.github.io/"}}
{"id": "sObfKsVkEFx", "cdate": 1630526991377, "mdate": 1630526991377, "content": {"title": "Iso-Points: Optimizing Neural Implicit Surfaces with Hybrid Representations", "abstract": "Neural implicit functions have emerged as a powerful representation for surfaces in 3D. Such a function can encode a high quality surface with intricate details into the parameters of a deep neural network. However, optimizing for the parameters for accurate and robust reconstructions remains a challenge, especially when the input data is noisy or incomplete. In this work, we develop a hybrid neural surface representation that allows us to impose geometry-aware sampling and regularization, which significantly improves the fidelity of reconstructions. We propose to use iso-points as an explicit representation for a neural implicit function. These points are computed and updated on-the-fly during training to capture important geometric features and impose geometric constraints on the optimization. We demonstrate that our method can be adopted to improve state-of-the-art techniques for reconstructing neural implicit surfaces from multi-view images or point clouds. Quantitative and qualitative evaluations show that, compared with existing sampling and optimization methods, our approach allows faster convergence, better generalization, and accurate recovery of details and topology."}}
{"id": "c2te4UBqI3T", "cdate": 1630526830813, "mdate": 1630526830813, "content": {"title": "Differentiable Surface Splatting for Point-based Geometry Processing", "abstract": "We propose Differentiable Surface Splatting (DSS), a high-fidelity differentiable renderer for point clouds. Gradients for point locations and normals are carefully designed to handle discontinuities of the rendering function. Regularization terms are introduced to ensure uniform distribution of the points on the underlying surface. We demonstrate applications of DSS to inverse rendering for geometry synthesis and denoising, where large scale topological changes, as well as small scale detail modifications, are accurately and robustly handled without requiring explicit connectivity, outperforming state-of-the-art techniques. The data and code are at https://github.com/yifita/DSS."}}
