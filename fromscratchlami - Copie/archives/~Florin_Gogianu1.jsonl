{"id": "dSXmVUU_wG", "cdate": 1633046400000, "mdate": 1696586347086, "content": {"title": "ObserveNet Control: A Vision-Dynamics Learning Approach to Predictive Control in Autonomous Vehicles", "abstract": "A key component in autonomous driving is the ability of the self-driving car to understand, track and predict the dynamics of the surrounding environment. Although there is significant work in the area of object detection, tracking and observations prediction, there is no prior work demonstrating that raw observations prediction can be used for motion planning and control. In this paper, we propose ObserveNet Control, which is a vision-dynamics approach to the predictive control problem of autonomous vehicles. Our method is composed of a i) deep neural network able to confidently predict future sensory data on a time horizon of up to 10 s and ii) a temporal planner designed to compute a safe vehicle state trajectory based on the predicted sensory data. Given the vehicle's historical state and sensing data in the form of Lidar point clouds, the method aims to learn the dynamics of the observed driving environment in a self-supervised manner, without the need to manually specify training labels. The experiments are performed both in simulation and real-life, using CARLA and RovisLab's AMTU mobile platform as a 1:4 scaled model of a car. We evaluate the capabilities of ObserveNet Control in aggressive driving contexts, such as overtaking maneuvers or side cut-off situations, while comparing the results with a baseline Dynamic Window Approach (DWA) and two state-of-the-art imitation learning systems, that is, Learning by Cheating (LBC) and World on Rails (WOR)."}}
{"id": "oQ-R9YxpP-7", "cdate": 1609459200000, "mdate": 1681728661600, "content": {"title": "Spectral Normalisation for Deep Reinforcement Learning: An Optimisation Perspective", "abstract": "Most of the recent deep reinforcement learning advances take an RL-centric perspective and focus on refinements of the training objective. We diverge from this view and show we can recover the perf..."}}
{"id": "IxAn6PDPaM", "cdate": 1609459200000, "mdate": 1696586347089, "content": {"title": "ObserveNet Control: A Vision-Dynamics Learning Approach to Predictive Control in Autonomous Vehicles", "abstract": "A key component in autonomous driving is the ability of the self-driving car to understand, track and predict the dynamics of the surrounding environment. Although there is significant work in the area of object detection, tracking and observations prediction, there is no prior work demonstrating that raw observations prediction can be used for motion planning and control. In this paper, we propose ObserveNet Control, which is a vision-dynamics approach to the predictive control problem of autonomous vehicles. Our method is composed of a: i) deep neural network able to confidently predict future sensory data on a time horizon of up to 10s and ii) a temporal planner designed to compute a safe vehicle state trajectory based on the predicted sensory data. Given the vehicle's historical state and sensing data in the form of Lidar point clouds, the method aims to learn the dynamics of the observed driving environment in a self-supervised manner, without the need to manually specify training labels. The experiments are performed both in simulation and real-life, using CARLA and RovisLab's AMTU mobile platform as a 1:4 scaled model of a car. We evaluate the capabilities of ObserveNet Control in aggressive driving contexts, such as overtaking maneuvers or side cut-off situations, while comparing the results with a baseline Dynamic Window Approach (DWA) and two state-of-the-art imitation learning systems, that is, Learning by Cheating (LBC) and World on Rails (WOR)."}}
{"id": "0dYLQkR9NU", "cdate": 1609459200000, "mdate": 1681728662134, "content": {"title": "Spectral Normalisation for Deep Reinforcement Learning: an Optimisation Perspective", "abstract": "Most of the recent deep reinforcement learning advances take an RL-centric perspective and focus on refinements of the training objective. We diverge from this view and show we can recover the performance of these developments not by changing the objective, but by regularising the value-function estimator. Constraining the Lipschitz constant of a single layer using spectral normalisation is sufficient to elevate the performance of a Categorical-DQN agent to that of a more elaborated \\rainbow{} agent on the challenging Atari domain. We conduct ablation studies to disentangle the various effects normalisation has on the learning dynamics and show that is sufficient to modulate the parameter updates to recover most of the performance of spectral normalisation. These findings hint towards the need to also focus on the neural component and its learning dynamics to tackle the peculiarities of Deep Reinforcement Learning."}}
{"id": "wJbgg5ZLEaW", "cdate": 1483228800000, "mdate": null, "content": {"title": "Learning to Maximize Return in a Stag Hunt Collaborative Scenario through Deep Reinforcement Learning", "abstract": "In this paper we present a deep reinforcement learning approach for learning to play a time extended social dilemma game in a simulated environment. Agents face different types of adversaries with different levels of commitment to a collaborative strategy. Our method builds on recent advances in policy gradient training using deep neural networks. We investigate multiple stochastic gradient algorithms such as Reinforce or Actor Critic with auxiliary tasks for faster convergence."}}
{"id": "9ILk_sQYuqG", "cdate": 1483228800000, "mdate": null, "content": {"title": "Random Projection Based Representations for Learning Policies in Deterministic Atari Games", "abstract": "Recent advances in sample efficient reinforcement learning algorithms in quasi-deterministic environments highlight the requirement for computationally inexpensive visual representations. Here we investigate non-parametric dimensionality reduction techniques based on random linear transformations and we provide empirical evidence on the importance of high-variance projections using sparse random matrices in the context of episodic controllers learning deterministic policies. We also propose a novel Maximum-Variance Random Projection and improve on the performance of the original Model-Free Episodic Control results with respect to both sample efficiency and final average score."}}
