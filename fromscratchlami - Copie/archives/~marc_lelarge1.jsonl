{"id": "kOVRjCkEkRw", "cdate": 1672531200000, "mdate": 1695969344681, "content": {"title": "FLEX: an Adaptive Exploration Algorithm for Nonlinear Systems", "abstract": "Model-based reinforcement learning is a powerful tool, but collecting data to fit an accurate model of the system can be costly. Exploring an unknown environment in a sample-efficient manner is hen..."}}
{"id": "OeeMl2eU3l", "cdate": 1672531200000, "mdate": 1683879053711, "content": {"title": "Convergence beyond the over-parameterized regime using Rayleigh quotients", "abstract": "In this paper, we present a new strategy to prove the convergence of deep learning architectures to a zero training (or even testing) loss by gradient flow. Our analysis is centered on the notion of Rayleigh quotients in order to prove Kurdyka-{\\L}ojasiewicz inequalities for a broader set of neural network architectures and loss functions. We show that Rayleigh quotients provide a unified view for several convergence analysis techniques in the literature. Our strategy produces a proof of convergence for various examples of parametric learning. In particular, our analysis does not require the number of parameters to tend to infinity, nor the number of samples to be finite, thus extending to test loss minimization and beyond the over-parameterized regime."}}
{"id": "D_73KObuZO", "cdate": 1672531200000, "mdate": 1695969344787, "content": {"title": "FLEX: an Adaptive Exploration Algorithm for Nonlinear Systems", "abstract": "Model-based reinforcement learning is a powerful tool, but collecting data to fit an accurate model of the system can be costly. Exploring an unknown environment in a sample-efficient manner is hence of great importance. However, the complexity of dynamics and the computational limitations of real systems make this task challenging. In this work, we introduce FLEX, an exploration algorithm for nonlinear dynamics based on optimal experimental design. Our policy maximizes the information of the next step and results in an adaptive exploration algorithm, compatible with generic parametric learning models and requiring minimal resources. We test our method on a number of nonlinear environments covering different settings, including time-varying dynamics. Keeping in mind that exploration is intended to serve an exploitation objective, we also test our algorithm on downstream model-based classical control tasks and compare it to other state-of-the-art model-based and model-free approaches. The performance achieved by FLEX is competitive and its computational cost is low."}}
{"id": "7W4lWxxzgDA", "cdate": 1664194171241, "mdate": null, "content": {"title": "Periodic Signal Recovery with Regularized Sine Neural Networks", "abstract": "We consider the problem of learning a periodic one-dimensional signal with neural networks, and designing models that are able to extrapolate the signal well beyond the training window. First, we show that multi-layer perceptrons with ReLU activations are provably unable to perform this task, and lead to poor performance in practice even close to the training window. Then, we propose a novel architecture using sine activation functions along with a well-chosen non-convex regularization, that is able to extrapolate the signal with low error well beyond the training window. Our architecture is several orders of magnitude better than its competitors for distant extrapolation (beyond 100 periods of the signal), while being able to accurately recover the frequency spectrum of the signal in a multi-tone setting."}}
{"id": "TpqJy1BmD6K", "cdate": 1663850272410, "mdate": null, "content": {"title": "Learning to solve the Hidden Clique Problem with Graph Neural Networks", "abstract": "We study data-driven methods for the hidden clique problem in random graphs. The training data is obtained by hiding a clique in the random graph, where the signal to noise ratio is tuned by choosing the size of the hidden clique and the density of the random graph. Using synthetic datasets allows us to test empirically the performance and generalization properties of various graph neural network (GNN) architectures at different levels of difficulties for the task. We compare message passing GNNs and GNNs augmented with a single quadratic operation (matrix multiplication) first introduced in \\citep{maron2019fgnn}. Adding skip connections and normalization to these augmented GNNs is shown to improve their learning process and their generalization properties without any loss in time complexity. For hard instances of our hidden clique problem, they are shown to outperform message passing GNNs."}}
{"id": "pl279jU4GOu", "cdate": 1652737415815, "mdate": null, "content": {"title": "Convergence beyond the over-parameterized regime using Rayleigh quotients", "abstract": "In this paper, we present a new strategy to prove the convergence of Deep Learning architectures to a zero training (or even testing) loss by gradient flow. Our analysis is centered on the notion of Rayleigh quotients in order to prove Kurdyka-Lojasiewicz inequalities for a broader set of neural network architectures and loss functions. We show that Rayleigh quotients provide a unified view for several convergence analysis techniques in the literature. Our strategy produces a proof of convergence for various examples of parametric learning. In particular, our analysis does not require the number of parameters to tend to infinity, nor the number of samples to be finite, thus extending to test loss minimization and beyond the over-parameterized regime."}}
{"id": "oG3QL-wy_Ej", "cdate": 1640995200000, "mdate": 1695969344787, "content": {"title": "Online greedy identification of linear dynamical systems", "abstract": "This work addresses the problem of exploration in an unknown environment. For multi-input multi-output, linear time-invariant dynamical systems, we use an experimental design framework and introduce an online greedy policy where the control maximizes the information of the next step. We evaluate our approach experimentally and compare it with more elaborate gradient-based methods. In a setting with a limited number of observations, our algorithm has low complexity and shows competitive performances. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>"}}
{"id": "_RLt0KFejC", "cdate": 1640995200000, "mdate": 1683880927148, "content": {"title": "SiMCa: Sinkhorn Matrix Factorization with Capacity Constraints", "abstract": "For a very broad range of problems, recommendation algorithms have been increasingly used over the past decade. In most of these algorithms, the predictions are built upon user-item affinity scores which are obtained from high-dimensional embeddings of items and users. In more complex scenarios, with geometrical or capacity constraints, prediction based on embeddings may not be sufficient and some additional features should be considered in the design of the algorithm. In this work, we study the recommendation problem in the setting where affinities between users and items are based both on their embeddings in a latent space and on their geographical distance in their underlying euclidean space (e.g., $\\mathbb{R}^2$), together with item capacity constraints. This framework is motivated by some real-world applications, for instance in healthcare: the task is to recommend hospitals to patients based on their location, pathology, and hospital capacities. In these applications, there is somewhat of an asymmetry between users and items: items are viewed as static points, their embeddings, capacities and locations constraining the allocation. Upon the observation of an optimal allocation, user embeddings, items capacities, and their positions in their underlying euclidean space, our aim is to recover item embeddings in the latent space; doing so, we are then able to use this estimate e.g. in order to predict future allocations. We propose an algorithm (SiMCa) based on matrix factorization enhanced with optimal transport steps to model user-item affinities and learn item embeddings from observed data. We then illustrate and discuss the results of such an approach for hospital recommendation on synthetic data."}}
{"id": "TSHCw8AzRJ", "cdate": 1640995200000, "mdate": 1683879053722, "content": {"title": "Convergence beyond the over-parameterized regime using Rayleigh quotients", "abstract": "In this paper, we present a new strategy to prove the convergence of Deep Learning architectures to a zero training (or even testing) loss by gradient flow. Our analysis is centered on the notion of Rayleigh quotients in order to prove Kurdyka-Lojasiewicz inequalities for a broader set of neural network architectures and loss functions. We show that Rayleigh quotients provide a unified view for several convergence analysis techniques in the literature. Our strategy produces a proof of convergence for various examples of parametric learning. In particular, our analysis does not require the number of parameters to tend to infinity, nor the number of samples to be finite, thus extending to test loss minimization and beyond the over-parameterized regime."}}
{"id": "SpeG6JPECa", "cdate": 1640995200000, "mdate": 1695969344784, "content": {"title": "Periodic signal recovery with regularized sine neural networks", "abstract": "We consider the problem of learning a periodic one-dimensional signal with neural networks, and designing models that are able to extrapolate the signal well beyond the training window. First, we s..."}}
