{"id": "Xkk37WaFsy1", "cdate": 1609459200000, "mdate": null, "content": {"title": "Contextualization and individualization for just-in-time adaptive interventions to reduce sedentary behavior", "abstract": "Wearable technology opens opportunities to reduce sedentary behavior; however, commercially available devices do not provide tailored coaching strategies. Just-In-Time Adaptive Interventions (JITAI) provide such a framework; however most JITAI are conceptual to date. We conduct a study to evaluate just-in-time nudges in free-living conditions in terms of receptiveness and nudge impact. We first quantify baseline behavioral patterns in context using features such as location and step count, and assess differences in individual responses. We show there is a strong inverse relationship between average daily step counts and time spent being sedentary indicating that steps are steadily taken throughout the day, rather than in large bursts. Interestingly, the effect of nudges delivered at the workplace is larger in terms of step count than those delivered at home. We develop Random Forest models to learn nudge receptiveness using both individualized and contextualized data. We show that step count is the least important identifier in nudge receptiveness, while location is the most important. Furthermore, we compare the developed models with a commercially available smart coach using post-hoc analysis. The results show that using the contextualized and individualized information significantly outperforms non-JITAI approaches to determine nudge receptiveness."}}
{"id": "kxrbMy6eUP", "cdate": 1546300800000, "mdate": null, "content": {"title": "Model-based Reinforcement Learning for Predictions and Control for Limit Order Books", "abstract": "We build a profitable electronic trading agent with Reinforcement Learning that places buy and sell orders in the stock market. An environment model is built only with historical observational data, and the RL agent learns the trading policy by interacting with the environment model instead of with the real-market to minimize the risk and potential monetary loss. Trained in unsupervised and self-supervised fashion, our environment model learned a temporal and causal representation of the market in latent space through deep neural networks. We demonstrate that the trading policy trained entirely within the environment model can be transferred back into the real market and maintain its profitability. We believe that this environment model can serve as a robust simulator that predicts market movement as well as trade impact for further studies."}}
{"id": "XrBYQXsXax", "cdate": 1546300800000, "mdate": null, "content": {"title": "Shapley Value Approximation with Divisive Clustering", "abstract": "In cooperative game theory, the two foremost problems are determining what coalitions will form and how a coalition's payoff should be divided. The Shapley value, a proven fair and unique payoff distribution, has become a central solution concept in the field. Computing the Shapley value is exponential in the number of agents, however, and has motivated many practical approximation methods, only two of which apply to arbitrary cooperative games. We propose a Shapley value approximation method using hierarchical clustering that partitions coalitions based on agent feature similarity and then interpolates the subcoalitions' Shapley values. Additionally, the approximation is guaranteed to satisfy the Shapley value's desirable fairness properties of symmetry, efficiency, and often null player. With a low runtime, experimental error, and a tuning parameter for error-runtime trade-off, this algorithm is the most practical for cooperative games requiring fast, near-optimally fair payoff distributions."}}
{"id": "6WKNkTlc5XM", "cdate": 1546300800000, "mdate": null, "content": {"title": "Mixed-Autonomy Traffic Control with Proximal Policy Optimization", "abstract": "This work studies mixed-autonomy traffic optimization at a network level with Deep Reinforcement Learning (DRL). In mixed-autonomy traffic, a mixture of connected autonomous vehicles (CAVs) and human driving vehicles is present on the roads at the same time. We hypothesize that controlling distributed CAVs at a network level can outperform the individually controlled CAVs. Our goal is to improve traffic fluidity in terms of the vehicle's average velocity and collision avoidance. We propose three distributed learning control policies for CAVs in mixed-autonomy traffic using Proximal Policy Optimization (PPO), a policy gradient DRL method. We conduct the experiments with different traffic settings and CAV penetration rates on the Flow framework, a new open-source microscopic traffic simulator. The experiments show that network-level RL policies for controlling CAVs outperform the individual-level RL policies in terms of the total rewards and the average velocity."}}
{"id": "-8upsG8Na14", "cdate": 1546300800000, "mdate": null, "content": {"title": "Decentralized Multi-Agent Actor-Critic with Generative Inference", "abstract": "Recent multi-agent actor-critic methods have utilized centralized training with decentralized execution to address the non-stationarity of co-adapting agents. This training paradigm constrains learning to the centralized phase such that only pre-learned policies may be used during the decentralized phase, which performs poorly when agent communications are delayed, noisy, or disrupted. In this work, we propose a new system that can gracefully handle partially-observable information due to communication disruptions during decentralized execution. Our approach augments the multi-agent actor-critic method's centralized training phase with generative modeling so that agents may infer other agents' observations when provided with locally available context. Our method is evaluated on three tasks that require agents to combine local and remote observations communicated by other agents. We evaluate our approach by introducing both partial observability during decentralized execution, and show that decentralized training on inferred observations performs as well or better than existing actor-critic methods."}}
{"id": "8Mf1hayZ3Sr", "cdate": 1514764800000, "mdate": null, "content": {"title": "Q-Learning Acceleration via State-Space Partitioning", "abstract": "One of the biggest obstacles of Reinforcement Learning (RL) is its slow convergence rate in large state spaces or with sparse rewards. It has been shown that single-agent RL can be accelerated within a cooperative multi-agent scenario with information sharing, however the speedup depends on how well the agents' information can be used together. We demonstrate in this paper that state-space partitioning among agents can be realized by reward design without hard coded rules. The partitioning-associated reward directs agents to focus on different partitions and thus share information more efficiently. This approach has two advantages: (1) agents' actions are not diminished and remain relatively independent from one another; (2) it can be used to accelerate learning in both structured state domains (where partitions can be pre-determined) and arbitrarily-structured state domains (where partitions may be developed dynamically by agent teams as they explore the environment). Finally, we validate the method's efficacy by comparing it to previous related work in a simplified soccer domain."}}
{"id": "7yQpenxcvtl", "cdate": 1483228800000, "mdate": null, "content": {"title": "Analysis of Meta-level Communication for Distributed Resource Allocation Problems", "abstract": "We examine the effects of meta-level-communication in the DSRAP (Distributed Sequential Resource Allocation Problem). In DSRAP, independent tasks are categorized into different types, where each task belonging to a particular task type shares a known distribution of task arrivals, durations, reward rates, maximum waiting times, and resource demands. We first look at a single task type DSRAP (SDSRAP) and develop an analytical model of the effect of meta-level communication about load on system performance for first-in-first-out (FIFO) local scheduling agents that forward tasks based on load. Through our analytical models and empirical results, we show how the frequency of communication affects performance for SDSRAP problems with one resource and task type. We then quantitatively measure the impact of meta-level communication on system performance with respect to the global measures of the system's load balance. We validate our analytical model's predictions experimentally, showing, e.g., as system load becomes unbalanced, performance decreases; organizational structure significantly impacts agent performance; and agents that can communicate and distribute tasks to neighbors significantly outperform agents working individually. Through our analysis on FIFO, routing algorithms, and our policy agents, we provide a framework for analyzing more complex task schedulers and task routing algorithms."}}
{"id": "2U17KRkpQBQ", "cdate": 1483228800000, "mdate": null, "content": {"title": "Towards Learning Efficient Intervention Policies for Wearable Devices", "abstract": "To serve large user populations, autonomous intervention systems (i.e. intelligent agents) are being developed to play more active roles such as fitness coaches and clinical disease prevention aids. Although generic user models have been developed, users may require extensive individualization to meet their personal needs. Machine learning techniques may be applied to learn tailored intervention policies for users. However, traditional machine learning requires significant amounts of data to learn an optimal policy. For wearable technology, this may mean probing the user to perform some activity and gauging user response. This paper presents a feasible intervention system model and discusses learners for tailoring user intervention policies. We examine how similar the general user model has to be with respect to the tailored model in order for our learner to perform well."}}
{"id": "bvOmNgL_A4Q", "cdate": 1293840000000, "mdate": null, "content": {"title": "Deploying power grid-integrated electric vehicles as a multi-agent system", "abstract": "Grid-Integrated Vehicles (GIVs) are plug-in Electric Drive Vehicles (EDVs) with power-management and other controls that allow them to respond to external commands sent by power-grid operators, or their affiliates, when parked and plugged-in to the grid. At a bare minimum, such GIVs should respond to demand-management commands or pricing signals to delay, reduce or switch-off the rate of charging when the demand for electricity is high. In more advanced cases, these GIVs might sell both power and storage capacity back to the grid in any of the several electric power markets --- a concept known as Vehicle-to-Grid power or V2G power. Although individual EDVs control too little power to sell in the market at an individual level, a large group of EDVs may form an aggregate or coalition that controls enough power to meaningfully sell, at a profit, in these markets. The profits made by such a coalition can then be used by the coalition members to offset the costs of the electric vehicles and batteries themselves. In this paper we describe an implemented and deployed multi-agent system that is used to integrate EDVs into the electricity grid managed by PJM, the largest transmission service operator in the world. We provide a brief introduction to GIVs and the various power markets and discuss why multi-agent systems are a good match for this application."}}
{"id": "jhhwx_yPmO2", "cdate": 1262304000000, "mdate": null, "content": {"title": "Coordination for uncertain outcomes using distributed neighbor exchange", "abstract": "Coordination of agent activites in non-deterministic, distributed environments is computationally difficult. Distributed Constraint Optimization (DCOP) provides a rich framework for modeling such multi-agent coordination problems, but existing representations, problem domains, and techniques for DCOP focus on small (<100 variables), deterministic solutions. We present a novel approach to DCOP for large-scale applications that contain uncertain outcomes. These types of real-time domains require distributed, scalable algorithms to meet difficult bounds on computation and communication time. To achieve this goal, we develop a new distributed neighbor exchange algorithm for DCOPs that scales to problems involving hundreds of variables and constraints and offers faster convergence to high quality solutions than existing DCOP algorithms. In addition, our complete solution includes new techniques for dynamic distributed constraint optimization and uncertainty in constraint processing. We validate our approach using test scenarios from the DARPA Coordinators program and show that our solution is very competitive with existing approaches."}}
