{"id": "8TQBiMFRr5s", "cdate": 1681974566049, "mdate": 1681974566049, "content": {"title": "Statistical Distance Based Deterministic Offspring Selection in SMC Methods", "abstract": "Over the years, sequential Monte Carlo (SMC) and, equivalently, particle filter (PF) theory has gained substantial attention from researchers. However, the performance of the resampling methodology, also known as offspring selection, has not advanced recently. We propose two deterministic offspring selection methods, which strive to minimize the Kullback-Leibler (KL) divergence and the total variation (TV) distance, respectively, between the particle distribution prior and subsequent to the offspring selection. By reducing the statistical distance between the selected offspring and the joint distribution, we obtain a heuristic search procedure that performs superior to a maximum likelihood search in precisely those contexts where the latter performs better than an SMC. For SMC and particle Markov chain Monte Carlo (pMCMC), our proposed offspring selection methods always outperform or compare favorably with the two state-of-the-art resampling schemes on two models commonly used as benchmarks from the literature."}}
{"id": "YRQbVcIRuXx", "cdate": 1679999942002, "mdate": 1679999942002, "content": {"title": "Multiple Importance Sampling ELBO and Deep Ensembles of Variational Approximations", "abstract": "In variational inference (VI), the marginal log-likelihood is estimated using the standard evidence lower bound (ELBO), or improved versions as the importance weighted ELBO (IWELBO). We propose the multiple importance sampling ELBO (MISELBO), a \\textit{versatile} yet \\textit{simple} framework. MISELBO is applicable in both amortized and classical VI, and it uses ensembles, e.g., deep ensembles, of independently inferred variational approximations. As far as we are aware, the concept of deep ensembles in amortized VI has not previously been established. We prove that MISELBO provides a tighter bound than the average of standard ELBOs, and demonstrate empirically that it gives tighter bounds than the average of IWELBOs. MISELBO is evaluated in density-estimation experiments that include MNIST and several real-data phylogenetic tree inference problems. First, on the MNIST dataset, MISELBO boosts the density-estimation performances of a state-of-the-art model, nouveau VAE. Second, in the phylogenetic tree inference setting, our framework enhances a state-of-the-art VI algorithm that uses normalizing flows. On top of the technical benefits of MISELBO, it allows to unveil connections between VI and recent advances in the importance sampling literature, paving the way for further methodological advances. We provide our code at \\url{this https URL}."}}
{"id": "3mKsb38me8", "cdate": 1671895008666, "mdate": 1671895008666, "content": {"title": "Graph Neural Networks for Nomination and Representation Learning of Web Elements", "abstract": "This paper tackles the under-explored problem of DOM element\nnomination and representation learning with three important contributions. First, we present a large-scale and realistic dataset of\nwebpages, far richer and more diverse than other datasets proposed\nfor element representation learning, classification and nomination\non the web. The dataset contains 51, 701 manually labeled product\npages from 8, 175 real e-commerce websites. Second, we adapt several Graph Neural Network (GNN) architectures to website DOM\ntrees and benchmark their performance on a diverse set of element\nnomination tasks using our proposed dataset. In element nomination, a single element on a page is selected for a given class. We\nshow that on our challenging dataset a simple Convolutional GNN\noutperforms state-of-the-art methods on web element nomination.\nFinally, we propose a new training method that further boosts the\nelement nomination accuracy. In nomination for the web, classification (assigning a class to a given element) is usually used as\na surrogate objective for nomination during training. Our novel\ntraining methodology steers the classification objective towards\nthe more complex and useful nomination objective."}}
{"id": "ULkdnAqaZTx", "cdate": 1663850017276, "mdate": null, "content": {"title": "Learning with MISELBO: The Mixture Cookbook", "abstract": "Mixture models in variational inference (VI) is an active field of research. Recent works have established their connection to multiple importance sampling (MIS) through the MISELBO and advanced the use of ensemble approximations for large-scale problems. However, as we show here, an independent learning of the ensemble components can lead to suboptimal diversity. Hence, we study the effect of instead using MISELBO as an objective function for learning mixtures, and we propose the first ever mixture of variational approximations for a normalizing flow-based hierarchical variational autoencoder (VAE) with VampPrior and a PixelCNN decoder network. Two major insights led to the construction of this novel \\textit{composite model}. First, mixture models have potential to be off-the-shelf tools for practitioners to obtain more flexible posterior approximations in VAEs. Therefore, we make them more accessible by demonstrating how to apply them to four popular architectures. Second, the mixture components cooperate in order to cover the target distribution while trying to maximize their diversity when MISELBO is the objective function. We explain this cooperative behavior by drawing a novel connection between VI and adaptive importance sampling. Finally, we demonstrate the superiority of the Mixture VAEs' learned feature representations on both image and single-cell transcriptome data, and obtain state-of-the-art results among VAE architectures in terms of negative log-likelihood on the MNIST and FashionMNIST datasets. Code available here: \\url{gitlink}."}}
{"id": "TIXwBZB3Jl6", "cdate": 1652737764850, "mdate": null, "content": {"title": "VaiPhy: a Variational Inference Based Algorithm for Phylogeny", "abstract": "Phylogenetics is a classical methodology in computational biology that today has become highly relevant for medical investigation of single-cell data, e.g., in the context of development of cancer.  The exponential size of the tree space is unfortunately a formidable obstacle for current Bayesian phylogenetic inference using Markov chain Monte Carlo based methods since these rely on local operations. And although more recent variational inference (VI) based methods offer speed improvements, they rely on expensive auto-differentiation operations for learning the variational parameters. We propose VaiPhy, a remarkably fast VI based algorithm for approximate posterior inference in an \\textit{augmented tree space}. VaiPhy produces marginal log-likelihood estimates on par with the state-of-the-art methods on real data, and is considerably faster since it does not require auto-differentiation. Instead, VaiPhy combines coordinate ascent update equations with two novel sampling schemes: (i) \\textit{SLANTIS}, a proposal distribution for tree topologies in the augmented tree space, and (ii) the \\textit{JC sampler}, the, to the best of our knowledge, first ever scheme for sampling branch lengths directly from the popular Jukes-Cantor model. We compare VaiPhy in terms of density estimation and runtime. Additionally, we evaluate the reproducibility of the baselines. We provide our code on GitHub: \\url{https://github.com/Lagergren-Lab/VaiPhy}."}}
{"id": "GfM1pBD9mI", "cdate": 1577836800000, "mdate": null, "content": {"title": "Orthogonal Mixture of Hidden Markov Models", "abstract": "Mixtures of Hidden Markov Models (MHMM) are widely used for clustering of sequential data, by letting each cluster correspond to a Hidden Markov Model (HMM). Expectation Maximization (EM) is the standard approach for learning the parameters of an MHMM. However, due to the non-convexity of the objective function, EM can converge to poor local optima. To tackle this problem, we propose a novel method, the Orthogonal Mixture of Hidden Markov Models (oMHMM), which aims to direct the search away from candidate solutions that include very similar HMMs, since those do not fully exploit the power of the mixture model. The directed search is achieved by including a penalty in the objective function that favors higher orthogonality between the transition matrices of the HMMs. Experimental results on both simulated and real-world datasets show that the oMHMM consistently finds equally good or better local optima than the standard EM for an MHMM; for some datasets, the clustering performance is significantly improved by our novel oMHMM (up\u00a0to 55 percentage points w.r.t. the v-measure). Moreover, the oMHMM may also decrease the computational cost substantially, reducing the number of iterations down to a fifth of those required by MHMM using standard EM."}}
{"id": "qOYf0inMPQZ", "cdate": 1483228800000, "mdate": null, "content": {"title": "Fast and general tests of genetic interaction for genome-wide association studies", "abstract": "Author summary Interaction between organic molecules forms the basis of all biological systems. The availability of high-throughput genotyping and sequencing platforms enables us to cost-effectively genotype a large number of individuals. For sufficiently large datasets it is possible to reconstruct the genetic dependencies that underlie complex traits and diseases. However, there is a need for efficient statistical methodologies that can tackle the large sample size and computational resources required to study interaction. In this work we provide theory that reduces the required computational resources, and enable multiple research groups to effectively combine their results."}}
{"id": "iWAJsUOakNv", "cdate": 1451606400000, "mdate": null, "content": {"title": "Computational Cancer Biology: An Evolutionary Perspective", "abstract": ""}}
{"id": "G392NVcsqW", "cdate": 1451606400000, "mdate": null, "content": {"title": "Probabilistic inference of lateral gene transfer events", "abstract": "Background Lateral gene transfer (LGT) is an evolutionary process that has an important role in biology. It challenges the traditional binary tree-like evolution of species and is attracting increasing attention of the molecular biologists due to its involvement in antibiotic resistance. A number of attempts have been made to model LGT in the presence of gene duplication and loss, but reliably placing LGT events in the species tree has remained a challenge. Results In this paper, we propose probabilistic methods that samples reconciliations of the gene tree with a dated species tree and computes maximum a posteriori probabilities. The MCMC-based method uses the probabilistic model DLTRS, that integrates LGT, gene duplication, gene loss, and sequence evolution under a relaxed molecular clock for substitution rates. We can estimate posterior distributions on gene trees and, in contrast to previous work, the actual placement of potential LGT, which can be used to, e.g., identify \u201chighways\u201d of LGT. Conclusions Based on a simulation study, we conclude that the method is able to infer the true LGT events on gene tree and reconcile it to the correct edges on the species tree in most cases. Applied to two biological datasets, containing gene families from Cyanobacteria and Molicutes, we find potential LGTs highways that corroborate other studies as well as previously undetected examples."}}
{"id": "vK2IeAJABkJ", "cdate": 1388534400000, "mdate": null, "content": {"title": "Learning Bounded Tree-width Bayesian Networks using Integer Linear Programming", "abstract": "In many applications one wants to compute conditional probabilities given a Bayesian network. This inference problem is NP-hard in general but becomes tractable when the network has low tree-width...."}}
