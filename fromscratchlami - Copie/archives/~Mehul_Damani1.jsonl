{"id": "Vk9RH9aL1Yv", "cdate": 1663850213724, "mdate": null, "content": {"title": "Continuous Goal Sampling: A Simple Technique to Accelerate Automatic Curriculum Learning", "abstract": "Goal-conditioned reinforcement learning (RL) tackles the problem of training an RL agent to reach multiple goals in an environment, often with sparse rewards only administered upon reaching the goal. \nIn this regard, automatic curriculum learning can improve an agent's learning by sampling goals in a structured order catered to the agent's current ability. \nThis work presents two contributions to improve learning in goal-conditioned RL environments.\nFirst, we present a simple, algorithm-agnostic technique to accelerate learning by continuous goal sampling, in which an agent's goals are sampled and changed multiple times within a single episode. \nSuch continuous goal sampling enables faster exploration of the goal space and allows curriculum methods to have a more significant impact on an agent's learning.\nSecond, we propose VDIFF, an automatic curriculum learning method that uses an agent's value function to create a self-paced curriculum by sampling goals on which the agent is demonstrating high learning progress.\nThrough results on 17 multi-goal robotic environments and navigation tasks, we show that continuous goal sampling and VDIFF work synergistically and result in performance gains over current state-of-the-art methods."}}
