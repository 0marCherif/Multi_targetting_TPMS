{"id": "YAtY8My_-D", "cdate": 1672531200000, "mdate": 1695520550542, "content": {"title": "A Rainbow in Deep Network Black Boxes", "abstract": "We introduce rainbow networks as a probabilistic model of trained deep neural networks. The model cascades random feature maps whose weight distributions are learned. It assumes that dependencies between weights at different layers are reduced to rotations which align the input activations. Neuron weights within a layer are independent after this alignment. Their activations define kernels which become deterministic in the infinite-width limit. This is verified numerically for ResNets trained on the ImageNet dataset. We also show that the learned weight distributions have low-rank covariances. Rainbow networks thus alternate between linear dimension reductions and non-linear high-dimensional embeddings with white random features. Gaussian rainbow networks are defined with Gaussian weight distributions. These models are validated numerically on image classification on the CIFAR-10 dataset, with wavelet scattering networks. We further show that during training, SGD updates the weight covariances while mostly preserving the Gaussian initialization."}}
{"id": "Q7ymbMzZ0Am", "cdate": 1672531200000, "mdate": 1681649837247, "content": {"title": "Learning multi-scale local conditional probability models of images", "abstract": ""}}
{"id": "VZX2I_VVJKH", "cdate": 1663850410199, "mdate": null, "content": {"title": "Learning multi-scale local conditional probability models of images", "abstract": "Deep neural networks can learn powerful prior probability models for images, as evidenced by the high-quality generations obtained with recent score-based diffusion methods. But the means by which these networks capture complex global statistical structure, apparently without suffering from the curse of dimensionality, remain a mystery. To study this, we incorporate diffusion methods into a multi-scale decomposition, reducing dimensionality by assuming a stationary local Markov model for wavelet coefficients conditioned on coarser-scale coefficients. We instantiate this model using convolutional neural networks (CNNs) with local receptive fields, which enforce both the stationarity and Markov properties. Global structures are captured using a CNN with receptive fields covering the entire (but small) low-pass image. We test this model on a dataset of face images, which are highly non-stationary and contain large-scale geometric structures.\nRemarkably, denoising, super-resolution, and image synthesis results all demonstrate that these structures can be captured with significantly smaller conditioning neighborhoods than required by a Markov model implemented in the pixel domain. Our results show that score estimation for large complex images can be reduced to low-dimensional Markov conditional models across scales,  alleviating the curse of dimensionality. "}}
{"id": "XYCoZ78PcfL", "cdate": 1662002157089, "mdate": 1662002157089, "content": {"title": "Wavelet Score-Based Generative Modeling", "abstract": "Score-based generative models (SGMs) synthesize new data samples from Gaussian white noise by running a time-reversed Stochastic Differential Equation (SDE) whose drift coefficient depends on some probabilistic score. The discretization of such SDEs typically requires a large number of time steps and hence a high computational cost. This is because of ill-conditioning properties of the score that we analyze mathematically. We show that SGMs can be considerably accelerated, by factorizing the data distribution into a product of conditional probabilities of wavelet coefficients across scales. The resulting Wavelet Score-based Generative Model (WSGM) synthesizes wavelet coefficients with the same number of time steps at all scales, and its time complexity therefore grows linearly with the image size. This is proved mathematically over Gaussian distributions, and shown numerically over physical processes at phase transition and natural image datasets."}}
{"id": "xZmjH3Pm2BK", "cdate": 1652737452579, "mdate": null, "content": {"title": "Wavelet Score-Based Generative Modeling", "abstract": "Score-based generative models (SGMs) synthesize new data samples from Gaussian white noise by running a time-reversed Stochastic Differential Equation (SDE) whose drift coefficient depends on some probabilistic score. The discretization of such SDEs typically requires a large number of time steps and hence a high computational cost. This is because of ill-conditioning properties of the score that we analyze mathematically. Previous approaches have relied on multiscale generation to considerably accelerate SGMs. We explain how this acceleration results from an implicit factorization of the data distribution into a product of conditional probabilities of wavelet coefficients across scales. The resulting Wavelet Score-based Generative Model (WSGM) synthesizes wavelet coefficients with the same number of time steps at all scales, and its time complexity therefore grows linearly with the image size. This is proved mathematically for Gaussian distributions, and shown numerically for physical processes at phase transition and natural image datasets."}}
{"id": "jdgnofphrOi", "cdate": 1640995200000, "mdate": 1681649837271, "content": {"title": "Phase Collapse in Neural Networks", "abstract": ""}}
{"id": "MyJd-qFRwZ", "cdate": 1640995200000, "mdate": 1681649837282, "content": {"title": "Wavelet Score-Based Generative Modeling", "abstract": ""}}
{"id": "iPHLcmtietq", "cdate": 1632875620714, "mdate": null, "content": {"title": "Phase Collapse in Neural Networks", "abstract": "Deep convolutional classifiers linearly separate image classes and improve accuracy as depth increases. They progressively reduce the spatial dimension whereas the number of channels grows with depth. Spatial variability is therefore transformed into variability along channels. A fundamental challenge is to understand the role of non-linearities together with convolutional filters in this transformation. ReLUs with biases are often interpreted as thresholding operators that improve discrimination through sparsity. This paper demonstrates that it is a different mechanism called \\emph{phase collapse} which eliminates spatial variability while linearly separating classes. We show that collapsing the phases of complex wavelet coefficients is sufficient to reach the classification accuracy of ResNets of similar depths. However, replacing the phase collapses with thresholding operators that enforce sparsity considerably degrades the performance. We explain these numerical results by showing that the iteration of phase collapses progressively improves separation of classes, as opposed to thresholding non-linearities."}}
{"id": "g3TlPr7VJN", "cdate": 1609459200000, "mdate": 1681649837305, "content": {"title": "Phase Collapse in Neural Networks", "abstract": ""}}
{"id": "3fqWB0yxN3", "cdate": 1609459200000, "mdate": 1681649837235, "content": {"title": "Separation and Concentration in Deep Networks", "abstract": ""}}
