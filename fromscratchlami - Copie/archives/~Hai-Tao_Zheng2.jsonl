{"id": "R0G0S6S-f4z", "cdate": 1702539709599, "mdate": 1702539709599, "content": {"title": "SeqGPT: An Out-of-the-box Large Language Model for Open Domain Sequence Understanding", "abstract": "Large language models (LLMs) have shown impressive ability for open-domain NLP tasks. However, LLMs are sometimes too footloose for natural language understanding (NLU) tasks which always have restricted output and input format. Their performances on NLU tasks are highly related to prompts or demonstrations and are shown to be poor at performing several representative NLU tasks, such as event extraction and entity typing. To this end, we present SeqGPT, a bilingual (i.e., English and Chinese) open-source autoregressive model specially enhanced for open-domain natural language understanding. We express all NLU tasks with two atomic tasks, which define fixed instructions to restrict the input and output format but still ``open'' for arbitrarily varied label sets. The model is first instruction-tuned with extremely fine-grained labeled data synthesized by ChatGPT and then further fine-tuned by 233 different atomic tasks from 152 datasets across various domains. The experimental results show that SeqGPT has decent classification and extraction ability, and is capable of performing language understanding tasks on unseen domains. We also conduct empirical studies on the scaling of data and model size as well as on the transfer across tasks. Our model is accessible at this https URL."}}
{"id": "4Fi-5Jiyy5w", "cdate": 1663850356698, "mdate": null, "content": {"title": "Applying Second Order Optimization to Deep Transformers with Parameter-Efficient Tuning", "abstract": "Despite the theoretical superiority in convergence issues, second-order optimizers are generally not among the top choices for training large-scale neural networks due to their high computational and memory cost. Nevertheless, introduced in recent progress of parameter-efficient tuning is a new paradigm that large-scale pre-trained models (PTMs) can be adapted to specific tasks by optimizing a tiny proportion of parameters, which might hopefully change the game. We associate this new paradigm with the computational tractability of second-order optimizers and succeed in applying them to large PTMs that are from hundreds of millions to billions in scale. Beyond verifying their tractability, we further investigate the stability-influencing factors in the optimization process and propose accordingly a Newton-step-clipping approach in which we clip the update tensors rather than the gradients. This approach stabilizes the convergence by gating the magnitude of Newton steps along the optimization trajectories through the rugged landscapes of deep transformers. \nWe conduct extensive experiments across different downstream tasks, demonstrating that, when equipped with our Newton-step-clipping strategy, second-order optimizers, especially Kronecker-factored curvature approximation (K-FAC), can attain comparable and even superior results and faster convergence to those state-of-the-art bars implemented with AdamW.  Furthermore, we scale the model up to 3 billion parameters and validate the tractability and effectiveness of our method. This work is not only the first successful application of second-order optimization on such large-scale models but also sheds light on the possibility of further optimization-wise analysis on large-scale models in the future."}}
{"id": "btmflCmNxDl", "cdate": 1663850080149, "mdate": null, "content": {"title": "Self-Consistent Learning: Cooperation between Generators and Discriminators", "abstract": "Using generated data to improve the performance of downstream discriminative models has recently gained popularity due to the great development of pre-trained language models. In most previous studies, generative models and discriminative models are trained separately and thus could not adapt to any changes in each other. As a result, the generated samples can easily deviate from the real data distribution, while the improvement of the discriminative model quickly reaches saturation. Generative adversarial networks (GANs) train generative models via an adversarial process with discriminative models to achieve joint training. However, the training of standard GANs is notoriously unstable and often falls short of convergence. In this paper, to address these issues, we propose a $\\textit{self-consistent learning}$ framework, in which a discriminator and a generator are cooperatively trained in a closed-loop form. The discriminator and the generator enhance each other during multiple rounds of alternating training until a scoring consensus is reached. This framework proves to be easy to train and free from instabilities such as mode collapse and non-convergence. Extensive experiments on sentence semantic matching demonstrate the effectiveness of the proposed framework: the discriminator achieves 10+ AP of improvement on the zero-shot setting and new state-of-the-art performance on the full-data setting. "}}
{"id": "9CGiwZeCAd", "cdate": 1663849986839, "mdate": null, "content": {"title": "Closing the Performance Gap between Cumbersome and Lightweight Contrastive Models", "abstract": "While self-supervised contrastive learning has made continuous progress utilizing big models, the performance lags far behind when the model size decreases. A common practice to address this problem requires a two-stage training procedure, where a larger model is pretrained in a self-supervised manner first, then its representational knowledge is transferred to a smaller model in the second stage. Despite its effectiveness, this method is highly time-consuming and is inapplicable to some resource-limited scenarios. In this work, we are aiming at directly training a lightweight contrastive model with satisfactory performance in the absence of a pretrained teacher model. Specifically, by empirically exploring the training recipes (e.g., MLP, lower temperature, et al), we boost the accuracy of different lightweight models by a large margin. Besides, we observe that smaller models are more sensitive to noisy labels, and propose a smooth version of InfoNCE loss to alleviate this problem. With these combined techniques, we successfully improve the linear evaluation results from 36.3\\% to 62.3\\% of MobileNet-V3-Large and from 42.2\\% to 65.8\\% of EfficientNet-B0 on ImageNet, closing the accuracy gap to ResNet-50 which contains $5\\times$ parameters. These results suggest the feasibility to train lightweight self-supervised models without distillation.\n\n"}}
{"id": "t3U4b23czH", "cdate": 1640995200000, "mdate": 1674035145411, "content": {"title": "Learning from the Dictionary: Heterogeneous Knowledge Guided Fine-tuning for Chinese Spell Checking", "abstract": ""}}
{"id": "qHMZvrYC1K", "cdate": 1640995200000, "mdate": 1674035145493, "content": {"title": "Modeling Latent Autocorrelation for Session-based Recommendation", "abstract": ""}}
{"id": "ohDf_Anbbr6", "cdate": 1640995200000, "mdate": 1674034959706, "content": {"title": "The Past Mistake is the Future Wisdom: Error-driven Contrastive Probability Optimization for Chinese Spell Checking", "abstract": ""}}
{"id": "kuDQG5nWYYk", "cdate": 1640995200000, "mdate": 1674035145365, "content": {"title": "Linguistic Rules-Based Corpus Generation for Native Chinese Grammatical Error Correction", "abstract": ""}}
{"id": "cv1gFh9nzER", "cdate": 1640995200000, "mdate": 1674035145478, "content": {"title": "LordBERT: Embedding Long Text by Segment Ordering with BERT", "abstract": ""}}
{"id": "b6hi-WoT4-", "cdate": 1640995200000, "mdate": 1674035145487, "content": {"title": "Contextual Similarity is More Valuable than Character Similarity: Curriculum Learning for Chinese Spell Checking", "abstract": ""}}
