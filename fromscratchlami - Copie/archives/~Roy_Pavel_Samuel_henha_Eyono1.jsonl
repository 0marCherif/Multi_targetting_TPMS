{"id": "Tj-bRjuhpJ5", "cdate": 1676355290856, "mdate": null, "content": {"title": "MoDTI: Modular Framework For Evaluating Inductive Biases in DTI Modeling", "abstract": "Drug-Target Interaction (DTI) prediction is a critical problem in drug discovery, and machine learning (ML) has shown great potential in feature-based DTI prediction. However, selecting an appropriate ML architecture from the vast number of available biomolecular representations is challenging. To address this issue, we propose MoDTI, a modular framework that enables the exploration of three key inductive biases in DTI prediction: protein representation, multi-view learning, and modularity. We evaluate the impact of each inductive bias on DTI prediction performance and compare the performance of MoDTI against existing state-of-the-art models on multiple benchmarks. Our experiments with MoDTI provide valuable insights into the role of modularity, capacity, representation redundancy, and orthogonality in terms of generalization and interpretability. They also enable the provision of general guidelines for the rapid development of more accurate DTI models."}}
{"id": "SIv3B8Zaws", "cdate": 1609459200000, "mdate": 1682422557553, "content": {"title": "AUTOKD: Automatic Knowledge Distillation Into A Student Architecture Family", "abstract": "State-of-the-art results in deep learning have been improving steadily, in good part due to the use of larger models. However, widespread use is constrained by device hardware limitations, resulting in a substantial performance gap between state-of-the-art models and those that can be effectively deployed on small devices. While Knowledge Distillation (KD) theoretically enables small student models to emulate larger teacher models, in practice selecting a good student architecture requires considerable human expertise. Neural Architecture Search (NAS) appears as a natural solution to this problem but most approaches can be inefficient, as most of the computation is spent comparing architectures sampled from the same distribution, with negligible differences in performance. In this paper, we propose to instead search for a family of student architectures sharing the property of being good at learning from a given teacher. Our approach AutoKD, powered by Bayesian Optimization, explores a flexible graph-based search space, enabling us to automatically learn the optimal student architecture distribution and KD parameters, while being 20x more sample efficient compared to existing state-of-the-art. We evaluate our method on 3 datasets; on large images specifically, we reach the teacher performance while using 3x less memory and 10x less parameters. Finally, while AutoKD uses the traditional KD loss, it outperforms more advanced KD variants using hand-designed students."}}
{"id": "QH_2kU8yx6", "cdate": 1609459200000, "mdate": 1667377086017, "content": {"title": "CCN GAC Workshop: Issues with learning in biological recurrent neural networks", "abstract": "We provide a brief review of the common assumptions about biological learning with findings from experimental neuroscience and contrast them with the efficiency of gradient-based learning in recurrent neural networks. The key issues discussed in this review include: synaptic plasticity, neural circuits, theory-experiment divide, and objective functions. We conclude with recommendations for both theoretical and experimental neuroscientists when designing new studies that could help bring clarity to these issues."}}
{"id": "2234Pp-9ikZ", "cdate": 1601308363422, "mdate": null, "content": {"title": "Don't be picky, all students in the right family can learn from good teachers", "abstract": "State-of-the-art results in deep learning have been improving steadily, in good part due to the use of larger models. However, widespread use is constrained by device hardware limitations, resulting in a substantial performance gap between state-of-the-art models and those that can be effectively deployed on small devices. \n\nWhile Knowledge Distillation (KD) theoretically enables small student models to emulate larger teacher models, in practice selecting a good student architecture requires considerable human expertise. Neural Architecture Search (NAS) appears as a natural solution to this problem but most approaches can be inefficient, as most of the computation is spent comparing architectures sampled from the same distribution, with negligible differences in performance. \n\nIn this paper, we propose to instead search for a family of student architectures sharing the property of being good at learning from a given teacher. \nOur approach AutoKD, powered by Bayesian Optimization, explores a flexible graph-based search space, enabling us to automatically learn the optimal student architecture distribution and KD parameters, while being 20x more sample efficient compared to existing state-of-the-art. We evaluate our method on 3 datasets; on large images specifically, we reach the teacher performance while using 3x less memory and 10x less parameters. Finally, while AutoKD uses the traditional KD loss, it outperforms more advanced KD variants using hand-designed students."}}
{"id": "lc_LE6gX2NQ", "cdate": 1596479211129, "mdate": null, "content": {"title": "Dimensionality and flexibility of learning in biological recurrent neural networks", "abstract": "Title: Dimensionality and flexibility of learning in biological recurrent neural networks\nScientific Question: Does full-rank gradient descent accurately describe the dynamics of\nsynaptic plasticity in biological recurrent neural networks?"}}
{"id": "r1lgvNr324", "cdate": 1558103127901, "mdate": null, "content": {"title": "Function changes in the Backpropagation equation is equivalent to an implicit learning rate", "abstract": "The backpropagation algorithm is the de-facto standard for credit assignment in artificial neural networks due to its empirical results. Since its conception, variants of the backpropagation algorithm have emerged. More specifically, variants that leverage function changes in the backpropagation equations to satisfy their specific requirements. Feedback Alignment is one such example, which replaces the weight transpose matrix in the backpropagation equations with a random matrix in search of a more biologically plausible credit assignment algorithm. In this work, we show that function changes in the  backpropagation procedure is equivalent to adding an implicit learning rate to an artificial neural network. Furthermore, we learn activation function derivatives in the backpropagation equations to demonstrate early convergence in these artificial neural networks. Our work reports competitive performances with early convergence on MNIST and CIFAR10 on sufficiently large deep neural network architectures."}}
