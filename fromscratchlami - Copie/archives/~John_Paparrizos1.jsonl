{"id": "VuexVDZtd7", "cdate": 1640995200000, "mdate": 1671733766889, "content": {"title": "Theseus: Navigating the Labyrinth of Time-Series Anomaly Detection", "abstract": ""}}
{"id": "L1TgPCym1YL", "cdate": 1640995200000, "mdate": 1671733766903, "content": {"title": "Fast Adaptive Similarity Search through Variance-Aware Quantization", "abstract": "With the explosive growth of high-dimensional data, approximate methods emerge as promising solutions for nearest neighbor search. Among alternatives, quantization methods have gained attention due to the fast query responses and the low encoding and storage costs. Quantization methods decompose data dimensions into non-overlapping subspaces and encode data using a different dictionary per subspace. The state-of-the-art approach assigns dictionary sizes uniformly across subspaces while attempting to balance the relative importance of subspaces. Unfortunately, a uniform balance is not always achievable and may lead to unsatisfactory performance. Similarly, hardware-accelerated quantization methods may sacrifice accuracy to speed up the query execution. We propose a Variance-Aware Quantization (VAQ) method to encode data by intelligently adapting dictionary sizes to subspaces to alleviate these significant drawbacks. VAQ exploits intrinsic dimensionality reduction properties to derive the subspaces and only partially balances the importance of subspaces. Then, VAQ solves a constrained optimization problem to assign dictionary sizes proportionally to the importance of each subspace. In addition, VAQ accelerates the query execution by skipping data and subspaces through a hardware-oblivious algorithmic solution. To demonstrate the robustness of VAQ, we perform an extensive evaluation against quantization, hashing, and indexing methods using five large-scale benchmarking datasets. VAQ significantly outperforms the strongest hashing and quantization methods in accuracy while achieving up to 5\u00d7 speedup. Compared to the fastest but less accurate hardware-accelerated method, VAQ achieves a speedup@recall performance up to 14\u00d7. Importantly, a rigorous statistical comparison using over one hundred datasets reveals that VAQ significantly outperforms rival methods even with a half budget. Notably, VAQ's simple data skipping solution achieves competitive or better performance against index-based methods, highlighting the need for new indices for quantization methods."}}
{"id": "KrbeMvT9fL3", "cdate": 1640995200000, "mdate": 1671733766856, "content": {"title": "TSB-UAD: An End-to-End Benchmark Suite for Univariate Time-Series Anomaly Detection", "abstract": ""}}
{"id": "KhrqVux4SK", "cdate": 1640995200000, "mdate": 1671733766880, "content": {"title": "Volume Under the Surface: A New Accuracy Evaluation Measure for Time-Series Anomaly Detection", "abstract": ""}}
{"id": "EEkjsM370-", "cdate": 1622215875627, "mdate": null, "content": {"title": "GRAIL: Efficient Time-Series Representation Learning", "abstract": "The analysis of time series is becoming increasingly prevalent across scientific disciplines and industrial applications. The effectiveness and the scalability of time-series mining techniques critically depend on design choices for three components responsible for (i) representing; (ii) comparing; and(iii) indexing time series.  Unfortunately, these components have to date been investigated and developed independently, often resulting in mutually incompatible methods.  The lack of a unified approach has hindered progress towards fast and accurate analytics over massive time-series collections.  To address this major drawback, we present GRAIL, a generic framework to learn compact time-series representations that preserve the properties of a user-specified comparison function.   Given the comparison function,  GRAIL  (i)  extracts landmark time series using clustering; (ii) optimizes necessary parameters;  and (iii) exploits approximations for kernel methods to construct representations in linear time and space by expressing each time series as a  combination of the landmark time series.  We extensively evaluate GRAILfor querying,  classification,  clustering,  sampling,  and visualization of time series.  For these tasks, methods leveraging GRAIL\u2019s representations are significantly faster and at least as accurate as state-of-the-art methods operating over the raw time series.  GRAIL shows promise as a new primitive for highly accurate, yet scalable, time-series analysis."}}
{"id": "oKtRdkkmNb", "cdate": 1622215710388, "mdate": null, "content": {"title": "k-Shape: Efficient and Accurate Clustering of Time Series", "abstract": "The proliferation and ubiquity of temporal data across many disciplines has generated substantial interest in the analysis and mining of time series. Clustering is one of the most popular data mining methods, not only due to its exploratory power but also as a preprocessing step or subroutine for other techniques. In this paper, we present k-Shape, a novel algorithm for time-series clustering. k-Shape relies on a scalable iterative refinement procedure, which creates homogeneous and well-separated clusters. As its distance measure, k-Shape uses a normalized version of the cross-correlation measure in order to consider the shapes of time series while comparing them. Based on the properties of that distance measure, we develop a method to compute cluster centroids, which are used in every iteration to update the assignment of time series to clusters. To demonstrate the robustness of k-Shape, we perform an extensive experimental evaluation of our approach against partitional, hierarchical, and spectral clustering methods, with combinations of the most competitive distance measures. k-Shape outperforms all scalable approaches in terms of accuracy. Furthermore,k-Shape also outperforms all non-scalable (and hence impractical) combinations, with one exception that achieves similar accuracy results. However, unlike k-Shape, this combination requires tuning of its distance measure and is two orders of magnitude slower thank-Shape. Overall,k-Shape emerges as a domain-independent, highly accurate, and highly efficient clustering approach for time series with broad applications"}}
{"id": "xPY_melcsg", "cdate": 1609459200000, "mdate": 1671733766953, "content": {"title": "SAND in Action: Subsequence Anomaly Detection for Streams", "abstract": ""}}
{"id": "iTVmIvrGSf", "cdate": 1609459200000, "mdate": 1671733766933, "content": {"title": "Decomposed Bounded Floats for Fast Compression and Queries", "abstract": ""}}
{"id": "bHH3ewVFLw", "cdate": 1609459200000, "mdate": 1671733766960, "content": {"title": "VergeDB: A Database for IoT Analytics on Edge Devices", "abstract": ""}}
{"id": "PCK8xpRdfvV", "cdate": 1609459200000, "mdate": 1671733766983, "content": {"title": "Good to the Last Bit: Data-Driven Encoding with CodecDB", "abstract": "Columnar databases rely on specialized encoding schemes to reduce storage requirements. These encodings also enable efficient in-situ data processing. Nevertheless, many existing columnar databases are encoding-oblivious. When storing the data, these systems rely on a global understanding of the dataset or the data types to derive simple rules for encoding selection. Such rule-based selection leads to unsatisfactory performance. Specifically, when performing queries, the systems always decode data into memory, ignoring the possibility of optimizing access to encoded data. We develop CodecDB, an encoding-aware columnar database, to demonstrate the benefit of tightly-coupling the database design with the data encoding schemes. CodecDB chooses in a principled manner the most efficient encoding for a given data column and relies on encoding-aware query operators to optimize access to encoded data. Storage-wise, CodecDB achieves on average 90% accuracy for selecting the best encoding and improves the compression ratio by up to 40% compared to the state-of-the-art encoding selection solution. Query-wise, CodecDB is on average one order of magnitude faster than the latest open-source and commercial columnar databases on the TPC-H benchmark, and on average 3x faster than a recent research project on the Star-Schema Benchmark (SSB)."}}
