{"id": "yDlnAgbryeI", "cdate": 1640995200000, "mdate": 1681436639741, "content": {"title": "Exploring Motion and Appearance Information for Temporal Sentence Grounding", "abstract": ""}}
{"id": "vxp5TCiG8e", "cdate": 1640995200000, "mdate": 1668022535129, "content": {"title": "Memory-Guided Semantic Learning Network for Temporal Sentence Grounding", "abstract": "Temporal sentence grounding (TSG) is crucial and fundamental for video understanding. Although existing methods train well-designed deep networks with large amount of data, we find that they can easily forget the rarely appeared cases during training due to the off-balance data distribution, which influences the model generalization and leads to unsatisfactory performance. To tackle this issue, we propose a memory-augmented network, called Memory-Guided Semantic Learning Network (MGSL-Net), that learns and memorizes the rarely appeared content in TSG task. Specifically, our proposed model consists of three main parts: cross-modal interaction module, memory augmentation module, and heterogeneous attention module. We first align the given video-query pair by a cross-modal graph convolutional network, and then utilize memory module to record the cross-modal shared semantic features in the domain-specific persistent memory. During training, the memory slots are dynamically associated with both common and rare cases, alleviating the forgetting issue. In testing, the rare cases can thus be enhanced by retrieving the stored memories, leading to better generalization. At last, the heterogeneous attention module is utilized to integrate the enhanced multi-modal features in both video and query domains. Experimental results on three benchmarks show the superiority of our method on both effectiveness and efficiency, which substantially improves the accuracy not only on the entire dataset but also on the rare cases."}}
{"id": "j5l3am8hgiC", "cdate": 1640995200000, "mdate": 1681436639758, "content": {"title": "Skimming, Locating, then Perusing: A Human-Like Framework for Natural Language Video Localization", "abstract": ""}}
{"id": "i1_aHXVeTG", "cdate": 1640995200000, "mdate": 1681436639668, "content": {"title": "Reducing the Vision and Language Bias for Temporal Sentence Grounding", "abstract": ""}}
{"id": "IEnxT1ZjUq", "cdate": 1640995200000, "mdate": 1668022535141, "content": {"title": "Unsupervised Temporal Video Grounding with Deep Semantic Clustering", "abstract": "Temporal video grounding (TVG) aims to localize a target segment in a video according to a given sentence query. Though respectable works have made decent achievements in this task, they severely rely on abundant video-query paired data, which is expensive to collect in real-world scenarios. In this paper, we explore whether a video grounding model can be learned without any paired annotations. To the best of our knowledge, this paper is the first work trying to address TVG in an unsupervised setting. Considering there is no paired supervision, we propose a novel Deep Semantic Clustering Network (DSCNet) to leverage all semantic information from the whole query set to compose the possible activity in each video for grounding. Specifically, we first develop a language semantic mining module, which extracts implicit semantic features from the whole query set. Then, these language semantic features serve as the guidance to compose the activity in video via a video-based semantic aggregation module. Finally, we utilize a foreground attention branch to filter out the redundant background activities and refine the grounding results. To validate the effectiveness of our DSCNet, we conduct experiments on both ActivityNet Captions and Charades-STA datasets. The results demonstrate that our DSCNet achieves competitive performance, and even outperforms most weakly-supervised approaches."}}
{"id": "CQcEH5-J5Ku", "cdate": 1640995200000, "mdate": 1676696352567, "content": {"title": "Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding", "abstract": ""}}
{"id": "2WD_-U6LjGJ", "cdate": 1640995200000, "mdate": 1681436639817, "content": {"title": "Learning to Focus on the Foreground for Temporal Sentence Grounding", "abstract": ""}}
{"id": "xaJ-QtJugAR", "cdate": 1609459200000, "mdate": 1636860190753, "content": {"title": "Progressively Guide to Attend: An Iterative Alignment Framework for Temporal Sentence Grounding", "abstract": "Daizong Liu, Xiaoye Qu, Pan Zhou. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021."}}
{"id": "aSIw8gQwvaG", "cdate": 1609459200000, "mdate": 1635651606566, "content": {"title": "Context-Aware Biaffine Localizing Network for Temporal Sentence Grounding", "abstract": "This paper addresses the problem of temporal sentence grounding (TSG), which aims to identify the temporal boundary of a specific segment from an untrimmed video by a sentence query. Previous works either compare pre-defined candidate segments with the query and select the best one by ranking, or directly regress the boundary timestamps of the target segment. In this paper, we propose a novel localization framework that scores all pairs of start and end indices within the video simultaneously with a biaffine mechanism. In particular, we present a Context-aware Biaffine Localizing Network (CBLN) which incorporates both local and global contexts into features of each start/end position for biaffine-based localization. The local contexts from the adjacent frames help distinguish the visually similar appearance, and the global contexts from the entire video contribute to reasoning the temporal relation. Besides, we also develop a multi-modal self-attention module to provide fine-grained query-guided video representation for this biaffine strategy. Extensive experiments show that our CBLN significantly outperforms state-of-the-arts on three public datasets (ActivityNet Captions, TACoS, and Charades-STA), demonstrating the effectiveness of the proposed localization framework."}}
{"id": "ZTdCzhcb3UR", "cdate": 1609459200000, "mdate": 1635651606556, "content": {"title": "Spatiotemporal Graph Neural Network based Mask Reconstruction for Video Object Segmentation", "abstract": "This paper addresses the task of segmenting class-agnostic objects in semi-supervised setting. Although previous detection based methods achieve relatively good performance, these approaches extract the best proposal by a greedy strategy, which may lose the local patch details outside the chosen candidate. In this paper, we propose a novel spatiotemporal graph neural network (STG-Net) to reconstruct more accurate masks for video object segmentation, which captures the local contexts by utilizing all proposals. In the spatial graph, we treat object proposals of a frame as nodes and represent their correlations with an edge weight strategy for mask context aggregation. To capture temporal information from previous frames, we use a memory network to refine the mask of current frame by retrieving historic masks in a temporal graph. The joint use of both local patch details and temporal relationships allow us to better address the challenges such as object occlusions and missing. Without online learning and fine-tuning, our STG-Net achieves state-of-the-art performance on four large benchmarks, demonstrating the effectiveness of the proposed approach."}}
