{"id": "b1tvKVCtEbQ", "cdate": 1695999679762, "mdate": null, "content": {"title": "Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity", "abstract": "While deep learning (DL) models are state-of-the-art in text and image domains, they have not yet consistently outperformed Gradient Boosted Decision Trees (GBDTs) on tabular Learning-To-Rank (LTR) problems. Most of the recent performance gains attained by DL models in text and image tasks have used unsupervised pretraining, which exploits orders of magnitude more unlabeled data than labeled data. To the best of our knowledge, unsupervised pretraining has not been applied to the LTR problem, which often produces vast amounts of unlabeled data. In this work, we study whether unsupervised pretraining can improve LTR performance over GBDTs and other non-pretrained models. Using simple design choices--including SimCLR-Rank, our ranking-specific modification of SimCLR (an unsupervised pretraining method for images)--we produce pretrained deep learning models that soundly outperform GBDTs (and other non-pretrained models) in the case where labeled data is vastly outnumbered by unlabeled data. We also show that pretrained models also often achieve significantly better robustness than non-pretrained models (GBDTs or DL models) in ranking outlier data."}}
{"id": "DUQEMMA70O", "cdate": 1661227054782, "mdate": 1661227054782, "content": {"title": "Bring Your Own Algorithm for Optimal Differentially Private Stochastic Minimax Optimization", "abstract": "We study differentially private (DP) algorithms for smooth stochastic minimax optimization, with stochastic minimization as a byproduct. The holy grail of these settings is to guarantee the optimal trade-off between the privacy and the excess population loss, using an algorithm with a linear time-complexity in the number of training samples. We provide a general framework for solving differentially private stochastic minimax optimization (DP-SMO) problems, which enables the practitioners to bring their own base optimization algorithm and use it as a black-box to obtain the near-optimal privacy-loss trade-off. Our framework is inspired from the recently proposed Phased-ERM method [20] for nonsmooth differentially private stochastic convex optimization (DP-SCO), which exploits the stability of the empirical risk minimization (ERM) for the privacy guarantee. The flexibility of our approach enables us to sidestep the requirement that the base algorithm needs to have bounded sensitivity, and allows the use of sophisticated variance-reduced accelerated methods to achieve near-linear time-complexity. To the best of our knowledge, these are the first linear-time optimal algorithms, up to logarithmic factors, for smooth DP-SMO when the objective is (strongly-)convex-(strongly-)concave. Additionally, based on our flexible framework, we derive a new family of near-linear time algorithms for smooth DP-SCO with optimal privacy-loss trade-offs for a wider range of smoothness parameters compared to previous algorithms."}}
{"id": "fRbvozXEGTb", "cdate": 1652737695439, "mdate": null, "content": {"title": "Bring Your Own Algorithm for Optimal Differentially Private Stochastic Minimax Optimization", "abstract": "We study differentially private (DP) algorithms for smooth stochastic minimax optimization, with stochastic minimization as a byproduct. The holy grail of these settings is to guarantee the optimal trade-off between the privacy and the excess population loss, using an algorithm with a linear time-complexity in the number of training samples. We provide a general framework for solving differentially private stochastic minimax optimization (DP-SMO) problems, which enables the practitioners to bring their own base optimization algorithm and use it as a black-box to obtain the near-optimal privacy-loss trade-off. Our framework is inspired from the recently proposed Phased-ERM method [22] for nonsmooth differentially private stochastic convex optimization (DP-SCO), which exploits the stability of the empirical risk minimization (ERM) for the privacy guarantee. The flexibility of our approach enables us to sidestep the requirement that the base algorithm needs to have bounded sensitivity, and allows the use of sophisticated variance-reduced accelerated methods to achieve near-linear time-complexity. To the best of our knowledge, these are the first near-linear time algorithms with near-optimal guarantees on the population duality gap for smooth DP-SMO, when the objective is (strongly-)convex--(strongly-)concave. Additionally, based on our flexible framework, we enrich the family of near-linear time algorithms for smooth DP-SCO with the near-optimal privacy-loss trade-off."}}
{"id": "OzvlColmjem", "cdate": 1650860694526, "mdate": null, "content": {"title": "Lifted Primal-Dual Method for Bilinearly Coupled Smooth Minimax Optimization", "abstract": "We study the bilinearly coupled minimax problem: $\\min_{x} \\max_{y} f(x) + y^\\top A x - h(y)$, where $f$ and $h$ are both strongly convex smooth functions and admit first-order gradient oracles. Surprisingly, no known first-order algorithms have hitherto achieved the lower complexity bound of $\\Omega((\\sqrt{\\frac{L_x}{\\mu_x}} + \\frac{\\|A\\|}{\\sqrt{\\mu_x \\mu_y}} + \\sqrt{\\frac{L_y}{\\mu_y}}) \\log(\\frac1{\\varepsilon}))$ for solving this problem up to an $\\varepsilon$ primal-dual gap in the general parameter regime, where $L_x, L_y,\\mu_x,\\mu_y$ are the corresponding  smoothness and strongly convexity constants. \n\nWe close this gap by devising the first optimal algorithm, the Lifted Primal-Dual (LPD) method. Our method lifts the objective into an extended form that allows both the smooth terms and the bilinear term to be handled optimally and seamlessly with the same primal-dual framework. Besides optimality, our method yields a desirably simple single-loop algorithm that uses only one gradient oracle call per iteration. Moreover, when $f$ is just convex, the same algorithm applied to a smoothed objective achieves the nearly optimal iteration complexity. We also provide a direct single-loop algorithm, using the LPD method, that achieves the iteration complexity of $O(\\sqrt{\\frac{L_x}{\\varepsilon}} + \\frac{\\|A\\|}{\\sqrt{\\mu_y \\varepsilon}} + \\sqrt{\\frac{L_y}{\\varepsilon}})$. Numerical experiments on quadratic minimax problems and policy evaluation problems further demonstrate the fast convergence of our algorithm in practice."}}
{"id": "zo2wPrZuLko", "cdate": 1650860483015, "mdate": null, "content": {"title": "FedChain: Chained Algorithms for Near-optimal Communication Cost in Federated Learning", "abstract": "Federated learning (FL) aims to minimize the communication complexity of training a model over heterogeneous data distributed across many clients. A common approach is local methods, where clients take multiple optimization steps over local data before communicating with the server (e.g., FedAvg).  Local methods can exploit similarity between clients' data. However, in existing analyses, this comes at the cost of slow convergence in terms of the dependence on the number of communication rounds R.  On the other hand, global methods, where clients simply return a gradient vector in each round (e.g., SGD), converge faster in terms of R but fail to exploit the similarity between clients even when clients are homogeneous.  We propose FedChain, an algorithmic framework that combines the strengths of local methods and global methods to achieve fast convergence in terms of R while  leveraging the similarity between clients.  Using FedChain, we instantiate algorithms that improve upon previously known rates in the general convex and PL settings, and are near-optimal (via an algorithm-independent lower bound that we show) for problems that satisfy strong convexity.  Empirical results support this theoretical gain over existing methods. "}}
{"id": "ZaVVVlcdaN", "cdate": 1632875538007, "mdate": null, "content": {"title": "FedChain: Chained Algorithms for Near-optimal Communication Cost in Federated Learning", "abstract": "Federated learning (FL) aims to minimize the communication complexity of training a model over heterogeneous data distributed across many clients. A common approach is local methods, where clients take multiple optimization steps over local data before communicating with the server (e.g., FedAvg).  Local methods can exploit similarity between clients' data. However, in existing analyses, this comes at the cost of slow convergence in terms of the dependence on the number of communication rounds R.  On the other hand, global methods, where clients simply return a gradient vector in each round (e.g., SGD), converge faster in terms of R but fail to exploit the similarity between clients even when clients are homogeneous.  We propose FedChain, an algorithmic framework that combines the strengths of local methods and global methods to achieve fast convergence in terms of R while  leveraging the similarity between clients.  Using FedChain, we instantiate algorithms that improve upon previously known rates in the general convex and PL settings, and are near-optimal (via an algorithm-independent lower bound that we show) for problems that satisfy strong convexity.  Empirical results support this theoretical gain over existing methods. "}}
{"id": "48LtSAkxjiX", "cdate": 1621630056953, "mdate": null, "content": {"title": "Statistically and Computationally Efficient Linear Meta-representation Learning", "abstract": "In typical few-shot learning, each task is not equipped with enough data to be learned in isolation. To cope with such data scarcity, meta-representation learning methods train across many related tasks to find a shared (lower-dimensional) representation of the data where all tasks can be solved accurately. It is hypothesized that any new arriving tasks can be rapidly trained on this low-dimensional representation using only a few samples. Despite the practical successes of this approach, its statistical and computational properties are less understood. Moreover, the prescribed algorithms in these studies have little resemblance to those used in practice or they are computationally intractable. To understand and explain the success of popular meta-representation learning approaches such as ANIL, MetaOptNet, R2D2, and OML, we study a alternating gradient-descent minimization (AltMinGD) method (and its variant alternating minimization (AltMin)) which underlies the aforementioned methods. For a simple but canonical setting of shared linear representations, we show that AltMinGD achieves nearly-optimal estimation error, requiring only $\\Omega(\\mathrm{polylog}\\,d)$ samples per task. This agrees with the observed efficacy of this algorithm in the practical few-shot learning scenarios."}}
{"id": "XPYMHtY7nh", "cdate": 1620373317186, "mdate": null, "content": {"title": "Sub-modularity of waterfilling with applications to online basestation allocation", "abstract": "We show that the popular water-filling algorithm for maximizing the mutual information in parallel Gaussian channels is sub-modular. The sub-modularity of water-filling algorithm is then used to derive online basestation allocation algorithms, where mobile users are assigned to one of many possible basestations immediately and irrevocably upon arrival without knowing the future user information. The goal of the allocation is to maximize the sum-rate of the system under power allocation at each basestation. We present online algorithms with competitive ratio of at most 2 when compared to offline algorithms that have knowledge of all future user arrivals."}}
{"id": "jNYmowP1twd", "cdate": 1620373186183, "mdate": null, "content": {"title": "Optimal Nonsmooth Frank-Wolfe method for Stochastic Regret Minimization", "abstract": "The current best-known algorithm for convex constrained nonsmooth online stochastic regret minimization using a Linear Minimization Oracle (LMO, a la Frank-Wolfe) and a Stochastic First-order Oracle (SFO) achieves a regret of O(K^{3/4}), where K is the number of iterations [26]. We provide two novel single-loop nonsmooth Frank-Wolfe methods, P-MOLES & PD-MOLES, which achieve the nearly-optimal online stochastic (non-adversarial) regret of O(K^{1/2} ln(K)) for this problem with a Lipschitz continuous function. Our methods only need a mild assumption that the function remains Lipschitz and can be queried on a slightly larger neighborhood around the constraint set. Further, the last-iterates of our methods are guaranteed to be eps-suboptimal feasible solutions just after using O(eps^{-2}) LMO calls and O(eps^{-2}) SFO calls. These offline oracle calls complexities are optimal, and compared to the state-of-the-art offline method, MOLES [48], P-MOLES & PDMOLES have the added advantage of being single-loop methods which use only one LMO call and one SFO call per iteration. This kind of simplicity is much preferred in practice, especially in the online setting."}}
{"id": "jBnFCTcLWD-", "cdate": 1620372849657, "mdate": null, "content": {"title": "Learning from comparisons and choices", "abstract": "hen tracking user-specific online activities, each user's preference is revealed in the form of choices and comparisons. For example, a user's purchase history is a record of her choices, i.e. which item was chosen among a subset of offerings. A user's preferences can be observed either explicitly as in movie ratings or implicitly as in viewing times of news articles. Given such individualized ordinal data in the form of comparisons and choices, we address the problem of collaboratively learning representations of the users and the items. The learned features can be used to predict a user's preference of an unseen item to be used in recommendation systems. This also allows one to compute similarities among users and items to be used for categorization and search. Motivated by the empirical successes of the MultiNomial Logit (MNL) model in marketing and transportation, and also more recent successes in word embedding and crowdsourced image embedding, we pose this problem as learning the MNL model parameters that best explain the data. We propose a convex relaxation for learning the MNL model, and show that it is minimax optimal up to a logarithmic factor by comparing its performance to a fundamental lower bound. This characterizes the minimax sample complexity of the problem, and proves that the proposed estimator cannot be improved upon other than by a logarithmic factor. Further, the analysis identifies how the accuracy depends on the topology of sampling via the spectrum of the sampling graph. This provides a guideline for designing surveys when one can choose which items are to be compared. This is accompanied by numerical simulations on synthetic and real data sets, confirming our theoretical predictions."}}
