{"id": "1kIZiRelqFt", "cdate": 1654539973041, "mdate": null, "content": {"title": "FLAIR: Federated Learning Annotated Image Repository", "abstract": "Cross-device federated learning is an emerging machine learning (ML) paradigm where a large population of devices collectively train an ML model while the data remains on the devices.\nThis research field has a unique set of practical challenges, and to systematically make advances, new datasets curated to be compatible with this paradigm are needed.\nExisting federated learning benchmarks in the image domain do not accurately capture the scale and heterogeneity of many real-world use cases. \nWe introduce FLAIR, a challenging large-scale annotated image dataset for multi-label classification suitable for federated learning.\nFLAIR has 429,078 images from  51,414  Flickr users and captures many of the intricacies typically encountered in federated learning, such as heterogeneous user data and a long-tailed label distribution.\nWe implement multiple baselines in different learning setups for different tasks on this dataset. \nWe believe FLAIR can serve as a challenging benchmark for advancing the state-of-the art in federated learning.\nDataset access and the code for the benchmark are available at https://github.com/apple/ml-flair.\n"}}
{"id": "rhz7nqYfF-q", "cdate": 1647024532047, "mdate": null, "content": {"title": "Training a Tokenizer for Free with Private Federated Learning", "abstract": "Federated learning with differential privacy, i.e. private federated learning (PFL), makes it possible to train models on private data distributed across users' devices without harming privacy. PFL is efficient for models, such as neural networks, that have a fixed number of parameters, and thus a fixed-dimensional gradient vector. Such models include neural-net language models, but not tokenizers, the topic of this work. Training a tokenizer requires frequencies of words from an unlimited vocabulary, and existing methods for finding an unlimited vocabulary need a separate privacy budget.\n\nA workaround is to train the tokenizer on publicly available data. However, in this paper we first show that a tokenizer trained on mismatched data results in worse model performance compared to a privacy-violating \"oracle\" tokenizer that accesses user data, with perplexity increasing by 20%. We also show that sub-word tokenizers are better suited to the federated context than word-level ones, since they can encode new words, though with more tokens per word.\n\nSecond, we propose a novel method to obtain a tokenizer without using any additional privacy budget. During private federated learning of the language model, we sample from the model, train a new tokenizer on the sampled sequences, and update the model embeddings. We then continue private federated learning, and obtain performance within 1% of the \"oracle\" tokenizer. We show that, since this process trains the tokenizer on the server using data for which the privacy loss has already been accounted for, our method spends no additional privacy budget.\n"}}
{"id": "S1lBTerYwH", "cdate": 1569439932636, "mdate": null, "content": {"title": "Generalized Zero-shot ICD Coding", "abstract": "The International Classification of Diseases (ICD) is a list of classification codes for the diagnoses. Automatic ICD coding is in high demand as the manual coding can be labor-intensive and error-prone. It is a multi-label text classification task with extremely long-tailed label distribution, making it difficult to perform fine-grained classification on both frequent and zero-shot codes at the same time. In this paper, we propose a latent feature generation framework for generalized zero-shot ICD coding, where we aim to improve the prediction on codes that have no labeled data without compromising the performance on seen codes. Our framework generates pseudo features conditioned on the ICD code descriptions and exploits the ICD code hierarchical structure. To guarantee the semantic consistency between the generated features and real features, we reconstruct the keywords in the input documents that are related to the conditioned ICD codes. To the best of our knowledge, this works represents the first one that proposes an adversarial generative model for the generalized zero-shot learning on multi-label text classification. Extensive experiments demonstrate the effectiveness of our approach. On the public MIMIC-III dataset, our methods improve the F1 score from nearly 0 to 20.91% for the zero-shot codes, and increase the AUC score by 3% (absolute improvement) from previous state of the art. We also show that the framework improves the performance on few-shot codes."}}
{"id": "SJeNz04tDS", "cdate": 1569439244007, "mdate": null, "content": {"title": "Overlearning Reveals Sensitive Attributes", "abstract": "``\"Overlearning'' means that a model trained for a seemingly simple\nobjective implicitly learns to recognize attributes and concepts that are\n(1) not part of the learning objective, and (2) sensitive from a privacy\nor bias perspective.  For example, a binary gender classifier of facial\nimages also learns to recognize races, even races that are\nnot represented in the training data, and identities.\n\nWe demonstrate overlearning in several vision and NLP models and analyze\nits harmful consequences.  First, inference-time representations of an\noverlearned model reveal sensitive attributes of the input, breaking\nprivacy protections such as model partitioning.  Second, an overlearned\nmodel can be \"`re-purposed'' for a different, privacy-violating task\neven in the absence of the original training data.\n\nWe show that overlearning is intrinsic for some tasks and cannot be\nprevented by censoring unwanted attributes.  Finally, we investigate\nwhere, when, and why overlearning happens during model training."}}
