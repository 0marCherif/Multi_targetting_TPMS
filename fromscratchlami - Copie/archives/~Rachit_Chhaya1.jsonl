{"id": "_ZWURLc900", "cdate": 1640995200000, "mdate": 1672034161102, "content": {"title": "On Coresets for Fair Regression and Individually Fair Clustering", "abstract": "In this paper we present coresets for Fair Regression with Statistical Parity (SP) constraints and for Individually Fair Clustering. Due to the fairness constraints, the classical coreset definition is not enough for these problems. We first define coresets for both the problems. We show that to obtain such coresets, it is sufficient to sample points based on the probabilities dependent on combination of sensitivity score and a carefully chosen term according to the fairness constraints. We give provable guarantees with relative error in preserving the cost and a small additive error in preserving fairness constraints for both problems. Since our coresets are much smaller in size as compared to $n$, the number of points, they can give huge benefits in computational costs (from polynomial to polylogarithmic in $n$), especially when $n \\gg d$, where $d$ is the input dimension. We support our theoretical claims with experimental evaluations."}}
{"id": "RByD8uuRnJS", "cdate": 1640995200000, "mdate": 1695959911605, "content": {"title": "Online Coresets for Parameteric and Non-Parametric Bregman Clustering", "abstract": "We present algorithms that create coresets in an online setting for clustering problems based on a wide subset of Bregman divergences. Notably, our coresets have a small additive error, similar in magnitude to the gap between expected and empirical loss (Bachem et. al. 2017), and take update time $O(d)$ for every incoming point where $d$ is the dimension of the point. Our first algorithm gives online coresets of size $\\tilde{O}(\\mbox{poly}(k,d,\\epsilon,\\mu))$ for $k$-clusterings according to any $\\mu$-similar Bregman divergence. We further extend this algorithm to show the existence of non-parametric coresets, where the coreset size is independent of $k$, the number of clusters, for the same subclass of Bregman divergences. Our non-parametric coresets also function as coresets for non-parametric versions of the Bregman clustering like DP-Means. While these coresets provide additive error guarantees, they are significantly smaller for high dimensional data than the (relative-error) coresets obtained in (Bachem et. al 2015) for DP-Means--- for the input of size $n$ our coresets grow as $O(\\log n)$ while being independent of $d$ as opposed to $O(d^d)$ for points in $\\~R^d$ (Bachem et. al 2015). We also present experiments to compare the performance of our algorithms with other sampling techniques."}}
{"id": "aUy24riCgiM", "cdate": 1577836800000, "mdate": null, "content": {"title": "Streaming Coresets for Symmetric Tensor Factorization", "abstract": "Factorizing tensors has recently become an important optimization module in a number of machine learning pipelines, especially in latent variable models. We show how to do this efficiently in the streaming setting. Given a set of $n$ vectors, each in $\\mathbb{R}^d$, we present algorithms to select a sublinear number of these vectors as coreset, while guaranteeing that the CP decomposition of the $p$-moment tensor of the coreset approximates the corresponding decomposition of the $p$-moment tensor computed from the full data. We introduce two novel algorithmic techniques: online filtering and kernelization. Using these two, we present six algorithms that achieve different tradeoffs of coreset size, update time and working space, beating or matching various state of the art algorithms. In the case of matrices ($2$-ordered tensor), our online row sampling algorithm guarantees $(1 \\pm \\epsilon)$ relative error spectral approximation. We show applications of our algorithms in learning single topic modeling."}}
{"id": "W7RIxV6NBlM", "cdate": 1577836800000, "mdate": null, "content": {"title": "Streaming Coresets for Symmetric Tensor Factorization", "abstract": "Factorizing tensors has recently become an important optimization module in a number of machine learning pipelines, especially in latent variable models. We show how to do this efficiently in the s..."}}
{"id": "Qy5B1SObAhY", "cdate": 1577836800000, "mdate": null, "content": {"title": "On Coresets For Regularized Regression", "abstract": "We study the effect of norm based regularization on the size of coresets for regression problems. Specifically, given a matrix $ \\mathbf{A} \\in {\\mathbb{R}}^{n \\times d}$ with $n\\gg d$ and a vector $\\mathbf{b} \\in \\mathbb{R} ^ n $ and $\\lambda > 0$, we analyze the size of coresets for regularized versions of regression of the form $\\|\\mathbf{Ax}-\\mathbf{b}\\|_p^r + \\lambda\\|{\\mathbf{x}}\\|_q^s$ . Prior work has shown that for ridge regression (where $p,q,r,s=2$) we can obtain a coreset that is smaller than the coreset for the unregularized counterpart i.e. least squares regression (Avron et al). We show that when $r \\neq s$, no coreset for regularized regression can have size smaller than the optimal coreset of the unregularized version. The well known lasso problem falls under this category and hence does not allow a coreset smaller than the one for least squares regression. We propose a modified version of the lasso problem and obtain for it a coreset of size smaller than the least square regression. We empirically show that the modified version of lasso also induces sparsity in solution, similar to the original lasso. We also obtain smaller coresets for $\\ell_p$ regression with $\\ell_p$ regularization. We extend our methods to multi response regularized regression. Finally, we empirically demonstrate the coreset performance for the modified lasso and the $\\ell_1$ regression with $\\ell_1$ regularization."}}
{"id": "FzWNh5SQLxD", "cdate": 1577836800000, "mdate": null, "content": {"title": "Online Coresets for Clustering with Bregman Divergences", "abstract": "We present algorithms that create coresets in an online setting for clustering problems according to a wide subset of Bregman divergences. Notably, our coresets have a small additive error, similar in magnitude to the lightweight coresets Bachem et. al. 2018, and take update time $O(d)$ for every incoming point where $d$ is dimension of the point. Our first algorithm gives online coresets of size $\\tilde{O}(\\mbox{poly}(k,d,\\epsilon,\\mu))$ for $k$-clusterings according to any $\\mu$-similar Bregman divergence. We further extend this algorithm to show existence of a non-parametric coresets, where the coreset size is independent of $k$, the number of clusters, for the same subclass of Bregman divergences. Our non-parametric coresets are larger by a factor of $O(\\log n)$ ($n$ is number of points) and have similar (small) additive guarantee. At the same time our coresets also function as lightweight coresets for non-parametric versions of the Bregman clustering like DP-Means. While these coresets provide additive error guarantees, they are also significantly smaller (scaling with $O(\\log n)$ as opposed to $O(d^d)$ for points in $\\~R^d$) than the (relative-error) coresets obtained in Bachem et. al. 2015 for DP-Means. While our non-parametric coresets are existential, we give an algorithmic version under certain assumptions."}}
{"id": "7HCMqIYWDBQ", "cdate": 1577836800000, "mdate": null, "content": {"title": "On Coresets for Regularized Regression", "abstract": "We study the effect of norm based regularization on the size of coresets for regression problems. Specifically, given a matrix $ \\mathbf{A} \\in {\\mathbb{R}}^{n \\times d}$ with $n\\gg d$ and a vector..."}}
