{"id": "VqDrqeQ8-C4", "cdate": 1663849839037, "mdate": null, "content": {"title": "Smooth-Reduce: Leveraging Patches for Improved Certified Robustness", "abstract": "Randomized smoothing (RS) has been shown to be a fast, scalable technique for certifying the robustness of deep neural network classifiers. However, methods based on RS require augmenting data with large amounts of noise, which leads to significant drops in accuracy. We propose a training-free, modified smoothing approach, Smooth-Reduce, that leverages patching and aggregation to provide improved classifier certificates. Our algorithm classifies overlapping patches extracted from an input image, and aggregates the predicted logits to certify a larger radius around the input. We study two aggregation schemes --- max and mean --- and show that both approaches provide better certificates in terms of certified accuracy, average certified radii and abstention rates as compared to concurrent approaches. We also provide theoretical guarantees for such certificates, and empirically show significant improvements over other randomized smoothing methods that require expensive retraining. Further, we extend our approach to videos and provide meaningful certificates for video classifiers."}}
{"id": "lFqjzuefwB9", "cdate": 1624022581363, "mdate": null, "content": {"title": "Empirical robustification of pre-trained classifiers", "abstract": "Most pre-trained classifiers, though they may work extremely well on the domain they were trained upon, are not trained in a robust fashion, and therefore are sensitive to adversarial attacks.  A recent technique, denoised-smoothing, demonstrated that it was possible to create certifiably robust classifiers from a pre-trained classifier (without any retraining) by pre-pending a denoising network and wrapping the entire pipeline within randomized smoothing.  However, this is a costly procedure, which requires multiple queries due to the randomized smoothing element, and which ultimately is very dependent on the quality of the denoiser.  In this paper, we demonstrate that a more conventional \u201cadversarial training\u201d approach also works when applied to this robustification process.   Specifically, we show that by training an image-to-image translation model, prepended to a pre-trained classifier, with losses that optimize for both the fidelity of the image reconstruction and the adversarial performance of the end-to-end system, we can robustify pre-trained classifiers to a higher empirical degree of accuracy than denoised smoothing.  Further, these robustifers are also transferable to some degree across multiple classifiers and even some architectures, illustrating that in some real sense they are removing the \u201cadversarial manifold\u201d from the input data, a task that has traditionally been very challenging for \u201cconventional\u201d preprocessing methods."}}
{"id": "t_GIuP-zs1_", "cdate": 1619546341383, "mdate": null, "content": {"title": "A Systematic Evaluation of Transfer Learning and Pseudo-labeling with BERT-based Ranking Models", "abstract": "Due to high annotation costs, making the best use of existing\nhuman-created training data is an important research direction.\nWe, therefore, carry out a systematic evaluation of transferability\nof BERT-based neural ranking models across five English datasets.\nPrevious studies focused primarily on zero-shot and few-shot transfer from a large dataset to a dataset with a small number of queries.\nIn contrast, each of our collections has a substantial number of\nqueries, which enables a full-shot evaluation mode and improves\nreliability of our results. Furthermore, since source datasets licences\noften prohibit commercial use, we compare transfer learning to\ntraining on pseudo-labels generated by a BM25 scorer. We find that\ntraining on pseudo-labels\u2014possibly with subsequent fine-tuning\nusing a modest number of annotated queries\u2014can produce a competitive or better model compared to transfer learning. However,\nthere is a need to improve the stability and/or effectiveness of the\nfew-shot training, which, in some cases, can degrade performance\nof a pretrained model."}}
{"id": "TWxgWQbcic2", "cdate": 1619546211012, "mdate": null, "content": {"title": "Exploring Classic and Neural Lexical Translation Models for Information Retrieval: Interpretability, Effectiveness, and Efficiency BenefitsL", "abstract": "We study the utility of the lexical translation model (IBM Model 1) for English text retrieval, in particular, its neural variants that are trained end-to-end. We use the neural Model1 as an aggregator layer applied to context-free or contextualized query/document embeddings. This new approach to design a neural ranking system has benefits for effectiveness, efficiency, and interpretability. Specifically, we show that adding an interpretable neural Model 1 layer on top of BERT-based contextualized embeddings (1) does not decrease accuracy and/or efficiency; and (2) may overcome the limitation on the maximum sequence length of existing BERT models. The context-free neural Model 1 is less effective than a BERT-based ranking model, but it can run efficiently on a CPU (without expensive index-time precomputation or query-time operations on large tensors). Using Model 1 we produced best neural and non-neural runs on the MS MARCO document ranking leaderboard in late 2020."}}
{"id": "QhykJQYnvZw", "cdate": 1577836800000, "mdate": null, "content": {"title": "SberQuAD - Russian Reading Comprehension Dataset: Description and Analysis", "abstract": "The paper presents SberQuAD \u2013 a large Russian reading comprehension (RC) dataset created similarly to English SQuAD. SberQuAD contains about 50K question-paragraph-answer triples and is seven times larger compared to the next competitor. We provide its description, thorough analysis, and baseline experimental results. We scrutinized various aspects of the dataset that can have impact on the task performance: question/paragraph similarity, misspellings in questions, answer structure, and question types. We applied five popular RC models to SberQuAD and analyzed their performance. We believe our work makes an important contribution to research in multilingual question answering."}}
{"id": "Ch1wvqpc-oD", "cdate": 1577836800000, "mdate": null, "content": {"title": "Which BM25 Do You Mean? A Large-Scale Reproducibility Study of Scoring Variants", "abstract": "When researchers speak of BM25, it is not entirely clear which variant they mean, since many tweaks to Robertson et al.\u2019s original formulation have been proposed. When practitioners speak of BM25, they most likely refer to the implementation in the Lucene open-source search library. Does this ambiguity \u201cmatter\u201d? We attempt to answer this question with a large-scale reproducibility study of BM25, considering eight variants. Experiments on three newswire collections show that there are no significant effectiveness differences between them, including Lucene\u2019s often maligned approximation of document length. As an added benefit, our empirical approach takes advantage of databases for rapid IR prototyping, which validates both the feasibility and methodological advantages claimed in previous work."}}
{"id": "BEOA5dq5MyW", "cdate": 1577836800000, "mdate": null, "content": {"title": "Traditional IR rivals neural models on the MS~MARCO Document Ranking Leaderboard", "abstract": "This short document describes a traditional IR system that achieved MRR@100 equal to 0.298 on the MS MARCO Document Ranking leaderboard (on 2020-12-06). Although inferior to most BERT-based models, it outperformed several neural runs (as well as all non-neural ones), including two submissions that used a large pretrained Transformer model for re-ranking. We provide software and data to reproduce our results."}}
{"id": "5eNJWLha8t3", "cdate": 1577836800000, "mdate": null, "content": {"title": "Flexible retrieval with NMSLIB and FlexNeuART", "abstract": "Our objective is to introduce to the NLP community an existing k-NN search library NMSLIB, a new retrieval toolkit FlexNeuART, as well as their integration capabilities. NMSLIB, while being one the fastest k-NN search libraries, is quite generic and supports a variety of distance/similarity functions. Because the library relies on the distance-based structure-agnostic algorithms, it can be further extended by adding new distances. FlexNeuART is a modular, extendible and flexible toolkit for candidate generation in IR and QA applications, which supports mixing of classic and neural ranking signals. FlexNeuART can efficiently retrieve mixed dense and sparse representations (with weights learned from training data), which is achieved by extending NMSLIB. In that, other retrieval systems work with purely sparse representations (e.g., Lucene), purely dense representations (e.g., FAISS and Annoy), or only perform mixing at the re-ranking stage."}}
{"id": "yYqojQFaFn", "cdate": 1546300800000, "mdate": null, "content": {"title": "SberQuAD - Russian Reading Comprehension Dataset: Description and Analysis", "abstract": "SberQuAD -- a large scale analog of Stanford SQuAD in the Russian language - is a valuable resource that has not been properly presented to the scientific community. We fill this gap by providing a description, a thorough analysis, and baseline experimental results."}}
{"id": "YNkBGWua4Iy", "cdate": 1546300800000, "mdate": null, "content": {"title": "Accurate and Fast Retrieval for Complex Non-metric Data via Neighborhood Graphs", "abstract": "We demonstrate that a graph-based search algorithm\u2014relying on the construction of an approximate neighborhood graph\u2014can directly work with challenging non-metric and/or non-symmetric distances without resorting to metric-space mapping and/or distance symmetrization, which, in turn, lead to substantial performance degradation. Although the straightforward metrization and symmetrization is usually ineffective, we find that constructing an index using a modified, e.g., symmetrized, distance can improve performance. This observation paves a way to a new line of research of designing index-specific graph-construction distance functions."}}
