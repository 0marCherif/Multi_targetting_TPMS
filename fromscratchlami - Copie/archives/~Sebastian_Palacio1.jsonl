{"id": "SyZoc6bubr", "cdate": 1514764800000, "mdate": null, "content": {"title": "What Do Deep Networks Like to See?", "abstract": "We propose a novel way to measure and understand convolutional neural networks by quantifying the amount of input signal they let in. To do this, an autoencoder (AE) was fine-tuned on gradients from a pre-trained classifier with fixed parameters. We compared the reconstructed samples from AEs that were fine-tuned on a set of image classifiers (AlexNet, VGG16, ResNet-50, and Inception~v3) and found substantial differences. The AE learns which aspects of the input space to preserve and which ones to ignore, based on the information encoded in the backpropagated gradients. Measuring the changes in accuracy when the signal of one classifier is used by a second one, a relation of total order emerges. This order depends directly on each classifier's input signal but it does not correlate with classification accuracy or network size. Further evidence of this phenomenon is provided by measuring the normalized mutual information between original images and auto-encoded reconstructions from different fine-tuned AEs. These findings break new ground in the area of neural network understanding, opening a new way to reason, debug, and interpret their results. We present four concrete examples in the literature where observations can now be explained in terms of the input signal that a model uses."}}
{"id": "ryh_8f9lg", "cdate": null, "mdate": null, "content": {"title": "Classless Association using Neural Networks", "abstract": "The goal of this paper is to train a model based on the relation between two instances that represent the same unknown class.  This scenario is inspired by the Symbol Grounding Problem and the association learning in infants.  We propose a novel model called Classless Association.  It has two parallel Multilayer Perceptrons (MLP) that uses one network as a target of the other network, and vice versa.  In addition, the presented model is trained based on an EM-approach, in which the output vectors are matched against a statistical distribution.  We generate four classless datasets based on MNIST, where the input is two different instances of the same digit.  In addition,  the digits have a uniform distribution.  Furthermore, our classless association model is evaluated against two scenarios: totally supervised and totally unsupervised.  In the first scenario, our model reaches a good performance in terms of accuracy and the classless constraint.  In the second scenario, our model reaches better results against two clustering algorithms.\n"}}
{"id": "rkB_5hEKe", "cdate": null, "mdate": null, "content": {"title": "Classless Association using Neural Networks", "abstract": "In this paper, we propose a model for the classless association between two instances of the same unknown class.  This scenario is inspired by the Symbol Grounding Problem and the association learning in infants.  Our model has two parallel Multilayer Perceptrons (MLPs) and relies on two components.  The first component is a EM-training rule that matches the output vectors of a MLP to a statistical distribution.  The second component exploits the output classification of one MLP as target of the another MLP in order to learn the agreement of the unknown class.   We generate four classless datasets (based on MNIST) with uniform distribution between the classes.  Our model is evaluated against totally supervised and totally unsupervised scenarios.  In the first scenario, our model reaches good performance in terms of accuracy and the classless constraint.  In the second scenario, our model reaches better results against two clustering algorithms."}}
