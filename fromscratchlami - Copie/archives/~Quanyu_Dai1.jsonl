{"id": "SQvhRoriHn", "cdate": 1682899200000, "mdate": 1696652559532, "content": {"title": "Graph Transfer Learning via Adversarial Domain Adaptation With Graph Convolution", "abstract": "This paper studies the problem of cross-network node classification to overcome the insufficiency of labeled data in a single network. It aims to leverage the label information in a partially labeled source network to assist node classification in a completely unlabeled or partially labeled target network. Existing methods for single network learning cannot solve this problem due to the domain shift across networks. Some multi-network learning methods heavily rely on the existence of cross-network connections, thus are inapplicable for this problem. To tackle this problem, we propose a novel graph transfer learning framework AdaGCN by leveraging the techniques of adversarial domain adaptation and graph convolution. It consists of two components: a semi-supervised learning component and an adversarial domain adaptation component. The former aims to learn class discriminative node representations with given label information of the source and target networks, while the latter contributes to mitigating the distribution divergence between the source and target domains to facilitate knowledge transfer. Extensive empirical evaluations on real-world datasets show that AdaGCN can successfully transfer class information with a low label rate on the source network and a substantial divergence between the source and target domains."}}
{"id": "L3a4uibmpZK", "cdate": 1675575244904, "mdate": 1675575244904, "content": {"title": "SimpleX: A Simple and Strong Baseline for Collaborative Filtering", "abstract": "Collaborative filtering (CF) is a widely studied research topic in recommender systems. The learning of a CF model generally depends on three major components, namely interaction encoder, loss function, and negative sampling. While many existing studies focus on the design of more powerful interaction encoders, the impacts of loss functions and negative sampling ratios have not yet been well explored. In this work, we show that the choice of loss function as well as negative sampling ratio is equivalently important. More specifically, we propose the cosine contrastive loss (CCL) and further incorporate it to a simple unified CF model, dubbed SimpleX. Extensive experiments have been conducted on 10 benchmark datasets and compared with 28 existing CF models in total. Surprisingly, the results show that, under our CCL loss and a large negative sampling ratio, SimpleX can surpass most sophisticated state-of-the-art models by a large margin (e.g., max 48.5% improvement in NDCG@20 over LightGCN). We believe that SimpleX could not only serve as a simple strong baseline to foster future research on CF, but also shed light on the potential research direction towards improving loss function and negative sampling."}}
{"id": "PHipfpBYNAq", "cdate": 1674013420566, "mdate": 1674013420566, "content": {"title": "Graph Transfer Learning via Adversarial Domain Adaptation with Graph Convolution", "abstract": "This paper studies the problem of cross-network node classification to overcome the insufficiency of labeled data in a single\nnetwork. It aims to leverage the label information in a partially labeled source network to assist node classification in a completely\nunlabeled or partially labeled target network. Existing methods for single network learning cannot solve this problem due to the domain\nshift across networks. Some multi-network learning methods heavily rely on the existence of cross-network connections, thus are\ninapplicable for this problem. To tackle this problem, we propose a novel graph transfer learning framework AdaGCN by leveraging the\ntechniques of adversarial domain adaptation and graph convolution. It consists of two components: a semi-supervised learning\ncomponent and an adversarial domain adaptation component. The former aims to learn class discriminative node representations with\ngiven label information of the source and target networks, while the latter contributes to mitigating the distribution divergence between\nthe source and target domains to facilitate knowledge transfer. Extensive empirical evaluations on real-world datasets show that\nAdaGCN can successfully transfer class information with a low label rate on the source network and a substantial divergence between\nthe source and target domains."}}
{"id": "un3iIcnPTfR", "cdate": 1672531200000, "mdate": 1706580634323, "content": {"title": "Only Encode Once: Making Content-based News Recommender Greener", "abstract": "Large pretrained language models (PLM) have become de facto news encoders in modern news recommender systems, due to their strong ability in comprehending textual content. These huge Transformer-based architectures, when finetuned on recommendation tasks, can greatly improve news recommendation performance. However, the PLM-based pretrain-finetune framework incurs high computational cost and energy consumption, primarily due to the extensive redundant processing of news encoding during each training epoch. In this paper, we propose the ``Only Encode Once'' framework for news recommendation (OLEO), by decoupling news representation learning from downstream recommendation task learning. The decoupled design makes content-based news recommender as green and efficient as id-based ones, leading to great reduction in computational cost and training resources. Extensive experiments show that our OLEO framework can reduce carbon emissions by up to 13 times compared with the state-of-the-art pretrain-finetune framework and maintain a competitive or even superior performance level. The source code is released for reproducibility."}}
{"id": "twmvsVGNo8", "cdate": 1672531200000, "mdate": 1706580634639, "content": {"title": "Semi-supervised Domain Adaptation on Graphs with Contrastive Learning and Minimax Entropy", "abstract": "Label scarcity in a graph is frequently encountered in real-world applications due to the high cost of data labeling. To this end, semi-supervised domain adaptation (SSDA) on graphs aims to leverage the knowledge of a labeled source graph to aid in node classification on a target graph with limited labels. SSDA tasks need to overcome the domain gap between the source and target graphs. However, to date, this challenging research problem has yet to be formally considered by the existing approaches designed for cross-graph node classification. To tackle the SSDA problem on graphs, a novel method called SemiGCL is proposed, which benefits from graph contrastive learning and minimax entropy training. SemiGCL generates informative node representations by contrasting the representations learned from a graph's local and global views. Additionally, SemiGCL is adversarially optimized with the entropy loss of unlabeled target nodes to reduce domain divergence. Experimental results on benchmark datasets demonstrate that SemiGCL outperforms the state-of-the-art baselines on the SSDA tasks."}}
{"id": "nY5mrQOY2m", "cdate": 1672531200000, "mdate": 1683880926799, "content": {"title": "Recommendation with Causality enhanced Natural Language Explanations", "abstract": "Explainable recommendation has recently attracted increasing attention from both academic and industry communities. Among different explainable strategies, generating natural language explanations is an important method, which can deliver more informative, flexible and readable explanations to facilitate better user decisions. Despite the effectiveness, existing models are mostly optimized based on the observed datasets, which can be skewed due to the selection or exposure bias. To alleviate this problem, in this paper, we formulate the task of explainable recommendation with a causal graph, and design a causality enhanced framework to generate unbiased explanations. More specifically, we firstly define an ideal unbiased learning objective, and then derive a tractable loss for the observational data based on the inverse propensity score (IPS), where the key is a sample re-weighting strategy for equalizing the loss and ideal objective in expectation. Considering that the IPS estimated from the sparse and noisy recommendation datasets can be inaccurate, we introduce a fault tolerant mechanism by minimizing the maximum loss induced by the sample weights near the IPS. For more comprehensive modeling, we further analyze and infer the potential latent confounders induced by the complex and diverse user personalities. We conduct extensive experiments by comparing with the state-of-the-art methods based on three real-world datasets to demonstrate the effectiveness of our method."}}
{"id": "hBrbNfYlPA", "cdate": 1672531200000, "mdate": 1683880926482, "content": {"title": "Task Adaptive Multi-learner Network for Joint CTR and CVR Estimation", "abstract": "CTR and CVR are critical factors in personalized applications, and many methods jointly estimate them via multi-task learning to alleviate the ultra-sparsity of conversion behaviors. However, it is still difficult to predict CVR accurately and robustly due to the limited and even biased knowledge extracted by the single model tower optimized on insufficient conversion samples. In this paper, we propose a task adaptive multi-learner (TAML) framework for joint CTR and CVR prediction. We design a hierarchical task adaptive knowledge representation module with different experts to capture knowledge in different granularities, which can effectively exploit the commonalities between CTR and CVR estimation tasks meanwhile keeping their unique characteristics. We apply multiple learners to extract data knowledge from various views and fuse their predictions to obtain accurate and robust scores. To facilitate knowledge sharing across learners, we further perform self-distillation that uses the fused scores to teach different learners. Thorough offline and online experiments show the superiority of TAML in different Ad ranking tasks, and we have deployed it in Huawei\u2019s online advertising platform to serve the main traffic."}}
{"id": "eXgc_y_EmTW", "cdate": 1672531200000, "mdate": 1695471772059, "content": {"title": "Out-of-distribution Detection with Implicit Outlier Transformation", "abstract": ""}}
{"id": "TT5lt-br1v_", "cdate": 1672531200000, "mdate": 1682317639371, "content": {"title": "Out-of-distribution Detection with Implicit Outlier Transformation", "abstract": "Outlier exposure (OE) is powerful in out-of-distribution (OOD) detection, enhancing detection capability via model fine-tuning with surrogate OOD data. However, surrogate data typically deviate from test OOD data. Thus, the performance of OE, when facing unseen OOD data, can be weakened. To address this issue, we propose a novel OE-based approach that makes the model perform well for unseen OOD situations, even for unseen OOD cases. It leads to a min-max learning scheme -- searching to synthesize OOD data that leads to worst judgments and learning from such OOD data for uniform performance in OOD detection. In our realization, these worst OOD data are synthesized by transforming original surrogate ones. Specifically, the associated transform functions are learned implicitly based on our novel insight that model perturbation leads to data transformation. Our methodology offers an efficient way of synthesizing OOD data, which can further benefit the detection model, besides the surrogate OOD data. We conduct extensive experiments under various OOD detection setups, demonstrating the effectiveness of our method against its advanced counterparts."}}
{"id": "RoZTu7aRE5s", "cdate": 1672531200000, "mdate": 1706580634323, "content": {"title": "Multiple Robust Learning for Recommendation", "abstract": "In recommender systems, a common problem is the presence of various biases in the collected data, which deteriorates the generalization ability of the recommendation models and leads to inaccurate predictions. Doubly robust (DR) learning has been studied in many tasks in RS, with the advantage that unbiased learning can be achieved when either a single imputation or a single propensity model is accurate. In this paper, we propose a multiple robust (MR) estimator that can take the advantage of multiple candidate imputation and propensity models to achieve unbiasedness. Specifically, the MR estimator is unbiased when any of the imputation or propensity models, or a linear combination of these models is accurate. Theoretical analysis shows that the proposed MR is an enhanced version of DR when only having a single imputation and propensity model, and has a smaller bias. Inspired by the generalization error bound of MR, we further propose a novel multiple robust learning approach with stabilization. We conduct extensive experiments on real-world and semi-synthetic datasets, which demonstrates the superiority of the proposed approach over state-of-the-art methods."}}
