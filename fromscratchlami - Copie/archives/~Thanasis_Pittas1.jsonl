{"id": "xw1KUW5_Z0", "cdate": 1672531200000, "mdate": 1683661396010, "content": {"title": "A Spectral Algorithm for List-Decodable Covariance Estimation in Relative Frobenius Norm", "abstract": "We study the problem of list-decodable Gaussian covariance estimation. Given a multiset $T$ of $n$ points in $\\mathbb R^d$ such that an unknown $\\alpha<1/2$ fraction of points in $T$ are i.i.d. samples from an unknown Gaussian $\\mathcal{N}(\\mu, \\Sigma)$, the goal is to output a list of $O(1/\\alpha)$ hypotheses at least one of which is close to $\\Sigma$ in relative Frobenius norm. Our main result is a $\\mathrm{poly}(d,1/\\alpha)$ sample and time algorithm for this task that guarantees relative Frobenius norm error of $\\mathrm{poly}(1/\\alpha)$. Importantly, our algorithm relies purely on spectral techniques. As a corollary, we obtain an efficient spectral algorithm for robust partial clustering of Gaussian mixture models (GMMs) -- a key ingredient in the recent work of [BDJ+22] on robustly learning arbitrary GMMs. Combined with the other components of [BDJ+22], our new method yields the first Sum-of-Squares-free algorithm for robustly learning GMMs. At the technical level, we develop a novel multi-filtering method for list-decodable covariance estimation that may be useful in other settings."}}
{"id": "ltqXBibiHsU", "cdate": 1672531200000, "mdate": 1684336018727, "content": {"title": "Nearly-Linear Time and Streaming Algorithms for Outlier-Robust PCA", "abstract": "We study principal component analysis (PCA), where given a dataset in $\\mathbb{R}^d$ from a distribution, the task is to find a unit vector $v$ that approximately maximizes the variance of the distribution after being projected along $v$. Despite being a classical task, standard estimators fail drastically if the data contains even a small fraction of outliers, motivating the problem of robust PCA. Recent work has developed computationally-efficient algorithms for robust PCA that either take super-linear time or have sub-optimal error guarantees. Our main contribution is to develop a nearly-linear time algorithm for robust PCA with near-optimal error guarantees. We also develop a single-pass streaming algorithm for robust PCA with memory usage nearly-linear in the dimension."}}
{"id": "LJdUUOmWjX", "cdate": 1652737841231, "mdate": null, "content": {"title": "List-Decodable Sparse Mean Estimation via Difference-of-Pairs Filtering", "abstract": "We study the problem of list-decodable sparse mean estimation. Specifically, for a parameter $\\alpha \\in (0, 1/2)$, we are given $m$ points in $\\mathbb{R}^n$, $\\lfloor \\alpha m \\rfloor$ of which are i.i.d. samples from a distribution $D$ with unknown $k$-sparse mean $\\mu$. No assumptions are made on the remaining points, which form the majority of the dataset. The goal is to return a small list of candidates containing a vector $\\hat \\mu$ such that $\\|\\hat \\mu - \\mu\\|_2$ is small. Prior work had studied the problem of list-decodable mean estimation in the dense setting. In this work, we develop a novel, conceptually simpler technique for list-decodable mean estimation. As the main application of our approach, we provide the first sample and computationally efficient algorithm for list-decodable sparse mean estimation. In particular, for distributions with  ``certifiably bounded'' $t$-th moments in $k$-sparse directions and sufficiently light tails, our algorithm achieves error of $(1/\\alpha)^{O(1/t)}$ with sample complexity $m = (k\\log(n))^{O(t)}/\\alpha$ and running time $\\mathrm{poly}(mn^t)$. For the special case of Gaussian inliers, our algorithm achieves the optimal error guarantee $\\Theta (\\sqrt{\\log(1/\\alpha)})$ with quasi-polynomial complexity. We complement our upper bounds with nearly-matching statistical query and low-degree polynomial testing lower bounds. "}}
{"id": "OZha9or-bUy", "cdate": 1640995200000, "mdate": 1674791575266, "content": {"title": "Robust Sparse Mean Estimation via Sum of Squares", "abstract": "We study the problem of high-dimensional sparse mean estimation in the presence of an $\\epsilon$-fraction of adversarial outliers. Prior work obtained sample and computationally efficient algorithm..."}}
{"id": "MG1U_DyNP1U", "cdate": 1640995200000, "mdate": 1683661396064, "content": {"title": "List-Decodable Sparse Mean Estimation via Difference-of-Pairs Filtering", "abstract": "We study the problem of list-decodable sparse mean estimation. Specifically, for a parameter $\\alpha \\in (0, 1/2)$, we are given $m$ points in $\\mathbb{R}^n$, $\\lfloor \\alpha m \\rfloor$ of which are i.i.d. samples from a distribution $D$ with unknown $k$-sparse mean $\\mu$. No assumptions are made on the remaining points, which form the majority of the dataset. The goal is to return a small list of candidates containing a vector $\\hat \\mu$ such that $\\|\\hat \\mu - \\mu\\|_2$ is small. Prior work had studied the problem of list-decodable mean estimation in the dense setting. In this work, we develop a novel, conceptually simpler technique for list-decodable mean estimation. As the main application of our approach, we provide the first sample and computationally efficient algorithm for list-decodable sparse mean estimation. In particular, for distributions with ``certifiably bounded'' $t$-th moments in $k$-sparse directions and sufficiently light tails, our algorithm achieves error of $(1/\\alpha)^{O(1/t)}$ with sample complexity $m = (k\\log(n))^{O(t)}/\\alpha$ and running time $\\mathrm{poly}(mn^t)$. For the special case of Gaussian inliers, our algorithm achieves the optimal error guarantee $\\Theta (\\sqrt{\\log(1/\\alpha)})$ with quasi-polynomial complexity. We complement our upper bounds with nearly-matching statistical query and low-degree polynomial testing lower bounds."}}
{"id": "DLIPkn8LcPx", "cdate": 1640995200000, "mdate": 1674791575266, "content": {"title": "Streaming Algorithms for High-Dimensional Robust Statistics", "abstract": "We study high-dimensional robust statistics tasks in the streaming model. A recent line of work obtained computationally efficient algorithms for a range of high-dimensional robust statistics tasks..."}}
{"id": "neRSyESg1GU", "cdate": 1621630254072, "mdate": null, "content": {"title": "Statistical Query Lower Bounds for List-Decodable Linear Regression", "abstract": "We study the problem of list-decodable linear regression, where an adversary can corrupt a majority of the examples. Specifically, we are given a set $T$ of labeled examples $(x, y) \\in \\mathbb{R}^d \\times \\mathbb{R}$ and a parameter $0< \\alpha <1/2$ such that an $\\alpha$-fraction of the points in $T$ are i.i.d. samples from a linear regression model with Gaussian covariates, and the remaining $(1-\\alpha)$-fraction of the points are drawn from an arbitrary noise distribution. The goal is to output a small list of hypothesis vectors such that at least one of them is close to the target regression vector. Our main result is a Statistical Query (SQ) lower bound of $d^{\\mathrm{poly}(1/\\alpha)}$ for this problem. Our SQ lower bound qualitatively matches the performance of previously developed algorithms, providing evidence that current upper bounds for this task are nearly best possible.\n"}}
{"id": "7pU_P1IbePx", "cdate": 1621630254072, "mdate": null, "content": {"title": "Statistical Query Lower Bounds for List-Decodable Linear Regression", "abstract": "We study the problem of list-decodable linear regression, where an adversary can corrupt a majority of the examples. Specifically, we are given a set $T$ of labeled examples $(x, y) \\in \\mathbb{R}^d \\times \\mathbb{R}$ and a parameter $0< \\alpha <1/2$ such that an $\\alpha$-fraction of the points in $T$ are i.i.d. samples from a linear regression model with Gaussian covariates, and the remaining $(1-\\alpha)$-fraction of the points are drawn from an arbitrary noise distribution. The goal is to output a small list of hypothesis vectors such that at least one of them is close to the target regression vector. Our main result is a Statistical Query (SQ) lower bound of $d^{\\mathrm{poly}(1/\\alpha)}$ for this problem. Our SQ lower bound qualitatively matches the performance of previously developed algorithms, providing evidence that current upper bounds for this task are nearly best possible.\n"}}
{"id": "o-jyQsbTy35", "cdate": 1609459200000, "mdate": 1664346296028, "content": {"title": "Estimating the Number of Induced Subgraphs from Incomplete Data and Neighborhood Queries", "abstract": "We consider a natural setting where network parameters are estimated from noisy and incomplete information about the network. More specifically, we investigate how we can efficiently estimate the number of small subgraphs (e.g., edges, triangles, etc.) based on full access to one or two noisy and incomplete samples of a large underlying network and on few queries revealing the neighborhood of carefully selected vertices. After specifying a random generator which removes edges from the underlying graph, we present estimators with strong provable performance guarantees, which exploit information from the noisy network samples and query a constant number of the most important vertices for the estimation. Our experimental evaluation shows that, in practice, a single noisy network sample and a couple of hundreds neighborhood queries suffice for accurately estimating the number of triangles in networks with millions of vertices and edges."}}
{"id": "E0vv2fU8W3", "cdate": 1609459200000, "mdate": 1649177577487, "content": {"title": "The Optimality of Polynomial Regression for Agnostic Learning under Gaussian Marginals in the SQ Model", "abstract": "We study the problem of agnostic learning under the Gaussian distribution in the Statistical Query (SQ) model. We develop a method for finding hard families of examples for a wide range of concept classes by using LP duality. For Boolean-valued concept classes, we show that the $L^1$-polynomial regression algorithm is essentially best possible among SQ algorithms, and therefore that the SQ complexity of agnostic learning is closely related to the polynomial degree required to approximate any function from the concept class in $L^1$-norm. Using this characterization along with additional analytic tools, we obtain explicit optimal SQ lower bounds for agnostically learning linear threshold functions and the first non-trivial explicit SQ lower bounds for polynomial threshold functions and intersections of halfspaces. We also develop an analogous theory for agnostically learning real-valued functions, and as an application prove near-optimal SQ lower bounds for agnostically learning ReLUs and sigmoids."}}
