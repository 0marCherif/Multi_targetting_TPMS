{"id": "X2Cxixkcpx", "cdate": 1621630330351, "mdate": null, "content": {"title": "Structured Reordering for Modeling Latent Alignments in Sequence Transduction", "abstract": "Despite success in many domains, neural models struggle in settings where train and test examples are drawn from different distributions. In particular, in contrast to humans, conventional sequence-to-sequence (seq2seq) models fail to generalize systematically, i.e., interpret sentences representing novel combinations of concepts (e.g., text segments) seen in training. Traditional grammar formalisms excel in such settings by implicitly encoding alignments between input and output segments, but are hard to scale and maintain.  Instead of engineering a grammar, we directly model segment-to-segment alignments as discrete structured latent variables within a neural seq2seq model. To efficiently explore the large space of alignments, we introduce a reorder-first align-later framework whose central component is a neural reordering module producing separable permutations. We present an efficient dynamic programming algorithm performing exact marginal inference of separable permutations, and, thus, enabling end-to-end differentiable training of our model.  The resulting seq2seq model exhibits better systematic generalization than standard models on synthetic problems and NLP tasks (i.e., semantic parsing and machine translation)."}}
{"id": "nCQiJRHPvp71", "cdate": 1609459200000, "mdate": 1637054144213, "content": {"title": "Learning from Executions for Semantic Parsing", "abstract": "Bailin Wang, Mirella Lapata, Ivan Titov. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021."}}
{"id": "hNqHyYaC-Lb4", "cdate": 1609459200000, "mdate": 1637054144362, "content": {"title": "Learning to Synthesize Data for Semantic Parsing", "abstract": "Bailin Wang, Wenpeng Yin, Xi Victoria Lin, Caiming Xiong. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021."}}
{"id": "cbPr2eVB5HG", "cdate": 1609459200000, "mdate": 1637054142852, "content": {"title": "Meta-Learning to Compositionally Generalize", "abstract": "Henry Conklin, Bailin Wang, Kenny Smith, Ivan Titov. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021."}}
{"id": "_64Xp9axbA", "cdate": 1609459200000, "mdate": 1637054143250, "content": {"title": "Meta-Learning for Domain Generalization in Semantic Parsing", "abstract": "Bailin Wang, Mirella Lapata, Ivan Titov. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021."}}
{"id": "NVTyw4FcWaa", "cdate": 1609459200000, "mdate": 1637054142854, "content": {"title": "Structured Reordering for Modeling Latent Alignments in Sequence Transduction", "abstract": "Despite success in many domains, neural models struggle in settings where train and test examples are drawn from different distributions. In particular, in contrast to humans, conventional sequence-to-sequence (seq2seq) models fail to generalize systematically, i.e., interpret sentences representing novel combinations of concepts (e.g., text segments) seen in training. Traditional grammar formalisms excel in such settings by implicitly encoding alignments between input and output segments, but are hard to scale and maintain. Instead of engineering a grammar, we directly model segment-to-segment alignments as discrete structured latent variables within a neural seq2seq model. To efficiently explore the large space of alignments, we introduce a reorder-first align-later framework whose central component is a neural reordering module producing {\\it separable} permutations. We present an efficient dynamic programming algorithm performing exact marginal inference of separable permutations, and, thus, enabling end-to-end differentiable training of our model. The resulting seq2seq model exhibits better systematic generalization than standard models on synthetic problems and NLP tasks (i.e., semantic parsing and machine translation)."}}
{"id": "kyaIeYj4zZ", "cdate": 1601308352435, "mdate": null, "content": {"title": "GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing", "abstract": "We present GraPPa, an effective pre-training approach for table semantic parsing that learns a compositional inductive bias in the joint representations of textual and tabular data. We construct synthetic question-SQL pairs over high-quality tables via a synchronous context-free grammar (SCFG). We pre-train our model on the synthetic data to inject important structural properties commonly found in semantic parsing into the pre-training language model. To maintain the model's ability to represent real-world data, we also include masked language modeling (MLM) on several existing table-related datasets to regularize our pre-training process.  Our proposed pre-training strategy is much data-efficient. When incorporated with strong base semantic parsers, GraPPa achieves new state-of-the-art results on four popular fully supervised and weakly supervised table semantic parsing tasks."}}
{"id": "wgjhTp3Rn-1", "cdate": 1577836800000, "mdate": 1637054143395, "content": {"title": "RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers", "abstract": "Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, Matthew Richardson. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020."}}
{"id": "H1egcgHtvB", "cdate": 1569439879810, "mdate": null, "content": {"title": "RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers", "abstract": "When translating natural language questions into SQL queries to answer questions from a database, contemporary semantic parsing models struggle to generalize to unseen database schemas.  The generalization challenge lies in (a) encoding the database relations in an accessible way for the semantic parser, and (b) modeling alignment between database columns and their mentions in a given query.  We present a unified framework, based on the relation-aware self-attention mechanism,to address schema encoding, schema linking, and feature representation within a text-to-SQL encoder. On the challenging Spider dataset this framework boosts the exact match accuracy to 53.7%, compared to 47.4% for the previous state-of-the-art model unaugmented with BERT embeddings. In addition, we observe qualitative improvements in the model\u2019s understanding of schema linking and alignment."}}
{"id": "aF5MHhSWk9U_", "cdate": 1546300800000, "mdate": 1637054143034, "content": {"title": "Learning Semantic Parsers from Denotations with Latent Structured Alignments and Abstract Programs", "abstract": "Bailin Wang, Ivan Titov, Mirella Lapata. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019."}}
