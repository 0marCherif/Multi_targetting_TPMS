{"id": "Ee2ugKwgvyy", "cdate": 1632875607803, "mdate": null, "content": {"title": "Graph Information Matters: Understanding Graph Filters from Interaction Probability", "abstract": "Graph Neural Networks (GNNs) have received extensive affirmation for their promising performance in graph learning problems. Despite their various neural architectures, most are intrinsically graph filters that provide theoretical foundations for model explanations.  In particular, low-pass filters show superiority in label prediction in many benchmarks.  However, recent empirical research suggests that models with only low pass filters do not always perform well.  Although increasing attempts to understand graph filters,  it is unclear how a  particular graph affects the performance of different filters.  In this paper,  we carry out a comprehensive theoretical analysis of the synergy of graph structure and node features on graph filters\u2019 behaviors in node classification, relying on the introduction of interaction probability and frequency distribution. We show that the homophily degree of graphs significantly affects the prediction error of graph filters.  Our theory provides a guideline for graph filter design in a data-driven manner. Since it is hard for a single graph filter to live up to this, we propose a general strategy for exploring a data-specified filter bank. Experimental results show that our model achieves consistent and significant performance improvements across all benchmarks.  Furthermore, we empirically validate our theoretical analysis and explain the behavior of baselines and our model."}}
{"id": "UFJOP5w0kV", "cdate": 1601308156394, "mdate": null, "content": {"title": "BiGCN: A Bi-directional Low-Pass Filtering Graph Neural Network", "abstract": "Graph convolutional networks have achieved great success on graph-structured data. Many graph convolutional networks can be regarded as low-pass filters for graph signals. In this paper, we propose a new model, BiGCN, which represents a graph neural network as a bi-directional low-pass filter. Specifically, we not only consider the original graph structure information but also the latent correlation between features, thus BiGCN can filter the signals along with both the original graph and a latent feature-connection graph. Our model outperforms previous graph neural networks in the tasks of node classification and link prediction on benchmark datasets, especially when we add noise to the node features."}}
{"id": "ZHADKD4pl5H", "cdate": 1601308156072, "mdate": null, "content": {"title": "Wasserstein diffusion on graphs with missing attributes", "abstract": "Many real-world graphs are attributed graphs where nodes are associated with non-topological features. While attributes can be missing anywhere in an attributed graph, most of existing node representation learning approaches do not consider such incomplete information.\nIn this paper, we propose a general non-parametric framework to mitigate this problem. Starting from a decomposition of the attribute matrix, we transform node features into discrete distributions in a lower-dimensional space equipped with the Wasserstein metric. On this Wasserstein space, we propose Wasserstein graph diffusion to smooth the distributional representations of nodes with information from their local neighborhoods. This allows us to reduce the distortion caused by missing attributes and obtain integrated representations expressing information of both topology structures and attributes. We then pull the nodes back to the original space and produce corresponding point representations to facilitate various downstream tasks. To show the power of our representation method, we designed two algorithms based on it for node classification (with missing attributes) and matrix completion respectively, and demonstrate their effectiveness in experiments."}}
