{"id": "rJWFpjb_Wr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Self-Attention Generative Adversarial Networks", "abstract": "In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolution..."}}
{"id": "HJZ_eoZ_bB", "cdate": 1546300800000, "mdate": null, "content": {"title": "TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing", "abstract": "Neural networks are difficult to interpret and debug. We introduce testing techniques for neural networks that can discover errors occurring only for rare inputs. Specifically, we develop coverage-..."}}
{"id": "BJ-7acZdbr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition", "abstract": "Adversarial examples are inputs to machine learning models designed by an adversary to cause an incorrect output. So far, adversarial examples have been studied most extensively in the image domain..."}}
{"id": "H1g0piA9tQ", "cdate": 1538087877629, "mdate": null, "content": {"title": "Evaluation Methodology for Attacks Against Confidence Thresholding Models", "abstract": "Current machine learning algorithms can be easily fooled by adversarial examples. One possible solution path is to make models that use confidence thresholding to avoid making mistakes. Such models refuse to make a prediction when they are not confident of their answer. We propose to evaluate such models in terms of tradeoff curves with the goal of high success rate on clean examples and low failure rate on adversarial examples. Existing untargeted attacks developed for models that do not use confidence thresholding tend to underestimate such models' vulnerability. We propose the MaxConfidence family of attacks, which are optimal in a variety of theoretical settings, including one realistic setting: attacks against linear models. Experiments show the attack attains good results in practice. We show that simple defenses are able to perform well on MNIST but not on CIFAR, contributing further to previous calls that MNIST should be retired as a benchmarking dataset for adversarial robustness research.  We release code for these evaluations as part of the cleverhans (Papernot et al 2018) library  (ICLR reviewers should be careful not to look at who contributed these features to cleverhans to avoid de-anonymizing this submission)."}}
{"id": "S1GkToR5tm", "cdate": 1538087863390, "mdate": null, "content": {"title": "Discriminator Rejection Sampling", "abstract": "We propose a rejection sampling scheme using the discriminator of a GAN to\napproximately correct errors in the GAN generator distribution. We show that\nunder quite strict assumptions, this will allow us to recover the data distribution\nexactly. We then examine where those strict assumptions break down and design a\npractical algorithm\u2014called Discriminator Rejection Sampling (DRS)\u2014that can be\nused on real data-sets. Finally, we demonstrate the efficacy of DRS on a mixture of\nGaussians and on the state of the art SAGAN model. On ImageNet, we train an\nimproved baseline that increases the best published Inception Score from 52.52 to\n62.36 and reduces the Frechet Inception Distance from 18.65 to 14.79. We then use\nDRS to further improve on this baseline, improving the Inception Score to 76.08\nand the FID to 13.75."}}
{"id": "Syx_Ss05tm", "cdate": 1538087743905, "mdate": null, "content": {"title": "Adversarial Reprogramming of Neural Networks", "abstract": "Deep neural networks are susceptible to adversarial attacks. In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as confusing a cat with a computer. Previous adversarial attacks have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker. We introduce attacks that instead reprogram the target model to perform a task chosen by the attacker without the attacker needing to specify or compute the desired output for each test-time input. This attack finds a single adversarial perturbation, that can be added to all test-time inputs to a machine learning model in order to cause the model to perform a task chosen by the adversary\u2014even if the model was not trained to do this task. These perturbations can thus be considered a program for the new task. We demonstrate adversarial reprogramming on six ImageNet classification models, repurposing these models to perform a counting task, as well as classification tasks: classification of MNIST and CIFAR-10 examples presented as inputs to the ImageNet model."}}
{"id": "S1fQSiCcYm", "cdate": 1538087738937, "mdate": null, "content": {"title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations."}}
{"id": "SJl2hl3Qgm", "cdate": 1528180915739, "mdate": null, "content": {"title": "Realistic Evaluation of Deep Semi-Supervised Learning Algorithms", "abstract": "Semi-supervised learning (SSL) provides a powerful framework for leveraging\nunlabeled data when labels are limited or expensive to obtain. SSL algorithms based\non deep neural networks have recently proven successful on standard benchmark\ntasks. However, we argue that these benchmarks fail to address many issues that\nthese algorithms would face in real-world applications. After creating a unified\nreimplementation of various widely-used SSL techniques, we test them in a suite\nof experiments designed to address these issues. We find that the performance\nof simple baselines which do not use unlabeled data is often underreported, that\nSSL methods differ in sensitivity to the amount of labeled and unlabeled data, and\nthat performance can degrade substantially when the unlabeled dataset contains\nout-of-class examples. To help guide SSL research towards real-world applicability,\nwe make our unified reimplemention and evaluation platform publicly available."}}
{"id": "ByQpn1ZA-", "cdate": 1518730174623, "mdate": null, "content": {"title": "Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step", "abstract": "Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players\u2019 parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step."}}
{"id": "SyUkxxZ0b", "cdate": 1518730173643, "mdate": null, "content": {"title": "Adversarial Spheres", "abstract": "    State of the art computer vision models have been shown to be vulnerable to small adversarial perturbations of the input. In other words, most images in the data distribution are both correctly classified by the model and are very close to a visually similar misclassified image. Despite substantial research interest, the cause of the phenomenon is still poorly understood and remains unsolved. We hypothesize that this counter intuitive behavior is a naturally occurring result of the high dimensional geometry of the data manifold. As a first step towards exploring this hypothesis, we study a simple synthetic dataset of classifying between two concentric high dimensional spheres. For this dataset we show a fundamental tradeoff between the amount of test error and the average distance to nearest error. In particular, we prove that any model which misclassifies a small constant fraction of a sphere will be vulnerable to adversarial perturbations of size $O(1/\\sqrt{d})$. Surprisingly, when we train several different architectures on this dataset, all of their error sets naturally approach this theoretical bound. As a result of the theory, the vulnerability of neural networks to small adversarial perturbations is a logical consequence of the amount of test error observed. We hope that our theoretical analysis of this very simple case will point the way forward to explore how the geometry of complex real-world data sets leads to adversarial examples."}}
