{"id": "CnqQ08FZHky", "cdate": 1683926073878, "mdate": 1683926073878, "content": {"title": "RoentGen: Vision-Language Foundation Model for Chest X-ray Generation", "abstract": "Multimodal models trained on large natural image-text pair datasets have exhibited astounding abilities in generating high-quality images. Medical imaging data is fundamentally different to natural images, and the language used to succinctly capture relevant details in medical data uses a different, narrow but semantically rich, domain-specific vocabulary. Not surprisingly, multi-modal models trained on natural image-text pairs do not tend to generalize well to the medical domain. Developing generative imaging models faithfully representing medical concepts while providing compositional diversity could mitigate the existing paucity of high-quality, annotated medical imaging datasets. In this work, we develop a strategy to overcome the large natural-medical distributional shift by adapting a pre-trained latent diffusion model on a corpus of publicly available chest x-rays (CXR) and their corresponding radiology (text) reports. We investigate the model's ability to generate high-fidelity, diverse synthetic CXR conditioned on text prompts. We assess the model outputs quantitatively using image quality metrics, and evaluate image quality and text-image alignment by human domain experts. We present evidence that the resulting model (RoentGen) is able to create visually convincing, diverse synthetic CXR images, and that the output can be controlled to a new extent by using free-form text prompts including radiology-specific language. Fine-tuning this model on a fixed training set and using it as a data augmentation method, we measure a 5% improvement of a classifier trained jointly on synthetic and real images, and a 3% improvement when trained  on a larger but purely synthetic training set. Finally, we observe that this finetuning distills in-domain knowledge in the text-encoder and can improve its representation capabilities of certain diseases like pneumothorax by 25%."}}
{"id": "B8e-iS9j43", "cdate": 1680930179254, "mdate": null, "content": {"title": "Automatic Contrast Phase Detection on Abdominal Computed Tomography using Clinically-Inspired Techniques", "abstract": "Accurately determining contrast phase in an abdominal computed tomography (CT) series is an important step prior to deploying downstream artificial intelligence methods trained to operateon the specific series. Inspired by how radiologists assess contrast phase status, this paper presents a simple approach to automatically detect the contrast phase. This method combines features extracted from the segmentation of key anatomical structures with a gradient boosting classifier for this task. The algorithm demonstrates high accuracy in categorizing the images into non-contrast (96.6\\% F1 score), arterial (78.9\\% F1 score), venous (92.2\\% F1 score), and delayed phases (95.0\\% F1 score), making it a valuable tool for enhancing AI applicability in medical imaging."}}
{"id": "VcgBBAQfMP", "cdate": 1680815033516, "mdate": null, "content": {"title": "Comp2Comp: Open-Source Body Composition Assessment on Computed Tomography", "abstract": "Computed tomography (CT) can provide quantitative body composition metrics of tissue volume, morphology, and quality which are valuable for disease prediction and prognostication. However, manually extracting these measures is a cumbersome and time-consuming task. Proprietary software to automate this process exist, but these software are closed-source, impeding large-scale access to and usage of these tools. To address this, we have built Comp2Comp, an open-source Python package for rapid and automated body composition analysis of CT scans. The primary advantages of Comp2Comp are its open-source nature, the inclusion of multiple tissue analysis capabilities within a single package, and its extensible design. We discuss the architecture of Comp2Comp and report initial validation results. Comp2Comp can be found at https://github.com/StanfordMIMI/Comp2Comp."}}
{"id": "xkmhsBITaCw", "cdate": 1673287858133, "mdate": null, "content": {"title": "Exploring Image Augmentations for Siamese Representation Learning with Chest X-Rays", "abstract": "Image augmentations are quintessential for effective visual representation learning across self-supervised learning techniques. While augmentation strategies for natural imaging have been studied extensively, medical images are vastly different from their natural counterparts. Thus, it is unknown whether common augmentation strategies employed in Siamese representation learning generalize to medical images and to what extent. To address this challenge, in this study, we systematically assess the effect of various augmentations on the quality and robustness of the learned representations. We train and evaluate Siamese Networks for abnormality detection on chest X-Rays across three large datasets (MIMIC-CXR, CheXpert and VinDr-CXR). We investigate the efficacy of the learned representations through experiments involving linear probing, fine-tuning, zero-shot transfer, and data efficiency. Finally, we identify a set of augmentations that yield robust representations that generalize well to both out-of-distribution data and diseases, while outperforming supervised baselines using just zero-shot transfer and linear probes by up to 20%. Our code is available at https://github.com/StanfordMIMI/siaug."}}
{"id": "yDxdGbBunD", "cdate": 1672531200000, "mdate": 1681660339343, "content": {"title": "DDM2: Self-Supervised Diffusion MRI Denoising with Generative Diffusion Models", "abstract": "Magnetic resonance imaging (MRI) is a common and life-saving medical imaging technique. However, acquiring high signal-to-noise ratio MRI scans requires long scan times, resulting in increased costs and patient discomfort, and decreased throughput. Thus, there is great interest in denoising MRI scans, especially for the subtype of diffusion MRI scans that are severely SNR-limited. While most prior MRI denoising methods are supervised in nature, acquiring supervised training datasets for the multitude of anatomies, MRI scanners, and scan parameters proves impractical. Here, we propose Denoising Diffusion Models for Denoising Diffusion MRI (DDM$^2$), a self-supervised denoising method for MRI denoising using diffusion denoising generative models. Our three-stage framework integrates statistic-based denoising theory into diffusion models and performs denoising through conditional generation. During inference, we represent input noisy measurements as a sample from an intermediate posterior distribution within the diffusion Markov chain. We conduct experiments on 4 real-world in-vivo diffusion MRI datasets and show that our DDM$^2$ demonstrates superior denoising performances ascertained with clinically-relevant visual qualitative and quantitative metrics."}}
{"id": "PFG11o6uM2V", "cdate": 1672531200000, "mdate": 1681660339347, "content": {"title": "Exploring Image Augmentations for Siamese Representation Learning with Chest X-Rays", "abstract": "Image augmentations are quintessential for effective visual representation learning across self-supervised learning techniques. While augmentation strategies for natural imaging have been studied extensively, medical images are vastly different from their natural counterparts. Thus, it is unknown whether common augmentation strategies employed in Siamese representation learning generalize to medical images and to what extent. To address this challenge, in this study, we systematically assess the effect of various augmentations on the quality and robustness of the learned representations. We train and evaluate Siamese Networks for abnormality detection on chest X-Rays across three large datasets (MIMIC-CXR, CheXpert and VinDR-CXR). We investigate the efficacy of the learned representations through experiments involving linear probing, fine-tuning, zero-shot transfer, and data efficiency. Finally, we identify a set of augmentations that yield robust representations that generalize well to both out-of-distribution data and diseases, while outperforming supervised baselines using just zero-shot transfer and linear probes by up to 20%. Our code is available at https://github.com/StanfordMIMI/siaug."}}
{"id": "7x6DKSYAqjZ", "cdate": 1672531200000, "mdate": 1681660339346, "content": {"title": "A scoping review of portable sensing for out-of-lab anterior cruciate ligament injury prevention and rehabilitation", "abstract": "Anterior cruciate ligament (ACL) injury and ACL reconstruction (ACLR) surgery are common. Laboratory-based biomechanical assessment can evaluate ACL injury risk and rehabilitation progress after ACLR; however, lab-based measurements are expensive and inaccessible to most people. Portable sensors such as wearables and cameras can be deployed during sporting activities, in clinics, and in patient homes. Although many portable sensing approaches have demonstrated promising results during various assessments related to ACL injury, they have not yet been widely adopted as tools for out-of-lab assessment. The purpose of this review is to summarize research on out-of-lab portable sensing applied to ACL and ACLR and offer our perspectives on new opportunities for future research and development. We identified 49 original research articles on out-of-lab ACL-related assessment; the most common sensing modalities were inertial measurement units, depth cameras, and RGB cameras. The studies combined portable sensors with direct feature extraction, physics-based modeling, or machine learning to estimate a range of biomechanical parameters (e.g., knee kinematics and kinetics) during jump-landing tasks, cutting, squats, and gait. Many of the reviewed studies depict proof-of-concept methods for potential future clinical applications including ACL injury risk screening, injury prevention training, and rehabilitation assessment. By synthesizing these results, we describe important opportunities that exist for clinical validation of existing approaches, using sophisticated modeling techniques, standardization of data collection, and creation of large benchmark datasets. If successful, these advances will enable widespread use of portable-sensing approaches to identify ACL injury risk factors, mitigate high-risk movements prior to injury, and optimize rehabilitation paradigms."}}
{"id": "1qZGETgHULq", "cdate": 1672531200000, "mdate": 1681660339348, "content": {"title": "Comp2Comp: Open-Source Body Composition Assessment on Computed Tomography", "abstract": "Computed tomography (CT) is routinely used in clinical practice to evaluate a wide variety of medical conditions. While CT scans provide diagnoses, they also offer the ability to extract quantitative body composition metrics to analyze tissue volume and quality. Extracting quantitative body composition measures manually from CT scans is a cumbersome and time-consuming task. Proprietary software has been developed recently to automate this process, but the closed-source nature impedes widespread use. There is a growing need for fully automated body composition software that is more accessible and easier to use, especially for clinicians and researchers who are not experts in medical image processing. To this end, we have built Comp2Comp, an open-source Python package for rapid and automated body composition analysis of CT scans. This package offers models, post-processing heuristics, body composition metrics, automated batching, and polychromatic visualizations. Comp2Comp currently computes body composition measures for bone, skeletal muscle, visceral adipose tissue, and subcutaneous adipose tissue on CT scans of the abdomen. We have created two pipelines for this purpose. The first pipeline computes vertebral measures, as well as muscle and adipose tissue measures, at the T12 - L5 vertebral levels from abdominal CT scans. The second pipeline computes muscle and adipose tissue measures on user-specified 2D axial slices. In this guide, we discuss the architecture of the Comp2Comp pipelines, provide usage instructions, and report internal and external validation results to measure the quality of segmentations and body composition measures. Comp2Comp can be found at https://github.com/StanfordMIMI/Comp2Comp."}}
{"id": "QtxbYdJVT8Q", "cdate": 1664943349572, "mdate": null, "content": {"title": "Adapting Pretrained Vision-Language Foundational Models to Medical Imaging Domains", "abstract": "Multi-modal foundational models are trained on millions of pairs of natural images and texts, frequently obtained through web-crawling approaches. Although their performance is excellent, these models do not generalize well to other domains, such as medical imaging, especially when these domains do not resemble the centric-like images that can be found on the web. In this study, we assess the ability of the stable diffusion model to generate domain-specific images in the particular case of medical imaging. Based on quantitative and qualitative evaluations of the main components of the stable diffusion pipeline (the variational autoencoder, the U-Net and the text-encoder), we explore several approaches to fine-tune stable diffusion to generate radiological images, which accurately represent the clinical content of conditional text prompts. Our best-performing model improves upon the stable diffusion baseline and can be correctly conditioned to insert an abnormality on a synthetic radiology image."}}
{"id": "0vqjc50HfcC", "cdate": 1663849881439, "mdate": null, "content": {"title": "DDM$^2$: Self-Supervised Diffusion MRI Denoising with Generative Diffusion Models", "abstract": "Magnetic resonance imaging (MRI) is a common and life-saving medical imaging technique. However, acquiring high signal-to-noise ratio MRI scans requires long scan times, resulting in increased costs and patient discomfort, and decreased throughput. Thus, there is great interest in denoising MRI scans, especially for the subtype of diffusion MRI scans that are severely SNR-limited. While most prior MRI denoising methods are supervised in nature, acquiring supervised training datasets for the multitude of anatomies, MRI scanners, and scan parameters proves impractical. Here, we propose Denoising Diffusion Models for Denoising Diffusion MRI (DDM^2), a self-supervised denoising method for MRI denoising using diffusion denoising generative models. Our three-stage framework integrates statistic-based denoising theory into diffusion models and performs denoising through conditional generation. During inference, we represent input noisy measurements as a sample from an intermediate posterior distribution within the diffusion Markov chain. We conduct experiments on 4 real-world in-vivo diffusion MRI datasets and show that our DDM^2 demonstrates superior denoising performances ascertained with clinically-relevant visual qualitative and quantitative metrics."}}
