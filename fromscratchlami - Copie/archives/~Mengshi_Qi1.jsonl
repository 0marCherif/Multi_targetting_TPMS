{"id": "YGLSOW-koE", "cdate": 1672531200000, "mdate": 1683795238642, "content": {"title": "GaitReload: A Reloading Framework for Defending Against On-Manifold Adversarial Gait Sequences", "abstract": "Recent on-manifold adversarial attacks can mislead gait recognition by generating adversarial walking postures (AWP) with image generation techniques. However, existing defense methods only eliminate adversarial perturbations on each frame isolatedly but ignore the temporal correlation of gait sequence, which leads to vulnerability of robust gait recognition. In this paper, we propose GaitReload, a post-processing adversarial defense method to defend against AWP for the gait recognition model with sequenced inputs. First, GaitReload utilizes sequenced entity recognition (SER) module to detect the adversarial frames by the temporal constraints of gait sequence. Then, we apply bayesian uncertainty filtering-based (BUF-based) gait interpolation to reform adversarial gait examples. After that, we reload the reformed gait sequence and rectify the recognition results with the guidance of reloading strategy. Specifically, SER has a bi-directional frame difference attention and a temporal feature aggregation to boost the detection performance. For training SER, we apply hidden posture selective attack (HPSA) to generate training samples. The extensive experimental results on CASIA-A, CASIA-B, and OU-ISIR demonstrate that GaitReload can defend against adversarial gait by large margins in both RGB and silhouette modes."}}
{"id": "ERtHi5L-pr5", "cdate": 1672531200000, "mdate": 1683795238756, "content": {"title": "Revisiting Transformer for Point Cloud-based 3D Scene Graph Generation", "abstract": "In this paper, we propose the semantic graph Transformer (SGT) for 3D scene graph generation. The task aims to parse a cloud point-based scene into a semantic structural graph, with the core challenge of modeling the complex global structure. Existing methods based on graph convolutional networks (GCNs) suffer from the over-smoothing dilemma and could only propagate information from limited neighboring nodes. In contrast, our SGT uses Transformer layers as the base building block to allow global information passing, with two types of proposed Transformer layers tailored for the 3D scene graph generation task. Specifically, we introduce the graph embedding layer to best utilize the global information in graph edges while maintaining comparable computation costs. Additionally, we propose the semantic injection layer to leverage categorical text labels and visual object knowledge. We benchmark our SGT on the established 3DSSG benchmark and achieve a 35.9% absolute improvement in relationship prediction's R@50 and an 80.4% boost on the subset with complex scenes over the state-of-the-art. Our analyses further show SGT's superiority in the long-tailed and zero-shot scenarios. We will release the code and model."}}
{"id": "4WGpeF29SS", "cdate": 1672531200000, "mdate": 1683795238751, "content": {"title": "Unsupervised Self-Driving Attention Prediction via Uncertainty Mining and Knowledge Embedding", "abstract": "Predicting attention regions of interest is an important yet challenging task for self-driving systems. Existing methodologies rely on large-scale labeled traffic datasets that are labor-intensive to obtain. Besides, the huge domain gap between natural scenes and traffic scenes in current datasets also limits the potential for model training. To address these challenges, we are the first to introduce an unsupervised way to predict self-driving attention by uncertainty modeling and driving knowledge integration. Our approach's Uncertainty Mining Branch (UMB) discovers commonalities and differences from multiple generated pseudo-labels achieved from models pre-trained on natural scenes by actively measuring the uncertainty. Meanwhile, our Knowledge Embedding Block (KEB) bridges the domain gap by incorporating driving knowledge to adaptively refine the generated pseudo-labels. Quantitative and qualitative results with equivalent or even more impressive performance compared to fully-supervised state-of-the-art approaches across all three public datasets demonstrate the effectiveness of the proposed method and the potential of this direction. The code will be made publicly available."}}
{"id": "-vA_YLMYK-", "cdate": 1672531200000, "mdate": 1683795238756, "content": {"title": "Weakly-Supervised Temporal Action Localization by Inferring Snippet-Feature Affinity", "abstract": "Weakly-supervised temporal action localization aims to locate action regions and identify action categories in untrimmed videos, only taking video-level labels as the supervised information. Pseudo label generation is a promising strategy to solve the challenging problem, but most existing methods are limited to employing snippet-wise classification results to guide the generation, and they ignore that the natural temporal structure of the video can also provide rich information to assist such a generation process. In this paper, we propose a novel weakly-supervised temporal action localization method by inferring snippet-feature affinity. First, we design an affinity inference module that exploits the affinity relationship between temporal neighbor snippets to generate initial coarse pseudo labels. Then, we introduce an information interaction module that refines the coarse labels by enhancing the discriminative nature of snippet-features through exploring intra- and inter-video relationships. Finally, the high-fidelity pseudo labels generated from the information interaction module are used to supervise the training of the action localization network. Extensive experiments on two publicly available datasets, i.e., THUMOS14 and ActivityNet v1.3, demonstrate our proposed method achieves significant improvements compared to the state-of-the-art methods."}}
{"id": "tTSBeBVpAFo", "cdate": 1640995200000, "mdate": 1683795238974, "content": {"title": "Attention-Aware Multiple Granularities Network for Player Re-Identification", "abstract": "With the development of deep learning technologies, the performance of person re-identification (ReID) has been greatly improved. However, as a subdomain of person ReID, the research for player ReID is important for the sports field yet lacks sufficient effort so far. Player ReID aims to retrieve a specified player from a gallery of players' images captured by different cameras at various time steps. Compared with the traditional person ReID, player ReID suffers from various difficult problems, e.g., the high similarity of players' appearance, limited-scale datasets, variable and low image resolution, and severe occlusion. To solve such a challenging task, we propose a method named as Attention-Aware Multiple Granularities Network (A$^2$MGN), which consists of multiple branches to capture discriminative features of players from different granularities. Through the criterion of triplet loss and cross-entropy loss, the model can localize different parts of the player and make a comprehensive comparison between each pair of images. Extensive experiments demonstrate the effectiveness and superiority of our method, and our team (MM2022-bupt) achieves the top-3 in the challenge of ACM MMSports 2022 with mAP of 0.96, Rank-1 of 0.99, and Rank-5 of 1.00."}}
{"id": "JKg-cgwBcCq", "cdate": 1640995200000, "mdate": 1683795238771, "content": {"title": "Towards Adversarial Robust Representation Through Adversarial Contrastive Decoupling", "abstract": "Adversarial training can boost the robustness of the model by aligning discriminative features between natural and generated adversarial samples. However, the generated adversarial samples tend to have more features derived from changed patterns in other categories along with the training process, which prevents better feature alignment between natural and adversarial samples. Unfortunately, existing adversarial training methods ignore such dynamicity of generated adversarial samples. In this paper, we propose Adversarial Contrastive Decoupling (ACD) to filter the features derived from changed patterns. Specificity, we decouple the changed patterns from adversarial samples and then extract robust representations from remaining features. First, we introduce a decoupling module with a dynamic labeling strategy to explore the dynamicity of generated adversarial samples. Then, we propose a siamese network with contrastive learning mechanism to align remaining robust representations between adversarial and natural samples. Extensive experimental results demonstrate the superior performance of ACD over baselines."}}
{"id": "Ir0RnmoewI", "cdate": 1640995200000, "mdate": 1668706529212, "content": {"title": "RGB-Depth Fusion GAN for Indoor Depth Completion", "abstract": "The raw depth image captured by the indoor depth sen-sor usually has an extensive range of missing depth values due to inherent limitations such as the inability to perceive transparent objects and limited distance range. The incomplete depth map burdens many downstream vision tasks, and a rising number of depth completion methods have been proposed to alleviate this issue. While most existing meth-ods can generate accurate dense depth maps from sparse and uniformly sampled depth maps, they are not suitable for complementing the large contiguous regions of missing depth values, which is common and critical. In this paper, we design a novel two-branch end-to-end fusion network, which takes a pair of RGB and incomplete depth images as input to predict a dense and completed depth map. The first branch employs an encoder-decoder structure to regress the local dense depth values from the raw depth map, with the help of local guidance information extracted from the RGB image. In the other branch, we propose an RGB-depth fusion GAN to transfer the RGB image to the fine-grained textured depth map. We adopt adaptive fusion modules named W-AdaIN to propagate the features across the two branches, and we append a confidence fusion head to fuse the two out-puts of the branches for the final depth map. Extensive ex-periments on NYU-Depth V2 and SUN RGB-D demonstrate that our proposed method clearly improves the depth completion performance, especially in a more realistic setting of indoor environments with the help of the pseudo depth map."}}
{"id": "8FrFPfSmWA", "cdate": 1640995200000, "mdate": 1683795238997, "content": {"title": "Coarse-to-Fine Video Denoising with Dual-Stage Spatial-Channel Transformer", "abstract": "Video denoising aims to recover high-quality frames from the noisy video. While most existing approaches adopt convolutional neural networks~(CNNs) to separate the noise from the original visual content, however, CNNs focus on local information and ignore the interactions between long-range regions in the frame. Furthermore, most related works directly take the output after basic spatio-temporal denoising as the final result, leading to neglect the fine-grained denoising process. In this paper, we propose a Dual-stage Spatial-Channel Transformer for coarse-to-fine video denoising, which inherits the advantages of both Transformer and CNNs. Specifically, DSCT is proposed based on a progressive dual-stage architecture, namely a coarse-level and a fine-level stage to extract dynamic features and static features, respectively. At both stages, a Spatial-Channel Encoding Module is designed to model the long-range contextual dependencies at both spatial and channel levels. Meanwhile, we design a Multi-Scale Residual Structure to preserve multiple aspects of information at different stages, which contains a Temporal Features Aggregation Module to summarize the dynamic representation. Extensive experiments on four publicly available datasets demonstrate our proposed method achieves significant improvements compared to the state-of-the-art methods."}}
{"id": "1j8bFa2O4ky", "cdate": 1640995200000, "mdate": 1668706529383, "content": {"title": "RGB-Depth Fusion GAN for Indoor Depth Completion", "abstract": "The raw depth image captured by the indoor depth sensor usually has an extensive range of missing depth values due to inherent limitations such as the inability to perceive transparent objects and limited distance range. The incomplete depth map burdens many downstream vision tasks, and a rising number of depth completion methods have been proposed to alleviate this issue. While most existing methods can generate accurate dense depth maps from sparse and uniformly sampled depth maps, they are not suitable for complementing the large contiguous regions of missing depth values, which is common and critical. In this paper, we design a novel two-branch end-to-end fusion network, which takes a pair of RGB and incomplete depth images as input to predict a dense and completed depth map. The first branch employs an encoder-decoder structure to regress the local dense depth values from the raw depth map, with the help of local guidance information extracted from the RGB image. In the other branch, we propose an RGB-depth fusion GAN to transfer the RGB image to the fine-grained textured depth map. We adopt adaptive fusion modules named W-AdaIN to propagate the features across the two branches, and we append a confidence fusion head to fuse the two outputs of the branches for the final depth map. Extensive experiments on NYU-Depth V2 and SUN RGB-D demonstrate that our proposed method clearly improves the depth completion performance, especially in a more realistic setting of indoor environments with the help of the pseudo depth map."}}
{"id": "w9EaDHVLJwn", "cdate": 1609459200000, "mdate": 1683798474717, "content": {"title": "Latent Memory-augmented Graph Transformer for Visual Storytelling", "abstract": "Visual storytelling aims to automatically generate a human-like short story given an image stream. Most existing works utilize either scene-level or object-level representations, neglecting the interaction among objects in each image and the sequential dependency between consecutive images. In this paper, we present a novel Latent Memory-augmented Graph Transformer~(LMGT ), a Transformer based framework for visual story generation. LMGT directly inherits the merits from the Transformer, which is further enhanced with two carefully designed components, i.e., a graph encoding module and a latent memory unit. Specifically, the graph encoding module exploits the semantic relationships among image regions and attentively aggregates critical visual features based on the parsed scene graphs. Furthermore, to better preserve inter-sentence coherence and topic consistency, we introduce an augmented latent memory unit that learns and records highly summarized latent information as the story line from the image stream and the sentence history. Experimental results on three widely-used datasets demonstrate the superior performance of LMGT over the state-of-the-art methods."}}
