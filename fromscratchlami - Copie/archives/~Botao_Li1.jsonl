{"id": "9zFYTavvQ6j", "cdate": 1681722999882, "mdate": 1681722999882, "content": {"title": "Testing the scale-dependent hemispherical asymmetry with the 21-cm power spectrum from the epoch of reionization", "abstract": "Hemispherical power asymmetry has emerged as a new challenge to cosmology in early Universe. While the cosmic microwave background (CMB) measurements indicated the asymmetry amplitude $A \u2243 0.07$ at the CMB scale $k_\\text{CMB}\u22430.0045\\text{Mpc}^{\u22121}\u2060$, the high-redshift quasar observations found no significant deviation from statistical isotropy. This conflict can be reconciled in some scale-dependent asymmetry models. We put forwards a new parametrization of scale-dependent asymmetric power spectrum, inspired by a multispeed inflation model. The 21-cm power spectrum from the epoch of reionization can be used to constrain the scale-dependent hemispherical asymmetry. We demonstrate that an optimum, multifrequency observation by the Square Kilometre Array (SKA) Phase 2 can impose a constraint on the amplitude of the power asymmetry anomaly at the level of $\\Delta A \u2243 0.2$ at $0.056\u2272k_\\text{21cm}\u22720.15\\text{Mpc}^{\u22121}\u2060$. This limit may be further improved by an order of magnitude as $\\Delta A \u2243 0.01$ with a cosmic variance limited experiment such as the Omniscope."}}
{"id": "BSR3Dm3rLOy", "cdate": 1681721849630, "mdate": 1681721849630, "content": {"title": "Hard-disk pressure computations\u2014a historic perspective", "abstract": "We discuss pressure computations for the hard-disk model performed since 1953 and compare them to the results that we obtain with a powerful event-chain Monte Carlo and a massively parallel Metropolis algorithm. Like other simple models in the sciences, such as the Drosophila model of biology, the hard-disk model has needed monumental efforts to be understood. In particular, we argue that the difficulty of estimating the pressure has not been fully realized in the decades-long controversy over the hard-disk phase-transition scenario. We present the physics of the hard-disk model, the definition of the pressure and its unbiased estimators, several of which are new. We further treat different sampling algorithms and crucial criteria for bounding mixing times in the absence of analytical predictions. Our definite results for the pressure, for up to one million disks, may serve as benchmarks for future sampling algorithms. A synopsis of hard-disk pressure data as well as different versions of the sampling algorithms and pressure estimators are made available in an open-source repository."}}
{"id": "dbpUa57K0MI", "cdate": 1681721341009, "mdate": 1681721341009, "content": {"title": "Sparse Hard-Disk Packings and Local Markov Chains", "abstract": "We propose locally stable sparse hard-disk packings, as introduced by B\u00f6r\u00f6czky, as a model for the analysis and benchmarking of Markov-chain Monte Carlo (MCMC) algorithms. We first generate such B\u00f6r\u00f6czky packings in a square box with periodic boundary conditions and analyze their properties. We then study how local MCMC algorithms, namely the Metropolis algorithm and several versions of event-chain Monte Carlo (ECMC), escape from configurations that are obtained from the packings by slightly reducing all disk radii by a relaxation parameter. We obtain two classes of ECMC, one in which the escape time varies algebraically with the relaxation parameter (as for the local Metropolis algorithm) and another in which the escape time scales as the logarithm of the relaxation parameter. A scaling analysis is confirmed by simulation results. We discuss the connectivity of the hard-disk sample space, the ergodicity of local MCMC algorithms, as well as the meaning of packings in the context of the NPT ensemble. Our work is accompanied by open-source, arbitrary-precision software for B\u00f6r\u00f6czky packings (in Python) and for straight, reflective, forward, and Newtonian ECMC (in Go)."}}
{"id": "gt22v5enJ1", "cdate": 1672531200000, "mdate": 1681651242855, "content": {"title": "The Probabilistic Stability of Stochastic Gradient Descent", "abstract": ""}}
{"id": "X6bp8ri8dV", "cdate": 1652737456646, "mdate": null, "content": {"title": "Exact Solutions of a Deep Linear Network", "abstract": "This work finds the analytical expression of the global minima of a deep linear network with weight decay and stochastic neurons, a fundamental model for understanding the landscape of neural networks. Our result implies that zero is a special point in deep neural network architecture. We show that weight decay strongly interacts with the model architecture and can create bad minima at zero in a network with more than $1$ hidden layer, qualitatively different from a network with only $1$ hidden layer. Practically, our result implies that common deep learning initialization methods are insufficient to ease the optimization of neural networks in general."}}
{"id": "XesuHidrag", "cdate": 1640995200000, "mdate": 1681651242815, "content": {"title": "SGD Can Converge to Local Maxima", "abstract": ""}}
{"id": "GPCMdxnvqx", "cdate": 1640995200000, "mdate": 1667532680020, "content": {"title": "Exact Solutions of a Deep Linear Network", "abstract": "This work finds the analytical expression of the global minima of a deep linear network with weight decay and stochastic neurons, a fundamental model for understanding the landscape of neural networks. Our result implies that zero is a special point in deep neural network architecture. We show that weight decay strongly interacts with the model architecture and can create bad minima at zero in a network with more than $1$ hidden layer, qualitatively different from a network with only $1$ hidden layer. Practically, our result implies that common deep learning initialization methods are insufficient to ease the optimization of neural networks in general."}}
{"id": "9XhPLAjjRB", "cdate": 1632875434007, "mdate": null, "content": {"title": "SGD Can Converge to Local Maxima", "abstract": "Previous works on stochastic gradient descent (SGD) often focus on its success. In this work, we construct worst-case optimization problems illustrating that, when not in the regimes that the previous works often assume, SGD can exhibit many strange and potentially undesirable behaviors. Specifically, we construct landscapes and data distributions such that (1) SGD converges to local maxima, (2) SGD escapes saddle points arbitrarily slowly, (3) SGD prefers sharp minima over flat ones, and (4) AMSGrad converges to local maxima. We also realize results in a minimal neural network-like example. Our results highlight the importance of simultaneously analyzing the minibatch sampling, discrete-time updates rules, and realistic landscapes to understand the role of SGD in deep learning."}}
{"id": "jsp2UffPxuD", "cdate": 1609459200000, "mdate": 1681720048103, "content": {"title": "Multithreaded event-chain Monte Carlo with local times", "abstract": ""}}
{"id": "Bu-fEiYJUx5", "cdate": 1609459200000, "mdate": 1645767068207, "content": {"title": "SGD May Never Escape Saddle Points", "abstract": "Previous works on stochastic gradient descent (SGD) often focus on its success. In this work, we construct worst-case optimization problems illustrating that, when not in the regimes that the previous works often assume, SGD can exhibit many strange and potentially undesirable behaviors. Specifically, we construct landscapes and data distributions such that (1) SGD converges to local maxima, (2) SGD escapes saddle points arbitrarily slowly, (3) SGD prefers sharp minima over flat ones, and (4) AMSGrad converges to local maxima. We also realize results in a minimal neural network-like example. Our results highlight the importance of simultaneously analyzing the minibatch sampling, discrete-time updates rules, and realistic landscapes to understand the role of SGD in deep learning."}}
