{"id": "TfF45UvI8W", "cdate": 1683882767947, "mdate": 1683882767947, "content": {"title": "Efficient Online Bayesian Inference for Neural Bandits", "abstract": "In this paper we present a new algorithm for online (sequential) inference in Bayesian neural networks, and show its suitability for tackling contextual bandit problems. The key idea is to combine the extended Kalman filter (which locally linearizes the likelihood function at each time step) with a (learned or random) low-dimensional affine subspace for the parameters; the use of a subspace enables us to scale our algorithm to models with \u223c1M parameters. While most other neural bandit methods need to store the entire past dataset in order to avoid the problem of \"catastrophic forgetting\", our approach uses constant memory. This is possible because we represent uncertainty about all the parameters in the model, not just the final linear layer. We show good results on the \"Deep Bayesian Bandit Showdown\" benchmark, as well as MNIST and a recommender system."}}
{"id": "asgeEt25kk", "cdate": 1667319434526, "mdate": null, "content": {"title": "On diagonal approximations to the extended Kalman filter for online training of Bayesian neural networks", "abstract": "We present two approaches to approximate online Bayesian inference for the parameters of DNNs. Both are based on diagonal Gaussian approximations and linearize the network at each step to ensure efficient computation.\nThe first approach optimizes the exclusive KL,  KL(p,q); this amounts to matching the marginal mean and {\\em precision} of p and q.\nThe second approach  optimizes the inclusive KL,  KL(q,p), which amounts to matching the marginal mean and {\\em variance} of p and q. The latter approach turns out to be equivalent to the previously proposed ``fully decoupled EKF'' approach.\nWe show experimentally that exclusive KL is more effective than both inclusive KL and one-pass SGD."}}
{"id": "T6QZmBPlfv6", "cdate": 1664928791006, "mdate": null, "content": {"title": "Reliability benchmarks for image segmentation", "abstract": "Recent work has shown the importance of reliability, where model performance is assessed under stress conditions pervasive in real-world deployment. In this work, we examine reliability tasks in the setting of semantic segmentation, a dense output problem that has typically only been evaluated using in-distribution predictive performance---for example, the mean intersection over union score on the Cityscapes validation set. To reduce the gap toward reliable deployment in the real world, we compile a benchmark involving existing (and newly constructed) distribution shifts and metrics. We evaluate current models and several baselines to determine how well segmentation models make robust predictions across multiple types of distribution shift and flag when they don\u2019t know.\n"}}
{"id": "jwgnijhdF3V", "cdate": 1663850025349, "mdate": null, "content": {"title": "Posterior Sampling Model-based Policy Optimization under Approximate Inference", "abstract": "Model-based reinforcement learning algorithms (MBRL) hold tremendous promise for improving the sample efficiency in online RL. However, many existing popular MBRL algorithms cannot deal with exploration and exploitation properly. Posterior sampling reinforcement learning (PSRL) serves as a promising approach for automatically trading off the exploration and exploitation, but the theoretical guarantees only hold under exact inference. In this paper, we show that adopting the same methodology as in exact PSRL can be fairly suboptimal under approximate inference. Motivated by the analysis, we propose an improved factorization for the posterior distribution of polices by removing the conditional independence between the policy and data given the model. By adopting such a posterior factorization, we further propose a general algorithmic framework for PSRL under approximate inference and a practical instantiation of it. Empirically, our algorithm can surpass the baseline methods by a significant margin on both dense rewards and sparse rewards tasks from DM control suite, OpenAI Gym and Metaworld benchmarks."}}
{"id": "6x0gB9gOHFg", "cdate": 1653595784051, "mdate": null, "content": {"title": "Plex: Towards Reliability using Pretrained Large Model Extensions", "abstract": "A recent trend in artificial intelligence is the use of pretrained models for language and vision tasks, which have achieved extraordinary performance but also puzzling failures. Probing these models' abilities in diverse ways is therefore critical to the field. In this paper, we explore the reliability of models, where we define a reliable model as one that not only achieves strong predictive performance but also performs well consistently over many decision-making tasks involving uncertainty (e.g., selective prediction, open set recognition), robust generalization (e.g., accuracy and proper scoring rules such as log-likelihood on in- and out-of-distribution datasets), and adaptation (e.g., active learning, few-shot uncertainty). We devise 10 types of tasks over 38 datasets in order to evaluate different aspects of reliability on both vision and language domains. To improve reliability, we developed ViT-Plex and T5-Plex, pretrained large model extensions (plex) for vision and language modalities, respectively. Plex greatly improves the state-of-the-art across reliability tasks, and simplifies the traditional protocol as it does not require designing scores or tuning the model for each individual task. We demonstrate scaling effects over model sizes up to 1B parameters and pretraining dataset sizes up to 4B examples. We also demonstrate Plex's capabilities on challenging tasks including zero-shot open set recognition, active learning, and uncertainty in conversational language understanding."}}
{"id": "xS-h9ecm8aq", "cdate": 1595572147614, "mdate": null, "content": {"title": "Regularized Autoencoders via Relaxed Injective Probability Flow", "abstract": "Invertible flow-based generative models are an effective method for learning to generate samples, while allowing for tractable likelihood computation and inference. However, the invertibility requirement restricts models to have the same latent dimensionality as the inputs. This imposes significant architectural, memory, and computational costs, making them more challenging to scale than other classes of generative models such as Variational Autoencoders (VAEs). We propose a generative model based on probability flows that does away with the bijectivity requirement on the model and only assumes injectivity. This also provides another perspective on regularized autoencoders (RAEs), with our final objectives resembling RAEs with specific regularizers that are derived by lower bounding the probability flow objective. We empirically demonstrate the promise of the proposed model, improving over VAEs and AEs in terms of sample quality."}}
{"id": "ZXiufAptHR", "cdate": 1579711294229, "mdate": null, "content": {"title": "Floors are Flat: Leveraging Semantics for Real-Time Surface Normal Prediction", "abstract": "We propose 4 insights that help to significantly improve the performance of deep learning models that predict surface normals and semantic labels from a single RGB image. These insights are:(1) denoise the\" ground truth\" surface normals in the training set to ensure consistency with the semantic labels;(2) concurrently train on a mix of real and synthetic data, instead of pretraining on synthetic and finetuning on real;(3) jointly predict normals and semantics using a shared model, but only backpropagate errors on pixels that have valid training labels;(4) slim down the model and use grayscale instead of color inputs. Despite the simplicity of these steps, we demonstrate consistently improved state of the art results on several datasets, using a model that runs at 12 fps on a standard mobile phone."}}
{"id": "HklxbgBKvr", "cdate": 1569439736148, "mdate": null, "content": {"title": "Model-based reinforcement learning for biological sequence design", "abstract": "The ability to design biological structures such as DNA or proteins would have considerable medical and industrial impact. Doing so presents a challenging black-box optimization problem characterized by the large-batch, low round setting due to the need for labor-intensive wet lab evaluations. In response, we propose using reinforcement learning (RL) based on proximal-policy optimization (PPO) for biological sequence design. RL provides a flexible framework for optimization generative sequence models to achieve specific criteria, such as diversity among the high-quality sequences discovered. We propose a model-based variant of PPO, DyNA-PPO, to improve sample efficiency, where the policy for a new round is trained offline using a simulator fit on functional measurements from prior rounds. To accommodate the growing number of observations across rounds, the simulator model is automatically selected at each round from a pool of diverse models of varying capacity.  On the tasks of designing DNA transcription factor binding sites, designing antimicrobial proteins, and optimizing the energy of Ising models based on protein structure, we find that DyNA-PPO performs significantly better than existing methods in settings in which modeling is feasible, while still not performing worse in situations in which a reliable model cannot be learned."}}
{"id": "rJgRMkrtDr", "cdate": 1569439509557, "mdate": null, "content": {"title": "Learning Video Representations using Contrastive Bidirectional Transformer", "abstract": "This paper proposes a self-supervised learning approach for video features that results in significantly improved performance on downstream tasks (such as video classification, captioning and segmentation) compared to existing methods. Our method extends the BERT model for text sequences to the case of sequences of real-valued feature vectors, by replacing the softmax loss with noise contrastive estimation (NCE). We also show how to learn representations from sequences of visual features and sequences of words derived from ASR (automatic speech recognition), and show that such cross-modal training (when possible) helps even more."}}
{"id": "BkxdqA4tvB", "cdate": 1569439376438, "mdate": null, "content": {"title": "Collapsed amortized variational inference for switching nonlinear dynamical systems", "abstract": "We propose an efficient inference method for switching nonlinear dynamical systems. The key idea is to learn an inference network which can be used as a proposal distribution for the continuous latent variables, while performing exact marginalization of the discrete latent variables. This allows us to use the reparameterization trick, and apply end-to-end training with SGD. We show that this method can successfully segment time series data (including videos) into meaningful \"regimes\", due to the use of piece-wise nonlinear dynamics."}}
