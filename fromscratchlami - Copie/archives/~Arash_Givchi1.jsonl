{"id": "jj-x_DG5G84", "cdate": 1602348637876, "mdate": null, "content": {"title": "Learning a manifold from a teacher\u2019s demonstrations", "abstract": "We consider the problem of learning a manifold from a teacher's demonstration. Extending existing approaches of learning from randomly sampled data points, we consider contexts where data may be chosen by a teacher. \n We analyze learning from teachers who can provide structured data such as individual examples (isolated data points) and demonstrations (sequences of points). Our analysis shows that for the purpose of teaching the topology of a manifold, demonstrations can yield remarkable decreases in the amount of data points required in comparison to teaching with randomly sampled points."}}
{"id": "l49lfaUKuF", "cdate": 1514764800000, "mdate": null, "content": {"title": "Off-policy temporal difference learning with distribution adaptation in fast mixing chains", "abstract": "In this paper, we investigate the possibility of covariate shift adaptation in off-policy temporal difference learning for the class of fast mixing Markov chains. Off-policy evaluation algorithms in reinforcement learning such as off-policy least squares temporal difference (LSTD) deal with the problem of evaluating a target policy different from the sampling (or behavior) policy. Off-policy LSTD may result in poor quality of solution due to the shift among stationary distributions of the chains induced by following the target and behavior policies. Previous works\u2014least squares temporal difference\u2013distribution optimization (LSTD-DO) and the recently proposed emphatic TD\u2014each tackles this problem by mapping distribution of states collected following the behavior policy (i.e. off-policy samples) to a new different distribution with better LSTD solution. In this paper, we consider off-policy LSTD in the class of target Markov chains with fast mixing time. For this class of problems, we propose adapting the distribution of off-policy state samples to the distribution of state samples after transition model adaptation, using a regularized covariate shift adaptation algorithm called least squares importance fitting. Empirical evaluations of our proposed approach on two classes of fast mixing chains show promising results in comparison with LSTD-DO and unadapted off-policy LSTD as the number of samples increases."}}
{"id": "HZbmdMeJD86", "cdate": 1514764800000, "mdate": null, "content": {"title": "Optimal Cooperative Inference", "abstract": "Cooperative transmission of data fosters rapid accumulation of knowledge by efficiently combining experiences across learners. Although well studied in human learning and increasingly in machine le..."}}
{"id": "Iz3qSROWqC", "cdate": 1388534400000, "mdate": null, "content": {"title": "Quasi Newton Temporal Difference Learning", "abstract": "Fast convergent and computationally inexpensive policy evaluation is an essential part of reinforcement learning algorithms based on policy iteration. Algorithms such as LSTD, LSPE, FPKF and NTD, h..."}}
