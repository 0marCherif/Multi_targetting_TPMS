{"id": "-3cHWtrbLYq", "cdate": 1652737708771, "mdate": null, "content": {"title": "Local Identifiability of Deep ReLU Neural Networks: the Theory", "abstract": "Is a sample rich enough to determine, at least locally, the parameters of a neural network? To answer this question, we introduce a new local parameterization of a given deep ReLU neural network by fixing the values of some of its weights. This allows us to define local lifting operators whose inverses are charts of a smooth manifold of a high dimensional space. The function implemented by the deep ReLU neural network composes the local lifting with a linear operator which depends on the sample. We derive from this convenient representation a geometrical necessary and sufficient condition of local identifiability. Looking at tangent spaces, the geometrical condition provides: 1/ a sharp and testable necessary condition of identifiability and 2/ a sharp and testable sufficient condition of local identifiability. The validity of the conditions can be tested numerically using backpropagation and matrix rank computations."}}
{"id": "YCPmfirAcc", "cdate": 1652737321948, "mdate": null, "content": {"title": "High-dimensional Additive Gaussian Processes under Monotonicity Constraints", "abstract": "We introduce an additive Gaussian process (GP) framework accounting for monotonicity constraints and scalable to high dimensions. Our contributions are threefold. First, we show that our framework enables to satisfy the constraints everywhere in the input space. We also show that more general componentwise linear inequality constraints can be handled similarly, such as componentwise convexity. Second, we propose the additive MaxMod algorithm for sequential dimension reduction. By sequentially maximizing a squared-norm criterion, MaxMod identifies the active input dimensions and refines the most important ones. This criterion can be computed explicitly at a linear cost. Finally, we provide open-source codes for our full framework. We demonstrate the performance and scalability of the methodology in several synthetic examples with hundreds of dimensions under monotonicity constraints as well as on a real-world flood application."}}
{"id": "zklGb6gI-TZ", "cdate": 1640995200000, "mdate": 1681649969275, "content": {"title": "Block-Diagonal Covariance Estimation and Application to the Shapley Effects in Sensitivity Analysis", "abstract": ""}}
{"id": "seT8f81NyA", "cdate": 1640995200000, "mdate": 1681649969213, "content": {"title": "High-dimensional additive Gaussian processes under monotonicity constraints", "abstract": ""}}
{"id": "ZmIuXpsGE8N", "cdate": 1640995200000, "mdate": 1681649969287, "content": {"title": "Regret Analysis of Dyadic Search", "abstract": ""}}
{"id": "UZcH-pSkt5", "cdate": 1640995200000, "mdate": 1681649969226, "content": {"title": "A Near-Optimal Algorithm for Univariate Zeroth-Order Budget Convex Optimization", "abstract": ""}}
{"id": "QMUQbPCq3hW", "cdate": 1640995200000, "mdate": 1681649969581, "content": {"title": "Gaussian Processes on Distributions based on Regularized Optimal Transport", "abstract": ""}}
{"id": "-NW1gsbHvV", "cdate": 1640995200000, "mdate": 1681649969211, "content": {"title": "Multioutput Gaussian processes with functional data: A study on coastal flood hazard assessment", "abstract": ""}}
{"id": "yrqn9rQO2YT", "cdate": 1621629990627, "mdate": null, "content": {"title": "Instance-Dependent Bounds for Zeroth-order Lipschitz Optimization with Error Certificates", "abstract": "We study the problem of zeroth-order (black-box) optimization of a Lipschitz function $f$ defined on a compact subset $\\mathcal{X}$ of $\\mathbb{R}^d$, with the additional constraint that algorithms must certify the accuracy of their recommendations. We characterize the optimal number of evaluations of any Lipschitz function $f$ to find and certify an approximate maximizer of $f$ at accuracy $\\varepsilon$. Under a weak assumption on $\\mathcal{X}$, this optimal sample complexity is shown to be nearly proportional to the integral $\\int_{\\mathcal{X}} \\mathrm{d}\\boldsymbol{x}/( \\max(f) - f(\\boldsymbol{x}) + \\varepsilon )^d$. This result, which was only (and partially) known in dimension $d=1$, solves an open problem dating back to 1991. In terms of techniques, our upper bound relies on a packing bound by Bouttier et al. (2020) for the Piyavskii-Shubert algorithm that we link to the above integral. We also show that a certified version of the computationally tractable DOO algorithm matches these packing and integral bounds. Our instance-dependent lower bound differs from traditional worst-case lower bounds in the Lipschitz setting and relies on a local worst-case analysis that could likely prove useful for other learning tasks."}}
{"id": "qGTluff9BXX", "cdate": 1609459200000, "mdate": 1681649969305, "content": {"title": "Gaussian Linear Approximation for the Estimation of the Shapley Effects", "abstract": ""}}
