{"id": "uki_1iaQj1f", "cdate": 1682899200000, "mdate": 1682321711723, "content": {"title": "Synergistic Task and Motion Planning With Reinforcement Learning-Based Non-Prehensile Actions", "abstract": "Robotic manipulation in cluttered environments requires synergistic planning among prehensile and non-prehensile actions. Previous works on sampling-based Task and Motion Planning (TAMP) algorithms, e.g. PDDLStream, provide a fast and generalizable solution for multi-modal manipulation. However, they are likely to fail in cluttered scenarios where no collision-free grasping approaches can be sampled without preliminary manipulations. To extend the ability of sampling-based algorithms, we integrate a vision-based Reinforcement Learning (RL) non-prehensile procedure, <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">pusher</i> . The pushing actions generated by <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">pusher</i> can eliminate interlocked situations and make the grasping problem solvable. Also, the sampling-based algorithm evaluates the pushing actions by providing rewards in the training process, thus the <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">pusher</i> can learn to avoid situations leading to irreversible failures. The proposed hybrid planning method is validated on a cluttered bin-picking problem and implemented in both simulation and real world. Results show that the <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">pusher</i> can effectively improve the success ratio of the previous sampling-based algorithm, while the sampling-based algorithm can help the <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">pusher</i> learn pushing skills."}}
{"id": "XT497lSqenB", "cdate": 1680159992674, "mdate": 1680159992674, "content": {"title": "Synergistic Task and Motion Planning with Reinforcement Learning Non-Prehensile Actions", "abstract": "Robotic manipulation in cluttered environments\nrequires synergistic planning among prehensile and non-\nprehensile actions. Previous work on sampling-based Task\nand Motion Planning (TAMP) algorithms, e.g. PDDLStream,\nprovide a fast and generalizable solution for multi-modal\nmanipulation. However, they are likely to fail in cluttered\nscenarios where no collision-free grasping approaches can be\nsampled without preliminary manipulations. To extend the\nability of sampling-based algorithms, we integrate a vision-\nbased Reinforcement Learning (RL) non-prehensile procedure,\npusher. The pushing actions generated by pusher can eliminate\ninterlocked situations and make the grasping problem solvable.\nAlso, the sampling-based algorithm evaluates the pushing ac-\ntions by providing rewards in the training process, thus the\npusher can learn to avoid situations leading to irreversible\nfailures. The proposed hybrid planning method is validated\non a cluttered bin picking problem and implemented in both\nsimulation and real world. Results show that the pusher can\neffectively improve the success ratio of the previous sampling-\nbased algorithm, while the sampling-based algorithm can help\nthe pusher to learn pushing skills."}}
{"id": "kpOavCpKMDC", "cdate": 1680159844912, "mdate": 1680159844912, "content": {"title": "Transferring Multiple Policies to Hotstart Reinforcement Learning in an Air Compressor Management Problem", "abstract": "Many instances of similar or almost-identical industrial machines or tools are often deployed\nat once, or in quick succession. For instance, a particular model of air compressor may be\ninstalled at hundreds of customers. Because these tools perform distinct but highly similar\ntasks, it is interesting to be able to quickly produce a high-quality controller for machine\nN+1 given the controllers already produced for machines 1..N . This is even more important\nwhen the controllers are learned through Reinforcement Learning, as training takes time,\nenergy and other resources. In this paper, we apply Policy Intersection, a Policy Shaping\nmethod, to help a Reinforcement Learning agent learn to solve a new variant of a compressors\ncontrol problem faster, by transferring knowledge from several previously learned controllers.\nWe show that our approach outperforms loading an old controller, and significantly improves\nperformance in the long run."}}
{"id": "29eWmxiWlU", "cdate": 1680159742720, "mdate": 1680159742720, "content": {"title": "Model-free control of compressors in an air network with unknown future air demand", "abstract": "Optimal control of complex systems often requires access to an exact or almost-exact model,\nand information about the (future) external stimuli applied to the system (load, demand, ...).\nThis is particularly true in the case of air networks, in which compressors have to fill an air\ntank, usually proportionally small compared to the production of the compressors and the\naverage downstream demand of air. The demand of air therefore largely impacts the pressure\nin the tank, and the compressors have to react quickly to changes in demand. In this paper,\nwe propose a method based on Reinforcement Learning to produce a high-quality controller\nfor 3 compressors connected to the same air network. The Reinforcement Learning agent\ndoes not assume any model (so the compressors, tubes, losses and demand do not have to be\nmodeled) and does not observe the future demand of air, or an approximation of it. Still, the\nlearned controller performs comparably to a highly-tuned Model Predictive Controller, and\nlargely outperforms MPC when even a small error exists in the predicted future demand.\nThis demonstrates that Reinforcement Learning allows to produce high-quality controllers\nin challenging industrial contexts."}}
{"id": "PEp5wDVTVfE", "cdate": 1672531200000, "mdate": 1682321712018, "content": {"title": "Transferring Multiple Policies to Hotstart Reinforcement Learning in an Air Compressor Management Problem", "abstract": "Many instances of similar or almost-identical industrial machines or tools are often deployed at once, or in quick succession. For instance, a particular model of air compressor may be installed at hundreds of customers. Because these tools perform distinct but highly similar tasks, it is interesting to be able to quickly produce a high-quality controller for machine $N+1$ given the controllers already produced for machines $1..N$. This is even more important when the controllers are learned through Reinforcement Learning, as training takes time, energy and other resources. In this paper, we apply Policy Intersection, a Policy Shaping method, to help a Reinforcement Learning agent learn to solve a new variant of a compressors control problem faster, by transferring knowledge from several previously learned controllers. We show that our approach outperforms loading an old controller, and significantly improves performance in the long run."}}
{"id": "22e9Y42YOO-", "cdate": 1640995200000, "mdate": 1682321712025, "content": {"title": "Fast Initialization of Control Parameters using Supervised Learning on Data from Similar Assets", "abstract": "This paper proposes a method to provide a good initialization of control parameters to be found when performing manual or automated control tuning during development, commissioning or periodic retuning. The method is based on treating the initialization problem as a supervised learning one; taking examples from similar machines and similar tasks for which good control parameters have been found, and using those examples to build models that predict good control parameters for new machines and tasks yet to be initialized. Two of such models are proposed, one based on random forest regressors and a second based on neural networks. The random forest is highly data-efficient but generalizes only moderately. The neural network is able to leverage a high-dimensional burner run input to perform automatic system identification and generalization. While the proposed approach can be applied to a variety of applications for which example data from well functioning controllers can be used to hot-start new ones, we applied it in this paper to three slider-crank setups performing a variety of similar tasks. We found that both models outperform a benchmark of using a physics-inspired model for the initialization. Using 20% of the data for training, the required number of experiments was reduced up to 44%, and the performance of the initial experiments was improved by up to 68% compared to the benchmark."}}
{"id": "iX6sP2gHQ1x", "cdate": 1617694968150, "mdate": null, "content": {"title": "Sample-Efficient Reinforcement Learning for Continuous Actions with Continuous BDPI", "abstract": "Bootstrapped Dual Policy Iteration is a model-free Reinforcement Learning algorithm that combines several off-policy\ncritics with an actor robust to off-policy critics. The critics are trained using a variant of Q-Learning, and the actor imitates their\naverage greedy policies with Conservative Policy Iteration. BDPI achieves state-of-the-art sample-efficiency in discrete-action do-\nmains, but is inapplicable to continuous-action domains, as both the actor and critic update rules rely on the ability to enumerate\nthe actions. In this paper, we present a novel implementation of the BDPI ideas, off-policy critics and an actor, for continuous actions. Our actor is built around a single discriminator network, easy to train to imitate greedy policies, and the critics take inspiration from the off-line batch RL literature to allow off-policy learning. In this early work (visionary) paper, we show that our Continuous BDPI is several times more sample-efficient than the Soft Actor-Critic on BipedalWalker, using naive hyper-parameters. We explain in our experimental section how future work will allow to characterize the behavior of Continuous BDPI regarding its hyper-parameters, which will allow it to be applied to more environments."}}
{"id": "_Sx5DCJyCZv", "cdate": 1609459200000, "mdate": 1682321711820, "content": {"title": "MoveRL: To a Safer Robotic Reinforcement Learning Environment", "abstract": "The deployment of Reinforcement Learning (RL) on physical robots still stumbles on several challenges, such as sample-efficiency, safety, reproducibility, cost, and software platforms. In this paper, we introduce MoveRL, an environment that exposes a standard OpenAI Gym interface, and allows any off-the-shelf RL agent to control a robot built on ROS, the Robot OS. ROS is the standard abstraction layer used by roboticists, and allows to observe and control both simulated and physical robots. By providing a bridge between the Gym and ROS, our environment allows an easy evaluation of RL algorithms in highly-accurate simulators, or real-world robots, without any change of software. In addition to a Gym-ROS bridge, our environment also leverages MoveIt, a state-of-the-art collision-aware robot motion planner, to prevent the RL agent from executing actions that would lead to a collision. Our experimental results show that a standard PPO agent is able to control a simulated commercial robot arm in an environment with moving obstacles, while almost perfectly avoiding collisions even in the early stages of learning. We also show that the use of MoveIt slightly increases the sample-efficiency of the RL agent. Combined, these results show that RL on robots is possible in a safe way, and that it is possible to leverage state-of-the-art robotic techniques to improve how an RL agent learns. We hope that our environment will allow more (future) RL algorithms to be evaluated on commercial robotic tasks. Github repository: https://github.com/Gaoyuan-Liu/MoveRL"}}
{"id": "IlJbTsygaI6", "cdate": 1601308047362, "mdate": null, "content": {"title": "Explainable Reinforcement Learning Through Goal-Based Interpretability", "abstract": "Deep Reinforcement Learning agents achieve state-of-the-art performance in many tasks at the cost of making them black-boxes, hard to interpret and understand, making their use difficult in trusted applications, such as robotics or industrial applications. We introduce goal-based interpretability, where the agent produces goals which show the reason for its current actions (reach the current goal) and future goals indicate its desired future behavior without having to run the environment, a useful property in environments with no simulator. Additionally, in many environments, the goals can be visualized to make them easier to understand for non-experts. To have a goal-producing agent without requiring domain knowledge, we use 2-layer hierarchical agents where the top layer produces goals and the bottom layer attempts to reach those goals. \n\nMost classical reinforcement learning algorithms cannot be used train  goal-producing hierarchical agents. We introduce a new algorithm to train these more interpretable agents, called HAC-General with Teacher, an extension of the Hindsight Actor-Critic (HAC)  algorithm that adds 2 key improvements: (1) the goals now consist of a state $s$ to be reached and a reward $r$ to be collected, making it possible for the goal-producing policy to incentivize the goal-reaching policy to go through high-reward paths and (2) an expert teacher is leveraged to improve the training of the hierarchical agent, in a process similar but distinct to imitation learning and distillation. Contrarily to HAC, there is no requirement that environments need to provide the desired end state. Additionally, our experiments show that it has better performance and learns faster than HAC, and can solve environments that HAC fails to solve."}}
{"id": "vldRfNcQdOA", "cdate": 1594384055931, "mdate": null, "content": {"title": "Sample-Efficient Model-Free Reinforcement Learning with Off-Policy Critics", "abstract": "Value-based reinforcement-learning algorithms provide state-of-the-art results in model-free discrete-action settings, and tend to outperform actor-critic algorithms. We argue that actor-critic algorithms are limited by their need for an on-policy critic. We propose Bootstrapped Dual Policy Iteration (BDPI), a novel model-free reinforcement-learning algorithm for continuous states and discrete actions, with an actor and\nseveral off-policy critics. Off-policy critics are compatible with experience-replay, ensuring high sample-efficiency, without the need for off-policy corrections. The actor, by slowly imitating the average greedy policy of the critics, leads to high-quality and state-specific exploration, which we compare to Thompson sampling. Because the actor and critics are fully decoupled, BDPI is remarkably stable, and unusually robust to its hyper-parameters. BDPI is significantly more sample-efficient than Bootstrapped DQN, PPO, and ACKTR, on discrete, continuous and pixel-based tasks."}}
