{"id": "458I8wip7Y", "cdate": 1640995200000, "mdate": 1672962538738, "content": {"title": "On-demand compute reduction with stochastic wav2vec 2.0", "abstract": ""}}
{"id": "Rf-i6THlPN", "cdate": 1609459200000, "mdate": 1672962538745, "content": {"title": "Lattice-Free Mmi Adaptation of Self-Supervised Pretrained Acoustic Models", "abstract": ""}}
{"id": "JwjE7UTCl1", "cdate": 1609459200000, "mdate": 1672962544996, "content": {"title": "Comparing CTC and LFMMI for Out-of-Domain Adaptation of wav2vec 2.0 Acoustic Model", "abstract": ""}}
{"id": "ONUryZVCRPq", "cdate": 1577836800000, "mdate": 1668688358059, "content": {"title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention", "abstract": "Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input\u2019s length, they are prohibitively slow for very long sequences. To addre..."}}
{"id": "NVRowQcV1Pg", "cdate": 1577836800000, "mdate": null, "content": {"title": "Pkwrap: a PyTorch Package for LF-MMI Training of Acoustic Models", "abstract": "We present a simple wrapper that is useful to train acoustic models in PyTorch using Kaldi's LF-MMI training framework. The wrapper, called pkwrap (short form of PyTorch kaldi wrapper), enables the user to utilize the flexibility provided by PyTorch in designing model architectures. It exposes the LF-MMI cost function as an autograd function. Other capabilities of Kaldi have also been ported to PyTorch. This includes the parallel training ability when multi-GPU environments are unavailable and decode with graphs created in Kaldi. The package is available on Github at https://github.com/idiap/pkwrap."}}
{"id": "EzgjNhrjWTK", "cdate": 1577836800000, "mdate": 1668688358129, "content": {"title": "Fast Transformers with Clustered Attention", "abstract": "Transformers have been proven a successful model for a variety of tasks in sequence modeling. However, computing the attention matrix, which is their key component, has quadratic complexity with respect to the sequence length, thus making them prohibitively expensive for large sequences. To address this, we propose clustered attention, which instead of computing the attention for every query, groups queries into clusters and computes attention just for the centroids. To further improve this approximation, we use the computed clusters to identify the keys with the highest attention per query and compute the exact key/query dot products. This results in a model with linear complexity with respect to the sequence length for a fixed number of clusters. We evaluate our approach on two automatic speech recognition datasets and show that our model consistently outperforms vanilla transformers for a given computational budget. Finally, we demonstrate that our model can approximate arbitrarily complex attention distributions with a minimal number of clusters by approximating a pretrained BERT model on GLUE and SQuAD benchmarks with only 25 clusters and no loss in performance."}}
{"id": "EPoZn8O1on", "cdate": 1546300800000, "mdate": null, "content": {"title": "Analyzing Uncertainties in Speech Recognition Using Dropout", "abstract": "The performance of Automatic Speech Recognition (ASR) systems is often measured using Word Error Rates (WER) which requires time-consuming and expensive manually transcribed data. In this paper, we use state-of-the-art ASR systems based on Deep Neural Networks (DNN) and propose a novel framework which uses \"Dropout\" at the test time to model uncertainty in prediction hypotheses. We systematically exploit this uncertainty to estimate WER without the need for explicit transcriptions. In addition, we show that the predictive uncertainty can also be used to accurately localize the errors made by the ASR system. We study the performance of our approach on Switchboard database where it predicts WER accurately within a range of 2.6% and 5.0% for HMM-DNN and Connectionist Temporal Classification (CTC) ASR systems, respectively."}}
{"id": "6ILzqg6-4bC", "cdate": 1546300800000, "mdate": null, "content": {"title": "Unbiased Semi-Supervised LF-MMI Training Using Dropout", "abstract": "The lattice-free MMI objective (LF-MMI) with finite-state transducer (FST) supervision lattice has been used in semi-supervised training of state-of-the-art neural network acoustic models for automatic speech recognition (ASR). However, the FST based supervision lattice does not sample from the posterior predictive distribution of word-sequences but only contains the decoding hypotheses corresponding to the Maximum Likelihood estimate of weights, so that the training might be biased towards incorrect hypotheses in the supervision lattice even if the best path is perfectly correct. In this paper, we propose a novel framework which uses Dropout at the test time to sample from the posterior predictive distribution of word-sequences to produce unbiased supervision lattices for semi-supervised training. We investigate the dropout sampling from both the acoustic model and the language model to generate supervision. Results on Fisher English show that the proposed approach achieves WER recovery of ~51.6% over regular semi-supervised LF-MMI training."}}
{"id": "B1NVSFZO-r", "cdate": 1514764800000, "mdate": null, "content": {"title": "Out-of-Distribution Detection Using an Ensemble of Self Supervised Leave-Out Classifiers", "abstract": "As deep learning methods form a critical part in commercially important applications such as autonomous driving and medical diagnostics, it is important to reliably detect out-of-distribution (OOD) inputs while employing these algorithms. In this work, we propose an OOD detection algorithm which comprises of an ensemble of classifiers. We train each classifier in a self-supervised manner by leaving out a random subset of training data as OOD data and the rest as in-distribution (ID) data. We propose a novel margin-based loss over the softmax output which seeks to maintain at least a margin m between the average entropy of the OOD and in-distribution samples. In conjunction with the standard cross-entropy loss, we minimize the novel loss to train an ensemble of classifiers. We also propose a novel method to combine the outputs of the ensemble of classifiers to obtain OOD detection score and class prediction. Overall, our method convincingly outperforms Hendrycks et\u00a0al.\u00a0[7] and the current state-of-the-art ODIN\u00a0[13] on several OOD detection benchmarks."}}
{"id": "ehq2nlZVtg", "cdate": 1451606400000, "mdate": 1672962544998, "content": {"title": "Power efficient compressive sensing for continuous monitoring of ECG and PPG in a wearable system", "abstract": ""}}
