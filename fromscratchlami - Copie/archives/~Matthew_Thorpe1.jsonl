{"id": "qVWyt4rllmM", "cdate": 1640995200000, "mdate": 1682318110658, "content": {"title": "GRAND++: Graph Neural Diffusion with A Source Term", "abstract": "We propose GRAph Neural Diffusion with a source term (GRAND++) for graph deep learning with a limited number of labeled nodes, i.e., low-labeling rate. GRAND++ is a class of continuous-depth graph deep learning architectures whose theoretical underpinning is the diffusion process on graphs with a source term. The source term guarantees two interesting theoretical properties of GRAND++: (i) the representation of graph nodes, under the dynamics of GRAND++, will not converge to a constant vector over all nodes even as the time goes to infinity, which mitigates the over-smoothing issue of graph neural networks and enables graph learning in very deep architectures. (ii) GRAND++ can provide accurate classification even when the model is trained with a very limited number of labeled training data. We experimentally verify the above two advantages on various graph deep learning benchmark tasks, showing a significant improvement over many existing graph neural networks."}}
{"id": "lla3VYmmOtW", "cdate": 1640995200000, "mdate": 1682318110310, "content": {"title": "Sliced Optimal Partial Transport", "abstract": "Optimal transport (OT) has become exceedingly popular in machine learning, data science, and computer vision. The core assumption in the OT problem is the equal total amount of mass in source and target measures, which limits its application. Optimal Partial Transport (OPT) is a recently proposed solution to this limitation. Similar to the OT problem, the computation of OPT relies on solving a linear programming problem (often in high dimensions), which can become computationally prohibitive. In this paper, we propose an efficient algorithm for calculating the OPT problem between two non-negative measures in one dimension. Next, following the idea of sliced OT distances, we utilize slicing to define the sliced OPT distance. Finally, we demonstrate the computational and accuracy benefits of the sliced OPT-based method in various numerical experiments. In particular, we show an application of our proposed Sliced-OPT in noisy point cloud registration."}}
{"id": "YFFGI9-yOV", "cdate": 1640995200000, "mdate": 1668682133113, "content": {"title": "The Linearized Hellinger-Kantorovich Distance", "abstract": "In this paper we study the local linearization of the Hellinger--Kantorovich distance via its Riemannian structure. We give explicit expressions for the logarithmic and exponential maps and identify a suitable notion of a Riemannian inner product. Samples can thus be represented as vectors in the tangent space of a suitable reference measure where the norm locally approximates the original metric. Working with the local linearization and the corresponding embeddings allows for the advantages of the Euclidean setting, such as faster computations and a plethora of data analysis tools, while still enjoying approximately the descriptive power of the Hellinger--Kantorovich metric."}}
{"id": "Xq8h5rT_BG", "cdate": 1640995200000, "mdate": 1682318110264, "content": {"title": "\u0393-Convergence of an Ambrosio-Tortorelli approximation scheme for image segmentation", "abstract": "Given an image $u_0$, the aim of minimising the Mumford-Shah functional is to find a decomposition of the image domain into sub-domains and a piecewise smooth approximation $u$ of $u_0$ such that $u$ varies smoothly within each sub-domain. Since the Mumford-Shah functional is highly non-smooth, regularizations such as the Ambrosio-Tortorelli approximation can be considered which is one of the most computationally efficient approximations of the Mumford-Shah functional for image segmentation. Our main result is the $\\Gamma$-convergence of the Ambrosio-Tortorelli approximation of the Mumford-Shah functional for piecewise smooth approximations. This requires the introduction of an appropriate function space. As a consequence of our $\\Gamma$-convergence result, we can infer the convergence of minimizers of the respective functionals."}}
{"id": "VC9R3vEKPc", "cdate": 1640995200000, "mdate": 1682318110337, "content": {"title": "Rates of Convergence for Regression with the Graph Poly-Laplacian", "abstract": "In the (special) smoothing spline problem one considers a variational problem with a quadratic data fidelity penalty and Laplacian regularisation. Higher order regularity can be obtained via replacing the Laplacian regulariser with a poly-Laplacian regulariser. The methodology is readily adapted to graphs and here we consider graph poly-Laplacian regularisation in a fully supervised, non-parametric, noise corrupted, regression problem. In particular, given a dataset $\\{x_i\\}_{i=1}^n$ and a set of noisy labels $\\{y_i\\}_{i=1}^n\\subset\\mathbb{R}$ we let $u_n:\\{x_i\\}_{i=1}^n\\to\\mathbb{R}$ be the minimiser of an energy which consists of a data fidelity term and an appropriately scaled graph poly-Laplacian term. When $y_i = g(x_i)+\\xi_i$, for iid noise $\\xi_i$, and using the geometric random graph, we identify (with high probability) the rate of convergence of $u_n$ to $g$ in the large data limit $n\\to\\infty$. Furthermore, our rate, up to logarithms, coincides with the known rate of convergence in the usual smoothing spline model."}}
{"id": "80Z2bEWVkq", "cdate": 1640995200000, "mdate": 1667900948698, "content": {"title": "Classification of datasets with imputed missing values: does imputation quality matter?", "abstract": "Classifying samples in incomplete datasets is a common aim for machine learning practitioners, but is non-trivial. Missing data is found in most real-world datasets and these missing values are typically imputed using established methods, followed by classification of the now complete, imputed, samples. The focus of the machine learning researcher is then to optimise the downstream classification performance. In this study, we highlight that it is imperative to consider the quality of the imputation. We demonstrate how the commonly used measures for assessing quality are flawed and propose a new class of discrepancy scores which focus on how well the method recreates the overall distribution of the data. To conclude, we highlight the compromised interpretability of classifier models trained using poorly imputed data."}}
{"id": "EMxu-dzvJk", "cdate": 1632875535689, "mdate": null, "content": {"title": "GRAND++: Graph Neural Diffusion with A Source Term", "abstract": "We propose GRAph Neural Diffusion with a source term (GRAND++) for graph deep learning with a limited number of labeled nodes, i.e., low-labeling rate. GRAND++ is a class of continuous-depth graph deep learning architectures whose theoretical underpinning is the diffusion process on graphs with a source term. The source term guarantees two interesting theoretical properties of GRAND++: (i) the representation of graph nodes, under the dynamics of GRAND++, will not converge to a constant vector over all nodes even as the time goes to infinity, which mitigates the over-smoothing issue of graph neural networks and enables graph learning in very deep architectures. (ii) GRAND++ can provide accurate classification even when the model is trained with a very limited number of labeled training data. We experimentally verify the above two advantages on various graph deep learning benchmark tasks, showing a significant improvement over many existing graph neural networks."}}
{"id": "FfgM1gymiW", "cdate": 1609459200000, "mdate": 1682318110748, "content": {"title": "Robust Certification for Laplace Learning on Geometric Graphs", "abstract": "Graph Laplacian (GL)-based semi-supervised learning is one of the most used approaches for clas- sifying nodes in a graph. Understanding and certifying the adversarial robustness of machine learn- ..."}}
{"id": "Cf1iNc1T1y", "cdate": 1609459200000, "mdate": 1682318110814, "content": {"title": "Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans", "abstract": "Many machine learning-based approaches have been developed for the prognosis and diagnosis of COVID-19 from medical images and this Analysis identifies over 2,200 relevant published papers and preprints in this area. After initial screening, 62 studies are analysed and the authors find they all have methodological flaws standing in the way of clinical utility. The authors have several recommendations to address these issues."}}
{"id": "cQyybLUoXxc", "cdate": 1601308039728, "mdate": null, "content": {"title": "Withdraw", "abstract": "we have withdrawn our paper."}}
