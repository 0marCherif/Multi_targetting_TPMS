{"id": "pcCBcVUFyY", "cdate": 1640995200000, "mdate": 1682254866695, "content": {"title": "A Johnson-Lindenstrauss Framework for Randomly Initialized CNNs", "abstract": "How does the geometric representation of a dataset change after the application of each randomly initialized layer of a neural network? The celebrated Johnson-Lindenstrauss lemma answers this question for linear fully-connected neural networks (FNNs), stating that the geometry is essentially preserved. For FNNs with the ReLU activation, the angle between two input contracts according to a known mapping. The question for non-linear convolutional neural networks (CNNs) becomes much more intricate. To answer this question, we introduce a geometric framework. For linear CNNs, we show that the Johnson--Lindenstrauss lemma continues to hold, namely, that the angle between two inputs is preserved. For CNNs with ReLU activation, on the other hand, the behavior is richer: The angle between the outputs contracts, where the level of contraction depends on the nature of the inputs. In particular, after one layer, the geometry of natural images is essentially preserved, whereas for Gaussian correlated inputs, CNNs exhibit the same contracting behavior as FNNs with ReLU activation."}}
{"id": "bCBRtxn6Fry", "cdate": 1640995200000, "mdate": 1682254866688, "content": {"title": "Optimal list decoding from noisy entropy inequality", "abstract": "A noisy entropy inequality for boolean functions by Samorodnitsky is applied to binary codes. It is shown that a binary code that achieves capacity on the binary erasure channel admits optimal list size for list decoding on some binary symmetric channels (in a regime where this optimal list size is exponentially large)."}}
{"id": "LfqKC8ttSA", "cdate": 1640995200000, "mdate": 1682254866697, "content": {"title": "An Initial Alignment between Neural Network and Target is Needed for Gradient Descent to Learn", "abstract": "This paper introduces the notion of \u201cInitial Alignment\u201d (INAL) between a neural network at initialization and a target function. It is proved that if a network and a Boolean target function do not ..."}}
{"id": "9aB-6i9pHV", "cdate": 1640995200000, "mdate": 1682254866695, "content": {"title": "An initial alignment between neural network and target is needed for gradient descent to learn", "abstract": "This paper introduces the notion of ``Initial Alignment'' (INAL) between a neural network at initialization and a target function. It is proved that if a network and a Boolean target function do not have a noticeable INAL, then noisy gradient descent on a fully connected network with normalized i.i.d. initialization will not learn in polynomial time. Thus a certain amount of knowledge about the target (measured by the INAL) is needed in the architecture design. This also provides an answer to an open problem posed in [AS20]. The results are based on deriving lower-bounds for descent algorithms on symmetric neural networks without explicit knowledge of the target function beyond its INAL."}}
{"id": "YX0lrvdPQc", "cdate": 1632875616347, "mdate": null, "content": {"title": "A Johnson-Lindenstrauss Framework for Randomly Initialized CNNs", "abstract": "How does the geometric representation of a dataset change after the application of each randomly initialized layer of a neural network? The celebrated Johnson-Lindenstrauss lemma answers this question for linear fully-connected neural networks (FNNs), stating that the geometry is essentially preserved. For FNNs with the ReLU activation, the angle between two input contracts according to a known mapping. The question for non-linear convolutional neural networks (CNNs) becomes much more intricate. To answer this question, we introduce a geometric framework. For linear CNNs, we show that the Johnson--Lindenstrauss lemma continues to hold, namely, that the angle between two inputs is preserved. For CNNs with ReLU activation, on the other hand, the behavior is richer: The angle between the outputs contracts, where the level of contraction depends on the nature of the inputs. In particular, after one layer, the geometry of natural images is essentially preserved, whereas for Gaussian correlated inputs, CNNs exhibit the same contracting behavior as FNNs with ReLU activation. "}}
{"id": "oSaxCWkqXP", "cdate": 1609459200000, "mdate": 1682254866705, "content": {"title": "A Johnson-Lindenstrauss Framework for Randomly Initialized CNNs", "abstract": "How does the geometric representation of a dataset change after the application of each randomly initialized layer of a neural network? The celebrated Johnson--Lindenstrauss lemma answers this question for linear fully-connected neural networks (FNNs), stating that the geometry is essentially preserved. For FNNs with the ReLU activation, the angle between two inputs contracts according to a known mapping. The question for non-linear convolutional neural networks (CNNs) becomes much more intricate. To answer this question, we introduce a geometric framework. For linear CNNs, we show that the Johnson--Lindenstrauss lemma continues to hold, namely, that the angle between two inputs is preserved. For CNNs with ReLU activation, on the other hand, the behavior is richer: The angle between the outputs contracts, where the level of contraction depends on the nature of the inputs. In particular, after one layer, the geometry of natural images is essentially preserved, whereas for Gaussian correlated inputs, CNNs exhibit the same contracting behavior as FNNs with ReLU activation."}}
{"id": "Oo6SNe45Rr3", "cdate": 1609459200000, "mdate": 1682254866709, "content": {"title": "Bayesian Decision Making in Groups is Hard", "abstract": "We study the computations that Bayesian agents undertake when exchanging opinions over a network. The agents act repeatedly on their private information and take myopic actions that maximize their expected utility according to a fully rational posterior belief. We show that such computations are NP-hard for two natural utility functions: one with binary actions and another where agents reveal their posterior beliefs. In fact, we show that distinguishing between posteriors that are concentrated on different states of the world is NP-hard. Therefore, even approximating the Bayesian posterior beliefs is hard. We also describe a natural search algorithm to compute agents\u2019 actions, which we call elimination of impossible signals, and show that if the network is transitive, the algorithm can be modified to run in polynomial time."}}
{"id": "LazZtYdkJp", "cdate": 1609459200000, "mdate": 1682254866703, "content": {"title": "Almost-Reed-Muller Codes Achieve Constant Rates for Random Errors", "abstract": "This paper considers \u201c <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\delta $ </tex-math></inline-formula> -almost Reed\u2013Muller codes\u201d, i.e., linear codes spanned by evaluations of all but a <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\delta $ </tex-math></inline-formula> fraction of monomials of degree at most <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$d$ </tex-math></inline-formula> . It is shown that for any <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\delta &gt; 0$ </tex-math></inline-formula> and any <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\varepsilon &gt;0$ </tex-math></inline-formula> , there exists a family of <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\delta $ </tex-math></inline-formula> -almost Reed\u2013Muller codes of constant rate that correct <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$1/2- \\varepsilon $ </tex-math></inline-formula> fraction of random errors with high probability. For exact Reed\u2013Muller codes, the analogous result is not known and represents a weaker version of the longstanding conjecture that Reed\u2013Muller codes achieve capacity for random errors (Abbe-Shpilka-Wigderson STOC \u201915). Our proof is based on the recent polarization result for Reed\u2013Muller codes, combined with a combinatorial approach to establishing inequalities between the Reed\u2013Muller code entropies."}}
{"id": "IT8l0ImEFd", "cdate": 1609459200000, "mdate": 1682254866711, "content": {"title": "On codes decoding a constant fraction of errors on the BSC", "abstract": "We strengthen the results from a recent work by the second author, achieving bounds on the weight distribution of binary linear codes that are successful under block-MAP (as well as bit-MAP) decoding on the BEC. We conclude that a linear code that is successful on the BEC can also decode over a range of binary memoryless symmetric (BMS) channels. In particular, applying the result of Kudekar, Kumar, Mondelli, Pfister, \u015ea\u015fo\u011flu and\u00a0Urbanke from STOC 2016, we prove that a Reed\u2013Muller code of positive rate R decodes errors on the p with high probability if p < 1/2 \u2212 \u221a2\u2212R(1\u22122\u2212R)."}}
{"id": "08zVClflkh", "cdate": 1609459200000, "mdate": 1682254866711, "content": {"title": "Regularization by Misclassification in ReLU Neural Networks", "abstract": "We study the implicit bias of ReLU neural networks trained by a variant of SGD where at each step, the label is changed with probability $p$ to a random label (label smoothing being a close variant of this procedure). Our experiments demonstrate that label noise propels the network to a sparse solution in the following sense: for a typical input, a small fraction of neurons are active, and the firing pattern of the hidden layers is sparser. In fact, for some instances, an appropriate amount of label noise does not only sparsify the network but further reduces the test error. We then turn to the theoretical analysis of such sparsification mechanisms, focusing on the extremal case of $p=1$. We show that in this case, the network withers as anticipated from experiments, but surprisingly, in different ways that depend on the learning rate and the presence of bias, with either weights vanishing or neurons ceasing to fire."}}
