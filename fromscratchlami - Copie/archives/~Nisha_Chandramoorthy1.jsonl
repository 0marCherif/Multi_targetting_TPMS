{"id": "4tGggvizjd8", "cdate": 1652737785799, "mdate": null, "content": {"title": "On the generalization of learning algorithms that do not converge", "abstract": "Generalization analyses of deep learning typically assume that the training converges to a fixed point. But, recent results indicate that in practice, the weights of deep neural networks optimized with stochastic gradient descent often oscillate indefinitely. To reduce this discrepancy between theory and practice, this paper focuses on the generalization of neural networks whose training dynamics do not necessarily converge to fixed points.  Our main contribution is to propose a notion of statistical algorithmic stability (SAS) that extends classical algorithmic stability to non-convergent algorithms and to study its connection to generalization. This ergodic-theoretic approach leads to new insights when compared to the traditional optimization and learning theory perspectives. We prove that the stability of the time-asymptotic behavior of a learning algorithm relates to its generalization and empirically demonstrate how loss dynamics can provide clues to generalization performance. Our findings provide evidence that networks that ``train stably generalize better'' even when the training continues indefinitely and the weights do not converge."}}
{"id": "oli9Q7kHno", "cdate": 1640995200000, "mdate": 1673302203416, "content": {"title": "On the generalization of learning algorithms that do not converge", "abstract": ""}}
{"id": "8AjcmU04fj", "cdate": 1640995200000, "mdate": 1683638337901, "content": {"title": "Efficient Computation of Linear Response of Chaotic Attractors with One-Dimensional Unstable Manifolds", "abstract": "The sensitivity of time averages in a chaotic system to an infinitesimal parameter perturbation grows exponentially with the averaging time. However, long-term averages or ensemble statistics often vary differentiably with system parameters. Ruelle's response theory gives a rigorous formula for these parametric derivatives of statistics or linear response. But the direct evaluation of this formula is ill-conditioned, and hence linear response and downstream applications of sensitivity analysis, such as optimization and uncertainty quantification, have been a computational challenge in chaotic dynamical systems. This paper presents the space-split sensitivity (S3) algorithm to transform Ruelle's formula into a well-conditioned ergodic-averaging computation. We prove a decomposition of Ruelle's formula that is differentiable on the unstable manifold, which we assume to be one-dimensional. This decomposition of Ruelle's formula ensures that one of the resulting terms, the stable contribution, can be computed using a regularized tangent equation, similarly as in a nonchaotic system. The remaining term, known as the unstable contribution, is regularized and converted into an efficiently computable ergodic average. In this process, we develop new algorithms, which may be useful beyond linear response, to compute the unstable derivatives of the regularized tangent vector field and the unstable direction. We prove that the S3 algorithm, which combines these computational ingredients that enter the stable and unstable contributions, converges like a Monte Carlo approximation of Ruelle's formula. The algorithm presented here is hence a first step toward full-fledged applications of sensitivity analysis in chaotic systems, wherever such applications have been limited due to lack of availability of long-term sensitivities."}}
{"id": "3QkH9cJ_iGp", "cdate": 1640995200000, "mdate": 1683638337905, "content": {"title": "On the generalization of learning algorithms that do not converge", "abstract": "Generalization analyses of deep learning typically assume that the training converges to a fixed point. But, recent results indicate that in practice, the weights of deep neural networks optimized with stochastic gradient descent often oscillate indefinitely. To reduce this discrepancy between theory and practice, this paper focuses on the generalization of neural networks whose training dynamics do not necessarily converge to fixed points. Our main contribution is to propose a notion of statistical algorithmic stability (SAS) that extends classical algorithmic stability to non-convergent algorithms and to study its connection to generalization. This ergodic-theoretic approach leads to new insights when compared to the traditional optimization and learning theory perspectives. We prove that the stability of the time-asymptotic behavior of a learning algorithm relates to its generalization and empirically demonstrate how loss dynamics can provide clues to generalization performance. Our findings provide evidence that networks that ``train stably generalize better'' even when the training continues indefinitely and the weights do not converge."}}
{"id": "jCK_f8xY9RR", "cdate": 1609459200000, "mdate": 1683638337905, "content": {"title": "On the probability of finding nonphysical solutions through shadowing", "abstract": ""}}
{"id": "i42pio2mx0T", "cdate": 1609459200000, "mdate": 1683638337899, "content": {"title": "Computational assessment of smooth and rough parameter dependence of statistics in chaotic dynamical systems", "abstract": ""}}
{"id": "uncpEznPs8", "cdate": 1546300800000, "mdate": 1683638337904, "content": {"title": "Toward computing sensitivities of average quantities in turbulent flows", "abstract": "Chaotic dynamical systems such as turbulent flows are characterized by an exponential divergence of infinitesimal perturbations to initial conditions. Therefore, conventional adjoint/tangent sensitivity analysis methods that are successful with RANS simulations fail in the case of chaotic LES/DNS. In this work, we discuss the limitations of current approaches, including ensemble-based and shadowing-based sensitivity methods, that were proposed as alternatives to conventional sensitivity analysis. We propose a new alternative, called the space-split sensitivity (S3) algorithm, that is computationally efficient and addresses these limitations. In this work, the derivation of the S3 algorithm is presented in the special case where the system converges to a stationary distribution that can be expressed with a probability density function everywhere in phase-space. Numerical examples of low-dimensional chaotic maps are discussed where S3 computation shows good agreement with finite-difference results, indicating potential for the development of the method in more generality."}}
