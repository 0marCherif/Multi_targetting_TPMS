{"id": "-gTqRt6RpqV", "cdate": 1663849915816, "mdate": null, "content": {"title": "Gated Class-Attention with Cascaded Feature Drift Compensation for Exemplar-free Continual Learning of Vision Transformers", "abstract": "Vision transformers (ViTs) have achieved remarkable successes across a broad range of computer vision applications. As a consequence there has been increasing interest in extending continual learning theory and techniques to ViT architectures. In this paper, we propose a new method for exemplar-free class incremental training of ViTs. The main challenge of exemplar-free continual learning is maintaining plasticity of the learner without causing catastrophic forgetting of previously learned tasks. This is often achieved via exemplar replay which can help recalibrate previous task classifiers to the feature drift which occurs when learning new tasks. Exemplar replay, however, comes at the cost of retaining samples from previous tasks which for some applications may not be possible.  To address the problem of continual ViT training, we first propose gated class-attention to minimize the drift in the final ViT transformer block. This mask-based gating is applied to class-attention mechanism of the last transformer block and strongly regulates the weights crucial for previous tasks. Secondly, we propose a new method of feature drift compensation that accommodates feature drift in the backbone when learning new tasks. The combination of gated class-attention and cascaded feature drift compensation allows for plasticity towards new tasks while limiting forgetting of previous ones. Extensive experiments performed on CIFAR-100 and Tiny-ImageNet demonstrate that our method outperforms existing exemplar-free state-of-the-art methods without the need to store any representative exemplars of past tasks."}}
{"id": "5J0rSbDcCHZ", "cdate": 1609459200000, "mdate": 1633525480096, "content": {"title": "Detection of Parkinson's Disease Early Progressors Using Routine Clinical Predictors", "abstract": "Parkinson's disease (PD) is a progressive, neurodegenerative disease characterised by the presence of motor and non-motor symptoms and signs. The symptoms of PD tend to begin very gradually and then become progressively more severe. The rate of PD progression is hard to predict and is different from one person to another. Namely, while in some patients the disease develops fast in just a few years from the diagnosis, in some the disease takes a more idle course and progresses slowly. We aimed to identify patients that develop severe motor symptoms within four years from PD diagnosis (early progressors) and separate them from those in whom severe symptoms develop beyond this point. We used data from the Parkinson\u2019s Progression Markers Initiative (PPMI) dataset to calculate motor progression of the disease by the use of motor scores as assessed by MDS-UPDRS III. The predictors were defined as baseline scores of selected clinical variables and the difference between motor scores at 1-year after enrolment in the study and the same scores at baseline. The rationale for predictor selection was that they should be readily available in routine clinical practice. We tested four different classifiers: logistic regression, decision tree, random forest, and gradient boosting. The best performing classifier was the logistic regression with an area under the ROC curve of 81%. We believe this can be the basis for a reliable and explainable classifier, using only standard clinical variables, for identifying early progressors with high recall (80%) three years in advance."}}
{"id": "rcHcCLNeRr", "cdate": 1577836800000, "mdate": 1633525480097, "content": {"title": "Recursive Recognition of Offline Handwritten Mathematical Expressions", "abstract": "In this paper we propose a method for Offline Handwritten Mathematical Expression recognition. The method is a fast and accurate thanks to its architecture, which include both a Convolutional Neural Network and a Recurrent Neural Network. The CNN extracts features from the image to recognize and its output is provided to the RNN which produces the mathematical expression encoded in the LATEX language. To process both sequential and non-sequential mathematical expressions we also included a deconvolutional module which, in a recursive way, segments the image for additional analysis trough a recursive process. The results obtained show a very high accuracy obtained on a large handwritten data set of 9100 samples of handwritten expressions."}}
