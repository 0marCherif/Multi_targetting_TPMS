{"id": "qMMzJGRPT2d", "cdate": 1591875144799, "mdate": null, "content": {"title": "Unsupervised Speech Separation Using Mixtures of Mixtures", "abstract": "Supervised approaches to single-channel speech separation rely on synthetic mixtures, so that the individual sources can be used as targets. Good performance depends upon how well the synthetic mixture data match real mixtures. However, matching synthetic data to the acoustic properties and distribution of sounds in a target domain can be challenging. Instead, we propose an unsupervised method that requires only single-channel acoustic mixtures, without ground-truth source signals. In this method, existing mixtures are mixed together to form a mixture of mixtures, which the model separates into latent sources.  We propose a novel loss that allows the latent sources to be remixed to approximate the original mixtures. Experiments show that this method can achieve competitive performance on speech separation compared to supervised methods. In a semi-supervised learning setting, our method enables domain adaptation by incorporating unsupervised mixtures from a matched domain. In particular, we demonstrate that significant improvement to reverberant speech separation performance can be achieved by incorporating reverberant mixtures."}}
{"id": "gQ_CmsJTuz_", "cdate": 1577836800000, "mdate": null, "content": {"title": "Performance Study of a Convolutional Time-Domain Audio Separation Network for Real-Time Speech Denoising", "abstract": "Time-domain audio separation networks based on dilated temporal convolutions have recently been shown to perform very well compared to methods that are based on a time-frequency representation in speech separation tasks, even outperforming an oracle binary time-frequency mask of the speakers. This paper investigates the performance of such a time-domain network (Conv-TasNet) for speech denoising in a real-time setting, comparing various parameter settings. Most importantly, different amounts of lookahead are evaluated and compared to the baseline of a fully causal model. We show that a large part of the increase in performance between a causal and non-causal model is achieved with a lookahead of only 20 milliseconds, demonstrating the usefulness of even small lookaheads for many real-time applications."}}
{"id": "V3c_pEcHwp8", "cdate": 1577836800000, "mdate": null, "content": {"title": "Unsupervised Sound Separation Using Mixtures of Mixtures", "abstract": "In recent years, rapid progress has been made on the problem of single-channel sound separation using supervised training of deep neural networks. In such supervised approaches, a model is trained to predict the component sources from synthetic mixtures created by adding up isolated ground-truth sources. Reliance on this synthetic training data is problematic because good performance depends upon the degree of match between the training data and real-world audio, especially in terms of the acoustic conditions and distribution of sources. The acoustic properties can be challenging to accurately simulate, and the distribution of sound types may be hard to replicate. In this paper, we propose a completely unsupervised method, mixture invariant training (MixIT), that requires only single-channel acoustic mixtures. In MixIT, training examples are constructed by mixing together existing mixtures, and the model separates them into a variable number of latent sources, such that the separated sources can be remixed to approximate the original mixtures. We show that MixIT can achieve competitive performance compared to supervised methods on speech separation. Using MixIT in a semi-supervised learning setting enables unsupervised domain adaptation and learning from large amounts of real world data without ground-truth source waveforms. In particular, we significantly improve reverberant speech separation performance by incorporating reverberant mixtures, train a speech enhancement system from noisy mixtures, and improve universal sound separation by incorporating a large amount of in-the-wild data."}}
{"id": "HsCkUZuUdGq", "cdate": 1577836800000, "mdate": null, "content": {"title": "Improving Sound Event Detection In Domestic Environments Using Sound Separation", "abstract": "Performing sound event detection on real-world recordings often implies dealing with overlapping target sound events and non-target sounds, also referred to as interference or noise. Until now these problems were mainly tackled at the classifier level. We propose to use sound separation as a pre-processing for sound event detection. In this paper we start from a sound separation model trained on the Free Universal Sound Separation dataset and the DCASE 2020 task 4 sound event detection baseline. We explore different methods to combine separated sound sources and the original mixture within the sound event detection. Furthermore, we investigate the impact of adapting the sound separation model to the sound event detection data on both the sound separation and the sound event detection."}}
{"id": "MPC_kbYTX_m", "cdate": 1546300800000, "mdate": null, "content": {"title": "Fixed-length asymmetric binary hashing for fingerprint verification through GMM-SVM based representations", "abstract": "Highlights \u2022 This study is a step towards a complete secure fingerprint authentication system in which fixed-length binary minutiae templates generated by our framework can be encrypted and matched via homomorphic encryption methods in an encrypted domain. \u2022 A method for converting unordered and varying number of minutiae into a fixed-length binary string is proposed. \u2022 The framework is based on GMM-SVM fingerprint verification scheme. Novel features are proposed for GMM-SVM since it is not clear what would be the best features to use for fingerprint. \u2022 GMM-SVM feature vectors are binarized using asymmetric locality sensitive hashing. \u2022 Higher recognition accuracies are obtained compared to the state-of-the-art methods. Abstract Fingerprint minutiae information is an unordered and variable-sized collection of minutiae locations and orientations. Advanced template protection algorithms which require a fixed-length binary template cannot operate on minutiae points. In this paper, we propose a novel framework that provides practical solutions that can be used in developing secure fingerprint verification systems. The framework, by using a GMM-SVM fingerprint representation scheme, first generates fixed-length feature vectors from minutiae point sets. The fixed-length representation enables the application of modern cryptographic alternatives based on homomorphic encryption to minutiae template protection. Our framework then utilizes an asymmetric locality sensitive hashing (ALSH) in order to convert the generated fixed-length but real valued GMM-SVM feature vector to a binary bit string. This binarization step transforms the matching process to calculating Hamming distance between binary vectors and expedites fingerprint matching. The verification performance of the framework is evaluated on FVC2002DB1A and DB2A databases."}}
{"id": "LscVkOTW_E5", "cdate": 1546300800000, "mdate": null, "content": {"title": "Single-channel Speech Extraction Using Speaker Inventory and Attention Network", "abstract": "Neural network-based speech separation has received a surge of interest in recent years. Previously proposed methods either are speaker independent or extract a target speaker's voice by using his or her voice snippet. In applications such as home devices or office meeting transcriptions, a possible speaker list is available, which can be leveraged for speech separation. This paper proposes a novel speech extraction method that utilizes an inventory of voice snippets of possible interfering speakers, or speaker enrollment data, in addition to that of the target speaker. Furthermore, an attention-based network architecture is proposed to form time-varying masks for both the target and other speakers during the separation process. This architecture does not reduce the enrollment audio of each speaker into a single vector, thereby allowing each short time frame of the input mixture signal to be aligned and accurately compared with the enrollment signals. We evaluate the proposed system on a speaker extraction task derived from the Libri corpus and show the effectiveness of the method."}}
{"id": "CBke-rbTpx", "cdate": 1546300800000, "mdate": null, "content": {"title": "Low-latency Speaker-independent Continuous Speech Separation", "abstract": "Speaker independent continuous speech separation (SI-CSS) is a task of converting a continuous audio stream, which may contain overlapping voices of unknown speakers, into a fixed number of continuous signals each of which contains no overlapping speech segment. A separated, or cleaned, version of each utterance is generated from one of SI-CSS's output channels nondeterministically without being split up and distributed to multiple channels. A typical application scenario is transcribing multi-party conversations, such as meetings, recorded with microphone arrays. The output signals can be simply sent to a speech recognition engine because they do not include speech overlaps. The previous SI-CSS method uses a neural network trained with permutation invariant training and a data-driven beamformer and thus requires much processing latency. This paper proposes a low-latency SI-CSS method whose performance is comparable to that of the previous method in a microphone array-based meeting transcription task. This is achieved (1) by using a new speech separation network architecture combined with a double buffering scheme and (2) by performing enhancement with a set of fixed beamformers followed by a neural post-filter."}}
{"id": "BXxTBcwVLty", "cdate": 1546300800000, "mdate": null, "content": {"title": "Using spatial overlap ratio of independent classifiers for likelihood map fusion in mean-shift tracking", "abstract": "We combine the outputs of independent classifiers for mean-shift tracking within the likelihood map fusion framework and introduce a novel likelihood fusion technique that directly employs the tracking confidences of likelihood maps which are generated by different binary classifiers. Our proposed measure tries to compensate drifting that may be caused by each likelihood map using their independent tracking results. We present results obtained with the proposed fusion approach using two different classifiers, where one models the tracked object and one models the background. The results show superior performance of the proposed fusion technique as compared to the others. We further discuss how the proposed likelihood map fusion approach can be generalized to any number and any kind of likelihood maps."}}
{"id": "6q7VqhpSOPg", "cdate": 1546300800000, "mdate": null, "content": {"title": "Universal Sound Separation", "abstract": "Recent deep learning approaches have achieved impressive performance on speech enhancement and separation tasks. However, these approaches have not been investigated for separating mixtures of arbitrary sounds of different types, a task we refer to as universal sound separation, and it is unknown how performance on speech tasks carries over to non-speech tasks. To study this question, we develop a dataset of mixtures containing arbitrary sounds, and use it to investigate the space of mask-based separation architectures, varying both the overall network architecture and the framewise analysis-synthesis basis for signal transformations. These network architectures include convolutional long short-term memory networks and time-dilated convolution stacks inspired by the recent success of time-domain enhancement networks like ConvTasNet. For the latter architecture, we also propose novel modifications that further improve separation performance. In terms of the framewise analysis-synthesis basis, we explore both a short-time Fourier transform (STFT) and a learnable basis, as used in ConvTasNet. For both of these bases, we also examine the effect of window size. In particular, for STFTs, we find that longer windows (25-50 ms) work best for speech/non-speech separation, while shorter windows (2.5 ms) work best for arbitrary sounds. For learnable bases, shorter windows (2.5 ms) work best on all tasks. Surprisingly, for universal sound separation, STFTs outperform learnable bases. Our best methods produce an improvement in scale-invariant signal-to-distortion ratio of over 13 dB for speech/non-speech separation and close to 10 dB for universal sound separation."}}
{"id": "-pq4bezKML8y", "cdate": 1546300800000, "mdate": null, "content": {"title": "Low-Latency Speaker-Independent Continuous Speech Separation", "abstract": "Speaker independent continuous speech separation (SI-CSS) is a task of converting a continuous audio stream, which may contain overlapping voices of unknown speakers, into a fixed number of continuous signals each of which contains no overlapping speech segment. A separated, or cleaned, version of each utterance is generated from one of SI-CSS's output channels nondeterministically without being split up and distributed to multiple channels. A typical application scenario is transcribing multi-party conversations, such as meetings, recorded with microphone arrays. The output signals can be simply sent to a speech recognition engine because they do not include speech overlaps. The previous SI-CSS method uses a neural network trained with permutation invariant training and a data-driven beamformer and thus requires much processing latency. This paper proposes a low-latency SI-CSS method whose performance is comparable to that of the previous method in a microphone array-based meeting transcription task.This is achieved (1) by using a new speech separation network architecture combined with a double buffering scheme and (2) by performing enhancement with a set of fixed beamformers followed by a neural post-filter."}}
