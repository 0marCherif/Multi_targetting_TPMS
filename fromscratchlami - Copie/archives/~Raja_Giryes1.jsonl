{"id": "FNpOIDeiEp", "cdate": 1675209600000, "mdate": 1682139418032, "content": {"title": "Mesh Draping: Parametrization-Free Neural Mesh Transfer", "abstract": ""}}
{"id": "QEsGYYCT7N", "cdate": 1672531200000, "mdate": 1681695866785, "content": {"title": "VisDA 2022 Challenge: Domain Adaptation for Industrial Waste Sorting", "abstract": "Label-efficient and reliable semantic segmentation is essential for many real-life applications, especially for industrial settings with high visual diversity, such as waste sorting. In industrial waste sorting, one of the biggest challenges is the extreme diversity of the input stream depending on factors like the location of the sorting facility, the equipment available in the facility, and the time of year, all of which significantly impact the composition and visual appearance of the waste stream. These changes in the data are called ``visual domains'', and label-efficient adaptation of models to such domains is needed for successful semantic segmentation of industrial waste. To test the abilities of computer vision models on this task, we present the VisDA 2022 Challenge on Domain Adaptation for Industrial Waste Sorting. Our challenge incorporates a fully-annotated waste sorting dataset, ZeroWaste, collected from two real material recovery facilities in different locations and seasons, as well as a novel procedurally generated synthetic waste sorting dataset, SynthWaste. In this competition, we aim to answer two questions: 1) can we leverage domain adaptation techniques to minimize the domain gap? and 2) can synthetic data augmentation improve performance on this task and help adapt to changing data distributions? The results of the competition show that industrial waste detection poses a real domain adaptation problem, that domain generalization techniques such as augmentations, ensembling, etc., improve the overall performance on the unlabeled target domain examples, and that leveraging synthetic data effectively remains an open problem. See https://ai.bu.edu/visda-2022/"}}
{"id": "Q-o5O6uMZf", "cdate": 1672531200000, "mdate": 1681157320359, "content": {"title": "TEXTure: Text-Guided Texturing of 3D Shapes", "abstract": ""}}
{"id": "8TsZXJRoD0", "cdate": 1672531200000, "mdate": 1682139413801, "content": {"title": "Set-the-Scene: Global-Local Training for Generating Controllable NeRF Scenes", "abstract": "Recent breakthroughs in text-guided image generation have led to remarkable progress in the field of 3D synthesis from text. By optimizing neural radiance fields (NeRF) directly from text, recent methods are able to produce remarkable results. Yet, these methods are limited in their control of each object's placement or appearance, as they represent the scene as a whole. This can be a major issue in scenarios that require refining or manipulating objects in the scene. To remedy this deficit, we propose a novel GlobalLocal training framework for synthesizing a 3D scene using object proxies. A proxy represents the object's placement in the generated scene and optionally defines its coarse geometry. The key to our approach is to represent each object as an independent NeRF. We alternate between optimizing each NeRF on its own and as part of the full scene. Thus, a complete representation of each object can be learned, while also creating a harmonious scene with style and lighting match. We show that using proxies allows a wide variety of editing options, such as adjusting the placement of each independent object, removing objects from a scene, or refining an object. Our results show that Set-the-Scene offers a powerful solution for scene synthesis and manipulation, filling a crucial gap in controllable text-to-3D synthesis."}}
{"id": "67o-oRpIH7l", "cdate": 1672531200000, "mdate": 1682139416019, "content": {"title": "A Function Space Analysis of Finite Neural Networks With Insights From Sampling Theory", "abstract": "This work suggests using sampling theory to analyze the function space represented by interpolating mappings. While the analysis in this paper is general, we focus it on neural networks with bounded weights that are known for their ability to interpolate (fit) the training data. First, we show, under the assumption of a finite input domain, which is the common case in training neural networks, that the function space generated by multi-layer networks with bounded weights, and non-expansive activation functions are smooth. This extends over previous works that show results for the case of infinite width ReLU networks. Then, under the assumption that the input is band-limited, we provide novel error bounds for univariate neural networks. We analyze both deterministic uniform and random sampling showing the advantage of the former."}}
{"id": "k9CF4h3muD", "cdate": 1663850402145, "mdate": null, "content": {"title": "Learning Low Dimensional State Spaces with Overparameterized Recurrent Neural Nets", "abstract": "Overparameterization in deep learning refers to settings where a trained Neural Network (NN) has representational capacity to fit the training data in many ways, some of which generalize well, while others do not. In the case of Recurrent Neural Networks (RNNs) there exists an additional layer of overparameterization, in the sense that a model may exhibit many solutions that generalize well for sequence lengths seen in training, some of which \\emph{extrapolate} to longer sequences, while others do not. Numerous works studied the tendency of Gradient Descent (GD) to fit overparameterized NNs with solutions that generalize well. On the other hand, its tendency to fit overparameterized RNNs with solutions that extrapolate has been discovered only lately, and is far less understood. In this paper, we analyze the extrapolation properties of GD when applied to overparameterized linear RNNs. In contrast to recent arguments suggesting an implicit bias towards short-term memory, we provide theoretical evidence for learning low dimensional state spaces, which can also model long-term memory. Our result relies on a dynamical characterization showing that GD (with small step size and near zero initialization) strives to maintain a certain form of balancedness, as well as tools developed in the context of the \\emph{moment problem} from statistics (recovery of discrete probability distribution from its moments). Experiments corroborate our theory, demonstrating extrapolation via learning low dimensional state spaces with both linear and non-linear RNNs."}}
{"id": "Hj1928tYlTf", "cdate": 1642351750805, "mdate": null, "content": {"title": "What is being transferred in transfer learning in NLP?", "abstract": "Despite vast and successful usage of transfer learning in NLP applications, we yet do not fully understand what enables a successful transfer and which part of the network is responsible for\nthat. In this paper, we address these fundamental questions for the transfer of word embeddings. \nWe report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors vs. the same CNN architecture trained on top of randomly initialized word vectors for multi-class question classification.\nThrough a series of analyses on word vectors transferring to block-shuffled sentences, we separate the effect of semantic feature reuse from learning low-level statistics of data.\nWe show that some of the benefits of transfer learning come from learning low-level data statistics.\nWe also show that when training from pre-trained word vectors, the models stay in the same basin of the loss landscape and different instances of such models are more similar in feature space and closer in parameter space than models trained with randomly initialized word vectors."}}
{"id": "z96qN8otPC", "cdate": 1640995200000, "mdate": 1682139413366, "content": {"title": "A Self Supervised StyleGAN for Image Annotation and Classification With Extremely Limited Labels", "abstract": "The recent success of learning-based algorithms can be greatly attributed to the immense amount of annotated data used for training. Yet, many datasets lack annotations due to the high costs associated with labeling, resulting in degraded performances of deep learning methods. Self-supervised learning is frequently adopted to mitigate the reliance on massive labeled datasets since it exploits unlabeled data to learn relevant feature representations. In this work, we propose SS-StyleGAN, a self-supervised approach for image annotation and classification suitable for extremely small annotated datasets. This novel framework adds self-supervision to the StyleGAN architecture by integrating an encoder that learns the embedding to the StyleGAN latent space, which is well-known for its disentangled properties. The learned latent space enables the smart selection of representatives from the data to be labeled for improved classification performance. We show that the proposed method attains strong classification results using small labeled datasets of sizes 50 and even 10. We demonstrate the superiority of our approach for the tasks of COVID-19 and liver tumor pathology identification."}}
{"id": "xJt_Z-03AH", "cdate": 1640995200000, "mdate": 1675665465012, "content": {"title": "A Diffusion Model Predicts 3D Shapes from 2D Microscopy Images", "abstract": "Diffusion models are a special type of generative model, capable of synthesising new data from a learnt distribution. We introduce DISPR, a diffusion-based model for solving the inverse problem of three-dimensional (3D) cell shape prediction from two-dimensional (2D) single cell microscopy images. Using the 2D microscopy image as a prior, DISPR is conditioned to predict realistic 3D shape reconstructions. To showcase the applicability of DISPR as a data augmentation tool in a feature-based single cell classification task, we extract morphological features from the red blood cells grouped into six highly imbalanced classes. Adding features from the DISPR predictions to the three minority classes improved the macro F1 score from $F1_\\text{macro} = 55.2 \\pm 4.6\\%$ to $F1_\\text{macro} = 72.2 \\pm 4.9\\%$. We thus demonstrate that diffusion models can be successfully applied to inverse biomedical problems, and that they learn to reconstruct 3D shapes with realistic morphological features from 2D microscopy images."}}
{"id": "wSIFjgs-VNW", "cdate": 1640995200000, "mdate": 1682139417195, "content": {"title": "NeuralMLS: Geometry-Aware Control Point Deformation", "abstract": "We introduce NeuralMLS, a space-based deformation technique, guided by a set of displaced control points. We leverage the power of neural networks to inject the underlying shape geometry into the deformation parameters. The goal of our technique is to enable a realistic and intuitive shape deformation. Our method is built upon moving least-squares (MLS), since it minimizes a weighted sum of the given control point displacements. Traditionally, the influence of each control point on every point in space (i.e., the weighting function) is defined using inverse distance heuristics. In this work, we opt to learn the weighting function, by training a neural network on the control points from a single input shape, and exploit the innate smoothness of neural networks. Our geometry-aware control point deformation is agnostic to the surface representation and quality; it can be applied to point clouds or meshes, including non-manifold and disconnected surface soups. We show that our technique facilitates intuitive piecewise smooth deformations, which are well suited for manufactured objects. We show the advantages of our approach compared to existing surface and space-based deformation techniques, both quantitatively and qualitatively."}}
