{"id": "cImlS4oD58F", "cdate": 1680225655651, "mdate": 1680225655651, "content": {"title": "From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked Transformers", "abstract": "In this paper we provide, to the best of our knowledge, the first comprehensive approach for incorporating various masking mechanisms into Transformers architectures in a scalable way. We show that recent results on linear causal attention\n(Choromanski et al., 2021) and log-linear RPEattention (Luo et al., 2021) are special cases of this\ngeneral mechanism. However by casting the problem as a topological (graph-based) modulation of\nunmasked attention, we obtain several results unknown before, including efficient d-dimensional\nRPE-masking and graph-kernel masking. We\nleverage many mathematical techniques ranging\nfrom spectral analysis through dynamic programming and random walks to new algorithms for\nsolving Markov processes on graphs. We provide\na corresponding empirical evaluation."}}
{"id": "Gyy2Izsijqe", "cdate": 1672531200000, "mdate": 1680225707296, "content": {"title": "Efficient Graph Field Integrators Meet Point Clouds", "abstract": ""}}
{"id": "yUxzNgTsTG", "cdate": 1640995200000, "mdate": 1680225707302, "content": {"title": "Meta-learning Pathologies from Radiology Reports using Variance Aware Prototypical Networks", "abstract": ""}}
{"id": "rEhr7zkExr-", "cdate": 1640995200000, "mdate": 1680225707298, "content": {"title": "From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked Transformers", "abstract": ""}}
{"id": "peQzdnKEO-O", "cdate": 1640995200000, "mdate": 1680225707297, "content": {"title": "Hybrid Random Features", "abstract": ""}}
{"id": "UVxHNeTDptP", "cdate": 1640995200000, "mdate": 1680225707294, "content": {"title": "Meta-learning Pathologies from Radiology Reports using Variance Aware Prototypical Networks", "abstract": ""}}
{"id": "N2ewUUG8AOk", "cdate": 1640995200000, "mdate": 1654317012873, "content": {"title": "Efficient Extraction of Pathologies from C-Spine Radiology Reports using Multi-Task Learning", "abstract": "Pretrained Transformer based models finetuned on domain specific corpora have changed the landscape of NLP. Generally, if one has multiple tasks on a given dataset, one may finetune different models or use task specific adapters. In this work, we show that a multi-task model can beat or achieve the performance of multiple BERT-based models finetuned on various tasks and various task specific adapter augmented BERT-based models. We validate our method on our internal radiologist's report dataset on cervical spine. We hypothesize that the tasks are semantically close and related and thus multitask learners are powerful classifiers. Our work opens the scope of using our method to radiologist's reports on various body parts."}}
{"id": "Ff1yfiP1__", "cdate": 1640995200000, "mdate": 1680225707300, "content": {"title": "Explaining the Effectiveness of Multi-Task Learning for Efficient Knowledge Extraction from Spine MRI Reports", "abstract": ""}}
{"id": "5TIh57n4Udb", "cdate": 1640995200000, "mdate": 1654317012871, "content": {"title": "Explaining the Effectiveness of Multi-Task Learning for Efficient Knowledge Extraction from Spine MRI Reports", "abstract": "Pretrained Transformer based models finetuned on domain specific corpora have changed the landscape of NLP. However, training or fine-tuning these models for individual tasks can be time consuming and resource intensive. Thus, a lot of current research is focused on using transformers for multi-task learning (Raffel et al.,2020) and how to group the tasks to help a multi-task model to learn effective representations that can be shared across tasks (Standley et al., 2020; Fifty et al., 2021). In this work, we show that a single multi-tasking model can match the performance of task specific models when the task specific models show similar representations across all of their hidden layers and their gradients are aligned, i.e. their gradients follow the same direction. We hypothesize that the above observations explain the effectiveness of multi-task learning. We validate our observations on our internal radiologist-annotated datasets on the cervical and lumbar spine. Our method is simple and intuitive, and can be used in a wide range of NLP problems."}}
{"id": "EMigfE6ZeS", "cdate": 1632875649520, "mdate": null, "content": {"title": "Hybrid Random Features", "abstract": "We propose a new class of random feature methods for linearizing softmax and Gaussian kernels called hybrid random features (HRFs) that automatically adapt the quality of kernel estimation to provide most accurate approximation in the defined regions of interest. Special instantiations of HRFs lead to well-known methods such as trigonometric (Rahimi & Recht, 2007) or (recently introduced in the context of linear-attention Transformers) positive random features (Choromanski et al., 2021). By generalizing Bochner\u2019s Theorem for softmax/Gaussian kernels and leveraging random features for compositional kernels, the HRF-mechanism provides strong theoretical guarantees - unbiased approximation and strictly smaller worst-case relative errors than its counterparts.  We conduct exhaustive empirical evaluation of HRF ranging from pointwise kernel estimation experiments, through tests on data admitting clustering structure to benchmarking implicit-attention Transformers (also for downstream Robotics applications), demonstrating its quality in a wide spectrum of machine learning problems."}}
