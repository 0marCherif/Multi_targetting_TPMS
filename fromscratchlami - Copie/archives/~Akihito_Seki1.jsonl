{"id": "v5yQPJ0sdDl", "cdate": 1668034205394, "mdate": 1668034205394, "content": {"title": "Simultaneous Optimization of Structure and Motion in Dynamic Scenes Using Unsynchronized Stereo Cameras", "abstract": "In this paper, we propose a simultaneous estimation method of structure and motion in dynamic scenes. Usual methods for obtaining structure and motion using stereo cameras require two kinds of operations: stereo correspondence and tracking. Therefore, we must separately determine the correspondence between stereo images and sequential images. This necessity complicates the algorithm and increases the possibility of mismatches because of the object's motion and visibility change in the images. Our proposed method makes two contributions. The first contribution is the method of corresponding all stereo images and sequential images at once. Therefore, we can obtain the structure and motion simultaneously and more accurately. On the other hand, most stereo correspondence algorithms are limited to use under a synchronized status. In a stereo rig using unsynchronized cameras, as are most commercially available cameras, the structure cannot be obtained by stereo correspondence and triangulation because of the unknown time offset between cameras. Therefore, our second contribution is a method of estimating structure, motion, and time offset simultaneously using unsynchronized stereo cameras. This latter task is accomplished by taking advantage of the first contribution scheme. Additionally, our method requires no preprocessing such as motion segmentation for separating identical-motion objects and advance calibration of the time offset. Finally, we present the experimental results using both synthetic and real images."}}
{"id": "W-iyAnClyjG", "cdate": 1668033998927, "mdate": null, "content": {"title": "SGM-Nets: Semi-Global Matching with Neural Networks", "abstract": "This paper deals with deep neural networks for predicting accurate dense disparity map with Semi-global matching (SGM). SGM is a widely used regularization method for real scenes because of its high accuracy and fast computation speed. Even though SGM can obtain accurate results, tuning of SGMs penalty-parameters, which control a smoothness and discontinuity of a disparity map, is uneasy and empirical methods have been proposed. We propose a learning based penalties estimation method, which we call SGM-Nets that consist of Convolutional Neural Networks. A small image patch and its position are input into SGMNets to predict the penalties for the 3D object structures. In order to train the networks, we introduce a novel loss function which is able to use sparsely annotated disparity maps such as captured by a LiDAR sensor in real environments. Moreover, we propose a novel SGM parameterization, which deploys different penalties depending on either positive or negative disparity changes in order to represent the object structures more discriminatively. Our SGM-Nets outperformed state of the art accuracy on KITTI benchmark datasets."}}
{"id": "ymyZM9QlL1", "cdate": 1609459200000, "mdate": 1667456788660, "content": {"title": "Multimodal Trajectory Predictions for Autonomous Driving without a Detailed Prior Map", "abstract": "Predicting the future trajectories of surrounding vehicles is a key competence for safe and efficient real-world autonomous driving systems. Previous works have presented deep neural network models for predictions using a detailed prior map which includes driving lanes and explicitly expresses the road rules like legal traffic directions and valid paths through intersections. Since it is unrealistic to assume the existence of the detailed prior maps for all areas, we use a map generated from only perceptual data (3D points measured by a LiDAR sensor). Such maps do not explicitly denote road rules, which makes prediction tasks more difficult. To overcome this problem, we propose a novel generative adversarial network (GAN) based framework. A discriminator in our framework can distinguish whether predicted trajectories follow road rules, and a generator can predict trajectories following it. Our framework implicitly extracts road rules by projecting trajectories onto the map via a differentiable function and training positional relations between trajectories and obstacles on the map. We also extend our framework to multimodal predictions so that various future trajectories are predicted. Experimental results show that our method outperforms other state-of-the-art methods in terms of trajectory errors and the ratio of trajectories that fall on drivable lanes."}}
{"id": "2KTN87Gx1pr", "cdate": 1609459200000, "mdate": 1667456788651, "content": {"title": "Absolute Scale from Varifocal Monocular Camera through SfM and Defocus Combined", "abstract": ""}}
{"id": "dRZ3bYPN8Xz", "cdate": 1577836800000, "mdate": 1667456788655, "content": {"title": "Uncertainty-Based Adaptive Sensor Fusion for Visual-Inertial Odometry under Various Motion Characteristics", "abstract": "We propose an uncertainty-based sensor fusion framework for visual-inertial odometry, which is the task of estimating relative motion using images and measurements from inertial measurement units. Visual-inertial odometry enables robust and scale-aware estimation of motion by incorporating sensor states, such as metric scale, velocity, and the direction of gravity, into the estimation. However, the observability of the states depends on sensor motion. For example, if the sensor moves in a constant velocity, scale and velocity cannot be observed from inertial measurements. Under these degenerate motions, existing methods may produce inaccurate results because they incorporate erroneous states estimated from non-informative inertial measurements. Our proposed framework is able to avoid this situation by adaptively switching estimation modes, which represents the states that should be incorporated, based on their uncertainties. These uncertainties can be obtained at a small computational cost by reusing the Jacobian matrices computed in bundle adjustment. Our approach consistently outperformed conventional sensor fusion in datasets with different motion characteristics, namely, the KITTI odometry dataset recorded by a ground vehicle and the EuRoC MAV dataset captured from a micro aerial vehicle."}}
{"id": "HPGHZ6-fjqh", "cdate": 1577836800000, "mdate": 1667456788656, "content": {"title": "Multimodal Trajectory Predictions for Urban Environments Using Geometric Relationships between a Vehicle and Lanes", "abstract": "Implementation of safe and efficient autonomous driving systems requires accurate prediction of the long-term trajectories of surrounding vehicles. High uncertainty in traffic behavior makes it difficult to predict trajectories in urban environments, which have various road geometries. To over-come this problem, we propose a method called lane-based multimodal prediction network (LAMP-Net), which can handle arbitrary shapes and numbers of traffic lanes and predict both the future trajectory along each lane and the probability of each lane being selected. A vector map is used to define the lane geometry and a novel lane feature is introduced to represent the generalized geometric relationships between the vehicle state and lanes. Our network takes this feature as the input and is trained to be versatile for arbitrarily shaped lanes. Moreover, we introduce a vehicle motion model constraint to our network. Our prediction method combined with the constraint significantly enhances prediction accuracy. We evaluate the prediction performance on two datasets which contain a wide variety of real-world traffic scenarios. Experimental results show that our proposed LAMP-Net outperforms state-of-the-art methods."}}
{"id": "XbZGZPmn3oL", "cdate": 1546300800000, "mdate": 1667456788655, "content": {"title": "SIR-Net: Scene-Independent End-to-End Trainable Visual Relocalizer", "abstract": "Visual relocalization addresses a problem of estimating the camera pose where a given query image is taken. In this paper, we focus on a scene-independent approach, which utilizes a database to maintain information of the scene. This approach thus enables adaptation to a new scene by switching a database. A standard procedure in this approach is to first retrieve images similar to the query from a database, then match 2D keypoints of the query image to 3D points visible in the retrieved images, and finally solve the perspective-n-point problem to estimate the pose. Recently, convolutional neural networks (CNNs) have been used for the retrieval and matching tasks in this process, and demonstrated promising accuracy and robustness. These CNNs are separately trained for retrieval and matching tasks, which could result in suboptimal relocalization accuracy. In this paper, we propose the first CNN-based relocalization framework, which is both scene-independent and end-to-end trainable. This framework jointly optimizes retrieval and matching tasks to maximize the relocalization accuracy by backpropagating the relocalization errors to both tasks. We demonstrate the effectiveness of the end-to-end training, robustness against new scenes, and the state-of-the-art accuracy on indoor and outdoor datasets, with computation done in real-time."}}
{"id": "OPVV13KDPow", "cdate": 1451606400000, "mdate": 1667456788804, "content": {"title": "Patch Based Confidence Prediction for Dense Disparity Map", "abstract": ""}}
{"id": "BLp5zg_Bqga", "cdate": 1451606400000, "mdate": 1667456788653, "content": {"title": "Quad-networks: unsupervised learning to rank for interest point detection", "abstract": "Several machine learning tasks require to represent the data using only a sparse set of interest points. An ideal detector is able to find the corresponding interest points even if the data undergo a transformation typical for a given domain. Since the task is of high practical interest in computer vision, many hand-crafted solutions were proposed. In this paper, we ask a fundamental question: can we learn such detectors from scratch? Since it is often unclear what points are \"interesting\", human labelling cannot be used to find a truly unbiased solution. Therefore, the task requires an unsupervised formulation. We are the first to propose such a formulation: training a neural network to rank points in a transformation-invariant manner. Interest points are then extracted from the top/bottom quantiles of this ranking. We validate our approach on two tasks: standard RGB image interest point detection and challenging cross-modal interest point detection between RGB and depth images. We quantitatively show that our unsupervised method performs better or on-par with baselines."}}
{"id": "p3XFdh9m2q", "cdate": 1388534400000, "mdate": 1667456788656, "content": {"title": "Reconstructing Fukushima: A Case Study", "abstract": "We present the application of 3D reconstruction technology to the inspection and decommissioning work at the damaged Fukushima Daiichi nuclear power station in Japan. We discuss the challenges of this project, such as the difficult image capture conditions (including under water), required use of limited imaging hardware, and capture by personnel inexperienced in 3D reconstruction. We present an overview of the system developed for this project, a real-time reconstruction pipeline with robust camera pose estimation, low-latency probabilistic dense depth estimation and a novel descriptor for point cloud alignment - the Co-occurrence Histogram of Angle and Distance (CHAD). We discuss the modifications required to standard algorithms in order to perform reliably in such a scenario. As well as quantitative evaluations of these components on existing datasets, we show qualitative 3D reconstruction results of debris from the damaged plant and its spent fuel pool. Such results have enabled planning of the critical process of debris removal, without the harmful requirement of extensive human presence on site."}}
