{"id": "dwKwB2Cd-Km", "cdate": 1652737786635, "mdate": null, "content": {"title": "Evaluation beyond Task Performance: Analyzing Concepts in AlphaZero in Hex", "abstract": "AlphaZero, an approach to reinforcement learning that couples neural networks and Monte Carlo tree search (MCTS), has produced state-of-the-art strategies for traditional board games like chess, Go, shogi, and Hex. While researchers and game commentators have suggested that AlphaZero uses concepts that humans consider important, it is unclear how these concepts are captured in the network. We investigate AlphaZero's internal representations in the game of Hex using two evaluation techniques from natural language processing (NLP): model probing and behavioral tests. In doing so, we introduce several new evaluation tools to the RL community, and illustrate how evaluations other than task performance can be used to provide a more complete picture of a model's strengths and weaknesses. Our analyses in the game of Hex reveal interesting patterns and generate some testable hypotheses about how such models learn in general. For example, we find that the MCTS discovers concepts before the neural network learns to encode them. We also find that concepts related to short-term end-game planning are best encoded in the final layers of the model, whereas concepts related to long-term planning are encoded in the middle layers of the model."}}
{"id": "mNtmhaDkAr", "cdate": 1601308425258, "mdate": null, "content": {"title": "Predicting Inductive Biases of Pre-Trained Models", "abstract": "Most current NLP systems are based on a pre-train-then-fine-tune paradigm, in which a large neural network is first trained in a self-supervised way designed to encourage the network to extract broadly-useful linguistic features, and then fine-tuned for a specific task of interest. Recent work attempts to understand why this recipe works and explain when it fails. Currently, such analyses have produced two sets of apparently-contradictory results. Work that analyzes the representations that result from pre-training (via \"probing classifiers\") finds evidence that rich features of linguistic structure can be decoded with high accuracy, but work that analyzes model behavior after fine-tuning (via \"challenge sets\") indicates that decisions are often not based on such structure but rather on spurious heuristics specific to the training set. In this work, we test the hypothesis that the extent to which a feature influences a model's decisions can be predicted using a combination of two factors: The feature's \"extractability\" after pre-training (measured using information-theoretic probing techniques), and the \"evidence\" available during fine-tuning (defined as the feature's co-occurrence rate with the label). In experiments with both synthetic and natural language data, we find strong evidence (statistically significant correlations) supporting this hypothesis."}}
{"id": "BJgtDa9GaH", "cdate": 1575296352866, "mdate": null, "content": {"title": "Reproducing \u201cTowards Interpretable ReinforcementLearning Using Attention Augmented Agents\u201d", "abstract": "We attempt to reproduce \"Towards Interpretable Reinforcement Learning UsingAttention Augment Agents\", a recent work which introduces a novel attention mechanism to understand the reasoning behind the predicted policy for canonical Atari games in a reinforcement learning setting. Like the original paper, our implementation utilizes the Importance Weighted Actor Critic Architecture (IMPALA). Our implementation, despite using considerable resources, was only trained for a fraction of the number of steps done by the original authors. Nonetheless, the results that we obtain from our early-training stage model indicate the validity of the authors\u2019 work."}}
{"id": "SkgJOAEtvr", "cdate": 1569439335358, "mdate": null, "content": {"title": "INTERNAL-CONSISTENCY CONSTRAINTS FOR EMERGENT COMMUNICATION", "abstract": "When communicating, humans rely on internally-consistent language representations. That is, as speakers, we expect listeners to behave the same way we do when we listen. This work proposes several methods for encouraging such internal consistency in dialog agents in an emergent communication setting. We consider two hypotheses about the effect of internal-consistency constraints: 1) that they improve agents\u2019 ability to refer to unseen referents, and 2) that they improve agents\u2019 ability to generalize across communicative roles (e.g. performing as a speaker de- spite only being trained as a listener). While we do not find evidence in favor of the former, our results show significant support for the latter."}}
