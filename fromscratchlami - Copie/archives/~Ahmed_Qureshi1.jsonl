{"id": "D0X97ODIYK", "cdate": 1686324877009, "mdate": null, "content": {"title": "Structural Concept Learning via Graph Attention for Multi-Level Rearrangement Planning", "abstract": "Robotic manipulation tasks, such as object rearrangement, play a crucial role in enabling robots to interact with complex and arbitrary environments. Existing work focuses primarily on single-level rearrangement planning and, even if multiple levels exist, dependency relations among substructures are geometrically simpler, like tower stacking. We propose Structural Concept Learning (SCL), a deep learning approach that leverages graph attention networks to perform multi-level object rearrangement planning for scenes with structural dependency hierarchies. It is trained on a self-generated simulation data set with intuitive structures, works for unseen scenes with an arbitrary number of objects and higher complexity of structures, infers independent substructures to allow for task parallelization over multiple manipulators, and generalizes to the real world. We compare our method with a range of classical and model-based baselines to show that our method leverages its scene understanding to achieve better performance, flexibility, and efficiency. The dataset, demonstration videos, supplementary details, and code implementation are available at: https://manavkulshrestha.github.io/scl"}}
{"id": "5TCkEk7g_5y", "cdate": 1686239567245, "mdate": 1686239567245, "content": {"title": "CoGrasp: 6-DoF Grasp Generation for Human-Robot Collaboration", "abstract": "Robot grasping is an actively studied area in \nrobotics, mainly focusing on the quality of generated grasps\nfor object manipulation. However, despite advancements, these\nmethods do not consider the human-robot collaboration settings\nwhere robots and humans will have to grasp the same objects\nconcurrently. Therefore, generating robot grasps compatible\nwith human preferences of simultaneously holding an object\nbecomes necessary to ensure a safe and natural collaboration\nexperience. In this paper, we propose a novel, deep neural\nnetwork-based method called CoGrasp that generates humanaware robot grasps by contextualizing human preference models of object grasping into the robot grasp selection process. We validate our approach against existing state-of-theart robot grasping methods through simulated and real-robot\nexperiments and user studies. In real robot experiments, our\nmethod achieves about 88% success rate in producing stable\ngrasps that also allow humans to interact and grasp objects\nsimultaneously in a socially compliant manner. Furthermore,\nour user study with 10 independent participants indicated our\napproach enables a safe, natural, and socially-aware humanrobot objects\u2019 co-grasping experience compared to a standard\nrobot grasping technique."}}
{"id": "I2Kd3yMLliA", "cdate": 1686239317979, "mdate": 1686239317979, "content": {"title": "Multi-Stage Monte Carlo Tree Search for Non-Monotone Object Rearrangement Planning in Narrow Confined Environments", "abstract": "Non-monotone object rearrangement planning in\nconfined spaces such as cabinets and shelves is a widely\noccurring but challenging problem in robotics. Both the robot\nmotion and the available regions for object relocation are\nhighly constrained because of the limited space. This work\nproposes a Multi-Stage Monte Carlo Tree Search (MS-MCTS)\nmethod to solve non-monotone object rearrangement planning\nproblems in confined spaces. Our approach decouples the\ncomplex problem into simpler subproblems using an object\nstage topology. A subgoal-focused tree expansion algorithm\nthat jointly considers the high-level planning and the lowlevel robot motion is designed to reduce the search space and\nbetter guide the search process. By fitting the task into the\nMCTS paradigm, our method produces optimistic solutions\nby balancing exploration and exploitation. The experiments\ndemonstrate that our method outperforms the existing methods\nregarding the planning time, the number of steps, and the total\nmove distance. Moreover, we deploy our MS-MCTS to a realworld robot system and verify its performance in different\nconfined environments."}}
{"id": "6JCmZbaPeR", "cdate": 1686239244491, "mdate": 1686239244491, "content": {"title": "Robot Active Neural Sensing and Planning in Unknown Cluttered Environments", "abstract": "Active sensing and planning in unknown, cluttered\nenvironments is an open challenge for robots intending to provide\nhome service, search and rescue, narrow-passage inspection, and\nmedical assistance. Although many active sensing methods exist,\nthey often consider open spaces, assume known settings, or mostly\ndo not generalize to real-world scenarios. We present the active\nneural sensing approach that generates the kinematically feasible\nviewpoint sequences for the robot manipulator with an in-hand\ncamera to gather the minimum number of observations needed to\nreconstruct the underlying environment. Our framework actively\ncollects the visual RGBD observations, aggregates them into scene\nrepresentation, and performs object shape inference to avoid\nunnecessary robot interactions with the environment. We train\nour approach on synthetic data with domain randomization and\ndemonstrate its successful execution via sim-to-real transfer in\nreconstructing narrow, covered, real-world cabinet environments\ncluttered with unknown objects. The natural cabinet scenarios\nimpose significant challenges for robot motion and scene reconstruction due to surrounding obstacles and low ambient lighting\nconditions. However, despite unfavorable settings, our method\nexhibits high performance compared to its baselines in terms of\nvarious environment reconstruction metrics, including planning\nspeed, the number of viewpoints, and overall scene coverage."}}
{"id": "rnmab4CQN_", "cdate": 1685890437487, "mdate": null, "content": {"title": "Progressive Learning for Physics-informed Neural Motion Planning", "abstract": "Neural motion planners (NMPs) demonstrate fast computational speed in finding path solutions but require a huge amount of expert trajectories for learning, thus adding a significant training computational load. In contrast, recent advancements have also led to a physics-informed NMP approach that directly solves the Eikonal equation for motion planning and does not require expert demonstrations for learning. However, experiments show that the physics-informed NMP approach performs poorly in complex environments and lacks scalability in high-dimensional real robot settings. To overcome these limitations, this paper presents a novel and tractable Eikonal equation formulation and introduces a new progressive learning strategy to train neural networks without expert data in complex, cluttered, high-dimensional robot motion planning scenarios. We show that our approach scales to the real robot set up in a narrow passage environment. \nThe proposed method's videos and code implementations are available at https://github.com/ruiqini/P-NTFields."}}
{"id": "7NcrDeuMM8", "cdate": 1676591079241, "mdate": null, "content": {"title": "Merging Decision Transformers: Weight Averaging for Forming Multi-Task Policies", "abstract": "Recent work has shown the promise of creating generalist, transformer-based, policies for language, vision, and sequential decision-making problems. To create such models, we generally require centralized training objectives, data, and compute. It is of interest if we can more flexibly create generalist policies, by merging together multiple, task-specific, individually trained policies. In this work, we take a preliminary step in this direction through merging, or averaging, subsets of Decision Transformers in weight space trained on different MuJoCo locomotion problems, forming multi-task models without centralized training. We also propose that when merging policies, we can obtain better results if all policies start from common, pre-trained initializations, while also co-training on shared auxiliary tasks during problem-specific finetuning. In general, we believe research in this direction can help democratize and distribute the process of which forms generally capable agents. "}}
{"id": "ApF0dmi1_9K", "cdate": 1663850150324, "mdate": null, "content": {"title": "NTFields: Neural Time Fields for Physics-Informed Robot Motion Planning", "abstract": "Neural Motion Planners (NMPs) have emerged as a promising tool for solving robot navigation tasks in complex environments. However, these methods often require expert data for learning, which limits their application to scenarios where data generation is time-consuming. Recent developments have also led to physics-informed deep neural models capable of representing complex dynamical Partial Differential Equations (PDEs). Inspired by these developments, we propose Neural Time Fields (NTFields) for robot motion planning in cluttered scenarios. Our framework represents a wave propagation model generating continuous arrival time to find path solutions informed by a nonlinear first-order PDE called Eikonal Equation. We evaluate our method in various cluttered 3D environments, including the Gibson dataset, and demonstrate its ability to solve motion planning problems for 4-DOF and 6-DOF robot manipulators where the traditional grid-based Eikonal planners often face the curse of dimensionality. Furthermore, the results show that our method exhibits high success rates and significantly lower computational times than the state-of-the-art methods, including NMPs that require training data from classical planners."}}
{"id": "LUOSN8opID1", "cdate": 1663849966006, "mdate": null, "content": {"title": "Constrained Hierarchical Deep Reinforcement Learning with Differentiable Formal Specifications", "abstract": "Formal logic specifications are a useful tool to describe desired agent behavior and have been explored as a means to shape rewards in Deep Reinforcement Learning (DRL) systems over a variety of problems and domains. Prior work, however, has failed to consider the possibility of making these specifications differentiable, which would yield a more informative signal of the objective via the specification gradient. This paper examines precisely such an approach by exploring a Lagrangian method to constrain policy updates using a differentiable style of temporal logic specifications that associates logic formulae with real-valued quantitative semantics. This constrained learning mechanism is then used in a hierarchical setting where a high-level specification-guided neural network path planner works with a low-level control policy to navigate through planned waypoints. The effectiveness of our approach is demonstrated over four robot dynamics with five different types of Linear Temporal Logic (LTL) specifications. Our demo videos are collected at https://sites.google.com/view/schrl."}}
{"id": "0V0_nNCiSLl", "cdate": 1640995200000, "mdate": 1666982185459, "content": {"title": "Constrained Motion Planning Networks X", "abstract": "Constrained motion planning is a challenging field of research, aiming for computationally efficient methods that can find a collision-free path on the constraint manifolds between a given start and goal configuration. These planning problems come up surprisingly frequently, such as in robot manipulation for performing daily life assistive tasks. However, few solutions to constrained motion planning are available, and those that exist struggle with high computational time complexity in finding a path solution on the manifolds. To address this challenge, we present Constrained Motion Planning Networks X (CoMPNetX). It is a neural planning approach, comprising a conditional deep neural generator and discriminator with neural gradients-based fast projection operator. We also introduce neural task and scene representations conditioned on which the CoMPNetX generates implicit manifold configurations to turbo-charge any underlying classical planner such as sampling-based motion planning methods for quickly solving complex constrained planning tasks. We show that our method finds path solutions with high success rates and lower computation times than state-of-the-art traditional path-finding tools on various challenging scenarios."}}
{"id": "6Jf6HX4MoLH", "cdate": 1632875674586, "mdate": null, "content": {"title": "Motion Planning Transformers: One Model to Plan them All", "abstract": "Transformers have become the powerhouse of natural language processing and recently found use in computer vision tasks. Their effective use of attention can be used in other contexts as well, and in this paper, we propose a transformer-based approach for efficiently solving complex motion planning problems. Traditional neural network-based motion planning uses convolutional networks to encode the planning space, but these methods are limited to fixed map sizes, which is often not realistic in the real world. Our approach first identifies regions on the map using transformers to provide attention to map areas likely to include the best path and then applies traditional planners to generate the final collision-free path. We validate our method on a variety of randomly generated environments with different map sizes, demonstrating reduction in planning complexity and achieving comparable accuracy to traditional planners.\n"}}
