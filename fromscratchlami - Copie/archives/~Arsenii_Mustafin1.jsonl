{"id": "_AkC4QYxF5", "cdate": 1663849866957, "mdate": null, "content": {"title": "Closing the Gap Between SVRG and TD-SVRG with Gradient Splitting", "abstract": "Temporal difference (TD) learning is a simple algorithm for policy evaluation\nin reinforcement learning. The performance of TD learning is affected by high\nvariance and it can be naturally enhanced with variance reduction techniques, such\nas the Stochastic Variance Reduced Gradient (SVRG) method. Recently, multiple\nworks have sought to fuse TD learning with SVRG to obtain a policy evaluation\nmethod with a linear rate of convergence. However, the resulting convergence rate\nis significantly weaker than what is achieved by SVRG in the setting of convex\noptimization. In this work we utilize a recent interpretation of TD-learning as the\nsplitting of the gradient of an appropriately chosen function, thus simplifying the\nalgorithm and fusing TD with SVRG. We prove a linear convergence bound that\nis identical to the convergence bound available for SVRG in the convex setting."}}
{"id": "dMebGZDEG6J", "cdate": 1640995200000, "mdate": 1683620130652, "content": {"title": "Closing the gap between SVRG and TD-SVRG with Gradient Splitting", "abstract": "Temporal difference (TD) learning is a policy evaluation in reinforcement learning whose performance can be enhanced by variance reduction techniques. Recently, multiple works have sought to fuse TD learning with SVRG to obtain a policy evaluation method with a geometric rate of convergence. However, the resulting convergence rate is significantly weaker than what is achieved by SVRG in the setting of convex optimization. In this work we utilize a recent interpretation of TD-learning as the splitting of the gradient of an appropriately chosen function, thus simplifying the algorithm and fusing TD with SVRG. Our main result is a geometric convergence bound with predetermined learning rate of $1/8$, which is identical to the convergence bound available for SVRG in the convex setting. Our theoretical findings are supported by a set of experiments."}}
{"id": "4wK8onfFaJ", "cdate": 1640995200000, "mdate": 1683557009961, "content": {"title": "Ani-GIFs: A benchmark dataset for domain generalization of action recognition from GIFs", "abstract": "Deep learning models perform remarkably well for the same task under the assumption that data is always coming from the same distribution. However, this is generally violated in practice, mainly due to the differences in data acquisition techniques and the lack of information about the underlying source of new data. Domain generalization targets the ability to generalize to test data of an unseen domain; while this problem is well-studied for images, such studies are significantly lacking in spatiotemporal visual content\u2014videos and GIFs. This is due to (1) the challenging nature of misalignment of temporal features and the varying appearance/motion of actors and actions in different domains, and (2) spatiotemporal datasets being laborious to collect and annotate for multiple domains. We collect and present the first synthetic video dataset of Animated GIFs for domain generalization, Ani-GIFs, that is used to study the domain gap of videos vs. GIFs, and animated vs. real GIFs, for the task of action recognition. We provide a training and testing setting for Ani-GIFs, and extend two domain generalization baseline approaches, based on data augmentation and explainability, to the spatiotemporal domain to catalyze research in this direction."}}
