{"id": "Z24qJD_IrXA", "cdate": 1672531200000, "mdate": 1679901406719, "content": {"title": "A Near-Optimal Algorithm for Bilevel Empirical Risk Minimization", "abstract": ""}}
{"id": "1uSzacpyWLH", "cdate": 1652737787285, "mdate": null, "content": {"title": "Benchopt: Reproducible, efficient and collaborative optimization benchmarks", "abstract": "Numerical validation is at the core of machine learning research as it allows us to assess the actual impact of new methods, and to confirm the agreement between theory and practice. Yet, the rapid development of the field poses several challenges: researchers are confronted with a profusion of methods to compare, limited transparency and consensus on best practices, as well as tedious re-implementation work. As a result, validation is often very partial, which can lead to wrong conclusions that slow down the progress of research. We propose Benchopt, a collaborative framework to automatize, publish and reproduce optimization benchmarks in machine learning across programming languages and hardware architectures. Benchopt simplifies benchmarking for the community by providing an off-the-shelf tool for running, sharing and extending experiments. To demonstrate its broad usability, we showcase benchmarks on three standard ML tasks: $\\ell_2$-regularized logistic regression, Lasso and ResNet18 training for image classification. These benchmarks highlight key practical findings that give a more nuanced view of state-of-the-art for these problems, showing that for practical evaluation, the devil is in the details."}}
{"id": "wlEOsQ917F", "cdate": 1652737351337, "mdate": null, "content": {"title": "A framework for bilevel optimization that enables  stochastic and global variance reduction algorithms", "abstract": "Bilevel optimization, the problem of minimizing a value function which involves the arg-minimum of another function, appears in many areas of machine learning. In a large scale empirical risk minimization setting where the number of samples is huge, it is crucial to develop stochastic methods, which only use a few samples at a time to progress. However, computing the gradient of the value function involves solving a linear system, which makes it difficult to derive unbiased stochastic estimates.\nTo overcome this problem we introduce a novel framework, in which the solution of the inner problem, the solution of the linear system, and the main variable evolve at the same time. These directions are written as a sum, making it straightforward to derive unbiased estimates.\nThe simplicity of our approach allows us to develop global variance reduction algorithms, where the dynamics of all variables is subject to variance reduction.\nWe demonstrate that SABA, an adaptation of the celebrated SAGA algorithm in our framework, has $O(\\frac1T)$ convergence rate, and that it achieves linear convergence under Polyak-Lojasciewicz assumption.\nThis is the first stochastic algorithm for bilevel optimization that verifies either of these properties.\nNumerical experiments validate the usefulness of our method."}}
