{"id": "q__4oPiZhQ", "cdate": 1699695292643, "mdate": 1699695292643, "content": {"title": "Hierarchical Compression and Compensation for Anomaly Detection", "abstract": "Targeting for detecting anomalies of various sizes for complicated normal patterns, we propose a Hierarchical Compression and Compensation method, which introduces two key techniques, scalable bottleneck compression and template-guided compensation, for anomaly-free feature restoration. Specially, our framework compresses image features by scalable bottleneck to preserve the most crucial features shared among normal samples, so that anomalous features could be filtered out during inference. Since the image features are distorted after compression, we choose the most similar normal sample as the template, and leverage the hierarchical features from the template to compensate the distorted features for anomaly-free feature restoration. Experimental results demonstrate the effectiveness of our approach, which achieves the state-of-the-art performance on the MVTec LOCO AD dataset."}}
{"id": "4aqHrRSv0HV", "cdate": 1668686558875, "mdate": 1668686558875, "content": {"title": "Inertia-Guided Flow Completion and Style Fusion for Video Inpainting", "abstract": "Physical objects have inertia, which resists changes in the velocity and motion direction. Inspired by this, we introduce inertia prior that optical flow, which reflects object motion in a local temporal window, keeps unchanged in the adjacent preceding or subsequent frame. We propose a flow completion network to align and aggregate flow features from the consecutive flow sequences based on the inertia prior. The corrupted flows are completed under the supervision of customized losses on reconstruction, flow smoothness, and consistent ternary census transform. The completed flows with high fidelity give rise to significant improvement on the video inpainting quality. Nevertheless, the existing flow-guided cross-frame warping methods fail to consider the lightening and sharpness variation across video frames, which leads to spatial incoherence after warping from other frames. To alleviate such problem, we propose the Adaptive Style Fusion Network (ASFN), which utilizes the style information extracted from the valid regions to guide the gradient refinement in the warped regions. Moreover, we design a data simulation pipeline to reduce the training difficulty of ASFN. Extensive experiments show the superiority of our method against the state-of-the-art methods quantitatively and qualitatively. The project page is at https://github.com/hitachinsk/ISVI.\n"}}
{"id": "ddmBrqYWzM", "cdate": 1668047667308, "mdate": 1668047667308, "content": {"title": "Flow-Guided Transformer for Video Inpainting", "abstract": "We propose a flow-guided transformer, which innovatively leverage the motion discrepancy exposed by optical flows to instruct\nthe attention retrieval in transformer for high fidelity video inpainting. More specially, we design a novel flow completion network to complete the corrupted flows by exploiting the relevant flow features in a local temporal window. With the completed flows, we propagate the content across video frames, and adopt the flow-guided transformer to synthesize the rest corrupted regions. We decouple transformers along temporal and spatial dimension, so that we can easily integrate the locally relevant completed flows to instruct spatial attention only. Furthermore, we design a flow-reweight module to precisely control the impact of completed flows on each spatial transformer. For the sake of efficiency, we introduce window partition strategy to both spatial and temporal transformers. Especially in spatial transformer, we design a dual perspective spatial MHSA, which integrates the global tokens to the window-based attention. Extensive experiments demonstrate the effectiveness of the proposed method qualitatively and quantitatively."}}
{"id": "LGyajrH1I0", "cdate": 1582000220270, "mdate": null, "content": {"title": "Real-time Gesture Recognition for Interactive Presentation with Depth Sensor", "abstract": "In this paper, we propose a method to enable real-time interaction between the projection contents and speaker through detecting and recognizing meaningful human gestures from depth maps captured by depth sensor, making projection screen as a kind of touch screen. Considering that depth noise and serious occlusion may ruin the construction of skeleton, our hand trajectory is derived from Potential Active Region. To cope with their inter-class and intra-class variations, hand trajectory is temporally segmented into movements, which are represented as Motion History Images. A novel set-based soft discriminative model is learned to recognize gestures from these movements. In addition, as it is a real-time system, a complexity reduction method is employed. The proposed approach is evaluated on our dataset and performs efficiently and robustly with 90% correct recognition rate."}}
{"id": "cfszJHmbpP", "cdate": 1581999287393, "mdate": null, "content": {"title": "Texture-assisted Kinect Depth Inpainting", "abstract": "The emergence of Kinect facilitates the possibility of depth capture in real-time and with low cost by consumers. It\nalso provides powerful tool and inspiration for researchers to engage in new array of technology development. However, the quality of the depth map captured from Kinect is still inadequate for many applications due to holes, noises and artifacts\nexisting within the depth information. In this paper, we present a texture assisted Kinect depth inpainting framework, aiming at obtaining improved depth information. In this framework, the relationship between texture and depth is investigated, and the characteristics of depth are also exploited. More specifically, texture edge information is extracted to assist the depth inpainting. Furthermore, filtering and diffusion are designed for hole filling and edge alignment. Experiment results demonstrate that the Kinect depth can be appropriately repaired in both smooth and edge region. Comparing with the original depth, the inpainted depth information enhances the quality of advanced processing such as 3D reconstruction"}}
{"id": "zlkew3ra4", "cdate": 1581999097150, "mdate": null, "content": {"title": "A High-Fidelity and Low-Interaction-Delay Screen Sharing System", "abstract": "The pervasive computing environment and wide network bandwidth provide users more opportunities to share screen content among multiple devices. In this article, we introduce a remote display system to enable screen sharing among multiple devices with high fidelity and responsive interaction. In the developed system, the frame-level screen content is compressed and transmitted to the client side for screen sharing, and the instant control inputs are simultaneously transmitted to the server side for interaction. Even if the screen responds immediately to the control messages and updates at a high frame rate on the server side, it is difficult to update the screen content with low delay and high frame rate in the client side due to non-negligible time consumption on the whole screen frame compression, transmission, and display buffer updating. To address this critical problem, we propose a layered structure for screen coding and rendering to deliver diverse screen content to the client side with an adaptive frame rate. More specifically, the interaction content with small region screen update is compressed by a blockwise screen codec and rendered at a high frame rate to achieve smooth interaction, while the natural video screen content is compressed by standard video codec and rendered at a regular frame rate for a smooth video display. Experimental results with real applications demonstrate that the proposed system can successfully reduce transmission bandwidth cost and interaction delay during screen sharing. Especially for user interaction in small regions, the proposed system can achieve a higher frame rate than most previous counterparts."}}
{"id": "BNWs87D3A4", "cdate": 1581998576406, "mdate": null, "content": {"title": "Layered Compression for High-Precision Depth Data", "abstract": "With the development of depth data acquisition technologies, access to high-precision depth with more than 8-b depths has become much easier and determining how to efficiently represent and compress high-precision depth is essential for practical depth storage and transmission systems. In this paper, we propose a layered high-precision depth compression framework based on an 8-b image/video encoder to achieve efficient compression with low complexity. Within this framework, considering the characteristics of the high-precision depth, a depth map is partitioned into two layers: 1) the most significant bits (MSBs) layer and 2) the least significant bits (LSBs) layer. The MSBs layer provides rough depth value distribution, while the LSBs layer records the details of the depth value variation. For the MSBs layer, an error-controllable pixel domain encoding scheme is proposed to exploit the data correlation of the general depth information with sharp edges and to guarantee the data format of LSBs layer is 8 b after taking the quantization error from MSBs layer. For the LSBs layer, standard 8-b image/video codec is leveraged to perform the compression. The experimental results demonstrate that the proposed coding scheme can achieve real-time depth compression with satisfactory reconstruction quality. Moreover, the compressed depth data generated from this scheme can achieve better performance in view synthesis and gesture recognition applications compared with the conventional coding schemes because of the error control algorithm."}}
{"id": "rJby5kz_bH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Feature Selective Networks for Object Detection", "abstract": "Objects for detection usually have distinct characteristics in different sub-regions and different aspect ratios. However, in prevalent two-stage object detection methods, Region-of-Interest (RoI) features are extracted by RoI pooling with little emphasis on these translation-variant feature components. We present feature selective networks to reform the feature representations of RoIs by exploiting their disparities among sub-regions and aspect ratios. Our network produces the sub-region attention bank and aspect ratio attention bank for the whole image. The RoI-based sub-region attention map and aspect ratio attention map are selectively pooled from the banks, and then used to refine the original RoI features for RoI classification. Equipped with a light-weight detection subnetwork, our network gets a consistent boost in detection performance based on general ConvNet backbones (ResNet-101, GoogLeNet and VGG-16). Without bells and whistles, our detectors equipped with ResNet-101 achieve more than 3% mAP improvement compared to counterparts on PASCAL VOC 2007, PASCAL VOC 2012 and MS COCO datasets."}}
