{"id": "AqZFrvqOD22", "cdate": 1663907299723, "mdate": 1663907299723, "content": {"title": "No Language Left Behind: Scaling Human-Centered Machine Translation", "abstract": "Driven by the goal of eradicating language barriers on a global scale, machine translation\nhas solidified itself as a key focus of artificial intelligence research today. However, such\nefforts have coalesced around a small subset of languages, leaving behind the vast majority\nof mostly low-resource languages. What does it take to break the 200 language barrier\nwhile ensuring safe, high quality results, all while keeping ethical considerations in mind?\nIn No Language Left Behind, we took on this challenge by first contextualizing the need\nfor low-resource language translation support through exploratory interviews with native\nspeakers. Then, we created datasets and models aimed at narrowing the performance gap\nbetween low and high-resource languages. More specifically, we developed a conditional\ncompute model based on Sparsely Gated Mixture of Experts that is trained on data obtained\nwith novel and effective data mining techniques tailored for low-resource languages. We\npropose multiple architectural and training improvements to counteract overfitting while\ntraining on thousands of tasks. Critically, we evaluated the performance of over 40,000\ndifferent translation directions using a human-translated benchmark, Flores-200, and\ncombined human evaluation with a novel toxicity benchmark covering all languages in\nFlores-200 to assess translation safety. Our model achieves an improvement of 44% BLEU\nrelative to the previous state-of-the-art, laying important groundwork towards realizing a\nuniversal translation system. Finally, we open source all contributions described in this\nwork, accessible at https://github.com/facebookresearch/fairseq/tree/nllb."}}
{"id": "6IfSrZJFPiu", "cdate": 1652937814767, "mdate": 1652937814767, "content": {"title": "FLAVA: A Foundational Language And Vision Alignment Model", "abstract": "State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal (with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising direction would be to use a single holistic universal model, as a \"foundation\", that targets all modalities at once -- a true vision and language foundation model should be good at vision tasks, language tasks, and cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate impressive performance on a wide range of 35 tasks spanning these target modalities."}}
{"id": "vsCCDVdTAx", "cdate": 1621629863877, "mdate": null, "content": {"title": "Human-Adversarial Visual Question Answering", "abstract": "Performance on the most commonly used Visual Question Answering dataset (VQA v2) is starting to approach human accuracy. However, in interacting with state-of-the-art VQA models, it is clear that the problem is far from being solved. In order to stress test VQA models, we benchmark them against human-adversarial examples. Human subjects interact with a state-of-the-art VQA model, and for each image in the dataset, attempt to find a question where the model\u2019s predicted answer is incorrect. We find that a wide range of state-of-the-art models perform poorly when evaluated on these examples. We conduct an extensive analysis of the collected adversarial examples and provide guidance on future research directions. We hope that this Adversarial VQA (AdVQA) benchmark can help drive progress in the field and advance the state of the art."}}
{"id": "rWs3UqbQCTX", "cdate": 1609459200000, "mdate": 1648684674507, "content": {"title": "Creative Sketch Generation", "abstract": "Sketching or doodling is a popular creative activity that people engage in. However, most existing work in automatic sketch understanding or generation has focused on sketches that are quite mundane. In this work, we introduce two datasets of creative sketches -- Creative Birds and Creative Creatures -- containing 10k sketches each along with part annotations. We propose DoodlerGAN -- a part-based Generative Adversarial Network (GAN) -- to generate unseen compositions of novel part appearances. Quantitative evaluations as well as human studies demonstrate that sketches generated by our approach are more creative and of higher quality than existing approaches. In fact, in Creative Birds, subjects prefer sketches generated by DoodlerGAN over those drawn by humans!"}}
{"id": "giwiOKPnxxf", "cdate": 1609459200000, "mdate": 1625886955128, "content": {"title": "Human-Adversarial Visual Question Answering", "abstract": "Performance on the most commonly used Visual Question Answering dataset (VQA v2) is starting to approach human accuracy. However, in interacting with state-of-the-art VQA models, it is clear that the problem is far from being solved. In order to stress test VQA models, we benchmark them against human-adversarial examples. Human subjects interact with a state-of-the-art VQA model, and for each image in the dataset, attempt to find a question where the model's predicted answer is incorrect. We find that a wide range of state-of-the-art models perform poorly when evaluated on these examples. We conduct an extensive analysis of the collected adversarial examples and provide guidance on future research directions. We hope that this Adversarial VQA (AdVQA) benchmark can help drive progress in the field and advance the state of the art."}}
{"id": "fgIz1ZyDSh0", "cdate": 1609459200000, "mdate": 1648684674506, "content": {"title": "MoVie: Revisiting Modulated Convolutions for Visual Counting and Beyond", "abstract": "This paper focuses on visual counting, which aims to predict the number of occurrences given a natural image and a query (e.g. a question or a category). Unlike most prior works that use explicit, symbolic models which can be computationally expensive and limited in generalization, we propose a simple and effective alternative by revisiting modulated convolutions that fuse the query and the image locally. Following the design of residual bottleneck, we call our method MoVie, short for Modulated conVolutional bottlenecks. Notably, MoVie reasons implicitly and holistically and only needs a single forward-pass during inference. Nevertheless, MoVie showcases strong performance for counting: 1) advancing the state-of-the-art on counting-specific VQA tasks while being more efficient; 2) outperforming prior-art on difficult benchmarks like COCO for common object counting; 3) helped us secure the first place of 2020 VQA challenge when integrated as a module for \u2018number\u2019 related questions in generic VQA models. Finally, we show evidence that modulated convolutions such as MoVie can serve as a general mechanism for reasoning tasks beyond counting."}}
{"id": "PkJJrbdQuwn", "cdate": 1609459200000, "mdate": 1648684674503, "content": {"title": "FLAVA: A Foundational Language And Vision Alignment Model", "abstract": "State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal (with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising direction would be to use a single holistic universal model, as a \"foundation\", that targets all modalities at once -- a true vision and language foundation model should be good at vision tasks, language tasks, and cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate impressive performance on a wide range of 35 tasks spanning these target modalities."}}
{"id": "EB6y9S6v3S6", "cdate": 1609459200000, "mdate": 1648684674561, "content": {"title": "Only Time Can Tell: Discovering Temporal Data for Temporal Modeling", "abstract": "Understanding temporal information and how the visual world changes over time, is a fundamental ability of intelligent systems. In video understanding, temporal information is at the core of many current challenges, including compression, efficient inference, motion estimation or summarization. However, in current video datasets it has been observed that action classes can often be recognized without any temporal information, from a single frame of video. As a result, both benchmarking and training in these datasets may give an unintentional advantage to models with strong image understanding capabilities, as opposed to those with strong temporal understanding. In other words, current datasets may not reward good temporal understanding, potentially hindering progress. In this paper we address this problem head on by identifying action classes where temporal information is actually necessary to recognize them and call these \"temporal classes\". Selecting temporal classes using a computational method would bias the process. Instead, we propose a methodology based on a simple and effective human annotation experiment. We remove just the temporal information, by shuffling frames in time, and measure if the action can still be recognized. Classes that cannot be recognized when frames are not in order, are included in the temporal set. We observe that this set is statistically different from other static classes, and that performance in it correlates with a network's ability to capture temporal information. Thus we use it as a benchmark on current popular networks, which reveals a series of interesting facts, like inflated convolutions bias networks towards classes where motion is not important. We also explore the effect of training on the temporal set, and observe that this leads to better generalization in unseen classes, demonstrating the need for more temporal data. We hope that the proposed dataset of temporal categories will help guide future research in temporal modeling for better video understanding."}}
{"id": "gwnoVHIES05", "cdate": 1601308051370, "mdate": null, "content": {"title": "Creative Sketch Generation", "abstract": "Sketching or doodling is a popular creative activity that people engage in. However, most existing work in automatic sketch understanding or generation has focused on sketches that are quite mundane. In this work, we introduce two datasets of creative sketches -- Creative Birds and Creative Creatures -- containing 10k sketches each along with part annotations. We propose DoodlerGAN -- a part-based Generative Adversarial Network (GAN) -- to generate unseen compositions of novel part appearances. Quantitative evaluations as well as human studies demonstrate that sketches generated by our approach are more creative and of higher quality than existing approaches. In fact, in Creative Birds, subjects prefer sketches generated by DoodlerGAN over those drawn by humans!"}}
{"id": "8e6BrwU6AjQ", "cdate": 1601308029819, "mdate": null, "content": {"title": "MoVie: Revisiting Modulated Convolutions for Visual Counting and Beyond", "abstract": "This paper focuses on visual counting, which aims to predict the number of occurrences given a natural image and a query (e.g. a question or a category). Unlike most prior works that use explicit, symbolic models which can be computationally expensive and limited in generalization, we propose a simple and effective alternative by revisiting modulated convolutions that fuse the query and the image locally. Following the design of residual bottleneck, we call our method MoVie, short for Modulated conVolutional bottlenecks. Notably, MoVie reasons implicitly and holistically and only needs a single forward-pass during inference. Nevertheless, MoVie showcases strong performance for counting: 1) advancing the state-of-the-art on counting-specific VQA tasks while being more efficient; 2) outperforming prior-art on difficult benchmarks like COCO for common object counting; 3) helped us secure the first place of 2020 VQA challenge when integrated as a module for \u2018number\u2019 related questions in generic VQA models. Finally, we show evidence that modulated convolutions such as MoVie can serve as a general mechanism for reasoning tasks beyond counting."}}
