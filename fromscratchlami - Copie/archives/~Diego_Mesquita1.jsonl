{"id": "m9-JnVFTJB", "cdate": 1696755617655, "mdate": 1696755617655, "content": {"title": "Human-in-the-Loop Causal Discovery under Latent Confounding using Ancestral GFlowNets", "abstract": "Structure learning is the crux of causal inference. Notably, causal discovery (CD) algorithms are brittle when data is scarce, possibly inferring imprecise causal relations that contradict expert knowledge -- especially when considering latent confounders. To aggravate the issue, most CD methods do not provide uncertainty estimates, making it hard for users to interpret results and improve the inference process. Surprisingly, while CD is a human-centered affair, no works have focused on building methods that both 1) output uncertainty estimates that can be verified by experts and 2) interact with those experts to iteratively refine CD. To solve these issues, we start by proposing to sample (causal) ancestral graphs proportionally to a belief distribution based on a score function, such as the Bayesian information criterion (BIC), using generative flow networks. Then, we leverage the diversity in candidate graphs and introduce an optimal experimental design to iteratively probe the expert about the relations among variables, effectively reducing the uncertainty of our belief over ancestral graphs. Finally, we update our samples to incorporate human feedback via importance sampling. Importantly, our method does not require causal sufficiency (i.e., unobserved confounders may exist). Experiments with synthetic observational data show that our method can accurately sample from distributions over ancestral graphs and that we can greatly improve inference quality with human aid."}}
{"id": "qRfUMYzIv9_", "cdate": 1672531200000, "mdate": 1701796334819, "content": {"title": "Distill n' Explain: explaining graph neural networks using simple surrogates", "abstract": "Explaining node predictions in graph neural networks (GNNs) often boils down to finding graph substructures that preserve predictions. Finding these structures usually implies back-propagating thro..."}}
{"id": "6bA-T7_7915", "cdate": 1664310942333, "mdate": null, "content": {"title": "Locking and Quacking:  Stacking Bayesian models predictions by log-pooling and superposition", "abstract": "Combining predictive distributions is a central problem in Bayesian inference and machine learning.  Currently, predictives are almost exclusively combined using linear density-mixtures such as Bayesian model averaging, Bayesian stacking, and  mixture of experts.\nNonetheless, linear mixtures impose traits that might be undesirable for some applications, such as multi-modality.\nWhile there are alternative strategies (e.g., geometric bridge or superposition), optimizing their parameters usually implies  computing intractable normalizing constant repeatedly.\nIn this extended abstract, we present two novel Bayesian model combination tools. They are generalizations of \\emph{stacking}, but combine posterior densities by log-linear pooling (\\emph{locking}) and quantum superposition (\\emph{quacking}). \nTo optimize model weights while avoiding the burden of normalizing constants, we maximize the  Hyv\\\"arinen score of the combined posterior predictions.  We demonstrate locking and quacking with an illustrative example."}}
{"id": "2neXknZg9ej", "cdate": 1664046170867, "mdate": null, "content": {"title": "Provably expressive temporal graph networks", "abstract": "Temporal graph networks (TGNs) have gained prominence as models for embedding dynamic interactions,  but little is known about their theoretical underpinnings. We establish fundamental results about the representational power and limits of the two main categories of TGNs: those that aggregate temporal walks (WA-TGNs), and those that augment local message passing with recurrent memory modules (MP-TGNs). Specifically, novel constructions reveal the inadequacy of MP-TGNs and WA-TGNs, proving that neither category subsumes the other. We extend the 1-WL (Weisfeiler-Leman) test to temporal graphs, and show that the most powerful MP-TGNs should use injective updates, as in this case they become as expressive as the temporal WL. Also, we show that sufficiently deep MP-TGNs cannot benefit from memory, and MP/WA-TGNs fail to compute graph properties such as girth. \n \nThese theoretical insights lead us to introduce PINT --- a novel architecture that leverages injective temporal message passing and relative positional features. Importantly, PINT is provably more expressive than both MP-TGNs and WA-TGNs.\nOur experiments demonstrate that PINT significantly outperforms existing TGNs on several real-world benchmarks."}}
{"id": "MwSXgQSxL5s", "cdate": 1652737697411, "mdate": null, "content": {"title": "Provably expressive temporal graph networks", "abstract": "Temporal graph networks (TGNs) have gained prominence as models for embedding dynamic interactions,  but little is known about their theoretical underpinnings. We establish fundamental results about the representational power and limits of the two main categories of TGNs: those that aggregate temporal walks (WA-TGNs), and those that augment local message passing with recurrent memory modules (MP-TGNs). Specifically, novel constructions reveal the inadequacy of MP-TGNs and WA-TGNs, proving that neither category subsumes the other. We extend the 1-WL (Weisfeiler-Leman) test to temporal graphs, and show that the most powerful MP-TGNs should use injective updates, as in this case they become as expressive as the temporal WL. Also, we show that sufficiently deep MP-TGNs cannot benefit from memory, and MP/WA-TGNs fail to compute graph properties such as girth. \n \nThese theoretical insights lead us to PINT --- a novel architecture that leverages injective temporal message passing and relative positional features. Importantly, PINT is provably more expressive than both MP-TGNs and WA-TGNs. PINT significantly outperforms existing TGNs on several real-world benchmarks."}}
{"id": "_cc62Pi37uE", "cdate": 1640995200000, "mdate": 1701796334809, "content": {"title": "Provably expressive temporal graph networks", "abstract": "Temporal graph networks (TGNs) have gained prominence as models for embedding dynamic interactions, but little is known about their theoretical underpinnings. We establish fundamental results about the representational power and limits of the two main categories of TGNs: those that aggregate temporal walks (WA-TGNs), and those that augment local message passing with recurrent memory modules (MP-TGNs). Specifically, novel constructions reveal the inadequacy of MP-TGNs and WA-TGNs, proving that neither category subsumes the other. We extend the 1-WL (Weisfeiler-Leman) test to temporal graphs, and show that the most powerful MP-TGNs should use injective updates, as in this case they become as expressive as the temporal WL. Also, we show that sufficiently deep MP-TGNs cannot benefit from memory, and MP/WA-TGNs fail to compute graph properties such as girth. These theoretical insights lead us to PINT --- a novel architecture that leverages injective temporal message passing and relative positional features. Importantly, PINT is provably more expressive than both MP-TGNs and WA-TGNs. PINT significantly outperforms existing TGNs on several real-world benchmarks."}}
{"id": "ONQX3wQM5O", "cdate": 1640995200000, "mdate": 1680067663018, "content": {"title": "Parallel MCMC Without Embarrassing Failures", "abstract": ""}}
{"id": "0IqFsR9wJvI", "cdate": 1632875693941, "mdate": null, "content": {"title": "Online graph nets", "abstract": "Temporal graph neural networks (T-GNNs) sequentially update node states and use temporal message passing to predict events in continuous-time dynamic graphs. While node states rest in the memory, the message-passing operations must be computed on-demand for each prediction. In practice, these operations are the computational bottleneck of state-of-the-art T-GNNs as they require topologically exploring large temporal graphs. To circumvent this caveat, we propose Online Graph Nets (OGNs). To avoid temporal message passing, OGN maintains a summary of the temporal neighbors of each node in a latent variable and updates it as events unroll, in an online fashion. At prediction time, OGN simply combines node states and their latents to obtain node-level representations. Consequently, the memory cost of OGN is constant with respect to the number of previous events. Remarkably, OGN outperforms most existing T-GNNs on temporal link prediction benchmarks while running orders of magnitude faster. For instance, OGN performs similarly to the best-known T-GNN on Reddit, with a $374\\times$ speedup. Also, since OGNs do not explore temporal graphs at prediction time, they are well-suited for on-device predictions (e.g., on mobile phones)."}}
{"id": "sr5zBmJQedK", "cdate": 1609459200000, "mdate": 1701796334814, "content": {"title": "Federated stochastic gradient Langevin dynamics", "abstract": "Stochastic gradient MCMC methods, such as stochastic gradient Langevin dynamics (SGLD), employ fast but noisy gradient estimates to enable large-scale posterior sampling. Although we can easily ext..."}}
{"id": "dkt6r9zbcWi", "cdate": 1609459200000, "mdate": null, "content": {"title": "Learning GPLVM with arbitrary kernels using the unscented transformation", "abstract": "Gaussian Process Latent Variable Model (GPLVM) is a flexible framework to handle uncertain inputs in Gaussian Processes (GPs) and incorporate GPs as components of larger graphical models. Nonetheless, the standard GPLVM variational inference approach is tractable only for a narrow family of kernel functions. The most popular implementations of GPLVM circumvent this limitation using quadrature methods, which may become a computational bottleneck even for relatively low dimensions. For instance, the widely employed Gauss-Hermite quadrature has exponential complexity on the number of dimensions. In this work, we propose using the unscented transformation instead. Overall, this method presents comparable, if not better, performance than off-the-shelf solutions to GPLVM, and its computational complexity scales only linearly on dimension. In contrast to Monte Carlo methods, our approach is deterministic and works well with quasi-Newton methods, such as the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm. We illustrate the applicability of our method with experiments on dimensionality reduction and multistep-ahead prediction with uncertainty propagation."}}
