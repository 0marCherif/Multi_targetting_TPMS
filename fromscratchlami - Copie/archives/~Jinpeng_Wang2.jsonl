{"id": "8JRQza2MaO4", "cdate": 1663850005820, "mdate": null, "content": {"title": "Revitalize Region Feature for Democratizing Video-language Pre-training of Retrieval", "abstract": "Recent dominant methods for video-language pre-training (VLP) learn transferable representations from the raw pixels in an end-to-end manner to achieve advanced performance on downstream video-language retrieval. Despite the impressive results, VLP research becomes extremely expensive with the need for massive data and a long training time, preventing further explorations. In this work, we revitalize region features of sparsely sampled video clips to significantly reduce both spatial and temporal visual redundancy towards democratizing VLP research at the same time achieving state-of-the-art results. Specifically, to fully explore the potential of region features, we introduce a novel bidirectional region-word alignment regularization that properly optimizes the fine-grained relations between regions and certain words in sentences, eliminating the domain/modality disconnections between pre-extracted region features and text. Extensive results of downstream video-language retrieval tasks on four datasets demonstrate the superiority of our method on both effectiveness and efficiency, e.g., our method achieves competing results with 80% fewer data and 85% less pre-training time compared to the most efficient VLP method so far."}}
{"id": "nE8_DvxAqAB", "cdate": 1652737454114, "mdate": null, "content": {"title": "Egocentric Video-Language Pretraining", "abstract": "Video-Language Pretraining (VLP), which aims to learn transferable representation to advance a wide range of video-text downstream tasks, has recently received increasing attention. Best performing works rely on large-scale, 3rd-person video-text datasets, such as HowTo100M. In this work, we exploit the recently released Ego4D dataset to pioneer Egocentric VLP along three directions. (i) We create EgoClip, a 1st-person video-text pretraining dataset comprising 3.8M clip-text pairs well-chosen from Ego4D, covering a large variety of human daily activities. (ii) We propose a novel pretraining objective, dubbed EgoNCE, which adapts video-text contrastive learning to the egocentric domain by mining egocentric-aware positive and negative samples. (iii) We introduce EgoMCQ, a development benchmark that is close to EgoClip and hence can support effective validation and fast exploration of our design decisions in EgoClip and EgoNCE. Furthermore, we demonstrate strong performance on five egocentric downstream tasks across three datasets: video-text retrieval on EPIC-KITCHENS-100; action recognition on Charades-Ego; natural language query, moment query, and object state change classification on Ego4D challenge benchmarks. The dataset and code are available at https://github.com/showlab/EgoVLP."}}
{"id": "ohRLbsD6J8", "cdate": 1640995200000, "mdate": 1667399871295, "content": {"title": "Suppressing Static Visual Cues via Normalizing Flows for Self-Supervised Video Representation Learning", "abstract": "Despite the great progress in video understanding made by deep convolutional neural networks, feature representation learned by existing methods may be biased to static visual cues. To address this issue, we propose a novel method to suppress static visual cues (SSVC) based on probabilistic analysis for self-supervised video representation learning. In our method, video frames are first encoded to obtain latent variables under standard normal distribution via normalizing flows. By modelling static factors in a video as a random variable, the conditional distribution of each latent variable becomes shifted and scaled normal. Then, the less-varying latent variables along time are selected as static cues and suppressed to generate motion-preserved videos. Finally, positive pairs are constructed by motion-preserved videos for contrastive learning to alleviate the problem of representation bias to static cues. The less-biased video representation can be better generalized to various downstream tasks. Extensive experiments on publicly available benchmarks demonstrate that the proposed method outperforms the state of the art when only single RGB modality is used for pre-training."}}
{"id": "mKu6-Ces2l", "cdate": 1640995200000, "mdate": 1667399871252, "content": {"title": "Learning to Mitigate Extreme Distribution Bias for Few-Shot Object Detection", "abstract": "Few-shot object detection is an important but challenging task where only a few instances of novel categories are available. The widely used approach is to pretrain a detector on base classes with abundant samples and then fine-tune it for novel classes. Due to the extreme data imbalance between base and novel classes, the detection performance of novel classes degrades with the distribution bias. To overcome this limitation, we propose a distribution calibration strategy and a class discrimination regularization method for better few-shot detection. Based on theoretical analysis on decision margins of base and novel classes, the decision area of novel classes is enlarged to balance the prediction probability. On the other hand, to increase the separability of inter-class distributions, the similarity between class-specific representations is minimized. Extensive experiments on PASCAL VOC and MS COCO datasets verify the effectiveness and generalization ability of our method to improve few-shot object detection."}}
{"id": "m1KHov8Ccx", "cdate": 1640995200000, "mdate": 1667399622613, "content": {"title": "Egocentric Video-Language Pretraining @ EPIC-KITCHENS-100 Multi-Instance Retrieval Challenge 2022", "abstract": "In this report, we propose a video-language pretraining (VLP) based solution \\cite{kevin2022egovlp} for the EPIC-KITCHENS-100 Multi-Instance Retrieval (MIR) challenge. Especially, we exploit the recently released Ego4D dataset \\cite{grauman2021ego4d} to pioneer Egocentric VLP from pretraining dataset, pretraining objective, and development set. Based on the above three designs, we develop a pretrained video-language model that is able to transfer its egocentric video-text representation to MIR benchmark. Furthermore, we devise an adaptive multi-instance max-margin loss to effectively fine-tune the model and equip the dual-softmax technique for reliable inference. Our best single model obtains strong performance on the challenge test set with 47.39% mAP and 61.44% nDCG. The code is available at https://github.com/showlab/EgoVLP."}}
{"id": "iajt59AzPb", "cdate": 1640995200000, "mdate": 1667399871211, "content": {"title": "Region-Interactive Proposal Network and Class-Interactive Feature Learning for Few-Shot Object Detection", "abstract": "Few-shot object detection is a promising approach to solving the problem of detecting novel objects with only limited annotated data for training. Most existing methods are developed based on the progress in few-shot classification, which pay little attention to improving the localization module and modelling class interrelation. To address these issues, this paper proposes two novel modules, namely Region-interactive Proposal Network (Ri-PN) and Class-interactive Feature Learning (Ci-FL), for better localization and classification performance, respectively. In the Ri-PN, regions of novel classes are interacted with base classes via graph convolution instead of background due to the stronger relevance between base and novel classes together with the guidance of supervised regions loss. On the other hand, the Ci-FL refines class-specific features in prototypical learning by attentive graph convolutional network. Experimental results on PASCAL VOC and MS COCO datasets verify the superiority of our method for few-shot object detection."}}
{"id": "hPQzaPDEJA", "cdate": 1640995200000, "mdate": 1667399622612, "content": {"title": "MILES: Visual BERT Pre-training with Injected Language Semantics for Video-text Retrieval", "abstract": "Dominant pre-training work for video-text retrieval mainly adopt the \"dual-encoder\" architectures to enable efficient retrieval, where two separate encoders are used to contrast global video and text representations, but ignore detailed local semantics. The recent success of image BERT pre-training with masked visual modeling that promotes the learning of local visual context, motivates a possible solution to address the above limitation. In this work, we for the first time investigate masked visual modeling in video-text pre-training with the \"dual-encoder\" architecture. We perform Masked visual modeling with Injected LanguagE Semantics (MILES) by employing an extra snapshot video encoder as an evolving \"tokenizer\" to produce reconstruction targets for masked video patch prediction. Given the corrupted video, the video encoder is trained to recover text-aligned features of the masked patches via reasoning with the visible regions along the spatial and temporal dimensions, which enhances the discriminativeness of local visual features and the fine-grained cross-modality alignment. Our method outperforms state-of-the-art methods for text-to-video retrieval on four datasets with both zero-shot and fine-tune evaluation protocols. Our approach also surpasses the baseline models significantly on zero-shot action recognition, which can be cast as video-to-text retrieval."}}
{"id": "cyt8r4Dbqqk", "cdate": 1640995200000, "mdate": 1667399871325, "content": {"title": "Multi-Level Temporal Dilated Dense Prediction for Action Recognition", "abstract": "3D convolutional neural networks have achieved great success for action recognition. However, large variations of temporal dynamics have not been properly processed and low-level features have not been fully exploited in most existing works. To solve these two problems, we present a general and flexible framework, namely multi-level temporal dilated dense prediction network, which can incorporate with most of existing methods as backbone to improve the temporal modeling capacity. In the proposed method, a novel temporal dilated dense prediction block is designed to fully utilize temporal features with various temporal dilated rates for dense prediction while maintaining relatively low computational cost. To fuse information from low to high levels, our method combines the predictions from multiple such blocks inserted at different stages of the backbone network. In-depth analysis is given to show that short- to long-term temporal dependencies can be captured and multi-level spatio-temporal features are effectively fused for video action recognition by the proposed method. Experimental results demonstrate that our method achieves impressive performance improvement on four publicly available action recognition benchmarks including Charades, Kinetics, Something-Something-V1 and HMDB51."}}
{"id": "S6JSNCavnb", "cdate": 1640995200000, "mdate": 1667399622614, "content": {"title": "Egocentric Video-Language Pretraining @ Ego4D Challenge 2022", "abstract": "In this report, we propose a video-language pretraining (VLP) based solution \\cite{kevin2022egovlp} for four Ego4D challenge tasks, including Natural Language Query (NLQ), Moment Query (MQ), Object State Change Classification (OSCC), and PNR Localization (PNR). Especially, we exploit the recently released Ego4D dataset \\cite{grauman2021ego4d} to pioneer Egocentric VLP from pretraining dataset, pretraining objective, and development set. Based on the above three designs, we develop a pretrained video-language model that is able to transfer its egocentric video-text representation or video-only representation to several video downstream tasks. Our Egocentric VLP achieves 10.46R@1&IoU @0.3 on NLQ, 10.33 mAP on MQ, 74% Acc on OSCC, 0.67 sec error on PNR. The code is available at https://github.com/showlab/EgoVLP."}}
{"id": "LrxVotxFTs", "cdate": 1640995200000, "mdate": 1667399622614, "content": {"title": "All in One: Exploring Unified Video-Language Pre-training", "abstract": "Mainstream Video-Language Pre-training models \\cite{actbert,clipbert,violet} consist of three parts, a video encoder, a text encoder, and a video-text fusion Transformer. They pursue better performance via utilizing heavier unimodal encoders or multimodal fusion Transformers, resulting in increased parameters with lower efficiency in downstream tasks. In this work, we for the first time introduce an end-to-end video-language model, namely \\textit{all-in-one Transformer}, that embeds raw video and textual signals into joint representations using a unified backbone architecture. We argue that the unique temporal information of video data turns out to be a key barrier hindering the design of a modality-agnostic Transformer. To overcome the challenge, we introduce a novel and effective token rolling operation to encode temporal representations from video clips in a non-parametric manner. The careful design enables the representation learning of both video-text multimodal inputs and unimodal inputs using a unified backbone model. Our pre-trained all-in-one Transformer is transferred to various downstream video-text tasks after fine-tuning, including text-video retrieval, video-question answering, multiple choice and visual commonsense reasoning. State-of-the-art performances with the minimal model FLOPs on nine datasets demonstrate the superiority of our method compared to the competitive counterparts. The code and pretrained model have been released in https://github.com/showlab/all-in-one."}}
