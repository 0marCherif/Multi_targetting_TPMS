{"id": "uRc0ZihcR2f", "cdate": 1676472362691, "mdate": null, "content": {"title": "Learning with Explanation Constraints", "abstract": "While supervised learning assumes the presence of labeled data, we may have prior information about how models should behave. In this paper, we formalize this notion as learning from explanation constraints and provide a learning theoretic framework to analyze how such explanations can improve the learning of our models. For what models would explanations be helpful? Our first key contribution addresses this question via the definition of what we call EPAC models (models that satisfy these constraints in expectation over new data), and we analyze this class of models using standard learning theoretic tools. Our second key contribution is to characterize these restrictions (in terms of their Rademacher complexities) for a canonical class of explanations given by gradient information for linear models and 2 layer neural networks. Finally, we provide an algorithmic solution for our framework, via a variational approximation that achieves better performance and satisfies these constraints more frequently, when compared to simpler augmented Lagrangian methods to incorporate these explanations. We demonstrate the benefits of our approach over synthetic and real-world experiments."}}
{"id": "7wV2d1WW37x", "cdate": 1672531200000, "mdate": 1682318902158, "content": {"title": "Learning with Explanation Constraints", "abstract": "While supervised learning assumes the presence of labeled data, we may have prior information about how models should behave. In this paper, we formalize this notion as learning from explanation constraints and provide a learning theoretic framework to analyze how such explanations can improve the learning of our models. For what models would explanations be helpful? Our first key contribution addresses this question via the definition of what we call EPAC models (models that satisfy these constraints in expectation over new data), and we analyze this class of models using standard learning theoretic tools. Our second key contribution is to characterize these restrictions (in terms of their Rademacher complexities) for a canonical class of explanations given by gradient information for linear models and two layer neural networks. Finally, we provide an algorithmic solution for our framework, via a variational approximation that achieves better performance and satisfies these constraints more frequently, when compared to simpler augmented Lagrangian methods to incorporate these explanations. We demonstrate the benefits of our approach over a large array of synthetic and real-world experiments."}}
{"id": "aCuFa-RRqtI", "cdate": 1663850423949, "mdate": null, "content": {"title": "Label Propagation with Weak Supervision", "abstract": "Semi-supervised learning and weakly supervised learning are important paradigms that aim to reduce the growing demand for labeled data in current machine learning applications. In this paper, we introduce a novel analysis of the classical label propagation algorithm (LPA) (Zhu & Ghahramani, 2002) that moreover takes advantage of useful prior information, specifically probabilistic hypothesized labels on the unlabeled data. We provide an error bound that exploits both the local geometric properties of the underlying graph and the quality of the prior information. We also propose a framework to incorporate multiple sources of noisy information. In particular, we consider the setting of weak supervision, where our sources of information are weak labelers. We demonstrate the ability of our approach on multiple benchmark weakly supervised classification tasks, showing improvements upon existing semi-supervised and weakly supervised methods. "}}
{"id": "yvcG59QgSU", "cdate": 1640995200000, "mdate": 1682318902060, "content": {"title": "Label Propagation with Weak Supervision", "abstract": "Semi-supervised learning and weakly supervised learning are important paradigms that aim to reduce the growing demand for labeled data in current machine learning applications. In this paper, we introduce a novel analysis of the classical label propagation algorithm (LPA) (Zhu & Ghahramani, 2002) that moreover takes advantage of useful prior information, specifically probabilistic hypothesized labels on the unlabeled data. We provide an error bound that exploits both the local geometric properties of the underlying graph and the quality of the prior information. We also propose a framework to incorporate multiple sources of noisy information. In particular, we consider the setting of weak supervision, where our sources of information are weak labelers. We demonstrate the ability of our approach on multiple benchmark weakly supervised classification tasks, showing improvements upon existing semi-supervised and weakly supervised methods."}}
{"id": "CYQV-hyuno", "cdate": 1640995200000, "mdate": 1682318902065, "content": {"title": "Losses over Labels: Weakly Supervised Learning via Direct Loss Construction", "abstract": "Owing to the prohibitive costs of generating large amounts of labeled data, programmatic weak supervision is a growing paradigm within machine learning. In this setting, users design heuristics that provide noisy labels for subsets of the data. These weak labels are combined (typically via a graphical model) to form pseudolabels, which are then used to train a downstream model. In this work, we question a foundational premise of the typical weakly supervised learning pipeline: given that the heuristic provides all ``label\" information, why do we need to generate pseudolabels at all? Instead, we propose to directly transform the heuristics themselves into corresponding loss functions that penalize differences between our model and the heuristic. By constructing losses directly from the heuristics, we can incorporate more information than is used in the standard weakly supervised pipeline, such as how the heuristics make their decisions, which explicitly informs feature selection during training. We call our method Losses over Labels (LoL) as it creates losses directly from heuristics without going through the intermediate step of a label. We show that LoL improves upon existing weak supervision methods on several benchmark text and image classification tasks and further demonstrate that incorporating gradient information leads to better performance on almost every task."}}
{"id": "5IOXSin9vX8", "cdate": 1640995200000, "mdate": 1682318902113, "content": {"title": "Improving self-supervised representation learning via sequential adversarial masking", "abstract": "Recent methods in self-supervised learning have demonstrated that masking-based pretext tasks extend beyond NLP, serving as useful pretraining objectives in computer vision. However, existing approaches apply random or ad hoc masking strategies that limit the difficulty of the reconstruction task and, consequently, the strength of the learnt representations. We improve upon current state-of-the-art work in learning adversarial masks by proposing a new framework that generates masks in a sequential fashion with different constraints on the adversary. This leads to improvements in performance on various downstream tasks, such as classification on ImageNet100, STL10, and CIFAR10/100 and segmentation on Pascal VOC. Our results further demonstrate the promising capabilities of masking-based approaches for SSL in computer vision."}}
{"id": "I4FCWXoGEd", "cdate": 1609459200000, "mdate": 1682318902061, "content": {"title": "Semi-Supervised Aggregation of Dependent Weak Supervision Sources With Performance Guarantees", "abstract": "We develop a novel method that provides theoretical guarantees for learning from weak labelers without the (mostly unrealistic) assumption that the errors of the weak labelers are independent or come from a particular family of distributions. We show a rigorous technique for efficiently selecting small subsets of the labelers so that a majority vote from such subsets has a provably low error rate. We explore several extensions of this method and provide experimental results over a range of labeled data set sizes on 45 image classification tasks. Our performance-guaranteed methods consistently match the best performing alternative, which varies based on problem difficulty. On tasks with accurate weak labelers, our methods are on average 3 percentage points more accurate than the state-of-the-art adversarial method. On tasks with inaccurate weak labelers, our methods are on average 15 percentage points more accurate than the semi-supervised Dawid-Skene model (which assumes independence)."}}
{"id": "I35pJh0yUL", "cdate": 1609459200000, "mdate": 1682318902014, "content": {"title": "Adversarial Multi Class Learning under Weak Supervision with Performance Guarantees", "abstract": "We develop a rigorous approach for using a set of arbitrarily correlated weak supervision sources in order to solve a multiclass classification task when only a very small set of labeled data is av..."}}
{"id": "SygawpcMaS", "cdate": 1575296357028, "mdate": null, "content": {"title": "[Re] No Press Diplomacy: Modeling Multi-Agent Gameplay", "abstract": "Diplomacy is a strategic board game where different powers battle over control of supply centers in Europe. The original authors [1] developed supervised learning and reinforcement learning models to learn to play the No Press version of Diplomacy, beating the existing state of the art rule-based bots. The original paper utilizes various different machine and reinforcement learning techniques, including attention, encoder and decoder blocks, graph convolutional networks (GCN), LSTM, and FiLM [2]. Their implementation and code built off of extensive existing software frameworks like DAIDE [3], developed by the Diplomacy research community for interfacing with other bots. Furthermore, the authors have also developed a game engine that provides a simple interface for playing Diplomacy games. Because the authors of the paper released all their code for their models, the paper is not entirely comprehensive with their implementation details. Without being able to refer to their code, these ambiguities proved to make replication fairly difficult. We relied on communication with the paper authors in order to resolve a variety of ambiguities. Ultimately, this report details our attempts to reproduce the paper. We failed to reproduce the results for many reasons, including architecture ambiguities, expensive training times/compute resources required that were unmentioned in the original paper, and the complexity of this project given a 2-month time frame.\n"}}
