{"id": "8PGB0SYTZrT", "cdate": 1672921881743, "mdate": 1672921881743, "content": {"title": "Analysing Training-Data Leakage from Gradients through Linear Systems and Gradient Matching", "abstract": "Recent works have demonstrated that it is possible to reconstruct training images and their labels from gradients of an image-classification model when its architecture is known. Unfortunately, there is still an incomplete theoretical understanding of the efficacy and failure of these gradient-leakage attacks. In this paper, we propose a novel framework to analyse training-data leakage from gradients that draws insights from both analytic and optimisation-based gradient-leakage attacks. We formulate the reconstruction problem as solving a linear system from each layer iteratively, accompanied by corrections using gradient matching. Under this framework, we claim that the solubility of the reconstruction problem is primarily determined by that of the linear system at each layer. As a result, we are able to partially attribute the leakage of the training data in a deep network to its architecture. We also propose a metric to measure the level of security of a deep learning model against gradient-based attacks on the training data."}}
{"id": "-RLCTAvUxuf", "cdate": 1639146345194, "mdate": null, "content": {"title": "Cell Anomaly Localisation using Structured Uncertainty Prediction Networks", "abstract": "This paper proposes an unsupervised approach to anomaly detection in bright-field or fluorescence cell microscopy, where our goal is to localise malaria parasites. This is achieved by building a generative model (a variational autoencoder) that describes healthy cell images, where we additionally model the structure of the predicted image uncertainty, rather than assuming pixelwise independence in the likelihood function. This provides a \u201cwhitened\u201d residual representation, where the anticipated structured mistakes by the generative model are reduced, but distinctive structures that did not occur in the training distribution, e.g. parasites are highlighted. We employ the recently published Structured Uncertainty Prediction Networks approach to enable tractable learning of the uncertainty structure. Here, the residual covariance matrix is efficiently approximated using a sparse Cholesky parameterisation. We demonstrate that our proposed approach is more effective for detecting real and synthetic structured image perturbations compared to diagonal Gaussian likelihoods."}}
{"id": "A8bxsnCAvDs", "cdate": 1637576009301, "mdate": null, "content": {"title": "Shooting Schr\u00f6dinger\u2019s Cat", "abstract": "We  present  a  variational  inference  scheme  to  learn  a  model  that  solves  the  Schr\u00f6dinger Bridge Problem (SBP). In contrast to previous work, our approach is solver-agnostic and guarantees solutions that respect the prior beyond the first fitting iteration.  Having this solution allows us to generate new samples from one of the distributions by first sampling from the other one and then solving the dynamical system.  We show that our model is able to learn the transformation between the Gaussian distribution and arbitrary data, as well as learning dynamics that follow a potential function."}}
{"id": "c63rqIcw66h", "cdate": 1631541387394, "mdate": null, "content": {"title": "Understanding Training-Data Leakage from Gradients in Neural Networks for ImageClassifications", "abstract": "Federated learning of deep learning models for supervised tasks, e.g. image classification and segmentation, has found many applications: for example in human-in-the-loop tasks such as film post-production where it enables sharing of domain expertise of human artists in an efficient and effective fashion. In many such applications, we need to protect the training data from being leaked when gradients are shared in the training process due to IP or privacy concerns. Recent works have demonstrated that it is possible to reconstruct the training data from gradients for an image-classification model when its architecture is known. However, there is still an incomplete theoretical understanding of the efficacy and failure of such attacks. In this paper, we analyse the source of training-data leakage from gradients. We formulate the problem of training data reconstruction as solving an optimisation problem iteratively for each layer. The layer-wise objective function is primarily defined by weights and gradients from the current layer as well as the output from the reconstruction of the subsequent layer, but it might also involve a \u2018pull-back\u2019 constraint from the preceding layer. Training data can be reconstructed when we solve the problem backward from the output of the network through each layer.  Based on this formulation, we are able to attribute the potential leakage of the training data in a deep network to its architecture. We also propose a metric to measure the level of security of a deep learning model against gradient-based attacks on the training data."}}
{"id": "_xdG5L4XXOO", "cdate": 1577836800000, "mdate": null, "content": {"title": "Monotonic Gaussian Process Flows", "abstract": "We propose a new framework for imposing monotonicity constraints in a Bayesian non-parametric setting based on numerical solutions of stochastic differential equations. We derive a nonparametric mo..."}}
{"id": "YbAGIDhFMF", "cdate": 1577836800000, "mdate": null, "content": {"title": "The GAN That Warped: Semantic Attribute Editing With Unpaired Data", "abstract": "Deep neural networks have recently been used to edit images with great success, in particular for faces. However, they are often limited to only being able to work at a restricted range of resolutions. Many methods are so flexible that face edits can often result in an unwanted loss of identity. This work proposes to learn how to perform semantic image edits through the application of smooth warp fields. Previous approaches that attempted to use warping for semantic edits required paired data, i.e. example images of the same subject with different semantic attributes. In contrast, we employ recent advances in Generative Adversarial Networks that allow our model to be trained with unpaired data. We demonstrate face editing at very high resolutions (4k images) with a single forward pass of a deep network at a lower resolution. We also show that our edits are substantially better at preserving the subject's identity. The robustness of our approach is demonstrated by showing plausible image editing results on the Cub200 birds dataset. To our knowledge this has not been previously accomplished, due the challenging nature of the dataset."}}
{"id": "srZmElLMa_", "cdate": 1576767868062, "mdate": null, "content": {"title": "Modeling Object Appearance using Context-Conditioned Component Analysis", "abstract": "Subspace models have been very successful at modeling the appearance of structured image datasets when the visual objects have been aligned in the images (eg, faces). Even with extensions that allow for global transformations or dense warps of the image, the set of visual objects whose appearance may be modeled by such methods is limited. They are unable to account for visual objects where occlusion leads to changing visibility of different object parts (without a strict layered structure) and where a one-to-one mapping between parts is not preserved. For example bunches of bananas contain different numbers of bananas but each individual banana shares an appearance subspace.\nIn this work we remove the image space alignment limitations of existing subspace models by conditioning the models on a shape dependent context that allows for the complex, non-linear structure of the appearance of the visual object to be captured and shared. This allows us to exploit the advantages of subspace appearance models with non-rigid, deformable objects whilst also dealing with complex occlusions and varying numbers of parts. We demonstrate the effectiveness of our new model with examples of structured inpainting and appearance transfer."}}
{"id": "tfV2F-yatfZ", "cdate": 1546300800000, "mdate": null, "content": {"title": "Modulated Bayesian Optimization using Latent Gaussian Process Models.", "abstract": "Bayesian optimization (BO) methods often rely on the assumption that the objective function is well-behaved, but in practice, this is seldom true for real-world objectives even if noise-free observations can be collected. Common approaches, which try to model the objective as precisely as possible, often fail to make progress by spending too many evaluations modeling irrelevant details. We address this issue by proposing surrogate models that focus on the well-behaved structure in the objective function, which is informative for search, while ignoring detrimental structure that is challenging to model from few observations. First, we demonstrate that surrogate models with appropriate noise distributions can absorb challenging structures in the objective function by treating them as irreducible uncertainty. Secondly, we show that a latent Gaussian process is an excellent surrogate for this purpose, comparing with Gaussian processes with standard noise distributions. We perform numerous experiments on a range of BO benchmarks and find that our approach improves reliability and performance when faced with challenging objective functions."}}
{"id": "sEx9a_7RAsN", "cdate": 1546300800000, "mdate": null, "content": {"title": "MegaParallax: Casual 360\u00b0 Panoramas with Motion Parallax", "abstract": "The ubiquity of smart mobile devices, such as phones and tablets, enables users to casually capture 360\u00b0 panoramas with a single camera sweep to share and relive experiences. However, panoramas lack motion parallax as they do not provide different views for different viewpoints. The motion parallax induced by translational head motion is a crucial depth cue in daily life. Alternatives, such as omnidirectional stereo panoramas, provide different views for each eye (binocular disparity), but they also lack motion parallax as the left and right eye panoramas are stitched statically. Methods based on explicit scene geometry reconstruct textured 3D geometry, which provides motion parallax, but suffers from visible reconstruction artefacts. The core of our method is a novel multi-perspective panorama representation, which can be casually captured and rendered with motion parallax for each eye on the fly. This provides a more realistic perception of panoramic environments which is particularly useful for virtual reality applications. Our approach uses a single consumer video camera to acquire 200-400 views of a real 360\u00b0 environment with a single sweep. By using novel-view synthesis with flow-based blending, we show how to turn these input views into an enriched 360\u00b0 panoramic experience that can be explored in real time, without relying on potentially unreliable reconstruction of scene geometry. We compare our results with existing omnidirectional stereo and image-based rendering methods to demonstrate the benefit of our approach, which is the first to enable casual consumers to capture and view high-quality 360\u00b0 panoramas with motion parallax."}}
{"id": "riWtQGUZWIK", "cdate": 1546300800000, "mdate": null, "content": {"title": "Fixing Implicit Derivatives: Trust-Region Based Learning of Continuous Energy Functions", "abstract": "We present a new technique for the learning of continuous energy functions that we refer to as Wibergian Learning. One common approach to inverse problems is to cast them as an energy minimisation problem, where the minimum cost solution found is used as an estimator of hidden parameters. Our new approach formally characterises the dependency between weights that control the shape of the energy function, and the location of minima, by describing minima as fixed points of optimisation methods. This allows for the use of gradient-based end-to- end training to integrate deep-learning and the classical inverse problem methods. We show how our approach can be applied to obtain state-of-the-art results in the diverse applications of tracker fusion and multiview 3D reconstruction."}}
