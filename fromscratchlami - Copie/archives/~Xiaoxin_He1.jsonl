{"id": "1PW_txDkX7", "cdate": 1677713807787, "mdate": null, "content": {"title": "One Student Knows All Experts Know: From Sparse to Dense", "abstract": "Human education system trains one student by multiple experts. Mixture-of-experts (MoE) is a powerful sparse architecture including multiple experts. However, sparse MoE model is easy to overfit, hard to deploy, and not hardware-friendly for practitioners. In this work, inspired by the human education model, we propose a novel task, knowledge integration, to obtain a dense student model (OneS) as knowledgeable as one sparse MoE. We investigate this task by exploring 4 different ways to gather knowledge from MoE to initialize a dense student model, and we then refine the dense student by knowledge distillation. We evaluate our model on both vision and language tasks. Experimental results show, with $3.7 \\times$ inference speedup, the dense student can still preserve $88.2\\%$ benefits from MoE counterpart."}}
{"id": "yZgiybwife", "cdate": 1672531200000, "mdate": 1695970797800, "content": {"title": "A Study on Transformer Configuration and Training Objective", "abstract": "Transformer-based models have delivered impressive results on many tasks, particularly vision and language tasks. In many model training situations, conventional configurations are often adopted. F..."}}
{"id": "lhB-1w7kwr", "cdate": 1672531200000, "mdate": 1695970797807, "content": {"title": "One Student Knows All Experts Know: From Sparse to Dense", "abstract": ""}}
{"id": "_Jx6f6EpCBP", "cdate": 1672531200000, "mdate": 1695970797807, "content": {"title": "A Generalization of ViT/MLP-Mixer to Graphs", "abstract": "Graph Neural Networks (GNNs) have shown great potential in the field of graph representation learning. Standard GNNs define a local message-passing mechanism which propagates information over the w..."}}
{"id": "Kt3yfRc8mN", "cdate": 1672531200000, "mdate": 1695970797807, "content": {"title": "Explanations as Features: LLM-Based Features for Text-Attributed Graphs", "abstract": "Representation learning on text-attributed graphs (TAGs) has become a critical research problem in recent years. A typical example of a TAG is a paper citation graph, where the text of each paper serves as node attributes. Most graph neural network (GNN) pipelines handle these text attributes by transforming them into shallow or hand-crafted features, such as skip-gram or bag-of-words features. Recent efforts have focused on enhancing these pipelines with language models. With the advent of powerful large language models (LLMs) such as GPT, which demonstrate an ability to reason and to utilize general knowledge, there is a growing need for techniques which combine the textual modelling abilities of LLMs with the structural learning capabilities of GNNs. Hence, in this work, we focus on leveraging LLMs to capture textual information as features, which can be used to boost GNN performance on downstream tasks. A key innovation is our use of \\emph{explanations as features}: we prompt an LLM to perform zero-shot classification and to provide textual explanations for its decisions, and find that the resulting explanations can be transformed into useful and informative features to augment downstream GNNs. Through experiments we show that our enriched features improve the performance of a variety of GNN models across different datasets. Notably, we achieve top-1 performance on \\texttt{ogbn-arxiv} by a significant margin over the closest baseline even with $2.88\\times$ lower computation time, as well as top-1 performance on TAG versions of the widely used \\texttt{PubMed} and \\texttt{Cora} benchmarks~\\footnote{Our codes and datasets are available at: \\url{https://github.com/XiaoxinHe/TAPE}}."}}
{"id": "N4k3klHNzQj", "cdate": 1663850301686, "mdate": null, "content": {"title": "Graph MLP-Mixer", "abstract": "Graph Neural Networks (GNNs) have shown great potential in the field of graph representation learning. Standard GNNs define a local message-passing mechanism which propagates information over the whole graph domain by stacking multiple layers. This paradigm suffers from two major limitations, over-squashing and poor long-range dependencies, that can be solved using global attention but significantly increases the computational cost to quadratic complexity. In this work, we consider an alternative approach to overcome these structural limitations while keeping a low complexity cost. Motivated by the recent MLP-Mixer architecture introduced in computer vision, we propose to generalize this network to graphs. This GNN model, namely Graph MLP-Mixer, can make long-range connections without over-squashing or high complexity due to the mixer layer applied to the graph patches extracted from the original graph. As a result, this architecture exhibits promising results when comparing standard GNNs vs. Graph MLP-Mixers on benchmark graph datasets."}}
{"id": "vZ4kWo05wOR", "cdate": 1640995200000, "mdate": 1668592973781, "content": {"title": "Deeper vs Wider: A Revisit of Transformer Configuration", "abstract": "Transformer-based models have delivered impressive results on many tasks, particularly vision and language tasks. In many model training situations, conventional configurations are typically adopted. For example, we often set the base model with hidden dimensions (i.e. model width) to be 768 and the number of transformer layers (i.e. model depth) to be 12. In this paper, we revisit these conventional configurations. Through theoretical analysis and experimental evaluation, we show that the masked autoencoder is effective in alleviating the over-smoothing issue in deep transformer training. Based on this finding, we propose Bamboo, an idea of using deeper and narrower transformer configurations, for masked autoencoder training. On ImageNet, with such a simple change in configuration, re-designed model achieves 87.1% top-1 accuracy and outperforms SoTA models like MAE and BEiT. On language tasks, re-designed model outperforms BERT with default setting by 1.1 points on average, on GLUE datasets."}}
{"id": "lYPJIAE4wa", "cdate": 1640995200000, "mdate": 1682326438465, "content": {"title": "A Generalization of ViT/MLP-Mixer to Graphs", "abstract": "Graph Neural Networks (GNNs) have shown great potential in the field of graph representation learning. Standard GNNs define a local message-passing mechanism which propagates information over the whole graph domain by stacking multiple layers. This paradigm suffers from two major limitations, over-squashing and poor long-range dependencies, that can be solved using global attention but significantly increases the computational cost to quadratic complexity. In this work, we propose an alternative approach to overcome these structural limitations by leveraging the ViT/MLP-Mixer architectures introduced in computer vision. We introduce a new class of GNNs, called Graph ViT/MLP-Mixer, that holds three key properties. First, they capture long-range dependency and mitigate the issue of over-squashing as demonstrated on Long Range Graph Benchmark and TreeNeighbourMatch datasets. Second, they offer better speed and memory efficiency with a complexity linear to the number of nodes and edges, surpassing the related Graph Transformer and expressive GNN models. Third, they show high expressivity in terms of graph isomorphism as they can distinguish at least 3-WL non-isomorphic graphs. We test our architecture on 4 simulated datasets and 7 real-world benchmarks, and show highly competitive results on all of them. The source code is available for reproducibility at: \\url{https://github.com/XiaoxinHe/Graph-ViT-MLPMixer}."}}
{"id": "lKFhJecHxRg", "cdate": 1640995200000, "mdate": 1668592973772, "content": {"title": "One Student Knows All Experts Know: From Sparse to Dense", "abstract": "Human education system trains one student by multiple experts. Mixture-of-experts (MoE) is a powerful sparse architecture including multiple experts. However, sparse MoE model is easy to overfit, hard to deploy, and not hardware-friendly for practitioners. In this work, inspired by the human education model, we propose a novel task, knowledge integration, to obtain a dense student model (OneS) as knowledgeable as one sparse MoE. We investigate this task by proposing a general training framework including knowledge gathering and knowledge distillation. Specifically, to gather key knowledge from different pre-trained experts, we first investigate four different possible knowledge gathering methods, \\ie summation, averaging, Top-K Knowledge Gathering (Top-KG), and Singular Value Decomposition Knowledge Gathering (SVD-KG) proposed in this paper. We then refine the dense student model by knowledge distillation to offset the noise from gathering. On ImageNet, our OneS preserves $61.7\\%$ benefits from MoE and achieves $78.4\\%$ top-1 accuracy ImageNet with only $15$M parameters. On four natural language processing datasets, OneS obtains $88.2\\%$ MoE benefits and outperforms the best baseline by $51.7\\%$ using the same architecture and training data. In addition, compared with the MoE counterpart, OneS can achieve $3.7 \\times$ inference speedup due to less computation and hardware-friendly architecture."}}
{"id": "qcG_sWPiUz", "cdate": 1609459200000, "mdate": 1668592973925, "content": {"title": "Large-Scale Deep Learning Optimizations: A Comprehensive Survey", "abstract": "Deep learning have achieved promising results on a wide spectrum of AI applications. Larger datasets and models consistently yield better performance. However, we generally spend longer training time on more computation and communication. In this survey, we aim to provide a clear sketch about the optimizations for large-scale deep learning with regard to the model accuracy and model efficiency. We investigate algorithms that are most commonly used for optimizing, elaborate the debatable topic of generalization gap arises in large-batch training, and review the SOTA strategies in addressing the communication overhead and reducing the memory footprints."}}
