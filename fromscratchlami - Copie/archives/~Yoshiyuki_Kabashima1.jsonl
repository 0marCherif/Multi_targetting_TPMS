{"id": "OOWLRfAI_V_", "cdate": 1663850257923, "mdate": null, "content": {"title": "Quantized Compressed Sensing with Score-Based Generative Models", "abstract": "We consider the general problem of recovering a high-dimensional signal from noisy quantized measurements. Quantization, especially coarse quantization such as 1-bit sign measurements, leads to severe information loss and thus a good prior knowledge of the unknown signal is helpful for accurate recovery. Motivated by the power of score-based generative models (SGM, also known as diffusion models) in capturing the rich structure of natural signals beyond simple sparsity, we propose an unsupervised data-driven approach called quantized compressed sensing with SGM (QCS-SGM), where the prior distribution is modeled by a pre-trained SGM. To perform posterior sampling, an annealed pseudo-likelihood score called ${\\textit{noise perturbed pseudo-likelihood score}}$ is introduced and combined with the prior score of SGM. The proposed QCS-SGM applies to an arbitrary number of quantization bits. Experiments on a variety of baseline datasets demonstrate that the proposed QCS-SGM significantly outperforms existing state-of-the-art algorithms by a large margin for both in-distribution and out-of-distribution samples. Moreover, as a posterior sampling method, QCS-SGM can be easily used to obtain confidence intervals or uncertainty estimates of the reconstructed results. $\\textit{The code  is available at}$ https://github.com/mengxiangming/QCS-SGM."}}
{"id": "oJHtryAxxr", "cdate": 1640995200000, "mdate": 1668235805661, "content": {"title": "Macroscopic Analysis of Vector Approximate Message Passing in a Model-Mismatched Setting", "abstract": "In this study, macroscopic properties of the vector approximate message passing (VAMP) algorithm for inference of generalized linear models are investigated using a non-rigorous heuristic method of statistical mechanics when the true posterior cannot be used and the measurement matrix is a sample from rotation-invariant random matrix ensembles. The focus is on the correspondence between the non-rigorous replica analysis of statistical mechanics and the performance assessment of VAMP in the model-mismatched setting. The correspondence of this kind is well-known when the measurement matrix has independent and identically distributed entries. However, when the measurement matrix follows a general rotation-invariant matrix ensemble, the correspondence has been validated only under limited cases, such as the Bayes optimal inference or the convex empirical risk minimization. The result presented in this paper is to extend the scope of such correspondence. Herein, we heuristically derive the explicit formula of state-evolution equations, which macroscopically describe VAMP dynamics for the current model-mismatched case, and show that their fixed point is generally consistent with the replica symmetric solution obtained by the replica method of statistical mechanics. We also show that the fixed point of VAMP can exhibit a microscopic instability, which indicates that message variables continue to move by VAMP while their macroscopically summarized quantities converge to fixed values. The critical condition the for microscopic instability agrees with that for breaking the replica symmetry that is derived within the non-rigorous replica analysis. The results of the numerical experiments cross-check our findings."}}
{"id": "yTXtUSV-gk4", "cdate": 1621630135934, "mdate": null, "content": {"title": "Ising Model Selection Using $\\ell_{1}$-Regularized Linear Regression: A Statistical Mechanics Analysis", "abstract": "We theoretically analyze the typical learning performance of $\\ell_{1}$-regularized linear regression ($\\ell_1$-LinR) for Ising model selection using the replica method from statistical mechanics. For typical random regular graphs in the paramagnetic phase, an accurate estimate of the typical sample complexity of $\\ell_1$-LinR is obtained.   Remarkably, despite the model misspecification, $\\ell_1$-LinR is model selection consistent with the same order of sample complexity as $\\ell_{1}$-regularized logistic regression ($\\ell_1$-LogR), i.e., $M=\\mathcal{O}\\left(\\log N\\right)$,  where $N$ is the number of variables of the Ising model. Moreover, we provide an efficient method to accurately predict the non-asymptotic behavior of $\\ell_1$-LinR for moderate $M, N$, such as precision and recall. Simulations show a fairly good agreement between theoretical predictions and experimental results, even for graphs with many loops, which supports our findings. Although this paper mainly focuses on $\\ell_1$-LinR, our method is readily applicable for precisely characterizing the typical learning performances of a wide class of  $\\ell_{1}$-regularized $M$-estimators including $\\ell_1$-LogR and interaction screening. "}}
{"id": "na2YbI75Zr7", "cdate": 1609459200000, "mdate": 1668235805797, "content": {"title": "Matrix completion based on Gaussian belief propagation", "abstract": "We develop a message-passing algorithm for noisy matrix completion problems based on matrix factorization. The algorithm is derived by approximating message distributions of belief propagation with Gaussian distributions that share the same first and second moments. We also derive a memory-friendly version of the proposed algorithm by applying a perturbation treatment commonly used in the literature of approximate message passing. In addition, a damping technique, which is demonstrated to be crucial for optimal performance, is introduced without computational strain, and the relationship to the message-passing version of alternating least squares, a method reported to be optimal in certain settings, is discussed. Experiments on synthetic datasets show that while the proposed algorithm quantitatively exhibits almost the same performance under settings where the earlier algorithm is optimal, it is advantageous when the observed datasets are corrupted by non-Gaussian noise. Experiments on real-world datasets also emphasize the performance differences between the two algorithms."}}
{"id": "kmxUYMWdr7", "cdate": 1609459200000, "mdate": 1668235805653, "content": {"title": "Decision Theoretic Cutoff and ROC Analysis for Bayesian Optimal Group Testing", "abstract": "We study the inference problem in the group testing to identify defective items from the perspective of the decision theory. We introduce Bayesian inference and consider the Bayesian optimal setting in which the true generative process of the test results is known. We demonstrate the adequacy of the posterior marginal probability in the Bayesian optimal setting as a diagnostic variable based on the area under the curve (AUC). Using the posterior marginal probability, we derive the general expression of the optimal cutoff value that yields the minimum expected risk function. Furthermore, we evaluate the performance of the Bayesian group testing without knowing the true states of the items: defective or non-defective. By introducing an analytical method from statistical physics, we derive the receiver operating characteristics curve, and quantify the corresponding AUC under the Bayesian optimal setting. The obtained analytical results precisely describes the actual performance of the belief propagation algorithm defined for single samples when the number of items is sufficiently large."}}
{"id": "QgydzS_Tn0", "cdate": 1609459200000, "mdate": 1668235805666, "content": {"title": "Ising Model Selection Using $\\ell_{1}$-Regularized Linear Regression: A Statistical Mechanics Analysis", "abstract": "We theoretically analyze the typical learning performance of $\\ell_{1}$-regularized linear regression ($\\ell_1$-LinR) for Ising model selection using the replica method from statistical mechanics. For typical random regular graphs in the paramagnetic phase, an accurate estimate of the typical sample complexity of $\\ell_1$-LinR is obtained. Remarkably, despite the model misspecification, $\\ell_1$-LinR is model selection consistent with the same order of sample complexity as $\\ell_{1}$-regularized logistic regression ($\\ell_1$-LogR), i.e., $M=\\mathcal{O}\\left(\\log N\\right)$, where $N$ is the number of variables of the Ising model. Moreover, we provide an efficient method to accurately predict the non-asymptotic behavior of $\\ell_1$-LinR for moderate $M, N$, such as precision and recall. Simulations show a fairly good agreement between theoretical predictions and experimental results, even for graphs with many loops, which supports our findings. Although this paper mainly focuses on $\\ell_1$-LinR, our method is readily applicable for precisely characterizing the typical learning performances of a wide class of $\\ell_{1}$-regularized $M$-estimators including $\\ell_1$-LogR and interaction screening."}}
{"id": "GygVZlcb47", "cdate": 1609459200000, "mdate": 1668235805658, "content": {"title": "On Model Selection Consistency of Lasso for High-Dimensional Ising Models on Tree-like Graphs", "abstract": "We theoretically analyze the model selection consistency of least absolute shrinkage and selection operator (Lasso), both with and without post-thresholding, for high-dimensional Ising models. For random regular (RR) graphs of size $p$ with regular node degree $d$ and uniform couplings $\\theta_0$, it is rigorously proved that Lasso \\textit{without post-thresholding} is model selection consistent in the whole paramagnetic phase with the same order of sample complexity $n=\\Omega{(d^3\\log{p})}$ as that of $\\ell_1$-regularized logistic regression ($\\ell_1$-LogR). This result is consistent with the conjecture in Meng, Obuchi, and Kabashima 2021 using the non-rigorous replica method from statistical physics and thus complements it with a rigorous proof. For general tree-like graphs, it is demonstrated that the same result as RR graphs can be obtained under mild assumptions of the dependency condition and incoherence condition. Moreover, we provide a rigorous proof of the model selection consistency of Lasso with post-thresholding for general tree-like graphs in the paramagnetic phase without further assumptions on the dependency and incoherence conditions. Experimental results agree well with our theoretical analysis."}}
{"id": "sZYJktdwR1D", "cdate": 1577836800000, "mdate": 1668235805663, "content": {"title": "Macroscopic Analysis of Vector Approximate Message Passing in a Model Mismatch Setting", "abstract": "Vector approximate message passing (VAMP) is an efficient approximate inference algorithm used for generalized linear models. Although VAMP exhibits excellent performance, particularly when measurement matrices are sampled from rotationally invariant ensembles, existing convergence and performance analyses have been limited mostly to cases in which the correct posterior distribution is available. Here, we extend the analyses for cases in which the correct posterior distribution is not used in the inference stage. We derive state evolution equations, which macroscopically describe the dynamics of VAMP, and show that their fixed point is consistent with the replica symmetric solution obtained by the replica method of statistical mechanics. We also show that the fixed point of VAMP can exhibit a microscopic instability, the critical condition of which agrees with that for breaking the replica symmetry. The results of numerical experiments support our findings."}}
{"id": "qJTGrT0UrmK", "cdate": 1577836800000, "mdate": 1668235805674, "content": {"title": "Semi-analytic approximate stability selection for correlated data in generalized linear models", "abstract": "We consider the variable selection problem of generalized linear models (GLMs). Stability selection (SS) is a promising method proposed for solving this problem. Although SS provides practical variable selection criteria, it is computationally demanding because it needs to fit GLMs to many re-sampled datasets. We propose a novel approximate inference algorithm that can conduct SS without the repeated fitting. The algorithm is based on the replica method of statistical mechanics and vector approximate message passing of information theory. For datasets characterized by rotation-invariant matrix ensembles, we derive state evolution equations that macroscopically describe the dynamics of the proposed algorithm. We also show that their fixed points are consistent with the replica symmetric solution obtained by the replica method. Numerical experiments indicate that the algorithm exhibits fast convergence and high approximation accuracy for both synthetic and real-world data."}}
{"id": "SI56NDNW3Wb", "cdate": 1577836800000, "mdate": 1668235805671, "content": {"title": "Inferring Neuronal Couplings From Spiking Data Using a Systematic Procedure With a Statistical Criterion", "abstract": "Recent remarkable advances in experimental techniques have provided a background for inferring neuronal couplings from point process data that include a great number of neurons. Here, we propose a systematic procedure for pre- and postprocessing generic point process data in an objective manner to handle data in the framework of a binary simple statistical model, the Ising or generalized McCulloch\u2013Pitts model. The procedure has two steps: (1) determining time bin size for transforming the point process data into discrete-time binary data and (2) screening relevant couplings from the estimated couplings. For the first step, we decide the optimal time bin size by introducing the null hypothesis that all neurons would fire independently, then choosing a time bin size so that the null hypothesis is rejected with the strict criteria. The likelihood associated with the null hypothesis is analytically evaluated and used for the rejection process. For the second postprocessing step, after a certain estimator of coupling is obtained based on the preprocessed data set (any estimator can be used with the proposed procedure), the estimate is compared with many other estimates derived from data sets obtained by randomizing the original data set in the time direction. We accept the original estimate as relevant only if its absolute value is sufficiently larger than those of randomized data sets. These manipulations suppress false positive couplings induced by statistical noise. We apply this inference procedure to spiking data from synthetic and in vitro neuronal networks. The results show that the proposed procedure identifies the presence or absence of synaptic couplings fairly well, including their signs, for the synthetic and experimental data. In particular, the results support that we can infer the physical connections of underlying systems in favorable situations, even when using a simple statistical model."}}
