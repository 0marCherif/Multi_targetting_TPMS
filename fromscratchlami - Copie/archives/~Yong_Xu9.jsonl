{"id": "OdZcJYT5Z4k", "cdate": 1663849885328, "mdate": null, "content": {"title": "Generalized structure-aware missing view completion network for incomplete multi-view clustering", "abstract": "In recent years, incomplete multi-view clustering has been widely regarded as a challenging problem. The missing views inevitably damage the effective information of the multi-view data itself. To date, existing methods for incomplete multi-view clustering usually bypass invalid views according to prior missing information, which is considered as a second-best scheme based on evasion. Other methods that attempt to recover missing information are mostly applicable to specific two-view datasets. To handle these problems, we design a general structure-aware missing view completion network (SMVC) for incomplete multi-view clustering. Concretely, we build a two-stage autoencoder network with the self-attention structure to synchronously extract high-level semantic representations of multiple views and recover the missing data. In addition, we develop a recurrent graph reconstruction mechanism that cleverly leverages the restored views to promote the representation learning and the further data reconstruction. Sufficient experimental results confirm that our SMVC has obvious advantages over other top methods."}}
{"id": "WcSm-iommPR", "cdate": 1663849857935, "mdate": null, "content": {"title": "Prompt-driven efficient Open-set Semi-supervised Learning", "abstract": "Open-set Semi-Supervised Learning (OSSL) has always been vulnerable to the unseen categories, i.e., outliers, that have never been seen in the labeled set. Then a out-of-distribution (OOD) detector is introduced to identify outliers unseen in the labeled training data that the unlabeled data may contain, to reduce the damage to the SSl algorithm. In this work, we suggest that using a visual prompting driven mechanism to obtain higher effectiveness in the OSSL task. To this end, we propose a prompt-driven efficient OSSL framework, called OpenPrompt, which can propagate class information from labeled to unlabeled data with only a small amount of trainable parameters in the input space. Besides, a prompt-driven joint space learning mechanism is proposed to detect OOD data by maximizing the distribution gap between ID and OOD samples in unlabeled data. The experimental results on three public datasets show that OpenPrompt outperforms state-of-the-art methods with less than 1% of trainable parameters. More importantly, OpenPrompt achieves a 4% improvement in term of AUROC on outlier detection over a fully supervised model on CIFAR10."}}
{"id": "NEovrGdok02", "cdate": 1649413636727, "mdate": 1649413636727, "content": {"title": "Online Learning-Based Multi-Stage Complexity Control for Live Video Coding", "abstract": "High Efficiency Video Coding (HEVC) can significantly improve the compression efficiency in comparison with the preceding H.264/Advanced Video Coding (AVC) but at the cost of extremely high computational complexity. Hence, it is challenging to realize live video applications on low-delay and power-constrained devices, such as the smart mobile devices. In this paper, we propose an online learning-based multi-stage complexity control method for live video coding. The proposed method consists of three stages: multi-accuracy Coding Unit (CU) decision, multi-stage complexity allocation, and Coding Tree Unit (CTU) level complexity control. Consequently, the encoding complexity can be accurately controlled to correspond with the computing capability of the video-capable device by replacing the traditional brute-force search with the proposed algorithm, which properly determines the optimal CU size. Specifically, the multi-accuracy CU decision model is obtained by an online learning approach to accommodate the different characteristics of input videos. In addition, multi-stage complexity allocation is implemented to reasonably allocate the complexity budgets to each coding level. In order to achieve a good trade-off between complexity control and rate distortion (RD) performance, the CTU-level complexity control is proposed to select the optimal accuracy of the CU decision model. The experimental results show that the proposed algorithm can accurately control the coding complexity from 100\\% to 40\\%. Furthermore, the proposed algorithm outperforms the state-of-the-art algorithms in terms of both accuracy of complexity control and RD performance."}}
{"id": "Z9q6jVPWyy", "cdate": 1649413472551, "mdate": 1649413472551, "content": {"title": "Abnormal Event Detection Using Deep Contrastive Learning for Intelligent Video Surveillance System", "abstract": "The continuous developments of urban and industrial environments have increased the demand for intelligent video surveillance. Deep learning has achieved remarkable performance for anomaly detection in surveillance videos. Previous approaches achieve anomaly detection with a single pretext task (image reconstruction or prediction) and detect anomalies by larger reconstruction error or poor prediction. However, they cannot fully exploit the discriminative semantics and temporal context information. Moreover, tackling anomaly detection with a single pretext task is suboptimal due to the non-alignment between the pretext task and anomaly detection. In this article, we propose a temporal-aware contrastive network (TAC-Net) to address the above problems of anomaly detection for intelligence video surveillance. TAC-Net is an unsupervised method that utilizes deep contrastive self-supervised learning to capture the high-level semantic features and tackles anomaly detection with multiple self-supervised tasks. During inference phase, the multiple task losses and contrastive similarity are utilized to calculate the anomaly score. Experimental results show that our method is superior to state-of-the-art approaches on three benchmarks, which demonstrates the validity and advancement of TAC-Net."}}
{"id": "bOZXBIV09n", "cdate": 1649413335907, "mdate": 1649413335907, "content": {"title": "Self-Supervision-Augmented Deep Autoencoder for Unsupervised Visual Anomaly Detection", "abstract": "Deep autoencoder has demonstrated promising performances in visual anomaly detection. Learning normal patterns on normal data, deep autoencoder is expected to yield larger reconstruction errors for anomalous samples, which is utilized as the criterion for detecting anomalies. However, this hypothesis cannot be always tenable, since the deep autoencoder usually captures the low-level shared features between normal and abnormal data, which leads to similar reconstruction errors for them. To tackle this problem, we propose a self-supervised representation-augmented deep autoencoder for unsupervised visual anomaly detection, which can enlarge the gap of anomaly scores between normal and abnormal samples by introducing autoencoding transformation. Essentially, autoencoding transformation is introduced to facilitate autoencoder to learn the high-level visual semantic features of normal images by introducing a self-supervision task (transformation reconstruction). In particular, our model inputs the original and transformed images into encoder for obtaining latent representations, afterwards they are fed to the decoder for reconstructing both the original image and applied transformation. In this way, our model can utilize both image and transformation reconstruction errors to detect anomaly. Extensive experiments indicate that the proposed method outperforms other state-of-the-art methods, which demonstrates the validity and advancement of our model. "}}
{"id": "nciFoPZPbcr", "cdate": 1649413129252, "mdate": 1649413129252, "content": {"title": "Self-Supervised Attentive Generative Adversarial Networks for Video Anomaly Detection", "abstract": "Video anomaly detection (VAD) refers to the discrimination of unexpected events in videos. Deep generative model (DGM)-based method learns the regular patterns on normal videos and expects the learned model to yield larger generative errors for abnormal frames. However, DGM cannot always do so, since it usually captures the shared patterns between normal and abnormal events, which results in similar generative errors for them. In this paper, we propose a novel self-supervised framework for unsupervised VAD to tackle the above problem. To this end, we design a novel self-supervised attentive generative adversarial network (SSAGAN) which is composed of the self-attentive predictor, the vanilla discriminator and the self-supervised discriminator. On the one hand, the self-attentive predictor can capture the long-term dependences for improving the prediction qualities of normal frames. On the other hand, the predicted frames are fed to the vanilla discriminator and self-supervised discriminator for performing true-false discrimination and self-supervised rotation detection, respectively. Essentially, the role of self-supervised task is to enable the predictor to encode semantic information into the predicted normal frames via adversarial training, so as to the angles of rotated normal frames can be detected. As a result, our self-supervised framework lessens the generalization ability of model to abnormal frames, resulting in larger detection errors for abnormal frames. Extensive experimental results indicate that SSAGAN outperforms other state-of-the-art methods, which demonstrates the validity and advancement of SSAGAN."}}
