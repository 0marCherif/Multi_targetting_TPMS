{"id": "vouQcZS8KfW", "cdate": 1663850453952, "mdate": null, "content": {"title": "Neural Causal Models for Counterfactual Identification and Estimation", "abstract": "Evaluating hypothetical statements about how the world would be had a different course of action been taken is arguably one key capability expected from modern AI systems. Counterfactual reasoning underpins discussions in fairness, the determination of blame and responsibility, credit assignment, and regret. In this paper, we study the evaluation of counterfactual statements through neural models. Specifically, we tackle two causal problems required to make such evaluations, i.e., counterfactual identification and estimation from an arbitrary combination of observational and experimental data. First, we show that neural causal models (NCMs) are expressive enough and encode the structural constraints necessary for performing counterfactual reasoning. Second, we develop an algorithm for simultaneously identifying and estimating counterfactual distributions. We show that this algorithm is sound and complete for deciding counterfactual identification in general settings. Third, considering the practical implications of these results, we introduce a new strategy for modeling NCMs using generative adversarial networks. Simulations corroborate with the proposed methodology."}}
{"id": "Q1WuWmydhhr", "cdate": 1640995200000, "mdate": 1682364592049, "content": {"title": "Neural Causal Models for Counterfactual Identification and Estimation", "abstract": "Evaluating hypothetical statements about how the world would be had a different course of action been taken is arguably one key capability expected from modern AI systems. Counterfactual reasoning underpins discussions in fairness, the determination of blame and responsibility, credit assignment, and regret. In this paper, we study the evaluation of counterfactual statements through neural models. Specifically, we tackle two causal problems required to make such evaluations, i.e., counterfactual identification and estimation from an arbitrary combination of observational and experimental data. First, we show that neural causal models (NCMs) are expressive enough and encode the structural constraints necessary for performing counterfactual reasoning. Second, we develop an algorithm for simultaneously identifying and estimating counterfactual distributions. We show that this algorithm is sound and complete for deciding counterfactual identification in general settings. Third, considering the practical implications of these results, we introduce a new strategy for modeling NCMs using generative adversarial networks. Simulations corroborate with the proposed methodology."}}
{"id": "DzWitBGU_6", "cdate": 1640995200000, "mdate": 1668615713069, "content": {"title": "Causal Transportability for Visual Recognition", "abstract": "Visual representations underlie object recognition tasks, but they often contain both robust and non-robust features. Our main observation is that image classifiers may perform poorly on out-of-distribution samples because spurious correlations between non-robust features and labels can be changed in a new environment. By analyzing procedures for out-of-distribution generalization with a causal graph, we show that standard classifiers fail because the association between images and labels is not transportable across settings. However, we then show that the causal effect, which severs all sources of confounding, remains invariant across domains. This motivates us to develop an algorithm to estimate the causal effect for image classification, which is transportable (i.e., invariant) across source and target environments. Without observing additional variables, we show that we can derive an estimand for the causal effect under empirical assumptions using representations in deep models as proxies. Theoretical analysis, empirical results, and visualizations show that our approach captures causal invariances and improves overall generalization."}}
{"id": "hGmrNwR8qQP", "cdate": 1621630324074, "mdate": null, "content": {"title": "The Causal-Neural Connection: Expressiveness, Learnability, and Inference", "abstract": "One of the central elements of any causal inference is an object called structural causal model (SCM), which represents a collection of mechanisms and exogenous sources of random variation of the system under investigation (Pearl, 2000). An important property of many kinds of neural networks is universal approximability: the ability to approximate any function to arbitrary precision. Given this property, one may be tempted to surmise that a collection of neural nets is capable of learning any SCM by training on data generated by that SCM. In this paper, we show this is not the case by disentangling the notions of expressivity and learnability. Specifically, we show that the causal hierarchy theorem (Thm. 1, Bareinboim et al., 2020), which describes the limits of what can be learned from data, still holds for neural models. For instance, an arbitrarily complex and expressive neural net is unable to predict the effects of interventions given observational data alone. Given this result, we introduce a special type of SCM called a neural causal model (NCM), and formalize a new type of inductive bias to encode structural constraints necessary for performing causal inferences. Building on this new class of models, we focus on solving two canonical tasks found in the literature known as causal  identification and estimation. Leveraging the neural toolbox, we develop an algorithm that is both sufficient and necessary to determine whether a causal effect can be learned from data (i.e., causal identifiability); it then estimates the effect whenever identifiability holds (causal estimation). Simulations corroborate the proposed approach."}}
{"id": "wcbZr9yxvD", "cdate": 1609459200000, "mdate": 1682364592049, "content": {"title": "The Causal-Neural Connection: Expressiveness, Learnability, and Inference", "abstract": "One of the central elements of any causal inference is an object called structural causal model (SCM), which represents a collection of mechanisms and exogenous sources of random variation of the system under investigation (Pearl, 2000). An important property of many kinds of neural networks is universal approximability: the ability to approximate any function to arbitrary precision. Given this property, one may be tempted to surmise that a collection of neural nets is capable of learning any SCM by training on data generated by that SCM. In this paper, we show this is not the case by disentangling the notions of expressivity and learnability. Specifically, we show that the causal hierarchy theorem (Thm. 1, Bareinboim et al., 2020), which describes the limits of what can be learned from data, still holds for neural models. For instance, an arbitrarily complex and expressive neural net is unable to predict the effects of interventions given observational data alone. Given this result, we introduce a special type of SCM called a neural causal model (NCM), and formalize a new type of inductive bias to encode structural constraints necessary for performing causal inferences. Building on this new class of models, we focus on solving two canonical tasks found in the literature known as causal identification and estimation. Leveraging the neural toolbox, we develop an algorithm that is both sufficient and necessary to determine whether a causal effect can be learned from data (i.e., causal identifiability); it then estimates the effect whenever identifiability holds (causal estimation). Simulations corroborate the proposed approach."}}
{"id": "4H0CH1GCtz", "cdate": 1609459200000, "mdate": 1682364592050, "content": {"title": "Development of a speed invariant deep learning model with application to condition monitoring of rotating machinery", "abstract": "The application of cutting-edge technologies such as AI, smart sensors, and IoT in factories is revolutionizing the manufacturing industry. This emerging trend, so called smart manufacturing, is a collection of various technologies that support decision-making in real-time in the presence of changing conditions in manufacturing activities; this may advance manufacturing competitiveness and sustainability. As a factory becomes highly automated, physical asset management comes to be a critical part of an operational life-cycle. Maintenance is one area where the collection of technologies may be applied to enhance operational reliability using a machine condition monitoring system. Data-driven models have been extensively applied to machine condition data to build a fault detection system. Most existing studies on fault detection were developed under a fixed set of operating conditions and tested with data obtained from that set of conditions. Therefore, variability in a model\u2019s performance from data obtained from different operating settings is not well reported. There have been limited studies considering changing operational conditions in a data-driven model. For practical applications, a model must identify a targeted fault under variable operational conditions. With this in mind, the goal of this paper is to study invariance of model to changing speed via a deep learning method, which can detect a mechanical imbalance, i.e., targeted fault, under varying speed settings. To study the speed invariance, experimental data obtained from a motor test-bed are processed, and time-series data and time\u2013frequency data are applied to long short-term memory and convolutional neural network, respectively, to evaluate their performance."}}
