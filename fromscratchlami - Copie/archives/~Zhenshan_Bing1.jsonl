{"id": "x7tkpKcwSH", "cdate": 1677628800000, "mdate": 1684152858552, "content": {"title": "Meta-Reinforcement Learning in Non-Stationary and Dynamic Environments", "abstract": "In recent years, the subject of deep reinforcement learning (DRL) has developed very rapidly, and is now applied in various fields, such as decision making and control tasks. However, artificial agents trained with RL algorithms require great amounts of training data, unlike humans that are able to learn new skills from very few examples. The concept of meta-reinforcement learning (meta-RL) has been recently proposed to enable agents to learn similar but new skills from a small amount of experience by leveraging a set of tasks with a shared structure. Due to the task representation learning strategy with few-shot adaptation, most recent work is limited to narrow task distributions and stationary environments, where tasks do not change within episodes. In this work, we address those limitations and introduce a training strategy that is applicable to non-stationary environments, as well as a task representation based on Gaussian mixture models to model clustered task distributions. We evaluate our method on several continuous robotic control benchmarks. Compared with state-of-the-art literature that is only applicable to stationary environments with few-shot adaption, our algorithm first achieves competitive asymptotic performance and superior sample efficiency in stationary environments with zero-shot adaption. Second, our algorithm learns to perform successfully in non-stationary settings as well as a continual learning setting, while learning well-structured task representations. Last, our algorithm learns basic distinct behaviors and well-structured task representations in task distributions with multiple qualitatively distinct tasks."}}
{"id": "eQ_ffqbKg9O", "cdate": 1675209600000, "mdate": 1684152858338, "content": {"title": "Reduced Model-Based Fault Detector and Controller Design for Discrete-Time Switching Fuzzy Systems", "abstract": "The reduced model-based coordinated design of fault detectors and controllers for discrete-time switching fuzzy systems is examined. First, the mean-square exponential stabilization of switching Takagi\u2013Sugeno fuzzy systems is performed using the average dwell time method under an arbitrary switching law. Next, using segmented Lyapunov function techniques, a dynamic full- and reduced-order fault detector and controller is designed to ensure that the overall dynamic residual system is mean-square exponentially stable with a balanced <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\mathcal {H}_{\\infty }$</tex-math></inline-formula> performance level <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$(\\xi, \\beta)$</tex-math></inline-formula> . The solvability conditions for the fault detector and controller are derived using a linearization method, and the relevant parameters can be determined using the mathematical linear matrix solver toolbox. Two examples including a switching Chua\u2019s circuit system are presented to demonstrate the effectiveness of the proposed fault detector and controller."}}
{"id": "wXlCXHttOW", "cdate": 1672531200000, "mdate": 1684152858353, "content": {"title": "Sequential Spatial Network for Collision Avoidance in Autonomous Driving", "abstract": "Several autonomous driving strategies have been applied to autonomous vehicles, especially in the collision avoidance area. The purpose of collision avoidance is achieved by adjusting the trajectory of autonomous vehicles (AV) to avoid intersection or overlap with the trajectory of surrounding vehicles. A large number of sophisticated vision algorithms have been designed for target inspection, classification, and other tasks, such as ResNet, YOLO, etc., which have achieved excellent performance in vision tasks because of their ability to accurately and quickly capture regional features. However, due to the variability of different tasks, the above models achieve good performance in capturing small regions but are still insufficient in correlating the regional features of the input image with each other. In this paper, we aim to solve this problem and develop an algorithm that takes into account the advantages of CNN in capturing regional features while establishing feature correlation between regions using variants of attention. Finally, our model achieves better performance in the test set of L5Kit compared to the other vision models. The average number of collisions is 19.4 per 10000 frames of driving distance, which greatly improves the success rate of collision avoidance."}}
{"id": "rWxwHYCJHk", "cdate": 1672531200000, "mdate": 1684152858370, "content": {"title": "Accelerate Training of Reinforcement Learning Agent by Utilization of Current and Previous Experience", "abstract": ""}}
{"id": "j33syIWQEWW", "cdate": 1672531200000, "mdate": 1683886475526, "content": {"title": "Meta-Reinforcement Learning Based on Self-Supervised Task Representation Learning", "abstract": "Meta-reinforcement learning enables artificial agents to learn from related training tasks and adapt to new tasks efficiently with minimal interaction data. However, most existing research is still limited to narrow task distributions that are parametric and stationary, and does not consider out-of-distribution tasks during the evaluation, thus, restricting its application. In this paper, we propose MoSS, a context-based Meta-reinforcement learning algorithm based on Self-Supervised task representation learning to address this challenge. We extend meta-RL to broad non-parametric task distributions which have never been explored before, and also achieve state-of-the-art results in non-stationary and out-of-distribution tasks. Specifically, MoSS consists of a task inference module and a policy module. We utilize the Gaussian mixture model for task representation to imitate the parametric and non-parametric task variations. Additionally, our online adaptation strategy enables the agent to react at the first sight of a task change, thus being applicable in non-stationary tasks. MoSS also exhibits strong generalization robustness in out-of-distributions tasks which benefits from the reliable and robust task representation. The policy is built on top of an off-policy RL algorithm and the entire network is trained completely off-policy to ensure high sample efficiency. On MuJoCo and Meta-World benchmarks, MoSS outperforms prior works in terms of asymptotic performance, sample efficiency (3-50x faster), adaptation efficiency, and generalization robustness on broad and diverse task distributions."}}
{"id": "iT-S0qZf4W", "cdate": 1672531200000, "mdate": 1684152858497, "content": {"title": "Human Robot Interaction with Triboelectric Nanogenerator for Tactile Sensing", "abstract": "The key success factors for Human Robot Interaction (HRI) interfaces heavily depend on the robot sensing technologies such as vision and touch sensors. In this paper we investigate the use of Nanogenerator technologies as a sensing device for robotic applications. The particular property that we explore here deals with Triboelectric Nanogenerator (TENG) sensor in which an electric signal can be generated automatically once the sensor is mechanically triggered. We propose to use TENG as a tactile sensor so robot is able to interact with human safely. The result shows that the TENG sensor is sensitive enough for a robotic safety task. The robot arm can stop when contacting the human hand and attempt to find a new way to reach a desired goal. Our experiment uncovers opportunities to use TENG sensors for real-time HRI applications."}}
{"id": "ZWLvenHnYE", "cdate": 1672531200000, "mdate": 1683911692563, "content": {"title": "Solving Robotic Manipulation With Sparse Reward Reinforcement Learning Via Graph-Based Diversity and Proximity", "abstract": "In multigoal reinforcement learning (RL), algorithms usually suffer from inefficiency in the collection of successful experiences in tasks with sparse rewards. By utilizing the ideas of relabeling hindsight experience and curriculum learning, some prior works have greatly improved the sample efficiency in robotic manipulation tasks, such as hindsight experience replay (HER), hindsight goal generation (HGG), graph-based HGG (G-HGG), and curriculum-guided HER (CHER). However, none of these can learn efficiently to solve challenging manipulation tasks with distant goals and obstacles, since they rely either on heuristic or simple distance-guided exploration. In this article, we introduce graph-curriculum-guided HGG (GC-HGG), an extension of CHER and G-HGG, which works by selecting hindsight goals on the basis of graph-based proximity and diversity. We evaluated GC-HGG in four challenging manipulation tasks involving obstacles in both simulations and real-world experiments, in which significant enhancements in both sample efficiency and overall success rates over prior works were demonstrated. Videos and codes can be viewed at this link:  <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://videoviewsite.wixsite.com/gc-hgg</uri> ."}}
{"id": "CXQJ_4PDHgH", "cdate": 1672531200000, "mdate": 1683886475583, "content": {"title": "Safety Guaranteed Manipulation Based on Reinforcement Learning Planner and Model Predictive Control Actor", "abstract": "Deep reinforcement learning (RL) has been endowed with high expectations in tackling challenging manipulation tasks in an autonomous and self-directed fashion. Despite the significant strides made in the development of reinforcement learning, the practical deployment of this paradigm is hindered by at least two barriers, namely, the engineering of a reward function and ensuring the safety guaranty of learning-based controllers. In this paper, we address these challenging limitations by proposing a framework that merges a reinforcement learning \\lstinline[columns=fixed]{planner} that is trained using sparse rewards with a model predictive controller (MPC) \\lstinline[columns=fixed]{actor}, thereby offering a safe policy. On the one hand, the RL \\lstinline[columns=fixed]{planner} learns from sparse rewards by selecting intermediate goals that are easy to achieve in the short term and promising to lead to target goals in the long term. On the other hand, the MPC \\lstinline[columns=fixed]{actor} takes the suggested intermediate goals from the RL \\lstinline[columns=fixed]{planner} as the input and predicts how the robot's action will enable it to reach that goal while avoiding any obstacles over a short period of time. We evaluated our method on four challenging manipulation tasks with dynamic obstacles and the results demonstrate that, by leveraging the complementary strengths of these two components, the agent can solve manipulation tasks in complex, dynamic environments safely with a $100\\%$ success rate. Videos are available at \\url{https://videoviewsite.wixsite.com/mpc-hgg}."}}
{"id": "tyZ1ChGZIKO", "cdate": 1663850536141, "mdate": null, "content": {"title": "Selective Frequency Network for Image Restoration", "abstract": "Image restoration aims to reconstruct the latent sharp image from its corrupted counterpart. Besides dealing with this long-standing task in the spatial domain, a few approaches seek solutions in the frequency domain in consideration of the large discrepancy between spectra of sharp/degraded image pairs. However, these works commonly utilize transformation tools, e.g., wavelet transform, to split features into several frequency parts, which is not flexible enough to select the most informative frequency component to recover. In this paper, we exploit a multi-branch and content-aware module to decompose features into separate frequency subbands dynamically and locally, and then accentuate the useful ones via channel-wise attention weights. In addition, to handle large-scale degradation blurs, we propose an extremely simple decoupling and modulation module to enlarge the receptive field via global and window-based average pooling. Integrating two developed modules into a U-Net backbone, the proposed Selective Frequency Network (SFNet) performs favorably against state-of-the-art algorithms on five image restoration tasks, including single-image defocus deblurring, image dehazing, image motion deblurring, image desnowing, and image deraining."}}
{"id": "k1sxUcLhzs", "cdate": 1640995200000, "mdate": 1683911692558, "content": {"title": "Complex Robotic Manipulation via Graph-Based Hindsight Goal Generation", "abstract": "Reinforcement learning algorithms, such as hindsight experience replay (HER) and hindsight goal generation (HGG), have been able to solve challenging robotic manipulation tasks in multigoal settings with sparse rewards. HER achieves its training success through hindsight replays of past experience with heuristic goals but underperforms in challenging tasks in which goals are difficult to explore. HGG enhances HER by selecting intermediate goals that are easy to achieve in the short term and promising to lead to target goals in the long term. This guided exploration makes HGG applicable to tasks in which target goals are far away from the object\u2019s initial position. However, the vanilla HGG is not applicable to manipulation tasks with obstacles because the Euclidean metric used for HGG is not an accurate distance metric in such an environment. Although, with the guidance of a handcrafted distance grid, grid-based HGG can solve manipulation tasks with obstacles, a more feasible method that can solve such tasks automatically is still in demand. In this article, we propose graph-based hindsight goal generation (G-HGG), an extension of HGG selecting hindsight goals based on shortest distances in an obstacle-avoiding graph, which is a discrete representation of the environment. We evaluated G-HGG on four challenging manipulation tasks with obstacles, where significant enhancements in both sample efficiency and overall success rate are shown over HGG and HER. Videos can be viewed at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://videoviewsite.wixsite.com/ghgg</uri> ."}}
