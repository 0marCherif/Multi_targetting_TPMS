{"id": "HBgzIHOUqgq", "cdate": 1646057534396, "mdate": null, "content": {"title": "Recognizing Actions using Object States", "abstract": "Object-centric actions cause changes in object states,  including their visual appearance and their immediate context.  We propose a computational framework that uses only two object states, start and end, and learns to recognize the under-lying actions.  Our approach has two modules that learn subtle changes induced by the action and suppress spurious correlations.  We demonstrate that only two object states are sufficient to recognize object-centric actions. Our framework per-forms better than approaches that use multiple frames and a relatively large model.Moreover, our method generalizes to unseen objects and unseen video datasets"}}
{"id": "ckm8KvA8sSzx", "cdate": 1609459200000, "mdate": 1665999914346, "content": {"title": "Diverse Video Generation using a Gaussian Process Trigger", "abstract": "Generating future frames given a few context (or past) frames is a challenging task. It requires modeling the temporal coherence of videos and multi-modality in terms of diversity in the potential future states. Current variational approaches for video generation tend to marginalize over multi-modal future outcomes. Instead, we propose to explicitly model the multi-modality in the future outcomes and leverage it to sample diverse futures. Our approach, Diverse Video Generator, uses a Gaussian Process (GP) to learn priors on future states given the past and maintains a probability distribution over possible futures given a particular sample. In addition, we leverage the changes in this distribution over time to control the sampling of diverse future states by estimating the end of ongoing sequences. That is, we use the variance of GP over the output function space to trigger a change in an action sequence. We achieve state-of-the-art results on diverse future frame generation in terms of reconstruction quality and diversity of the generated sequences."}}
{"id": "cdXfxIKUYG-j", "cdate": 1609459200000, "mdate": 1665999914300, "content": {"title": "Hierarchical Video Prediction Using Relational Layouts for Human-Object Interactions", "abstract": "Learning to model and predict how humans interact with objects while performing an action is challenging, and most of the existing video prediction models are ineffective in modeling complicated human-object interactions. Our work builds on hierarchical video prediction models, which disentangle the video generation process into two stages: predicting a high-level representation, such as pose sequence, and then learning a pose-to-pixels translation model for pixel generation. An action sequence for a human-object interaction task is typically very complicated, involving the evolution of pose, person's appearance, object locations, and object appearances over time. To this end, we propose a Hierarchical Video Prediction model using Relational Layouts. In the first stage, we learn to predict a sequence of layouts. A layout is a high-level representation of the video containing both pose and objects' information for every frame. The layout sequence is learned by modeling the relationships between the pose and objects using relational reasoning and recurrent neural networks. The layout sequence acts as a strong structure prior to the second stage that learns to map the layouts into pixel space. Experimental evaluation of our method on two datasets, UMD-HOI and Bimanual, shows significant improvements in standard video evaluation metrics such as LPIPS, PSNR, and SSIM. We also perform a detailed qualitative analysis of our model to demonstrate various generalizations."}}
{"id": "P1rEtIi6R0K", "cdate": 1609459200000, "mdate": 1665999914198, "content": {"title": "Diverse Video Generation using a Gaussian Process Trigger", "abstract": "Generating future frames given a few context (or past) frames is a challenging task. It requires modeling the temporal coherence of videos as well as multi-modality in terms of diversity in the potential future states. Current variational approaches for video generation tend to marginalize over multi-modal future outcomes. Instead, we propose to explicitly model the multi-modality in the future outcomes and leverage it to sample diverse futures. Our approach, Diverse Video Generator, uses a GP to learn priors on future states given the past and maintains a probability distribution over possible futures given a particular sample. We leverage the changes in this distribution over time to control the sampling of diverse future states by estimating the end of on-going sequences. In particular, we use the variance of GP over the output function space to trigger a change in the action sequence. We achieve state-of-the-art results on diverse future frame generation in terms of reconstruction quality and diversity of the generated sequences."}}
{"id": "XZzriKGEj0_", "cdate": 1601308119774, "mdate": null, "content": {"title": "Learning What Not to Model: Gaussian Process Regression with Negative Constraints", "abstract": "Gaussian Process (GP) regression fits a curve on a set of datapairs, with each pair consisting of an input point '$\\mathbf{x}$' and its corresponding target regression value '$y(\\mathbf{x})$' (a positive datapair). But, what if for an input point '$\\bar{\\mathbf{x}}$', we want to constrain the GP to avoid a target regression value '$\\bar{y}(\\bar{\\mathbf{x}})$' (a negative datapair)? This requirement can often appear in real-world navigation tasks, where an agent would want to avoid obstacles, like furniture items in a room when planning a trajectory to navigate. In this work, we propose to incorporate such negative constraints in a GP regression framework. Our approach, 'GP-NC' or Gaussian Process with Negative Constraints, fits over the positive datapairs while avoiding the negative datapairs. Specifically, our key idea is to model the negative datapairs using small blobs of Gaussian distribution and maximize its KL divergence from the GP. We jointly optimize the GP-NC for both the positive and negative datapairs. We empirically demonstrate that our GP-NC framework performs better than the traditional GP learning and that our framework does not affect the scalability of Gaussian Process regression and helps the model converge faster as the size of the data increases."}}
{"id": "Qm7R_SdqTpT", "cdate": 1601308117970, "mdate": null, "content": {"title": "Diverse Video Generation using a Gaussian Process Trigger", "abstract": "Generating future frames given a few context (or past) frames is a challenging task. It requires modeling the temporal coherence of videos as well as multi-modality in terms of diversity in the potential future states. Current variational approaches for video generation tend to marginalize over multi-modal future outcomes. Instead, we propose to explicitly model the multi-modality in the future outcomes and leverage it to sample diverse futures. Our approach, Diverse Video Generator, uses a GP to learn priors on future states given the past and maintains a probability distribution over possible futures given a particular sample. We leverage the changes in this distribution over time to control the sampling of diverse future states by estimating the end of on-going sequences. In particular, we use the variance of GP over the output function space to trigger a change in the action sequence. We achieve state-of-the-art results on diverse future frame generation in terms of reconstruction quality and diversity of the generated sequences."}}
