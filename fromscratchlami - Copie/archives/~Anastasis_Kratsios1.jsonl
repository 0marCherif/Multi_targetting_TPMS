{"id": "s_isV3awYv", "cdate": 1679945385990, "mdate": 1679945385990, "content": {"title": "Designing Universal Causal Deep Learning Models: The Geometric (Hyper)Transformer", "abstract": "Several problems in stochastic analysis are defined through their geometry, and preserving that geometric structure is essential to generating meaningful predictions. Nevertheless, how to design principled deep learning (DL) models capable of encoding these geometric structures remains largely unknown. We address this open problem by introducing a universal causal geometric DL framework in which the user specifies a suitable pair of metric spaces \ud835\udcb3 and \ud835\udcb4 and our framework returns a DL model capable of causally approximating any ``regular'' map sending time series in \ud835\udcb3\u2124 to time series in \ud835\udcb4\u2124 while respecting their forward flow of information throughout time. Suitable geometries on \ud835\udcb4 include various (adapted) Wasserstein spaces arising in optimal stopping problems, a variety of statistical manifolds describing the conditional distribution of continuous-time finite state Markov chains, and all Fr\u00e9chet spaces admitting a Schauder basis, e.g. as in classical finance. Suitable spaces \ud835\udcb3 are compact subsets of any Euclidean space. Our results all quantitatively express the number of parameters needed for our DL model to achieve a given approximation error as a function of the target map's regularity and the geometric structure both of \ud835\udcb3 and of \ud835\udcb4. Even when omitting any temporal structure, our universal approximation theorems are the first guarantees that H\u00f6lder functions, defined between such \ud835\udcb3 and \ud835\udcb4 can be approximated by DL models. "}}
{"id": "b9rZTeYzW2", "cdate": 1679945333190, "mdate": null, "content": {"title": "Universal Regular Conditional Distributions", "abstract": "We introduce a deep learning model that can universally approximate regular conditional distributions (RCDs). The proposed model operates in three phases: first, it linearizes inputs from a given metric space to via a feature map, then a deep feedforward neural network processes these linearized features, and then the network's outputs are then transformed to the -Wasserstein space via a probabilistic extension of the attention mechanism of Bahdanau et al.\\ (2014). Our model, called the \\textit{probabilistic transformer (PT)}, can approximate any continuous function from to uniformly on compact sets, quantitatively. We identify two ways in which the PT avoids the curse of dimensionality when approximating -valued functions. The first strategy builds functions in which can be efficiently approximated by a PT, uniformly on any given compact subset of . In the second approach, given any function in , we build compact subsets of whereon can be efficiently approximated by a PT."}}
{"id": "2MTn3zHzSgA", "cdate": 1679945304236, "mdate": 1679945304236, "content": {"title": "Small Transformers Compute Universal Metric Embeddings", "abstract": "We study representations of data from an arbitrary metric space in the space of univariate Gaussian mixtures with a transport metric (Delon and Desolneux 2020). We derive embedding guarantees for feature maps implemented by small neural networks called \\emph{probabilistic transformers}. Our guarantees are of memorization type: we prove that a probabilistic transformer of depth about and width about can bi-H\\\"{o}lder embed any -point dataset from with low metric distortion, thus avoiding the curse of dimensionality. We further derive probabilistic bi-Lipschitz guarantees which trade off the amount of distortion and the probability that a randomly chosen pair of points embeds with that distortion. If 's geometry is sufficiently regular, we obtain stronger, bi-Lipschitz guarantees for all points in the dataset. As applications we derive neural embedding guarantees for datasets from Riemannian manifolds, metric trees, and certain types of combinatorial graphs."}}
{"id": "9RxEOKLdQt", "cdate": 1679945230875, "mdate": 1679945230875, "content": {"title": "NEU: A Meta-Algorithm for Universal UAP-Invariant Feature Representation ", "abstract": "Effective feature representation is key to the predictive performance of any algorithm. This paper introduces a meta-procedure, called Non-Euclidean Upgrading (NEU), which learns feature maps that are expressive enough to embed the universal approximation property (UAP) into most model classes while only outputting feature maps that preserve any model class's UAP. We show that NEU can learn any feature map with these two properties if that feature map is asymptotically deformable into the identity. We also find that the feature-representations learned by NEU are always submanifolds of the feature space. NEU's properties are derived from a new deep neural model that is universal amongst all orientation-preserving homeomorphisms on the input space. We derive qualitative and quantitative approximation guarantees for this architecture. We quantify the number of parameters required for this new architecture to memorize any set of input-output pairs while simultaneously fixing every point of the input space lying outside some compact set, and we quantify the size of this set as a function of our model's depth. Moreover, we show that deep feedforward networks with most commonly used activation functions typically do not have all these properties. NEU's performance is evaluated against competing machine learning methods on various regression and dimension reduction tasks both with financial and simulated data. "}}
{"id": "uTd_ZmEBxB", "cdate": 1679945174582, "mdate": 1679945174582, "content": {"title": "Universal Approximation Theorems for Differentiable Geometric Deep Learning", "abstract": "This paper addresses the growing need to process non-Euclidean data, by introducing a geometric deep learning (GDL) framework for building universal feedforward-type models compatible with differentiable manifold geometries. We show that our GDL models can approximate any continuous target function uniformly on compact sets of a controlled maximum diameter. We obtain curvature dependant lower-bounds on this maximum diameter and upper-bounds on the depth of our approximating GDL models. Conversely, we find that there is always a continuous function between any two non-degenerate compact manifolds that any \u201clocally-defined\u201d GDL model cannot uniformly approximate. Our last main result identifies data-dependent conditions guaranteeing that the GDL model implementing our approximation breaks \u201cthe curse of dimensionality.\u201d We find that any \u201creal-world\u201d(ie finite) dataset always satisfies our condition and, conversely, any dataset satisfies our requirement if the target function is smooth. As applications, we confirm the universal approximation capabilities of the following GDL models: Ganea et al.(2018)\u2019s hyperbolic feedforward networks, the architecture implementing Krishnan et al.(2015)\u2019s deep Kalman-Filter, and deep softmax classifiers. We build universal extensions/variants of: the SPD-matrix regressor of Meyer et al.(2011b), and Fletcher et al.(2009)\u2019s Procrustean regressor. In the Euclidean setting, our results imply a quantitative version of Kidger and Lyons (2020)\u2019s approximation theorem and a data-dependent version of Yarotsky and Zhevnerchuk (2020)\u2019s uncursed approximation rates."}}
{"id": "rZPuBHQryN0", "cdate": 1675520703490, "mdate": 1675520703490, "content": {"title": "Instance-Dependent Generalization Bounds via Optimal Transport", "abstract": "Existing generalization bounds fail to explain crucial factors that drive generalization of modern neural networks. Since such bounds often hold uniformly over all parameters, they suffer from over-parametrization, and fail to account for the strong inductive bias of initialization and stochastic gradient descent. As an alternative, we propose a novel optimal transport interpretation of the generalization problem. This allows us to derive instance-dependent generalization bounds that depend on the local Lipschitz regularity of the earned prediction function in the data space. Therefore, our bounds are agnostic to the parametrization of the model and work well when the number of training samples is much smaller than the number of parameters. With small modifications, our approach yields accelerated rates for data on low-dimensional manifolds, and guarantees under distribution shifts. We empirically analyze our generalization bounds for neural networks, showing that the bound values are meaningful and capture the effect of popular regularization methods during training."}}
{"id": "JGO8CvG5S9", "cdate": 1632875684339, "mdate": null, "content": {"title": "Universal Approximation Under Constraints is Possible with Transformers", "abstract": "Many practical problems need the output of a machine learning model to satisfy a set of constraints, $K$.  Nevertheless, there is no known guarantee that classical neural network architectures can exactly encode constraints while simultaneously achieving universality.  We provide a quantitative constrained universal approximation theorem which guarantees that for any non-convex compact set $K$ and any continuous function $f:\\mathbb{R}^n\\rightarrow K$, there is a probabilistic transformer $\\hat{F}$ whose randomized outputs all lie in $K$ and whose expected output uniformly approximates $f$.  Our second main result is a ``deep neural version'' of Berge's Maximum Theorem (1963).  The result guarantees that given an objective function $L$, a constraint set $K$, and a family of soft constraint sets, there is a probabilistic transformer $\\hat{F}$ that approximately minimizes $L$ and whose outputs belong to $K$; moreover, $\\hat{F}$ approximately satisfies the soft constraints.  Our results imply the first universal approximation theorem for classical transformers with exact convex constraint satisfaction.  They also yield that a chart-free universal approximation theorem for Riemannian manifold-valued functions subject to suitable geodesically convex constraints."}}
