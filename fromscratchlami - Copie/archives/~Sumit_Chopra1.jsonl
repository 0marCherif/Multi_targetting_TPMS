{"id": "iA0XwM0IU08", "cdate": 1673287857494, "mdate": null, "content": {"title": "On Sensitivity and Robustness of Normalization Schemes to Input Distribution Shifts in Automatic MR Image Diagnosis", "abstract": "Magnetic Resonance Imaging (MRI) is considered the gold standard of medical imaging because of the excellent soft-tissue contrast exhibited in the images reconstructed by the MRI pipeline, which in-turn enables the human radiologist to discern many pathologies easily. \nMore recently, Deep Learning~(DL) models have also achieved state-of-the-art performance in diagnosing multiple diseases using these reconstructed images as input. However, the image reconstruction process within the MRI pipeline, which requires the use of complex hardware and adjustment of a large number of scanner parameters, is highly susceptible to noise of various forms, resulting in arbitrary artifacts within the images. Furthermore, the noise distribution is not stationary and varies within a machine, across machines, and patients, leading to varying artifacts within the images. Unfortunately, DL models are quite sensitive to these varying artifacts as it leads to changes in the input data distribution between the training and testing phases. The lack of robustness of these models against varying artifacts impedes their use in medical applications where safety is critical. In this work, we focus on improving the generalization performance of these models in the presence of multiple varying artifacts that manifest due to the complexity of the MR data acquisition. In our experiments, we observe that Batch Normalization (BN), a widely used technique during the training of DL models for medical image analysis, is a significant cause of performance degradation in these changing environments. As a solution, we propose to use other normalization techniques, such as Group Normalization (GN) and Layer Normalization (LN), to inject robustness into model performance against varying image artifacts.\nThrough a systematic set of experiments, we show that GN and LN provide better accuracy for various MR artifacts and distribution shifts. "}}
{"id": "S9EfOVFJIxQh", "cdate": 1673287853038, "mdate": null, "content": {"title": "Radiology Reports Improve Visual Representations Learned from Radiographs", "abstract": "Although human\u2019s ability to visually understand the structure of the World plays a\ncrucial role in perceiving the World and making appropriate decisions, human perception\ndoes not solely rely on vision but amalgamates the information from acoustic, verbal, and\nvisual stimuli. An active area of research has been revolving around designing an efficient\nframework that adapts to multiple modalities and ideally improves the performance of existing\ntasks. While numerous frameworks have proved effective on natural datasets like\nImageNet, a limited number of studies have been carried out in the biomedical domain.\nIn this work, we extend the available frameworks for natural data to biomedical data by\nleveraging the abundant, unstructured multi-modal data available as radiology images and\nreports. We attempt to answer the question, \u201dFor multi-modal learning, self-supervised\nlearning and joint learning using both learning strategies, which one improves the visual\nrepresentation for downstream chest radiographs classification tasks the most?\u201d. Our experiments\nindicated that in limited labeled data settings with 1% and 10% labeled data,\nthe joint learning with multi-modal and self-supervised models outperforms self-supervised\nlearning and is at par with multi-modal learning. Additionally, we found that multi-modal\nlearning is generally more robust on out-of-distribution datasets. "}}
{"id": "lJylJ2195Q2", "cdate": 1546300800000, "mdate": null, "content": {"title": "Generative Image Translation for Data Augmentation of Bone Lesion Pathology", "abstract": "Insufficient training data and severe class imbalance are often limiting factors when developing machine learning models for the classification of rare diseases. In this work, we address the problem of classifying bone lesions from X-ray images by increasing the small number of positive samples in the training set. We propose a generative data augmentation approach based on a cycle-consistent generative adversarial network that synthesizes bone lesions on images without pathology. We pose the generative task as an image-patch translation problem that we optimize specifically for distinct bones (humerus, tibia, femur). In experimental results, we confirm that the described method mitigates the class imbalance problem in the binary classification task of bone lesion detection. We show that the augmented training sets enable the training of superior classifiers achieving better performance on a held-out test set. Additionally, we demonstrate the feasibility of transfer learning and apply a generative model that was trained on one body part to another."}}
{"id": "XyV-hJDK4oH", "cdate": 1546300800000, "mdate": null, "content": {"title": "Generative Image Translation for Data Augmentation of Bone Lesion Pathology", "abstract": "Insufficient training data and severe class imbalance are often limiting factors when developing machine learning models for the classification of rare diseases. In this work, we address the proble..."}}
{"id": "S1Z3rJZObr", "cdate": 1514764800000, "mdate": null, "content": {"title": "StarSpace: Embed All The Things!", "abstract": "We present StarSpace, a general-purpose neural embedding model that can solve a wide variety of problems: labeling tasks such as text classification, ranking tasks such as information retrieval/web search, collaborative filtering-based or content-based recommendation, embedding of multi-relational graphs, and learning word, sentence or document level embeddings. In each case the model works by embedding those entities comprised of discrete features and comparing them against each other -- learning similarities dependent on the task. Empirical results on a number of tasks show that StarSpace is highly competitive with existing methods, whilst also being generally applicable to new cases where those methods are not."}}
{"id": "OIqMFST3FZ9x", "cdate": 1514764800000, "mdate": null, "content": {"title": "Deep neural network improves fracture detection by clinicians", "abstract": "Historically, computer-assisted detection (CAD) in radiology has failed to achieve improvements in diagnostic accuracy, decreasing clinician sensitivity and leading to unnecessary further diagnostic tests. With the advent of deep learning approaches to CAD, there is great excitement about its application to medicine, yet there is little evidence demonstrating improved diagnostic accuracy in clinically-relevant applications. We trained a deep learning model to detect fractures on radiographs with a diagnostic accuracy similar to that of senior subspecialized orthopedic surgeons. We demonstrate that when emergency medicine clinicians are provided with the assistance of the trained model, their ability to accurately detect fractures significantly improves."}}
{"id": "ryVo2QZOWS", "cdate": 1451606400000, "mdate": null, "content": {"title": "Abstractive Sentence Summarization with Attentive Recurrent Neural Networks", "abstract": ""}}
{"id": "7TuMjGDzGLl", "cdate": 1451606400000, "mdate": null, "content": {"title": "Learning Through Dialogue Interactions", "abstract": "A good dialogue agent should have the ability to interact with users by both responding to questions and by asking questions, and importantly to learn from both types of interaction. In this work, we explore this direction by designing a simulator and a set of synthetic tasks in the movie domain that allow such interactions between a learner and a teacher. We investigate how a learner can benefit from asking questions in both offline and online reinforcement learning settings, and demonstrate that the learner improves when asking questions. Finally, real experiments with Mechanical Turk validate the approach. Our work represents a first step in developing such end-to-end learned interactive dialogue agents."}}
{"id": "SkbCnbz_bS", "cdate": 1420070400000, "mdate": null, "content": {"title": "A Neural Attention Model for Abstractive Sentence Summarization", "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines."}}
{"id": "r1ZMaWzdWr", "cdate": 1388534400000, "mdate": null, "content": {"title": "Question Answering with Subgraph Embeddings", "abstract": "This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few hand-crafted features. Our model learns low-dimensional embeddings of words and knowledge base constituents; these representations are used to score natural language questions against candidate answers. Training our system using pairs of questions and structured representations of their answers, and pairs of question paraphrases, yields competitive results on a recent benchmark of the literature."}}
