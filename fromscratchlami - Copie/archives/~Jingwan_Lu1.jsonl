{"id": "jZ_mVcEk04C", "cdate": 1668187511129, "mdate": 1668187511129, "content": {"title": "Collaging Class-specific GANs for Semantic Image Synthesis", "abstract": "We propose a new approach for high resolution semantic image synthesis. It consists of one base image generator and multiple class-specific generators. The base generator generates high quality images based on a segmentation map. To further improve the quality of different objects, we create a bank of Generative Adversarial Networks (GANs) by separately training class-specific models. This has several benefits including--dedicated weights for each class; centrally aligned data for each model; additional training data from other sources, potential of higher resolution and quality; and easy manipulation of a specific object in the scene. Experiments show that our approach can generate high quality images in high resolution while having flexibility of object-level control by using class-specific generators."}}
{"id": "BMpY8evOwh", "cdate": 1668021774402, "mdate": 1668021774402, "content": {"title": "Spatially-Adaptive Multilayer Selection for GAN Inversion and Editing", "abstract": "Existing GAN inversion and editing methods work well for aligned objects with a clean background, such as portraits and animal faces, but often struggle for more difficult categories with complex scene layouts and object occlusions, such as cars, animals, and outdoor images. We propose a new method to invert and edit such complex images in the latent space of GANs, such as StyleGAN2. Our key idea is to explore inversion with a collection of layers, spatially adapting the inversion process to the difficulty of the image. We learn to predict the \"invertibility\" of different image segments and project each segment into a latent layer. Easier regions can be inverted into an earlier layer in the generator's latent space, while more challenging regions can be inverted into a later feature space. Experiments show that our method obtains better inversion results compared to the recent approaches on complex categories, while maintaining downstream editability"}}
{"id": "pOW5pkR6rk", "cdate": 1667797672474, "mdate": 1667797672474, "content": {"title": "StylePortraitVideo: Editing Portrait Videos with Expression Optimization", "abstract": "High-quality portrait image editing has been made easier by recent advances in GANs (e.g., StyleGAN) and GAN inversion methods that project images onto a pre-trained GAN's latent space. However, extending the existing image editing methods, it is hard to edit videos to produce temporally coherent and natural-looking videos. We find challenges in reproducing diverse video frames and preserving the natural motion after editing. In this work, we propose solutions for these challenges. First, we propose a video adaptation method that enables the generator to reconstruct the original input identity, unusual poses, and expressions in the video. Second, we propose an expression dynamics optimization that tweaks the latent codes to maintain the meaningful motion in the original video. Based on these methods, we build a StyleGAN-based high-quality portrait video editing system that can edit videos in the wild in a temporally coherent way at up to 4K resolution."}}
{"id": "OTLyDvp77q", "cdate": 1667334430033, "mdate": 1667334430033, "content": {"title": "CM-GAN: Image Inpainting with Cascaded Modulation GAN and Object-Aware Training", "abstract": "Recent image inpainting methods have made great progress but often struggle to generate plausible image structures when dealing with large holes in complex images. This is partially due to the lack of effective network structures that can capture both the long-range dependency and high-level semantics of an image. To address these problems, we propose cascaded modulation GAN (CM-GAN), a new network design consisting of an encoder with Fourier convolution blocks that extract multi-scale feature representations from the input image with holes and a StyleGAN-like decoder with a novel cascaded global-spatial modulation block at each scale level. In each decoder block, global modulation is first applied to perform coarse semantic-aware structure synthesis, then spatial modulation is applied on the output of global modulation to further adjust the feature map in a spatially adaptive fashion. In addition, we design an object-aware training scheme to prevent the network from hallucinating new objects inside holes, fulfilling the needs of object removal tasks in real-world scenarios. Extensive experiments are conducted to show that our method significantly outperforms existing methods in both quantitative and qualitative evaluation."}}
{"id": "wF_fuvozD-d", "cdate": 1667334272927, "mdate": 1667334272927, "content": {"title": "Semantic Layout Manipulation with High-resolution Sparse Attention", "abstract": "We tackle the problem of semantic image layout manipulation, which aims to manipulate an input image by editing its semantic label map. A core problem of this task is how to transfer visual details from the input images to the new semantic layout while making the resulting image visually realistic. Recent work on learning cross-domain correspondence has shown promising results for global layout transfer with dense attention-based warping. However, this method tends to lose texture details due to the resolution limitation and the lack of smoothness constraint on correspondence. To adapt this paradigm for the layout manipulation task, we propose a high-resolution sparse attention module that effectively transfers visual details to new layouts at a resolution up to 512x512. To further improve visual quality, we introduce a novel generator architecture consisting of a semantic encoder and a two-stage decoder for coarse-to-fine synthesis. Experiments on the ADE20k and Places365 datasets demonstrate that our proposed approach achieves substantial improvements over the existing inpainting and layout manipulation methods."}}
{"id": "9mk3Ao7e0m", "cdate": 1665508930792, "mdate": 1665508930792, "content": {"title": "Swapping Autoencoder for Deep Image Manipulation", "abstract": "Deep generative models have become increasingly effective at producing realistic images from randomly sampled seeds, but using such models for controllable manipulation of existing images remains challenging. We propose the Swapping Autoencoder, a deep model designed specifically for image manipulation, rather than random sampling. The key idea is to encode an image with two independent components and enforce that any swapped combination maps to a realistic image. In particular, we encourage the components to represent structure and texture, by enforcing one component to encode co-occurrent patch statistics across different parts of an image. As our method is trained with an encoder, finding the latent codes for a new input image becomes trivial, rather than cumbersome. As a result, it can be used to manipulate real input images in various ways, including texture swapping, local and global editing, and latent code vector arithmetic. Experiments on multiple datasets show that our model produces better results and is substantially more efficient compared to recent generative models."}}
{"id": "B9W7BV6fRC", "cdate": 1664816290251, "mdate": null, "content": {"title": "Contrastive Learning on Synthetic Videos for GAN Latent Disentangling", "abstract": "In this paper, we present a method to disentangle appearance and structural information in the latent space of StyleGAN. We train an autoencoder whose encoder extracts appearance and structural features from an input latent code and then reconstructs the original input using the decoder. To train this network, We propose a video-based latent contrastive learning framework. With the observation that the appearance of a face does not change within a short video, the encoder learns to pull appearance representations of various video frames together while pushing appearance representations of different faces apart. Similarly, the structural representations of augmented versions of the same frame are pulled together, while the representation across different frames are pushed apart. As face video datasets lack sufficient number of unique identities, we propose a method to synthetically generate videos. This allows our disentangling network to observe a larger variation of appearances, expressions, and poses during training. We evaluate our approach on the tasks of expression transfer in images and motion transfer in videos."}}
{"id": "T9iojz-kOfU", "cdate": 1663850381228, "mdate": null, "content": {"title": "Panoptically guided Image Inpainting with Image-level and Object-level Semantic Discriminators", "abstract": "Recent image inpainting methods have made great progress. However, the existing approaches often struggle to hallucinate realistic object instances in natural scenes. Such a limitation is partially due to the lack of semantic-level constraints inside the hole as well as the lack of a mechanism to enforce the realism of local objects. To tackle the challenging object inpainting task, we propose a new panoptically guided image inpainting task that leverages a panoptic segmentation map to guide the completion of object instances. To enforce the realism of the generated objects, we propose a semantic discriminator that leverages pretrained visual features to improve the generated semantics. Furthermore, we propose object-level discriminators that take aligned instances as input to enforce the realism of individual objects. Experiments on the large-scale Places2 dataset demonstrate the significant improvement by our method on object completion, verified in both quantitative and qualitative evaluation. Furthermore, our framework is flexible and can be generalized to other inpainting tasks including segmentation-guided inpainting, edge-guided inpainting, as well as standard image inpainting without guidance. Consequently, our approach achieves new state-of-the-art performance on the various inpainting tasks and impressive results on object completion. "}}
{"id": "NxS-AQCSCUO", "cdate": 1608502917740, "mdate": null, "content": {"title": "Multiplanes: Assisted Freehand VR Sketching", "abstract": "The  presence  of  a  third  dimension makes  accurate  drawing  in virtual  reality  (VR)  more challenging than  2D  drawing. These challenges include higher demands on spatial cognition and motor skills,  as  well  as  the  potential  for  mistakes  caused  by  depth perception errors. We present Multiplanes, a VR drawing system that  supports  both  the  flexibility  of  freehand  drawing  and  the ability to draw accurate shapes in 3D by affording both planar and beautified  drawing.The  system was  designed  to  address  the above-mentioned   challenges. Multiplanes generates   snapping planes  and  beautification  trigger  points  based  on  previous  and current   strokes   and   the   current   controller   pose. Based   on geometrical   relationships   to   previous   strokes,beautification trigger points serve to guide the user to reach specific positions in space. The system also beautifies user\u2019s strokes based on the most probable  intended  shape  while  the  user  is  drawing  them. With Multiplanes,  in  contrast  to  other  systems,  users  do  not  need  to manually  activate  such  guides,  allowing  them to  focus  on  the creative process"}}
{"id": "C3A-7Q8Yw", "cdate": 1580440280824, "mdate": null, "content": {"title": "ON THE CONTINUITY OF ROTATION REPRESENTATIONS IN NEURAL NETWORKS ", "abstract": "In neural networks, it is often desirable to work with various representations of the same space. For example, 3D rotations can be represented with quaternions or Euler angles. In this paper, we advance a definition of a continuous representation, which can be helpful for training deep neural networks. We relate this to topological concepts such as homeomorphism and embedding. We then investigate what are continuous and discontinuous representations for 2D, 3D, and n-dimensional rotations. We demonstrate that for 3D rotations, all representations are discontinuous in the real Euclidean spaces of four or fewer dimensions. Thus, widely used representations such as quaternions and Euler angles are discontinuous and difficult for neural networks to learn. We show that the 3D rotations have continuous representations in 5D and 6D, which are more suitable for learning. We also present continuous representations for the general case of the n-dimensional rotation group SO(n). While our main focus is on rotations, we also show that our constructions apply to other groups such as the orthogonal group and similarity transforms. We finally present empirical results, which show that our continuous rotation representations outperform discontinuous ones for several practical problems in graphics and vision, including a simple autoencoder sanity test, a rotation estimator for 3D point clouds, and an inverse kinematics solver for 3D human poses. "}}
