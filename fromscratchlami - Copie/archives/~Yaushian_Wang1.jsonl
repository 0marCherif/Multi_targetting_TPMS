{"id": "Yis34V11U2G", "cdate": 1672531200000, "mdate": 1683909913175, "content": {"title": "Generation-driven Contrastive Self-training for Zero-shot Text Classification with Instruction-tuned GPT", "abstract": "Moreover, GPT-based zero-shot classification models tend to make independent predictions over test instances, which can be sub-optimal as the instance correlations and the decision boundaries in the target space are ignored. To address these difficulties and limitations, we propose a new approach to zero-shot text classification, namely \\ourmodelshort, which leverages the strong generative power of GPT to assist in training a smaller, more adaptable, and efficient sentence encoder classifier with contrastive self-training. Specifically, GenCo applies GPT in two ways: firstly, it generates multiple augmented texts for each input instance to enhance the semantic embedding of the instance and improve the mapping to relevant labels; secondly, it generates augmented texts conditioned on the predicted label during self-training, which makes the generative process tailored to the decision boundaries in the target space. In our experiments, GenCo outperforms previous state-of-the-art methods on multiple benchmark datasets, even when only limited in-domain text data is available."}}
{"id": "L_WLqn9LU6o", "cdate": 1672531200000, "mdate": 1683909913291, "content": {"title": "Long-tailed Extreme Multi-label Text Classification by the Retrieval of Generated Pseudo Label Descriptions", "abstract": ""}}
{"id": "HyFRwYUBj0", "cdate": 1672531200000, "mdate": 1687832171703, "content": {"title": "PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification", "abstract": "We present PESCO, a novel contrastive learning framework that substantially improves the performance of zero-shot text classification. We formulate text classification as a neural text matching problem where each document is treated as a query, and the system learns the mapping from each query to the relevant class labels by (1) adding prompts to enhance label matching, and (2) using retrieved labels to enrich the training set in a self-training loop of contrastive learning. PESCO achieves state-of-the-art performance on four benchmark text classification datasets. On DBpedia, we achieve 98.5\\% accuracy without any labeled data, which is close to the fully-supervised result. Extensive experiments and analyses show all the components of PESCO are necessary for improving the performance of zero-shot text classification."}}
{"id": "773yb3buMt", "cdate": 1672531200000, "mdate": 1694426075019, "content": {"title": "PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification", "abstract": ""}}
{"id": "pbwn8-G1Wr", "cdate": 1640995200000, "mdate": 1680214722161, "content": {"title": "Long-tailed Extreme Multi-label Text Classification with Generated Pseudo Label Descriptions", "abstract": ""}}
{"id": "hsdSFSD_JW3", "cdate": 1640995200000, "mdate": 1680214722160, "content": {"title": "Exploiting Local and Global Features in Transformer-based Extreme Multi-label Text Classification", "abstract": ""}}
{"id": "MghLSMwmaS", "cdate": 1640995200000, "mdate": 1694426074826, "content": {"title": "English Contrastive Learning Can Learn Universal Cross-lingual Sentence Embeddings", "abstract": "Universal cross-lingual sentence embeddings map semantically similar cross-lingual sentences into a shared embedding space. Aligning cross-lingual sentence embeddings usually requires supervised cross-lingual parallel sentences. In this work, we propose mSimCSE, which extends SimCSE to multilingual settings and reveal that contrastive learning on English data can surprisingly learn high-quality universal cross-lingual sentence embeddings without any parallel data. In unsupervised and weakly supervised settings, mSimCSE significantly improves previous sentence embedding methods on cross-lingual retrieval and multilingual STS tasks. The performance of unsupervised mSimCSE is comparable to fully supervised methods in retrieving low-resource languages and multilingual STS. The performance can be further enhanced when cross-lingual NLI data is available. Our code is publicly available at https://github.com/yaushian/mSimCSE."}}
{"id": "JKYPTy9flc", "cdate": 1640995200000, "mdate": 1680214722171, "content": {"title": "English Contrastive Learning Can Learn Universal Cross-lingual Sentence Embeddings", "abstract": ""}}
{"id": "31B6lfFsmg", "cdate": 1640995200000, "mdate": 1680214722165, "content": {"title": "Toxicity Detection with Generative Prompt-based Inference", "abstract": ""}}
{"id": "Hyus1XV9AKa", "cdate": 1609459200000, "mdate": 1680214722148, "content": {"title": "Are you doing what I say? On modalities alignment in ALFRED", "abstract": ""}}
