{"id": "xndYVgUPs1", "cdate": 1672531200000, "mdate": 1695252233662, "content": {"title": "Communication Efficient Distributed Newton Method with Fast Convergence Rates", "abstract": "We propose a communication and computation efficient second-order method for distributed optimization. For each iteration, our method only requires O (d) communication complexity, where d is the problem dimension. We also provide theoretical analysis to show the proposed method has the similar convergence rate as the classical second-order optimization algorithms. Concretely, our method can find (\u2208, \u221adLe,)-second-order stationary points for nonconvex problem by O (\u221adL,\u2208-3/2) iterations, where L is the Lipschitz constant of Hessian. Moreover, it enjoys a local superlinear convergence under the strongly-convex assumption. Experiments on both convex and nonconvex problems show that our proposed method performs significantly better than baselines."}}
{"id": "pELM0QgWIjn", "cdate": 1652737502902, "mdate": null, "content": {"title": "Quasi-Newton Methods for Saddle Point Problems", "abstract": "This paper studies quasi-Newton methods for strongly-convex-strongly-concave  saddle point problems. \nWe propose random Broyden family updates, which have explicit local superlinear convergence rate of ${\\mathcal O}\\big(\\big(1-1/(d\\varkappa^2)\\big)^{k(k-1)/2}\\big)$, where $d$ is the dimension of the problem, $\\varkappa$ is the condition number and $k$ is the number of iterations. The design and analysis of proposed algorithm are based on estimating the square of indefinite Hessian matrix, which is different from classical quasi-Newton methods in convex optimization. We also present two specific Broyden family algorithms with BFGS-type and SR1-type updates, which enjoy the faster local convergence rate of $\\mathcal O\\big(\\big(1-1/d\\big)^{k(k-1)/2}\\big)$. Our numerical experiments show proposed algorithms outperform classical first-order methods."}}
{"id": "p2MD8gvXgY", "cdate": 1640995200000, "mdate": 1683948852262, "content": {"title": "Quasi-Newton Methods for Saddle Point Problems", "abstract": "This paper studies quasi-Newton methods for strongly-convex-strongly-concave saddle point problems. We propose random Broyden family updates, which have explicit local superlinear convergence rate of ${\\mathcal O}\\big(\\big(1-1/(d\\varkappa^2)\\big)^{k(k-1)/2}\\big)$, where $d$ is the dimension of the problem, $\\varkappa$ is the condition number and $k$ is the number of iterations. The design and analysis of proposed algorithm are based on estimating the square of indefinite Hessian matrix, which is different from classical quasi-Newton methods in convex optimization. We also present two specific Broyden family algorithms with BFGS-type and SR1-type updates, which enjoy the faster local convergence rate of $\\mathcal O\\big(\\big(1-1/d\\big)^{k(k-1)/2}\\big)$. Our numerical experiments show proposed algorithms outperform classical first-order methods."}}
{"id": "9k4_W0Hp8_", "cdate": 1640995200000, "mdate": 1674734802136, "content": {"title": "Partial-Quasi-Newton Methods: Efficient Algorithms for Minimax Optimization Problems with Unbalanced Dimensionality", "abstract": "This paper studies the strongly-convex-strongly-concave minimax optimization with unbalanced dimensionality. Such problems contain several popular applications in data science such as few shot learning and fairness-aware machine learning task. The design of conventional iterative algorithm for minimax optimization typically focuses on reducing the total number of oracle calls, which ignores the unbalanced computational cost for accessing the information from two different variables in minimax. We propose a novel second-order optimization algorithm, called Partial-Quasi-Newton (PQN) method, which takes the advantage of unbalanced structure in the problem to establish the Hessian estimate efficiently. We theoretically prove our PQN method converges to the saddle point faster than existing minimax optimization algorithms. The numerical experiments on real-world applications show the proposed PQN performs significantly better than the state-of-the-art methods."}}
