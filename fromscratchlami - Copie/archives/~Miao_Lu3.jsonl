{"id": "PbkBDQ5_UbV", "cdate": 1663849945386, "mdate": null, "content": {"title": "Pessimism in the Face of Confounders: Provably Efficient Offline Reinforcement Learning in Partially Observable Markov Decision Processes", "abstract": "We study offline reinforcement learning (RL) in partially observable Markov decision processes. In particular, we aim to learn an optimal policy from a dataset collected by a behavior policy which possibly depends on the latent state. Such a dataset is confounded in the sense that the latent state simultaneously affects the action and the observation, which is prohibitive for existing offline RL algorithms. To this end, we propose the \\underline{P}roxy variable \\underline{P}essimistic \\underline{P}olicy \\underline{O}ptimization (\\texttt{P3O}) algorithm, which addresses the confounding bias and the distributional shift between the optimal and behavior policies in the context of general function approximation. At the core of \\texttt{P3O} is a coupled sequence of pessimistic confidence regions  constructed via proximal causal inference, which is  formulated as  minimax estimation. Under a partial coverage assumption on the confounded dataset, we prove that \\texttt{P3O} achieves a $n^{-1/2}$-suboptimality, where $n$ is the number of trajectories in the dataset. To our best knowledge, \\texttt{P3O} is the first provably efficient offline RL algorithm for POMDPs with a confounded dataset."}}
{"id": "WrqWxYJjK3", "cdate": 1640995200000, "mdate": 1681570440171, "content": {"title": "Learning Robust Policy against Disturbance in Transition Dynamics via State-Conservative Policy Optimization", "abstract": ""}}
{"id": "QbhWWszMTw3", "cdate": 1640995200000, "mdate": 1682319264823, "content": {"title": "Welfare Maximization in Competitive Equilibrium: Reinforcement Learning for Markov Exchange Economy", "abstract": "We study a bilevel economic system, which we refer to as a <em>Markov exchange economy</em> (MEE), from the point of view of multi-agent reinforcement learning (MARL). An MEE involves a central pla..."}}
{"id": "O1DEtITim__", "cdate": 1632875530791, "mdate": null, "content": {"title": "Learning Pruning-Friendly Networks via Frank-Wolfe: One-Shot, Any-Sparsity, And No Retraining", "abstract": "We present a novel framework to train a large deep neural network (DNN) for only $\\textit{once}$, which can then be pruned to $\\textit{any sparsity ratio}$ to preserve competitive accuracy $\\textit{without any re-training}$. Conventional methods often require (iterative) pruning followed by re-training, which not only incurs large overhead beyond the original DNN training but also can be sensitive to retraining hyperparameters. Our core idea is to re-cast the DNN training as an explicit $\\textit{pruning-aware}$ process: that is formulated with an auxiliary $K$-sparse polytope constraint, to encourage network weights to lie in a convex hull spanned by $K$-sparse vectors, potentially resulting in more sparse weight matrices. We then leverage a stochastic Frank-Wolfe (SFW) algorithm to solve this new constrained optimization, which naturally leads to sparse weight updates each time. We further note an overlooked fact that existing DNN initializations were derived to enhance SGD training (e.g., avoid gradient explosion or collapse), but was unaligned with the challenges of training with SFW. We hence also present the first learning-based initialization scheme specifically for boosting SFW-based DNN training. Experiments on CIFAR-10 and Tiny-ImageNet datasets demonstrate that our new framework named $\\textbf{SFW-pruning}$ consistently achieves the state-of-the-art performance on various benchmark DNNs over a wide range of pruning ratios. Moreover, SFW-pruning only needs to train once on the same model and dataset, for obtaining arbitrary ratios, while requiring neither iterative pruning nor retraining. All codes will be released to the public. "}}
