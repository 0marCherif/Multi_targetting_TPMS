{"id": "77pEfwI5Nxg", "cdate": 1668714422196, "mdate": 1668714422196, "content": {"title": "Adversarial example detection using latent neighborhood graph", "abstract": "Detection of adversarial examples with high accuracy is critical for the security of deployed deep neural network-based models. We present the first graph-based adversarial detection method that constructs a Latent Neighborhood Graph (LNG) around an input example to determine if the input example is adversarial. Given an input example, selected reference adversarial and benign examples (represented as LNG nodes in Figure 1) are used to capture the local manifold in the vicinity of the input example. The LNG node connectivity parameters are optimized jointly with the parameters of a graph attention network in an end-to-end manner to determine the optimal graph topology for adversarial example detection. The graph attention network is used to determine if the LNG is derived from an adversarial or benign input example. Experimental evaluations on CIFAR-10, STL-10, and ImageNet datasets, using six adversarial attack methods, demonstrate that the proposed method outperforms state-of-the-art adversarial detection methods in white-box and gray-box settings. The proposed method is able to successfully detect adversarial examples crafted with small perturbations using unseen attacks."}}
{"id": "nXCfFTzobLA", "cdate": 1652914267203, "mdate": 1652914267203, "content": {"title": "An Investigation of Data Poisoning Defenses for Online Learning", "abstract": "Data poisoning attacks -- where an adversary can modify a small fraction of training data, with the goal of forcing the trained classifier to high loss -- are an important threat for machine learning in many applications. While a body of prior work has developed attacks and defenses, there is not much general understanding on when various attacks and defenses are effective. In this work, we undertake a rigorous study of defenses against data poisoning for online learning. First, we study four standard defenses in a powerful threat model, and provide conditions under which they can allow or resist rapid poisoning. We then consider a weaker and more realistic threat model, and show that the success of the adversary in the presence of data poisoning defenses there depends on the \"ease\" of the learning problem."}}
{"id": "eWaf_ZMi5hU", "cdate": 1652914143175, "mdate": null, "content": {"title": "Data Poisoning Attack against Online Learning", "abstract": "We consider data poisoning attacks, a class of adversarial attacks on machine learning where an adversary has the power to alter a small fraction of the training data in order to make the trained classifier satisfy certain objectives. While there has been much prior work on data poisoning, most of it is in the offline setting, and attacks for online learning, where training data arrives in a streaming manner, are not well understood. In this work, we initiate a systematic investigation of data poisoning attacks for online learning. We formalize the problem into two settings, and we propose a general attack strategy, formulated as an optimization problem, that applies to both with some modifications. We propose three solution strategies, and perform extensive experimental evaluation. Finally, we discuss the implications of our findings for building successful defenses."}}
{"id": "WBp4dli3No6", "cdate": 1652737596591, "mdate": null, "content": {"title": "Robust Learning against Relational Adversaries", "abstract": "Test-time adversarial attacks have posed serious challenges to the robustness of machine-learning models, and in many settings the adversarial perturbation need not be bounded by small $\\ell_p$-norms. Motivated by attacks in program analysis and security tasks, we investigate $\\textit{relational adversaries}$, a broad class of attackers who create adversarial examples in a reflexive-transitive closure of a logical relation. We analyze the conditions for robustness against relational adversaries and investigate different levels of robustness-accuracy trade-off due to various patterns in a relation. Inspired by the insights, we propose $\\textit{normalize-and-predict}$, a learning framework that leverages input normalization to achieve provable robustness. The framework solves the pain points of adversarial training against relational adversaries and can be combined with adversarial training for the benefits of both approaches. Guided by our theoretical findings, we apply our framework to source code authorship attribution and malware detection. Results of both tasks show our learning framework significantly improves the robustness of models against relational adversaries. In the process, it outperforms adversarial training, the most noteworthy defense mechanism, by a wide margin."}}
{"id": "XtPeiGx6BwC", "cdate": 1601308026249, "mdate": null, "content": {"title": "Robustness against Relational Adversary", "abstract": "Test-time adversarial attacks have posed serious challenges to the robustness of machine-learning models, and in many settings the adversarial perturbation need not be bounded by small lp-norms.  Motivated by the semantics-preserving attacks in vision and security domain, we investigate relational adversaries, a broad class of attackers who create adversarial examples that are in a re\ufb02exive-transitive closure of a logical relation. We analyze the conditions for robustness and propose normalize-and-predict \u2013 a learning framework with provable robustness guarantee. We compare our approach with adversarial training and derive an uni\ufb01ed framework that provides bene\ufb01ts of both approaches. Guided by our theoretical \ufb01ndings, we apply our framework to malware detection and image classi\ufb01cation. Results of both tasks show that attacks using relational adversaries frequently fool existing models, but our uni\ufb01ed framework can signi\ufb01cantly enhance their robustness.\n"}}
{"id": "HyZ1Mnb_WH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Analyzing the Robustness of Nearest Neighbors to Adversarial Examples", "abstract": "Motivated by safety-critical applications, test-time attacks on classifiers via adversarial examples has recently received a great deal of attention. However, there is a general lack of understandi..."}}
