{"id": "MlNzmdm9jz", "cdate": 1672531200000, "mdate": 1681665102311, "content": {"title": "SLaM: Student-Label Mixing for Semi-Supervised Knowledge Distillation", "abstract": "Knowledge distillation with unlabeled examples is a powerful training paradigm for generating compact and lightweight student models in applications where the amount of labeled data is limited but one has access to a large pool of unlabeled data. In this setting, a large teacher model generates ``soft'' pseudo-labels for the unlabeled dataset which are then used for training the student model. Despite its success in a wide variety of applications, a shortcoming of this approach is that the teacher's pseudo-labels are often noisy, leading to impaired student performance. In this paper, we present a principled method for knowledge distillation with unlabeled examples that we call Student-Label Mixing (SLaM) and we show that it consistently improves over prior approaches by evaluating it on several standard benchmarks. Finally, we show that SLaM comes with theoretical guarantees; along the way we give an algorithm improving the best-known sample complexity for learning halfspaces with margin under random classification noise, and provide the first convergence analysis for so-called ``forward loss-adjustment\" methods."}}
{"id": "KcH5cEjnxP", "cdate": 1672531200000, "mdate": 1684173515233, "content": {"title": "LayerNAS: Neural Architecture Search in Polynomial Complexity", "abstract": "Neural Architecture Search (NAS) has become a popular method for discovering effective model architectures, especially for target hardware. As such, NAS methods that find optimal architectures under constraints are essential. In our paper, we propose LayerNAS to address the challenge of multi-objective NAS by transforming it into a combinatorial optimization problem, which effectively constrains the search complexity to be polynomial. For a model architecture with $L$ layers, we perform layerwise-search for each layer, selecting from a set of search options $\\mathbb{S}$. LayerNAS groups model candidates based on one objective, such as model size or latency, and searches for the optimal model based on another objective, thereby splitting the cost and reward elements of the search. This approach limits the search complexity to $ O(H \\cdot |\\mathbb{S}| \\cdot L) $, where $H$ is a constant set in LayerNAS. Our experiments show that LayerNAS is able to consistently discover superior models across a variety of search spaces in comparison to strong baselines, including search spaces derived from NATS-Bench, MobileNetV2 and MobileNetV3."}}
{"id": "FT-LUdDlCHz", "cdate": 1672531200000, "mdate": 1696011473804, "content": {"title": "Robust Active Distillation", "abstract": ""}}
{"id": "ALDM5SN2r7M", "cdate": 1663849924177, "mdate": null, "content": {"title": "Robust Active Distillation", "abstract": "Distilling knowledge from a large teacher model to a lightweight one is a widely successful approach for generating compact, powerful models in the semi-supervised learning setting where a limited amount of labeled data is available. In large-scale applications, however, the teacher tends to provide a large number of incorrect soft-labels that impairs student performance. The sheer size of the teacher additionally constrains the number of soft-labels that can be queried due to prohibitive computational and/or financial costs. The difficulty in achieving simultaneous \\emph{efficiency} (i.e., minimizing soft-label queries) and \\emph{robustness} (i.e., avoiding student inaccuracies due to incorrect labels) hurts the widespread application of knowledge distillation to many modern tasks. In this paper, we present a parameter-free approach with provable guarantees to query the soft-labels of points that are simultaneously informative and correctly labeled by the teacher. At the core of our work lies a game-theoretic formulation that explicitly considers the inherent trade-off between the informativeness and correctness of input instances. We establish bounds on the expected performance of our approach that hold even in worst-case distillation instances. We present empirical evaluations on popular benchmarks that demonstrate the improved distillation performance enabled by our work relative to that of state-of-the-art active learning and active distillation methods."}}
{"id": "M34VHvEU4NZ", "cdate": 1652737639379, "mdate": null, "content": {"title": "Weighted Distillation with Unlabeled Examples", "abstract": "Distillation with unlabeled examples is a popular and powerful method for training deep neural networks in settings where the amount of labeled data is limited: A large \u201cteacher\u201d neural network is trained on the labeled data available, and then it is used to generate labels on an unlabeled dataset (typically much larger in size). These labels are then utilized to train the smaller \u201cstudent\u201d model which will actually be deployed. Naturally, the success of the approach depends on the quality of the teacher\u2019s labels, since the student could be confused if trained on inaccurate data. This paper proposes a principled approach for addressing this issue based on a \u201cdebiasing\" reweighting of the student\u2019s loss function tailored to the distillation training paradigm. Our method is hyper-parameter free, data-agnostic, and simple to implement. We demonstrate significant improvements on popular academic datasets and we accompany our results with a theoretical analysis which rigorously justifies the performance of our method in certain settings.\n"}}
{"id": "zAuujoiCog", "cdate": 1640995200000, "mdate": 1681665102317, "content": {"title": "Weighted Distillation with Unlabeled Examples", "abstract": "Distillation with unlabeled examples is a popular and powerful method for training deep neural networks in settings where the amount of labeled data is limited: A large ''teacher'' neural network is trained on the labeled data available, and then it is used to generate labels on an unlabeled dataset (typically much larger in size). These labels are then utilized to train the smaller ''student'' model which will actually be deployed. Naturally, the success of the approach depends on the quality of the teacher's labels, since the student could be confused if trained on inaccurate data. This paper proposes a principled approach for addressing this issue based on a ''debiasing'' reweighting of the student's loss function tailored to the distillation training paradigm. Our method is hyper-parameter free, data-agnostic, and simple to implement. We demonstrate significant improvements on popular academic datasets and we accompany our results with a theoretical analysis which rigorously justifies the performance of our method in certain settings."}}
{"id": "gqWIo1f2t2", "cdate": 1640995200000, "mdate": 1681665102317, "content": {"title": "Efficiently list-edge coloring multigraphs asymptotically optimally", "abstract": "We give polynomial time algorithms for the seminal results of Kahn, who showed that the Goldberg\u2013Seymour and list-coloring conjectures for (list-)edge coloring multigraphs hold asymptotically. Kahn's..."}}
{"id": "eonVPwUCDih", "cdate": 1640995200000, "mdate": 1683712378891, "content": {"title": "Weighted Distillation with Unlabeled Examples", "abstract": "Distillation with unlabeled examples is a popular and powerful method for training deep neural networks in settings where the amount of labeled data is limited: A large \u201cteacher\u201d neural network is trained on the labeled data available, and then it is used to generate labels on an unlabeled dataset (typically much larger in size). These labels are then utilized to train the smaller \u201cstudent\u201d model which will actually be deployed. Naturally, the success of the approach depends on the quality of the teacher\u2019s labels, since the student could be confused if trained on inaccurate data. This paper proposes a principled approach for addressing this issue based on a \u201cdebiasing\" reweighting of the student\u2019s loss function tailored to the distillation training paradigm. Our method is hyper-parameter free, data-agnostic, and simple to implement. We demonstrate significant improvements on popular academic datasets and we accompany our results with a theoretical analysis which rigorously justifies the performance of our method in certain settings."}}
{"id": "8B4fwRChbhE", "cdate": 1640995200000, "mdate": 1681665102313, "content": {"title": "Robust Active Distillation", "abstract": "Distilling knowledge from a large teacher model to a lightweight one is a widely successful approach for generating compact, powerful models in the semi-supervised learning setting where a limited amount of labeled data is available. In large-scale applications, however, the teacher tends to provide a large number of incorrect soft-labels that impairs student performance. The sheer size of the teacher additionally constrains the number of soft-labels that can be queried due to prohibitive computational and/or financial costs. The difficulty in achieving simultaneous \\emph{efficiency} (i.e., minimizing soft-label queries) and \\emph{robustness} (i.e., avoiding student inaccuracies due to incorrect labels) hurts the widespread application of knowledge distillation to many modern tasks. In this paper, we present a parameter-free approach with provable guarantees to query the soft-labels of points that are simultaneously informative and correctly labeled by the teacher. At the core of our work lies a game-theoretic formulation that explicitly considers the inherent trade-off between the informativeness and correctness of input instances. We establish bounds on the expected performance of our approach that hold even in worst-case distillation instances. We present empirical evaluations on popular benchmarks that demonstrate the improved distillation performance enabled by our work relative to that of state-of-the-art active learning and active distillation methods."}}
{"id": "sQbHsArZvx9", "cdate": 1609459200000, "mdate": 1681665102321, "content": {"title": "Improved Bounds for Coloring Locally Sparse Hypergraphs", "abstract": "We show that, for every k \u2265 2, every k-uniform hypergaph of degree \u0394 and girth at least 5 is efficiently (1+o(1))(k-1) (\u0394 / ln \u0394)^{1/(k-1)}-list colorable. As an application we obtain the currently best deterministic algorithm for list-coloring random hypergraphs of bounded average degree."}}
