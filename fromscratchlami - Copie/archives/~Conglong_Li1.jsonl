{"id": "hfGJklOnlB", "cdate": 1672531200000, "mdate": 1695948904167, "content": {"title": "Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam", "abstract": ""}}
{"id": "V7_kibffyWi", "cdate": 1672531200000, "mdate": 1695948904110, "content": {"title": "DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention", "abstract": "Most of the existing multi-modal models, hindered by their incapacity to adeptly manage interleaved image-and-text inputs in multi-image, multi-round dialogues, face substantial constraints in resource allocation for training and data accessibility, impacting their adaptability and scalability across varied interaction realms. To address this, we present the DeepSpeed-VisualChat framework, designed to optimize Large Language Models (LLMs) by incorporating multi-modal capabilities, with a focus on enhancing the proficiency of Large Vision and Language Models in handling interleaved inputs. Our framework is notable for (1) its open-source support for multi-round and multi-image dialogues, (2) introducing an innovative multi-modal causal attention mechanism, and (3) utilizing data blending techniques on existing datasets to assure seamless interactions in multi-round, multi-image conversations. Compared to existing frameworks, DeepSpeed-VisualChat shows superior scalability up to 70B parameter language model size, representing a significant advancement in multi-modal language models and setting a solid foundation for future explorations."}}
{"id": "-hjd3d77ZqW", "cdate": 1672531200000, "mdate": 1695948904169, "content": {"title": "DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales", "abstract": "ChatGPT-like models have revolutionized various applications in artificial intelligence, from summarization and coding to translation, matching or even surpassing human performance. However, the current landscape lacks an accessible, efficient, and cost-effective end-to-end RLHF (Reinforcement Learning with Human Feedback) training pipeline for these powerful models, particularly when training at the scale of billions of parameters. This paper introduces DeepSpeed-Chat, a novel system that democratizes RLHF training, making it accessible to the AI community. DeepSpeed-Chat offers three key capabilities: an easy-to-use training and inference experience for ChatGPT-like models, a DeepSpeed-RLHF pipeline that replicates the training pipeline from InstructGPT, and a robust DeepSpeed-RLHF system that combines various optimizations for training and inference in a unified way. The system delivers unparalleled efficiency and scalability, enabling training of models with hundreds of billions of parameters in record time and at a fraction of the cost. With this development, DeepSpeed-Chat paves the way for broader access to advanced RLHF training, even for data scientists with limited resources, thereby fostering innovation and further development in the field of AI."}}
{"id": "CPg5IRu9PL", "cdate": 1663850228677, "mdate": null, "content": {"title": "Efficient Large-scale Transformer Training via Random and Layerwise Token Dropping", "abstract": "Large-scale transformer models have become the de-facto architectures for various machine learning applications, e.g., CV and NLP. \nHowever, those large models also introduce prohibitive training costs. \nTo mitigate this issue, we propose a novel random and layerwise token dropping method (\\OURS), which skips the computation of a subset of the input tokens at all middle layers.\nParticularly, \\OURS achieves considerable speedups and comparable accuracy as the standard training baseline. \nCompared to other token dropping methods, \\OURS does not require (1) any importance score-based metrics, (2) any   special token treatment (e.g., \\texttt{[CLS]}), and (3) many layers in full sequence length training except the first and the last layers. \nBesides, a new \\layertoken learning rate schedule is proposed for pretraining problems that resolve the heavy tuning requirement for our proposed training mechanism. \nFinally, we demonstrate that \\OURS can be applied to broader applications, including \\gpt and \\bert pretraining as well as ViT and \\gpt finetuning tasks. \nOur results show that \\OURS can save about 33.3\\% theoretical compute cost and 25.6\\% wall-clock training time while achieving similar zero-shot evaluations on \\gptb as compared to baseline."}}
{"id": "HO2q49XYRC", "cdate": 1663850161930, "mdate": null, "content": {"title": "SaMoE: Parameter Efficient MoE Language Models via Self-Adaptive Expert Combination", "abstract": "Recently, Mixture-of-Experts (MoE) has demonstrated success in scaling models to have large amounts of parameters without significant increases in computational cost. However, MoEs have been also reported to be parameter inefficient such that larger models do not always lead to better performance.  \nIn this work, we study how to build parameter-efficient MoE models. Our analysis identifies that MoE layers exhibit poor gradient flow as the number of experts increases, leading to insufficient training of experts. To overcome this issue, we propose a new MoE architecture design (SaMoE), which improves the parameter efficiency of MoE models by learning a soft combination of a global set of expert layers for each MoE layer. Such a scheme enables substantial parameter savings on MoE while achieving comparable or better accuracy than the standard MoE training baseline. Extensive experiments on billion-scale GPT-3 style autoregressive MoE language models demonstrate that SaMoE significantly improves the parameter efficiency of MoE models by reducing up to 5.2X total parameters while obtaining superior pre-training and zero-shot generalization results as compared to baseline. "}}
{"id": "-CefY2EOupj", "cdate": 1663849920638, "mdate": null, "content": {"title": "Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam", "abstract": "1-bit gradient compression and local steps are two representative techniques that enable drastic communication reduction in distributed SGD. Their benefits, however, remain an open question on Adam-based large model pre-training (e.g. BERT and GPT). In this paper, we demonstrate the non-linearity in Adam causes slow convergence even when 1-bit compression or local steps are individually applied. To alleviate this limitation, we propose \\textbf{0/1 Adam} that linearizes each Adam step via approximating its optimizer states using their stale estimates and linear correlation. \\textbf{0/1 Adam} performs an Adam-like step to preserve the adaptivity, while its linearity allows utilizing 1-bit compression and local steps simultaneously for wall-clock time speed up. We provide convergence guarantee for \\textbf{0/1 Adam} on smooth non-convex objectives. On various large-scale benchmarks such as BERT-Base, BERT-Large, GPT-2 pre-training and ImageNet, we demonstrate on up to 128 GPUs that \\textbf{0/1 Adam} is able to reduce up to 87\\% of data volume, 54\\% of communication rounds, and achieve up to 2$\\times$ higher training throughput and end-to-end training time reduction compared to the state-of-the-art baseline 1-bit Adam; while enjoying the same statistical convergence speed and end task model accuracy on GLUE dataset and ImageNet validation set. "}}
{"id": "xNeAhc2CNAl", "cdate": 1652737656006, "mdate": null, "content": {"title": "XTC: Extreme Compression for Pre-trained Transformers Made Simple and Efficient", "abstract": "Extreme compression, particularly ultra-low bit precision (binary/ternary) quantization, has been proposed to fit large NLP models on resource-constraint devices. \nHowever, to preserve the accuracy for such aggressive compression schemes, cutting-edge methods usually introduce complicated compression pipelines, e.g., multi-stage expensive knowledge distillation with extensive hyperparameter tuning. \nAlso, they oftentimes focus less on smaller transformer models that have already been heavily compressed via knowledge distillation and lack a systematic study to show the effectiveness of their methods.\nIn this paper, we perform a very comprehensive systematic study to measure the impact of many key hyperparameters and training strategies from previous. \nAs a result, we find out that previous baselines for ultra-low bit precision quantization are significantly under-trained. \nBased on our study, we propose a simple yet effective compression pipeline for extreme compression. \nOur simplified pipeline demonstrates that\n(1) we can skip the pre-training knowledge distillation to obtain a 5-layer \\bert while achieving better performance than previous state-of-the-art methods, like TinyBERT; \n(2) extreme quantization plus layer reduction is able to reduce the model size by 50x, resulting in new state-of-the-art results on GLUE tasks."}}
{"id": "JpZ5du_Kdh", "cdate": 1652737640848, "mdate": null, "content": {"title": "The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models", "abstract": "Recent works have demonstrated great success in pre-training large-scale autoregressive language models (e.g., GPT-3) on massive GPUs. To reduce the wall-clock training time, a common practice is to increase the batch size and learning rate. However, such practice is often brittle and leads to a so-called stability-efficiency dilemma: increasing the batch sizes and learning rates leads to better training efficiency but can also result in training instability, leading to poor generalization accuracy or failed runs. To better understand this phenomenon, we conduct an in-depth analysis on large-scale pre-training experiments replicating the GPT-2 model with public dataset. We find that there is a strong correlation between training instability and extreme values of gradient variance. We further identify that samples with long sequence lengths contribute to these extreme gradient variance values, especially at the beginning of the training, indicating that long sequence length can be a main source of training instability.\n\nBased on the analysis, we present a simple yet effective Sequence Length Warmup method that aims to solve the training stability-efficiency dilemma by avoiding extreme gradient variance values. Moreover, we present a lightweight tuning strategy that allows us to tune our method with just a small portion of the expensive full training. Experiments replicating GPT-2 models (117M and 1.5B) show that our approach enables stable training with 8x larger batch size and 4x larger learning rate, whereas the baseline approach struggles with training instability. To achieve the same or better zero-shot evaluation results, our method reduces the required number of training tokens and wall clock time by up to 2.2x and 3.7x, respectively. Experiments replicating GPT-3 model (125M) show that our approach enables stable training with 8x larger batch size and 40x larger learning rate, and retains 99\\% of the zero-shot accuracy on 11 tasks using 10x less data and 17x less time compared to the original GPT-3 training recipe, while the baseline diverges under the same settings and only retain 95\\% of accuracy under lower learning rate."}}
{"id": "f-fVCElZ-G1", "cdate": 1652737484803, "mdate": null, "content": {"title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers", "abstract": "How to efficiently serve ever-larger trained natural language models in practice has become exceptionally challenging even for powerful cloud servers due to their prohibitive memory/computation requirements.\nIn this work, we present an efficient and affordable post-training quantization approach to compress large Transformer-based models, termed as \\OURS. \n\\OURS is an end-to-end quantization and inference pipeline with three main components: \n(1) a fine-grained hardware-friendly quantization scheme for both weight and activations; \n(2) a novel affordable layer-by-layer knowledge distillation algorithm (\\lwd) even without the original training data access;\n(3) a highly-optimized quantization system backend support to remove the quantization/dequantization overhead.\nAs such, we are able to show that:\n(1) \\OURS can reduce the precision for weight and activations to INT8 in a cost-free way for both \\bert and \\gpt-style \nmodels with minimal accuracy impact, which leads to up to 5.19x/4.16x speedup on \\bert/\\gpt-style models compared to FP16 inference, separately;\n(2) \\OURS plus \\lwd can affordably quantize the weights in the fully-connected module to INT4 along with INT8 weights in the attention module and INT8 activations, resulting in 3x memory footprint reduction compared to the FP16 model;\n(3) \\OURS can be directly applied to two of the largest open-sourced language models, including \\gptneox, for which our INT8 model achieves similar accuracy as the FP16 model but achieves 5.2x better efficiency.\nOur code is open-sourced at~\\cite{code_compression}."}}
{"id": "wDdQgOIR0E", "cdate": 1640995200000, "mdate": 1679901168333, "content": {"title": "DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing", "abstract": ""}}
