{"id": "lJFHU-p2wAB", "cdate": 1672531200000, "mdate": 1681670596977, "content": {"title": "Controllability-Aware Unsupervised Skill Discovery", "abstract": "One of the key capabilities of intelligent agents is the ability to discover useful skills without external supervision. However, the current unsupervised skill discovery methods are often limited to acquiring simple, easy-to-learn skills due to the lack of incentives to discover more complex, challenging behaviors. We introduce a novel unsupervised skill discovery method, Controllability-aware Skill Discovery (CSD), which actively seeks complex, hard-to-control skills without supervision. The key component of CSD is a controllability-aware distance function, which assigns larger values to state transitions that are harder to achieve with the current skills. Combined with distance-maximizing skill discovery, CSD progressively learns more challenging skills over the course of training as our jointly trained distance function reduces rewards for easy-to-achieve skills. Our experimental results in six robotic manipulation and locomotion environments demonstrate that CSD can discover diverse complex skills including object manipulation and locomotion skills with no supervision, significantly outperforming prior unsupervised skill discovery methods. Videos and code are available at https://seohong.me/projects/csd/"}}
{"id": "XoPRDE00E7", "cdate": 1672531200000, "mdate": 1681670597003, "content": {"title": "Predictable MDP Abstraction for Unsupervised Model-Based RL", "abstract": "A key component of model-based reinforcement learning (RL) is a dynamics model that predicts the outcomes of actions. Errors in this predictive model can degrade the performance of model-based controllers, and complex Markov decision processes (MDPs) can present exceptionally difficult prediction problems. To mitigate this issue, we propose predictable MDP abstraction (PMA): instead of training a predictive model on the original MDP, we train a model on a transformed MDP with a learned action space that only permits predictable, easy-to-model actions, while covering the original state-action space as much as possible. As a result, model learning becomes easier and more accurate, which allows robust, stable model-based planning or model-based RL. This transformation is learned in an unsupervised manner, before any task is specified by the user. Downstream tasks can then be solved with model-based control in a zero-shot fashion, without additional environment interactions. We theoretically analyze PMA and empirically demonstrate that PMA leads to significant improvements over prior unsupervised model-based RL approaches in a range of benchmark environments. Our code and videos are available at https://seohong.me/projects/pma/"}}
{"id": "sWNT5lT7l9G", "cdate": 1652737761990, "mdate": null, "content": {"title": "Constrained GPI for Zero-Shot Transfer in Reinforcement Learning", "abstract": "For zero-shot transfer in reinforcement learning where the reward function varies between different tasks, the successor features framework has been one of the popular approaches. However, in this framework, the transfer to new target tasks with generalized policy improvement (GPI) relies on only the source successor features [5] or additional successor features obtained from the function approximators\u2019 generalization to novel inputs [11]. The goal of this work is to improve the transfer by more tightly bounding the value approximation errors of successor features on the new target tasks. Given a set of source tasks with their successor features, we present lower and upper bounds on the optimal values for novel task vectors that are expressible as linear combinations of source task vectors. Based on the bounds, we propose constrained GPI as a simple test-time approach that can improve transfer by constraining action-value approximation errors on new target tasks. Through experiments in the Scavenger and Reacher environment with state observations as well as the DeepMind Lab environment with visual observations, we show that the proposed constrained GPI significantly outperforms the prior GPI\u2019s transfer performance. Our code and additional information are available at https://jaekyeom.github.io/projects/cgpi/."}}
{"id": "hzP-7_X-GM", "cdate": 1640995200000, "mdate": 1681652243141, "content": {"title": "Lipschitz-constrained Unsupervised Skill Discovery", "abstract": ""}}
{"id": "FSN6ITjvAF", "cdate": 1640995200000, "mdate": 1681652243157, "content": {"title": "Lipschitz-constrained Unsupervised Skill Discovery", "abstract": ""}}
{"id": "BGvt0ghNgA", "cdate": 1632875526414, "mdate": null, "content": {"title": "Lipschitz-constrained Unsupervised Skill Discovery", "abstract": "We study the problem of unsupervised skill discovery, whose goal is to learn a set of diverse and useful skills with no external reward. There have been a number of skill discovery methods based on maximizing the mutual information (MI) between skills and states. However, we point out that their MI objectives usually prefer static skills to dynamic ones, which may hinder the application for downstream tasks. To address this issue, we propose Lipschitz-constrained Skill Discovery (LSD), which encourages the agent to discover more diverse, dynamic, and far-reaching skills. Another benefit of LSD is that its learned representation function can be utilized for solving goal-following downstream tasks even in a zero-shot manner \u2014 i.e., without further training or complex planning. Through experiments on various MuJoCo robotic locomotion and manipulation environments, we demonstrate that LSD outperforms previous approaches in terms of skill diversity, state space coverage, and performance on seven downstream tasks including the challenging task of following multiple goals on Humanoid. Our code and videos are available at https://shpark.me/projects/lsd/."}}
{"id": "xNmhYNQruJX", "cdate": 1621629999389, "mdate": null, "content": {"title": "Time Discretization-Invariant Safe Action Repetition for Policy Gradient Methods", "abstract": "In reinforcement learning, continuous time is often discretized by a time scale $\\delta$, to which the resulting performance is known to be highly sensitive. In this work, we seek to find a $\\delta$-invariant algorithm for policy gradient (PG) methods, which performs well regardless of the value of $\\delta$. We first identify the underlying reasons that cause PG methods to fail as $\\delta \\to 0$, proving that the variance of the PG estimator can diverge to infinity in stochastic environments under a certain assumption of stochasticity. While durative actions or action repetition can be employed to have $\\delta$-invariance, previous action repetition methods cannot immediately react to unexpected situations in stochastic environments. We thus propose a novel $\\delta$-invariant method named Safe Action Repetition (SAR) applicable to any existing PG algorithm. SAR can handle the stochasticity of environments by adaptively reacting to changes in states during action repetition. We empirically show that our method is not only $\\delta$-invariant but also robust to stochasticity, outperforming previous $\\delta$-invariant approaches on eight MuJoCo environments with both deterministic and stochastic settings. Our code is available at https://vision.snu.ac.kr/projects/sar."}}
{"id": "J4JDaltdCx", "cdate": 1609459200000, "mdate": 1681652243314, "content": {"title": "Time Discretization-Invariant Safe Action Repetition for Policy Gradient Methods", "abstract": ""}}
{"id": "AKaQJPwBmk_", "cdate": 1609459200000, "mdate": 1629182074565, "content": {"title": "Unsupervised Skill Discovery with Bottleneck Option Learning", "abstract": "Having the ability to acquire inherent skills from environments without any external rewards or supervision like humans is an important problem. We propose a novel unsupervised skill discovery method named Information Bottleneck Option Learning (IBOL). On top of the linearization of environments that promotes more various and distant state transitions, IBOL enables the discovery of diverse skills. It provides the abstraction of the skills learned with the information bottleneck framework for the options with improved stability and encouraged disentanglement. We empirically demonstrate that IBOL outperforms multiple state-of-the-art unsupervised skill discovery methods on the information-theoretic evaluations and downstream tasks in MuJoCo environments, including Ant, HalfCheetah, Hopper and D'Kitty."}}
{"id": "30bKEjzNrA1", "cdate": 1609459200000, "mdate": 1681652243231, "content": {"title": "Time Discretization-Invariant Safe Action Repetition for Policy Gradient Methods", "abstract": ""}}
