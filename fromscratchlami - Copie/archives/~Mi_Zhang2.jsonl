{"id": "yA_Q3KMOyT", "cdate": 1680307200000, "mdate": 1695907868172, "content": {"title": "Slowing Down the Aging of Learning-Based Malware Detectors With API Knowledge", "abstract": "Learning-based malware detectors are widely used in practice to safeguard real-world computers. One major challenge is known as model aging, where the effectiveness of these models drops drastically as malware variants keep evolving. To tackle model aging, most existing works choose to label new samples to retrain the aged models. However, such data-perspective methods often require excessive costs in labeling and retraining. In this article, we observe that during evolution, malware samples often preserve similar malicious semantics while switching to new implementations with semantically equivalent APIs. Such observation enables us to look into the problem from a different perspective: feature space. More specifically, if the models can capture the intrinsic semantics of malware variants from feature space, it will help slow down the aging of learning-based detectors. Based on this insight, we design <sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">APIGraph</small> to automatically extract API knowledge from API documentation and incorporate these knowledge into the training of malware detection models. We use <sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">APIGraph</small> to enhance 5 state-of-the-art malware detectors, covering both Android and Windows platforms and various learning algorithms. Experiments on large-scale, evolutionary datasets with nearly 340K samples show that <sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">APIGraph</small> can help slow down the aging of these models by 5.9% to 19.6%, as well as reduce labeling efforts from 33.07% to 96.30% on top of data-perspective methods."}}
{"id": "wQ8T79TPxoY", "cdate": 1672531200000, "mdate": 1699170395476, "content": {"title": "Black-Box Adversarial Attack on Time Series Classification", "abstract": "With the increasing use of deep neural network (DNN) in time series classification (TSC), recent work reveals the threat of adversarial attack, where the adversary can construct adversarial examples to cause model mistakes. However, existing researches on the adversarial attack of TSC typically adopt an unrealistic white-box setting with model details transparent to the adversary. In this work, we study a more rigorous black-box setting with attack detection applied, which restricts gradient access and requires the adversarial example to be also stealthy. Theoretical analyses reveal that the key lies in: estimating black-box gradient with diversity and non-convexity of TSC models resolved, and restricting the l0 norm of the perturbation to construct adversarial samples. Towards this end, we propose a new framework named BlackTreeS, which solves the hard optimization issue for adversarial example construction with two simple yet effective modules. In particular, we propose a tree search strategy to find influential positions in a sequence, and independently estimate the black-box gradients for these positions. Extensive experiments on three real-world TSC datasets and five DNN based models validate the effectiveness of BlackTreeS, e.g., it improves the attack success rate from 19.3% to 27.3%, and decreases the detection success rate from 90.9% to 6.8% for LSTM on the UWave dataset."}}
{"id": "sRJgVgaMVrY", "cdate": 1672531200000, "mdate": 1699170395301, "content": {"title": "Exorcising \"Wraith\": Protecting LiDAR-based Object Detector in Automated Driving System from Appearing Attacks", "abstract": ""}}
{"id": "ggdQL8Nadi", "cdate": 1672531200000, "mdate": 1687841030849, "content": {"title": "Anti-FakeU: Defending Shilling Attacks on Graph Neural Network based Recommender Model", "abstract": ""}}
{"id": "d_e6A5_gMMG", "cdate": 1672531200000, "mdate": 1699170395477, "content": {"title": "SlowBERT: Slow-down Attacks on Input-adaptive Multi-exit BERT", "abstract": ""}}
{"id": "aEmgGLRoHh_", "cdate": 1672531200000, "mdate": 1699170396210, "content": {"title": "Cracking White-box DNN Watermarks via Invariant Neuron Transforms", "abstract": "Recently, how to protect the Intellectual Property (IP) of deep neural networks (DNN) becomes a major concern for the AI industry. To combat potential model piracy, recent works explore various watermarking strategies to embed secret identity messages into the prediction behaviors or the internals (e.g., weights and neuron activation) of the target model. Sacrificing less functionality and involving more knowledge about the target model, the latter branch of watermarking schemes (i.e., white-box model watermarking) is claimed to be accurate, credible and secure against most known watermark removal attacks, with emerging research efforts and applications in the industry. In this paper, we present the first effective removal attack which cracks almost all the existing white-box watermarking schemes with provably no performance overhead and no required prior knowledge. By analyzing these IP protection mechanisms at the granularity of neurons, we for the first time discover their common dependence on a set of fragile features of a local neuron group, all of which can be arbitrarily tampered by our proposed chain of invariant neuron transforms. On nine state-of-the-art white-box watermarking schemes and a broad set of industry-level DNN architectures, our attack for the first time reduces the embedded identity message in the protected models to be almost random. Meanwhile, unlike known removal attacks, our attack requires no prior knowledge on the training data distribution or the adopted watermark algorithms, and leaves model functionality intact."}}
{"id": "T2Vh8ig0d8H", "cdate": 1672531200000, "mdate": 1699170395476, "content": {"title": "Rethinking White-Box Watermarks on Deep Learning Models under Neural Structural Obfuscation", "abstract": "Copyright protection for deep neural networks (DNNs) is an urgent need for AI corporations. To trace illegally distributed model copies, DNN watermarking is an emerging technique for embedding and verifying secret identity messages in the prediction behaviors or the model internals. Sacrificing less functionality and involving more knowledge about the target DNN, the latter branch called \\textit{white-box DNN watermarking} is believed to be accurate, credible and secure against most known watermark removal attacks, with emerging research efforts in both the academy and the industry. In this paper, we present the first systematic study on how the mainstream white-box DNN watermarks are commonly vulnerable to neural structural obfuscation with \\textit{dummy neurons}, a group of neurons which can be added to a target model but leave the model behavior invariant. Devising a comprehensive framework to automatically generate and inject dummy neurons with high stealthiness, our novel attack intensively modifies the architecture of the target model to inhibit the success of watermark verification. With extensive evaluation, our work for the first time shows that nine published watermarking schemes require amendments to their verification procedures."}}
{"id": "Fu3v-hSpbi", "cdate": 1672531200000, "mdate": 1683893390016, "content": {"title": "Exorcising \"Wraith\": Protecting LiDAR-based Object Detector in Automated Driving System from Appearing Attacks", "abstract": "Automated driving systems rely on 3D object detectors to recognize possible obstacles from LiDAR point clouds. However, recent works show the adversary can forge non-existent cars in the prediction results with a few fake points (i.e., appearing attack). By removing statistical outliers, existing defenses are however designed for specific attacks or biased by predefined heuristic rules. Towards more comprehensive mitigation, we first systematically inspect the mechanism of recent appearing attacks: Their common weaknesses are observed in crafting fake obstacles which (i) have obvious differences in the local parts compared with real obstacles and (ii) violate the physical relation between depth and point density. In this paper, we propose a novel plug-and-play defensive module which works by side of a trained LiDAR-based object detector to eliminate forged obstacles where a major proportion of local parts have low objectness, i.e., to what degree it belongs to a real object. At the core of our module is a local objectness predictor, which explicitly incorporates the depth information to model the relation between depth and point density, and predicts each local part of an obstacle with an objectness score. Extensive experiments show, our proposed defense eliminates at least 70% cars forged by three known appearing attacks in most cases, while, for the best previous defense, less than 30% forged cars are eliminated. Meanwhile, under the same circumstance, our defense incurs less overhead for AP/precision on cars compared with existing defenses. Furthermore, We validate the effectiveness of our proposed defense on simulation-based closed-loop control driving tests in the open-source system of Baidu's Apollo."}}
{"id": "CjOn92plKqb", "cdate": 1672531200000, "mdate": 1699170396207, "content": {"title": "Rethinking White-Box Watermarks on Deep Learning Models under Neural Structural Obfuscation", "abstract": ""}}
{"id": "8_2h4_-RBcc", "cdate": 1672531200000, "mdate": 1699170395335, "content": {"title": "Enhancing Time Series Predictors With Generalized Extreme Value Loss", "abstract": "Time series prediction has wide applications in many safety-critical scenarios, including meteorology and finance. According to previous studies, time series of recorded events, e.g., river level and stock price, usually contain a non-trivial proportion of <i>extreme events</i> (e.g., flood and financial crisis), which are featured with extremely large/small values, occur in time series data with a relatively low frequency, and may have huge societal consequences if overlooked by a predictive model (i.e., predictor). Despite its significance in time series, we however observe the conventional square loss in time series prediction would ignore the modeling of extreme events. Specifically, we prove the square loss as a learning objective of the predictor behaves equivalently as a Gaussian kernel density estimator (KDE) on the recorded events, which is light-tailed itself and unable to model the ground-truth event distribution, usually heavy-tailed due to the existence of extreme events. Considering the benefits of forecasting extreme events, we propose a unified loss form called Generalized Extreme Value Loss (GEVL), which bridges the misalignment between the tail parts of the estimation and the ground-truth via transformations on either the observed events or the estimator. Following the proposed framework, we present three heavy-tailed kernels, i.e., shifted Gaussian, Gumbel and Fr\u00e9chet kernels, and derive the corresponding GEVLs which show different levels of trade-off between modeling effectiveness and computational resources, suitable for various downstream tasks. Comprehensive experiments on a diverse set of time series predictors and real-world datasets validate that, our novel loss form substantially enhances representative time series predictors in modeling extreme events. For example, for CO2 concentration rate prediction and stock price prediction, our proposed Fr\u00e9chet GEVL respectively reduces the RMSE of 6 representative DNN-based time series predictors on extreme events by over <inline-formula><tex-math notation=\"LaTeX\">$20\\%$</tex-math></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">$17\\%$</tex-math></inline-formula> on average, with a maximum reduction of <inline-formula><tex-math notation=\"LaTeX\">$29.8\\%$</tex-math></inline-formula> ."}}
