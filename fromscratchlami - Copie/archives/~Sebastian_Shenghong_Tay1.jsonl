{"id": "aFtVWKe-xpn", "cdate": 1672531200000, "mdate": 1693566007412, "content": {"title": "No-regret Sample-efficient Bayesian Optimization for Finding Nash Equilibria with Unknown Utilities", "abstract": "The Nash equilibrium (NE) is a classic solution concept for normal-form games that is stable under potential unilateral deviations by self-interested agents. Bayesian optimization (BO) has been use..."}}
{"id": "gm1Z1xKNX-", "cdate": 1640995200000, "mdate": 1661237105604, "content": {"title": "Efficient Distributionally Robust Bayesian Optimization with Worst-case Sensitivity", "abstract": "In distributionally robust Bayesian optimization (DRBO), an exact computation of the worst-case expected value requires solving an expensive convex optimization problem. We develop a fast approxima..."}}
{"id": "806MV-eXQv_", "cdate": 1640995200000, "mdate": 1661237105562, "content": {"title": "Incentivizing Collaboration in Machine Learning via Synthetic Data Rewards", "abstract": "This paper presents a novel collaborative generative modeling (CGM) framework that incentivizes collaboration among self-interested parties to contribute data to a pool for training a generative model (e.g., GAN), from which synthetic data are drawn and distributed to the parties as rewards commensurate to their contributions. Distributing synthetic data as rewards (instead of trained models or money) offers task- and model-agnostic benefits for downstream learning tasks and is less likely to violate data privacy regulation. To realize the framework, we firstly propose a data valuation function using maximum mean discrepancy (MMD) that values data based on its quantity and quality in terms of its closeness to the true data distribution and provide theoretical results guiding the kernel choice in our MMD-based data valuation function. Then, we formulate the reward scheme as a linear optimization problem that when solved, guarantees certain incentives such as fairness in the CGM framework. We devise a weighted sampling algorithm for generating synthetic data to be distributed to each party as reward such that the value of its data and the synthetic data combined matches its assigned reward value by the reward scheme. We empirically show using simulated and real-world datasets that the parties' synthetic data rewards are commensurate to their contributions."}}
{"id": "QeciG-M8d13", "cdate": 1609459200000, "mdate": 1674743963880, "content": {"title": "Top-k Ranking Bayesian Optimization", "abstract": "This paper presents a novel approach to top-k ranking Bayesian optimization (top-k ranking BO) which is a practical and significant generalization of preferential BO to handle top-k ranking and tie/indifference observations. We first design a surrogate model that is not only capable of catering to the above observations, but is also supported by a classic random utility model. Another equally important contribution is the introduction of the first information-theoretic acquisition function in BO with preferential observation called multinomial predictive entropy search (MPES) which is flexible in handling these observations and optimized for all inputs of a query jointly. MPES possesses superior performance compared with existing acquisition functions that select the inputs of a query one at a time greedily. We empirically evaluate the performance of MPES using several synthetic benchmark functions, CIFAR-10 dataset, and SUSHI preference dataset."}}
