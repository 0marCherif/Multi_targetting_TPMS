{"id": "Udho-Hry4RZ", "cdate": 1663850227096, "mdate": null, "content": {"title": "COMBAT: Alternated Training for Near-Perfect Clean-Label Backdoor Attacks", "abstract": "Backdoor attacks pose a critical concern to the practice of using third-party data for AI development. The data can be poisoned to make a trained model misbehaves when a predefined trigger pattern appears, granting the attackers illegal benefits. While most proposed backdoor attacks are dirty-label, clean-label attacks are more desirable by keeping data labels unchanged to dodge human inspection. However, designing a working clean-label attack is a challenging task, and existing clean-label attacks show underwhelming performance. In this paper, we propose a novel mechanism to develop clean-label attacks with near-perfect attack performance. The key component is a trigger pattern generator, which is trained together with a surrogate model in an alternate manner. Our proposed mechanism is flexible and customizable, allowing different backdoor trigger types and behaviors for either single or multiple target labels. Our backdoor attacks can reach near-perfect attack success rates and bypass all state-of-the-art backdoor defenses, as illustrated via comprehensive experiments on three standard benchmark datasets, including CIFAR-10, GTSRB, and CelebA."}}
{"id": "uftHKjlWC4", "cdate": 1640995200000, "mdate": 1681690743176, "content": {"title": "Improving Mini-batch Optimal Transport via Partial Transportation", "abstract": "Mini-batch optimal transport (m-OT) has been widely used recently to deal with the memory issue of OT in large-scale applications. Despite their practicality, m-OT suffers from misspecified mapping..."}}
{"id": "J4tmGGzyPqR", "cdate": 1640995200000, "mdate": 1682346239717, "content": {"title": "Entropic Gromov-Wasserstein between Gaussian Distributions", "abstract": "We study the entropic Gromov-Wasserstein and its unbalanced version between (unbalanced) Gaussian distributions with different dimensions. When the metric is the inner product, which we refer to as..."}}
{"id": "IIsaJSxYI-U", "cdate": 1640995200000, "mdate": 1681690743186, "content": {"title": "On Multimarginal Partial Optimal Transport: Equivalent Forms and Computational Complexity", "abstract": "We study the multi-marginal partial optimal transport (POT) problem between $m$ discrete (unbalanced) measures with at most $n$ supports. We first prove that we can obtain two equivalent forms of the multimarginal POT problem in terms of the multimarginal optimal transport problem via novel extensions of cost tensors. The first equivalent form is derived under the assumptions that the total masses of each measure are sufficiently close while the second equivalent form does not require any conditions on these masses but at the price of more sophisticated extended cost tensor. Our proof techniques for obtaining these equivalent forms rely on novel procedures of moving mass in graph theory to push transportation plan into appropriate regions. Finally, based on the equivalent forms, we develop an optimization algorithm, named the ApproxMPOT algorithm, that builds upon the Sinkhorn algorithm for solving the entropic regularized multimarginal optimal transport. We demonstrate that the ApproxMPOT algorithm can approximate the optimal value of multimarginal POT problem with a computational complexity upper bound of the order $\\bigOtil(m^3(n+1)^{m}/ \\varepsilon^2)$ where $\\varepsilon > 0$ stands for the desired tolerance."}}
{"id": "9Sf8fbue1br", "cdate": 1632875595838, "mdate": null, "content": {"title": "Improving Mini-batch Optimal Transport via Partial Transportation", "abstract": "Mini-batch optimal transport (m-OT) has been widely used recently to deal with the memory issue of OT in large-scale applications. Despite their practicality, m-OT suffers from misspecified mappings, namely, mappings that are optimal on the mini-batch level but are partially wrong in the comparison with the optimal transportation plan between the original measures. To address the misspecified mappings issue, we propose a novel mini-batch method by using partial optimal transport (POT) between mini-batch empirical measures, which we refer to as mini-batch partial optimal transport (m-POT). Leveraging the insight from the partial transportation, we explain the source of misspecified mappings from the m-OT and motivate why limiting the amount of transported masses among mini-batches via POT can alleviate the incorrect mappings. Finally, we carry out extensive experiments on various applications to compare m-POT with m-OT and recently proposed mini-batch method, mini-batch unbalanced optimal transport (m-UOT). We observe that m-POT is better than m-OT in deep domain adaptation applications while having comparable performance with m-UOT. On other applications, such as deep generative model and color transfer, m-POT yields more favorable performance than m-OT while m-UOT is non-trivial to apply. "}}
{"id": "YRDlrT00BP", "cdate": 1632875589348, "mdate": null, "content": {"title": "On Transportation of Mini-batches: A Hierarchical Approach", "abstract": "Mini-batch optimal transport (m-OT) has been successfully used in practical applications that involve probability measures with a very high number of supports. The m-OT solves several smaller optimal transport problems and then returns the average of their costs and transportation plans. Despite its scalability advantage, the m-OT does not consider the relationship between mini-batches which leads to undesirable estimation. Moreover, the m-OT does not approximate a proper metric between probability measures since the identity property is not satisfied. To address these problems, we propose a novel mini-batching scheme for optimal transport, named Batch of Mini-batches Optimal Transport (BoMb-OT), that finds the optimal coupling between mini-batches and it can be seen as an approximation to a well-defined distance on the space of probability measures. Furthermore, we show that the m-OT is a limit of the entropic regularized version of the BoMb-OT when the regularized parameter goes to infinity. Finally, we present the new algorithms of the BoMb-OT in various applications, such as deep generative models and deep domain adaptation. From extensive experiments, we observe that the BoMb-OT achieves a favorable performance in deep learning models such as deep generative models and deep domain adaptation. In other applications such as approximate Bayesian computation, color transfer, and gradient flow, the BoMb-OT also yields either a lower quantitative result or a better qualitative result than the m-OT."}}
{"id": "xRLT28nnlFV", "cdate": 1621630096967, "mdate": null, "content": {"title": "On Robust Optimal Transport: Computational Complexity and Barycenter Computation", "abstract": "We consider robust variants of the standard optimal transport, named robust optimal transport, where marginal constraints are relaxed via Kullback-Leibler divergence. We show that Sinkhorn-based algorithms can approximate the optimal cost of robust optimal transport in $\\widetilde{\\mathcal{O}}(\\frac{n^2}{\\varepsilon})$ time, in which $n$ is the number of supports of the probability distributions and $\\varepsilon$ is the desired error. Furthermore, we investigate a fixed-support robust barycenter problem between $m$ discrete probability distributions with at most $n$ number of supports and develop an approximating algorithm based on iterative Bregman projections (IBP). For the specific case $m = 2$, we show that this algorithm can approximate the optimal barycenter value in $\\widetilde{\\mathcal{O}}(\\frac{mn^2}{\\varepsilon})$ time, thus being better than the previous complexity $\\widetilde{\\mathcal{O}}(\\frac{mn^2}{\\varepsilon^2})$ of the IBP algorithm for approximating the Wasserstein barycenter."}}
{"id": "zvrhWBh-eT", "cdate": 1609459200000, "mdate": 1668134016067, "content": {"title": "Point-set Distances for Learning Representations of 3D Point Clouds", "abstract": ""}}
{"id": "ljfdnoRbLsJ", "cdate": 1609459200000, "mdate": 1681690743206, "content": {"title": "Improving Relational Regularized Autoencoders with Spherical Sliced Fused Gromov Wasserstein", "abstract": "Relational regularized autoencoder (RAE) is a framework to learn the distribution of data by minimizing a reconstruction loss together with a relational regularization on the prior of latent space...."}}
{"id": "kpkPYi8dsEd", "cdate": 1609459200000, "mdate": 1631640762646, "content": {"title": "Entropic Gromov-Wasserstein between Gaussian Distributions", "abstract": "We study the entropic Gromov-Wasserstein and its unbalanced version between (unbalanced) Gaussian distributions with different dimensions. When the metric is the inner product, which we refer to as inner product Gromov-Wasserstein (IGW), we demonstrate that the optimal transportation plans of entropic IGW and its unbalanced variant are (unbalanced) Gaussian distributions. Via an application of von Neumann's trace inequality, we obtain closed-form expressions for the entropic IGW between these Gaussian distributions. Finally, we consider an entropic inner product Gromov-Wasserstein barycenter of multiple Gaussian distributions. We prove that the barycenter is a Gaussian distribution when the entropic regularization parameter is small. We further derive a closed-form expression for the covariance matrix of the barycenter."}}
