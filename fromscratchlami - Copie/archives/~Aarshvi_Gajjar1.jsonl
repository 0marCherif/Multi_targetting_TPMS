{"id": "fA85bseGskN", "cdate": 1683558761156, "mdate": 1683558761156, "content": {"title": "Subspace Embeddings under nonlinear transformations", "abstract": "We consider low-distortion embeddings for subspaces under entrywise nonlinear transformations. In particular we seek embeddings that preserve the norm of all vectors in a space S = {y : y = f(x) for x \u2208 Z}, where Z is a k-dimensional subspace of R^n and f(x) is a nonlinear activation function applied entrywise to x. When f is the identity, and so S is just a k-dimensional subspace, it is known that, with high probability, a random embedding into O(k/\u000f2) dimensions preserves the norm of all y \u2208 S up to (1\u00b1 epsilon\u000f) relative error. Such embeddings are known as subspace embeddings, and have found widespread use in compressed sensing and approximation algorithms. We give the first low-distortion embeddings for a wide class of nonlinear functions f. In particular, we give additive \u000f error embeddings into O(k log(n/\u000f) 2 ) dimensions for a class of nonlinearities that includes the popular Sigmoid, SoftPlus, and Gaussian functions. We strengthen this result to give relative error embeddings under some further restrictions, which are satisfied e.g., by the Tanh, SoftSign, Exponential Linear Unit, and many other \u2018soft\u2019 step functions and rectifying units. Understanding embeddings for subspaces under nonlinear transformations is a key step towards extending random sketching and compressing sensing techniques for linear problems to nonlinear ones. We discuss example applications of our results to improved bounds for compressed sensing via generative neural networks. "}}
{"id": "HdJpNjrvTF", "cdate": 1665013556090, "mdate": null, "content": {"title": "Provable Active Learning of Neural Networks for Parametric PDEs", "abstract": "Neural networks have proven effective in constructing surrogate models for parametric partial differential equations (PDEs) and for approximating high-dimensional quantity of interest (QoI) surfaces. A major cost is training such models is collecting training data, which requires solving the target PDE for a variety of different parameter settings. Active learning and experimental design methods have the potential to reduce this cost, but are not yet widely used for training neural networks, nor do there exist methods with strong theoretical foundations. \n\t\nIn this work we provide evidence, both empirical and theoretical, that existing active sampling techniques can be used successfully for fitting neural network models for high-dimensional parameteric PDEs. In particular, we show the effectiveness of ``coherence motivated'' sampling methods (i.e., leverage score sampling), which are widely used to fit PDE surrogate models based on polynomials. We prove that leverage score sampling yields strong theoretical guarantees for fitting single neuron models, even under adversarial label noise. Our theoretical bounds apply to any single neuron model with a Lipschitz non-linearity (ReLU, sigmoid, absolute value, low-degree polynomial, etc.)."}}
