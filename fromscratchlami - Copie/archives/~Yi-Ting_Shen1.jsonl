{"id": "IUXAo-N9AGh", "cdate": 1663850034281, "mdate": null, "content": {"title": "Progressive Transformation Learning For Leveraging Virtual Images in Training", "abstract": "To effectively interrogate UAV-based images for detecting objects of interest, such as humans, it is essential to acquire large-scale UAV-based datasets that include human instances with various poses captured from widely varying viewing angles. As a viable alternative to laborious and costly data curation, we introduce Progressive Transformation Learning (PTL), which gradually augments a training dataset by adding transformed virtual images with enhanced realism. Generally, a virtual2real transformation generator in the conditional GAN framework suffers from quality degradation when a large domain gap exists between real and virtual images. To deal with the domain gap, PTL takes a novel approach that progressively iterates the following three steps: 1) select a subset from a pool of virtual images according to the domain gap, 2) transform the selected virtual images to enhance realism, and 3) add the transformed virtual images to the training set while removing them from the pool. In PTL, accurately quantifying the domain gap is critical. To do that, we theoretically demonstrate that the feature representation space of a given object detector can be modeled as a multivariate Gaussian distribution from which the Mahalanobis distance between a virtual object and the Gaussian distribution of each object category in the representation space can be readily computed. Experiments show that PTL results in a substantial performance increase over the baseline, especially in the small data and the cross-domain regime."}}
{"id": "g71bYadwf6", "cdate": 1640995200000, "mdate": 1668079307559, "content": {"title": "Progressive Transformation Learning For Leveraging Virtual Images in Training", "abstract": "To effectively interrogate UAV-based images for detecting objects of interest, such as humans, it is essential to acquire large-scale UAV-based datasets that include human instances with various poses captured from widely varying viewing angles. As a viable alternative to laborious and costly data curation, we introduce Progressive Transformation Learning (PTL), which gradually augments a training dataset by adding transformed virtual images with enhanced realism. Generally, a virtual2real transformation generator in the conditional GAN framework suffers from quality degradation when a large domain gap exists between real and virtual images. To deal with the domain gap, PTL takes a novel approach that progressively iterates the following three steps: 1) select a subset from a pool of virtual images according to the domain gap, 2) transform the selected virtual images to enhance realism, and 3) add the transformed virtual images to the training set while removing them from the pool. In PTL, accurately quantifying the domain gap is critical. To do that, we theoretically demonstrate that the feature representation space of a given object detector can be modeled as a multivariate Gaussian distribution from which the Mahalanobis distance between a virtual object and the Gaussian distribution of each object category in the representation space can be readily computed. Experiments show that PTL results in a substantial performance increase over the baseline, especially in the small data and the cross-domain regime."}}
{"id": "7kmdVSp83pZ", "cdate": 1640995200000, "mdate": 1683917297648, "content": {"title": "Archangel: A Hybrid UAV-based Human Detection Benchmark with Position and Pose Metadata", "abstract": "Learning to detect objects, such as humans, in imagery captured by an unmanned aerial vehicle (UAV) usually suffers from tremendous variations caused by the UAV's position towards the objects. In addition, existing UAV-based benchmark datasets do not provide adequate dataset metadata, which is essential for precise model diagnosis and learning features invariant to those variations. In this paper, we introduce Archangel, the first UAV-based object detection dataset composed of real and synthetic subsets captured with similar imagining conditions and UAV position and object pose metadata. A series of experiments are carefully designed with a state-of-the-art object detector to demonstrate the benefits of leveraging the metadata during model evaluation. Moreover, several crucial insights involving both real and synthetic data during model optimization are presented. In the end, we discuss the advantages, limitations, and future directions regarding Archangel to highlight its distinct value for the broader machine learning community."}}
{"id": "jgi9BgXBn20", "cdate": 1609459200000, "mdate": 1631926962083, "content": {"title": "DCT-based Hyperspectral Image Classification on Resource-Constrained Platforms", "abstract": "Deep learning based approaches to hyperspectral image analysis have attracted large attention and exhibited high performance in image classification tasks. However, deployment of deep learning based hyperspectral image analysis systems is challenging due to the computational complexity of deep learning and the large amount of data involved in hyperspectral images. To address this problem, this paper introduces a novel framework that integrates deep neural network (DNN) based image analysis by learning the network from discrete cosine transform (DCT) coefficients for hyperspectral image classification. The framework allows designers to derive diverse implementation configurations using a variable number of DCT coefficients for training. These configurations can be used to flexibly trade off classification accuracy and computational cost (e.g., based on specific characteristics of the device being used or based on time-varying operating requirements). Through experiments using a publicly available remote sensing dataset and a resource constrained Android platform, we demonstrate that our proposed approach enables the streamlined deployment of DNN-based hyperspectral image classification."}}
{"id": "Dif_NiwipHT", "cdate": 1546300800000, "mdate": 1667436076989, "content": {"title": "What Synthesis Is Missing: Depth Adaptation Integrated With Weak Supervision for Indoor Scene Parsing", "abstract": "Scene Parsing is a crucial step to enable autonomous systems to understand and interact with their surroundings. Supervised deep learning methods have made great progress in solving scene parsing problems, however, come at the cost of laborious manual pixel-level annotation. Synthetic data as well as weak supervision have been investigated to alleviate this effort. Nonetheless, synthetically generated data still suffers from severe domain shift while weak labels often lack precision. Moreover, most existing works for weakly supervised scene parsing are limited to salient foreground objects. The aim of this work is hence twofold: Exploit synthetic data where feasible and integrate weak supervision where necessary. More concretely, we address this goal by utilizing depth as transfer domain because its synthetic-to-real discrepancy is much lower than for color. At the same time, we perform weak localization from easily obtainable image level labels and integrate both using a novel contour-based scheme. Our approach is implemented as a teacher-student learning framework to solve the transfer learning problem by generating a pseudo ground truth. Using only depth-based adaptation, this approach already outperforms previous transfer learning approaches on the popular indoor scene parsing SUN RGB-D dataset. Our proposed two-stage integration more than halves the gap towards fully supervised methods when compared to previous state-of-the-art in transfer learning."}}
{"id": "eipb43kd6W-", "cdate": 1514764800000, "mdate": 1683917297650, "content": {"title": "Simple online and realtime tracking with spherical panoramic camera", "abstract": "In this paper, a simple yet effective method for online and real-time multi-object tracking (MOT) in 360-degree equi-rectangular panoramic videos is proposed. Based on the current state-of-the-art tracking-by-detection paradigm, several improvements have been made to overcome the challenges of full field-of-view (FOV) of Spherical Panoramic Camera (SPC). In addition, two datasets are presented for evaluation. It is shown that the proposed method outperforms the baseline by 28.6% and 27.8% in terms of average Multiple Object Tracking Accuracy (MOTA) on each dataset."}}
{"id": "wmPPsbgdzBh", "cdate": 1451606400000, "mdate": 1683917297650, "content": {"title": "3-D perception enhancement in autostereoscopic TV by depth cue for 3-D model interaction", "abstract": "Autostereoscopic TV provides users 3-D experience without wearing glasses. To overcome its poor resolution and to improve the 3-D perception, in this paper, based on Human Visual System (HVS) theory, some depth cues were added into the proposed system. It is shown in the result that the viewers can perceive the 3-D models twice as much as the conventional one. A 3-D model interaction system was built with the proposed framework."}}
