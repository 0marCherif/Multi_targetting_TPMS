{"id": "iWmPNHyrkIq", "cdate": 1676557333869, "mdate": 1676557333869, "content": {"title": "Knowledge Distillation by On-the-Fly Native Ensemble", "abstract": "Knowledge distillation is effective to train small and generalisable network models for meeting the low-memory and fast running requirements. Existing offline\ndistillation methods rely on a strong pre-trained teacher, which enables favourable\nknowledge discovery and transfer but requires a complex two-phase training procedure. Online counterparts address this limitation at the price of lacking a highcapacity teacher. In this work, we present an On-the-fly Native Ensemble (ONE)\nlearning strategy for one-stage online distillation. Specifically, ONE trains only a\nsingle multi-branch network while simultaneously establishing a strong teacher onthe-fly to enhance the learning of target network. Extensive evaluations show that\nONE improves the generalisation performance a variety of deep neural networks\nmore significantly than alternative methods on four image classification dataset:\nCIFAR10, CIFAR100, SVHN, and ImageNet, whilst having the computational\nefficiency advantages."}}
{"id": "ir4nDw91SY", "cdate": 1672531200000, "mdate": 1681764731476, "content": {"title": "DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion", "abstract": "We propose a new formulation of temporal action detection (TAD) with denoising diffusion, DiffTAD in short. Taking as input random temporal proposals, it can yield action proposals accurately given an untrimmed long video. This presents a generative modeling perspective, against previous discriminative learning manners. This capability is achieved by first diffusing the ground-truth proposals to random ones (i.e., the forward/noising process) and then learning to reverse the noising process (i.e., the backward/denoising process). Concretely, we establish the denoising process in the Transformer decoder (e.g., DETR) by introducing a temporal location query design with faster convergence in training. We further propose a cross-step selective conditioning algorithm for inference acceleration. Extensive evaluations on ActivityNet and THUMOS show that our DiffTAD achieves top performance compared to previous art alternatives. The code will be made available at https://github.com/sauradip/DiffusionTAD."}}
{"id": "iOEOv-YMpGA", "cdate": 1672531200000, "mdate": 1681764731475, "content": {"title": "Generative Semantic Segmentation", "abstract": "We present Generative Semantic Segmentation (GSS), a generative learning approach for semantic segmentation. Uniquely, we cast semantic segmentation as an image-conditioned mask generation problem. This is achieved by replacing the conventional per-pixel discriminative learning with a latent prior learning process. Specifically, we model the variational posterior distribution of latent variables given the segmentation mask. To that end, the segmentation mask is expressed with a special type of image (dubbed as maskige). This posterior distribution allows to generate segmentation masks unconditionally. To achieve semantic segmentation on a given image, we further introduce a conditioning network. It is optimized by minimizing the divergence between the posterior distribution of maskige (i.e., segmentation masks) and the latent prior distribution of input training images. Extensive experiments on standard benchmarks show that our GSS can perform competitively to prior art alternatives in the standard semantic segmentation setting, whilst achieving a new state of the art in the more challenging cross-domain setting."}}
{"id": "elTs5VSxFj", "cdate": 1672531200000, "mdate": 1681764731354, "content": {"title": "Unsupervised Hashing via Similarity Distribution Calibration", "abstract": "Existing unsupervised hashing methods typically adopt a feature similarity preservation paradigm. As a result, they overlook the intrinsic similarity capacity discrepancy between the continuous feature and discrete hash code spaces. Specifically, since the feature similarity distribution is intrinsically biased (e.g., moderately positive similarity scores on negative pairs), the hash code similarities of positive and negative pairs often become inseparable (i.e., the similarity collapse problem). To solve this problem, in this paper a novel Similarity Distribution Calibration (SDC) method is introduced. Instead of matching individual pairwise similarity scores, SDC aligns the hash code similarity distribution towards a calibration distribution (e.g., beta distribution) with sufficient spread across the entire similarity capacity/range, to alleviate the similarity collapse problem. Extensive experiments show that our SDC outperforms the state-of-the-art alternatives on both coarse category-level and instance-level image retrieval tasks, often by a large margin. Code is available at https://github.com/kamwoh/sdc."}}
{"id": "cqP2E5mQ2BM", "cdate": 1672531200000, "mdate": 1678877478029, "content": {"title": "FAME-ViL: Multi-Tasking Vision-Language Model for Heterogeneous Fashion Tasks", "abstract": ""}}
{"id": "9Z3znfYcIAY", "cdate": 1672531200000, "mdate": 1681764731348, "content": {"title": "PersonalTailor: Personalizing 2D Pattern Design from 3D Garment Point Clouds", "abstract": "Garment pattern design aims to convert a 3D garment to the corresponding 2D panels and their sewing structure. Existing methods rely either on template fitting with heuristics and prior assumptions, or on model learning with complicated shape parameterization. Importantly, both approaches do not allow for personalization of the output garment, which today has increasing demands. To fill this demand, we introduce PersonalTailor: a personalized 2D pattern design method, where the user can input specific constraints or demands (in language or sketch) for personal 2D panel fabrication from 3D point clouds. PersonalTailor first learns a multi-modal panel embeddings based on unsupervised cross-modal association and attentive fusion. It then predicts a binary panel masks individually using a transformer encoder-decoder framework. Extensive experiments show that our PersonalTailor excels on both personalized and standard pattern fabrication tasks."}}
{"id": "0V_di_eAWq", "cdate": 1672531200000, "mdate": 1681764731345, "content": {"title": "Preconditioned Score-based Generative Models", "abstract": "Score-based generative models (SGMs) have recently emerged as a promising class of generative models. However, a fundamental limitation is that their sampling process is slow due to a need for many (\\eg, $2000$) iterations of sequential computations. An intuitive acceleration method is to reduce the sampling iterations which however causes severe performance degradation. We assault this problem to the ill-conditioned issues of the Langevin dynamics and reverse diffusion in the sampling process. Under this insight, we propose a model-agnostic {\\bf\\em preconditioned diffusion sampling} (PDS) method that leverages matrix preconditioning to alleviate the aforementioned problem. PDS alters the sampling process of a vanilla SGM at marginal extra computation cost, and without model retraining. Theoretically, we prove that PDS preserves the output distribution of the SGM, no risk of inducing systematical bias to the original sampling process. We further theoretically reveal a relation between the parameter of PDS and the sampling iterations,easing the parameter estimation under varying sampling iterations. Extensive experiments on various image datasets with a variety of resolutions and diversity validate that our PDS consistently accelerates off-the-shelf SGMs whilst maintaining the synthesis quality. In particular, PDS can accelerate by up to $29\\times$ on more challenging high resolution (1024$\\times$1024) image generation. Compared with the latest generative models (\\eg, CLD-SGM, DDIM, and Analytic-DDIM), PDS can achieve the best sampling quality on CIFAR-10 at a FID score of 1.99. Our code is made publicly available to foster any further research https://github.com/fudan-zvg/PDS."}}
{"id": "XwOTHNHAjF-", "cdate": 1669112839501, "mdate": 1669112839501, "content": {"title": "Class Discriminative Adversarial Learning for Unsupervised Domain Adaptation", "abstract": "As a state-of-the-art family of Unsupervised Domain Adaptation (UDA), bi-classifier adversarial learning methods are formulated in\nan adversarial (minimax) learning framework with a single feature extractor and two classifiers. Model training alternates between\ntwo steps: (I) constraining the learning of the two classifiers to maximize the prediction discrepancy of unlabeled target domain\ndata, and (II) constraining the learning of the feature extractor to minimize this discrepancy. Despite being an elegant formulation,\nthis approach has a fundamental limitation: Maximizing and minimizing the classifier discrepancy is not class discriminative for the\ntarget domain, finally leading to a suboptimal adapted model. To solve this problem, we propose a novel Class Discriminative Adversarial Learning (CDAL) method characterized by discovering class discrimination knowledge and leveraging this knowledge to discriminatively regulate the classifier discrepancy constraints onthe-fly. This is realized by introducing an evaluation criterion for judging each classifier\u2019s capability and each target domain sample\u2019s feature reorientation via objective loss reformulation. Extensive\nexperiments on three standard benchmarks show that our CDAL method yields new state-of-the-art performance. Our code is made\navailable at https://github.com/buerzlh/CDAL."}}
{"id": "3_tWQ9N0m_", "cdate": 1668589556135, "mdate": 1668589556135, "content": {"title": "Learning Ego 3D Representation as Ray Tracing", "abstract": "A self-driving perception model aims to extract 3D semantic representations from multiple cameras collectively into the bird's-eye-view (BEV) coordinate frame of the ego car in order to ground downstream planner. Existing perception methods often rely on error-prone depth estimation of the whole scene or learning sparse virtual 3D representations without the target geometry structure, both of which remain limited in performance and/or capability. In this paper, we present a novel end-to-end architecture for ego 3D representation learning from an arbitrary number of unconstrained camera views. Inspired by the ray tracing principle, we design a polarized grid of \"imaginary eyes\" as the learnable ego 3D representation and formulate the learning process with the adaptive attention mechanism in conjunction with the 3D-to-2D projection. Critically, this formulation allows extracting rich 3D representation from 2D images without any depth supervision, and with the built-in geometry structure consistent w.r.t. BEV. Despite its simplicity and versatility, extensive experiments on standard BEV visual tasks (e.g., camera-based 3D object detection and BEV segmentation) show that our model outperforms all state-of-the-art alternatives significantly, with an extra advantage in computational efficiency from multi-task learning."}}
{"id": "FhJP5hxGhU", "cdate": 1668589432830, "mdate": 1668589432830, "content": {"title": "Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers", "abstract": "Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (ie, without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission."}}
