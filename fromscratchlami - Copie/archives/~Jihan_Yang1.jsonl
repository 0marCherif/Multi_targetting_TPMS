{"id": "8wg8VWT_RPb", "cdate": 1682899200000, "mdate": 1682321739545, "content": {"title": "ST3D++: Denoised Self-Training for Unsupervised Domain Adaptation on 3D Object Detection", "abstract": "In this paper, we present a self-training method, named ST3D++, with a holistic pseudo label denoising pipeline for unsupervised domain adaptation on 3D object detection. ST3D++ aims at reducing noise in pseudo label generation as well as alleviating the negative impacts of noisy pseudo labels on model training. First, ST3D++ pre-trains the 3D object detector on the labeled source domain with random object scaling (ROS) which is designed to reduce target domain pseudo label noise arising from object scale bias of the source domain. Then, the detector is progressively improved through alternating between generating pseudo labels and training the object detector with pseudo-labeled target domain data. Here, we equip the pseudo label generation process with a hybrid quality-aware triplet memory to improve the quality and stability of generated pseudo labels. Meanwhile, in the model training stage, we propose a source data assisted training strategy and a curriculum data augmentation policy to effectively rectify noisy gradient directions and avoid model over-fitting to noisy pseudo labeled data. These specific designs enable the detector to be trained on meticulously refined pseudo labeled target data with denoised training signals, and thus effectively facilitate adapting an object detector to a target domain without requiring annotations. Finally, our method is assessed on four 3D benchmark datasets (i.e., Waymo, KITTI, Lyft, and nuScenes) for three common categories (i.e., car, pedestrian and bicycle). ST3D++ achieves state-of-the-art performance on all evaluated settings, outperforming the corresponding baseline by a large margin (e.g., 9.6% <inline-formula><tex-math notation=\"LaTeX\">$\\sim$</tex-math></inline-formula> 38.16% on Waymo <inline-formula><tex-math notation=\"LaTeX\">$\\rightarrow$</tex-math></inline-formula> KITTI in terms of AP <inline-formula><tex-math notation=\"LaTeX\">$_{\\text{3D}}$</tex-math></inline-formula> ), and even surpasses the fully supervised oracle results on the KITTI 3D object detection benchmark with target prior. Code is available at <uri>https://github.com/CVMI-Lab/ST3D</uri> ."}}
{"id": "r2NsCBGaUjs", "cdate": 1672531200000, "mdate": 1696219112250, "content": {"title": "RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding", "abstract": "Existing 3D scene understanding tasks have achieved high performance on close-set benchmarks but fail to handle novel categories in real-world applications. To this end, we propose a Regional Point-Language Contrastive learning framework, namely RegionPLC, for open-world 3D scene understanding, which equips models trained on closed-set datasets with open-vocabulary recognition capabilities. We propose dense visual prompts to elicit region-level visual-language knowledge from 2D foundation models via captioning, which further allows us to build dense regional point-language associations. Then, we design a point-discriminative contrastive learning objective to enable point-independent learning from captions for dense scene understanding. We conduct extensive experiments on ScanNet, ScanNet200, and nuScenes datasets. Our RegionPLC significantly outperforms previous base-annotated 3D open-world scene understanding approaches by an average of 11.6\\% and 6.6\\% for semantic and instance segmentation, respectively. It also shows promising open-world results in absence of any human annotation with low training and inference costs. Code will be released."}}
{"id": "ByJnWLlVXWi", "cdate": 1672531200000, "mdate": 1696219112245, "content": {"title": "Lowis3D: Language-Driven Open-World Instance-Level 3D Scene Understanding", "abstract": "Open-world instance-level scene understanding aims to locate and recognize unseen object categories that are not present in the annotated dataset. This task is challenging because the model needs to both localize novel 3D objects and infer their semantic categories. A key factor for the recent progress in 2D open-world perception is the availability of large-scale image-text pairs from the Internet, which cover a wide range of vocabulary concepts. However, this success is hard to replicate in 3D scenarios due to the scarcity of 3D-text pairs. To address this challenge, we propose to harness pre-trained vision-language (VL) foundation models that encode extensive knowledge from image-text pairs to generate captions for multi-view images of 3D scenes. This allows us to establish explicit associations between 3D shapes and semantic-rich captions. Moreover, to enhance the fine-grained visual-semantic representation learning from captions for object-level categorization, we design hierarchical point-caption association methods to learn semantic-aware embeddings that exploit the 3D geometry between 3D points and multi-view images. In addition, to tackle the localization challenge for novel classes in the open-world setting, we develop debiased instance localization, which involves training object grouping modules on unlabeled data using instance-level pseudo supervision. This significantly improves the generalization capabilities of instance grouping and thus the ability to accurately locate novel objects. We conduct extensive experiments on 3D semantic, instance, and panoptic segmentation tasks, covering indoor and outdoor scenes across three datasets. Our method outperforms baseline methods by a significant margin in semantic segmentation (e.g. 34.5%$\\sim$65.3%), instance segmentation (e.g. 21.8%$\\sim$54.0%) and panoptic segmentation (e.g. 14.7%$\\sim$43.3%). Code will be available."}}
{"id": "1tnVNogPUz9", "cdate": 1652737283943, "mdate": null, "content": {"title": "Towards Efficient 3D Object Detection with Knowledge Distillation", "abstract": "Despite substantial progress in 3D object detection, advanced 3D detectors often suffer from heavy computation overheads. To this end, we explore the potential of knowledge distillation (KD) for developing efficient 3D object detectors, focusing on popular pillar- and voxel-based detectors. In the absence of well-developed teacher-student pairs, we first study how to obtain student models with good trade offs between accuracy and efficiency from the perspectives of model compression and input resolution reduction. Then, we build a benchmark to assess existing KD methods developed in the 2D domain for 3D object detection upon six well-constructed teacher-student pairs. Further, we propose an improved KD pipeline incorporating an enhanced logit KD method that performs KD on only a few pivotal positions determined by teacher classification response and a teacher-guided student model initialization to facilitate transferring teacher model's feature extraction ability to students through weight inheritance. Finally, we conduct extensive experiments on the Waymo dataset. Our best performing model achieves $65.75\\%$ LEVEL 2 mAPH surpassing its teacher model and requiring only $44\\%$ of teacher flops. Our most efficient model runs 51 FPS on an NVIDIA A100, which is $2.2\\times$ faster than PointPillar with even higher accuracy. Code will be available."}}
{"id": "wBCURNcqUs", "cdate": 1640995200000, "mdate": 1667681620607, "content": {"title": "DODA: Data-Oriented Sim-to-Real Domain Adaptation for 3D Semantic Segmentation", "abstract": "Deep learning approaches achieve prominent success in 3D semantic segmentation. However, collecting densely annotated real-world 3D datasets is extremely time-consuming and expensive. Training models on synthetic data and generalizing on real-world scenarios becomes an appealing alternative, but unfortunately suffers from notorious domain shifts. In this work, we propose a Data-Oriented Domain Adaptation (DODA) framework to mitigate pattern and context gaps caused by different sensing mechanisms and layout placements across domains. Our DODA encompasses virtual scan simulation to imitate real-world point cloud patterns and tail-aware cuboid mixing to alleviate the interior context gap with a cuboid-based intermediate domain. The first unsupervised sim-to-real adaptation benchmark on 3D indoor semantic segmentation is also built on 3D-FRONT, ScanNet and S3DIS along with 8 popular Unsupervised Domain Adaptation (UDA) methods. Our DODA surpasses existing UDA approaches by over 13% on both 3D-FRONT $$\\rightarrow $$ ScanNet and 3D-FRONT $$\\rightarrow $$ S3DIS. Code is available at https://github.com/CVMI-Lab/DODA ."}}
{"id": "n-sMjoHYUIP", "cdate": 1640995200000, "mdate": 1667358345305, "content": {"title": "Towards Efficient 3D Object Detection with Knowledge Distillation", "abstract": "Despite substantial progress in 3D object detection, advanced 3D detectors often suffer from heavy computation overheads. To this end, we explore the potential of knowledge distillation (KD) for developing efficient 3D object detectors, focusing on popular pillar- and voxel-based detectors.In the absence of well-developed teacher-student pairs, we first study how to obtain student models with good trade offs between accuracy and efficiency from the perspectives of model compression and input resolution reduction. Then, we build a benchmark to assess existing KD methods developed in the 2D domain for 3D object detection upon six well-constructed teacher-student pairs. Further, we propose an improved KD pipeline incorporating an enhanced logit KD method that performs KD on only a few pivotal positions determined by teacher classification response, and a teacher-guided student model initialization to facilitate transferring teacher model's feature extraction ability to students through weight inheritance. Finally, we conduct extensive experiments on the Waymo dataset. Our best performing model achieves $65.75\\%$ LEVEL 2 mAPH, surpassing its teacher model and requiring only $44\\%$ of teacher flops. Our most efficient model runs 51 FPS on an NVIDIA A100, which is $2.2\\times$ faster than PointPillar with even higher accuracy. Code is available at \\url{https://github.com/CVMI-Lab/SparseKD}."}}
{"id": "hSPts5ERLp", "cdate": 1640995200000, "mdate": 1667358345326, "content": {"title": "DODA: Data-oriented Sim-to-Real Domain Adaptation for 3D Indoor Semantic Segmentation", "abstract": "Deep learning approaches achieve prominent success in 3D semantic segmentation. However, collecting densely annotated real-world 3D datasets is extremely time-consuming and expensive. Training models on synthetic data and generalizing on real-world scenarios becomes an appealing alternative, but unfortunately suffers from notorious domain shifts. In this work, we propose a Data-Oriented Domain Adaptation (DODA) framework to mitigate pattern and context gaps caused by different sensing mechanisms and layout placements across domains. Our DODA encompasses virtual scan simulation to imitate real-world point cloud patterns and tail-aware cuboid mixing to alleviate the interior context gap with a cuboid-based intermediate domain. The first unsupervised sim-to-real adaptation benchmark on 3D indoor semantic segmentation is also built on 3D-FRONT, ScanNet and S3DIS along with 7 popular Unsupervised Domain Adaptation (UDA) methods. Our DODA surpasses existing UDA approaches by over 13% on both 3D-FRONT -> ScanNet and 3D-FRONT -> S3DIS. Code is available at https://github.com/CVMI-Lab/DODA."}}
{"id": "cAMBf4hck3t", "cdate": 1640995200000, "mdate": 1667358345364, "content": {"title": "Knowledge Distillation as Efficient Pre-training: Faster Convergence, Higher Data-efficiency, and Better Transferability", "abstract": "Large-scale pre-training has been proven to be crucial for various computer vision tasks. However, with the increase of pre-training data amount, model architecture amount, and the private/inaccessible data, it is not very efficient or possible to pre-train all the model architectures on large-scale datasets. In this work, we investigate an alternative strategy for pre-training, namely Knowledge Distillation as Efficient Pre-training (KDEP), aiming to efficiently transfer the learned feature representation from existing pre-trained models to new student models for future downstream tasks. We observe that existing Knowledge Distillation (KD) methods are unsuitable towards pre-training since they normally distill the logits that are going to be discarded when transferred to downstream tasks. To resolve this problem, we propose a feature-based KD method with non-parametric feature dimension aligning. Notably, our method performs comparably with supervised pre-training counterparts in 3 downstream tasks and 9 downstream datasets requiring 10x less data and 5x less pre-training time. Code is available at https://github.com/CVMI-Lab/KDEP."}}
{"id": "WzTKajhSNZ", "cdate": 1640995200000, "mdate": 1696219112247, "content": {"title": "Towards Efficient 3D Object Detection with Knowledge Distillation", "abstract": "Despite substantial progress in 3D object detection, advanced 3D detectors often suffer from heavy computation overheads. To this end, we explore the potential of knowledge distillation (KD) for developing efficient 3D object detectors, focusing on popular pillar- and voxel-based detectors. In the absence of well-developed teacher-student pairs, we first study how to obtain student models with good trade offs between accuracy and efficiency from the perspectives of model compression and input resolution reduction. Then, we build a benchmark to assess existing KD methods developed in the 2D domain for 3D object detection upon six well-constructed teacher-student pairs. Further, we propose an improved KD pipeline incorporating an enhanced logit KD method that performs KD on only a few pivotal positions determined by teacher classification response and a teacher-guided student model initialization to facilitate transferring teacher model's feature extraction ability to students through weight inheritance. Finally, we conduct extensive experiments on the Waymo dataset. Our best performing model achieves $65.75\\%$ LEVEL 2 mAPH surpassing its teacher model and requiring only $44\\%$ of teacher flops. Our most efficient model runs 51 FPS on an NVIDIA A100, which is $2.2\\times$ faster than PointPillar with even higher accuracy. Code will be available."}}
{"id": "WkytSyM-fh1", "cdate": 1640995200000, "mdate": 1680770612804, "content": {"title": "Language-driven Open-Vocabulary 3D Scene Understanding", "abstract": ""}}
