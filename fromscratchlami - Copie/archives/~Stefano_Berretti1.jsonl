{"id": "X7ZTGukncfV", "cdate": 1649404150784, "mdate": 1649404150784, "content": {"title": "Sparse to Dense Dynamic 3D Facial Expression Generation", "abstract": "In this paper, we propose a solution to the task of generating dynamic 3D facial expressions from a neutral 3D face and an expression label. This involves solving two sub-problems: (i) modeling the temporal dynamics of expressions, and (ii) deforming the neutral mesh to obtain the expressive counterpart. We represent the temporal evolution of expressions using the motion of a sparse set of 3D landmarks that we learn to generate by training a manifold-valued GAN (Motion3DGAN). To better encode the expression-induced deformation and disentangle it from the identity information, the generated motion is represented as per-frame displacement from a neutral configuration. To generate the expressive meshes, we train a Sparse2Dense mesh Decoder (S2D-Dec) that maps the landmark displacements to a dense, per-vertex displacement. This allows us to learn how the motion of a sparse set of landmarks influences the deformation of the overall face surface, independently from the identity. Experimental results on the CoMA and D3DFACS datasets show that our solution brings significant improvements with respect to previous solutions in terms of both dynamic expression generation and mesh reconstruction, while retaining good generalization to unseen data. The code and the pretrained model will be made publicly available"}}
{"id": "SYlDPKUNq6l", "cdate": 1649403832505, "mdate": 1649403832505, "content": {"title": "Dynamic facial expression generation on hilbert hypersphere with conditional wasserstein generative adversarial nets", "abstract": "In this work, we propose a novel approach for generating videos of the six basic facial expressions given a neutral\nface image. We propose to exploit the face geometry by modeling the facial landmarks motion as curves encoded as points on a\nhypersphere. By proposing a conditional version of manifold-valued Wasserstein generative adversarial network (GAN) for motion\ngeneration on the hypersphere, we learn the distribution of facial expression dynamics of different classes, from which we synthesize\nnew facial expression motions. The resulting motions can be transformed to sequences of landmarks and then to images sequences by\nediting the texture information using another conditional Generative Adversarial Network. To the best of our knowledge, this is the first\nwork that explores manifold-valued representations with GAN to address the problem of dynamic facial expression generation. We\nevaluate our proposed approach both quantitatively and qualitatively on two public datasets; Oulu-CASIA and MUG Facial Expression.\nOur experimental results demonstrate the effectiveness of our approach in generating realistic videos with continuous motion, realistic\nappearance and identity preservation. We also show the efficiency of our framework for dynamic facial expressions generation,\ndynamic facial expression transfer and data augmentation for training improved emotion recognition models."}}
{"id": "SddeKG0rFRF", "cdate": 1643892240524, "mdate": 1643892240524, "content": {"title": "A Sparse and Locally Coherent Morphable Face Model for Dense Semantic Correspondence Across Heterogeneous 3D Faces", "abstract": "The 3D Morphable Model (3DMM) is a powerful statistical tool for representing 3D face shapes. To build a 3DMM, a training set of face scans in full point-to-point correspondence is required, and its modeling capabilities directly depend on the variability contained in the training data. Thus, to increase the descriptive power of the 3DMM, establishing a dense correspondence across heterogeneous scans with sufficient diversity in terms of identities, ethnicities, or expressions becomes essential. In this manuscript, we present a fully automatic approach that leverages a 3DMM to transfer its dense semantic annotation across raw 3D faces, establishing a dense correspondence between them. We propose a novel formulation to learn a set of sparse deformation components with local support on the face that, together with an original non-rigid deformation algorithm, allow the 3DMM to precisely fit unseen faces and transfer its semantic annotation. We extensively experimented our approach, showing it can effectively generalize to highly diverse samples and accurately establish a dense correspondence even in presence of complex facial expressions. The accuracy of the dense registration is demonstrated by building a heterogeneous, large-scale 3DMM from more than 9,000 fully registered scans obtained by joining three large datasets together."}}
{"id": "ya2zj6pGrC", "cdate": 1640995200000, "mdate": 1668438526577, "content": {"title": "Generating Complex 4D Expression Transitions by Learning Face Landmark Trajectories", "abstract": "In this work, we address the problem of 4D facial expressions generation. This is usually addressed by animating a neutral 3D face to reach an expression peak, and then get back to the neutral state. In the real world though, people show more complex expressions, and switch from one expression to another. We thus propose a new model that generates transitions between different expressions, and synthesizes long and composed 4D expressions. This involves three sub-problems: (i) modeling the temporal dynamics of expressions, (ii) learning transitions between them, and (iii) deforming a generic mesh. We propose to encode the temporal evolution of expressions using the motion of a set of 3D landmarks, that we learn to generate by training a manifold-valued GAN (Motion3DGAN). To allow the generation of composed expressions, this model accepts two labels encoding the starting and the ending expressions. The final sequence of meshes is generated by a Sparse2Dense mesh Decoder (S2D-Dec) that maps the landmark displacements to a dense, per-vertex displacement of a known mesh topology. By explicitly working with motion trajectories, the model is totally independent from the identity. Extensive experiments on five public datasets show that our proposed approach brings significant improvements with respect to previous solutions, while retaining good generalization to unseen data."}}
{"id": "yAI9DESuo16", "cdate": 1640995200000, "mdate": 1668438526398, "content": {"title": "Guest Editorial: Medical Data Security Solution for Healthcare Industries", "abstract": "Since smart healthcare systems are highly connected to advanced wearable devices, internet of things (IoT) and mobile internet, valuable patient information and other significant medical records are easily transmitted over the public network. The patient information and clinical records are also stored on the existing databases and local servers of hospitals and healthcare centres. These materials not only provide a reference for healthcare professionals to make correct decisions on the patients, but also provide a strong basis for other professionals to undertake effective treatment and develop plans for correct diagnosis. Furthermore, the databases may be used by various research communities for research, without any possibility of privacy violations. However, leaking of healthcare data is highly likely. Therefore, medical data security is becoming very important in smart healthcare."}}
{"id": "wfDweRBCV3", "cdate": 1640995200000, "mdate": 1668438526397, "content": {"title": "A Sparse and Locally Coherent Morphable Face Model for Dense Semantic Correspondence Across Heterogeneous 3D Faces", "abstract": "The 3D Morphable Model (3DMM) is a powerful statistical tool for representing 3D face shapes. To build a 3DMM, a training set of face scans in full point-to-point correspondence is required, and its modeling capabilities directly depend on the variability contained in the training data. Thus, to increase the descriptive power of the 3DMM, establishing a dense correspondence across heterogeneous scans with sufficient diversity in terms of identities, ethnicities, or expressions becomes essential. In this manuscript, we present a fully automatic approach that leverages a 3DMM to transfer its dense semantic annotation across raw 3D faces, establishing a dense correspondence between them. We propose a novel formulation to learn a set of sparse deformation components with local support on the face that, together with an original non-rigid deformation algorithm, allow the 3DMM to precisely fit unseen faces and transfer its semantic annotation. We extensively experimented our approach, showing it can effectively generalize to highly diverse samples and accurately establish a dense correspondence even in presence of complex facial expressions. The accuracy of the dense registration is demonstrated by building a heterogeneous, large-scale 3DMM from more than 9,000 fully registered scans obtained by joining three large datasets together."}}
{"id": "tIZEBLvYfpZ", "cdate": 1640995200000, "mdate": 1668438526397, "content": {"title": "A Psychologically Inspired Fuzzy Cognitive Deep Learning Framework to Predict Crowd Behavior", "abstract": "In an intelligent surveillance system, detecting and predicting diverse collective crowd behaviors has emerged as a challenging problem for efficient crowd management. In real-world scenarios, potential disasters and hazards can be averted by considering crowd psychology for predicting crowd behaviors. This article proposes an approach that exploits the psychological and cognitive aspects of human behavior in determining nine diverse crowd behaviors. The proposed approach is a combination of two cognitive deep learning frameworks and a psychological fuzzy computational model that utilizes OCC theory of emotions, OCEAN five-factor model of personality and visual attention for detecting crowd behaviors. Experiments are performed on different datasets and the results prove that our approach is successful in detecting and predicting crowd behavior in confronting situations and also outperforms the state-of-the-art methods. In particular, considering psychological aspects and cognition in determining crowd behavior is beneficial for rectifying the semantic ambiguity in identifying crowd behaviors."}}
{"id": "oFQhVYKsXS", "cdate": 1640995200000, "mdate": 1668438526400, "content": {"title": "Dynamic Facial Expression Generation on Hilbert Hypersphere With Conditional Wasserstein Generative Adversarial Nets", "abstract": "In this work, we propose a novel approach for generating videos of the six basic facial expressions given a neutral face image. We propose to exploit the face geometry by modeling the facial landmarks motion as curves encoded as points on a hypersphere. By proposing a conditional version of manifold-valued Wasserstein generative adversarial network (GAN) for motion generation on the hypersphere, we learn the distribution of facial expression dynamics of different classes, from which we synthesize new facial expression motions. The resulting motions can be transformed to sequences of landmarks and then to images sequences by editing the texture information using another conditional Generative Adversarial Network. To the best of our knowledge, this is the first work that explores manifold-valued representations with GAN to address the problem of dynamic facial expression generation. We evaluate our proposed approach both quantitatively and qualitatively on two public datasets; Oulu-CASIA and MUG Facial Expression. Our experimental results demonstrate the effectiveness of our approach in generating realistic videos with continuous motion, realistic appearance and identity preservation. We also show the efficiency of our framework for dynamic facial expressions generation, dynamic facial expression transfer and data augmentation for training improved emotion recognition models."}}
{"id": "l0hroEdJGl", "cdate": 1640995200000, "mdate": 1668438526396, "content": {"title": "Guest Editorial Emerging IoT-Driven Smart Health: From Cloud to Edge", "abstract": "The papers in this special section focus on emerging Internet of Medical Things. Recent advances in advances in healthcare can be experienced with the development of smart sensorial things, Artificial Intelligence (AI), Machine Learning (ML), Deep Learning (DL), edge computing, Edge AI, 6G, cloud computing, and connected healthcare have attracted a great deal of attention and a wide range of views. However, the need to deliver real-time and accurate healthcare services to patients, while reducing costs is a challenging issue [1]. Especially, COVID-19 has recently demonstrated the importance of fast, comprehensive, and accurate intelligent healthcare involving different types of medical, physiological, and epidemiological investigation data to diagnose the virus. Smart health is a real-time, intelligent, ubiquitous healthcare service based on Internet of bioMedical Things (IoMT). With the rapid development of related technologies such as deep learning, edge computing and IoT, smart health is playing vital role in healthcare industry to increase the accuracy, reliability, and productivity of mobile sensory devices."}}
{"id": "l-n150fTSZa", "cdate": 1640995200000, "mdate": 1668438526579, "content": {"title": "SPEAKER VGG CCT: Cross-corpus Speech Emotion Recognition with Speaker Embedding and Vision Transformers", "abstract": "In recent years, Speech Emotion Recognition (SER) has been investigated mainly transforming the speech signal into spectrograms that are then classified using Convolutional Neural Networks pretrained on generic images and fine tuned with spectrograms. In this paper, we start from the general idea above and develop a new learning solution for SER, which is based on Compact Convolutional Transformers (CCTs) combined with a speaker embedding. With CCTs, the learning power of Vision Transformers (ViT) is combined with a diminished need for large volume of data as made possible by the convolution. This is important in SER, where large corpora of data are usually not available. The speaker embedding allows the network to extract an identity representation of the speaker, which is then integrated by means of a self-attention mechanism with the features that the CCT extracts from the spectrogram. Overall, the solution is capable of operating in real-time showing promising results in a cross-corpus scenario, where training and test datasets are kept separate. Experiments have been performed on several benchmarks in a cross-corpus setting as rarely used in the literature, with results that are comparable or superior to those obtained with state-of-the-art network architectures. Our code is available at https://github.com/JabuMlDev/Speaker-VGG-CCT."}}
