{"id": "zxgjp1eQSA8", "cdate": 1672531200000, "mdate": 1681559408529, "content": {"title": "Population-based Evaluation in Repeated Rock-Paper-Scissors as a Benchmark for Multiagent Reinforcement Learning", "abstract": ""}}
{"id": "Ge-KIUzZtcJ", "cdate": 1628612192575, "mdate": 1628612192575, "content": {"title": "No Press Diplomacy: Modeling Multi-Agent Gameplay", "abstract": "Diplomacy is a seven-player non-stochastic, non-cooperative game, where agents\nacquire resources through a mix of teamwork and betrayal. Reliance on trust and\ncoordination makes Diplomacy the first non-cooperative multi-agent benchmark\nfor complex sequential social dilemmas in a rich environment. In this work, we\nfocus on training an agent that learns to play the No Press version of Diplomacy\nwhere there is no dedicated communication channel between players. We present\nDipNet, a neural-network-based policy model for No Press Diplomacy. The model\nwas trained on a new dataset of more than 150,000 human games. Our model is\ntrained by supervised learning (SL) from expert trajectories, which is then used to\ninitialize a reinforcement learning (RL) agent trained through self-play. Both the\nSL and RL agents demonstrate state-of-the-art No Press performance by beating\npopular rule-based bots."}}
{"id": "AxWLtSsRtyd", "cdate": 1628612052849, "mdate": 1628612052849, "content": {"title": "Learning to Play Against Any Mixture of Opponents", "abstract": "Intuitively, experience playing against one mixture\nof opponents in a given domain should be relevant\nfor a different mixture in the same domain.\nWe propose a transfer learning method, Q-Mixing,\nthat starts by learning Q-values against each purestrategy\nopponent. Then a Q-value for any distribution\nof opponent strategies is approximated\nby appropriately averaging the separately learned\nQ-values. From these components, we construct\npolicies against all opponent mixtures without\nany further training. We empirically validate QMixing\nin two environments: a simple grid-world\nsoccer environment, and a social dilemma game.\nWe find that Q-Mixing is able to successfully\ntransfer knowledge across any mixture of opponents.\nWe next consider the use of observations\nduring play to update the believed distribution of\nopponents. We introduce an opponent classifier\u2014\ntrained in parallel to Q-learning, reusing data\u2014\nand use the classifier results to refine the mixing\nof Q-values. We find that Q-Mixing augmented\nwith the opponent policy classifier performs better,\nwith higher variance, than training directly\nagainst a mixed-strategy opponent."}}
{"id": "y1YgOKlgnNW", "cdate": 1609459200000, "mdate": 1681686721141, "content": {"title": "Iterative Empirical Game Solving via Single Policy Best Response", "abstract": "Policy-Space Response Oracles (PSRO) is a general algorithmic framework for learning policies in multiagent systems by interleaving empirical game analysis with deep reinforcement learning (Deep RL). At each iteration, Deep RL is invoked to train a best response to a mixture of opponent policies. The repeated application of Deep RL poses an expensive computational burden as we look to apply this algorithm to more complex domains. We introduce two variations of PSRO designed to reduce the amount of simulation required during Deep RL training. Both algorithms modify how PSRO adds new policies to the empirical game, based on learned responses to a single opponent policy. The first, Mixed-Oracles, transfers knowledge from previous iterations of Deep RL, requiring training only against the opponent's newest policy. The second, Mixed-Opponents, constructs a pure-strategy opponent by mixing existing strategy's action-value estimates, instead of their policies. Learning against a single policy mitigates variance in state outcomes that is induced by an unobserved distribution of opponents. We empirically demonstrate that these algorithms substantially reduce the amount of simulation during training required by PSRO, while producing equivalent or better solutions to the game."}}
{"id": "R5GO1sd2-c", "cdate": 1609459200000, "mdate": 1681686722145, "content": {"title": "Iterative Empirical Game Solving via Single Policy Best Response", "abstract": "Policy-Space Response Oracles (PSRO) is a general algorithmic framework for learning policies in multiagent systems by interleaving empirical game analysis with deep reinforcement learning (DRL). At each iteration, DRL is invoked to train a best response to a mixture of opponent policies. The repeated application of DRL poses an expensive computational burden as we look to apply this algorithm to more complex domains. We introduce two variations of PSRO designed to reduce the amount of simulation required during DRL training. Both algorithms modify how PSRO adds new policies to the empirical game, based on learned responses to a single opponent policy. The first, Mixed-Oracles, transfers knowledge from previous iterations of DRL, requiring training only against the opponent's newest policy. The second, Mixed-Opponents, constructs a pure-strategy opponent by mixing existing strategy's action-value estimates, instead of their policies. Learning against a single policy mitigates conflicting experiences on behalf of a learner facing an unobserved distribution of opponents. We empirically demonstrate that these algorithms substantially reduce the amount of simulation during training required by PSRO, while producing equivalent or better solutions to the game."}}
{"id": "R4aWTjmrEKM", "cdate": 1601308188165, "mdate": null, "content": {"title": "Iterative Empirical Game Solving via Single Policy Best Response", "abstract": "Policy-Space Response Oracles (PSRO) is a general algorithmic framework for learning policies in multiagent systems by interleaving empirical game analysis with deep reinforcement learning (DRL).\nAt each iteration, DRL is invoked to train a best response to a mixture of opponent policies.\nThe repeated application of DRL poses an expensive computational burden as we look to apply this algorithm to more complex domains.\nWe introduce two variations of PSRO designed to reduce the amount of simulation required during DRL training.\nBoth algorithms modify how PSRO adds new policies to the empirical game, based on learned responses to a single opponent policy.\nThe first, Mixed-Oracles, transfers knowledge from previous iterations of DRL, requiring training only against the opponent's newest policy.\nThe second, Mixed-Opponents, constructs a pure-strategy opponent by mixing existing strategy's action-value estimates, instead of their policies.\nLearning against a single policy mitigates conflicting experiences on behalf of a learner facing an unobserved distribution of opponents.\nWe empirically demonstrate that these algorithms substantially reduce the amount of simulation during training required by PSRO, while producing equivalent or better solutions to the game."}}
{"id": "JprHbSY3rFp", "cdate": 1577836800000, "mdate": null, "content": {"title": "Learning to Play against Any Mixture of Opponents", "abstract": "Intuitively, experience playing against one mixture of opponents in a given domain should be relevant for a different mixture in the same domain. We propose a transfer learning method, Q-Mixing, that starts by learning Q-values against each pure-strategy opponent. Then a Q-value for any distribution of opponent strategies is approximated by appropriately averaging the separately learned Q-values. From these components, we construct policies against all opponent mixtures without any further training. We empirically validate Q-Mixing in two environments: a simple grid-world soccer environment, and a complicated cyber-security game. We find that Q-Mixing is able to successfully transfer knowledge across any mixture of opponents. We next consider the use of observations during play to update the believed distribution of opponents. We introduce an opponent classifier -- trained in parallel to Q-learning, using the same data -- and use the classifier results to refine the mixing of Q-values. We find that Q-Mixing augmented with the opponent classifier function performs comparably, and with lower variance, than training directly against a mixed-strategy opponent."}}
{"id": "HybG4mbu-S", "cdate": 1514764800000, "mdate": null, "content": {"title": "Speaker Naming in Movies", "abstract": "We propose a new model for speaker naming in movies that leverages visual, textual, and acoustic modalities in an unified optimization framework. To evaluate the performance of our model, we introduce a new dataset consisting of six episodes of the Big Bang Theory TV show and eighteen full movies covering different genres. Our experiments show that our multimodal model significantly outperforms several competitive baselines on the average weighted F-score metric. To demonstrate the effectiveness of our framework, we design an end-to-end memory network model that leverages our speaker naming model and achieves state-of-the-art results on the subtitles task of the MovieQA 2017 Challenge."}}
{"id": "lprvS7U6v1a", "cdate": 1420070400000, "mdate": null, "content": {"title": "A unified framework for automatic wound segmentation and analysis with deep convolutional neural networks", "abstract": "Wound surface area changes over multiple weeks are highly predictive of the wound healing process. Furthermore, the quality and quantity of the tissue in the wound bed also offer important prognostic information. Unfortunately, accurate measurements of wound surface area changes are out of reach in the busy wound practice setting. Currently, clinicians estimate wound size by estimating wound width and length using a scalpel after wound treatment, which is highly inaccurate. To address this problem, we propose an integrated system to automatically segment wound regions and analyze wound conditions in wound images. Different from previous segmentation techniques which rely on handcrafted features or unsupervised approaches, our proposed deep learning method jointly learns task-relevant visual features and performs wound segmentation. Moreover, learned features are applied to further analysis of wounds in two ways: infection detection and healing progress prediction. To the best of our knowledge, this is the first attempt to automate long-term predictions of general wound healing progress. Our method is computationally efficient and takes less than 5 seconds per wound image (480 by 640 pixels) on a typical laptop computer. Our evaluations on a large-scale wound database demonstrate the effectiveness and reliability of the proposed system."}}
