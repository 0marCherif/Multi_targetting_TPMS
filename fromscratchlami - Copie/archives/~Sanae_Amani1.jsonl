{"id": "rAHB4qkWYz", "cdate": 1685624089533, "mdate": null, "content": {"title": "Scaling Distributed Multi-task Reinforcement Learning with Experience Sharing", "abstract": "Recently, DARPA launched the ShELL program, which aims to explore how experience sharing can benefit distributed lifelong learning agents in adapting to new challenges. In this paper, we address this issue by conducting both theoretical and empirical research on distributed multi-task reinforcement learning (RL), where a group of $N$ agents collaboratively solves $M$ tasks without prior knowledge of their identities. We approach the problem by formulating it as linearly parameterized contextual Markov decision processes (MDPs), where each task is represented by a context that specifies the transition dynamics and rewards. To tackle this problem, we propose an algorithm called DistMT-LSVI. First, the agents identify the tasks, and then they exchange information through a central server to derive $\\epsilon$-optimal policies for the tasks. Our research demonstrates that to achieve $\\epsilon$-optimal policies for all $M$ tasks, a single agent using DistMT-LSVI needs to run a total number of episodes that is at most $\\tilde{\\mathcal{O}}({d^3H^6(\\epsilon^{-2}+c_{\\rm sep}^{-2})}\\cdot M/N)$, where $c_{\\rm sep}>0$ is a constant representing task separability, $H$ is the horizon of each episode, and $d$ is the feature dimension of the dynamics and rewards. Notably, DistMT-LSVI improves the sample complexity of non-distributed settings by a factor of $1/N$, as each agent independently learns $\\epsilon$-optimal policies for all $M$ tasks using $\\tilde{\\mathcal{O}}(d^3H^6M\\epsilon^{-2})$ episodes. Additionally, we provide numerical experiments conducted on OpenAI Gym Atari environments that validate our theoretical findings."}}
{"id": "EBVMVA8qo7", "cdate": 1683880345916, "mdate": 1683880345916, "content": {"title": "A Generalization of Weighted Sparse Decomposition to Negative Weights", "abstract": "Sparse solutions of underdetermined linear systems of equations are widely used in different fields of signal processing. This problem can also be seen as a sparse decomposition problem. Traditional sparse decomposition gives the same priority to all atoms for being included in the decomposition or not. However, in some applications, one may want to assign different priorities to different atoms for being included in the decomposition. This results to the so called \u201cweighted sparse decomposition\u201d problem [Babaie-Zadeh et al. 2012]. However, Babaie-Zadeh et al. studied this problem only for positive weights; but in some applications (e.g. classification) better performance can be obtained if some weights become negative. In this paper, we consider \u201cweighted sparse decomposition\u201d problem in its general form (positive and negative weights). A tight uniqueness condition and some applications for the general case will be presented."}}
{"id": "R1MUxc2DeiP", "cdate": 1683880093574, "mdate": 1683880093574, "content": {"title": "Safe Reinforcement Learning with Linear Function Approximation", "abstract": "Safety in reinforcement learning has become increasingly important in recent years. Yet, existing solutions either fail to strictly avoid choosing unsafe actions, which may lead to catastrophic results in safety-critical systems, or fail to provide regret guarantees for settings where safety constraints need to be learned. In this paper, we address both problems by first modeling safety as an unknown linear cost function of states and actions, which must always fall below a certain threshold. We then present algorithms, termed SLUCB-QVI and RSLUCB-QVI, for finite-horizon Markov decision processes (MDPs) with linear function approximation. We show that SLUCB-QVI and RSLUCB-QVI, while with \\emph{no safety violation}, achieve a $\\tilde{\\mathcal{O}}\\left(\\kappa\\sqrt{d^3H^3T}\\right)$ regret, nearly matching that of state-of-the-art unsafe algorithms, where $H$ is the duration of each episode, $d$ is the dimension of the feature mapping, $\\kappa$ is a constant characterizing the safety constraints, and $T$ is the total number of action played. We further present numerical simulations that corroborate our theoretical findings."}}
{"id": "ysG-BDYyp3u", "cdate": 1683879840181, "mdate": 1683879840181, "content": {"title": "Doubly pessimistic algorithms for strictly safe off-policy optimization", "abstract": "We study offline reinforcement learning (RL) in the presence of safety requirements: from a dataset collected a priori and without direct access to the true environment, learn an optimal policy that is guaranteed to respect the safety constraints. We address this problem by modeling the safety requirement as an unknown cost function of states and actions, whose expected value with respect to the policy must fall below a certain threshold. We then present an algorithm in the context of finite-horizon Markov decision processes (MDPs), termed Safe-DPVI that performs in a doubly pessimistic manner when 1) it constructs a conservative set of safe policies; and 2) when it selects a good policy from that conservative set. Without assuming the sufficient coverage of the dataset or any structure for the underlying MDPs, we establish a data-dependent upper bound on the suboptimality gap of the safe policy Safe-DPVI returns. We then specialize our results to linear MDPs with appropriate assumptions on dataset being well-explored. Both data-dependent and specialized bounds nearly match that of state-of-the-art unsafe offline RL algorithms, with an additional multiplicative factor $\\frac{\\sum_{h=1}^H \\alpha_h}{H}$, where $\\alpha_h$ characterizes the safety constraint at time-step h. We further present numerical\nsimulations that corroborate our theoretical findings."}}
{"id": "Qd0p0bl-A9t", "cdate": 1663850196800, "mdate": null, "content": {"title": "Provably Efficient Lifelong Reinforcement Learning with Linear Representation", "abstract": "We theoretically study lifelong reinforcement learning (RL) with linear representation in a regret minimization setting. The goal of the agent is to learn a multi-task policy based on a linear representation while solving a sequence of tasks that may be adaptively chosen based on the agent's past behaviors. We frame the problem as a linearly parameterized contextual Markov decision process (MDP), where each task is specified by a context and the transition dynamics is context-independent, and we introduce a new completeness-style assumption on the representation which is sufficient to ensure the optimal multi-task policy is realizable under the linear representation. Under this assumption, we propose an algorithm, called UCB Lifelong Value Distillation (UCBlvd), that provably achieves sublinear regret for any sequence of tasks while using only sublinear planning calls. Specifically, for $K$ task episodes of horizon $H$, our algorithm has a regret bound $\\tilde{\\mathcal{O}}(\\sqrt{(d^3+d^\\prime d)H^4K})$ using $\\mathcal{O}(dH\\log(K))$ number of planning calls, where $d$ and $d^\\prime$ are the feature dimensions of the dynamics and rewards, respectively. This theoretical guarantee implies that our algorithm can enable a lifelong learning agent to learn to internalize experiences into a multi-task policy and rapidly solve new tasks."}}
{"id": "Jhp38rtUTV", "cdate": 1621630084713, "mdate": null, "content": {"title": "UCB-based Algorithms for Multinomial Logistic Regression Bandits", "abstract": "Out of the rich family of generalized linear bandits, perhaps the most well studied ones are logistic bandits that are used in problems with binary rewards: for instance, when the learner aims to maximize the profit over a user that can select one of two possible outcomes (e.g., `click' vs `no-click'). Despite remarkable recent progress and improved algorithms for logistic bandits, existing works do not address practical situations where the number of outcomes that can be selected by the user is larger than two (e.g., `click', `show me later', `never show again', `no click'). In this paper, we study such an extension. We use multinomial logit (MNL) to model the probability of each one of $K+1\\geq 2$ possible outcomes (+1 stands for the `not click' outcome): we assume that for a learner's action $\\mathbf{x}_t$, the user selects one of $K+1\\geq 2$ outcomes, say outcome $i$, with a MNL probabilistic model with corresponding unknown parameter $\\bar{\\boldsymbol{\\theta}}_{\\ast i}$. Each outcome $i$ is also associated with a revenue parameter $\\rho_i$ and the goal is to maximize the expected revenue. For this problem, we present MNL-UCB, an upper confidence bound (UCB)-based algorithm, that achieves regret $\\tilde{\\mathcal{O}}(dK\\sqrt{T})$ with small dependency on problem-dependent constants that can otherwise be arbitrarily large and lead to loose regret bounds. We present numerical simulations that corroborate our theoretical results."}}
{"id": "UDAOqa7QBgG", "cdate": 1620331061125, "mdate": null, "content": {"title": "Regret Bound for Safe Gaussian Process Bandit Optimization", "abstract": "Many applications require a learner to make sequential decisions given uncertainty regarding both the system\u2019s payoff function and safety constraints. In safety-critical systems, it is paramount that the learner\u2019s actions do not violate the safety constraints at any stage of the learning process. In this paper, we study a stochastic bandit optimization problem where the unknown payoff and constraint functions are sampled from Gaussian Processes (GPs) first considered in [Srinivas et al., 2010]. We develop a safe variant of GP-UCB called SGP-UCB, with necessary modifications to respect safety constraints at every round. The algorithm has two distinct phases. The first phase seeks to estimate the set of safe actions in the decision set, while the second phase follows the GP-UCB decision rule. Our main contribution is to derive the first sub-linear regret bounds for this problem. We numerically compare SGP-UCB against existing safe Bayesian GP optimization algorithms."}}
{"id": "7v34C8fciH-", "cdate": 1620330920230, "mdate": null, "content": {"title": "Generalized Linear Bandits with Safety Constraints", "abstract": "The classical multi-armed bandit is a class of sequential decision making problems where selecting actions incurs costs that are sampled independently from an unknown underlying distribution. Bandit algorithms have many applications in safety critical systems, where several constraints must be respected during the run of the algorithm in spite of uncertainty about problem parameters. This paper formulates a generalized linear stochastic multi-armed bandit problem with generalized linear safety constraints that depend on an unknown parameter vector. In this setting, we propose a Safe UCB-GLM algorithm for which we provide general and problem-dependent regret bounds."}}
{"id": "LPijAbxuK4d", "cdate": 1620330681940, "mdate": null, "content": {"title": "Safe Linear Thompson Sampling with Side Information", "abstract": "The design and performance analysis of bandit algorithms in the presence of stage-wise safety or reliability constraints has recently garnered significant interest. In this work, we consider the linear stochastic bandit problem under additional \\textit{linear safety constraints} that need to be satisfied at each round. We provide a new safe algorithm based on linear Thompson Sampling (TS) for this problem and show a frequentist regret of order $\\mathcal{O} (d^{3/2}\\log^{1/2}d \\cdot T^{1/2}\\log^{3/2}T)$, which remarkably matches the results provided by \\cite{abeille2017linear} for the standard linear TS algorithm in the absence of safety constraints. We compare the performance of our algorithm with a UCB-based safe algorithm and highlight how the inherently randomized nature of TS leads to a superior performance in expanding the set of safe actions the algorithm has access to at each round."}}
{"id": "AhiC_giDSel", "cdate": 1620330562568, "mdate": null, "content": {"title": "Decentralized Multi-Agent Linear Bandits with Safety Constraints", "abstract": "We study decentralized stochastic linear bandits, where a network of $N$ agents acts cooperatively to efficiently solve a linear bandit-optimization problem over a $d$-dimensional space. For this problem, we propose DLUCB: a fully decentralized algorithm that minimizes the cumulative regret over the entire network. At each round of the algorithm each agent chooses its actions following an upper confidence bound (UCB) strategy and agents share information with their immediate neighbors through a carefully designed consensus procedure that repeats over cycles. Our analysis adjusts the duration of these communication cycles ensuring near-optimal regret performance $\\mathcal{O}(d\\log{NT}\\sqrt{NT})$ at a communication rate of $\\mathcal{O}(dN^2)$ per round. The structure of the network affects the regret performance via a small additive term \u2013 coined the regret of delay \u2013 that depends on the spectral gap of the underlying graph. Notably, our results apply to arbitrary network topologies without a requirement for a dedicated agent acting as a server. In consideration of situations with high communication cost, we propose RC-DLUCB: a modification of DLUCB with rare communication among agents. The new algorithm trades off regret performance for a significantly reduced total communication cost of $\\mathcal{O}(d^3N^{2.5})$ over all $T$ rounds. Finally, we show that our ideas extend naturally to the emerging, albeit more challenging, setting of safe bandits. For the recently studied problem of\nlinear bandits with unknown linear safety constraints, we propose the first safe decentralized algorithm. Our study contributes towards applying bandit techniques in safety-critical distributed systems that repeatedly deal with unknown stochastic environments. We present numerical simulations for various network topologies that corroborate our theoretical findings."}}
