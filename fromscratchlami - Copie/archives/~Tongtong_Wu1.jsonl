{"id": "u9-wIM4meXR", "cdate": 1640995200000, "mdate": 1681650942841, "content": {"title": "Variational Autoencoder with Disentanglement Priors for Low-Resource Task-Specific Natural Language Generation", "abstract": ""}}
{"id": "t-BznSScxyM", "cdate": 1640995200000, "mdate": 1667372007645, "content": {"title": "Pretrained Language Model in Continual Learning: A Comparative Study", "abstract": "Continual learning (CL) is a setting in which a model learns from a stream of incoming data while avoiding to forget previously learned knowledge. Pre-trained language models (PLMs) have been successfully employed in continual learning of different natural language problems. With the rapid development of many continual learning methods and PLMs, understanding and disentangling their interactions become essential for continued improvement of continual learning performance. In this paper, we thoroughly compare the continual learning performance over the combination of 5 PLMs and 4 CL approaches on 3 benchmarks in 2 typical incremental settings. Our extensive experimental analyses reveal interesting performance differences across PLMs and across CL methods. Furthermore, our representativeness probing analyses dissect PLMs\u2019 performance characteristics in a layer-wise and task-wise manner, uncovering the extent to which their inner layers suffer from forgetting, and the effect of different CL approaches on each layer. Finally, our observations and analyses open up a number of important research questions that will inform and guide the design of effective continual learning techniques."}}
{"id": "PSWJYgssP1", "cdate": 1640995200000, "mdate": 1681806971763, "content": {"title": "Event Causality Identification via Derivative Prompt Joint Learning", "abstract": ""}}
{"id": "A8rvL3UdvNK", "cdate": 1640995200000, "mdate": 1681806971739, "content": {"title": "Towards relation extraction from speech", "abstract": ""}}
{"id": "figzpGMrdD", "cdate": 1632875649383, "mdate": null, "content": {"title": "Pretrained Language Model in Continual Learning: A Comparative Study", "abstract": "Continual learning (CL) is a  setting in which a model learns from a stream of incoming data while avoiding to forget previously learned knowledge. Pre-trained language models (PLMs) have been successfully employed in continual learning of different natural language problems. With the rapid development of many continual learning methods and PLMs, understanding and disentangling their interactions become essential for continued improvement of continual learning performance. In this paper, we thoroughly compare the continual learning performance over the combination of 5 PLMs and 4 CL approaches on 3 benchmarks in 2 typical incremental settings. Our extensive experimental analyses reveal interesting performance differences across PLMs and across CL methods. Furthermore, our representativeness probing analyses dissect PLMs\u2019 performance characteristics in a layer-wise and task-wise manner, uncovering the extent to which their inner layers suffer from forgetting, and the effect of different CL approaches on each layer. Finally, our observations and analyses open up a number of important research questions that will inform and guide the design of effective continual learning techniques."}}
{"id": "r_-Ulm1NSTF", "cdate": 1609459200000, "mdate": 1667372007672, "content": {"title": "Curriculum-Meta Learning for Order-Robust Continual Relation Extraction", "abstract": "Continual relation extraction is an important task that focuses on extracting new facts incrementally from unstructured text. Given the sequential arrival order of the relations, this task is prone to two serious challenges, namely catastrophic forgetting and order-sensitivity. We propose a novel curriculum-meta learning method to tackle the above two challenges in continual relation extraction. We combine meta learning and curriculum learning to quickly adapt model parameters to a new task and to reduce interference of previously seen tasks on the current task. We design a novel relation representation learning method through the distribution of domain and range types of relations. Such representations are utilized to quantify the difficulty of tasks for the construction of curricula. Moreover, we also present novel difficulty-based metrics to quantitatively measure the extent of order-sensitivity of a given model, suggesting new ways to evaluate model robustness. Our comprehensive experiments on three benchmark datasets show that our proposed method outperforms the state-of-the-art techniques. The code is available at the anonymous GitHub repository: https://github.com/wutong8023/AAAI_CML."}}
{"id": "bLEwr50_Hgf", "cdate": 1609459200000, "mdate": 1667372007857, "content": {"title": "Adaptive Knowledge-Enhanced Bayesian Meta-Learning for Few-shot Event Detection", "abstract": ""}}
