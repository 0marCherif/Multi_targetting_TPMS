{"id": "nz2iUi-iZLQ", "cdate": 1621630035063, "mdate": null, "content": {"title": "Structural Credit Assignment in Neural Networks using Reinforcement Learning", "abstract": "Structural credit assignment in neural networks is a long-standing problem, with a variety of alternatives to backpropagation proposed to allow for local training of nodes. One of the early strategies was to treat each node as an agent and use a reinforcement learning method called REINFORCE to update each node locally with only a global reward signal. In this work, we revisit this approach and investigate if we can leverage other reinforcement learning approaches to improve learning. We first formalize training a neural network as a finite-horizon reinforcement learning problem and discuss how this facilitates using ideas from reinforcement learning like off-policy learning. We show that the standard on-policy REINFORCE approach, even with a variety of variance reduction approaches, learns suboptimal solutions. We introduce an off-policy approach, to facilitate reasoning about the greedy action for other agents and help overcome stochasticity in other agents. We conclude by showing that these networks of agents can be more robust to correlated samples when learning online."}}
{"id": "5dSfJh2_KBa", "cdate": 1577836800000, "mdate": null, "content": {"title": "Asynchronous Coagent Networks", "abstract": "Coagent policy gradient algorithms (CPGAs) are reinforcement learning algorithms for training a class of stochastic neural networks called coagent networks. In this work, we prove that CPGAs conver..."}}
{"id": "rkWCLs-_Wr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Learning Action Representations for Reinforcement Learning", "abstract": "Most model-free reinforcement learning methods leverage state representations (embeddings) for generalization, but either ignore structure in the space of actions or assume the structure is provide..."}}
{"id": "OmUMWaqeRe6", "cdate": 1546300800000, "mdate": null, "content": {"title": "Classical Policy Gradient: Preserving Bellman's Principle of Optimality", "abstract": "We propose a new objective function for finite-horizon episodic Markov decision processes that better captures Bellman's principle of optimality, and provide an expression for the gradient of the objective."}}
