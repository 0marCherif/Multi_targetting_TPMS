{"id": "u30A3Q6T7Rh", "cdate": 1681833046109, "mdate": null, "content": {"title": "Online Laplace Model Selection Revisited", "abstract": "The Laplace approximation provides a closed-form model selection objective for neural networks (NN). Online variants, which optimise NN parameters jointly with hyperparameters, like weight decay strength, have seen renewed interest in the Bayesian deep learning community. However, these methods violate Laplace\u2019s method\u2019s critical assumption that the approximation is performed around a mode of the loss, calling into question their soundness. This work re-derives online Laplace methods, showing them to target a variational bound on a mode-corrected variant of the Laplace evidence which does not make stationarity assumptions. Online Laplace and its mode-corrected counterpart share stationary points where 1. the NN parameters are a maximum a posteriori, satisfying the Laplace method\u2019s assumption, and 2. the hyperparameters maximise the Laplace evidence, motivating online methods. We demonstrate that these optima are roughly attained in practise by online algorithms using full-batch gradient descent on UCI regression datasets. The optimised hyperparameters prevent overfitting and outperform validation-based early stopping."}}
{"id": "20dw9d0qlP", "cdate": 1672655960204, "mdate": 1672655960204, "content": {"title": "Sampling-based inference for large linear models, with application to linearised Laplace", "abstract": "Large-scale linear models are ubiquitous throughout machine learning, with con- temporary application as surrogate models for neural network uncertainty quan- tification; that is, the linearised Laplace method. Alas, the computational cost associated with Bayesian linear models constrains this method\u2019s application to small networks, small output spaces and small datasets. We address this limitation by introducing a scalable sample-based Bayesian inference method for conjugate Gaussian multi-output linear models, together with a matching method for hy- perparameter (regularisation) selection. Furthermore, we use a classic feature normalisation method (the g-prior) to resolve a previously highlighted pathology of the linearised Laplace method. Together, these contributions allow us to perform linearised neural network inference with ResNet-18 on CIFAR100 (11M parame- ters, 100 output dimensions \u00d7 50k datapoints) and with a U-Net on a high-resolution\ntomographic reconstruction task (2M parameters, 251k output dimensions)."}}
{"id": "6DPVXzjnbDK", "cdate": 1664815579658, "mdate": null, "content": {"title": "Deep End-to-end Causal Inference", "abstract": "Causal inference is essential for data-driven decision making across domains such as business engagement, medical treatment and policy making.  However, research on causal discovery has evolved separately from causal inference, preventing straightforward combination of methods from both fields. In this work, we develop Deep End-to-end Causal Inference (DECI), a non-linear additive noise model with neural network functional relationships that takes in observational data and can perform both causal discovery and inference, including conditional average treatment effect (CATE) estimation. We provide a theoretical guarantee that DECI can asymptotically recover the ground truth causal graph and treatment effects when correctly specified. Our results show the competitive performance of DECI when compared to relevant baselines for both causal discovery and (C)ATE estimation in over a thousand experiments on both synthetic datasets and causal machine learning benchmarks."}}
{"id": "Ff1N3et1IV", "cdate": 1664194183186, "mdate": null, "content": {"title": "Learning Generative Models with Invariance to Symmetries", "abstract": "While imbuing a model with invariance under symmetry transformations can improve data efficiency and predictive performance, most methods require specialised architectures and, thus, prior knowledge of the symmetries. Unfortunately, we don\u2019t always know what symmetries are present in the data. Recent work has solved this problem by jointly learning the invariance (or the degree of invariance) with the model from the data alone. But, this work has focused on discriminative models. We describe a method for learning invariant generative models. We demonstrate that our method can learn a generative model of handwritten digits that is invariant to rotation."}}
{"id": "aoDyX6vSqsd", "cdate": 1663850290586, "mdate": null, "content": {"title": "Sampling-based inference for large linear models, with application to linearised Laplace", "abstract": "Large-scale linear models are ubiquitous throughout machine learning, with contemporary application as surrogate models for neural network uncertainty quantification; that is, the linearised Laplace method. Alas, the computational cost associated with Bayesian linear models constrains this method's application to small networks, small output spaces and small datasets. We address this limitation by introducing a scalable sample-based Bayesian inference method for conjugate Gaussian multi-output linear models, together with a matching method for hyperparameter (regularisation) selection. Furthermore, we use a classic feature normalisation method (the g-prior) to resolve a previously highlighted pathology of the linearised Laplace method. Together, these contributions allow us to perform linearised neural network inference with ResNet-18 on CIFAR100 (11M parameters, 100 output dimensions \u00d7 50k datapoints) and with a U-Net on a high-resolution tomographic reconstruction task (2M parameters, 251k output dimensions)."}}
{"id": "220Z65e1uc3", "cdate": 1663840876341, "mdate": 1663840876341, "content": {"title": "Adapting the Linearised Laplace Model Evidence for Modern Deep Learning", "abstract": "The linearised Laplace method for estimating model uncertainty has received renewed attention in the Bayesian deep learning community. The method provides reliable error bars and admits a closed form expression for the model evidence, allowing for scalable selection of model hyperparameters. In this work, we examine the assumptions behind this method, particularly in conjunction with model selection. We show that these interact poorly with some now-standard tools of deep learning\u2014stochastic approximation methods and normalisation layers\u2014and make recommendations for how to better adapt this classic method to the modern setting. We provide theoretical support for our recommendations and validate them empirically on MLPs, classic CNNs, residual networks with and without normalisation\nlayers, generative autoencoders and transformers."}}
{"id": "uUH8x-h9zdB", "cdate": 1637576010106, "mdate": null, "content": {"title": "Linearised Laplace Inference in Networks with Normalisation Layers and the Neural g-Prior", "abstract": "We show that for neural networks (NN) with normalisation layers, i.e. batch norm, layer norm, or group norm, the Laplace model evidence does not approximate the volume of a posterior mode and is thus unsuitable for model selection. We instead propose to use the Laplace evidence of the linearized network, which is robust to the presence of these layers. We also identify heterogeneity in the scale of Jacobian entries corresponding to different weights. We ameliorate this issue by extending the scale-invariant g-prior to NNs. We demonstrate these methods on toy regression, and image classification with a CNN."}}
{"id": "qtFPfwJWowM", "cdate": 1637576009674, "mdate": null, "content": {"title": "A Probabilistic Deep Image Prior over Image Space", "abstract": "The deep image prior regularises under-specified image reconstruction problems by reparametrising the target image as the output of a CNN. We induce a prior over images by scoring CNN outputs using a classical image reconstruction regulariser. We translate this functional prior into weight space using a change of variables and propose an efficient linearised Laplace inference algorithm. Hyperparameters are optimised with Type-II MAP. We obtain pixel-wise uncertainty estimates, which we show to be calibrated to the reconstruction error."}}
{"id": "gVi-oIwRIks", "cdate": 1632328762192, "mdate": null, "content": {"title": "Addressing Bias in Active Learning with Depth Uncertainty Networks... or Not", "abstract": "Farquhar et al. [2021] show that correcting for active learning bias with underparameterised models leads to improved downstream performance. For overparameterised models such as NNs, however, correction leads either to decreased or unchanged performance. They suggest that this is due to an \u201coverfitting bias\u201d which offsets the active learning bias. We show that depth uncertainty networks operate in a low overfitting regime, much like underparameterised models. They should therefore see an increase in performance with bias correction. Surprisingly, they do not. We propose that this negative result, as well as the results Farquhar et al. [2021], can be explained via the lens of the bias-variance decomposition of generalisation error."}}
{"id": "WqMlMsZ07A", "cdate": 1606146138302, "mdate": null, "content": {"title": "Expressive yet Tractable Bayesian Deep Learning via Subnetwork Inference", "abstract": "Scaling Bayesian inference to the large parameter spaces of deep neural networks requires restrictive approximations. We propose performing inference over only a small subset of the model parameters while keeping all others as point estimates. This enables us to use expressive posterior approximations that are intractable in the full model. In particular, we develop a practical and scalable Bayesian deep learning method that first trains a point estimate, and then infers a full covariance Gaussian posterior approximation over a subnetwork. We propose a subnetwork selection procedure which aims to optimally preserve posterior uncertainty. Empirical studies demonstrate the effectiveness of our approach."}}
