{"id": "M5gEdo_gMY_", "cdate": 1633378001596, "mdate": 1633378001596, "content": {"title": "Knowledge Distillation with Adaptive Asymmetric Label Sharpening for Semi-supervised Fracture Detection in Chest X-rays", "abstract": "Exploiting available medical records to train high performance computer-aided diagnosis (CAD) models via the semi-supervised learning (SSL) setting is emerging to tackle the prohibitively high labor costs involved in large-scale medical image annotations. Despite the extensive attentions received on SSL, previous methods failed to 1) account for the low disease prevalence in medical records and 2) utilize the image-level diagnosis indicated from the medical records. Both issues are unique to SSL for CAD models. In this work, we propose a new knowledge distillation method that effectively exploits large-scale image-level labels extracted from the medical records, augmented with limited expert annotated region-level labels, to train a rib and clavicle fracture CAD model for chest X-ray (CXR). Our method leverages the teacher-student model paradigm and features a novel adaptive asymmetric label sharpening (AALS) algorithm to address the label imbalance problem that specially exists in medical domain. Our approach is extensively evaluated on all CXR (N = 65,845) from the trauma registry of Chang Gung Memorial Hospital over a period of 9 years (2008-2016), on the most common rib and clavicle fractures. The experiment results demonstrate that our method achieves the state-of-the-art fracture detection performance, i.e., an area under receiver operating characteristic curve (AUROC) of 0.9318 and a free-response receiver operating characteristic (FROC) score of 0.8914 on the rib fractures, significantly outperforming previous approaches by an AUROC gap of 1.63% and an FROC improvement by 3.74%. Consistent performance gains are also observed for clavicle fracture detection."}}
{"id": "V2QLLsXaR_", "cdate": 1631225178393, "mdate": 1631225178393, "content": {"title": "Contour Transformer Network for One-shot Segmentation of Anatomical Structures", "abstract": "Accurate segmentation of anatomical structures is vital for medical image analysis. The state-of-the art accuracy is typically achieved by supervised learning methods, where gathering the requisite expert-labeled image annotations in a scalable manner remains a main obstacle. Therefore, annotation-efficient methods that permit to produce accurate anatomical structure segmentation are highly desirable. In this work, we present Contour Transformer Network (CTN), a one-shot anatomy segmentation method with a naturally built-in human-in-the-loop mechanism. We formulate anatomy segmentation as a contour evolution process and model the evolution behavior by graph convolutional networks (GCNs). Training the CTN model requires only one labeled image exemplar and leverages additional unlabeled data through newly introduced loss functions that measure the global shape and appearance consistency of contours. On segmentation tasks of four different anatomies, we demonstrate that our one-shot learning method significantly outperforms non-learning based methods and performs competitively to the state-of-the-art fully supervised deep learning methods. With minimal human-in-the-loop editing feedback, the segmentation performance can be further improved to surpass the fully supervised methods."}}
{"id": "YUdTmj5uFzg", "cdate": 1631224665000, "mdate": 1631224665000, "content": {"title": "Semi-Supervised Learning for Bone Mineral Density Estimation in Hip X-ray Images", "abstract": "Bone mineral density (BMD) is a clinically critical indicator of osteoporosis, usually measured by dual-energy X-ray absorptiometry (DEXA). Due to the limited accessibility of DEXA machines and examinations, osteoporosis is often under-diagnosed and under-treated, leading to increased fragility fracture risks. Thus it is highly desirable to obtain BMDs with alternative cost-effective and more accessible medical imaging examinations such as X-ray plain films. In this work, we formulate the BMD estimation from plain hip X-ray images as a regression problem. Specifically, we propose a new semi-supervised self-training algorithm to train a BMD regression model using images coupled with DEXA measured BMDs and unlabeled images with pseudo BMDs. Pseudo BMDs are generated and refined iteratively for unlabeled images during self-training. We also present a novel adaptive triplet loss to improve the model's regression accuracy. On an in-house dataset of 1,090 images (819 unique patients), our BMD estimation method achieves a high Pearson correlation coefficient of 0.8805 to ground-truth BMDs. It offers good feasibility to use the more accessible and cheaper X-ray imaging for opportunistic osteoporosis screening."}}
{"id": "0wblcjbC2sN", "cdate": 1617712206295, "mdate": null, "content": {"title": "Interpretable Medical Image Classification with Self-Supervised Anatomical Embedding and Prior Knowledge", "abstract": "In medical image analysis tasks, it is important to make machine learning models focus on correct anatomical locations, so as to improve interpretability and robustness of the model. We adopt a latest algorithm called self-supervised anatomical embedding (SAM) to locate point of interest (POI) on computed tomography (CT) scans. SAM can detect arbitrary POI with only one labeled sample needed. Then, we can extract targeted features from the POIs to train a simple prediction model guided by clinical prior knowledge. This approach mimics the practice of human radiologists, thus is interpretable, controllable, and robust. We illustrate our approach on the application of CT contrast phase classification and it outperforms an existing deep learning based method trained on the whole image."}}
{"id": "tKgwFur5Fn", "cdate": 1581710342613, "mdate": null, "content": {"title": "Deep Esophageal Clinical Target Volume Delineation using Encoded 3D Spatial Context of Tumors, Lymph Nodes, and Organs At Risk", "abstract": "Clinical target volume (CTV) delineation from radiotherapy\ncomputed tomography (RTCT) images is used to define the treatment\nareas containing the gross tumor volume (GTV) and/or sub-clinical malignant disease for radiotherapy (RT). High intra- and inter-user variability makes this a particularly difficult task for esophageal cancer. This\nmotivates automated solutions, which is the aim of our work. Because\nCTV delineation is highly context-dependent\u2014it must encompass the\nGTV and regional lymph nodes (LNs) while also avoiding excessive exposure to the organs at risk (OARs)\u2014we formulate it as a deep contextual appearance-based problem using encoded spatial contexts of these\nanatomical structures. This allows the deep network to better learn from\nand emulate the margin- and appearance-based delineation performed by\nhuman physicians. Additionally, we develop domain-specific data augmentation to inject robustness to our system. Finally, we show that a\nsimple 3D progressive holistically nested network (PHNN), which avoids\ncomputationally heavy decoding paths while still aggregating features at\ndifferent levels of context, can outperform more complicated networks.\nCross-validated experiments on a dataset of 135 esophageal cancer patients demonstrate that our encoded spatial context approach can produce concrete performance improvements, with an average Dice score of\n83\n.\n9\n\u00b1\n5\n.4% and an average surface distance of 4\n.\n2\n\u00b1\n2\n.7 mm, representing\nimprovements of 3\n.8% and 2\n.4mm, respectively, over the state-of-the-art\napproach.\n"}}
{"id": "ZIYTakRCjK", "cdate": 1581710276769, "mdate": null, "content": {"title": "Accurate Esophageal Gross Tumor Volume Segmentation in PET/CT using Two-Stream Chained 3D Deep Network Fusion", "abstract": ". Gross tumor volume (GTV) segmentation is a critical step\nin esophageal cancer radiotherapy treatment planning. Inconsistencies\nacross oncologists and prohibitive labor costs motivate automated approaches for this task. However, leading approaches are only applied\nto radiotherapy computed tomography (RTCT) images taken prior to\ntreatment. This limits the performance as RTCT suffers from low contrast between the esophagus, tumor, and surrounding tissues. In this\npaper, we aim to exploit both RTCT and positron emission tomography\n(PET) imaging modalities to facilitate more accurate GTV segmentation. By utilizing PET, we emulate medical professionals who frequently\ndelineate GTV boundaries through observation of the RTCT images obtained after prescribing radiotherapy and PET/CT images acquired earlier for cancer staging. To take advantage of both modalities, we present a\ntwo-stream chained segmentation approach that effectively fuses the CT\nand PET modalities via early and late 3D deep-network-based fusion.\nFurthermore, to effect the fusion and segmentation we propose a simple\nyet effective progressive semantically nested network (PSNN) model that\noutperforms more complicated models. Extensive 5-fold cross-validation\non 110 esophageal cancer patients, the largest analysis to date, demonstrates that both the proposed two-stream chained segmentation pipeline\nand the PSNN model can significantly improve the quantitative performance over the previous state-of-the-art work by 11% in absolute Dice\nscore (DSC) (from 0\n.654\n\u00b1\n0\n.210 to 0\n.764\n\u00b1\n0\n.134) and, at the same time,\nreducing the Hausdorff distance from 129\n\u00b1 73mm to 47\n\u00b1 56mm\n."}}
{"id": "WmKGQn1vR", "cdate": 1581710183904, "mdate": null, "content": {"title": "Lesion Harvester: Iteratively Mining Unlabeled Lesions and Hard-Negative Examples at Scale", "abstract": "Acquiring large-scale medical image data, necessary for training machine learning algorithms, is frequently\nintractable, due to prohibitive expert-driven annotation costs.\nRecent datasets extracted from hospital archives, e.g. DeepLesion,\nhave begun to address this problem. However, these are often\nincompletely or noisily labeled, e.g. DeepLesion leaves over 50%\nof its lesions unlabeled. Thus, effective methods to harvest missing\nannotations are critical for continued progress in medical image\nanalysis. This is the goal of our work, where we develop a powerful system to harvest missing lesions from the DeepLesion dataset\nat high precision. Accepting the need for some degree of expert\nlabor to achieve high fidelity, we exploit a small fully-labeled\nsubset of medical image volumes and use it to intelligently mine\nannotations from the remainder. To do this, we chain together\na highly sensitive lesion proposal generator and a very selective\nlesion proposal classifier. While our framework is generic, we\noptimize our performance by proposing a 3D contextual lesion\nproposal generator and by using a multi-view multi-scale lesion\nproposal classifier. These produce harvested and hard-negative\nproposals, which we then re-use to finetune our proposal generator by using a novel hard negative suppression loss, continuing\nthis process until no extra lesions are found. Extensive experimental analysis demonstrates that our method can harvest an\nadditional 9, 805 lesions while keeping precision above 90%. As\nthese harvested lesions represent a significant enhancement of an\nalready invaluable dataset, we publicly release them, along with\na new test set of completely annotated DeepLesion volumes. We\nalso present a pseudo 3D IoU evaluation metric, demonstrating\nthat it corresponds much closer to the real 3D IoU than the\ncurrent evaluation metric used in DeepLesion. To demonstrate\nthe benefits of our approach, we show that lesion detectors\ntrained on our harvested lesions can significantly outperform the\nsame variants only trained on the original annotations, with boost\nof average precision of 7 to 10%. We open source our annotations\nat https://github.com/JimmyCai91/DeepLesionAnnotation."}}
{"id": "E_OyljnBY8", "cdate": 1581710064240, "mdate": null, "content": {"title": "Weakly Supervised Universal Fracture Detection in Pelvic X-rays", "abstract": "Hip and pelvic fractures are serious injuries with life-threatening\ncomplications. However, diagnostic errors of fractures in pelvic X-rays\n(PXRs) are very common, driving the demand for computer-aided diagnosis (CAD) solutions. A major challenge lies in the fact that fractures\nare localized patterns that require localized analyses. Unfortunately, the\nPXRs residing in hospital picture archiving and communication system\ndo not typically specify region of interests. In this paper, we propose\na two-stage hip and pelvic fracture detection method that executes localized fracture classification using weakly supervised ROI mining. The\nfirst stage uses a large capacity fully-convolutional network, i.e., deep\nwith high levels of abstraction, in a multiple instance learning setting\nto automatically mine probable true positive and definite hard negative\nROIs from the whole PXR in the training data. The second stage trains\na smaller capacity model, i.e., shallower and more generalizable, with the\nmined ROIs to perform localized analyses to classify fractures. During\ninference, our method detects hip and pelvic fractures in one pass by\nchaining the probability outputs of the two stages together. We evaluate\nour method on 4 410 PXRs, reporting an area under the ROC curve value\nof 0\n.975, the highest among state-of-the-art fracture detection methods.\nMoreover, we show that our two-stage approach can perform comparably to human physicians (even outperforming emergency physicians and\nsurgeons), in a preliminary reader study of 23 readers."}}
{"id": "SkWsvJzdZS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Deep Lesion Graphs in the Wild: Relationship Learning and Organization of Significant Radiology Image Findings in a Diverse Large-Scale Lesion Database", "abstract": "Radiologists in their daily work routinely find and annotate significant abnormalities on a large number of radiology images. Such abnormalities, or lesions, have collected over years and stored in hospitals' picture archiving and communication systems. However, they are basically unsorted and lack semantic annotations like type and location. In this paper, we aim to organize and explore them by learning a deep feature representation for each lesion. A large-scale and comprehensive dataset, DeepLesion, is introduced for this task. DeepLesion contains bounding boxes and size measurements of over 32K lesions. To model their similarity relationship, we leverage multiple supervision information including types, self-supervised location coordinates, and sizes. They require little manual annotation effort but describe useful attributes of the lesions. Then, a triplet network is utilized to learn lesion embeddings with a sequential sampling strategy to depict their hierarchical similarity structure. Experiments show promising qualitative and quantitative results on lesion retrieval, clustering, and classification. The learned embeddings can be further employed to build a lesion graph for various clinically useful applications. An algorithm for intra-patient lesion matching is proposed and validated with experiments."}}
{"id": "H1byGR-OWB", "cdate": 1514764800000, "mdate": null, "content": {"title": "TieNet: Text-Image Embedding Network for Common Thorax Disease Classification and Reporting in Chest X-Rays", "abstract": "Chest X-rays are one of the most common radiological examinations in daily clinical routines. Reporting thorax diseases using chest X-rays is often an entry-level task for radiologist trainees. Yet, reading a chest X-ray image remains a challenging job for learning-oriented machine intelligence, due to (1) shortage of large-scale machine-learnable medical image datasets, and (2) lack of techniques that can mimic the high-level reasoning of human radiologists that requires years of knowledge accumulation and professional training. In this paper, we show the clinical free-text radiological reports can be utilized as a priori knowledge for tackling these two key problems. We propose a novel Text-Image Embedding network (TieNet) for extracting the distinctive image and text representations. Multi-level attention models are integrated into an end-to-end trainable CNN-RNN architecture for highlighting the meaningful text words and image regions. We first apply TieNet to classify the chest X-rays by using both image features and text embeddings extracted from associated reports. The proposed auto-annotation framework achieves high accuracy (over 0.9 on average in AUCs) in assigning disease labels for our hand-label evaluation dataset. Furthermore, we transform the TieNet into a chest X-ray reporting system. It simulates the reporting process and can output disease classification and a preliminary report together. The classification results are significantly improved (6% increase on average in AUCs) compared to the state-of-the-art baseline on an unseen and hand-labeled dataset (OpenI)."}}
