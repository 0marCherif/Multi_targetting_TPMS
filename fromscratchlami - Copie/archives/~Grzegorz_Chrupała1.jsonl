{"id": "CQnUe_l9nuR", "cdate": 1675250182781, "mdate": null, "content": {"title": "Putting Natural in Natural Language Processing", "abstract": "Human language is firstly spoken and only secondarily written.\n  Text, however, is a very convenient and efficient\n  representation of language, and modern civilization has made it\n  ubiquitous. Thus the field of NLP has\n  overwhelmingly focused on processing written rather than spoken\n  language. Work on spoken language, on the other hand, has been\n  siloed off within the largely separate speech processing community\n  which has been inordinately preoccupied with transcribing speech into\n  text.\n  Recent advances in deep learning have led to a fortuitous\n  convergence in methods between speech processing and mainstream NLP.\n  Arguably, the time is ripe for a unification of these two fields,\n  and for starting to take spoken language seriously as the primary\n  mode of human communication. Truly natural language processing\n  could lead to better integration with the rest of language science\n  and could lead to systems which are more data-efficient and more\n  human-like, and which can communicate beyond the textual\n  modality."}}
{"id": "s2yRXBlykw", "cdate": 1672531200000, "mdate": 1695975308321, "content": {"title": "Quantifying Context Mixing in Transformers", "abstract": "Self-attention weights and their transformed variants have been the main source of information for analyzing token-to-token interactions in Transformer-based models. But despite their ease of interpretation, these weights are not faithful to the models' decisions as they are only one part of an encoder, and other components in the encoder layer can have considerable impact on information mixing in the output representations. In this work, by expanding the scope of analysis to the whole encoder block, we propose Value Zeroing, a novel context mixing score customized for Transformers that provides us with a deeper understanding of how information is mixed at each encoder layer. We demonstrate the superiority of our context mixing score over other analysis methods through a series of complementary evaluations with different viewpoints based on linguistically informed rationales, probing, and faithfulness analysis."}}
{"id": "hRB08pV8yHf", "cdate": 1672531200000, "mdate": 1687855434242, "content": {"title": "Wave to Syntax: Probing spoken language models for syntax", "abstract": "Understanding which information is encoded in deep models of spoken and written language has been the focus of much research in recent years, as it is crucial for debugging and improving these architectures. Most previous work has focused on probing for speaker characteristics, acoustic and phonological information in models of spoken language, and for syntactic information in models of written language. Here we focus on the encoding of syntax in several self-supervised and visually grounded models of spoken language. We employ two complementary probing methods, combined with baselines and reference representations to quantify the degree to which syntactic structure is encoded in the activations of the target models. We show that syntax is captured most prominently in the middle layers of the networks, and more explicitly within models with more parameters."}}
{"id": "flchwwdm4G", "cdate": 1672531200000, "mdate": 1687855450130, "content": {"title": "Wave to Syntax: Probing spoken language models for syntax", "abstract": "Understanding which information is encoded in deep models of spoken and written language has been the focus of much research in recent years, as it is crucial for debugging and improving these architectures. Most previous work has focused on probing for speaker characteristics, acoustic and phonological information in models of spoken language, and for syntactic information in models of written language. Here we focus on the encoding of syntax in several self-supervised and visually grounded models of spoken language. We employ two complementary probing methods, combined with baselines and reference representations to quantify the degree to which syntactic structure is encoded in the activations of the target models. We show that syntax is captured most prominently in the middle layers of the networks, and more explicitly within models with more parameters."}}
{"id": "IBuvcjHrZKx", "cdate": 1672531200000, "mdate": 1687855434356, "content": {"title": "Quantifying Context Mixing in Transformers", "abstract": ""}}
{"id": "ABEB2UKry0", "cdate": 1672531200000, "mdate": 1687855434216, "content": {"title": "Putting Natural in Natural Language Processing", "abstract": "Human language is firstly spoken and only secondarily written. Text, however, is a very convenient and efficient representation of language, and modern civilization has made it ubiquitous. Thus the field of NLP has overwhelmingly focused on processing written rather than spoken language. Work on spoken language, on the other hand, has been siloed off within the largely separate speech processing community which has been inordinately preoccupied with transcribing speech into text. Recent advances in deep learning have led to a fortuitous convergence in methods between speech processing and mainstream NLP. Arguably, the time is ripe for a unification of these two fields, and for starting to take spoken language seriously as the primary mode of human communication. Truly natural language processing could lead to better integration with the rest of language science and could lead to systems which are more data-efficient and more human-like, and which can communicate beyond the textual modality."}}
{"id": "8QV_YRbM2f", "cdate": 1672531200000, "mdate": 1695975308328, "content": {"title": "Putting Natural in Natural Language Processing", "abstract": ""}}
{"id": "7C8UabnzLp", "cdate": 1672531200000, "mdate": 1687855450104, "content": {"title": "Putting Natural in Natural Language Processing", "abstract": "Human language is firstly spoken and only secondarily written. Text, however, is a very convenient and efficient representation of language, and modern civilization has made it ubiquitous. Thus the field of NLP has overwhelmingly focused on processing written rather than spoken language. Work on spoken language, on the other hand, has been siloed off within the largely separate speech processing community which has been inordinately preoccupied with transcribing speech into text. Recent advances in deep learning have led to a fortuitous convergence in methods between speech processing and mainstream NLP. Arguably, the time is ripe for a unification of these two fields, and for starting to take spoken language seriously as the primary mode of human communication. Truly natural language processing could lead to better integration with the rest of language science and could lead to systems which are more data-efficient and more human-like, and which can communicate beyond the textual modality."}}
{"id": "56Tpx7zMO-", "cdate": 1672531200000, "mdate": 1687855450040, "content": {"title": "Quantifying Context Mixing in Transformers", "abstract": ""}}
{"id": "xFCJuLeHhg2", "cdate": 1640995200000, "mdate": 1687855434288, "content": {"title": "Learning English with Peppa Pig", "abstract": "Recent computational models of the acquisition of spoken language via grounding in perception exploit associations between the spoken and visual modalities and learn to represent speech and visual data in a joint vector space. A major unresolved issue from the point of ecological validity is the training data, typically consisting of images or videos paired with spoken descriptions of what is depicted. Such a setup guarantees an unrealistically strong correlation between speech and the visual data.\u00a0 In the real world the coupling between the linguistic and the visual modality is loose, and often confounded by correlations with non-semantic aspects of the speech signal. Here we address this shortcoming by using a dataset based on the children's cartoon Peppa Pig.\u00a0 We train a simple bi-modal architecture on the portion of the data consisting of dialog between characters, and evaluate on segments containing descriptive narrations. Despite the weak and confounded signal in this training data our model succeeds at learning aspects of the visual semantics of spoken language."}}
