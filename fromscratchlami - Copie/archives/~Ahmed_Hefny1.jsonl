{"id": "lqHl-h-peo", "cdate": 1661316989886, "mdate": 1661316989886, "content": {"title": "Multipath++: EFFICIENT INFORMATION FUSION AND TRAJECTORY AGGREGATION FOR BEHAVIOR PREDICTION", "abstract": "Predicting the future behavior of road users is one of the most challenging and important problems\nin autonomous driving. Applying deep learning to this problem requires fusing heterogeneous\nworld state in the form of rich perception signals and map information, and inferring highly multimodal distributions over possible futures. In this paper, we present MultiPath++, a future prediction\nmodel that achieves state-of-the-art performance on popular benchmarks. MultiPath++ improves the\nMultiPath architecture [45] by revisiting many design choices. The first key design difference is a\ndeparture from dense image-based encoding of the input world state in favor of a sparse encoding of\nheterogeneous scene elements: MultiPath++ consumes compact and efficient polylines to describe\nroad features, and raw agent state information directly (e.g., position, velocity, acceleration). We\npropose a context-aware fusion of these elements and develop a reusable multi-context gating fusion\ncomponent. Second, we reconsider the choice of pre-defined, static anchors, and develop a way\nto learn latent anchor embeddings end-to-end in the model. Lastly, we explore ensembling and\noutput aggregation techniques\u2014common in other ML domains\u2014and find effective variants for\nour probabilistic multimodal output representation. We perform an extensive ablation on these\ndesign choices, and show that our proposed model achieves state-of-the-art performance on the\nArgoverse Motion Forecasting Competition [12] and the Waymo Open Dataset Motion Prediction\nChallenge [18]."}}
{"id": "ifQhiEUG4IV", "cdate": 1514764800000, "mdate": null, "content": {"title": "Query-based Workload Forecasting for Self-Driving Database Management Systems.", "abstract": "The first step towards an autonomous database management system (DBMS) is the ability to model the target application's workload. This is necessary to allow the system to anticipate future workload needs and select the proper optimizations in a timely manner. Previous forecasting techniques model the resource utilization of the queries. Such metrics, however, change whenever the physical design of the database and the hardware resources change, thereby rendering previous forecasting models useless. We present a robust forecasting framework called QueryBot 5000 that allows a DBMS to predict the expected arrival rate of queries in the future based on historical data. To better support highly dynamic environments, our approach uses the logical composition of queries in the workload rather than the amount of physical resources used for query execution. It provides multiple horizons (short- vs. long-term) with different aggregation intervals. We also present a clustering-based technique for reducing the total number of forecasting models to maintain. To evaluate our approach, we compare our forecasting models against other state-of-the-art models on three real-world database traces. We implemented our models in an external controller for PostgreSQL and MySQL and demonstrate their effectiveness in selecting indexes."}}
{"id": "SyVrb1Z_Zr", "cdate": 1514764800000, "mdate": null, "content": {"title": "An Efficient, Expressive and Local Minima-Free Method for Learning Controlled Dynamical Systems", "abstract": "We propose a framework for modeling and estimating the state of controlled dynamical systems, where an agent can affect the system through actions and receives partial observations. Based on this framework, we propose the Predictive State Representation with Random Fourier Features (RFFPSR). A key property in RFF-PSRs is that the state estimate is represented by a conditional distribution of future observations given future actions. RFF-PSRs combine this representation with moment-matching, kernel embedding and local optimization to achieve a method that enjoys several favorable qualities: It can represent controlled environments which can be affected by actions; it has an efficient and theoretically justified learning algorithm; it uses a non-parametric representation that has expressive power to represent continuous non-linear dynamics. We provide a detailed formulation, a theoretical analysis and an experimental evaluation that demonstrates the effectiveness of our method."}}
{"id": "SyNRNoWdbH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Recurrent Predictive State Policy Networks", "abstract": "We introduce Recurrent Predictive State Policy(RPSP) networks, a recurrent architecture that brings insights from predictive state representations to reinforcement learning in partially ob-servable..."}}
{"id": "sKiy4fwxCRL", "cdate": 1483228800000, "mdate": null, "content": {"title": "Rows versus Columns: Randomized Kaczmarz or Gauss-Seidel for Ridge Regression.", "abstract": "The Kaczmarz and Gauss--Seidel methods aim to solve an $m \\times n$ linear system $X{\\beta} = {y}$ by iteratively refining the solution estimate; the former uses random rows of $X$ to update ${\\beta}$ given the corresponding equations and the latter uses random columns of $X$ to update corresponding coordinates in ${\\beta}$. Recent work analyzed these algorithms in a parallel comparison for the overcomplete and undercomplete systems, showing convergence to the ordinary least squares (OLS) solution and the minimum Euclidean norm solution, respectively. This paper considers the natural follow-up to the OLS problem---ridge or Tikhonov regularized regression. By viewing them as variants of randomized coordinate descent, we present variants of the randomized Kaczmarz (RK) and randomized Gauss--Siedel (RGS) for solving this system and derive their convergence rates. We prove that a recent proposal, which can be interpreted as randomizing over both rows and columns, is strictly suboptimal---instead, one should always work with randomly selected columns (RGS) when $m > n$ (\\#rows $>$ \\#cols) and with randomly selected rows (RK) when $ n > m$ (\\#cols $>$ \\#rows)."}}
{"id": "Wlv3Wp2a5Aq", "cdate": 1483228800000, "mdate": null, "content": {"title": "Practical Learning of Predictive State Representations.", "abstract": "Over the past decade there has been considerable interest in spectral algorithms for learning Predictive State Representations (PSRs). Spectral algorithms have appealing theoretical guarantees; however, the resulting models do not always perform well on inference tasks in practice. One reason for this behavior is the mismatch between the intended task (accurate filtering or prediction) and the loss function being optimized by the algorithm (estimation error in model parameters). A natural idea is to improve performance by refining PSRs using an algorithm such as EM. Unfortunately it is not obvious how to apply apply an EM style algorithm in the context of PSRs as the Log Likelihood is not well defined for all PSRs. We show that it is possible to overcome this problem using ideas from Predictive State Inference Machines. We combine spectral algorithms for PSRs as a consistent and efficient initialization with PSIM-style updates to refine the resulting model parameters. By combining these two ideas we develop Inference Gradients, a simple, fast, and robust method for practical learning of PSRs. Inference Gradients performs gradient descent in the PSR parameter space to optimize an inference-based loss function like PSIM. Because Inference Gradients uses a spectral initialization we get the same consistency benefits as PSRs. We show that Inference Gradients outperforms both PSRs and PSIMs on real and synthetic data sets."}}
{"id": "SJErxv-dZH", "cdate": 1483228800000, "mdate": null, "content": {"title": "Predictive State Recurrent Neural Networks", "abstract": "We present a new model, Predictive State Recurrent Neural Networks (PSRNNs), for filtering and prediction in dynamical systems. PSRNNs draw on insights from both Recurrent Neural Networks (RNNs) and Predictive State Representations (PSRs), and inherit advantages from both types of models. Like many successful RNN architectures, PSRNNs use (potentially deeply composed) bilinear transfer functions to combine information from multiple sources. We show that such bilinear functions arise naturally from state updates in Bayes filters like PSRs, in which observations can be viewed as gating belief states. We also show that PSRNNs can be learned effectively by combining Backpropogation Through Time (BPTT) with an initialization derived from a statistically consistent learning algorithm for PSRs called two-stage regression (2SR). Finally, we show that PSRNNs can be factorized using tensor decomposition, reducing model size and suggesting interesting connections to existing multiplicative architectures such as LSTMs and GRUs. We apply PSRNNs to 4 datasets, and show that we outperform several popular alternative approaches to modeling dynamical systems in all cases."}}
{"id": "Sy4_qsZdZH", "cdate": 1451606400000, "mdate": null, "content": {"title": "Stochastic Variance Reduction for Nonconvex Optimization", "abstract": "We study nonconvex finite-sum problems and analyze stochastic variance reduced gradient (SVRG) methods for them. SVRG and related methods have recently surged into prominence for convex optimizatio..."}}
{"id": "Wua-gK36lSh", "cdate": 1420070400000, "mdate": null, "content": {"title": "A New View of Predictive State Methods for Dynamical System Learning.", "abstract": "Recently there has been substantial interest in spectral methods for learning dynamical systems. These methods are popular since they often offer a good tradeoff between computational and statistical efficiency. Unfortunately, they can be difficult to use and extend in practice: e.g., they can make it difficult to incorporate prior information such as sparsity or structure. To address this problem, we present a new view of dynamical system learning: we show how to learn dynamical systems by solving a sequence of ordinary supervised learning problems, thereby allowing users to incorporate prior knowledge via standard techniques such as L1 regularization. Many existing spectral methods are special cases of this new framework, using linear regression as the supervised learner. We demonstrate the effectiveness of our framework by showing examples where nonlinear regression or lasso let us learn better state representations than plain linear regression does; the correctness of these instances follows directly from our general analysis."}}
{"id": "BkNakwZOZr", "cdate": 1420070400000, "mdate": null, "content": {"title": "On Variance Reduction in Stochastic Gradient Descent and its Asynchronous Variants", "abstract": "We study optimization algorithms based on variance reduction for stochastic gradientdescent (SGD). Remarkable recent progress has been made in this directionthrough development of algorithms like SAG, SVRG, SAGA. These algorithmshave been shown to outperform SGD, both theoretically and empirically. However,asynchronous versions of these algorithms\u2014a crucial requirement for modernlarge-scale applications\u2014have not been studied. We bridge this gap by presentinga unifying framework that captures many variance reduction techniques.Subsequently, we propose an asynchronous algorithm grounded in our framework,with fast convergence rates. An important consequence of our general approachis that it yields asynchronous versions of variance reduction algorithms such asSVRG, SAGA as a byproduct. Our method achieves near linear speedup in sparsesettings common to machine learning. We demonstrate the empirical performanceof our method through a concrete realization of asynchronous SVRG."}}
