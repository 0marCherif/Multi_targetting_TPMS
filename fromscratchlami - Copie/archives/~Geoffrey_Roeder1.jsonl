{"id": "UHU_Q6lRPs", "cdate": 1693321627962, "mdate": 1693321627962, "content": {"title": "Probabilistic Graphical Models and Tensor Networks: A Hybrid Framework", "abstract": "We investigate a correspondence between two formalisms for discrete probabilistic\nmodeling: probabilistic graphical models (PGMs) and tensor networks (TNs), a\npowerful modeling framework for simulating complex quantum systems. The\ngraphical calculus of PGMs and TNs exhibits many similarities, with discrete\nundirected graphical models (UGMs) being a special case of TNs. However,\nmore general probabilistic TN models such as Born machines (BMs) employ\ncomplex-valued hidden states to produce novel forms of correlation among the\nprobabilities. While representing a new modeling resource for capturing structure in\ndiscrete probability distributions, this behavior also renders the direct application of\nstandard PGM tools impossible. We aim to bridge this gap by introducing a hybrid\nPGM-TN formalism that integrates quantum-like correlations into PGM models\nin a principled manner, using the physically-motivated concept of decoherence.\nWe first prove that applying decoherence to the entirety of a BM model converts it\ninto a discrete UGM, and conversely, that any subgraph of a discrete UGM can be\nrepresented as a decohered BM. This method allows a broad family of probabilistic\nTN models to be encoded as partially decohered BMs, a fact we leverage to\ncombine the representational strengths of both model families. We experimentally\nverify the performance of such hybrid models in a sequential modeling task, and\nidentify promising uses of our method within the context of existing applications\nof graphical models."}}
{"id": "LE43Bc3Pp3t", "cdate": 1621616609376, "mdate": null, "content": {"title": "Learning Composable Energy Surrogates for PDE Order Reduction", "abstract": "Meta-materials are an important emerging class of engineered materials in which complex macroscopic behaviour--whether electromagnetic, thermal, or mechanical--arises from modular substructure. Simulation and optimization of these materials are computationally challenging, as rich substructures necessitate high-fidelity finite element meshes to solve the governing PDEs. To address this, we leverage parametric modular structure to learn component-level surrogates, enabling cheaper high-fidelity simulation. We use a neural network to model the stored potential energy in a component given boundary conditions. This yields a structured prediction task: macroscopic behavior is determined by the minimizer of the system's total potential energy, which can be approximated by composing these surrogate models. Composable energy surrogates thus permit simulation in the reduced basis of component boundaries. Costly ground-truth simulation of the full structure is avoided, as training data are generated by performing finite element analysis with individual components. Using dataset aggregation to choose training boundary conditions allows us to learn energy surrogates which produce accurate macroscopic behavior when composed, accelerating simulation of parametric meta-materials."}}
{"id": "RHY_9ZVcTa_", "cdate": 1601308290189, "mdate": null, "content": {"title": "On Linear Identifiability of Learned Representations", "abstract": "Identifiability is a desirable property of a statistical model: it implies that the true model parameters may be estimated to any desired precision, given sufficient computational resources and data. We study identifiability in the context of representation learning: discovering nonlinear data representations that are optimal with respect to some downstream task. When parameterized as deep neural networks, such representation functions lack identifiability in parameter space, because they are overparameterized by design. In this paper, building on recent advances in nonlinear Independent Components Analysis, we aim to rehabilitate identifiability by showing that a large family of discriminative models are in fact identifiable in function space, up to a linear indeterminacy. Many models for representation learning in a wide variety of domains have been identifiable in this sense, including text, images and audio, state-of-the-art at time of publication. We derive sufficient conditions for linear identifiability and provide empirical support for the result on both simulated and real-world data."}}
{"id": "SJVuG3b_bB", "cdate": 1546300800000, "mdate": null, "content": {"title": "Efficient Amortised Bayesian Inference for Hierarchical and Nonlinear Dynamical Systems", "abstract": "We introduce a flexible, scalable Bayesian inference framework for nonlinear dynamical systems characterised by distinct and hierarchical variability at the individual, group, and population levels..."}}
{"id": "SyzKd1bCW", "cdate": 1518730175862, "mdate": null, "content": {"title": "Backpropagation through the Void: Optimizing control variates for black-box gradient estimation", "abstract": "Gradient-based optimization is the foundation of deep learning and reinforcement learning.\nEven when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function.\nThese estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings. We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic. We also demonstrate this framework for training discrete latent-variable models."}}
{"id": "BkfTTF1DM", "cdate": 1518472633951, "mdate": null, "content": {"title": "Design Motifs for Probabilistic Generative Design", "abstract": "Generative models can be used to produce designs that obey hard-to-specify constraints while still producing plausible examples.\nRecent examples of this include drug design, text with desired sentiment, or images with desired captions.\nHowever, most previous applications of generative models to design are based on bespoke, ad-hoc procedures.\nWe give a unifying treatment of generative design based on probabilistic generative models.\nSome of these models can be trained end-to-end, can take advantage of both labelled and unlabelled examples, and automatically trade off between different design goals."}}
{"id": "Hy-R9_-O-r", "cdate": 1483228800000, "mdate": null, "content": {"title": "Sticking the Landing: Simple, Lower-Variance Gradient Estimators for Variational Inference", "abstract": "We propose a simple and general variant of the standard reparameterized gradient estimator for the variational evidence lower bound. Specifically, we remove a part of the total derivative with respect to the variational parameters that corresponds to the score function. Removing this term produces an unbiased gradient estimator whose variance approaches zero as the approximate posterior approaches the exact posterior. We analyze the behavior of this gradient estimator theoretically and empirically, and generalize it to more complex variational distributions such as mixtures and importance-weighted posteriors."}}
