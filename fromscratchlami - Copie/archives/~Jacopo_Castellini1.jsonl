{"id": "kGQ4ZnnJwo", "cdate": 1674036935383, "mdate": 1674036935383, "content": {"title": "Biased Recommender Systems And Supplier Competition", "abstract": "Recommender systems are prevalent across digital platforms. They use machine learning techniques to help consumers make choices by predicting their preferred items. If RS had perfect information about consumer preferences and item attributes, they could recommend the most suitable item for each consumer. However, in practice, recommender systems have incomplete information, and their prediction models can exhibit biases. Drawing on a simplified theoretical model, this paper examines how such biases can lead to dampened competition between the suppliers selling through digital platform, arising from the fact that biased recommendations are less closely linked to true preferences. Three specific types of bias are examined and are shown to have subtly different effects. Competition remains stronger where suppliers can compete to gain the benefit of the bias, a form of competition for the market. The worst market outcomes can be avoided if consumers can reject unsuitable recommendations, since this helps to restore the competitive constraint on suppliers. A model extension considers the impact of endogenizing vertical quality. Importantly, in choosing its recommender system, the platform\u2019s preferences are not typically aligned with those of consumers."}}
{"id": "_SjlJ-fwpG4", "cdate": 1609459200000, "mdate": 1648717762615, "content": {"title": "Analysing factorizations of action-value networks for cooperative multi-agent reinforcement learning", "abstract": "Recent years have seen the application of deep reinforcement learning techniques to cooperative multi-agent systems, with great empirical success. However, given the lack of theoretical insight, it remains unclear what the employed neural networks are learning, or how we should enhance their learning power to address the problems on which they fail. In this work, we empirically investigate the learning power of various network architectures on a series of one-shot games. Despite their simplicity, these games capture many of the crucial problems that arise in the multi-agent setting, such as an exponential number of joint actions or the lack of an explicit coordination mechanism. Our results extend those in Castellini et al. (Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS\u201919.International Foundation for Autonomous Agents and Multiagent Systems, pp 1862\u20131864, 2019) and quantify how well various approaches can represent the requisite value functions, and help us identify the reasons that can impede good performance, like sparsity of the values or too tight coordination requirements."}}
{"id": "H9oz-qDN5sk", "cdate": 1609459200000, "mdate": 1648717762614, "content": {"title": "Difference Rewards Policy Gradients", "abstract": "Policy gradient methods have become one of the most popular classes of algorithms for multi-agent reinforcement learning. A key challenge, however, that is not addressed by many of these methods is multi-agent credit assignment: assessing an agent's contribution to the overall performance, which is crucial for learning good policies. We propose a novel algorithm called Dr.Reinforce that explicitly tackles this by combining difference rewards with policy gradients to allow for learning decentralized policies when the reward function is known. By differencing the reward function directly, Dr.Reinforce avoids difficulties associated with learning the Q-function as done by Counterfactual Multiagent Policy Gradients (COMA), a state-of-the-art difference rewards method. For applications where the reward function is unknown, we show the effectiveness of a version of Dr.Reinforce that learns a reward network that is used to estimate the difference rewards."}}
{"id": "SpYqpIbI34a", "cdate": 1577836800000, "mdate": 1648717762641, "content": {"title": "Difference Rewards Policy Gradients", "abstract": "Policy gradient methods have become one of the most popular classes of algorithms for multi-agent reinforcement learning. A key challenge, however, that is not addressed by many of these methods is multi-agent credit assignment: assessing an agent's contribution to the overall performance, which is crucial for learning good policies. We propose a novel algorithm called Dr.Reinforce that explicitly tackles this by combining difference rewards with policy gradients to allow for learning decentralized policies when the reward function is known. By differencing the reward function directly, Dr.Reinforce avoids difficulties associated with learning the Q-function as done by Counterfactual Multiagent Policy Gradients (COMA), a state-of-the-art difference rewards method. For applications where the reward function is unknown, we show the effectiveness of a version of Dr.Reinforce that learns an additional reward network that is used to estimate the difference rewards."}}
{"id": "wlNtrGq32aX", "cdate": 1546300800000, "mdate": 1648717762633, "content": {"title": "The Representational Capacity of Action-Value Networks for Multi-Agent Reinforcement Learning", "abstract": "Recent years have seen the application of deep reinforcement learning techniques to cooperative multi-agent systems, with great empirical success. In this work, we empirically investigate the representational power of various network architectures on a series of one-shot games. Despite their simplicity, these games capture many of the crucial problems that arise in the multi-agent setting, such as an exponential number of joint actions or the lack of an explicit coordination mechanism. Our results quantify how well various approaches can represent the requisite value functions, and help us identify issues that can impede good performance."}}
{"id": "qMG3hEOfPwy", "cdate": 1546300800000, "mdate": 1648717762760, "content": {"title": "Learning Numeracy: Binary Arithmetic with Neural Turing Machines", "abstract": "One of the main problems encountered so far with recurrent neural networks is that they struggle to retain long-time information dependencies in their recurrent connections. Neural Turing Machines (NTMs) attempt to mitigate this issue by providing the neural network with an external portion of memory, in which information can be stored and manipulated later on. The whole mechanism is differentiable end-to-end, allowing the network to learn how to utilise this long-term memory via stochastic gradient descent. This allows NTMs to infer simple algorithms directly from data sequences. Nonetheless, the model can be hard to train due to a large number of parameters and interacting components and little related work is present. In this work we use NTMs to learn and generalise two arithmetical tasks: binary addition and multiplication. These tasks are two fundamental algorithmic examples in computer science, and are a lot more challenging than the previously explored ones, with which we aim to shed some light on the real capabilities on this neural model."}}
{"id": "bBE2sGTlpwR", "cdate": 1546300800000, "mdate": 1648717762800, "content": {"title": "Krylov Iterative Methods for the Geometric Mean of Two Matrices Times a Vector", "abstract": "In this work, we are presenting an efficient way to compute the geometric mean of two positive definite matrices times a vector. For this purpose, we are inspecting the application of methods based on Krylov spaces to compute the square root of a matrix. These methods, using only matrix-vector products, are capable of producing a good approximation of the result with a small computational cost."}}
{"id": "U1UnG9k29M-", "cdate": 1546300800000, "mdate": 1648717762614, "content": {"title": "The Representational Capacity of Action-Value Networks for Multi-Agent Reinforcement Learning", "abstract": "Recent years have seen the application of deep reinforcement learning techniques to cooperative multi-agent systems, with great empirical success. However, given the lack of theoretical insight, it remains unclear what the employed neural networks are learning, or how we should enhance their representational power to address the problems on which they fail. In this work, we empirically investigate the representational power of various network architectures on a series of one-shot games. Despite their simplicity, these games capture many of the crucial problems that arise in the multi-agent setting, such as an exponential number of joint actions or the lack of an explicit coordination mechanism. Our results quantify how well various approaches can represent the requisite value functions, and help us identify issues that can impede good performance."}}
{"id": "Z4emKnTYW-t", "cdate": 1483228800000, "mdate": 1648717762760, "content": {"title": "Krylov iterative methods for the geometric mean of two matrices times a vector", "abstract": "In this work, we are presenting an efficient way to compute the geometric mean of two positive definite matrices times a vector. For this purpose, we are inspecting the application of methods based on Krylov spaces to compute the square root of a matrix. These methods, using only matrix-vector products, are capable of producing a good approximation of the result with a small computational cost."}}
{"id": "0W5MEiDTlnI", "cdate": 1483228800000, "mdate": 1648717762759, "content": {"title": "Fake Twitter followers detection by denoising autoencoder", "abstract": "Gaining followers on the Twitter platform has become a rapid way to increase one's credibility on this social network, that in the last few years has become a launch pad for new trends and to influence people opinions. So, many people have begun to buy fake followers on underground markets appositely created to sold them. Therefore, identifying fake followers profiles is useful to maintain the balance between real influential people on the network and people who simply exploited this mechanism. This work presents a model based on artificial neural networks able to detect fake Twitter profiles. In particular, a denoising autoencoder has been implemented as anomaly detector trained with a semi-supervised learning approach. The model has been tested on a benchmark already used in literature and results are presented."}}
