{"id": "w-4PpDITdR", "cdate": 1640995200000, "mdate": 1681667512910, "content": {"title": "Pragmatics in Grounded Language Learning: Phenomena, Tasks, and Modeling Approaches", "abstract": "People rely heavily on context to enrich meaning beyond what is literally said, enabling concise but effective communication. To interact successfully and naturally with people, user-facing artificial intelligence systems will require similar skills in pragmatics: relying on various types of context -- from shared linguistic goals and conventions, to the visual and embodied world -- to use language effectively. We survey existing grounded settings and pragmatic modeling approaches and analyze how the task goals, environmental contexts, and communicative affordances in each work enrich linguistic meaning. We present recommendations for future grounded task design to naturally elicit pragmatic phenomena, and suggest directions that focus on a broader range of communicative contexts and affordances."}}
{"id": "szdRSikt6C", "cdate": 1640995200000, "mdate": 1688032336770, "content": {"title": "Generalizing to New Domains by Mapping Natural Language to Lifted LTL", "abstract": "Recent work on using natural language to specify commands to robots has grounded that language to LTL. However, mapping natural language task specifications to LTL task specifications using language models require probability distributions over finite vocabulary. Existing state-of-the-art methods have extended this finite vocabulary to include unseen terms from the input sequence to improve output generalization. However, novel out-of-vocabulary atomic propositions cannot be generated using these methods. To overcome this, we introduce an intermediate contextual query representation which can be learned from single positive task specification examples, associating a contextual query with an LTL template. We demonstrate that this intermediate representation allows for generalization over unseen object references, assuming accurate groundings are available. We compare our method of mapping natural language task specifications to intermediate contextual queries against state-of-the-art CopyNet models capable of translating natural language to LTL, by evaluating whether correct LTL for manipulation and navigation task specifications can be output, and show that our method outperforms the CopyNet model on unseen object references. We demonstrate that the grounded LTL our method outputs can be used for planning in a simulated OO-MDP environment. Finally, we discuss some common failure modes encountered when translating natural language task specifications to grounded LTL."}}
{"id": "TuCdAJJU3v", "cdate": 1640995200000, "mdate": 1688032336779, "content": {"title": "Hierarchical planning with state abstractions for temporal task specifications", "abstract": "We often specify tasks for a robot using temporal language that can include different levels of abstraction. For example, the command \u201cgo to the kitchen before going to the second floor\u201d contains spatial abstraction, given that \u201cfloor\u201d consists of individual rooms that can also be referred to in isolation (\u201ckitchen\u201d, for example). There is also a temporal ordering of events, defined by the word \u201cbefore\u201d. Previous works have used syntactically co-safe Linear Temporal Logic (sc-LTL) to interpret temporal language (such as \u201cbefore\u201d), and Abstract Markov Decision Processes (AMDPs) to interpret hierarchical abstractions (such as \u201ckitchen\u201d and \u201csecond floor\u201d), separately. To handle both types of commands at once, we introduce the Abstract Product Markov Decision Process (AP-MDP), a novel approach capable of representing non-Markovian reward functions at different levels of abstractions. The AP-MDP framework translates LTL into its corresponding automata, creates a product Markov Decision Process (MDP) of the LTL specification and the environment MDP, and decomposes the problem into subproblems to enable efficient planning with abstractions. AP-MDP performs faster than a non-hierarchical method of solving LTL problems in over $$95 \\%$$ 95 % of path planning tasks, and this number only increases as the size of the environment domain increases. In a cleanup world domain, AP-MDP performs faster in over $$98\\%$$ 98 % of tasks. We also present a neural sequence-to-sequence model trained to translate language commands into LTL expression, and a new corpus of non-Markovian language commands spanning different levels of abstraction. We test our framework with the collected language commands on two drones, demonstrating that our approach enables robots to efficiently solve temporal commands at different levels of abstraction in both indoor and outdoor environments."}}
{"id": "KElpsE3n4K_", "cdate": 1640995200000, "mdate": 1683899703853, "content": {"title": "Affordance-based robot object retrieval", "abstract": "Natural language object retrieval is a highly useful yet challenging task for robots in human-centric environments. Previous work has primarily focused on commands specifying the desired object\u2019s type such as \u201cscissors\u201d and/or visual attributes such as \u201cred,\u201d thus limiting the robot to only known object classes. We develop a model to retrieve objects based on descriptions of their usage. The model takes in a language command containing a verb, for example \u201cHand me something to cut,\u201d and RGB images of candidate objects; and outputs the object that best satisfies the task specified by the verb. Our model directly predicts an object\u2019s appearance from the object\u2019s use specified by a verb phrase, without needing an object\u2019s class label. Based on contextual information present in the language commands, our model can generalize to unseen object classes and unknown nouns in the commands. Our model correctly selects objects out of sets of five candidates to fulfill natural language commands, and achieves a mean reciprocal rank of 77.4% on a held-out test set of unseen ImageNet object classes and 69.1% on unseen object classes and unknown nouns. Our model also achieves a mean reciprocal rank of 71.8% on unseen YCB object classes, which have a different image distribution from ImageNet. We demonstrate our model on a KUKA LBR iiwa robot arm, enabling the robot to retrieve objects based on natural language descriptions of their usage (Video recordings of the robot demonstrations can be found at https://youtu.be/WMAdGhMmXEQ ). We also present a new dataset of 655 verb-object pairs denoting object usage over 50 verbs and 216 object classes (The dataset and code for the project can be found at https://github.com/Thaonguyen3095/affordance-language )."}}
{"id": "9UR-3NZ8RqP", "cdate": 1640995200000, "mdate": 1688032336678, "content": {"title": "Mapping Language Models to Grounded Conceptual Spaces", "abstract": "A fundamental criticism of text-only language models (LMs) is their lack of grounding---that is, the ability to tie a word for which they have learned a representation, to its actual use in the world. However, despite this limitation, large pre-trained LMs have been shown to have a remarkable grasp of the conceptual structure of language, as demonstrated by their ability to answer questions, generate fluent text, or make inferences about entities, objects, and properties that they have never physically observed. In this work we investigate the extent to which the rich conceptual structure that LMs learn indeed reflects the conceptual structure of the non-linguistic world---which is something that LMs have never observed. We do this by testing whether the LMs can learn to map an entire conceptual domain (e.g., direction or colour) onto a grounded world representation given only a small number of examples. For example, we show a model what the word ``left\" means using a textual depiction of a grid world, and assess how well it can generalise to related concepts, for example, the word ``right\", in a similar grid world. We investigate a range of generative language models of varying sizes (including GPT-2 and GPT-3), and see that although the smaller models struggle to perform this mapping, the largest model can not only learn to ground the concepts that it is explicitly taught, but appears to generalise to several instances of unseen concepts as well. Our results suggest an alternative means of building grounded language models: rather than learning grounded representations ``from scratch'', it is possible that large text-only models learn a sufficiently rich conceptual structure that could allow them to be grounded in a data-efficient way."}}
{"id": "3Y8QAHx8vWT", "cdate": 1640995200000, "mdate": 1688032336587, "content": {"title": "RLang: A Declarative Language for Expression Prior Knowledge for Reinforcement Learning", "abstract": "We introduce RLang, a domain-specific language (DSL) for communicating domain knowledge to an RL agent. Unlike existing RL DSLs that ground to \\textit{single} elements of a decision-making formalism (e.g., the reward function or policy), RLang can specify information about every element of a Markov decision process. We define precise syntax and grounding semantics for RLang, and provide a parser that grounds RLang programs to an algorithm-agnostic \\textit{partial} world model and policy that can be exploited by an RL agent. We provide a series of example RLang programs demonstrating how different RL methods can exploit the resulting knowledge, encompassing model-free and model-based tabular algorithms, policy gradient and value-based methods, hierarchical approaches, and deep methods."}}
{"id": "gJcEM8sxHK", "cdate": 1632875678560, "mdate": null, "content": {"title": "Mapping Language Models to Grounded Conceptual Spaces", "abstract": "A fundamental criticism of text-only language models (LMs) is their lack of grounding---that is, the ability to tie a word for which they have learned a representation, to its actual use in the world. However, despite this limitation, large pre-trained LMs have been shown to have a remarkable grasp of the conceptual structure of language, as demonstrated by their ability to answer questions, generate fluent text, or make inferences about entities, objects, and properties that they have never physically observed. In this work we investigate the extent to which the rich conceptual structure that LMs learn indeed reflects the conceptual structure of the non-linguistic world---which is something that LMs have never observed. We do this by testing whether the LMs can learn to map an entire conceptual domain (e.g., direction or colour) onto a grounded world representation given only a small number of examples. For example, we show a model what the word ``left\" means using a textual depiction of a grid world, and assess how well it can generalise to related concepts, for example, the word ``right\", in a similar grid world. We investigate a range of generative language models of varying sizes (including GPT-2 and GPT-3), and see that although the smaller models struggle to perform this mapping, the largest model can not only learn to ground the concepts that it is explicitly taught, but appears to generalise to several instances of unseen concepts as well. Our results suggest an alternative means of building grounded language models: rather than learning grounded representations ``from scratch'', it is possible that large text-only models learn a sufficiently rich conceptual structure that could allow them to be grounded in a data-efficient way."}}
{"id": "jOso3zlf6c-", "cdate": 1609459200000, "mdate": 1642002969968, "content": {"title": "\"Was it \"stated\" or was it \"claimed\"?: How linguistic bias affects generative language models", "abstract": "Roma Patel, Ellie Pavlick. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021."}}
{"id": "dHRwDDauUcV", "cdate": 1609459200000, "mdate": 1642002969964, "content": {"title": "Generalizing to New Domains by Mapping Natural Language to Lifted LTL", "abstract": "Recent work on using natural language to specify commands to robots has grounded that language to LTL. However, mapping natural language task specifications to LTL task specifications using language models require probability distributions over finite vocabulary. Existing state-of-the-art methods have extended this finite vocabulary to include unseen terms from the input sequence to improve output generalization. However, novel out-of-vocabulary atomic propositions cannot be generated using these methods. To overcome this, we introduce an intermediate contextual query representation which can be learned from single positive task specification examples, associating a contextual query with an LTL template. We demonstrate that this intermediate representation allows for generalization over unseen object references, assuming accurate groundings are available. We compare our method of mapping natural language task specifications to intermediate contextual queries against state-of-the-art CopyNet models capable of translating natural language to LTL, by evaluating whether correct LTL for manipulation and navigation task specifications can be output, and show that our method outperforms the CopyNet model on unseen object references. We demonstrate that the grounded LTL our method outputs can be used for planning in a simulated OO-MDP environment. Finally, we discuss some common failure modes encountered when translating natural language task specifications to grounded LTL."}}
{"id": "bY_l2Q8Oimc", "cdate": 1609459200000, "mdate": 1642002969993, "content": {"title": "Game-theoretic Vocabulary Selection via the Shapley Value and Banzhaf Index", "abstract": "Roma Patel, Marta Garnelo, Ian Gemp, Chris Dyer, Yoram Bachrach. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021."}}
