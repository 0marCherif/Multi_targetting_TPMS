{"id": "hDDV1lsRV8", "cdate": 1663850390995, "mdate": null, "content": {"title": "Federated Learning from Small Datasets", "abstract": "Federated learning allows multiple parties to collaboratively train a joint model without having to share any local data. It enables applications of machine learning in settings where data is inherently distributed and undisclosable, such as in the medical domain. Joint training is usually achieved by aggregating local models. When local datasets are small, locally trained models can vary greatly from a globally good model. Bad local models can arbitrarily deteriorate the aggregate model quality, causing federating learning to fail in these settings. We propose a novel approach that avoids this problem by interleaving model aggregation and permutation steps. During a permutation step we redistribute local models across clients through the server, while preserving data privacy, to allow each local model to train on a daisy chain of local datasets. This enables successful training in data-sparse domains. Combined with model aggregation, this approach enables effective learning even if the local datasets are extremely small, while retaining the privacy benefits of federated learning."}}
{"id": "ZXWPAS5MCd", "cdate": 1640995200000, "mdate": 1679929474751, "content": {"title": "Plant 'n' Seek: Can You Find the Winning Ticket?", "abstract": ""}}
{"id": "H7YBig65kd", "cdate": 1640995200000, "mdate": 1679929474727, "content": {"title": "Label-Descriptive Patterns and Their Application to Characterizing Classification Errors", "abstract": ""}}
{"id": "1KS6CJ3wROi", "cdate": 1640995200000, "mdate": 1679929474841, "content": {"title": "Estimating Mutual Information via Geodesic kNN", "abstract": ""}}
{"id": "GVDwiINkMR", "cdate": 1632875704251, "mdate": null, "content": {"title": "Picking Daisies in Private: Federated Learning from Small Datasets", "abstract": "Federated learning allows multiple parties to collaboratively train a joint model without sharing local data. This enables applications of machine learning in settings of inherently distributed, undisclosable data such as in the medical domain. In practice, joint training is usually achieved by aggregating local models, for which local training objectives have to be in expectation similar to the joint (global) objective. Often, however, local datasets are so small that local objectives differ greatly from the global objective, resulting in federated learning to fail. We propose a novel approach that intertwines model aggregations with permutations of local models. The permutations expose each local model to a daisy chain of local datasets resulting in more efficient training in data-sparse domains. This enables training on extremely small local datasets, such as patient data across hospitals, while retaining the training efficiency and privacy benefits of federated learning."}}
{"id": "9n9c8sf0xm", "cdate": 1632875526828, "mdate": null, "content": {"title": "Plant 'n' Seek: Can You Find the Winning Ticket?", "abstract": "The lottery ticket hypothesis has sparked the rapid development of pruning algorithms that aim to reduce the computational costs associated with deep learning during training and model deployment. Currently, such algorithms are primarily evaluated on imaging data, for which we lack ground truth information and thus the understanding of how sparse lottery tickets could be. To fill this gap, we develop a framework that allows us to plant and hide winning tickets with desirable properties in randomly initialized neural networks. To analyze the ability of state-of-the-art pruning to identify tickets of extreme sparsity, we design and hide such tickets solving four challenging tasks. In extensive experiments, we observe similar trends as in imaging studies, indicating that our framework can provide transferable insights into realistic problems. Additionally, we can now see beyond such relative trends and highlight limitations of current pruning methods. Based on our results, we conclude that the current limitations in ticket sparsity are likely of algorithmic rather than fundamental nature. We anticipate that comparisons to planted tickets will facilitate future developments of efficient pruning algorithms."}}
{"id": "diRlP4NzeBu", "cdate": 1609459200000, "mdate": 1679929474808, "content": {"title": "Towards strong pruning for lottery tickets with non-zero biases", "abstract": ""}}
{"id": "SBrWvCw03kc", "cdate": 1609459200000, "mdate": 1645172687034, "content": {"title": "Federated Learning from Small Datasets", "abstract": "Federated learning allows multiple parties to collaboratively train a joint model without sharing local data. This enables applications of machine learning in settings of inherently distributed, undisclosable data such as in the medical domain. In practice, joint training is usually achieved by aggregating local models, for which local training objectives have to be in expectation similar to the joint (global) objective. Often, however, local datasets are so small that local objectives differ greatly from the global objective, resulting in federated learning to fail. We propose a novel approach that intertwines model aggregations with permutations of local models. The permutations expose each local model to a daisy chain of local datasets resulting in more efficient training in data-sparse domains. This enables training on extremely small local datasets, such as patient data across hospitals, while retaining the training efficiency and privacy benefits of federated learning."}}
{"id": "HffbPCD0hyq", "cdate": 1609459200000, "mdate": 1645172687035, "content": {"title": "Factoring out prior knowledge from low-dimensional embeddings", "abstract": "Low-dimensional embedding techniques such as tSNE and UMAP allow visualizing high-dimensional data and therewith facilitate the discovery of interesting structure. Although they are widely used, they visualize data as is, rather than in light of the background knowledge we have about the data. What we already know, however, strongly determines what is novel and hence interesting. In this paper we propose two methods for factoring out prior knowledge in the form of distance matrices from low-dimensional embeddings. To factor out prior knowledge from tSNE embeddings, we propose JEDI that adapts the tSNE objective in a principled way using Jensen-Shannon divergence. To factor out prior knowledge from any downstream embedding approach, we propose CONFETTI, in which we directly operate on the input distance matrices. Extensive experiments on both synthetic and real world data show that both methods work well, providing embeddings that exhibit meaningful structure that would otherwise remain hidden."}}
{"id": "BtZ-vCwR2k9", "cdate": 1609459200000, "mdate": 1645172687033, "content": {"title": "What's in the Box? Exploring the Inner Life of Neural Networks with Robust Rules", "abstract": "We propose a novel method for exploring how neurons within neural networks interact. In particular, we consider activation values of a network for given data, and propose to mine noise-robust rules..."}}
