{"id": "QpqS9INk4S", "cdate": 1708394615880, "mdate": 1708394615880, "content": {"title": "JaxMARL: Multi-Agent RL Environments in JAX", "abstract": "Benchmarks play an important role in the development of machine learning algorithms. Reinforcement learning environments are traditionally run on the CPU, limiting their scalability with typical academic compute. However, recent advancements in JAX have enabled the wider use of hardware acceleration to overcome these computational hurdles by producing massively parallel RL training pipelines and environments.\n\nThis is particularly useful for multi-agent reinforcement learning (MARL) research where not only multiple agents must be considered at each environment step, adding additional computational burden, but also the sample complexity is increased due to non-stationarity, decentralised partial observability, or other MARL challenges.\n\nIn this paper, we present JaxMARL, the first open-source code base that combines ease-of-use with GPU enabled efficiency, and supports a large number of commonly used MARL environments as well as popular baseline algorithms. Our experiments show that our JAX-based implementations are up to 1400x faster than existing single-threaded baselines. This enables efficient and thorough evaluations, with the potential to alleviate the evaluation crisis of the field. We also introduce and benchmark SMAX, a vectorised, simplified version of the StarCraft Multi-Agent Challenge, which removes the need to run the StarCraft II game engine. This not only enables GPU acceleration, but also provides a more flexible MARL environment, unlocking the potential for self-play, meta-learning, and other future applications in MARL."}}
{"id": "Q5uQecAw0vO", "cdate": 1663850368941, "mdate": null, "content": {"title": "Reinforcement Learning for Bandits with Continuous Actions and Large Context Spaces", "abstract": "We consider the challenging scenario of contextual bandits with continuous actions and large input ``context'' spaces, e.g. images. We posit that by modifying reinforcement learning (RL) algorithms for continuous control, we can outperform hand-crafted contextual bandit algorithms for continuous actions on standard benchmark datasets, i.e. vector contexts. We demonstrate that parametric policy networks outperform recently published tree-based policies in both average regret and costs on held-out samples. Furthermore, in contrast to previous work, we successfully demonstrate that RL algorithms can generalise contextual bandit problems with continuous actions to large context spaces. We obtain state-of-the-art performance using RL and significantly outperform previous methods on image contexts. Lastly, we introduce a new contextual bandits domain with multi-dimensional continuous action space and image context. "}}
{"id": "Ix4Ytiwor4U", "cdate": 1663850199826, "mdate": null, "content": {"title": "DITTO: Offline Imitation Learning with World Models", "abstract": "We propose DITTO, a fully offline approach to imitation learning which addresses the problem of covariate shift without access to an oracle or any additional online interactions. By unrolling agent policies in the latent space of a learned world model and penalizing drift from expert demonstrations, we can use online reinforcement learning algorithms to learn policies which solve the imitation objective, without access to the underlying environment or reward function. Decoupling policy and world model learning lets us leverage datasets of any quality to learn latent representations which provide a natural reward signal for imitation learning, avoiding the need for complex adversarial or sparse imitation-inducing rewards. Compared to competitive baselines, our method achieves state-of-the-art performance in a variety of challenging environments from pixel observations alone."}}
{"id": "KwdUWypQ5gC", "cdate": 1655376333980, "mdate": null, "content": {"title": "Bayesian Reinforcement Learning for Single-Episode Missions in Partially Unknown Environments", "abstract": "We consider planning for mobile robots conducting missions in real-world domains where a priori unknown dynamics affect the robot\u2019s costs and transitions. We study single-episode missions where it is crucial that the robot appropriately trades off exploration and exploitation, such that the learning of the environment dynamics is just enough to effectively complete the mission. Thus, we propose modelling unknown dynamics using Gaussian processes, which provide a principled Bayesian framework for incorporating online observations made by the robot, and using them to predict the dynamics in unexplored areas. We then formulate the problem of mission planning in Markov decision processes under Gaussian process predictions as Bayesian model-based reinforcement learning. This allows us to employ solution techniques that plan more efficiently than previous Gaussian process planning methods are able to. We empirically evaluate the benefits of our formulation in an underwater autonomous vehicle navigation task and robot mission planning in a realistic simulation of a nuclear environment."}}
{"id": "nrksGSRT7kX", "cdate": 1652737355652, "mdate": null, "content": {"title": "RAMBO-RL: Robust Adversarial Model-Based Offline Reinforcement Learning", "abstract": "Offline reinforcement learning (RL) aims to find performant policies from logged data without further environment interaction. Model-based algorithms, which learn a model of the environment from the dataset and perform conservative policy optimisation within that model, have emerged as a promising approach to this problem. In this work, we present Robust Adversarial Model-Based Offline RL (RAMBO), a novel approach to model-based offline RL. We formulate the problem as a two-player zero sum game against an adversarial environment model. The model is trained to minimise the value function while still accurately predicting the transitions in the dataset, forcing the policy to act conservatively in areas not covered by the dataset. To approximately solve the two-player game, we alternate between optimising the policy and adversarially optimising the model. The problem formulation that we address is theoretically grounded, resulting in a probably approximately correct (PAC) performance guarantee and a pessimistic value function which lower bounds the value function in the true environment. We evaluate our approach on widely studied offline RL benchmarks, and demonstrate that it outperforms existing state-of-the-art baselines."}}
{"id": "xmX-WjAsf8y", "cdate": 1621629775307, "mdate": null, "content": {"title": "Risk-Averse Bayes-Adaptive Reinforcement Learning", "abstract": "In this work, we address risk-averse Bayes-adaptive reinforcement learning. We pose the problem of optimising the conditional value at risk (CVaR) of the total return in Bayes-adaptive Markov decision processes (MDPs).  We show that a policy optimising CVaR in this setting is risk-averse to both the epistemic uncertainty due to the prior distribution over MDPs, and the aleatoric uncertainty due to the inherent stochasticity of MDPs. We reformulate the problem as a two-player stochastic game and propose an approximate algorithm based on Monte Carlo tree search and Bayesian optimisation. Our experiments demonstrate that our approach significantly outperforms baseline approaches for this problem."}}
{"id": "dxaINwQdXh1", "cdate": 1621629775307, "mdate": null, "content": {"title": "Risk-Averse Bayes-Adaptive Reinforcement Learning", "abstract": "In this work, we address risk-averse Bayes-adaptive reinforcement learning. We pose the problem of optimising the conditional value at risk (CVaR) of the total return in Bayes-adaptive Markov decision processes (MDPs).  We show that a policy optimising CVaR in this setting is risk-averse to both the epistemic uncertainty due to the prior distribution over MDPs, and the aleatoric uncertainty due to the inherent stochasticity of MDPs. We reformulate the problem as a two-player stochastic game and propose an approximate algorithm based on Monte Carlo tree search and Bayesian optimisation. Our experiments demonstrate that our approach significantly outperforms baseline approaches for this problem."}}
{"id": "ThaZXiBVjaK", "cdate": 1609459200000, "mdate": null, "content": {"title": "Fault-tolerant Control of Robot Manipulators with Sensory Faults using Unbiased Active Inference", "abstract": "This work presents a novel fault-tolerant control scheme based on active inference. Specifically, a new formulation of active inference which, unlike previous solutions, provides unbiased state estimation and simplifies the definition of probabilistically robust thresholds for fault-tolerant control of robotic systems using the free-energy. The proposed solution makes use of the sensory prediction errors in the free-energy for the generation of residuals and thresholds for fault detection and isolation of sensory faults, and it does not require additional controllers for fault recovery. Results validating the benefits in a simulated 2-DOF manipulator are presented, and future directions to improve the current fault recovery approach are discussed."}}
{"id": "RMwMWfK9q6B", "cdate": 1609459200000, "mdate": null, "content": {"title": "Online plan modification in uncertain resource-constrained environments", "abstract": "Highlights \u2022 Significant uncertainty means it is infeasible to calculate an optimal plan offline. \u2022 Online modification of robot plan addresses resource uncertainty. \u2022 Removing goals from a plan during execution reduces the risk of plan failure. \u2022 Adding goals mid-mission increases reward without adversely affecting success rate. Abstract This paper presents an approach to planning under uncertainty in resource-constrained environments. We describe our novel method for online plan modification and execution monitoring, which augments an existing plan with pre-computed plan fragments in response to observed resource availability. Our plan merging algorithm uses causal structure to interleave actions, creating solutions online using observations of the true state without introducing significant computational cost. Our system monitors resource availability, reasoning about the probability of successfully completing the goals. We show that when the probability of completing a plan decreases, by removing low-priority goals our system reduces the risk of plan failure, increasing mission success rate. Conversely, when resource availability allows, by including additional goals our system increases reward without adversely affecting success rate. We evaluate our approach using the example domain of long-range autonomous underwater vehicle (AUV) missions, in which a vehicle spends months at sea with little or no opportunity for intervention. We compare the performance to a state-of-the-art oversubscription planner. Planning within such domains is challenging because significant resource usage uncertainty means it is computationally infeasible to calculate the optimal strategy in advance. We also evaluate the applicability of our plan merging algorithm to existing IPC domains, presenting a discussion of the domain characteristics which favour the use of our approach."}}
{"id": "HitYKD0Q2VS", "cdate": 1609459200000, "mdate": null, "content": {"title": "Risk-Averse Bayes-Adaptive Reinforcement Learning", "abstract": "In this work, we address risk-averse Bayes-adaptive reinforcement learning. We pose the problem of optimising the conditional value at risk (CVaR) of the total return in Bayes-adaptive Markov decision processes (MDPs). We show that a policy optimising CVaR in this setting is risk-averse to both the parametric uncertainty due to the prior distribution over MDPs, and the internal uncertainty due to the inherent stochasticity of MDPs. We reformulate the problem as a two-player stochastic game and propose an approximate algorithm based on Monte Carlo tree search and Bayesian optimisation. Our experiments demonstrate that our approach significantly outperforms baseline approaches for this problem."}}
