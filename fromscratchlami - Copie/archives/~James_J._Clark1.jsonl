{"id": "DqNwU9s-Le", "cdate": 1672531200000, "mdate": 1682349434369, "content": {"title": "GliTr: Glimpse Transformers with Spatiotemporal Consistency for Online Action Prediction", "abstract": "Many online action prediction models observe complete frames to locate and attend to informative subregions in the frames called glimpses and recognize an ongoing action based on global and local information. However, in applications with constrained resources, an agent may not be able to observe the complete frame, yet must still locate useful glimpses to predict an incomplete action based on local information only. In this paper, we develop Glimpse Transformers (GliTr), which observe only narrow glimpses at all times, thus predicting an ongoing action and the following most informative glimpse location based on the partial spatiotemporal information collected so far. In the absence of a ground truth for the optimal glimpse locations for action recognition, we train GliTr using a novel spatiotemporal consistency objective: We require GliTr to attend to the glimpses with features similar to the corresponding complete frames (i.e. spatial consistency) and the resultant class logits at time t equivalent to the ones predicted using whole frames up to t (i.e. temporal consistency). Inclusion of our proposed consistency objective yields \u223c 10% higher accuracy on the Something-Something-v2 (SSv2) dataset than the baseline cross-entropy objective. Overall, despite observing only \u223c 33% of the total area per frame, GliTr achieves 53.02% and 93.91% accuracy on the SSv2 and Jester datasets, respectively."}}
{"id": "vUz6oSUQf6J", "cdate": 1655302984529, "mdate": 1655302984529, "content": {"title": "Towards a Machine Vision-Based Yield Monitor for the Counting and Quality Mapping of Shallots", "abstract": "In comparison to field crops such as cereals, cotton, hay and grain, specialty crops often require more resources, are usually more sensitive to sudden changes in growth conditions and are known to produce higher value products. Providing quality and quantity assessment of specialty crops during harvesting is crucial for securing higher returns and improving management practices. Technical advancements in computer and machine vision have improved the detection, quality assessment and yield estimation processes for various fruit crops, but similar methods capable of exporting a detailed yield map for vegetable crops have yet to be fully developed. A machine vision-based yield monitor was designed to perform size categorization and continuous counting of shallots in-situ during the harvesting process. Coupled with a software developed in Python, the system is composed of a video logger and a global navigation satellite system. Computer vision analysis is performed within the tractor while an RGB camera collects real-time video data of the crops under natural sunlight conditions. Vegetables are first segmented using Watershed segmentation, detected on the conveyor, and then classified by size. The system detected shallots in a subsample of the dataset with a precision of 76%. The software was also evaluated on its ability to classify the shallots into three size categories. The best performance was achieved in the large class (73%), followed by the small class (59%) and medium class (44%). Based on these results, the occasional occlusion of vegetables and inconsistent lighting conditions were the main factors that hindered performance. Although further enhancements are envisioned for the prototype system, its modular and novel design permits the mapping of a selection of other horticultural crops. Moreover, it has the potential to benefit many producers of small vegetable crops by providing them with useful harvest information in real-time."}}
{"id": "nQx_BIKEgIt", "cdate": 1655241510402, "mdate": 1655241510402, "content": {"title": " Development of a Machine Vision Yield Monitor for Shallot Onion Harvesters", "abstract": "Crop yield estimation and mapping are important tools that can help growers efficiently use their available resources and have access to detailed representations of their farm. Technical advancements in computer vision have improved the detection, quality assessment and yield estimation processes for crops, including apples, citrus, mangoes, maize, figs and many other fruits. However, similar methods capable of exporting a detailed yield map for vegetable crops have not yet been fully developed. A machine vision-based yield monitor was designed to perform identification and continuous counting of shallot onions in-situ during the harvesting process. The system is composed of a video and position logger, coupled with acomputer software, and can be used within the tractor itself. A modular camera bracket collected video data of the crops while positioned directly above the harvesting conveyor. Video data was collected in real-time with natural sunlight conditions and in a semi-controlled lighting environment using an artificial light source to enhance vegetable areas. Computational analysis was performed to track detected vegetables on the conveyor. The system is to be tested for a full continuous run during the summer 2018 harvesting season. Based on preliminary results, occasional occlusion of vegetables and inconsistent light conditions are the main limiting factors that may inhibit performance. Although further enhancements are envisioned for the prototype system developed, it has the potential to benefit many producers of small vegetable crops by providing them with useful harvest information in real time and can help to improve harvesting logistics."}}
{"id": "zjF7GvD2OW", "cdate": 1640995200000, "mdate": 1682349434494, "content": {"title": "BERTPerf: Inference Latency Predictor for BERT on ARM big.LITTLE Multi-Core Processors", "abstract": "Hardware-aware Neural Architecture Search (NAS) and mapping & scheduling optimization methods are being used to find efficient implementations of computationally-intense language models such as BERT. This requires measuring real hardware inference latency: good design decisions simply cannot be made with proxy metrics such as FLOPs or the number of parameters. However, the time required to perform on-device latency measurements is prohibitive (e.g., a few days to a few weeks over the course of an optimization run). To address this, we present BERTPerf, a low-cost, highly-accurate method to predict the inference time of BERT on ARM big.LITTLE multi-core processors. BERTPerf exploits latency patterns at the layer-level to reduce on-device latency measurements, and captures the effect of caching and intermediate tensor allocations to reduce latency prediction error. BERTPerf reduces the maximum prediction error by 7\u201311% compared to the state-of-the-art, and requires 75% less on-device measurements compared to existing work at the same prediction error."}}
{"id": "y-4wmayyni1", "cdate": 1640995200000, "mdate": 1682349433538, "content": {"title": "Efficient Fine-Tuning of BERT Models on the Edge", "abstract": "Resource-constrained devices are increasingly the deployment targets of machine learning applications. Static models, however, do not always suffice for dynamic environments. On-device training of models allows for quick adaptability to new scenarios. With the increasing size of deep neural networks, as noted with the likes of BERT and other natural language processing models, comes increased resource requirements, namely memory, computation, energy, and time. Furthermore, training is far more resource intensive than inference. Resource-constrained on-device learning is thus doubly difficult, especially with large BERT-like models. By reducing the memory usage of fine-tuning, pre-trained BERT models can become efficient enough to fine-tune on resource-constrained devices. We propose Freeze And ReconFigure (FAR), a memory-efficient training regime for BERT-like models that reduces the memory usage of activation maps during fine-tuning by avoiding unnecessary parameter updates. FAR reduces fine-tuning time on the DistilBERT model and CoLA dataset by 30 %, and time spent on memory operations by 47%. More broadly, reductions in metric performance on the GLUE and SQuAD datasets are around 1% on average."}}
{"id": "w9eSId6PatU", "cdate": 1640995200000, "mdate": 1682349434081, "content": {"title": "Target Features Affect Visual Search, A Study of Eye Fixations", "abstract": "Visual Search is referred to the task of finding a target object among a set of distracting objects in a visual display. In this paper, based on an independent analysis of the COCO-Search18 dataset, we investigate how the performance of human participants during visual search is affected by different parameters such as the size and eccentricity of the target object. We also study the correlation between the error rate of participants and search performance. Our studies show that a bigger and more eccentric target is found faster with fewer number of fixations. Our code for the graphics are publicly available at https://github.com/ManooshSamiei/COCOSearch18_Analysis."}}
{"id": "vwGSMjeaK54", "cdate": 1640995200000, "mdate": 1682349434062, "content": {"title": "CES-KD: Curriculum-based Expert Selection for Guided Knowledge Distillation", "abstract": "Knowledge distillation (KD) is an effective tool for compressing deep classification models for edge devices. However, the performance of KD is affected by the large capacity gap between the teacher and student networks. Recent methods have resorted to a multiple teacher assistant (TA) setting for KD, which sequentially decreases the size of the teacher model to relatively bridge the size gap between these models. This paper proposes a new technique called Curriculum Expert Selection for Knowledge Distillation (CES-KD) to efficiently enhance the learning of a compact student under the capacity gap problem. This technique is built upon the hypothesis that a student network should be guided gradually using stratified teaching curriculum as it learns easy (hard) data samples better and faster from a lower (higher) capacity teacher network. Specifically, our method is a gradual TA-based KD technique that selects a single teacher per input image based on a curriculum driven by the difficulty in classifying the image. In this work, we empirically verify our hypothesis and rigorously experiment with CIFAR-10, CIFAR-100, CINIC-10, and ImageNet datasets and show improved accuracy on VGG-like models, ResNets, and WideResNets architectures."}}
{"id": "tjQmOLy0MrH", "cdate": 1640995200000, "mdate": 1682349434111, "content": {"title": "Work-in-Progress: Utilizing latency and accuracy predictors for efficient hardware-aware NAS", "abstract": "With the increased size and complexity of state-of-the-art language models such as BERT, deploying them on resource-constrained devices has become challenging. Latency-aware Neural Architecture Search (NAS) is an effective solution for finding an efficient implementation of complex models that satisfy hardware limitations. However, collecting on-device accuracy and latency feedback would significantly slow down the search process, making NAS impractical. To address this, we propose a low-cost method that models both accuracy and latency of BERT-based models on the target device, NVIDIA Jetson TX2, and removes the hardware-related delays from the search loop. Using a Random Forest regressor, our predictors outperform the state-of-the-art and achieve up to 57x speedup while finding a set of near-optimal models."}}
{"id": "t8CIbElYLMO", "cdate": 1640995200000, "mdate": 1667934165669, "content": {"title": "Consistency driven Sequential Transformers Attention Model for Partially Observable Scenes", "abstract": "Most hard attention models initially observe a complete scene to locate and sense informative glimpses, and predict class-label of a scene based on glimpses. However, in many applications (e.g., aerial imaging), observing an entire scene is not always feasible due to the limited time and resources available for acquisition. In this paper, we develop a Sequential Transformers Attention Model (STAM) that only partially observes a complete image and predicts informative glimpse locations solely based on past glimpses. We design our agent using DeiT-distilled [44] and train it with a one-step actorcritic algorithm. Furthermore, to improve classification performance, we introduce a novel training objective, which enforces consistency between the class distribution predicted by a teacher model from a complete image and the class distribution predicted by our agent using glimpses. When the agent senses only 4% of the total image area, the inclusion of the proposed consistency loss in our training objective yields 3% and 8% higher accuracy on ImageNet and fMoW datasets, respectively. Moreover, our agent outperforms previous state-of-the-art by observing nearly 27% and 42% fewer pixels in glimpses on ImageNet and fMoW."}}
{"id": "rVrFN5zR60A", "cdate": 1640995200000, "mdate": 1682349433003, "content": {"title": "Interaction Classification with Key Actor Detection in Multi-Person Sports Videos", "abstract": "Interaction recognition from multi-person videos is a challenging yet essential task in computer vision. Often the videos depict actions with multiple actors involved, some of whom participate in the main event, and the rest are present in the scene without being part of the actual event. This paper proposes a model to tackle the problem of interaction recognition from multi-person videos. Our model consists of a Recurrent Neural Network (RNN) equipped with a time-varying attention mechanism. It receives scene features and localized actors features to predict the interaction class. Additionally, the attention model identifies the people responsible for the main event. We chose penalty classification from ice hockey broadcast videos as our application. These videos are multi-persons and depict complex interactions between players in a non-laboratory recording setup. We evaluate our model on a new dataset of ice hockey penalty videos and report 93.93% classification accuracy. We include a qualitative analysis of the attention mechanism by visualizing the attention weights. Our code is publicly available <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> ."}}
