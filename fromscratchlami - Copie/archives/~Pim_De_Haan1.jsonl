{"id": "BbFl6GOleK", "cdate": 1684497031742, "mdate": null, "content": {"title": "Geometric Algebra Transformers", "abstract": "Problems involving geometric data arise in a variety of fields, including computer vision, robotics, chemistry, and physics. Such data can take numerous forms, such as points, direction vectors, planes, or transformations, but to date there is no single architecture that can be applied to such a wide variety of geometric types while respecting their symmetries. In this paper we introduce the Geometric Algebra Transformer (GATr), a general-purpose architecture for geometric data. GATr represents inputs, outputs, and hidden states in the projective geometric algebra, which offers an efficient 16-dimensional vector space representation of common geometric objects and operators acting on them. GATr is equivariant with respect to E(3), the symmetry group of 3D Euclidean space. As a transformer, GATr is scalable, expressive, and versatile. In experiments with n-body modeling and robotic planning, GATr shows strong improvements over non-geometric baselines."}}
{"id": "OrbWCpidbt", "cdate": 1676591079733, "mdate": null, "content": {"title": "EDGI: Equivariant Diffusion for Planning with Embodied Agents", "abstract": "Embodied agents operate in a structured world, often solving tasks with spatial, temporal, and permutation symmetries. Most algorithms for planning and model-based reinforcement learning (MBRL) do not take this rich geometric structure into account, leading to sample inefficiency and poor generalization. We introduce the Equivariant Diffuser for Generating Interactions (EDGI), an algorithm for MBRL and planning that is equivariant with respect to the product of the spatial symmetry group SE(3), the discrete-time translation group \u2124, and the object permutation group S\u2099. EDGI follows the Diffuser framework (Janner et al. 2022) in treating both learning a world model and planning in it as a conditional generative modeling problem, training a diffusion model on an offline trajectory dataset. We introduce a new SE(3) \u00d7 \u2124 \u00d7 S\u2099-equivariant diffusion model that supports multiple representations. We integrate this model in a planning loop, where conditioning and classifier-based guidance allow us to softly break the symmetry for specific tasks as needed. On navigation and object manipulation tasks, EDGI improves sample efficiency and generalization."}}
{"id": "wZYhLNRqN4J", "cdate": 1672531200000, "mdate": 1683631573742, "content": {"title": "EDGI: Equivariant Diffusion for Planning with Embodied Agents", "abstract": "Embodied agents operate in a structured world, often solving tasks with spatial, temporal, and permutation symmetries. Most algorithms for planning and model-based reinforcement learning (MBRL) do not take this rich geometric structure into account, leading to sample inefficiency and poor generalization. We introduce the Equivariant Diffuser for Generating Interactions (EDGI), an algorithm for MBRL and planning that is equivariant with respect to the product of the spatial symmetry group $\\mathrm{SE(3)}$, the discrete-time translation group $\\mathbb{Z}$, and the object permutation group $\\mathrm{S}_n$. EDGI follows the Diffuser framework (Janner et al. 2022) in treating both learning a world model and planning in it as a conditional generative modeling problem, training a diffusion model on an offline trajectory dataset. We introduce a new $\\mathrm{SE(3)} \\times \\mathbb{Z} \\times \\mathrm{S}_n$-equivariant diffusion model that supports multiple representations. We integrate this model in a planning loop, where conditioning and classifier-based guidance allow us to softly break the symmetry for specific tasks as needed. On navigation and object manipulation tasks, EDGI improves sample efficiency and generalization."}}
{"id": "VFYYtYM0iA", "cdate": 1672531200000, "mdate": 1683631573931, "content": {"title": "Rigid body flows for sampling molecular crystal structures", "abstract": "Normalizing flows (NF) are a class of powerful generative models that have gained popularity in recent years due to their ability to model complex distributions with high flexibility and expressiveness. In this work, we introduce a new type of normalizing flow that is tailored for modeling positions and orientations of multiple objects in three-dimensional space, such as molecules in a crystal. Our approach is based on two key ideas: first, we define smooth and expressive flows on the group of unit quaternions, which allows us to capture the continuous rotational motion of rigid bodies; second, we use the double cover property of unit quaternions to define a proper density on the rotation group. This ensures that our model can be trained using standard likelihood-based methods or variational inference with respect to a thermodynamic target density. We evaluate the method by training Boltzmann generators for two molecular examples, namely the multi-modal density of a tetrahedral system in an external field and the ice XI phase in the TIP4P water model. Our flows can be combined with flows operating on the internal degrees of freedom of molecules and constitute an important step towards the modeling of distributions of many interacting molecules."}}
{"id": "hgNn3n5pRKC", "cdate": 1665251234736, "mdate": null, "content": {"title": "Deconfounded Imitation Learning", "abstract": "Standard imitation learning can fail when the expert demonstrators have different sensory inputs than the imitating agent. This partial observability gives rise to hidden confounders in the causal graph, which lead to the failure to imitate. We break down the space of confounded imitation learning problems and identify three settings with different data requirements in which the correct imitation policy can be identified. We then introduce an algorithm for deconfounded imitation learning, which trains an inference model jointly with a latent-conditional policy. At test time, the agent alternates between updating its belief over the latent and acting under the belief. We show in theory and practice that this algorithm converges to the correct interventional policy, solves the confounding issue, and can under certain assumptions achieve an asymptotically optimal imitation performance.\n"}}
{"id": "kig2SKv0bok", "cdate": 1664055782300, "mdate": null, "content": {"title": "$\\mathrm{SE}(3)$-equivariant hemodynamics estimation on arterial surface meshes using graph convolutional networks", "abstract": "Hemodynamic field estimation on the artery surface is valuable for patient-specific prognosis, diagnosis, and treatment of cardiovascular disease. Medical biomarkers like wall shear stress (WSS) can be obtained from computational fluid dynamics (CFD) simulation of the blood flow. Machine-learning methods could accelerate or replace the time-intensive CFD simulation. We propose a graph convolutional network (GCN) that predicts hemodynamic fields mapped to the vertices of a finite-element surface mesh. Our neural network is end-to-end $\\mathrm{SE}(3)$-equivariant and uses anisotropic convolution filters, as well as pooling layers, informed by the mesh structure. We generate a large dataset of CFD simulations in synthetic arteries which we use to train and evaluate our neural network. We show that our method can accurately predict WSS, up to two orders of magnitude faster than CFD."}}
{"id": "oQOfMrkGVEu", "cdate": 1654886253404, "mdate": null, "content": {"title": "Weakly supervised causal representation learning", "abstract": "Learning high-level causal representations together with a causal model from unstructured low-level data such as pixels is impossible from observational data alone. We prove under mild assumptions that this representation is however identifiable in a weakly supervised setting. This requires a dataset with paired samples before and after random, unknown interventions, but no further labels. We then introduce implicit latent causal models, variational autoencoders that represent causal variables and causal structure without having to optimize an explicit discrete graph structure. On simple image data, including a novel dataset of simulated robotic manipulation, we demonstrate that such models can reliably identify the causal structure and disentangle causal variables."}}
{"id": "dz79MhQXWvg", "cdate": 1652737402989, "mdate": null, "content": {"title": "Weakly supervised causal representation learning", "abstract": "Learning high-level causal representations together with a causal model from unstructured low-level data such as pixels is impossible from observational data alone. We prove under mild assumptions that this representation is however identifiable in a weakly supervised setting. This involves a dataset with paired samples before and after random, unknown interventions, but no further labels. We then introduce implicit latent causal models, variational autoencoders that represent causal variables and causal structure without having to optimize an explicit discrete graph structure. On simple image data, including a novel dataset of simulated robotic manipulation, we demonstrate that such models can reliably identify the causal structure and disentangle causal variables."}}
{"id": "rUXOUBuUcg5", "cdate": 1646057534218, "mdate": null, "content": {"title": "Weakly supervised causal representation learning", "abstract": "Learning high-level causal representations together with a causal model from unstructured low-level data such as pixels is impossible from observational data alone. We prove under mild assumptions that this representation is identifiable in a weakly supervised setting. This requires a dataset with paired samples before and after random, unknown interventions, but no further labels. Finally, we show that we can infer the representation and causal graph reliably in a simple synthetic domain using a variational autoencoder with a structural causal model as prior."}}
{"id": "_cBDZvkDVOn", "cdate": 1640995200000, "mdate": 1683631573403, "content": {"title": "Weakly supervised causal representation learning", "abstract": "Learning high-level causal representations together with a causal model from unstructured low-level data such as pixels is impossible from observational data alone. We prove under mild assumptions that this representation is however identifiable in a weakly supervised setting. This involves a dataset with paired samples before and after random, unknown interventions, but no further labels. We then introduce implicit latent causal models, variational autoencoders that represent causal variables and causal structure without having to optimize an explicit discrete graph structure. On simple image data, including a novel dataset of simulated robotic manipulation, we demonstrate that such models can reliably identify the causal structure and disentangle causal variables."}}
