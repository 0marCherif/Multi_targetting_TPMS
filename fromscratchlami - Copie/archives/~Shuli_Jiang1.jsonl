{"id": "cK-q7_Kgnwp", "cdate": 1640995200000, "mdate": 1675477294130, "content": {"title": "D.MCA: Outlier Detection with Explicit Micro-Cluster Assignments", "abstract": "How can we detect outliers, both scattered and clustered, and also explicitly assign them to respective micro-clusters, without knowing apriori how many micro-clusters exist? How can we perform both tasks in-house, i.e., without any post-hoc processing, so that both detection and assignment can benefit simultaneously from each other? Presenting outliers in separate micro-clusters is informative to analysts in many real-world applications. However, a na\u00efve solution based on post-hoc clustering of the outliers detected by any existing method suffers from two main drawbacks: (a) appropriate hyperparameter values are commonly unknown for clustering, and most algorithms struggle with clusters of varying shapes and densities; (b) detection and assignment cannot benefit from one another. In this paper, we propose D.MCA to Detect outliers with explicit Micro-Cluster Assignment. Our method performs both detection and assignment iteratively, and in-house, by using a novel strategy that prunes entire micro-clusters out of the training set to improve the performance of the detection. It also benefits from a novel strategy that avoids clustered outliers to mask each other, which is a well-known problem in the literature. Also, D.MCA is designed to be robust to a critical hyperparameter by employing a hyperensemble \u201cwarm up\u201d phase. Experiments performed on 16 real-world and synthetic datasets demonstrate that D.MCA outperforms 8 state-of-the-art competitors, especially on the explicit outlier micro-cluster assignment task."}}
{"id": "rxud5HYKX55", "cdate": 1621630061540, "mdate": null, "content": {"title": "Optimal Sketching for Trace Estimation", "abstract": "Matrix trace estimation is ubiquitous in machine learning applications and has traditionally relied on Hutchinson's method, which requires $O(\\log(1/\\delta)/\\epsilon^2)$ matrix-vector product queries to achieve a $(1 \\pm \\epsilon)$-multiplicative approximation to $\\text{trace}(A)$ with failure probability $\\delta$ on positive-semidefinite input matrices $A$. Recently, the Hutch++ algorithm was proposed, which reduces the number of matrix-vector queries from $O(1/\\epsilon^2)$ to the optimal $O(1/\\epsilon)$, and the algorithm succeeds with constant probability. However, in the high probability setting, the non-adaptive Hutch++ algorithm suffers an extra $O(\\sqrt{\\log(1/\\delta)})$ multiplicative factor in its query complexity. Non-adaptive methods are important, as they correspond to sketching algorithms, which are mergeable, highly parallelizable, and provide low-memory streaming algorithms as well as low-communication distributed protocols. In this work, we close the gap between non-adaptive and adaptive algorithms, showing that even non-adaptive algorithms can achieve $O(\\sqrt{\\log(1/\\delta)}/\\epsilon + \\log(1/\\delta))$ matrix-vector products. In addition, we prove matching lower bounds demonstrating that, up to a $\\log \\log(1/\\delta)$ factor, no further improvement in the dependence on $\\delta$ or $\\epsilon$ is possible by any non-adaptive algorithm. Finally, our experiments demonstrate the superior performance of our sketch over the adaptive Hutch++ algorithm, which is less parallelizable, as well as over the non-adaptive Hutchinson's method."}}
{"id": "vxKIDlje9Y", "cdate": 1609459200000, "mdate": 1681959839587, "content": {"title": "Optimal Sketching for Trace Estimation", "abstract": "Matrix trace estimation is ubiquitous in machine learning applications and has traditionally relied on Hutchinson's method, which requires $O(\\log(1/\\delta)/\\epsilon^2)$ matrix-vector product queries to achieve a $(1 \\pm \\epsilon)$-multiplicative approximation to $\\text{trace}(A)$ with failure probability $\\delta$ on positive-semidefinite input matrices $A$. Recently, the Hutch++ algorithm was proposed, which reduces the number of matrix-vector queries from $O(1/\\epsilon^2)$ to the optimal $O(1/\\epsilon)$, and the algorithm succeeds with constant probability. However, in the high probability setting, the non-adaptive Hutch++ algorithm suffers an extra $O(\\sqrt{\\log(1/\\delta)})$ multiplicative factor in its query complexity. Non-adaptive methods are important, as they correspond to sketching algorithms, which are mergeable, highly parallelizable, and provide low-memory streaming algorithms as well as low-communication distributed protocols. In this work, we close the gap between non-adaptive and adaptive algorithms, showing that even non-adaptive algorithms can achieve $O(\\sqrt{\\log(1/\\delta)}/\\epsilon + \\log(1/\\delta))$ matrix-vector products. In addition, we prove matching lower bounds demonstrating that, up to a $\\log \\log(1/\\delta)$ factor, no further improvement in the dependence on $\\delta$ or $\\epsilon$ is possible by any non-adaptive algorithm. Finally, our experiments demonstrate the superior performance of our sketch over the adaptive Hutch++ algorithm, which is less parallelizable, as well as over the non-adaptive Hutchinson's method."}}
{"id": "7O6L1eLum6", "cdate": 1609459200000, "mdate": 1681959839605, "content": {"title": "Streaming and Distributed Algorithms for Robust Column Subset Selection", "abstract": "We give the first single-pass streaming algorithm for Column Subset Selection with respect to the entrywise $\\ell_p$-norm with $1 \\leq p < 2$. We study the $\\ell_p$ norm loss since it is often cons..."}}
{"id": "n1wPkibo2R", "cdate": 1601308315252, "mdate": null, "content": {"title": "An Efficient Protocol for Distributed Column Subset Selection in the Entrywise $\\ell_p$ Norm", "abstract": "We give a distributed protocol with nearly-optimal communication and number of rounds for Column Subset Selection with respect to the entrywise {$\\ell_1$} norm ($k$-CSS$_1$), and more generally, for the $\\ell_p$-norm with $1 \\leq p < 2$. We study matrix factorization in $\\ell_1$-norm loss, rather than the more standard Frobenius norm loss, because the $\\ell_1$ norm is more robust to noise, which is observed to lead to improved performance in a wide range of computer vision and robotics problems.\nIn the distributed setting, we consider $s$ servers in the standard coordinator model of communication, where the columns of the input matrix $A \\in \\mathbb{R}^{d \\times n}$ ($n \\gg d$) are distributed across the $s$ servers. We give a protocol in this model with $\\widetilde{O}(sdk)$ communication, $1$ round, and polynomial running time, and which achieves a multiplicative $k^{\\frac{1}{p} - \\frac{1}{2}}\\poly(\\log nd)$-approximation to the best possible column subset. A key ingredient in our proof is the reduction to the $\\ell_{p,2}$-norm, which corresponds to the $p$-norm of the vector of Euclidean norms of each of the columns of $A$. This enables us to use strong coreset constructions for Euclidean norms, which previously had not been used in this context. This naturally also allows us to implement our algorithm in the popular streaming model of computation. We further propose a greedy algorithm for selecting columns, which can be used by the coordinator, and show the first provable guarantees for a greedy algorithm for the $\\ell_{1,2}$ norm. Finally, we implement our protocol and give significant practical advantages on real-world data analysis tasks."}}
{"id": "IK5YJII6XI", "cdate": 1514764800000, "mdate": 1681959839606, "content": {"title": "A Demonstration of the OtterTune Automatic Database Management System Tuning Service", "abstract": ""}}
