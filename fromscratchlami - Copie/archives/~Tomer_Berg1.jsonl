{"id": "kQfZbUVPYt", "cdate": 1640995200000, "mdate": 1683982398817, "content": {"title": "On The Memory Complexity of Uniformity Testing", "abstract": "In this paper we consider the problem of uniformity testing with limited memory. We observe a sequence of independent identically distributed random variables drawn from a distribution $p$ over $[n..."}}
{"id": "Y4iHBBHwwd", "cdate": 1640995200000, "mdate": 1683982398846, "content": {"title": "Deterministic Finite-Memory Bias Estimation", "abstract": "In this paper we consider the problem of estimating a Bernoulli parameter using finite memory. Let $X_1,X_2,\\ldots$ be a sequence of independent identically distributed Bernoulli random variables with expectation $\\theta$, where $\\theta \\in [0,1]$. Consider a finite-memory deterministic machine with $S$ states, that updates its state $M_n \\in \\{1,2,\\ldots,S\\}$ at each time according to the rule $M_n = f(M_{n-1},X_n)$, where $f$ is a deterministic time-invariant function. Assume that the machine outputs an estimate at each time point according to some fixed mapping from the state space to the unit interval. The quality of the estimation procedure is measured by the asymptotic risk, which is the long-term average of the instantaneous quadratic risk. The main contribution of this paper is an upper bound on the smallest worst-case asymptotic risk any such machine can attain. This bound coincides with a lower bound derived by Leighton and Rivest, to imply that $\\Theta(1/S)$ is the minimax asymptotic risk for deterministic $S$-state machines. In particular, our result disproves a longstanding $\\Theta(\\log S/S)$ conjecture for this quantity, also posed by Leighton and Rivest."}}
{"id": "KrxWqaeFXiJ", "cdate": 1640995200000, "mdate": 1683982398871, "content": {"title": "On The Memory Complexity of Uniformity Testing", "abstract": "In this paper we consider the problem of uniformity testing with limited memory. We observe a sequence of independent identically distributed random variables drawn from a distribution $p$ over $[n]$, which is either uniform or is $\\varepsilon$-far from uniform under the total variation distance, and our goal is to determine the correct hypothesis. At each time point we are allowed to update the state of a finite-memory machine with $S$ states, where each state of the machine is assigned one of the hypotheses, and we are interested in obtaining an asymptotic probability of error at most $0<\\delta<1/2$ uniformly under both hypotheses. The main contribution of this paper is deriving upper and lower bounds on the number of states $S$ needed in order to achieve a constant error probability $\\delta$, as a function of $n$ and $\\varepsilon$, where our upper bound is $O(\\frac{n\\log n}{\\varepsilon})$ and our lower bound is $\\Omega (n+\\frac{1}{\\varepsilon})$. Prior works in the field have almost exclusively used collision counting for upper bounds, and the Paninski mixture for lower bounds. Somewhat surprisingly, in the limited memory with unlimited samples setup, the optimal solution does not involve counting collisions, and the Paninski prior is not hard. Thus, different proof techniques are needed in order to attain our bounds."}}
{"id": "pVqbzpKdS6", "cdate": 1609459200000, "mdate": 1683982398879, "content": {"title": "Distributed Source Simulation With No Communication", "abstract": "We consider the problem of distributed source simulation with no communication, in which Alice and Bob observe sequences <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$U^{n}$ </tex-math></inline-formula> and <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$V^{n}$ </tex-math></inline-formula> respectively, drawn from a joint distribution <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$p_{UV}^ {\\otimes n}$ </tex-math></inline-formula> , and wish to locally generate sequences <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$X^{n}$ </tex-math></inline-formula> and <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$Y^{n}$ </tex-math></inline-formula> respectively with a joint distribution that is close (in KL divergence) to <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$p_{XY}^ {\\otimes n}$ </tex-math></inline-formula> . We provide a single-letter condition under which such a simulation is asymptotically possible with a vanishing KL divergence. Our condition is nontrivial only in the case where the G\u00e0cs-K\u00f6rner (GK) common information between <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$U$ </tex-math></inline-formula> and <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$V$ </tex-math></inline-formula> is nonzero, and we conjecture that only scalar Markov chains <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$X-U-V-Y$ </tex-math></inline-formula> can be simulated otherwise. Motivated by this conjecture, we further examine the case where both <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$p_{UV}$ </tex-math></inline-formula> and <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$p_{XY}$ </tex-math></inline-formula> are doubly symmetric binary sources with parameters <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$p,q\\leq 1/2$ </tex-math></inline-formula> respectively. While it is trivial that in this case <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$p\\leq q$ </tex-math></inline-formula> is both necessary and sufficient, we use Fourier analytic tools to show that when <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$p$ </tex-math></inline-formula> is close to <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$q$ </tex-math></inline-formula> then any successful simulation is close to being scalar in the total variation sense."}}
{"id": "W4d1QZ2KnCx", "cdate": 1609459200000, "mdate": 1683982398881, "content": {"title": "Deterministic Finite-Memory Bias Estimation", "abstract": "In this paper we consider the problem of estimating a Bernoulli parameter using finite memory. Let $X_1,X_2,\\ldots$ be a sequence of independent identically distributed Bernoulli random variables w..."}}
{"id": "u7eQHamf5j", "cdate": 1577836800000, "mdate": 1683982398880, "content": {"title": "Binary Hypothesis Testing with Deterministic Finite-Memory Decision Rules", "abstract": "In this paper we consider the problem of binary hypothesis testing with finite memory systems. Let X <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sub> , X <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sub> , .. . be a sequence of independent identically distributed Bernoulli random variables, with expectation p under H <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">0</sub> and q under H <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sub> . Consider a finite-memory deterministic machine with S states that updates its state M <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">n</sub> \u2208 {1, 2, ... , S} at each time according to the rule M <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">n</sub> = f(M <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">n-1</sub> , X <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">n</sub> ), where f is a deterministic time-invariant function. Assume that we let the process run for a very long time (n \u2192 \u221e), and then make our decision according to some mapping from the state space to the hypothesis space. The main contribution of this paper is a lower bound on the Bayes error probability P <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">e</sub> of any such machine. In particular, our findings show that the ratio between the maximal exponential decay rate of P <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">e</sub> with S for a deterministic machine and for a randomized one, can become unbounded, complementing a result by Hellman."}}
{"id": "9Ts_v_jp1Q", "cdate": 1577836800000, "mdate": 1683982398835, "content": {"title": "Binary Hypothesis Testing with Deterministic Finite-Memory Decision Rules", "abstract": "In this paper we consider the problem of binary hypothesis testing with finite memory systems. Let $X_1,X_2,\\ldots$ be a sequence of independent identically distributed Bernoulli random variables, with expectation $p$ under $\\mathcal{H}_0$ and $q$ under $\\mathcal{H}_1$. Consider a finite-memory deterministic machine with $S$ states that updates its state $M_n \\in \\{1,2,\\ldots,S\\}$ at each time according to the rule $M_n = f(M_{n-1},X_n)$, where $f$ is a deterministic time-invariant function. Assume that we let the process run for a very long time ($n\\rightarrow \\infty)$, and then make our decision according to some mapping from the state space to the hypothesis space. The main contribution of this paper is a lower bound on the Bayes error probability $P_e$ of any such machine. In particular, our findings show that the ratio between the maximal exponential decay rate of $P_e$ with $S$ for a deterministic machine and for a randomized one, can become unbounded, complementing a result by Hellman."}}
{"id": "HVa-5VZ7OAK", "cdate": 1546300800000, "mdate": 1683982398831, "content": {"title": "Distributed Source Simulation With No Communication", "abstract": "We consider the problem of distributed source simulation with no communication, in which Alice and Bob observe sequences $U^n$ and $V^n$ respectively, drawn from a joint distribution $p_{UV}^{\\otimes n}$, and wish to locally generate sequences $X^n$ and $Y^n$ respectively with a joint distribution that is close (in KL divergence) to $p_{XY}^{\\otimes n}$. We provide a single-letter condition under which such a simulation is asymptotically possible with a vanishing KL divergence. Our condition is nontrivial only in the case where the G\\`acs-K\\\"orner (GK) common information between $U$ and $V$ is nonzero, and we conjecture that only scalar Markov chains $X-U-V-Y$ can be simulated otherwise. Motivated by this conjecture, we further examine the case where both $p_{UV}$ and $p_{XY}$ are doubly symmetric binary sources with parameters $p,q\\leq 1/2$ respectively. While it is trivial that in this case $p\\leq q$ is both necessary and sufficient, we show that when $p$ is close to $q$ then any successful simulation is close to being scalar in the total variation sense."}}
{"id": "AUK5vHqKMM", "cdate": 1546300800000, "mdate": 1683982398869, "content": {"title": "Some Results on Distributed Source Simulation with no Communication", "abstract": "We consider the problem of distributed source simulation with no communication, in which Alice and Bob observe sequences U <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">n</sup> and V <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">n</sup> respectively, drawn from a joint distribution $p_{UV}^{\\otimes n}$, and wish to locally generate sequences X <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">n</sup> and Y <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">n</sup> respectively with a joint distribution that is close (in KL divergence) to $p_{XY}^{\\otimes n}$. We provide a single-letter condition under which such a simulation is asymptotically possible with a vanishing KL divergence. Our condition is nontrivial only in the case where the G\u00e0cs-K\u00f6rner (GK) common information between U and V is nonzero, and we conjecture that only scalar Markov chains $X-U-V-Y$ can be simulated otherwise. Motivated by this conjecture, we further examine the case where both p <inf xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">UV</inf> and p <inf xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">XY</inf> are doubly symmetric binary sources with parameters $p, q\\leq 1/2$ respectively. While it is trivial that in this case $p\\leq q$ is both necessary and sufficient, we show that when p is close to q then any successful simulation is close to being scalar in the total variation sense."}}
