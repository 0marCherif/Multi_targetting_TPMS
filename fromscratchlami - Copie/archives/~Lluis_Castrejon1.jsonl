{"id": "P9wa0rGtFIl", "cdate": 1687232052672, "mdate": 1687232052672, "content": {"title": "Encyclopedic VQA: Visual questions about detailed properties of fine-grained categories", "abstract": "We propose Encyclopedic-VQA, a large scale visual question answering (VQA) dataset featuring visual questions about detailed properties of fine-grained categories and instances. It contains 221k unique question+answer pairs each matched with (up to) 5 images, resulting in a total of 1M VQA samples. Moreover, our dataset comes with a controlled knowledge base derived from Wikipedia, marking the evidence to support each answer. Empirically, we show that our dataset poses a hard challenge for large vision+language models as they perform poorly on our dataset: PaLI [14] is state-of-the-art on OK-VQA [37], yet it only achieves 13.0% accuracy on our dataset. Moreover, we experimentally show that progress on answering our encyclopedic questions can be achieved by augmenting large models with a mechanism that retrieves relevant information from the knowledge base. An oracle experiment with perfect retrieval achieves 87.0% accuracy on the single-hop portion of our dataset, and an automatic retrieval-augmented prototype yields 48.8%. We believe that our dataset enables future research on retrieval-augmented vision+language models."}}
{"id": "SI7NHHO85eq", "cdate": 1646057533137, "mdate": null, "content": {"title": "INFERNO: Inferring Object-Centric 3D Scene Representations without Supervision", "abstract": "We propose INFERNO, a method to infer object-centric representations of visual scenes without annotations.\nOur method decomposes a scene into multiple objects, with each object having a structured representation that disentangles its shape, appearance and pose.\nEach object representation defines a localized neural radiance field used to generate 2D views of the scene through differentiable rendering. \nOur model is subsequently trained by minimizing a reconstruction loss between inputs and corresponding rendered scenes.  \nWe empirically show that INFERNO discovers objects in a scene without supervision.\nWe also validate the interpretability of the learned representations by manipulating inferred scenes and showing the corresponding effect in the rendered output.\nFinally, we demonstrate the usefulness of our 3D object representations in a visual reasoning task using the CATER dataset."}}
{"id": "bDOU-vlPM8n", "cdate": 1635261618823, "mdate": null, "content": {"title": "VIM: Variational Independent Modules for Video Prediction", "abstract": "We introduce a variational inference model called VIM, for Variational Independent Modules, for sequential data that learns and infers latent representations as a set of objects and discovers modular causal mechanisms over these objects. These mechanisms - which we call modules - are independently parametrized, define the stochastic transitions of entities and are shared across entities.  At each time step, our model infers from a low-level input sequence a high-level sequence of categorical latent variables to select which transition modules to apply to which high-level object. We evaluate this model in video prediction tasks where the goal is to predict multi-modal future events given previous observations. We demonstrate empirically that VIM can model 2D visual sequences in an interpretable way and is able to identify the underlying dynamically instantiated mechanisms of the generation process.  We additionally show that the learnt modules can be composed at test time to generalize to out-of-distribution observations."}}
{"id": "YVa8X_2I1b", "cdate": 1632875526557, "mdate": null, "content": {"title": "INFERNO: Inferring Object-Centric 3D Scene Representations without Supervision", "abstract": "We propose INFERNO, a method to infer object-centric representations of visual scenes without relying on annotations. Our method learns to decompose a scene into multiple objects, each object having a structured representation that disentangles its shape, appearance and 3D pose. To impose this structure we rely on recent advances in neural 3D rendering. Each object representation defines a localized neural radiance field that is used to generate 2D views of the scene through a differentiable rendering process. Our model is subsequently trained by minimizing a reconstruction loss between inputs and corresponding rendered scenes. We empirically show that INFERNO discovers objects in a scene without supervision. We also validate the interpretability of the learned representations by manipulating inferred scenes and showing the corresponding effect in the rendered output. Finally, we demonstrate the usefulness of our 3D object representations in a visual reasoning task using the CATER dataset."}}
{"id": "-kigPjfTIGd", "cdate": 1601308250938, "mdate": null, "content": {"title": "SSW-GAN: Scalable Stage-wise Training of Video GANs", "abstract": "Current state-of-the-art generative models for videos have high computational requirements that impede high resolution generations beyond a few frames. In this work we propose a stage-wise strategy to train Generative Adversarial Networks (GANs) for videos.  We decompose the generative process to first produce a downsampled video that is then spatially upscaled and temporally interpolated by subsequent stages. Upsampling stages are applied locally on temporal chunks of previous outputs to manage the computational complexity. Stages are defined as Generative Adversarial Networks, which are trained sequentially and independently. We validate our approach on Kinetics-600 and BDD100K, for which we train a three stage model capable of generating 128x128 videos with 100 frames."}}
{"id": "m0ECRXO6QlP", "cdate": 1601308227391, "mdate": null, "content": {"title": "Supervision Accelerates Pre-training in Contrastive Semi-Supervised Learning of Visual Representations", "abstract": "We investigate a strategy for improving the efficiency of contrastive learning of visual representations by leveraging a small amount of supervised information during pre-training. We propose a semi-supervised loss, SuNCEt, based on noise-contrastive estimation and neighbourhood component analysis, that aims to distinguish examples of different classes in addition to the self-supervised instance-wise pretext tasks. On ImageNet, we find that SuNCEt can be used to match the semi-supervised learning accuracy of previous contrastive approaches while using less than half the amount of pre-training and compute. Our main insight is that leveraging even a small amount of labeled data during pre-training, and not only during fine-tuning, provides an important signal that can significantly accelerate contrastive learning of visual representations."}}
{"id": "4bW5rotZZ0c", "cdate": 1599055435364, "mdate": null, "content": {"title": "Recovering petaflops in contrastive semi-supervised learning of visual representations", "abstract": "We investigate a strategy for improving the computational efficiency of contrastive learning of visual representations by leveraging a small amount of supervised information during pre-training. We propose a semi-supervised loss, SuNCEt, based on noise-contrastive estimation, that aims to distinguish examples of different classes in addition to the self-supervised instance-wise pretext tasks. We find that SuNCEt can be used to match the semi-supervised learning accuracy of previous contrastive approaches with significantly less computational effort. Our main insight is that leveraging even a small amount of labeled data during pre-training, and not only during fine-tuning, provides an important signal that can significantly accelerate contrastive learning of visual representations."}}
{"id": "SQDsYEl_pH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Cross-Modal Scene Networks.", "abstract": "People can recognize scenes across many different modalities beyond natural images. In this paper, we investigate how to learn cross-modal scene representations that transfer across modalities. To study this problem, we introduce a new cross-modal scene dataset. While convolutional neural networks can categorize scenes well, they also learn an intermediate representation not aligned across modalities, which is undesirable for cross-modal transfer applications. We present methods to regularize cross-modal convolutional neural networks so that they have a shared representation that is agnostic of the modality. Our experiments suggest that our scene representation can help transfer representations across modalities for retrieval. Moreover, our visualizations suggest that units emerge in the shared representation that tend to activate on consistent concepts independently of the modality."}}
{"id": "HJbVaa-_ZS", "cdate": 1514764800000, "mdate": null, "content": {"title": "MovieGraphs: Towards Understanding Human-Centric Situations From Videos", "abstract": "There is growing interest in artificial intelligence to build socially intelligent robots. This requires machines to have the ability to \"read\" people's emotions, motivations, and other factors that affect behavior. Towards this goal, we introduce a novel dataset called MovieGraphs which provides detailed, graph-based annotations of social situations depicted in movie clips. Each graph consists of several types of nodes, to capture who is present in the clip, their emotional and physical attributes, their relationships (i.e., parent/child), and the interactions between them. Most interactions are associated with topics that provide additional details, and reasons that give motivations for actions. In addition, most interactions and many attributes are grounded in the video with time stamps. We provide a thorough analysis of our dataset, showing interesting common-sense correlations between different social aspects of scenes, as well as across scenes over time. We propose a method for querying videos and text with graphs, and show that: 1) our graphs contain rich and sufficient information to summarize and localize each scene; and 2) subgraphs allow us to describe situations at an abstract level and retrieve multiple semantically relevant situations. We also propose methods for interaction understanding via ordering, and reason understanding. MovieGraphs is the first benchmark to focus on inferred properties of human-centric situations, and opens up an exciting avenue towards socially-intelligent AI agents."}}
{"id": "r1WRCp-_bS", "cdate": 1483228800000, "mdate": null, "content": {"title": "Annotating Object Instances with a Polygon-RNN", "abstract": "We propose an approach for semi-automatic annotation of object instances. While most current methods treat object segmentation as a pixel-labeling problem, we here cast it as a polygon prediction task, mimicking how most current datasets have been annotated. In particular, our approach takes as input an image crop and sequentially produces vertices of the polygon outlining the object. This allows a human annotator to interfere at any time and correct a vertex if needed, producing as accurate segmentation as desired by the annotator. We show that our approach speeds up the annotation process by a factor of 4.7 across all classes in Cityscapes, while achieving 78:4% agreement in IoU with original ground-truth, matching the typical agreement between human annotators. For cars, our speed-up factor is 7.3 for an agreement of 82:2%. We further show generalization capabilities of our approach to unseen datasets."}}
