{"id": "wgDzU5E53iW", "cdate": 1699878177899, "mdate": 1699878177899, "content": {"title": "Large-scale generative simulation artificial intelligence: The next hotspot", "abstract": "The concept of GenAI has been developed for decades. Until recently, it has impressed us with substantial breakthroughs in natural language processing and computer vision, actively engaging in industrial scenarios. Noticing the practical challenges, e.g., limited learning resources, and overly dependencies on scientific discovery empiricism, we nominate large-scale generative simulation artificial intelligence (LS-GenAI) as the next hotspot for GenAI to connect. Key to generative simulation artificial intelligence are scenario generation and fast skill transfer, which form a closed loop in practice."}}
{"id": "YPx39wi8KkA", "cdate": 1672065820809, "mdate": 1672065820809, "content": {"title": "Model-based Meta Reinforcement Learning using Graph Structured Surrogate Models and Amortized Policy Search", "abstract": "Reinforcement learning is a promising paradigm for solving sequential decision-making problems, but low data efficiency and weak generalization across tasks are bottlenecks in real-world applications. Model-based meta reinforcement learning addresses these issues by learning dynamics and leveraging knowledge from prior experience. In this paper, we take a closer look at this framework and propose a new posterior sampling based approach that consists of a new model to identify task dynamics together with an amortized policy optimization step. We show that our model, called a graph structured surrogate model (GSSM), achieves competitive dynamics prediction performance with lower model complexity. Moreover, our approach in policy search is able to obtain high returns and allows fast execution by avoiding test-time policy gradient updates.\n"}}
{"id": "A7v2DqLjZdq", "cdate": 1663850032670, "mdate": null, "content": {"title": "Bridge the Inference Gaps of Neural Processes via Expectation Maximization", "abstract": "The neural process (NP) is a family of computationally efficient models for learning distributions over functions. However, it suffers from under-fitting and shows suboptimal performance in practice. Researchers have primarily focused on incorporating diverse structural inductive biases, e.g. attention or convolution, in modeling. The topic of inference suboptimality and an analysis of the NP from the optimization objective perspective has hardly been studied in earlier work. To fix this issue, we propose a surrogate objective of the target log-likelihood of the meta dataset within the expectation maximization framework. The resulting model, referred to as the Self-normalized Importance weighted Neural Process (SI-NP), can learn a more accurate functional prior and has an improvement guarantee concerning the target log-likelihood. Experimental results show the competitive performance of SI-NP over other NPs objectives and illustrate that structural inductive biases, such as attention modules, can also augment our method to achieve SOTA performance."}}
{"id": "ju38DG3sbg6", "cdate": 1652737533204, "mdate": null, "content": {"title": "Learning Expressive Meta-Representations with Mixture of Expert Neural Processes", "abstract": "Neural processes (NPs) formulate exchangeable stochastic processes and are promising models for meta learning that do not require gradient updates during the testing phase. \nHowever, most NP variants place a strong emphasis on a global latent variable. \nThis weakens the approximation power and restricts the scope of applications using NP variants, especially when data generative processes are complicated.\nTo resolve these issues, we propose to combine the Mixture of Expert models with Neural Processes to develop more expressive exchangeable stochastic processes, referred to as Mixture of Expert Neural Processes (MoE-NPs). \nThen we apply MoE-NPs to both few-shot supervised learning and meta reinforcement learning tasks. \nEmpirical results demonstrate MoE-NPs' strong generalization capability to unseen tasks in these benchmarks."}}
{"id": "FRZaKOdHGq", "cdate": 1577836800000, "mdate": null, "content": {"title": "Doubly Stochastic Variational Inference for Neural Processes with Hierarchical Latent Variables", "abstract": "Neural processes (NPs) constitute a family of variational approximate models for stochastic processes with promising properties in computational efficiency and uncertainty quantification. These processes use neural networks with latent variable inputs to induce predictive distributions. However, the expressiveness of vanilla NPs is limited as they only use a global latent variable, while target specific local variation may be crucial sometimes. To address this challenge, we investigate NPs systematically and present a new variant of NP model that we call Doubly Stochastic Variational Neural Process (DSVNP). This model combines the global latent variable and local latent variables for prediction. We evaluate this model in several experiments, and our results demonstrate competitive prediction performance in multi-output regression and uncertainty estimation in classification."}}
