{"id": "zKvm1ETDOq", "cdate": 1663850170452, "mdate": null, "content": {"title": "Is Adversarial Training Really a Silver Bullet for Mitigating Data Poisoning?", "abstract": "Indiscriminate data poisoning can decrease the clean test accuracy of a deep learning model by slightly perturbing its training samples.\nThere is a consensus that such poisons can hardly harm adversarially-trained (AT) models when the adversarial training budget is no less than the poison budget, i.e., $\\epsilon_\\mathrm{adv}\\geq\\epsilon_\\mathrm{poi}$. This consensus, however, is challenged in this paper based on our new attack strategy that induces \\textit{entangled features} (EntF). The existence of entangled features makes the poisoned data become less useful for training a model, no matter if AT is applied or not. We demonstrate that for attacking a CIFAR-10 AT model under a reasonable setting with $\\epsilon_\\mathrm{adv}=\\epsilon_\\mathrm{poi}=8/255$, our EntF yields an accuracy drop of $13.31\\%$, which is $7\\times$ better than existing methods and equal to discarding $83\\%$ training data. We further show the generalizability of EntF to more challenging settings, e.g., higher AT budgets, partial poisoning, unseen model architectures, and stronger (ensemble or adaptive) defenses. We finally provide new insights into the distinct roles of non-robust vs. robust features in poisoning standard vs. AT models and demonstrate the possibility of using a hybrid attack to poison standard and AT models simultaneously. Our code is available at~\\url{https://github.com/WenRuiUSTC/EntF}."}}
{"id": "TUJYLRf2caH", "cdate": 1653595783905, "mdate": null, "content": {"title": "Memorization in NLP Fine-tuning Methods", "abstract": "Large language models are shown to present privacy risks through memorization of training data, and several recent works have studied such risks for the pre-training phase. Little attention, however, has been given to the fine-tuning phase and it is not well understood how different fine-tuning  methods (such as fine-tuning the full model, the model head, and adapter) compare in terms of memorization risk. This presents increasing concern as the ``pre-train and fine-tune'' paradigm proliferates. In this paper, we empirically study memorization of fine-tuning methods using membership inference and extraction attacks, and show that their susceptibility to attacks is very different. We observe that fine-tuning the head of the model has the highest susceptibility to attacks, whereas fine-tuning smaller adapters appears to be less vulnerable to known extraction attacks."}}
{"id": "_KvEGFkNBN", "cdate": 1609459200000, "mdate": null, "content": {"title": "Locally Differentially Private Heavy Hitter Identification", "abstract": "The notion of Local Differential Privacy (LDP) enables users to answer sensitive questions while preserving their privacy. The basic LDP frequency oracle protocol enables the aggregator to estimate the frequency of any value. But when the domain of input values is large, finding the most frequent values, also known as the heavy hitters, by estimating the frequencies of all possible values, is computationally infeasible. In this paper, we propose an LDP protocol for identifying heavy hitters. In our proposed protocol, which we call Prefix Extending Method (PEM), users are divided into groups, with each group reporting a prefix of her value. We analyze how to choose optimal parameters for the protocol and identify two design principles for designing LDP protocols with high utility. Experiments show that under the same privacy guarantee and computational cost, PEM has better utility on both synthetic and real-world datasets than existing solutions."}}
{"id": "Ey8AbDN1_Yb", "cdate": 1609459200000, "mdate": null, "content": {"title": "Graph Unlearning", "abstract": "Machine unlearning is a process of removing the impact of some training data from the machine learning (ML) models upon receiving removal requests. While straightforward and legitimate, retraining the ML model from scratch incurs a high computational overhead. To address this issue, a number of approximate algorithms have been proposed in the domain of image and text data, among which SISA is the state-of-the-art solution. It randomly partitions the training set into multiple shards and trains a constituent model for each shard. However, directly applying SISA to the graph data can severely damage the graph structural information, and thereby the resulting ML model utility. In this paper, we propose GraphEraser, a novel machine unlearning framework tailored to graph data. Its contributions include two novel graph partition algorithms and a learning-based aggregation method. We conduct extensive experiments on five real-world graph datasets to illustrate the unlearning efficiency and model utility of GraphEraser. It achieves 2.06$\\times$ (small dataset) to 35.94$\\times$ (large dataset) unlearning time improvement. On the other hand, GraphEraser achieves up to $62.5\\%$ higher F1 score and our proposed learning-based aggregation method achieves up to $112\\%$ higher F1 score.\\footnote{Our code is available at \\url{https://github.com/MinChen00/Graph-Unlearning}.}"}}
{"id": "s4qS4sYqtK", "cdate": 1577836800000, "mdate": null, "content": {"title": "Improving Utility and Security of the Shuffler-based Differential Privacy", "abstract": ""}}
{"id": "pkS65FrcMM", "cdate": 1577836800000, "mdate": null, "content": {"title": "Locally Differentially Private Frequency Estimation with Consistency", "abstract": ""}}
{"id": "pLFzn9frg82", "cdate": 1577836800000, "mdate": null, "content": {"title": "Towards Effective Differential Privacy Communication for Users' Data Sharing Decision and Comprehension", "abstract": "Differential privacy protects an individual's privacy by perturbing data on an aggregated level (DP) or individual level (LDP). We report four online human-subject experiments investigating the effects of using different approaches to communicate differential privacy techniques to laypersons in a health app data collection setting. Experiments 1 and 2 investigated participants' data disclosure decisions for low-sensitive and high-sensitive personal information when given different DP or LDP descriptions. Experiments 3 and 4 uncovered reasons behind participants' data sharing decisions, and examined participants' subjective and objective comprehensions of these DP or LDP descriptions. When shown descriptions that explain the implications instead of the definition/processes of DP or LDP technique, participants demonstrated better comprehension and showed more willingness to share information with LDP than with DP, indicating their understanding of LDP's stronger privacy guarantee compared with DP."}}
{"id": "pD2kYF74F_O", "cdate": 1577836800000, "mdate": null, "content": {"title": "When Machine Unlearning Jeopardizes Privacy", "abstract": "The right to be forgotten states that a data owner has the right to erase their data from an entity storing it. In the context of machine learning (ML), the right to be forgotten requires an ML model owner to remove the data owner's data from the training set used to build the ML model, a process known as machine unlearning. While originally designed to protect the privacy of the data owner, we argue that machine unlearning may leave some imprint of the data in the ML model and thus create unintended privacy risks. In this paper, we perform the first study on investigating the unintended information leakage caused by machine unlearning. We propose a novel membership inference attack that leverages the different outputs of an ML model's two versions to infer whether a target sample is part of the training set of the original model but out of the training set of the corresponding unlearned model. Our experiments demonstrate that the proposed membership inference attack achieves strong performance. More importantly, we show that our attack in multiple cases outperforms the classical membership inference attack on the original ML model, which indicates that machine unlearning can have counterproductive effects on privacy. We notice that the privacy degradation is especially significant for well-generalized ML models where classical membership inference does not perform well. We further investigate four mechanisms to mitigate the newly discovered privacy risks and show that releasing the predicted label only, temperature scaling, and differential privacy are effective. We believe that our results can help improve privacy protection in practical implementations of machine unlearning. Our code is available at https://github.com/MinChen00/UnlearningLeaks."}}
{"id": "d8UxjntzIWT", "cdate": 1577836800000, "mdate": null, "content": {"title": "PrivSyn: Differentially Private Data Synthesis", "abstract": "In differential privacy (DP), a challenging problem is to generate synthetic datasets that efficiently capture the useful information in the private data. The synthetic dataset enables any task to be done without privacy concern and modification to existing algorithms. In this paper, we present PrivSyn, the first automatic synthetic data generation method that can handle general tabular datasets (with 100 attributes and domain size $>2^{500}$). PrivSyn is composed of a new method to automatically and privately identify correlations in the data, and a novel method to generate sample data from a dense graphic model. We extensively evaluate different methods on multiple datasets to demonstrate the performance of our method."}}
{"id": "cuafFhkPlXx", "cdate": 1577836800000, "mdate": null, "content": {"title": "PURE: A Framework for Analyzing Proximity-based Contact Tracing Protocols", "abstract": "Many proximity-based tracing (PCT) protocols have been proposed and deployed to combat the spreading of COVID-19. In this paper, we take a systematic approach to analyze PCT protocols. We identify a list of desired properties of a contact tracing design from the four aspects of Privacy, Utility, Resiliency, and Efficiency (PURE). We also identify two main design choices for PCT protocols: what information patients report to the server, and which party performs the matching. These two choices determine most of the PURE properties and enable us to conduct a comprehensive analysis and comparison of the existing protocols."}}
