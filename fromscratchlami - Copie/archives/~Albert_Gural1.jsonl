{"id": "SkeXL0NKwH", "cdate": 1569439307144, "mdate": null, "content": {"title": "Low Rank Training of Deep Neural Networks for Emerging Memory Technology", "abstract": "The recent success of neural networks for solving difficult decision tasks has incentivized incorporating smart decision making \"at the edge.\" However, this work has traditionally focused on neural network inference, rather than training, due to memory and compute limitations, especially in emerging non-volatile memory systems, where writes are energetically costly and reduce lifespan. Yet, the ability to train at the edge is becoming increasingly important as it enables applications such as real-time adaptability to device drift and environmental variation, user customization, and federated learning across devices. In this work, we address four key challenges for training on edge devices with non-volatile memory: low weight update density, weight quantization, low auxiliary memory, and online learning. We present a low-rank training scheme that addresses these four challenges while maintaining computational efficiency. We then demonstrate the technique on a representative convolutional neural network across several adaptation problems, where it out-performs standard SGD both in accuracy and in number of weight updates."}}
{"id": "ry-eosWd-S", "cdate": 1546300800000, "mdate": null, "content": {"title": "Memory-Optimal Direct Convolutions for Maximizing Classification Accuracy in Embedded Applications", "abstract": "In the age of Internet of Things (IoT), embedded devices ranging from ARM Cortex M0s with hundreds of KB of RAM to Arduinos with 2KB RAM are expected to perform increasingly sophisticated classific..."}}
