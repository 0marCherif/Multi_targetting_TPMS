{"id": "NIZqs6qn3Q", "cdate": 1671901195026, "mdate": 1671901195026, "content": {"title": "The good, the bad and the ugly sides of data augmentation: An implicit spectral regularization perspective", "abstract": "Data augmentation (DA) is a powerful workhorse for bolstering performance in modern machine\nlearning. Specific augmentations like translations and scaling in computer vision are traditionally\nbelieved to improve generalization by generating new (artificial) data from the same distribution.\nHowever, this traditional viewpoint does not explain the success of prevalent augmentations\nin modern machine learning (e.g. randomized masking, cutout, mixup), that greatly alter the\ntraining data distribution. In this work, we develop a new theoretical framework to characterize\nthe impact of a general class of DA on underparameterized and overparameterized linear model\ngeneralization. Our framework reveals that DA induces implicit spectral regularization through a\ncombination of two distinct effects: a) manipulating the relative proportion of eigenvalues of\nthe data covariance matrix in a training-data-dependent manner, and b) uniformly boosting\nthe entire spectrum of the data covariance matrix through ridge regression. These effects,\nwhen applied to popular augmentations, give rise to a wide variety of phenomena, including\ndiscrepancies in generalization between over-parameterized and under-parameterized regimes and\ndifferences between regression and classification tasks. Our framework highlights the nuanced\nand sometimes surprising impacts of DA on generalization, and serves as a testbed for novel\naugmentation design."}}
{"id": "QX2KFE5pMtb", "cdate": 1663268552946, "mdate": 1663268552946, "content": {"title": "Mine Your Own vieW: Self-Supervised Learning Through Across-Sample Prediction", "abstract": "State-of-the-art methods for self-supervised learning (SSL) build representations by maximizing the similarity between different transformed \"views\" of a sample. Without sufficient diversity in the transformations used to create views, however, it can be difficult to overcome nuisance variables in the data and build rich representations. This motivates the use of the dataset itself to find similar, yet distinct, samples to serve as views for one another. In this paper, we introduce Mine Your Own vieW (MYOW), a new approach for self-supervised learning that looks within the dataset to define diverse targets for prediction. The idea behind our approach is to actively mine views, finding samples that are neighbors in the representation space of the network, and then predict, from one sample's latent representation, the representation of a nearby sample. After showing the promise of MYOW on benchmarks used in computer vision, we highlight the power of this idea in a novel application in neuroscience where SSL has yet to be applied. When tested on multi-unit neural recordings, we find that MYOW outperforms other self-supervised approaches in all examples (in some cases by more than 10%), and often surpasses the supervised baseline. With MYOW, we show that it is possible to harness the diversity of the data to build rich views and leverage self-supervision in new domains where augmentations are limited or unknown."}}
{"id": "ZRPRjfAF3yd", "cdate": 1621630020326, "mdate": null, "content": {"title": "Drop, Swap, and Generate: A Self-Supervised Approach for Generating Neural Activity", "abstract": "Meaningful and simplified representations of neural activity can yield insights into how and what information is being processed within a neural circuit. However, without labels, finding representations that reveal the link between the brain and behavior can be challenging. Here, we introduce a novel unsupervised approach for learning disentangled representations of neural activity called Swap-VAE. Our approach combines a generative modeling framework with an instance-specific alignment loss that tries to maximize the representational similarity between transformed views of the input (brain state). These transformed (or augmented) views are created by dropping out neurons and jittering samples in time, which intuitively should lead the network to a representation that maintains both temporal consistency and invariance to the specific neurons used to represent the neural state. Through evaluations on both synthetic data and neural recordings from hundreds of neurons in different primate brains, we show that it is possible to build representations that disentangle neural datasets along relevant latent dimensions linked to behavior."}}
{"id": "XHxFdH0dgSt", "cdate": 1620838166201, "mdate": null, "content": {"title": "Making transport more robust and interpretable by moving data through a small number of anchor points", "abstract": "Optimal transport (OT) is a widely used technique for distribution alignment, with applications throughout the machine learning, graphics, and vision communities. Without any additional structural assumptions on trans-port, however, OT can be fragile to outliers or noise, especially in high dimensions. Here, we introduce a new form of structured OT that simultaneously learns low-dimensional structure in data while leveraging this structure to solve the alignment task. Compared with OT, the resulting transport plan has better structural interpretability, highlighting the connections between individual data points and local geometry, and is more robust to noise and sampling. We apply the method to synthetic as well as real datasets, where we show that our method can facilitate alignment in noisy settings and can be used to both correct and interpret domain shift."}}
{"id": "IVk_yq6gtho", "cdate": 1620760874619, "mdate": null, "content": {"title": "A Modular Analysis of Provable Acceleration via Polyak's momentum: Training a Wide ReLU Network and a Deep Linear Network", "abstract": "Incorporating a so-called ``momentum'' dynamic in gradient descent methods is widely used in neural net training as it has been broadly observed that, at least empirically, it often leads to significantly faster convergence. At the same time, there are very few theoretical guarantees in the literature to explain this apparent acceleration effect. Even for the classical strongly convex quadratic problems, several existing results only show Polyak's momentum has an accelerated linear rate asymptotically. In this paper, we first revisit the quadratic problems and show a non-asymptotic accelerated linear rate of Polyak's momentum. Then, we provably show that Polyak's momentum achieves acceleration for training a one-layer wide ReLU network and a deep linear network,\nwhich are perhaps the two most popular canonical models for studying optimization and deep learning in the literature. Prior work Du et al. and Wu et al. showed that using vanilla gradient descent, and with an use of over-parameterization, the error decays as $(1- \\Theta(\\frac{1}{ \\kappa'}))^t$ after $t$ iterations, where $\\kappa'$ is the condition number of a Gram Matrix. Our result shows that with the appropriate choice of parameters Polyak's momentum has a rate of $(1-\\Theta(\\frac{1}{\\sqrt{\\kappa'}}))^t$. For the deep linear network, prior work Hu et al. showed that vanilla gradient descent has a rate of $(1-\\Theta(\\frac{1}{\\kappa}))^t$, where $\\kappa$ is the condition number of a data matrix.  Our result shows an acceleration rate $(1- \\Theta(\\frac{1}{\\sqrt{\\kappa}}))^t$ is achievable by Polyak's momentum. All the results in this work are obtained from a modular analysis, which can be of independent interest. This work establishes that momentum does indeed speed up neural net training.\n"}}
{"id": "rkeNfp4tPr", "cdate": 1569438987728, "mdate": null, "content": {"title": "Escaping Saddle Points Faster with Stochastic Momentum", "abstract": "Stochastic gradient descent (SGD) with stochastic momentum is popular in nonconvex stochastic optimization and particularly for the training of deep neural networks. In standard SGD, parameters are updated by improving along the path of the gradient at the current iterate on a batch of examples, where the addition of a ``momentum'' term biases the update in the direction of the previous change in parameters. In non-stochastic convex optimization one can show that a momentum adjustment provably reduces convergence time in many settings, yet such results have been elusive in the stochastic and non-convex settings. At the same time, a widely-observed empirical phenomenon is that in training deep networks stochastic momentum appears to significantly improve convergence time, variants of it have flourished in the development of other popular update methods, e.g. ADAM, AMSGrad, etc. Yet theoretical justification for the use of stochastic momentum has remained a significant open question. In this paper we propose an answer: stochastic momentum improves deep network training because it modifies SGD to escape saddle points faster and, consequently, to more quickly find a second order stationary point. Our theoretical results also shed light on the related question of how to choose the ideal momentum parameter--our analysis suggests that $\\beta \\in [0,1)$ should be large (close to 1), which comports with empirical findings. We also provide experimental findings that further validate these conclusions."}}
