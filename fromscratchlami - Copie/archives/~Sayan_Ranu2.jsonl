{"id": "r1kArqMZcD", "cdate": 1684248175489, "mdate": 1684248175489, "content": {"title": "LiMIP: Lifelong learning to solve mixed integer programs", "abstract": "Mixed Integer programs (MIPs) are typically solved by the\nBranch-and-Bound algorithm. Recently, Learning to imitate\nfast approximations of the expert strong branching heuristic\nhas gained attention due to its success in reducing the running\ntime for solving MIPs. However, existing learning-to-branch\nmethods assume that the entire training data is available in a\nsingle session of training. This assumption is often not true,\nand if the training data is supplied in continual fashion over\ntime, existing techniques suffer from catastrophic forgetting.\nIn this work, we study the hitherto unexplored paradigm of\nLifelong Learning to Branch on Mixed Integer Programs. To\nmitigate catastrophic forgetting, we propose LIMIP, which is\npowered by the idea of modeling an MIP instance in the form\nof a bipartite graph, which we map to an embedding space\nusing a bipartite Graph Attention Network. This rich embed-\nding space avoids catastrophic forgetting through the appli-\ncation of knowledge distillation and elastic weight consolida-\ntion, wherein we learn the parameters key towards retaining\nefficacy and are therefore protected from significant drift. We\nevaluate LIMIP on a series of NP-hard problems and estab-\nlish that in comparison to existing baselines, LIMIP is up to\n50% better when confronted with lifelong learning"}}
{"id": "Ugl-B_at5n", "cdate": 1675970199316, "mdate": null, "content": {"title": "Learning the Dynamics of Physical Systems with Hamiltonian Graph Neural Networks", "abstract": "Inductive biases in the form of conservation laws have been shown to provide superior performance for modeling physical systems. Here, we present Hamiltonian graph neural network (HGNN), a physics-informed GNN that learns the dynamics directly from the trajectory. We evaluate the performance of HGNN on spring, pendulum, and gravitational systems and show that it outperforms other Hamiltonian-based neural networks. We also demonstrate the zero-shot generalizability of HGNN to unseen hybrid spring-pendulum systems and system sizes that are two orders of magnitude larger than the training systems. HGNN provides excellent inference in all the systems providing a stable trajectory. Altogether, HGNN presents a promising approach to modeling complex physical systems directly from their trajectory."}}
{"id": "LG_1rz2g42", "cdate": 1672871514189, "mdate": 1672871514189, "content": {"title": "Inferring Uncertain Trajectories from Partial Observations", "abstract": "The explosion in the availability of GPS-enabled devices has resulted in an abundance of trajectory data. In reality, however, majority of these trajectories are collected at a low sampling rate and only provide partial observations on their actually traversed routes. Consequently, they are mired with uncertainty. In this paper, we develop a technique called InferTra to infer uncertain trajectories from network-constrained partial observations. Rather than predicting the most likely route, the inferred uncertain trajectory takes the form of an edge-weighted graph and summarizes all probable routes in a holistic manner. For trajectory inference, InferTra employs Gibbs sampling by learning a Network Mobility Model (NMM) from a database of historical trajectories. Extensive experiments on real trajectory databases show that the graph-based approach of InferTra is up to 50% more accurate, 20 times faster, and immensely more versatile than state-of-the-art techniques."}}
{"id": "ATLEl_izD87", "cdate": 1663850310157, "mdate": null, "content": {"title": "Enhancing the Inductive Biases of Graph Neural ODE for Modeling Physical Systems", "abstract": "Neural networks with physics-based inductive biases such as Lagrangian neural networks (LNNs), and Hamiltonian neural networks (HNNs) learn the dynamics of physical systems by encoding strong inductive biases. Alternatively, Neural ODEs with appropriate inductive biases have also been shown to give similar performances. However, these models, when applied to particle-based systems, are transductive in nature and hence, do not generalize to large system sizes. In this paper, we present a graph-based neural ODE, GNODE, to learn the time evolution of dynamical systems. Further, we carefully analyze the role of different inductive biases on the performance of GNODE. We show that similar to LNN and HNN, encoding the constraints explicitly can significantly improve the training efficiency and performance of GNODE significantly. Our experiments also assess the value of additional inductive biases, such as Newton\u2019s third law, on the final performance of the model. We demonstrate that inducing these biases can enhance the performance of the model by orders of magnitude in terms of both energy violation and rollout error. Interestingly, we observe that the GNODE trained with the most effective inductive biases, namely MCGNODE, outperforms the graph versions of LNN and HNN, namely, Lagrangian graph networks (LGN) and Hamiltonian graph networks (HGN) in terms of energy violation error by \u223c4 orders of magnitude for a pendulum system, and \u223c2 orders of magnitude for spring systems. These results suggest that NODE-based systems can give competitive performances with energy-conserving neural networks by employing appropriate inductive biases."}}
{"id": "tXEe-Ew_ikh", "cdate": 1654498451092, "mdate": null, "content": {"title": "Unravelling the Performance of Physics-informed Graph Neural Networks for Dynamical Systems", "abstract": "Recently, graph neural networks have been gaining a lot of attention to simulate dynamical systems due to their inductive nature leading to zero-shot generalizability. Similarly, physics-informed inductive biases in deep-learning frameworks have been shown to give superior performance in learning the dynamics of physical systems. There is a growing volume of literature that attempts to combine these two approaches. Here, we evaluate the performance of thirteen different graph neural networks, namely, Hamiltonian and Lagrangian graph neural networks, graph neural ODE, and their variants with explicit constraints and different architectures. We briefly explain the theoretical formulation highlighting the similarities and differences in the inductive biases and graph architecture of these systems. Then, we evaluate them on spring, pendulum, and gravitational and 3D deformable solid systems to compare the performance in terms of rollout error, conserved quantities such as energy and momentum, and generalizability to unseen system sizes. Our study demonstrates that GNNs with additional inductive biases, such as explicit constraints and decoupling of kinetic and potential energies, exhibit significantly enhanced performance. Further, all the physics-informed GNNs exhibit zero-shot generalizability to system sizes an order of magnitude larger than the training system, thus providing a promising route to simulate large-scale realistic systems."}}
{"id": "nOdfIbo3A-F", "cdate": 1652737733754, "mdate": null, "content": {"title": "Learning Articulated Rigid Body Dynamics with Lagrangian Graph Neural Network", "abstract": "Lagrangian  and Hamiltonian neural networks LNN and HNNs, respectively) encode strong inductive biases that allow them to outperform other models of physical systems significantly. However, these models have, thus far, mostly been limited to simple systems such as pendulums and springs or a single rigid body such as a gyroscope or a rigid rotor. Here, we present a Lagrangian graph neural network (LGNN) that can learn the dynamics of articulated rigid bodies by exploiting their topology. We demonstrate the performance of LGNN by learning the dynamics of ropes, chains, and trusses with the bars modeled as rigid bodies. LGNN also exhibits generalizability---LGNN trained on chains with a few segments exhibits generalizability to simulate a chain with large number of links and arbitrary link length. We also show that the LGNN can simulate unseen hybrid systems including bars and chains, on which they have not been trained on. Specifically, we show that the LGNN can be used to model the dynamics of complex real-world structures such as the stability of tensegrity structures. Finally, we discuss the non-diagonal nature of the mass matrix and its ability to generalize in complex systems."}}
{"id": "3LBxVcnsEkV", "cdate": 1652737464828, "mdate": null, "content": {"title": "GREED: A Neural Framework for Learning Graph Distance Functions", "abstract": "Similarity search in graph databases is one of the most fundamental operations in graph analytics. Among various distance functions, graph and subgraph edit distances (GED and SED respectively) are two of the most popular and expressive measures. Unfortunately, exact computations for both are NP-hard. To overcome this computational bottleneck, neural approaches to learn and predict edit distance in polynomial time have received much interest. While considerable progress has been made, there exist limitations that need to be addressed. First, the efficacy of an approximate distance function lies not only in its approximation accuracy, but also in the preservation of its properties. To elaborate, although GED is a metric, its neural approximations do not provide such a guarantee. This prohibits their usage in higher order tasks that rely on metric distance functions, such as clustering or indexing. Second, several existing frameworks for GED do not extend to SED due to SED being asymmetric. In this work, we design a novel siamese graph neural network called Greed, which through a carefully crafted inductive bias, learns GED and SED in a property-preserving manner. Through extensive experiments across $10$ real graph datasets containing up to $7$ million edges, we establish that Greed is not only more accurate than the state of the art, but also up to $3$ orders of magnitude faster. Even more significantly, due to preserving the triangle inequality, the generated embeddings are indexable and consequently, even in a CPU-only environment, Greed is up to $50$ times faster than GPU-powered computations of the closest baseline."}}
{"id": "OD_dnx57ksK", "cdate": 1632875614859, "mdate": null, "content": {"title": "Momentum Conserving Lagrangian Neural Networks", "abstract": "Realistic models of physical world rely on differentiable symmetries that, in turn, correspond to conservation laws. Recent works on Lagrangian and Hamiltonian neural networks show that the underlying symmetries of a system can be easily learned by a neural network when provided with an appropriate inductive bias. However, these models still suffer from issues such as inability to generalize to arbitrary system sizes, poor interpretability, and most importantly, inability to learn translational and rotational symmetries, which lead to the conservation laws of linear and angular momentum, respectively. Here, we present a momentum conserving Lagrangian neural network (MCLNN) that learns the Lagrangian of a system, while also preserving the translational and rotational symmetries. We test our approach on linear and non-linear spring systems, and a gravitational system, demonstrating the energy and momentum conservation. We also show that the model developed can generalize to systems of any arbitrary size. Finally, we discuss the interpretability of the MCLNN, which directly provides physical insights into the interactions of multi-particle systems."}}
{"id": "b30Yre8MzuN", "cdate": 1632875479790, "mdate": null, "content": {"title": "NeuroSED: Learning Subgraph Similarity via Graph Neural Networks", "abstract": "Subgraph similarity search is a fundamental operator in graph analysis. In this framework, given a query graph and a graph database, the goal is to identify subgraphs of the database graphs that are structurally similar to the query. Subgraph edit distance (SED) is one of the most expressive measures of subgraph similarity. In this work, we study the problem of learning SED from a training set of graph pairs and their SED values. Towards that end, we design a novel siamese graph neural network called NeuroSED, which learns an embedding space with a rich structure reminiscent of SED. With the help of a specially crafted inductive bias, NeuroSED not only enables high accuracy but also ensures that the predicted SED, like true SED, satisfies triangle inequality. The design is generic enough to also model graph edit distance (GED), while ensuring that the predicted GED space is metric, like the true GED space. Extensive experiments on real graph datasets, for both SED and GED, establish that NeuroSED achieves $\\approx 2$ times lower RMSE than the state of the art and is $\\approx 18$ times faster than the fastest baseline. Further, owing to its pair-independent embeddings and theoretical properties, NeuroSED allows orders-of-magnitude faster graph/subgraph retrieval."}}
{"id": "q7XJj9_egih", "cdate": 1623139013402, "mdate": null, "content": {"title": "Particulate Matter Dataset Collected with Vehicle Mounted IoT Devices in Delhi-NCR", "abstract": "Air pollution is one of the biggest concerns faced by developing countries like India and the world at large. The capital of India, Delhi and the National Capital Region (NCR), sees life threatening air pollution levels. This paper presents a new Particulate Matter (PM) dataset for Delhi-NCR, which contains PM data recorded over three months from November 2020 to January 2021 over an area spanning 559 square Kms. The data has been collected using vehicle-mounted mobile sensors in collaboration with the Delhi Integrated Multi-Modal Transit System (DIMTS) buses. The 13 bus dataset has been compared with the data over the same period obtained from the pre-existing static sensors, which the buses pass by. Several Machine Learning (ML) problems have been outlined, that can be studied using this dataset, two of which, spatio-temporal interpolation and anomaly detection in IoT networks are detailed in this paper. The dataset is public at https://www.cse.iitd.ac.in/pollutiondata, along with appropriate documentation. We will keep augmenting the website as new data get collected, with more buses and other pollutant sensors (SOx, NOx, COx) added to our deployment in future. "}}
