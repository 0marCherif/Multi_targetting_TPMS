{"id": "ixGrYpp2eyz", "cdate": 1683779751231, "mdate": 1683779751231, "content": {"title": "AlphaEvolve: A Learning Framework to Discover Novel Alphas in Quantitative Investment", "abstract": "Alphas are stock prediction models capturing trading signals in a stock market. A set of effective alphas can generate weakly correlated high returns to diversify the risk. Existing alphas can be categorized into two classes: Formulaic alphas are simple algebraic expressions of scalar features, and thus can generalize well and be mined into a weakly correlated set. Machine learning alphas are data-driven models over vector and matrix features. They are more predictive than formulaic alphas, but are too complex to mine into a weakly correlated set. In this paper, we introduce a new class of alphas to model scalar, vector, and matrix features which possess the strengths of these two existing classes. The new alphas predict returns with high accuracy and can be mined into a weakly correlated set. In addition, we propose a novel alpha mining framework based on AutoML, called AlphaEvolve, to generate the new alphas. To this end, we first propose operators for generating the new alphas and selectively injecting relational domain knowledge to model the relations between stocks. We then accelerate the alpha mining by proposing a pruning technique for redundant alphas. Experiments show that AlphaEvolve can evolve initial alphas into\nthe new alphas with high returns and weak correlations.\n"}}
{"id": "z2q2Xr6OzQ", "cdate": 1672531200000, "mdate": 1674377194681, "content": {"title": "A Dietary Nutrition-aided Healthcare Platform via Effective Food Recognition on a Localized Singaporean Food Dataset", "abstract": "Singapore has been striving to improve the provision of healthcare services to her people. In this course, the government has taken note of the deficiency in regulating and supervising people's nutrient intake, which is identified as a contributing factor to the development of chronic diseases. Consequently, this issue has garnered significant attention. In this paper, we share our experience in addressing this issue and attaining medical-grade nutrient intake information to benefit Singaporeans in different aspects. To this end, we develop the FoodSG platform to incubate diverse healthcare-oriented applications as a service in Singapore, taking into account their shared requirements. We further identify the profound meaning of localized food datasets and systematically clean and curate a localized Singaporean food dataset FoodSG-233. To overcome the hurdle in recognition performance brought by Singaporean multifarious food dishes, we propose to integrate supervised contrastive learning into our food recognition model FoodSG-SCL for the intrinsic capability to mine hard positive/negative samples and therefore boost the accuracy. Through a comprehensive evaluation, we present performance results of the proposed model and insights on food-related healthcare applications. The FoodSG-233 dataset has been released in https://foodlg.comp.nus.edu.sg/."}}
{"id": "ynv3BzYF8E", "cdate": 1672531200000, "mdate": 1699854041791, "content": {"title": "Robust and Transferable Log-based Anomaly Detection", "abstract": "Log messages provide a valuable source of runtime information for ensuring the safety and consistency of systems. Recently, many machine learning and deep learning methods have been proposed to automatically detect anomalous log messages, obviating the need for manual detection by experts. However, we find that in practice, the effectiveness of existing learning-based methods is severely affected by incomplete information and distribution shift. Specifically, each log message can actually be parsed into a fixed number of key information fields, while existing methods analyze log messages using only the log event information and ignore other useful information fields that can be critical to anomaly detection. Further, the distribution of real-world log messages changes continuously due to the dynamic nature of the runtime environment and thus, a detection model conventionally trained based on the unrealistic i.i.d. assumption may not provide the expected and consistent performance. In this paper, we present a robust and transferable anomaly detection framework RT-Log to address the above problems. To perform a comprehensive analysis of log messages, we introduce an adaptive relation modeling technique, which captures feature interactions among log information fields selectively and dynamically for effective and interpretable log representations. To establish its robustness and transferability, we propose a general environment generalization technique for learning the environment invariant representations that can generalize across different runtime environments. We evaluate the anomaly detection performance of RT-Log on large real-world datasets. Extensive experimental results demonstrate that RT-Log consistently outperforms state-of-the-art methods by a significant margin under different settings."}}
{"id": "p3B6T_xoNuC", "cdate": 1672531200000, "mdate": 1684241001389, "content": {"title": "DUET: A Tuning-Free Device-Cloud Collaborative Parameters Generation Framework for Efficient Device Model Generalization", "abstract": "Device Model Generalization (DMG) is a practical yet under-investigated research topic for on-device machine learning applications. It aims to improve the generalization ability of pre-trained models when deployed on resource-constrained devices, such as improving the performance of pre-trained cloud models on smart mobiles. While quite a lot of works have investigated the data distribution shift across clouds and devices, most of them focus on model fine-tuning on personalized data for individual devices to facilitate DMG. Despite their promising, these approaches require on-device re-training, which is practically infeasible due to the overfitting problem and high time delay when performing gradient calculation on real-time data. In this paper, we argue that the computational cost brought by fine-tuning can be rather unnecessary. We consequently present a novel perspective to improving DMG without increasing computational cost, i.e., device-specific parameter generation which directly maps data distribution to parameters. Specifically, we propose an efficient Device-cloUd collaborative parametErs generaTion framework (DUET). DUET is deployed on a powerful cloud server that only requires the low cost of forwarding propagation and low time delay of data transmission between the device and the cloud. By doing so, DUET can rehearse the device-specific model weight realizations conditioned on the personalized real-time data for an individual device. Importantly, our DUET elegantly connects the cloud and device as a \u201cduet\u201d collaboration, frees the DMG from fine-tuning, and enables a faster and more accurate DMG paradigm. We conduct an extensive experimental study of DUET on three public datasets, and the experimental results confirm our framework\u2019s effectiveness and generalisability for different DMG tasks."}}
{"id": "nPjQGZ3LDX", "cdate": 1672531200000, "mdate": 1682317738162, "content": {"title": "GlassDB: An Efficient Verifiable Ledger Database System Through Transparency", "abstract": ""}}
{"id": "ixYE7AUDhV", "cdate": 1672531200000, "mdate": 1699854041725, "content": {"title": "The Metaverse Data Deluge: What Can We Do About It?", "abstract": "In the metaverse the physical space and the virtual space co-exist, and interact simultaneously. While the physical space is virtually enhanced with information, the virtual space is continuously refreshed with real-time, real-world information. To allow users to process and manipulate information seamlessly between the real and digital spaces, novel technologies must be developed. These include smart interfaces, new augmented realities, and efficient data storage, management, and dissemination techniques. In this paper, we first discuss some promising co-space applications. These applications offer opportunities that neither of the spaces can realize on its own. We then discuss challenges. Finally, we discuss and envision what are likely to be required from the database and system perspectives."}}
{"id": "cF_Q4twve6", "cdate": 1672531200000, "mdate": 1699854041786, "content": {"title": "First Workshop on Verifiable Database Systems", "abstract": "Verifiable database systems ensure strong integrity guarantee, that is, the database operations are executed correctly over untampered data. While general-purpose verifiable computation techniques do exist, they suffer from poor performance. Therefore, a practical verifiable database system must make trade-offs between security, performance, and functionalities. This workshop brings together researchers and engineers from academia and industry to discuss ideas and techniques for building such practical systems. The main goals include identifying new abstractions, applications, challenges and solutions related to verifiable database systems."}}
{"id": "cB40bVR_91", "cdate": 1672531200000, "mdate": 1699854041755, "content": {"title": "VeriBench: Analyzing the Performance of Database Systems with Verifiability", "abstract": ""}}
{"id": "UM_JVI6zbg", "cdate": 1672531200000, "mdate": 1699854041789, "content": {"title": "Enabling Secure and Efficient Data Analytics Pipeline Evolution with Trusted Execution Environment", "abstract": ""}}
{"id": "SJ3dwjyNYDw", "cdate": 1672531200000, "mdate": 1699854041771, "content": {"title": "CAusal and collaborative proxy-tasKs lEarning for Semi-Supervised Domain Adaptation", "abstract": "Semi-supervised domain adaptation (SSDA) adapts a learner to a new domain by effectively utilizing source domain data and a few labeled target samples. It is a practical yet under-investigated research topic. In this paper, we analyze the SSDA problem from two perspectives that have previously been overlooked, and correspondingly decompose it into two \\emph{key subproblems}: \\emph{robust domain adaptation (DA) learning} and \\emph{maximal cross-domain data utilization}. \\textbf{(i)} From a causal theoretical view, a robust DA model should distinguish the invariant ``concept'' (key clue to image label) from the nuisance of confounding factors across domains. To achieve this goal, we propose to generate \\emph{concept-invariant samples} to enable the model to classify the samples through causal intervention, yielding improved generalization guarantees; \\textbf{(ii)} Based on the robust DA theory, we aim to exploit the maximal utilization of rich source domain data and a few labeled target samples to boost SSDA further. Consequently, we propose a collaboratively debiasing learning framework that utilizes two complementary semi-supervised learning (SSL) classifiers to mutually exchange their unbiased knowledge, which helps unleash the potential of source and target domain training data, thereby producing more convincing pseudo-labels. Such obtained labels facilitate cross-domain feature alignment and duly improve the invariant concept learning. In our experimental study, we show that the proposed model significantly outperforms SOTA methods in terms of effectiveness and generalisability on SSDA datasets."}}
