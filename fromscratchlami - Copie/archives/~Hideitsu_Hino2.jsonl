{"id": "_1ydd9QvUQH", "cdate": 1672347514124, "mdate": 1672347514124, "content": {"title": "Information Geometrically Generalized Covariate Shift Adaptation", "abstract": "Many machine learning methods assume that the training and test data follow the same distribution. However, in the real world, this assumption is often violated. In particular, the marginal distribution of the data changes, called covariate shift, is one of the most important research topics in machine learning. We show that the well-known family of covariate shift adaptation methods is unified in the framework of information geometry. Furthermore, we show that parameter search for a geometrically generalized covariate shift adaptation method can be achieved efficiently. Numerical experiments show that our generalization can achieve better performance than the existing methods it encompasses."}}
{"id": "fpnBndFl4_", "cdate": 1672347426641, "mdate": 1672347426641, "content": {"title": "Unsupervised Domain Adaptation for Extra Features in the Target Domain Using Optimal Transport", "abstract": "Domain adaptation aims to transfer knowledge of labeled instances obtained from a source domain to a target domain to fill the gap between the domains. Most domain adaptation methods assume that the source and target domains have the same dimensionality. Methods that are applicable when the number of features is different in each domain have rarely been studied, especially when no label information is given for the test data obtained from the target domain. In this letter, it is assumed that common features exist in both domains and that extra (new additional) features are observed in the target domain; hence, the dimensionality of the target domain is higher than that of the source domain. To leverage the homogeneity of the common features, the adaptation between these source and target domains is formulated as an optimal transport (OT) problem. In addition, a learning bound in the target domain for the proposed OT-based method is derived. The proposed algorithm is validated using both simulated and real-world data."}}
{"id": "oCvol2jO7Y", "cdate": 1672347337763, "mdate": 1672347337763, "content": {"title": "Gaussian Process Koopman Mode Decomposition", "abstract": "We propose a nonlinear probabilistic generative model of Koopman mode decomposition based on an unsupervised gaussian process. Existing data-driven methods for Koopman mode decomposition have focused on estimating the quantities specified by Koopman mode decomposition: eigenvalues, eigenfunctions, and modes. Our model enables the simultaneous estimation of these quantities and latent variables governed by an unknown dynamical system. Furthermore, we introduce an efficient strategy to estimate the parameters of our model by low-rank approximations of covariance matrices. Applying the proposed model to both synthetic data and a real-world epidemiological data set, we show that various analyses are available using the estimated parameters."}}
{"id": "gSvhfbQhxfR", "cdate": 1672347223311, "mdate": 1672347223311, "content": {"title": "Geometry of EM and related iterative algorithms", "abstract": "The Expectation\u2013Maximization (EM) algorithm is a simple meta-algorithm that has been used for many years as a methodology for statistical inference when there are missing measurements in the observed data or when the data is composed of observables and unobservables. Its general properties are well studied, and also, there are countless ways to apply it to individual problems. In this paper, we introduce the em algorithm, an information geometric formulation of the EM algorithm, and its extensions and applications to various problems. Specifically, we will see that it is possible to formulate an outlier\u2013robust inference algorithm, an algorithm for calculating channel capacity, parameter estimation methods on probability simplex, particular multivariate analysis methods such as principal component analysis in a space of probability models and modal regression, matrix factorization, and learning generative models, which have recently attracted attention in deep learning, from the geometric perspective provided by Amari."}}
{"id": "rSmLOqhgTYC", "cdate": 1672347102772, "mdate": 1672347102772, "content": {"title": "Active learning by query by committee with robust divergences", "abstract": "Active learning is a widely used methodology for various problems with high measurement costs. In active learning, the next object to be measured is selected by an acquisition function, and measurements are performed sequentially. The query by committee is a well-known acquisition function. In conventional methods, committee disagreement is quantified by the Kullback\u2013Leibler divergence. In this paper, the measure of disagreement is defined by the Bregman divergence, which includes the Kullback\u2013Leibler divergence as an instance, and the dual \ud835\udefe-power divergence. As a particular class of the Bregman divergence, the \ud835\udefd-divergence is considered. By deriving the influence function, we show that the proposed method using \ud835\udefd-divergence and dual \ud835\udefe-power divergence are more robust than the conventional method in which the measure of disagreement is defined by the Kullback\u2013Leibler divergence. Experimental results show that the proposed method performs as well as or better than the conventional method."}}
{"id": "t2JtXYzEpNq", "cdate": 1672347016547, "mdate": 1672347016547, "content": {"title": "End-condition for solution small angle X-ray scattering measurements by kernel density estimation", "abstract": "We develop a method for calculating the minimum X-ray exposure time for X-ray solution scattering experiments using statistical and mathematical approaches to enhance the measurement efficiency while maintaining data quality. Experts can determine the X-ray exposure time for samples by investigating time-series data of scattering profiles and considering the quality of scattering profiles and the samples\u2019 X-ray damage. In this study, we apply a statistical inequality to estimate the kernel density estimation method\u2019s error to determine the minimum X-ray exposure time for protein solution scattering experiments and evaluate the method\u2019s validity using the results of Guinier analysis as an indicator."}}
{"id": "uke6INbzOh", "cdate": 1640995200000, "mdate": 1652830993120, "content": {"title": "One-bit Submission for Locally Private Quasi-MLE: Its Asymptotic Normality and Limitation", "abstract": "Local differential privacy~(LDP) is an information-theoretic privacy definition suitable for statistical surveys that involve an untrusted data curator. An LDP version of quasi-maximum likelihood estimator~(QMLE) has been developed, but the existing method to build LDP QMLE is difficult to implement for a large-scale survey system in the real world due to long waiting time, expensive communication cost, and the boundedness assumption of derivative of a log-likelihood function. We provided an alternative LDP protocol without those issues, which is potentially much easily deployable to a large-scale survey. We also provided sufficient conditions for the consistency and asymptotic normality and limitations of our protocol. Our protocol is less burdensome for the users, and the theoretical guarantees cover more realistic cases than those for the existing method."}}
{"id": "X8Flm0KFOOg", "cdate": 1640995200000, "mdate": 1652830993163, "content": {"title": "Cost-effective Framework for Gradual Domain Adaptation with Multifidelity", "abstract": "In domain adaptation, when there is a large distance between the source and target domains, the prediction performance will degrade. Gradual domain adaptation is one of the solutions to such an issue, assuming that we have access to intermediate domains, which shift gradually from the source to the target domain. In previous works, it was assumed that the number of samples in the intermediate domains was sufficiently large; hence, self-training was possible without the need for labeled data. If the number of accessible intermediate domains is restricted, the distances between domains become large, and self-training will fail. Practically, the cost of samples in intermediate domains will vary, and it is natural to consider that the closer an intermediate domain is to the target domain, the higher the cost of obtaining samples from the intermediate domain is. To solve the trade-off between cost and accuracy, we propose a framework that combines multifidelity and active domain adaptation. The effectiveness of the proposed method is evaluated by experiments with real-world datasets."}}
{"id": "EC-ukwcBCta", "cdate": 1640995200000, "mdate": 1652830993122, "content": {"title": "Detecting cell assemblies by NMF-based clustering from calcium imaging data", "abstract": ""}}
{"id": "qx4HHO6QZGg", "cdate": 1609459200000, "mdate": 1652830993032, "content": {"title": "Bayesian Dynamic Mode Decomposition with Variational Matrix Factorization", "abstract": "Dynamic mode decomposition (DMD) and its extensions are data-driven methods that have substantially contributed to our understanding of dynamical systems. However, because DMD and most of its extensions are deterministic, it is difficult to treat probabilistic representations of parameters and predictions. In this work, we propose a novel formulation of a Bayesian DMD model. Our Bayesian DMD model is consistent with the procedure of standard DMD, which is to first determine the subspace of observations, and then compute the modes on that subspace. Variational matrix factorization makes it possible to realize a fully-Bayesian scheme of DMD. Moreover, we derive a Bayesian DMD model for incomplete data, which demonstrates the advantage of probabilistic modeling. Finally, both of nonlinear simulated and real-world datasets are used to illustrate the potential of the proposed method."}}
