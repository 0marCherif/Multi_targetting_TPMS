{"id": "fWHOcnHb1n", "cdate": 1652737852091, "mdate": null, "content": {"title": "Parameter-free Regret in High Probability with Heavy Tails", "abstract": "We present new algorithms for online convex optimization over unbounded domains that obtain parameter-free regret in high-probability given access only to potentially heavy-tailed subgradient estimates. Previous work in unbounded domains con- siders only in-expectation results for sub-exponential subgradients. Unlike in the bounded domain case, we cannot rely on straight-forward martingale concentration due to exponentially large iterates produced by the algorithm. We develop new regularization techniques to overcome these problems. Overall, with probability at most \u03b4, for all comparators u our algorithm achieves regret O \u0303(\u2225u\u2225T 1/p log(1/\u03b4)) for subgradients with bounded pth moments for some p \u2208 (1, 2]."}}
{"id": "nlcCY_-HCb", "cdate": 1640995200000, "mdate": 1682559764479, "content": {"title": "Parameter-free Regret in High Probability with Heavy Tails", "abstract": "We present new algorithms for online convex optimization over unbounded domains that obtain parameter-free regret in high-probability given access only to potentially heavy-tailed subgradient estimates. Previous work in unbounded domains considers only in-expectation results for sub-exponential subgradients. Unlike in the bounded domain case, we cannot rely on straight-forward martingale concentration due to exponentially large iterates produced by the algorithm. We develop new regularization techniques to overcome these problems. Overall, with probability at most $\\delta$, for all comparators $\\mathbf{u}$ our algorithm achieves regret $\\tilde{O}(\\| \\mathbf{u} \\| T^{1/\\mathfrak{p}} \\log (1/\\delta))$ for subgradients with bounded $\\mathfrak{p}^{th}$ moments for some $\\mathfrak{p} \\in (1, 2]$."}}
