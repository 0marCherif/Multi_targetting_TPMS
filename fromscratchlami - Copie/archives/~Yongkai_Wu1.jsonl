{"id": "dhP5n5xjN-a", "cdate": 1609459200000, "mdate": 1623681707053, "content": {"title": "A Generative Adversarial Framework for Bounding Confounded Causal Effects", "abstract": "Causal inference from observational data is receiving wide applications in many fields. However, unidentifiable situations, where causal effects cannot be uniquely computed from observational data, pose critical barriers to applying causal inference to complicated real applications. In this paper, we develop a bounding method for estimating the average causal effect (ACE) under unidentifiable situations due to hidden confounding based on Pearl's structural causal model. We propose to parameterize the unknown exogenous random variables and structural equations of a causal model using neural networks and implicit generative models. Then, using an adversarial learning framework, we search the parameter space to explicitly traverse causal models that agree with the given observational distribution, and find those that minimize or maximize the ACE to obtain its lower and upper bounds. The proposed method does not make assumption about the type of structural equations and variables. Experiments using both synthetic and real-world datasets are conducted."}}
{"id": "zxClcKrP3nJ", "cdate": 1577836800000, "mdate": null, "content": {"title": "Fair Multiple Decision Making Through Soft Interventions", "abstract": "Previous research in fair classification mostly focuses on a single decision model. In reality, there usually exist multiple decision models within a system and all of which may contain a certain amount of discrimination. Such realistic scenarios introduce new challenges to fair classification: since discrimination may be transmitted from upstream models to downstream models, building decision models separately without taking upstream models into consideration cannot guarantee to achieve fairness. In this paper, we propose an approach that learns multiple classifiers and achieves fairness for all of them simultaneously, by treating each decision model as a soft intervention and inferring the post-intervention distributions to formulate the loss function as well as the fairness constraints. We adopt surrogate functions to smooth the loss function and constraints, and theoretically show that the excess risk of the proposed loss function can be bounded in a form that is the same as that for traditional surrogated loss functions. Experiments using both synthetic and real-world datasets show the effectiveness of our approach."}}
{"id": "ohRDwW3b-8", "cdate": 1577836800000, "mdate": null, "content": {"title": "Fairness through Equality of Effort", "abstract": "Fair machine learning is receiving an increasing attention in machine learning fields. Researchers in fair learning have developed correlation or association-based measures such as demographic disparity, mistreatment disparity, calibration, causal-based measures such as total effect, direct and indirect discrimination, and counterfactual fairness, and fairness notions such as equality of opportunity and equalized odds that consider both decisions in the training data and decisions made by predictive models. In this paper, we develop a new causal-based fairness notation, called equality of effort. Different from existing fairness notions which mainly focus on discovering the disparity of decisions between two groups of individuals, the proposed equality of effort notation helps answer questions like to what extend a legitimate variable should change to make a particular individual achieve a certain outcome level and addresses the concerns whether the efforts made to achieve the same outcome level for individuals from the protected group and that from the unprotected group are different. We develop algorithms for determining whether an individual or a group of individuals is discriminated in terms of equality of effort. We also develop an optimization-based method for removing discriminatory effects from the data if discrimination is detected. We conduct empirical evaluations to compare the equality of effort and existing fairness notion and show the effectiveness of our proposed algorithms."}}
{"id": "ipDBaKfQHT6", "cdate": 1577836800000, "mdate": null, "content": {"title": "Multi-cause Discrimination Analysis Using Potential Outcomes", "abstract": "Discrimination analysis recently aroused wide attention in the fairness-aware learning field. Most existing causal modeling based fair learning research focuses on single cause effect of one protected attribute on decision. In this paper, we focus on discrimination discovery when multiple protected attributes and redlining attributes are present in addition to other covariates. We regard those protected and redlining attributes as multiple causes of the outcome variable. To deal with unobserved variables, especially hidden confounders, we adopt the potential outcome framework and leverage the state-of-the-art deconfounder algorithm to do causal inference under multiple causes. The deconfounder algorithm infers a latent variable as a substitute for unobserved confounders and then uses that substitute to perform causal inference. Our approach is more appropriate for discrimination discovery as it is able to relax the Markovian assumption and avoid the unidentifiability issue in structural causal modeling approaches. We conduct empirical evaluation on both synthetic data and real data. Empirical evaluation results demonstrate the effectiveness of our proposed approach."}}
{"id": "HylYDEHlLH", "cdate": 1567802464926, "mdate": null, "content": {"title": "PC-Fairness: A Unified Framework for Measuring Causality-based Fairness", "abstract": "Fair machine learning studies how to construct predictive machine learning models such that decisions made with their assistance fairly treat all groups of people. A recent trend in this field is to define fairness as causality-based notions which concern the causal connection between protected attributes and decisions. However, one common challenge of all causality-based fairness notions is identifiability, i.e., whether they can be accurately measured from observational data, which is a critical barrier to applying these notions to real situations. In this paper, we develop a framework for measuring different causality-based fairness notions. We propose a unified definition that covers most of previous causality-based fairness notions, namely the path-specific counterfactual fairness (PC Fairness). Based on that, we propose a general method in the form of a constrained optimization problem for bounding PC fairness under all unidentifiable situations. Experiments on synthetic and real-world datasets show the correctness and effectiveness of our method."}}
{"id": "wGn0TsjvGg0", "cdate": 1546300800000, "mdate": null, "content": {"title": "Causal Modeling-Based Discrimination Discovery and Removal: Criteria, Bounds, and Algorithms", "abstract": "Anti-discrimination is an increasingly important task in data science. In this paper, we investigate the problem of discovering both direct and indirect discrimination from the historical data, and removing the discriminatory effects before the data are used for predictive analysis (e.g., building classifiers). The main drawback of existing methods is that they cannot distinguish the part of influence that is really caused by discrimination from all correlated influences. In our approach, we make use of the causal graph to capture the causal structure of the data. Then, we model direct and indirect discrimination as the path-specific effects, which accurately identify the two types of discrimination as the causal effects transmitted along different paths in the graph. For certain situations where indirect discrimination cannot be exactly measured due to the unidentifiability of some path-specific effects, we develop an upper bound and a lower bound to the effect of indirect discrimination. Based on the theoretical results, we propose effective algorithms for discovering direct and indirect discrimination, as well as algorithms for precisely removing both types of discrimination while retaining good data utility. Experiments using the real dataset show the effectiveness of our approaches."}}
{"id": "ryWHkMZO-H", "cdate": 1546300800000, "mdate": null, "content": {"title": "On Convexity and Bounds of Fairness-aware Classification", "abstract": "In this paper, we study the fairness-aware classification problem by formulating it as a constrained optimization problem. Several limitations exist in previous works due to the lack of a theoretical framework for guiding the formulation. We propose a general fairness-aware framework to address previous limitations. Our framework provides: (1) various fairness metrics that can be incorporated into classic classification models as constraints; (2) the convex constrained optimization problem that can be solved efficiently; and (3) the lower and upper bounds of real-world fairness measures that are established using surrogate functions, providing a fairness guarantee for constrained classifiers. Within the framework, we propose a constraint-free criterion under which any learned classifier is guaranteed to be fair in terms of the specified fairness metric. If the constraint-free criterion fails to satisfy, we further develop the method based on the bounds for constructing fair classifiers. The experiments using real-world datasets demonstrate our theoretical results and show the effectiveness of the proposed framework."}}
{"id": "lNvp6kTtrTI", "cdate": 1546300800000, "mdate": null, "content": {"title": "Fairness through Equality of Effort", "abstract": "Fair machine learning is receiving an increasing attention in machine learning fields. Researchers in fair learning have developed correlation or association-based measures such as demographic disparity, mistreatment disparity, calibration, causal-based measures such as total effect, direct and indirect discrimination, and counterfactual fairness, and fairness notions such as equality of opportunity and equal odds that consider both decisions in the training data and decisions made by predictive models. In this paper, we develop a new causal-based fairness notation, called equality of effort. Different from existing fairness notions which mainly focus on discovering the disparity of decisions between two groups of individuals, the proposed equality of effort notation helps answer questions like to what extend a legitimate variable should change to make a particular individual achieve a certain outcome level and addresses the concerns whether the efforts made to achieve the same outcome level for individuals from the protected group and that from the unprotected group are different. We develop algorithms for determining whether an individual or a group of individuals is discriminated in terms of equality of effort. We also develop an optimization-based method for removing discriminatory effects from the data if discrimination is detected. We conduct empirical evaluations to compare the equality of effort and existing fairness notion and show the effectiveness of our proposed algorithms."}}
{"id": "ayxD8NX6Hn", "cdate": 1546300800000, "mdate": null, "content": {"title": "Achieving Causal Fairness through Generative Adversarial Networks.", "abstract": "Achieving fairness in learning models is currently an imperative task in machine learning. Meanwhile, recent research showed that fairness should be studied from the causal perspective, and proposed a number of fairness criteria based on Pearl's causal modeling framework. In this paper, we investigate the problem of building causal fairness-aware generative adversarial networks (CFGAN), which can learn a close distribution from a given dataset, while also ensuring various causal fairness criteria based on a given causal graph. CFGAN adopts two generators, whose structures are purposefully designed to reflect the structures of causal graph and interventional graph. Therefore, the two generators can respectively simulate the underlying causal model that generates the real data, as well as the causal model after the intervention. On the other hand, two discriminators are used for producing a close-to-real distribution, as well as for achieving various fairness criteria based on causal quantities simulated by generators. Experiments on a real-world dataset show that CFGAN can generate high quality fair data."}}
{"id": "CCBLxKWg2it", "cdate": 1546300800000, "mdate": null, "content": {"title": "Counterfactual Fairness: Unidentification, Bound and Algorithm", "abstract": "Fairness-aware learning studies the problem of building machine learning models that are subject to fairness requirements. Counterfactual fairness is a notion of fairness derived from Pearl's causal model, which considers a model is fair if for a particular individual or group its prediction in the real world is the same as that in the counterfactual world where the individual(s) had belonged to a different demographic group. However, an inherent limitation of counterfactual fairness is that it cannot be uniquely quantified from the observational data in certain situations, due to the unidentifiability of the counterfactual quantity. In this paper, we address this limitation by mathematically bounding the unidentifiable counterfactual quantity, and develop a theoretically sound algorithm for constructing counterfactually fair classifiers. We evaluate our method in the experiments using both synthetic and real-world datasets, as well as compare with existing methods. The results validate our theory and show the effectiveness of our method."}}
