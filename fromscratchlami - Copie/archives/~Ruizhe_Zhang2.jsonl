{"id": "qVXaYcOPaH", "cdate": 1672531200000, "mdate": 1682318410299, "content": {"title": "A General Algorithm for Solving Rank-one Matrix Sensing", "abstract": "Matrix sensing has many real-world applications in science and engineering, such as system control, distance embedding, and computer vision. The goal of matrix sensing is to recover a matrix $A_\\star \\in \\mathbb{R}^{n \\times n}$, based on a sequence of measurements $(u_i,b_i) \\in \\mathbb{R}^{n} \\times \\mathbb{R}$ such that $u_i^\\top A_\\star u_i = b_i$. Previous work [ZJD15] focused on the scenario where matrix $A_{\\star}$ has a small rank, e.g. rank-$k$. Their analysis heavily relies on the RIP assumption, making it unclear how to generalize to high-rank matrices. In this paper, we relax that rank-$k$ assumption and solve a much more general matrix sensing problem. Given an accuracy parameter $\\delta \\in (0,1)$, we can compute $A \\in \\mathbb{R}^{n \\times n}$ in $\\widetilde{O}(m^{3/2} n^2 \\delta^{-1} )$, such that $ |u_i^\\top A u_i - b_i| \\leq \\delta$ for all $i \\in [m]$. We design an efficient algorithm with provable convergence guarantees using stochastic gradient descent for this problem."}}
{"id": "Y2tdPBiO-M", "cdate": 1672531200000, "mdate": 1682318410060, "content": {"title": "Convergence and Generalization of Wide Neural Networks with Large Bias", "abstract": "This work studies training one-hidden-layer overparameterized ReLU networks via gradient descent in the neural tangent kernel (NTK) regime, where the networks' biases are initialized to some constant rather than zero. The tantalizing benefit of such initialization is that the neural network will provably have sparse activation through the entire training process, which enables fast training procedures. The first set of results characterizes the convergence of gradient descent training. Surprisingly, it is shown that the network after sparsification can achieve as fast convergence as the dense network, in comparison to the previous work indicating that the sparse networks converge slower. Further, the required width is improved to ensure gradient descent can drive the training error towards zero at a linear rate. Secondly, the networks' generalization is studied: a width-sparsity dependence is provided which yields a sparsity-dependent Rademacher complexity and generalization bound. To our knowledge, this is the first sparsity-dependent generalization result via Rademacher complexity. Lastly, this work further studies the least eigenvalue of the limiting NTK. Surprisingly, while it is not shown that trainable biases are necessary, trainable bias, which is enabled by our improved analysis scheme, helps to identify a nice data-dependent region where a much finer analysis of the NTK's smallest eigenvalue can be conducted. This leads to a much sharper lower bound on the NTK's smallest eigenvalue than the one previously known and, consequently, an improved generalization bound."}}
{"id": "G6-oxjbc_mK", "cdate": 1663850136782, "mdate": null, "content": {"title": "Sharper Analysis of Sparsely Activated Wide Neural Networks with Trainable Biases", "abstract": "This work studies training one-hidden-layer overparameterized ReLU networks via gradient descent in the neural tangent kernel (NTK) regime, where, differently from the previous works, the networks' biases are trainable and are initialized to some constant rather than zero. The tantalizing benefit of such initialization is that the neural network will provably have sparse activation pattern before, during and after training, which can enable fast training procedures and, therefore, reduce the training cost. The first set of results of this work characterize the convergence of the network's gradient descent dynamics. The required width is provided to ensure gradient descent can drive the training error towards zero in linear rate. The contribution over previous work is that not only the bias is allowed to be updated by gradient descent under our setting but also a finer analysis is given such that the required width to ensure the network's closeness to its NTK is improved. Secondly, the networks' generalization bound after training is provided. A width-sparsity dependence is presented which yields sparsity-dependent localized Rademacher complexity and same-as-previous (ignoring logarithmic factors) generalization bound. Up to our knowledge, this is the first sparsity-dependent generalization result via localized Rademacher complexity. As a by-product, if the bias initialization is chosen to be zero, the width requirement improves the previous bound for the shallow networks' generalization. Lastly, since the generalization bound has dependence on the smallest eigenvalue of the limiting NTK and the bounds from previous works yield vacuous generalization, this work further studies the least eigenvalue of the limiting NTK. Surprisingly, while it is not shown that trainable biases are necessary, trainable bias helps to identify a nice data-dependent region where a much finer analysis of the NTK's smallest eigenvalue can be conducted, which leads to a much sharper lower bound than the previously known worst-case bound and, consequently, a non-vacuous generalization bound. Experimental evaluation is provided to evaluate our results. "}}
{"id": "zofwPmKL-DO", "cdate": 1652737663496, "mdate": null, "content": {"title": "Quantum Algorithms for Sampling Log-Concave Distributions and Estimating Normalizing Constants", "abstract": "Given a convex function $f\\colon\\mathbb{R}^{d}\\to\\mathbb{R}$, the problem of sampling from a distribution $\\propto e^{-f(x)}$ is called log-concave sampling. This task has wide applications in machine learning, physics, statistics, etc. In this work, we develop quantum algorithms for sampling log-concave distributions and for estimating their normalizing constants $\\int_{\\mathbb{R}^d}e^{-f(x)}\\mathrm{d} x$. First, we use underdamped Langevin diffusion to develop quantum algorithms that match the query complexity (in terms of the condition number $\\kappa$ and dimension $d$) of analogous classical algorithms that use gradient (first-order) queries, even though the quantum algorithms use only evaluation (zeroth-order) queries. For estimating normalizing constants, these algorithms also achieve quadratic speedup in the multiplicative error $\\epsilon$. Second, we develop quantum Metropolis-adjusted Langevin algorithms with query complexity $\\widetilde{O}(\\kappa^{1/2}d)$ and $\\widetilde{O}(\\kappa^{1/2}d^{3/2}/\\epsilon)$ for log-concave sampling and normalizing constant estimation, respectively, achieving polynomial speedups in $\\kappa,d,\\epsilon$ over the best known classical algorithms by exploiting quantum analogs of the Monte Carlo method and quantum walks. We also prove a $1/\\epsilon^{1-o(1)}$ quantum lower bound for estimating normalizing constants, implying near-optimality of our quantum algorithms in $\\epsilon$."}}
{"id": "hGdAzemIK1X", "cdate": 1652737661915, "mdate": null, "content": {"title": "Quantum Speedups of Optimizing Approximately Convex Functions with Applications to Logarithmic Regret Stochastic Convex Bandits", "abstract": "We initiate the study of quantum algorithms for optimizing approximately convex functions. Given a convex set $\\mathcal{K}\\subseteq\\mathbb{R}^{n}$ and a function $F\\colon\\mathbb{R}^{n}\\to\\mathbb{R}$ such that there exists a convex function $f\\colon\\mathcal{K}\\to\\mathbb{R}$ satisfying $\\sup_{x\\in\\mathcal{K}}|F(x)-f(x)|\\leq \\epsilon/n$, our quantum algorithm finds an $x^{*}\\in\\mathcal{K}$ such that $F(x^{*})-\\min_{x\\in\\mathcal{K}} F(x)\\leq\\epsilon$ using $\\tilde{O}(n^{3})$ quantum evaluation queries to $F$. This achieves a polynomial quantum speedup compared to the best-known classical algorithms. As an application, we give a quantum algorithm for zeroth-order stochastic convex bandits with $\\tilde{O}(n^{5}\\log^{2} T)$ regret, an exponential speedup in $T$ compared to the classical $\\Omega(\\sqrt{T})$ lower bound. Technically, we achieve quantum speedup in $n$ by exploiting a quantum framework of simulated annealing and adopting a quantum version of the hit-and-run walk. Our speedup in $T$ for zeroth-order stochastic convex bandits is due to a quadratic quantum speedup in multiplicative error of mean estimation."}}
{"id": "ZCGDqdK0zG", "cdate": 1652737440747, "mdate": null, "content": {"title": "Fast Distance Oracles for Any Symmetric Norm", "abstract": "In the \\emph{Distance Oracle} problem, the goal is to preprocess $n$ vectors $x_1, x_2, \\cdots, x_n$ in a $d$-dimensional normed space $(\\mathbb{X}^d, \\| \\cdot \\|_l)$ into a cheap data structure, so that given a query vector $q \\in \\mathbb{X}^d$, all distances $\\| q - x_i \\|_l$ to the data points $\\{x_i\\}_{i\\in [n]}$ can be quickly approximated (faster than the trivial $\\sim nd$ query time). This primitive is a basic subroutine in machine learning, data mining and similarity search applications. In the case of $\\ell_p$ norms, the problem is well understood, and optimal data structures are known for most values of $p$.  \n\nOur main contribution is a fast $(1\\pm \\varepsilon)$ distance oracle for  \\emph{any symmetric} norm $\\|\\cdot\\|_l$. This class includes $\\ell_p$ norms and Orlicz norms as special cases, as well as other norms used in practice, e.g. top-$k$ norms, max-mixture and sum-mixture of $\\ell_p$ norms, small-support norms and the box-norm. We propose a novel data structure with  $\\tilde{O}(n (d + \\mathrm{mmc}(l)^2 ) )$ preprocessing time and space, and $t_q = \\tilde{O}(d + n \\cdot \\mathrm{mmc}(l)^2)$ query time, where $\\mathrm{mmc}(l)$ is a complexity-measure (modulus) of the symmetric norm under consideration. When $l = \\ell_{p}$ , this runtime matches the aforementioned state-of-art oracles. "}}
{"id": "v8thEngIN3p", "cdate": 1640995200000, "mdate": 1682318410175, "content": {"title": "Quantum Meets the Minimum Circuit Size Problem", "abstract": "In this work, we initiate the study of the Minimum Circuit Size Problem (MCSP) in the quantum setting. MCSP is a problem to compute the circuit complexity of Boolean functions. It is a fascinating problem in complexity theory - its hardness is mysterious, and a better understanding of its hardness can have surprising implications to many fields in computer science. We first define and investigate the basic complexity-theoretic properties of minimum quantum circuit size problems for three natural objects: Boolean functions, unitaries, and quantum states. We show that these problems are not trivially in NP but in QCMA (or have QCMA protocols). Next, we explore the relations between the three quantum MCSPs and their variants. We discover that some reductions that are not known for classical MCSP exist for quantum MCSPs for unitaries and states, e.g., search-to-decision reductions and self-reductions. Finally, we systematically generalize results known for classical MCSP to the quantum setting (including quantum cryptography, quantum learning theory, quantum circuit lower bounds, and quantum fine-grained complexity) and also find new connections to tomography and quantum gravity. Due to the fundamental differences between classical and quantum circuits, most of our results require extra care and reveal properties and phenomena unique to the quantum setting. Our findings could be of interest for future studies, and we post several open problems for further exploration along this direction."}}
{"id": "pIAMj4sPLt", "cdate": 1640995200000, "mdate": 1682318410154, "content": {"title": "Bypass Exponential Time Preprocessing: Fast Neural Network Training via Weight-Data Correlation Preprocessing", "abstract": "Over the last decade, deep neural networks have transformed our society, and they are already widely applied in various machine learning applications. State-of-art deep neural networks are becoming larger in size every year to deliver increasing model accuracy, and as a result, model training consumes substantial computing resources and will only consume more in the future. Using current training methods, in each iteration, to process a data point $x \\in \\mathbb{R}^d$ in a layer, we need to spend $\\Theta(md)$ time to evaluate all the $m$ neurons in the layer. This means processing the entire layer takes $\\Theta(nmd)$ time for $n$ data points. Recent work [Song, Yang and Zhang, NeurIPS 2021] reduces this time per iteration to $o(nmd)$, but requires exponential time to preprocess either the data or the neural network weights, making it unlikely to have practical usage. In this work, we present a new preprocessing method that simply stores the weight-data correlation in a tree data structure in order to quickly, dynamically detect which neurons fire at each iteration. Our method requires only $O(nmd)$ time in preprocessing and still achieves $o(nmd)$ time per iteration. We complement our new algorithm with a lower bound, proving that assuming a popular conjecture from complexity theory, one could not substantially speed up our algorithm for dynamic detection of firing neurons."}}
{"id": "jAetp2WQHX6", "cdate": 1640995200000, "mdate": 1653668273151, "content": {"title": "Eigenstripping, Spectral Decay, and Edge-Expansion on Posets", "abstract": "Fast mixing of random walks on hypergraphs has led to myriad breakthroughs in theoretical computer science in the last five years. On the other hand, many important applications (e.g. to locally testable codes, 2-2 games) rely on a more general class of underlying structures called posets, and crucially take advantage of non-simplicial structure. These works make it clear the global expansion properties of posets depend strongly on their underlying architecture (e.g. simplicial, cubical, linear algebraic), but the overall phenomenon remains poorly understood. In this work, we quantify the advantage of different poset architectures in both a spectral and combinatorial sense, highlighting how regularity controls the spectral decay and edge-expansion of corresponding random walks. We show that the spectra of walks on expanding posets (Dikstein, Dinur, Filmus, Harsha APPROX-RANDOM 2018) concentrate in strips around a small number of approximate eigenvalues controlled by the regularity of the underlying poset. This gives a simple condition to identify poset architectures (e.g. the Grassmann) that exhibit exponential decay of eigenvalues, versus architectures like hypergraphs whose eigenvalues decay linearly -- a crucial distinction in applications to hardness of approximation such as the recent proof of the 2-2 Games Conjecture (Khot, Minzer, Safra FOCS 2018). We show these results lead to a tight characterization of edge-expansion on posets in the $\\ell_2$-regime (generalizing recent work of Bafna, Hopkins, Kaufman, and Lovett (SODA 2022)), and pay special attention to the case of the Grassmann where we show our results are tight for a natural set of sparsifications of the Grassmann graphs. We note for clarity that our results do not recover the characterization of expansion used in the proof of the 2-2 Games Conjecture which relies on $\\ell_\\infty$ rather than $\\ell_2$-structure."}}
{"id": "h8zZZxj7Rr", "cdate": 1640995200000, "mdate": 1682318410339, "content": {"title": "Quartic Samples Suffice for Fourier Interpolation", "abstract": "We study the problem of interpolating a noisy Fourier-sparse signal in the time duration $[0, T]$ from noisy samples in the same range, where the ground truth signal can be any $k$-Fourier-sparse signal with band-limit $[-F, F]$. Our main result is an efficient Fourier Interpolation algorithm that improves the previous best algorithm by [Chen, Kane, Price, and Song, FOCS 2016] in the following three aspects: $\\bullet$ The sample complexity is improved from $\\widetilde{O}(k^{51})$ to $\\widetilde{O}(k^{4})$. $\\bullet$ The time complexity is improved from $ \\widetilde{O}(k^{10\\omega+40})$ to $\\widetilde{O}(k^{4 \\omega})$. $\\bullet$ The output sparsity is improved from $\\widetilde{O}(k^{10})$ to $\\widetilde{O}(k^{4})$. Here, $\\omega$ denotes the exponent of fast matrix multiplication. The state-of-the-art sample complexity of this problem is $\\sim k^4$, but was only known to be achieved by an *exponential-time* algorithm. Our algorithm uses the same number of samples but has a polynomial runtime, laying the groundwork for an efficient Fourier Interpolation algorithm. The centerpiece of our algorithm is a new sufficient condition for the frequency estimation task -- a high signal-to-noise (SNR) band condition -- which allows for efficient and accurate signal reconstruction. Based on this condition together with a new structural decomposition of Fourier signals (Signal Equivalent Method), we design a cheap algorithm for estimating each \"significant\" frequency within a narrow range, which is then combined with a signal estimation algorithm into a new Fourier Interpolation framework to reconstruct the ground-truth signal."}}
