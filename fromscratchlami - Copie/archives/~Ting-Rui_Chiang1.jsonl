{"id": "MGnPyYQ2QAA", "cdate": 1663850428343, "mdate": null, "content": {"title": "On a Benefit of Masked Language Model Pretraining: Robustness to Simplicity Bias", "abstract": "Despite the success of pretrained masked language models (MLM), why MLM pretraining is useful is still a question not fully answered. In this work we theoretically and empirically show that MLM pretraining makes models robust to lexicon-level spurious features, partly answering the question. Our explanation is that MLM pretraining may alleviate problems brought by simplicity bias (Shahet al., 2020), which refers to the phenomenon that a deep model tends to rely excessively on simple features. In NLP tasks, those simple features could be token-level features whose spurious association with the label can be learned easily. We show that MLM pretraining makes learning from the context easier. Thus, pretrained models are less likely to rely excessively on a single token. We also explore the theoretical explanations of MLM\u2019s efficacy in causal settings. Compared with Wei et al. (2021), we achieve similar results with milder assumptions. Finally, we close the gap between our theories and real-world practices by conducting experiments on real-world tasks."}}
{"id": "HyENJmZ_ZB", "cdate": 1546300800000, "mdate": null, "content": {"title": "Semantically-Aligned Equation Generation for Solving and Reasoning Math Word Problems", "abstract": "Ting-Rui Chiang, Yun-Nung Chen. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019."}}
