{"id": "OuaGvW-eL7G", "cdate": 1664731451572, "mdate": null, "content": {"title": "Quadratic minimization: from conjugate gradients to an adaptive heavy-ball method with Polyak step-sizes", "abstract": "In this work, we propose an adaptive variation on the classical heavy-ball method for convex quadratic minimization. The adaptivity crucially relies on so-called ``Polyak step-sizes'', which consists in using the knowledge of the optimal value of the optimization problem at hand instead of problem parameters such as a few eigenvalues of the Hessian of the problem. This method happens to  also be equivalent to a variation of the classical conjugate gradient method, and thereby inherits many of its attractive features, including its finite-time convergence, instance optimality, and its worst-case convergence rates.\n\nThe classical gradient method with Polyak step-sizes is known to behave very well in situations in which it can be used, and the question of whether incorporating momentum in this method is possible and can improve the method itself appeared to be open.\nWe provide a definitive answer to this question for minimizing convex quadratic functions, a arguably necessary first step for developing such methods in more general setups.\n"}}
{"id": "s1yaWFDLxVG", "cdate": 1652737873971, "mdate": null, "content": {"title": "Gradient Descent Is Optimal Under Lower Restricted Secant Inequality And Upper Error Bound", "abstract": "The study of first-order optimization is sensitive to the assumptions made on the objective functions.\nThese assumptions induce complexity classes which play a key role in worst-case analysis, including\nthe fundamental concept of algorithm optimality. Recent work argues that strong convexity and\nsmoothness\u2014popular assumptions in literature\u2014lead to a pathological definition of the condition\nnumber. Motivated by this result, we focus on the class of functions\nsatisfying a lower restricted secant inequality and an upper error bound. On top of being robust to\nthe aforementioned pathological behavior and including some non-convex functions, this pair of\nconditions displays interesting geometrical properties. In particular, the necessary and sufficient\nconditions to interpolate a set of points and their gradients within the class can be separated into\nsimple conditions on each sampled gradient. This allows the performance estimation problem (PEP)\n to be solved analytically, leading to a lower bound\non the convergence rate that proves gradient descent to be exactly optimal on this class of functions\namong all first-order algorithms."}}
{"id": "gUntOLY6yGR", "cdate": 1650586869251, "mdate": 1650586869251, "content": {"title": "PEPit: computer-assisted worst-case analyses of first-order optimization methods in Python", "abstract": "PEPit is a Python package aiming at simplifying the access to worst-case analyses of a large family of first-order optimization methods possibly involving gradient, projection, proximal, or linear optimization oracles, along with their approximate, or Bregman variants. In short, PEPit is a package enabling computer-assisted worst-case analyses of first-order optimization methods. The key underlying idea is to cast the problem of performing a worst-case analysis, often referred to as a performance estimation problem (PEP), as a semidefinite program (SDP) which can be solved numerically. For doing that, the package users are only required to write first-order methods nearly as they would have implemented them. The package then takes care of the SDP modelling parts, and the worst-case analysis is performed numerically via a standard solver."}}
{"id": "xDwOMWsHp_A", "cdate": 1650586647390, "mdate": 1650586647390, "content": {"title": "Super-Acceleration with Cyclical Step-sizes", "abstract": "We develop a convergence-rate analysis of momentum with cyclical step-sizes. We show that under some assumption on the spectral gap of Hessians in machine learning, cyclical step-sizes are provably faster than constant step-sizes. More precisely, we develop a convergence rate analysis for quadratic objectives that provides optimal parameters and shows that cyclical learning rates can improve upon traditional lower complexity bounds. We further propose a systematic approach to design optimal first order methods for quadratic minimization with a given spectral structure. Finally, we provide a local convergence rate analysis beyond quadratic minimization for the proposed methods and illustrate our findings through benchmarks on least squares and logistic regression problems."}}
{"id": "AVx0r_GppCu", "cdate": 1621630022002, "mdate": null, "content": {"title": "Super-Acceleration with Cyclical Step-sizes", "abstract": "Cyclical step-sizes are becoming increasingly popular in the optimization of deep learning problems. Motivated by recent observations on the spectral gaps of Hessians in machine learning, we show that these step-size schedules offer a simple way to exploit them. More precisely, we develop a convergence rate analysis for quadratic objectives that provides optimal parameters and shows that cyclical learning rates can improve upon traditional lower complexity bounds. We further propose a systematic approach to design optimal first order methods for quadratic minimization with a given spectral structure. Finally, we provide a local convergence rate analysis beyond quadratic minimization for the proposed methods and illustrate our findings through benchmarks on least squares and logistic regression problems."}}
{"id": "zp9rO8_wRRW", "cdate": 1577836800000, "mdate": null, "content": {"title": "A Study of Condition Numbers for First-Order Optimization", "abstract": "The study of first-order optimization algorithms (FOA) typically starts with assumptions on the objective functions, most commonly smoothness and strong convexity. These metrics are used to tune the hyperparameters of FOA. We introduce a class of perturbations quantified via a new norm, called *-norm. We show that adding a small perturbation to the objective function has an equivalently small impact on the behavior of any FOA, which suggests that it should have a minor impact on the tuning of the algorithm. However, we show that smoothness and strong convexity can be heavily impacted by arbitrarily small perturbations, leading to excessively conservative tunings and convergence issues. In view of these observations, we propose a notion of continuity of the metrics, which is essential for a robust tuning strategy. Since smoothness and strong convexity are not continuous, we propose a comprehensive study of existing alternative metrics which we prove to be continuous. We describe their mutual relations and provide their guaranteed convergence rates for the Gradient Descent algorithm accordingly tuned. Finally we discuss how our work impacts the theoretical understanding of FOA and their performances."}}
