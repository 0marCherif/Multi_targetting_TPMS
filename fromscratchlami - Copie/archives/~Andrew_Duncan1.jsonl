{"id": "izSH5kHGdzW", "cdate": 1684775664349, "mdate": 1684775664349, "content": {"title": "Energy-Based Models for Functional Data using Path Measure Tilting", "abstract": "Energy-Based Models (EBMs) have proven to be a highly effective approach for modelling densities on finite-dimensional spaces. Their ability to incorporate domain-specific choices and constraints into the structure of the model through composition make EBMs an appealing candidate for applications in physics, biology and computer vision and various other fields. Recently, Energy-Based Processes (EBP) for modelling stochastic processes was proposed for \\textit{unconditional} exchangeable data (e.g., point clouds). In this work, we present a novel subclass of EBPs, called F-EBM for \\textit{conditional} exchangeable data, which is able to learn distributions of functions (such as curves or surfaces) from functional samples evaluated at finitely many points. Two unique challenges arise in the functional context. Firstly, training data is often not evaluated along a fixed set of points. Secondly, steps must be taken to control the behaviour of the model between evaluation points, to mitigate overfitting. The proposed model is an energy based model on function space that is decomposed spectrally, where a Gaussian Process path measure is used to reweight the distribution to capture smoothness properties of the underlying process being modelled. The resulting model has the ability to utilize irregularly sampled training data and can output predictions at any resolution, providing an effective approach to up-scaling functional data. We demonstrate the efficacy of our proposed approach for modelling a range of datasets, including data collected from Standard and Poor's 500 (S\\&P) and UK National grid."}}
{"id": "lw-MMUdp7Ic", "cdate": 1672531200000, "mdate": 1684773352844, "content": {"title": "A High-dimensional Convergence Theorem for U-statistics with Applications to Kernel-based Testing", "abstract": "We prove a convergence theorem for U-statistics of degree two, where the data dimension $d$ is allowed to scale with sample size $n$. We find that the limiting distribution of a U-statistic undergoes a phase transition from the non-degenerate Gaussian limit to the degenerate limit, regardless of its degeneracy and depending only on a moment ratio. A surprising consequence is that a non-degenerate U-statistic in high dimensions can have a non-Gaussian limit with a larger variance and asymmetric distribution. Our bounds are valid for any finite $n$ and $d$, independent of individual eigenvalues of the underlying function, and dimension-independent under a mild assumption. As an application, we apply our theory to two popular kernel-based distribution tests, MMD and KSD, whose high-dimensional performance has been challenging to study. In a simple empirical setting, our results correctly predict how the test power at a fixed threshold scales with $d$ and the bandwidth."}}
{"id": "VnfSROoz1I", "cdate": 1672531200000, "mdate": 1684773352835, "content": {"title": "Hierarchical Bayesian modeling for knowledge transfer across engineering fleets via multitask learning", "abstract": ""}}
{"id": "PONVfLyD9u", "cdate": 1672531200000, "mdate": 1684773352844, "content": {"title": "Using Perturbation to Improve Goodness-of-Fit Tests based on Kernelized Stein Discrepancy", "abstract": "Kernelized Stein discrepancy (KSD) is a score-based discrepancy widely used in goodness-of-fit tests. It can be applied even when the target distribution has an unknown normalising factor, such as in Bayesian analysis. We show theoretically and empirically that the KSD test can suffer from low power when the target and the alternative distributions have the same well-separated modes but differ in mixing proportions. We propose to perturb the observed sample via Markov transition kernels, with respect to which the target distribution is invariant. This allows us to then employ the KSD test on the perturbed sample. We provide numerical evidence that with suitably chosen transition kernels the proposed approach can lead to substantially higher power than the KSD test."}}
{"id": "MGCfil1mN6", "cdate": 1672531200000, "mdate": 1684773352840, "content": {"title": "Encoding Domain Expertise into Multilevel Models for Source Location", "abstract": "Data from populations of systems are prevalent in many industrial applications. Machines and infrastructure are increasingly instrumented with sensing systems, emitting streams of telemetry data with complex interdependencies. In practice, data-centric monitoring procedures tend to consider these assets (and respective models) as distinct -- operating in isolation and associated with independent data. In contrast, this work captures the statistical correlations and interdependencies between models of a group of systems. Utilising a Bayesian multilevel approach, the value of data can be extended, since the population can be considered as a whole, rather than constituent parts. Most interestingly, domain expertise and knowledge of the underlying physics can be encoded in the model at the system, subgroup, or population level. We present an example of acoustic emission (time-of-arrival) mapping for source location, to illustrate how multilevel models naturally lend themselves to representing aggregate systems in engineering. In particular, we focus on constraining the combined models with domain knowledge to enhance transfer learning and enable further insights at the population level."}}
{"id": "iubaXtWi0Y3", "cdate": 1664310938210, "mdate": null, "content": {"title": "Using Perturbation to Improve Goodness-of-Fit Tests based on Kernelized Stein Discrepancy", "abstract": "Kernelized Stein discrepancy (KSD) is a score-based discrepancy widely employed in goodness-of-fit tests. It is applicable even when the target distribution has an unknown normalising factor, such as in Bayesian analysis. We show theoretically and empirically that the power of the KSD test can be low when the target distribution has well-separated modes, which is due to insufficient data in regions where the score functions of the alternative and the target distributions differ the most. To improve its test power, we propose to perturb the target and alternative distributions before applying the KSD test. The perturbation uses a Markov transition kernel that leaves the target invariant but perturbs alternatives. We provide numerical evidence that the proposed approach can lead to a substantially higher power than the KSD test when the target and the alternative are mixture distributions that differ only in mixing weights."}}
{"id": "rxhBZtql7C1", "cdate": 1640995200000, "mdate": 1684773352841, "content": {"title": "Batch Bayesian Optimization via Particle Gradient Flows", "abstract": "Bayesian Optimisation (BO) methods seek to find global optima of objective functions which are only available as a black-box or are expensive to evaluate. Such methods construct a surrogate model for the objective function, quantifying the uncertainty in that surrogate through Bayesian inference. Objective evaluations are sequentially determined by maximising an acquisition function at each step. However, this ancilliary optimisation problem can be highly non-trivial to solve, due to the non-convexity of the acquisition function, particularly in the case of batch Bayesian optimisation, where multiple points are selected in every step. In this work we reformulate batch BO as an optimisation problem over the space of probability measures. We construct a new acquisition function based on multipoint expected improvement which is convex over the space of probability measures. Practical schemes for solving this `inner' optimisation problem arise naturally as gradient flows of this objective function. We demonstrate the efficacy of this new method on different benchmark functions and compare with state-of-the-art batch BO methods."}}
{"id": "fuQffqNeOD4", "cdate": 1640995200000, "mdate": 1684773352843, "content": {"title": "Prior-informed Uncertainty Modelling with Bayesian Polynomial Approximations", "abstract": "Orthogonal polynomial approximations form the foundation to a set of well-established methods for uncertainty quantification known as polynomial chaos. These approximations deliver models for emulating physical systems in a variety of computational engineering applications. In this paper, we describe a Bayesian formulation of polynomial approximations capable of incorporating uncertainties in input data. Through different priors in a hierarchical structure, this permits us to incorporate expert knowledge on the inference task via different approaches. These include beliefs of sparsity in the model; approximate knowledge of the polynomial coefficients (e.g. through low-fidelity estimates) or output mean, and correlated models that share similar functional and/or physical behaviours. We show that through a Bayesian framework, such prior knowledge can be leveraged to produce orthogonal polynomial approximations with enhanced predictive accuracy."}}
{"id": "_ZWIbATvLE", "cdate": 1640995200000, "mdate": 1684773352868, "content": {"title": "Grassmann Stein Variational Gradient Descent", "abstract": "Stein variational gradient descent (SVGD) is a deterministic particle inference algorithm that provides an efficient alternative to Markov chain Monte Carlo. However, SVGD has been found to suffer from variance underestimation when the dimensionality of the target distribution is high. Recent developments have advocated projecting both the score function and the data onto real lines to sidestep this issue, although this can severely overestimate the epistemic (model) uncertainty. In this work, we propose Grassmann Stein variational gradient descent (GSVGD) as an alternative approach, which permits projections onto arbitrary dimensional subspaces. Compared with other variants of SVGD that rely on dimensionality reduction, GSVGD updates the projectors simultaneously for the score function and the data, and the optimal projectors are determined through a coupled Grassmann-valued diffusion process which explores favourable subspaces. Both our theoretical and experimental results suggest that GSVGD enjoys efficient state-space exploration in high-dimensional problems that have an intrinsic low-dimensional structure."}}
{"id": "TYh1NAr0LAK", "cdate": 1640995200000, "mdate": 1684773352842, "content": {"title": "Knowledge Transfer in Engineering Fleets: Hierarchical Bayesian Modelling for Multi-Task Learning", "abstract": "A population-level analysis is proposed to address data sparsity when building predictive models for engineering infrastructure. Utilising an interpretable hierarchical Bayesian approach and operational fleet data, domain expertise is naturally encoded (and appropriately shared) between different sub-groups, representing (i) use-type, (ii) component, or (iii) operating condition. Specifically, domain expertise is exploited to constrain the model via assumptions (and prior distributions) allowing the methodology to automatically share information between similar assets, improving the survival analysis of a truck fleet and power prediction in a wind farm. In each asset management example, a set of correlated functions is learnt over the fleet, in a combined inference, to learn a population model. Parameter estimation is improved when sub-fleets share correlated information at different levels of the hierarchy. In turn, groups with incomplete data automatically borrow statistical strength from those that are data-rich. The statistical correlations enable knowledge transfer via Bayesian transfer learning, and the correlations can be inspected to inform which assets share information for which effect (i.e. parameter). Both case studies demonstrate the wide applicability to practical infrastructure monitoring, since the approach is naturally adapted between interpretable fleet models of different in situ examples."}}
