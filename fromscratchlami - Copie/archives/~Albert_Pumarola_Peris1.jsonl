{"id": "ejo-mx44FrQ", "cdate": 1667578037105, "mdate": 1667578037105, "content": {"title": "D-NeRF: Neural Radiance Fields for Dynamic Scenes", "abstract": "Neural rendering techniques combining machine learning with geometric reasoning have arisen as one of the most\npromising approaches for synthesizing novel views of a\nscene from a sparse set of images. Among these, stands out\nthe Neural radiance fields (NeRF) [31], which trains a deep\nnetwork to map 5D input coordinates (representing spatial\nlocation and viewing direction) into a volume density and\nview-dependent emitted radiance. However, despite achieving an unprecedented level of photorealism on the generated images, NeRF is only applicable to static scenes, where\nthe same spatial location can be queried from different images. In this paper we introduce D-NeRF, a method that\nextends neural radiance fields to a dynamic domain, allowing to reconstruct and render novel images of objects under\nrigid and non-rigid motions from a single camera moving\naround the scene. For this purpose we consider time as an\nadditional input to the system, and split the learning process\nin two main stages: one that encodes the scene into a canonical space and another that maps this canonical representation into the deformed scene at a particular time. Both\nmappings are simultaneously learned using fully-connected\nnetworks. Once the networks are trained, D-NeRF can render novel images, controlling both the camera view and the\ntime variable, and thus, the object movement. We demonstrate the effectiveness of our approach on scenes with objects under rigid, articulated and non-rigid motions. Code model weights and the dynamic scenes dataset will be available at https://www.albertpumarola.com/research/D-NeRF/index.html."}}
{"id": "459MKVmrRne", "cdate": 1667577820354, "mdate": 1667577820354, "content": {"title": "SMPLicit: Topology-aware Generative Model for Clothed People", "abstract": "In this paper we introduce SMPLicit, a novel generative\nmodel to jointly represent body pose, shape and clothing geometry. In contrast to existing learning-based approaches\nthat require training specific models for each type of garment, SMPLicit can represent in a unified manner different\ngarment topologies (e.g. from sleeveless tops to hoodies and\nto open jackets), while controlling other properties like the\ngarment size or tightness/looseness. We show our model to\nbe applicable to a large variety of garments including Tshirts, hoodies, jackets, shorts, pants, skirts, shoes and even\nhair. The representation flexibility of SMPLicit builds upon\nan implicit model conditioned with the SMPL human body\nparameters and a learnable latent space which is semantically interpretable and aligned with the clothing attributes.\nThe proposed model is fully differentiable, allowing for its\nuse into larger end-to-end trainable systems. In the experimental section, we demonstrate SMPLicit can be readily\nused for fitting 3D scans and for 3D reconstruction in images of dressed people. In both cases we are able to go\nbeyond state of the art, by retrieving complex garment geometries, handling situations with multiple clothing layers\nand providing a tool for easy outfit editing. To stimulate further research in this direction, we will make our code and\nmodel publicly available at http://www.iri.upc.edu/people/ecorona/smplicit/."}}
{"id": "W4ub8fyCpED", "cdate": 1663849817875, "mdate": null, "content": {"title": "Learning a 3D-Aware Encoder for Style-based Generative Radiance Field", "abstract": "We tackle the task of GAN inversion for 3D generative radiance field, (e.g., StyleNeRF). In the inversion task, we aim to learn an inversion function to project an input image to the latent space of a generator and then synthesize novel views of the original image based on the latent code. Compared with GAN inversion for 2D generative models, 3D inversion not only needs to 1) preserve the identity of the input image, but also 2) ensure 3D consistency in generated novel views. This requires the latent code obtained from the single view image to be invariant across multiple views. To address this new challenge, we propose a two-stage encoder for 3D generative NeRF inversion. In the first stage, we introduce a base encoder that converts the input image to a latent code. To ensure the latent code can be used to synthesize identity preserving and 3D consistent novel view images, we utilize identity contrastive learning to train the base encoder. Since collecting real-world multi-view images of the same identity is expensive, we leverage multi-view images synthesized by the generator itself for contrastive learning. Second, to better preserve the identity of the input image, we introduce a residual encoder to refine the latent code and add finer details to the output image. Through extensive experiments, we demonstrate that our proposed two-stage encoder qualitatively and quantitatively exhibits superiority over the existing encoders for GAN inver- sion in both image reconstruction and novel-view rendering."}}
{"id": "tX_dIvk4j-s", "cdate": 1652737844998, "mdate": null, "content": {"title": "VisCo Grids: Surface Reconstruction with Viscosity and Coarea Grids", "abstract": "Surface reconstruction has been seeing a lot of progress lately by utilizing Implicit Neural Representations (INRs). Despite their success, INRs often introduce hard to control inductive bias (i.e., the solution surface can exhibit unexplainable behaviours), have costly inference, and are slow to train.  The goal of this work is to show that replacing neural networks with simple grid functions, along with two novel geometric priors achieve comparable results to INRs, with instant inference, and improved training times. To that end we introduce VisCo Grids: a grid-based surface reconstruction method incorporating Viscosity and Coarea priors. Intuitively, the Viscosity prior replaces the smoothness inductive bias of INRs, while the Coarea favors a minimal area solution. Experimenting with VisCo Grids on a standard reconstruction baseline provided comparable results to the best performing INRs on this dataset."}}
{"id": "usPOg9WJKP6", "cdate": 1649403531607, "mdate": 1649403531607, "content": {"title": "H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction", "abstract": "Recent learning approaches that implicitly represent surface geometry using coordinate-based neural representations have shown impressive results in the problem of multi-view 3D reconstruction. The effectiveness of these techniques is, however, subject to the availability of a large number (several tens) of input views of the scene, and computationally demanding optimizations. In this paper, we tackle these limitations for the specific problem of few-shot full 3D head reconstruction, by endowing coordinate-based representations with a probabilistic shape prior that enables faster convergence and better generalization when using few input images (down to three). First, we learn a shape model of 3D heads from thousands of incomplete raw scans using implicit representations. At test time, we jointly overfit two coordinate-based neural networks to the scene, one modeling the geometry and another estimating the surface radiance, using implicit differentiable rendering. We devise a two-stage optimization strategy in which the learned prior is used to initialize and constrain the geometry during an initial optimization phase. Then, the prior is unfrozen and fine-tuned to the scene. By doing this, we achieve high-fidelity head reconstructions, including hair and shoulders, and with a high level of detail that consistently outperforms both state-of-the-art 3D Morphable Models methods in the few-shot scenario, and non-parametric methods when large sets of views are available."}}
{"id": "kj8TBnJ0SXh", "cdate": 1632875704320, "mdate": null, "content": {"title": "FaceDet3D: Facial Expressions with 3D Geometric Detail Hallucination", "abstract": "Facial Expressions induce a variety of high-level details on the 3D face geometry. For example, a smile causes the wrinkling of cheeks or the formation of dimples, while being angry often causes wrinkling of the forehead. Morphable Models (3DMMs) of the human face fail to capture such fine details in their PCA-based representations  and consequently cannot generate such details when  used to edit expressions. In this work, we introduce FaceDet3D, a method that generates - from a single image - geometric facial details that are consistent with any desired target expression.  The facial details are represented as a vertex displacement map and used then by a Neural Renderer to photo-realistically render novel images of any single image in any desired expression and view. "}}
{"id": "SybAKAZObB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Geometry-Aware Network for Non-Rigid Shape Prediction From a Single View", "abstract": "We propose a method for predicting the 3D shape of a deformable surface from a single view. By contrast with previous approaches, we do not need a pre-registered template of the surface, and our method is robust to the lack of texture and partial occlusions. At the core of our approach is a geometry-aware deep architecture that tackles the problem as usually done in analytic solutions: first perform 2D detection of the mesh and then estimate a 3D shape that is geometrically consistent with the image. We train this architecture in an end-to-end manner using a large dataset of synthetic renderings of shapes under different levels of deformation, material properties, textures and lighting conditions. We evaluate our approach on a test split of this dataset and available real benchmarks, consistently improving state-of-the-art solutions with a significantly lower computational time."}}
{"id": "Skbyb6Wd-r", "cdate": 1514764800000, "mdate": null, "content": {"title": "Unsupervised Person Image Synthesis in Arbitrary Poses", "abstract": "We present a novel approach for synthesizing photo-realistic images of people in arbitrary poses using generative adversarial learning. Given an input image of a person and a desired pose represented by a 2D skeleton, our model renders the image of the same person under the new pose, synthesizing novel views of the parts visible in the input image and hallucinating those that are not seen. This problem has recently been addressed in a supervised manner, i.e., during training the ground truth images under the new poses are given to the network. We go beyond these approaches by proposing a fully unsupervised strategy. We tackle this challenging scenario by splitting the problem into two principal subtasks. First, we consider a pose conditioned bidirectional generator that maps back the initially rendered image to the original pose, hence being directly comparable to the input image without the need to resort to any training image. Second, we devise a novel loss function that incorporates content and style terms, and aims at producing images of high perceptual quality. Extensive experiments conducted on the DeepFashion dataset demonstrate that the images rendered by our model are very close in appearance to those obtained by fully supervised approaches."}}
{"id": "Hy442qWdZH", "cdate": 1514764800000, "mdate": null, "content": {"title": "GANimation: Anatomically-Aware Facial Animation from a Single Image", "abstract": "Recent advances in Generative Adversarial Networks (GANs) have shown impressive results for task of facial expression synthesis. The most successful architecture is StarGAN, that conditions GANs\u2019 generation process with images of a specific domain, namely a set of images of persons sharing the same expression. While effective, this approach can only generate a discrete number of expressions, determined by the content of the dataset. To address this limitation, in this paper, we introduce a novel GAN conditioning scheme based on Action Units (AU) annotations, which describes in a continuous manifold the anatomical facial movements defining a human expression. Our approach allows controlling the magnitude of activation of each AU and combine several of them. Additionally, we propose a fully unsupervised strategy to train the model, that only requires images annotated with their activated AUs, and exploit attention mechanisms that make our network robust to changing backgrounds and lighting conditions. Extensive evaluation show that our approach goes beyond competing conditional generators both in the capability to synthesize a much wider range of expressions ruled by anatomically feasible muscle movements, as in the capacity of dealing with images in the wild."}}
