{"id": "hZRxiAZFJC", "cdate": 1663850260875, "mdate": null, "content": {"title": "FairGrad: Fairness Aware Gradient Descent", "abstract": "We address the problem of group fairness in classification, where the objective is to learn models that do not unjustly discriminate against subgroups of the population. Most existing approaches are limited to simple binary tasks or involve difficult to implement training mechanisms. This reduces their practical applicability. In this paper, we propose FairGrad, a method to enforce fairness based on a reweighting scheme that iteratively learns group specific weights based on whether they are advantaged or not. FairGrad is easy to implement and can accommodate various standard fairness definitions. Furthermore, we show that it is competitive with standard baselines over various datasets including ones used in natural language processing and computer vision."}}
{"id": "r3OMM2pv9B_", "cdate": 1640995200000, "mdate": 1680705837849, "content": {"title": "Optimal Tensor Transport", "abstract": ""}}
{"id": "q8YS5aqEft", "cdate": 1640995200000, "mdate": 1680705886169, "content": {"title": "FairGrad: Fairness Aware Gradient Descent", "abstract": ""}}
{"id": "axkngSWpok", "cdate": 1640995200000, "mdate": 1675237705214, "content": {"title": "Fairness Certificates for Differentially Private Classification", "abstract": "We theoretically study the impact of differential privacy on fairness in classification. We prove that, given a class of models, popular group fairness measures are pointwise Lipschitz-continuous with respect to the parameters of the model. This result is a consequence of a more general statement on accuracy conditioned on an arbitrary event (such as membership to a sensitive group), which may be of independent interest. We use the aforementioned Lipschitz property to prove a high probability bound showing that, given enough examples, the fairness level of private models is close to the one of their non-private counterparts."}}
{"id": "Wxkp_UGm5P", "cdate": 1640995200000, "mdate": 1680705886236, "content": {"title": "A Revenue Function for Comparison-Based Hierarchical Clustering", "abstract": ""}}
{"id": "RfryHKJe_aq", "cdate": 1577836800000, "mdate": null, "content": {"title": "Too Relaxed to Be Fair", "abstract": "We address the problem of classification under fairness constraints. Given a notion of fairness, the goal is to learn a classifier that is not discriminatory against a group of individuals. In the ..."}}
{"id": "5vfaf4i6zOk", "cdate": 1577836800000, "mdate": 1680705886204, "content": {"title": "Near-Optimal Comparison Based Clustering", "abstract": ""}}
{"id": "HJgDnVHxIS", "cdate": 1567802542839, "mdate": null, "content": {"title": "Foundations of Comparison-Based Hierarchical Clustering", "abstract": "We address the classical problem of hierarchical clustering, but in a framework where one does not have access to a representation of the objects or their pairwise similarities. Instead, we assume that only a set of comparisons between objects is available, that is, statements of the form ``objects i and j are more similar than objects k and l.'' Such a scenario is commonly encountered in crowdsourcing applications. The focus of this work is to develop comparison-based hierarchical clustering algorithms that do not rely on the principles of ordinal embedding. We show that single and complete linkage are inherently comparison-based and we develop variants of average linkage. We provide statistical guarantees for the different methods under a planted hierarchical partition model. We also empirically demonstrate the performance of the proposed approaches on several datasets."}}
{"id": "oN3KgJmF5hf", "cdate": 1546300800000, "mdate": null, "content": {"title": "Boosting for Comparison-Based Learning", "abstract": "We consider the problem of classification in a comparison-based setting: given a set of objects, we only have access to triplet comparisons of the form ``object A is closer to object B than to object C.'' In this paper we introduce TripletBoost, a new method that can learn a classifier just from such triplet comparisons. The main idea is to aggregate the triplets information into weak classifiers, which can subsequently be boosted to a strong classifier. Our method has two main advantages: (i) it is applicable to data from any metric space, and (ii) it can deal with large scale problems using only passively obtained and noisy triplets. We derive theoretical generalization guarantees and a lower bound on the number of necessary triplets, and we empirically show that our method is both competitive with state of the art approaches and resistant to noise."}}
{"id": "CpxNOjjqJ5E", "cdate": 1514764800000, "mdate": 1680705886203, "content": {"title": "Foundations of Comparison-Based Hierarchical Clustering", "abstract": ""}}
