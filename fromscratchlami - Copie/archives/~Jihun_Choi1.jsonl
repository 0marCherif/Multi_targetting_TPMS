{"id": "Sci35YW4L8I", "cdate": 1640995200000, "mdate": 1676254117270, "content": {"title": "RecipeMind:  Guiding Ingredient Choices from Food Pairing to Recipe Completion using Cascaded Set Transformer", "abstract": "We propose a computational approach for recipe ideation, a downstream task that helps users select and gather ingredients for creating dishes. To perform this task, we developed RecipeMind, a food affinity score prediction model that quantifies the suitability of adding an ingredient to set of other ingredients. We constructed a large-scale dataset containing ingredient co-occurrence based scores to train and evaluate RecipeMind on food affinity score prediction. Deployed in recipe ideation, RecipeMind helps the user expand an initial set of ingredients by suggesting additional ingredients. Experiments and qualitative analysis show RecipeMind's potential in fulfilling its assistive role in cuisine domain."}}
{"id": "GLklT30_aO", "cdate": 1640995200000, "mdate": 1676254117270, "content": {"title": "RecipeMind: Guiding Ingredient Choices from Food Pairing to Recipe Completion using Cascaded Set Transformer", "abstract": "We propose a computational approach for recipe ideation, a downstream task that helps users select and gather ingredients for creating dishes. To perform this task, we developed RecipeMind, a food affinity score prediction model that quantifies the suitability of adding an ingredient to set of other ingredients. We constructed a large-scale dataset containing ingredient co-occurrence based scores to train and evaluate RecipeMind on food affinity score prediction. Deployed in recipe ideation, RecipeMind helps the user expand an initial set of ingredients by suggesting additional ingredients. Experiments and qualitative analysis show RecipeMind's potential in fulfilling its assistive role in cuisine domain."}}
{"id": "t7SRjWU50gt", "cdate": 1577836800000, "mdate": 1642047680233, "content": {"title": "Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction", "abstract": "With the recent success and popularity of pre-trained language models (LMs) in natural language processing, there has been a rise in efforts to understand their inner workings. In line with such interest, we propose a novel method that assists us in investigating the extent to which pre-trained LMs capture the syntactic notion of constituency. Our method provides an effective way of extracting constituency trees from the pre-trained LMs without training. In addition, we report intriguing findings in the induced trees, including the fact that pre-trained LMs outperform other approaches in correctly demarcating adverb phrases in sentences."}}
{"id": "d3VHua1nD6", "cdate": 1577836800000, "mdate": 1676254117269, "content": {"title": "Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction", "abstract": "With the recent success and popularity of pre-trained language models (LMs) in natural language processing, there has been a rise in efforts to understand their inner workings. In line with such interest, we propose a novel method that assists us in investigating the extent to which pre-trained LMs capture the syntactic notion of constituency. Our method provides an effective way of extracting constituency trees from the pre-trained LMs without training. In addition, we report intriguing findings in the induced trees, including the fact that pre-trained LMs outperform other approaches in correctly demarcating adverb phrases in sentences."}}
{"id": "H1xPR3NtPB", "cdate": 1569438926806, "mdate": null, "content": {"title": "Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction", "abstract": "With the recent success and popularity of pre-trained language models (LMs) in natural language processing, there has been a rise in efforts to understand their inner workings. \nIn line with such interest, we propose a novel method that assists us in investigating the extent to which pre-trained LMs capture the syntactic notion of constituency. \nOur method provides an effective way of extracting constituency trees from the pre-trained LMs without training. \nIn addition, we report intriguing findings in the induced trees, including the fact that pre-trained LMs outperform other approaches in correctly demarcating adverb phrases in sentences."}}
{"id": "pynvr6LvCe4", "cdate": 1546300800000, "mdate": 1636119084325, "content": {"title": "SNU_IDS at SemEval-2019 Task 3: Addressing Training-Test Class Distribution Mismatch in Conversational Classification", "abstract": "We present several techniques to tackle the mismatch in class distributions between training and test data in the Contextual Emotion Detection task of SemEval 2019, by extending the existing methods for class imbalance problem. Reducing the distance between the distribution of prediction and ground truth, they consistently show positive effects on the performance. Also we propose a novel neural architecture which utilizes representation of overall context as well as of each utterance. The combination of the methods and the models achieved micro F1 score of about 0.766 on the final evaluation."}}
{"id": "klb7BIryQ1", "cdate": 1546300800000, "mdate": 1676254117404, "content": {"title": "A Cross-Sentence Latent Variable Model for Semi-Supervised Text Sequence Matching", "abstract": "We present a latent variable model for predicting the relationship between a pair of text sequences. Unlike previous auto-encoding--based approaches that consider each sequence separately, our proposed framework utilizes both sequences within a single model by generating a sequence that has a given relationship with a source sequence. We further extend the cross-sentence generating framework to facilitate semi-supervised training. We also define novel semantic constraints that lead the decoder network to generate semantically plausible and diverse sequences. We demonstrate the effectiveness of the proposed model from quantitative and qualitative experiments, while achieving state-of-the-art results on semi-supervised natural language inference and paraphrase identification."}}
{"id": "iVFyFUMkSFz", "cdate": 1546300800000, "mdate": 1636119084085, "content": {"title": "Dynamic Compositionality in Recursive Neural Networks with Structure-Aware Tag Representations", "abstract": "Most existing recursive neural network (RvNN) architectures utilize only the structure of parse trees, ignoring syntactic tags which are provided as by-products of parsing. We present a novel RvNN architecture that can provide dynamic compositionality by considering comprehensive syntactic information derived from both the structure and linguistic tags. Specifically, we introduce a structure-aware tag representation constructed by a separate tag-level tree-LSTM. With this, we can control the composition function of the existing wordlevel tree-LSTM by augmenting the representation as a supplementary input to the gate functions of the tree-LSTM. In extensive experiments, we show that models built upon the proposed architecture obtain superior or competitive performance on several sentence-level tasks such as sentiment analysis and natural language inference when compared against previous tree-structured models and other sophisticated neural models."}}
{"id": "i0bgSAiQD9Z", "cdate": 1546300800000, "mdate": null, "content": {"title": "Cell-aware Stacked LSTMs for Modeling Sentences", "abstract": "We propose a method of stacking multiple long short-term memory (LSTM) layers for modeling sentences. In contrast to the conventional stacked LSTMs where only hidden states are fed as input to the ..."}}
{"id": "3GkHokwqoc6", "cdate": 1546300800000, "mdate": 1636119084260, "content": {"title": "SNU IDS at SemEval-2019 Task 3: Addressing Training-Test Class Distribution Mismatch in Conversational Classification", "abstract": "Sanghwan Bae, Jihun Choi, Sang-goo Lee. Proceedings of the 13th International Workshop on Semantic Evaluation. 2019."}}
