{"id": "pA2uDAvTpp", "cdate": 1676472364161, "mdate": null, "content": {"title": "Feature-Interpretable Real Concept Drift Detection", "abstract": "Classifiers deployed in production degrade in performance due to changes in the posterior distribution, a phenomenon referred to as real concept drift. Knowledge of such distribution shifts is helpful for two main reasons: (i) it helps retain classifier performance across time by telling us when to retrain it; and (ii) understanding the nature of shift in the relationship between input features and output labels, which can be of value for business analytics (e.g., understanding change in demand helps manage inventory) or scientific study (e.g., understanding virus behavior across changing demographics helps distribute drugs better). An interpretable real concept drift detection method is ideal for achieving this knowledge. Existing interpretable methods in this space only track covariate shifts, thus, are insensitive to the optimal decision boundary (true posterior distribution) and vulnerable to benign drifts in streaming data. Our work addresses this issue by proposing an interpretable method that leverages gradients of a classifier in a feature-wise hypothesis-testing framework to detect real concept drift. We also extend our method to a more realistic unsupervised setting where labels are not available to detect drift. Our experiments on various datasets show that the proposed method outperforms existing interpretable methods and performs at par with state-of-the-art supervised drift detection methods w.r.t the average model classification accuracy metric. Qualitatively, our method identifies features that are relevant to the drift in the USENET2 dataset, thus providing interpretability and accurate drift detection. "}}
{"id": "e8Eletuo1hk", "cdate": 1676472363837, "mdate": null, "content": {"title": "Do Models see Corruption as we see? An Item Response Theory based study in Computer Vision", "abstract": "On a given dataset, some models perform better than others. Can we examine this performance w.r.t. different strata of the dataset rather than just focusing on an aggregate metric (such as accuracy)? Given that noise and corruption are natural in real-world settings, can we study model failure under such scenarios? For a particular corruption type, do some classes become more difficult to classify than others? To answer such fine-grained questions, in this paper, we explore the use of Item Response Theory (IRT) in computer vision tasks to gain deeper insights into the behavior of models and datasets, especially under corruption. We show that incorporating IRT can provide instance-level understanding beyond what classical metrics (such as accuracy) can provide. Our findings highlight the ability of IRT to detect changes in the distribution of the dataset when it is perturbed through corruption, using latent parameters derived from IRT models. These latent parameters can effectively suggest annotation errors, informative images, and class-level information while highlighting the robustness of different models and dataset classes under consideration."}}
{"id": "vMHlxIZAiV", "cdate": 1672531200000, "mdate": 1681649878593, "content": {"title": "ARUBA: An Architecture-Agnostic Balanced Loss for Aerial Object Detection", "abstract": ""}}
{"id": "Wwror1rD6zt", "cdate": 1672531200000, "mdate": 1681649878587, "content": {"title": "Learning Causal Attributions in Neural Networks: Beyond Direct Effects", "abstract": ""}}
{"id": "RQISiqVUsJ3", "cdate": 1672531200000, "mdate": 1681649878842, "content": {"title": "Learning Style Subspaces for Controllable Unpaired Domain Translation", "abstract": ""}}
{"id": "JOzJkub1cdX", "cdate": 1672531200000, "mdate": 1681649878501, "content": {"title": "RetroKD : Leveraging Past States for Regularizing Targets in Teacher-Student Learning", "abstract": ""}}
{"id": "J9jWsLOGkf", "cdate": 1672531200000, "mdate": 1681649878571, "content": {"title": "\u0394-Networks for Efficient Model Patching", "abstract": ""}}
{"id": "HNCTNDYi0d", "cdate": 1672531200000, "mdate": 1681649878623, "content": {"title": "Towards Estimating Transferability using Hard Subsets", "abstract": ""}}
{"id": "XFyRx_a2kO", "cdate": 1669249461886, "mdate": 1669249461886, "content": {"title": "Proto2Proto: Can you recognize the car, the way I do?", "abstract": "Prototypical methods have recently gained a lot of attention due to their intrinsic interpretable nature, which is obtained through the prototypes. With growing use cases of model reuse and distillation, there is a need to also study transfer of interpretability from one model to another. We present Proto2Proto, a novel method to transfer interpretability of one prototypical part network to another via knowledge distillation. Our approach aims to add interpretability to the \u201cdark\u201d knowledge transferred from the teacher to the shallower student model. We propose two novel losses: \u201cGlobal Explanation\u201d loss and \u201cPatch-Prototype Correspondence\u201d loss to facilitate such a transfer. Global Explanation loss forces the student prototypes to be close to teacher prototypes, and Patch-Prototype Cor-respondence loss enforces the local representations of the student to be similar to that of the teacher. Further, we propose three novel metrics to evaluate the student's proximity to the teacher as measures of interpretability transfer in our settings. We qualitatively and quantitatively demon-strate the effectiveness of our method on CUB-200-2011 and Stanford Cars datasets. Our experiments show that the proposed method indeed achieves interpretability transfer from teacher to student while simultaneously exhibiting competitive performance. The code is available at h t tps: //github.com/archmaester/proto2proto\u00a0"}}
{"id": "POkTmN4gax", "cdate": 1667315145805, "mdate": null, "content": {"title": "Leveraging Test-Time Consensus Prediction for Robustness against Unseen Noise", "abstract": "We propose a method to improve DNN robustness against unseen noisy corruptions, such as Gaussian noise, Shot Noise, Impulse Noise, Speckle noise with different levels of severity by leveraging ensemble technique through a consensus based prediction method using self-supervised learning at inference time. We also propose to enhance the model training by considering other aspects of the issue ie noise in data and better representation learning which shows even better generalization performance with the consensus based prediction strategy. We report results of each noisy corruption on the standard CIFAR10-C and ImageNet-C benchmark which shows significant boost in performance over previous methods. We also introduce results for MNIST-C and TinyImagenet-C to show usefulness of our method across datasets of different complexities to provide robustness against unseen noise. We show results with different architectures to validate our method against other baseline methods, and also conduct experiments to show the usefulness of each part of our method."}}
