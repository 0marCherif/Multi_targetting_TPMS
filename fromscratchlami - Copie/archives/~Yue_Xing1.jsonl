{"id": "qqG3Y2zTU7F", "cdate": 1683908060768, "mdate": null, "content": {"title": "Benefit of interpolation in nearest neighbor algorithms", "abstract": "In some studies (e.g., [C. Zhang et al. in Proceedings of the 5th International Conference on Learning Representations, OpenReview.net, 2017]) of deep learning, it is observed that overparametrized deep neural networks achieve a small testing error even when the training error is almost zero. Despite numerous works toward understanding this so-called double-descent phenomenon (e.g., [M. Belkin et al., Proc. Natl. Acad. Sci. USA, 116 (2019), pp. 15849--15854; M. Belkin, D. Hsu, and J. Xu, SIAM J. Math. Data Sci., 2 (2020), pp. 1167--1180]), in this paper, we turn to another way to enforce zero training error (without overparametrization) through a data interpolation mechanism. Specifically, we consider a class of interpolated weighting schemes in the nearest neighbors (NN) algorithms. By carefully characterizing the multiplicative constant in the statistical risk, we reveal a U-shaped performance curve for the level of data interpolation in both classification and regression setups. This sharpens the existing result [M. Belkin, A. Rakhlin, and A. B. Tsybakov, in Proceedings of Machine Learning Research 89, PMLR, 2019, pp. 1611--1619] that zero training error does not necessarily jeopardize predictive performances and claims a counterintuitive result that a mild degree of data interpolation actually strictly improves the prediction performance and statistical stability over those of the (uninterpolated) \ud835\udc58\n-NN algorithm. In the end, the universality of our results, such as change of distance measure and corrupted testing data, will also be discussed."}}
{"id": "x_qYCaFaqg", "cdate": 1683908012310, "mdate": 1683908012310, "content": {"title": "Distributed Censored Quantile Regression", "abstract": "This article discusses an extension of censored quantile regression to a distributed setting. With the growing availability of massive datasets, it is oftentimes an arduous task to analyze all the data with limited computational facilities efficiently. Our proposed method, which attempts to overcome this challenge, is comprised of two key steps, namely: (i) estimation of both Kaplan-Meier estimator and model coefficients in a parallel computing environment; (ii) aggregation of coefficient estimations from individual machines. We study the upper limit of the order of the number of machines for this computing environment, which, if fulfilled, guarantees that the proposed estimator converges at a comparable rate to that of the oracle estimator. In addition, we also provide two further modifications for distributed systems including (i) a communication-facilitated adaptation in the sense of Chen, Liu, and Zhang and (ii) a nonparametric counterpart along the direction of Kong and Xia for censored quantile regression. Numerical experiments are conducted to compare the proposed and the existing estimators. The promising results demonstrate the computation efficiency of the proposed methods. Finally, for practical concerns, a cross-validation procedure is also developed which can better select the hyperparameters for the proposed methodologies. Supplementary materials for this article are available online."}}
{"id": "qjBRNFLzW9", "cdate": 1683907909180, "mdate": 1683907909180, "content": {"title": "Variance Reduction for Risk Measures with Importance Sampling in Nested Simulation", "abstract": "Value-at-Risk (VaR) and Conditional Value-at-Risk (CVaR) are two standard risk measures that are widely adopted in both financial and insurance industries. Simulation-based approaches including nested simulation and least-squares Monte Carlo are effective strategies to yield reliable estimates of these risk measures, but there remain open questions on how importance sampling can be incorporated to improve estimation efficiency. In this paper, we extend the scope of importance sampling from simple Monte Carlo to nested simulation settings and its adaptations for American-type options; we also establish the asymptotic consistency of importance sampling. Numerical results consistent with our theoretical analysis are provided to verify its effectiveness."}}
{"id": "W-Z8n9HrWn0", "cdate": 1652737343348, "mdate": null, "content": {"title": "Why Do Artificially Generated Data Help Adversarial Robustness", "abstract": "In the adversarial training framework of \\cite{carmon2019unlabeled,gowal2021improving}, people use generated/real unlabeled data with pseudolabels to improve adversarial robustness. We provide statistical insights to explain why the artificially generated data improve adversarial training. In particular, we study how the attack strength and the quality of the unlabeled data affect adversarial robustness in this framework. Our results show that with a high-quality unlabeled data generator, adversarial training can benefit greatly from this framework under large attack strength, while a poor generator can still help to some extent. To make adaptions concerning the quality of generated data, we propose an algorithm that performs online adjustment to the weight between the labeled real data and the generated data, aiming to optimize the adversarial risk. Numerical studies are conducted to verify our theories and show the effectiveness of the proposed algorithm."}}
{"id": "gwsnBjNcVEe", "cdate": 1652737276536, "mdate": null, "content": {"title": "Phase Transition from Clean Training to Adversarial Training", "abstract": "Adversarial training is one important algorithm to achieve robust machine learning models. However, numerous empirical results show a great performance degradation from clean training to adversarial training (e.g., 90+\\% vs 67\\% testing accuracy on CIFAR-10 dataset), which does not match the theoretical guarantee delivered by the existing studies. Such a gap inspires us to explore the existence of an (asymptotic) phase transition phenomenon with respect to the attack strength: adversarial training is as well behaved as clean training in the small-attack regime, but there is a sharp transition from clean training to adversarial training in the large-attack regime. We validate this conjecture in linear regression models, and conduct comprehensive experiments in deep neural networks."}}
{"id": "GXz2KmaZYR", "cdate": 1640995200000, "mdate": 1682427988954, "content": {"title": "Unlabeled Data Help: Minimax Analysis and Adversarial Robustness", "abstract": "The recent proposed self-supervised learning (SSL) approaches successfully demonstrate the great potential of supplementing learning algorithms with additional unlabeled data. However, it is still unclear whether the existing SSL algorithms can fully utilize the information of both labelled and unlabeled data. This paper gives an affirmative answer for the reconstruction-based SSL algorithm (Lee et al., 2020) under several statistical models. While existing literature only focuses on establishing the upper bound of the convergence rate, we provide a rigorous minimax analysis, and successfully justify the rate-optimality of the reconstruction-based SSL algorithm under different data generation models. Furthermore, we incorporate the reconstruction-based SSL into the exist- ing adversarial training algorithms and show that learning from unlabeled data helps improve the robustness."}}
{"id": "xz80iPFIjvG", "cdate": 1621629789353, "mdate": null, "content": {"title": "On the Algorithmic Stability of Adversarial Training", "abstract": "The adversarial training is a popular tool to remedy the vulnerability of deep learning models against adversarial attacks, and there is rich theoretical literature on the training loss of adversarial training algorithms. In contrast, this paper studies the algorithmic stability of a generic adversarial training algorithm, which can further help to establish an upper bound for generalization error. By figuring out the stability upper bound and lower bound, we argue that the non-differentiability issue of adversarial training causes worse algorithmic stability than their natural counterparts. To tackle this problem, we consider a noise injection method. While the non-differentiability problem seriously affects the stability of adversarial training, injecting noise enables the training trajectory to avoid the occurrence of non-differentiability with dominating probability, hence enhancing the stability performance of adversarial training. Our analysis also studies the relation between the algorithm stability and numerical approximation error of adversarial attacks."}}
{"id": "93ssAr8rHl-", "cdate": 1620327539069, "mdate": null, "content": {"title": "Directional Pruning of Deep Neural Networks", "abstract": "In the light of the fact that the stochastic gradient descent (SGD) often finds a flat minimum valley in the training loss, we propose a novel directional pruning method which searches for a sparse minimizer in or close to that flat region. The proposed pruning method does not require retraining or the expert knowledge on the sparsity level. To overcome the computational formidability of estimating the flat directions, we propose to use a carefully tuned \u21131 proximal gradient algorithm which can provably achieve the directional pruning with a small learning rate after sufficient training. The empirical results demonstrate the promising results of our solution in highly sparse regime (92% sparsity) among many existing pruning methods on the ResNet50 with the ImageNet, while using only a slightly higher wall time and memory footprint than the SGD. Using the VGG16 and the wide ResNet 28x10 on the CIFAR-10 and CIFAR-100, we demonstrate that our solution reaches the same minima valley as the SGD, and the minima found by our solution and the SGD do not deviate in directions that impact the training loss. "}}
{"id": "6YhoNYwItGr", "cdate": 1620327471361, "mdate": null, "content": {"title": "Adversarially Robust Estimate and Risk Analysis in Linear Regression", "abstract": "Adversarial robust learning aims to design algorithms that are robust to small adversarial perturbations on input variables. Beyond the existing studies on the predictive performance to adversarial samples, our goal is to understand statistical properties of adversarial robust estimates and analyze adversarial risk in the setup of linear regression models. By discovering the statistical minimax rate of convergence of adversarial robust estimators, we emphasize the importance of incorporating model information, e.g., sparsity, in adversarial robust learning. Further, we reveal an explicit connection of adversarial and standard estimates, and propose a straightforward two-stage adversarial training framework, which facilitates to utilize model structure information to improve adversarial robustness. In theory, the consistency of the adversarial robust estimator is proven and its Bahadur representation is also developed for the statistical inference purpose. The proposed estimator converges in a sharp rate under either low-dimensional or sparse scenario. Moreover, our theory confirms two phenomena in adversarial robust learning: adversarial robustness hurts generalization, and unlabeled data help improve the generalization. In the end, we conduct numerical simulations to verify our theory."}}
{"id": "koNHNL8gMDn", "cdate": 1620327391556, "mdate": null, "content": {"title": "Predictive Power of Nearest Neighbors Algorithm under Random Perturbation", "abstract": "This work investigates the predictive performance of the classical \ud835\udc58 Nearest Neighbors (\ud835\udc58-NN) algorithm when the testing data are corrupted by random perturbation. The impact of corruption level on the asymptotic regret is carefully characterized and we reveal a phase-transition phenomenon that, when the corruption level of the random perturbation \ud835\udf14 is below a critical order (i.e., small-\ud835\udf14 regime), the asymptotic regret remains the same; when it is beyond that order (i.e., large-\ud835\udf14 regime), the asymptotic regret deteriorates polynomially. More importantly, the regret of \ud835\udc58-NN classifier heuristically matches the rate of minimax regret for randomly perturbed testing data, thus implies the strong robustness of \ud835\udc58-NN against random perturbation on testing data. In fact, we show that the classical \ud835\udc58-NN can achieve no worse predictive performance, compared to the NN classifiers trained via the popular noise-injection strategy. Our numerical experiment also illustrates that combining \ud835\udc58-NN component with modern learning algorithms will inherit the strong robustness of \ud835\udc58-NN. As a technical by-product, we prove that under different model assumptions, the pre-processed 1-NN proposed in \\cite{xue2017achieving} will at most achieve a sub-optimal rate when the data dimension \ud835\udc51>4 even if \ud835\udc58 is chosen optimally in the pre-processing step."}}
