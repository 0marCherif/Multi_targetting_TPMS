{"id": "I3HCE7Ro78H", "cdate": 1663850588389, "mdate": null, "content": {"title": "Finding Actual Descent Directions for Adversarial Training", "abstract": "Adversarial Training using a strong first-order adversary (PGD) is the gold standard for training Deep Neural Networks that are robust to adversarial examples. We show that, contrary to the general understanding of the method, the gradient at an optimal adversarial example may increase, rather than decrease, the adversarially robust loss. This holds independently of the learning rate. More precisely, we provide a counterexample to a corollary of Danskin's Theorem presented in the seminal paper of Madry et al. (2018) which states that a solution of the inner maximization problem can yield a descent direction for the adversarially robust loss. Based on a correct interpretation of Danskin's Theorem, we propose Danskin's Descent Direction (DDi) and we verify experimentally that it provides better directions than those obtained by a PGD adversary. Using the CIFAR10 dataset we further provide a real world example showing that our method achieves a steeper increase in robustness levels in the early stages of training, and is more stable than the PGD baseline. As a limitation, PGD training of ReLU+BatchNorm networks still performs better, but current theory is unable to explain this.\n"}}
{"id": "k98U0cb0Ig", "cdate": 1652737762618, "mdate": null, "content": {"title": "Adaptive Stochastic Variance Reduction for Non-convex Finite-Sum Minimization", "abstract": "We propose an adaptive variance-reduction method, called AdaSpider, for minimization of $L$-smooth, non-convex functions with a finite-sum structure. In essence, AdaSpider combines an AdaGrad-inspired (Duchi et al., 2011), but a fairly distinct, adaptive step-size schedule with the recursive \\textit{stochastic path integrated estimator} proposed in (Fang et al., 2018). To our knowledge, AdaSpider is the first parameter-free non-convex variance-reduction method in the sense that it does not require the knowledge of problem-dependent parameters, such as smoothness constant $L$, target accuracy $\\epsilon$ or any bound on gradient norms. In doing so, we are able to compute an $\\epsilon$-stationary point with $\\tilde{O}\\left(n + \\sqrt{n}/\\epsilon^2\\right)$ oracle-calls, which matches the respective lower bound up to logarithmic factors."}}
{"id": "fJI0NMpR_VB", "cdate": 1640995200000, "mdate": 1682323382715, "content": {"title": "The Spectral Bias of Polynomial Neural Networks", "abstract": "Polynomial neural networks (PNNs) have been recently shown to be particularly effective at image generation and face recognition, where high-frequency information is critical. Previous studies have..."}}
{"id": "_ieMo4Xf4TT", "cdate": 1640995200000, "mdate": 1682321038239, "content": {"title": "Adaptive Stochastic Variance Reduction for Non-convex Finite-Sum Minimization", "abstract": "We propose an adaptive variance-reduction method, called AdaSpider, for minimization of $L$-smooth, non-convex functions with a finite-sum structure. In essence, AdaSpider combines an AdaGrad-inspired [Duchi et al., 2011, McMahan & Streeter, 2010], but a fairly distinct, adaptive step-size schedule with the recursive stochastic path integrated estimator proposed in [Fang et al., 2018]. To our knowledge, Adaspider is the first parameter-free non-convex variance-reduction method in the sense that it does not require the knowledge of problem-dependent parameters, such as smoothness constant $L$, target accuracy $\\epsilon$ or any bound on gradient norms. In doing so, we are able to compute an $\\epsilon$-stationary point with $\\tilde{O}\\left(n + \\sqrt{n}/\\epsilon^2\\right)$ oracle-calls, which matches the respective lower bound up to logarithmic factors."}}
{"id": "P7FLfMLTSEX", "cdate": 1632875681412, "mdate": null, "content": {"title": "The Spectral Bias of Polynomial Neural Networks", "abstract": "Polynomial neural networks (PNNs) have been recently shown to be particularly effective at image generation and face recognition, where high-frequency information is critical. Previous studies have revealed that neural networks demonstrate a $\\text{\\it{spectral bias}}$ towards low-frequency functions, which yields faster learning of low-frequency components during training. Inspired by such studies, we conduct a spectral analysis of the Neural Tangent Kernel (NTK) of PNNs. We find that the $\\Pi$-Net family, i.e., a recently proposed parametrization of PNNs, speeds up the learning of the higher frequencies. \nWe verify the theoretical bias through extensive experiments. We expect our analysis to provide novel insights into designing architectures and learning frameworks by incorporating multiplicative interactions via polynomials. \n"}}
{"id": "_hKvtsqItc", "cdate": 1621630172306, "mdate": null, "content": {"title": "The Effect of the Intrinsic Dimension on the Generalization of Quadratic Classifiers", "abstract": "It has been recently observed that neural networks, unlike kernel methods, enjoy a reduced sample complexity when the distribution is isotropic (i.e., when the covariance matrix is the identity). We find that this sensitivity to the data distribution is not exclusive to neural networks, and the same phenomenon can be observed on the class of quadratic classifiers (i.e., the sign of a quadratic polynomial) with a nuclear-norm constraint. We demonstrate this by deriving an upper bound on the Rademacher Complexity that depends on two key quantities: (i) the intrinsic dimension, which is a measure of isotropy, and (ii) the largest eigenvalue of the second moment (covariance) matrix of the distribution. Our result improves the dependence on the dimension over the best previously known bound and precisely quantifies the relation between the sample complexity and the level of isotropy of the distribution."}}
{"id": "kgkzl6GglX", "cdate": 1609459200000, "mdate": 1682327555503, "content": {"title": "The Effect of the Intrinsic Dimension on the Generalization of Quadratic Classifiers", "abstract": "It has been recently observed that neural networks, unlike kernel methods, enjoy a reduced sample complexity when the distribution is isotropic (i.e., when the covariance matrix is the identity). We find that this sensitivity to the data distribution is not exclusive to neural networks, and the same phenomenon can be observed on the class of quadratic classifiers (i.e., the sign of a quadratic polynomial) with a nuclear-norm constraint. We demonstrate this by deriving an upper bound on the Rademacher Complexity that depends on two key quantities: (i) the intrinsic dimension, which is a measure of isotropy, and (ii) the largest eigenvalue of the second moment (covariance) matrix of the distribution. Our result improves the dependence on the dimension over the best previously known bound and precisely quantifies the relation between the sample complexity and the level of isotropy of the distribution."}}
