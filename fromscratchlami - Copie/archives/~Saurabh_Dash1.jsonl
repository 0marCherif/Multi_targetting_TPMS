{"id": "ZCStthyW-TD", "cdate": 1663850388980, "mdate": null, "content": {"title": "Associative Memory Augmented Asynchronous Spatiotemporal Representation Learning for Event-based Perception", "abstract": "We propose $\\textit{EventFormer}$, a computationally efficient event-based representation learning framework for asynchronously processing event camera data. EventFormer treats sparse input events as a spatially unordered set and models their spatial interactions using self-attention mechanism. An associative memory-augmented recurrent module is used to correlate with the stored representation computed from past events. A memory addressing mechanism is proposed to store and retrieve the latent states only $\\textit{where}$ these events occur and update them only $\\textit{when}$ they occur. The representation learning shift from input space to the latent memory space resulting in reduced computation cost for processing each event. We show that EventFormer achieves 0.5$\\%$ and 9$\\%$ better accuracy with 30000$\\times$ and 200$\\times$ less computation compared to the state-of-the-art dense and event-based method, respectively, on event-based object recognition datasets."}}
{"id": "fdRh3699oSC", "cdate": 1640995200000, "mdate": 1668687937241, "content": {"title": "Robust Processing-In-Memory With Multibit ReRAM Using Hessian-Driven Mixed-Precision Computation", "abstract": "This article presents an algorithmic approach to design reliable deep neural networks (DNNs) in the presence of stochastic variations in the network parameters induced by process variations in the bit cells in a processing-in-memory (PIM) architecture. We propose and derive a Hessian-based sensitivity metric that can be computed without computing or storing the full Hessian to identify and protect the \u201cimportant\u201d network parameters while allowing large variations in unprotected parameters. We also show that this metric can be used to aggressively quantize unprotected network parameters in the PIM for improved inference efficiency and compute density. Experiments on modern DNNs like ResNet, MobileNetv2, and DenseNet on CIFAR10 using measured RRAM device data shows the effectiveness of our approach."}}
{"id": "aWnYCeVQwF", "cdate": 1640995200000, "mdate": 1668687937242, "content": {"title": "Unsupervised Hebbian Learning on Point Sets in StarCraft II", "abstract": "Learning the evolution of real-time strategy (RTS) game is a challenging problem in artificial intelligent (AI) system. In this paper, we present a novel Hebbian learning method to extract the global feature of point sets in StarCraft II game units, and its application to predict the movement of the points. Our model includes encoder, LSTM, and decoder, and we train the encoder with the unsupervised learning method. We introduce the concept of neuron activity aware learning combined with k-Winner-Takes-All. The optimal value of neuron activity is mathematically derived, and experiments support the effectiveness of the concept over the downstream task. Our Hebbian learning rule benefits the prediction with lower loss compared to self-supervised learning. Also, our model significantly saves the computational cost such as activations and FLOPs compared to a frame-based approach."}}
{"id": "VjLw1gdu_9T", "cdate": 1640995200000, "mdate": 1668687937238, "content": {"title": "Sequence Approximation using Feedforward Spiking Neural Network for Spatiotemporal Learning: Theory and Optimization Methods", "abstract": "A dynamical system of spiking neurons with only feedforward connections can classify spatiotemporal patterns without recurrent connections. However, the theoretical construct of a feedforward spiking neural network (SNN) for approximating a temporal sequence remains unclear, making it challenging to optimize SNN architectures for learning complex spatiotemporal patterns. In this work, we establish a theoretical framework to understand and improve sequence approximation using a feedforward SNN. Our framework shows that a feedforward SNN with one neuron per layer and skip-layer connections can approximate the mapping function between any arbitrary pairs of input and output spike train on a compact domain. Moreover, we prove that heterogeneous neurons with varying dynamics and skip-layer connections improve sequence approximation using feedforward SNN. Consequently, we propose SNN architectures incorporating the preceding constructs that are trained using supervised backpropagation-through-time (BPTT) and unsupervised spiking-timing-dependent plasticity (STDP) algorithms for classification of spatiotemporal data. A dual-search-space Bayesian optimization method is developed to optimize architecture and parameters of the proposed SNN with heterogeneous neuron dynamics and skip-layer connections."}}
{"id": "J241P3El-7K", "cdate": 1640995200000, "mdate": 1668687937241, "content": {"title": "Unsupervised Hebbian Learning on Point Sets in StarCraft II", "abstract": "Learning the evolution of real-time strategy (RTS) game is a challenging problem in artificial intelligent (AI) system. In this paper, we present a novel Hebbian learning method to extract the global feature of a point set in StarCraft II game units, and its application to predict the movement of the points. Our model includes encoder, LSTM, and decoder, and we train the encoder with the unsupervised learning method. We introduce the concept of neuron activity aware learning combined with k-Winner-Takes-All. The optimal value of neuron activity is mathematically derived, and experiments support the effectiveness of the concept over the downstream task. Our Hebbian learning rule benefits the prediction with lower loss compared to self-supervised learning. Also, our model significantly saves the computational cost such as activations and FLOPs compared to a frame-based approach."}}
{"id": "2IrYSYjgt2F", "cdate": 1640995200000, "mdate": 1668687937543, "content": {"title": "Learning Point Processes using Recurrent Graph Network", "abstract": "We present a novel Recurrent Graph Network (RGN) approach for predicting discrete marked event sequences by learning the underlying complex stochastic process. Using the framework of Point Processes, we interpret a marked discrete event sequence as the superposition of different sequences each of a unique type. The nodes of the Graph Network use LSTM to incorporate past information whereas a Graph Attention Network (GAT Network) introduces strong inductive biases to capture the interaction between these different types of events. By changing the self-attention mechanism from attending over past events to attending over event types, we obtain a reduction in time and space complexity from <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathcal{O}(N^{2})$</tex> (total number of events) to <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathcal{O}(\\vert \\mathcal{Y}\\vert^{2})$</tex> (number of event types). Experiments show that the proposed approach improves performance in log-likelihood, prediction and goodness-of-fit tasks with lower time and space complexity compared to state-of-the art Transformer based architectures."}}
{"id": "2CYr5NIfoXa", "cdate": 1640995200000, "mdate": 1668687937245, "content": {"title": "Learning Point Processes using Recurrent Graph Network", "abstract": "We present a novel Recurrent Graph Network (RGN) approach for predicting discrete marked event sequences by learning the underlying complex stochastic process. Using the framework of Point Processes, we interpret a marked discrete event sequence as the superposition of different sequences each of a unique type. The nodes of the Graph Network use LSTM to incorporate past information whereas a Graph Attention Network (GAT Network) introduces strong inductive biases to capture the interaction between these different types of events. By changing the self-attention mechanism from attending over past events to attending over event types, we obtain a reduction in time and space complexity from $\\mathcal{O}(N^2)$ (total number of events) to $\\mathcal{O}(|\\mathcal{Y}|^2)$ (number of event types). Experiments show that the proposed approach improves performance in log-likelihood, prediction and goodness-of-fit tasks with lower time and space complexity compared to state-of-the art Transformer based architectures."}}
{"id": "bp-LJ4y_XC", "cdate": 1632875767646, "mdate": null, "content": {"title": "Sequence Approximation using Feedforward Spiking Neural Network for Spatiotemporal Learning: Theory and Optimization Methods", "abstract": "A dynamical system of spiking neurons with only feedforward connections can classify spatiotemporal patterns without recurrent connections. However, the theoretical construct of a feedforward spiking neural network (SNN) for approximating a temporal sequence remains unclear, making it challenging to optimize SNN architectures for learning complex spatiotemporal patterns. In this work, we establish a theoretical framework to understand and improve sequence approximation using a feedforward SNN. Our framework shows that a feedforward SNN with one neuron per layer and skip-layer connections can approximate the mapping function between any arbitrary pairs of input and output spike train on a compact domain. Moreover, we prove that heterogeneous neurons with varying dynamics and skip-layer connections improve sequence approximation using feedforward SNN. Consequently, we propose SNN architectures incorporating the preceding constructs that are trained using supervised backpropagation-through-time (BPTT) and unsupervised spiking-timing-dependent plasticity (STDP) algorithms for classification of spatiotemporal data. A dual-search-space Bayesian optimization method is developed to optimize architecture and parameters of the proposed SNN with heterogeneous neuron dynamics and skip-layer connections. "}}
{"id": "dNXPKA2cge", "cdate": 1609459200000, "mdate": 1668687937246, "content": {"title": "Physics-incorporated convolutional recurrent neural networks for source identification and forecasting of dynamical systems", "abstract": ""}}
{"id": "BHI8o3QMDC", "cdate": 1609459200000, "mdate": 1668687937247, "content": {"title": "Characterization of Drain Current Variations in FeFETs for PIM-based DNN Accelerators", "abstract": "We analyze the impact of drain current (I <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">DS</sub> ) variation in 28 nm high-K metal-gate Ferroelectric FET devices on FeFET-based processing-in-memory (PIM) deep neural network (DNN) accelerators. Non-Normal variation in I <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">DS</sub> is observed due to repeated read operation on FeFET devices with different channel dimensions at various read frequencies. Device-circuit co-analysis using the measured current distribution shows a 1 to 3 percent accuracy degradation of an FeFET-based PIM platform when classifying the Fashion-MNIST dataset with the LeNET-5 DNN model. This accuracy drop can be fully recovered with variation-aware training methods, showing that individual FeFET device current variation over many read cycles is not prohibitive to the design of DNN accelerators."}}
