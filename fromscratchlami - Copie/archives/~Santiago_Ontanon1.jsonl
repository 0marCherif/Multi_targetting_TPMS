{"id": "HAGeIS_Lcg9", "cdate": 1646057533682, "mdate": null, "content": {"title": "LogicInference: A new Datasaet for Teaching Logical Inference to seq2seq Models", "abstract": "Machine learning models such as Transformers or LSTMs struggle with tasks that are compositional in nature such as those involving reasoning/inference. Although many datasets exist to evaluate compositional generalization, when it comes to evaluating inference abilities, options are more limited. This paper presents LogicInference, a new dataset to evaluate the ability of models to perform logical inference. The dataset focuses on inference using propositional logic and a small subset of first-order logic, represented both in semi-formal logical notation, as well as in natural language. We also report initial results using a collection of machine learning models to establish an initial baseline in this dataset."}}
{"id": "SaRbsVSaaxr", "cdate": 1635535597416, "mdate": null, "content": {"title": "ETC: Encoding Long and Structured Inputs in Transformers", "abstract": "Transformer models have advanced the state\nof the art in many Natural Language Processing (NLP) tasks. In this paper, we\npresent a new Transformer architecture, Extended Transformer Construction (ETC), that\naddresses two key challenges of standard\nTransformer architectures, namely scaling input length and encoding structured inputs. To\nscale attention to longer inputs, we introduce\na novel global-local attention mechanism between global tokens and regular input tokens.\nWe also show that combining global-local attention with relative position encodings and\na Contrastive Predictive Coding (CPC) pretraining objective allows ETC to encode structured inputs. We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs."}}
{"id": "H4ph4BG-Hfs", "cdate": 1635535329337, "mdate": null, "content": {"title": "Big Bird: Transformers for Longer Sequences", "abstract": "Transformers-based models, such as BERT, have been one of the most successful\ndeep learning models for NLP. Unfortunately, one of their core limitations is the\nquadratic dependency (mainly in terms of memory) on the sequence length due to\ntheir full attention mechanism. To remedy this, we propose, BIGBIRD, a sparse\nattention mechanism that reduces this quadratic dependency to linear. We show\nthat BIGBIRD is a universal approximator of sequence functions and is Turing\ncomplete, thereby preserving these properties of the quadratic, full attention model.\nAlong the way, our theoretical analysis reveals some of the benefits of having\nO(1) global tokens (such as CLS), that attend to the entire sequence as part of the\nsparse attention mechanism. The proposed sparse attention can handle sequences\nof length up to 8x of what was previously possible using similar hardware. As\na consequence of the capability to handle longer context, BIGBIRD drastically\nimproves performance on various NLP tasks such as question answering and\nsummarization. We also propose novel applications to genomics data."}}
{"id": "Rh3khfuQUYk", "cdate": 1632875667809, "mdate": null, "content": {"title": "Iterative Decoding for Compositional Generalization in Transformers", "abstract": "Deep learning models do well at generalizing to in-distribution data but struggle to generalize compositionally, i.e., to combine a set of learned primitives to solve more complex tasks. In particular, in sequence-to-sequence (seq2seq) learning, transformers are often unable to predict even marginally longer examples than those seen during training. This paper introduces iterative decoding, an alternative to seq2seq learning that (i) improves transformer compositional generalization and (ii) evidences that, in general, seq2seq transformers do not learn iterations that are not unrolled. Inspired by the idea of compositionality---that complex tasks can be solved by composing basic primitives---training examples are broken down into a sequence of intermediate steps that the transformer then learns iteratively. At inference time, the intermediate outputs are fed back to the transformer as intermediate inputs until an end-of-iteration token is predicted. Through numerical experiments, we show that transfomers trained via iterative decoding outperform their seq2seq counterparts on the PCFG dataset, and solve the problem of calculating Cartesian products between vectors longer than those seen during training with 100% accuracy, a task at which seq2seq models have been shown to fail. We also illustrate a limitation of iterative decoding, specifically, that it can make sorting harder to learn on the CFQ dataset."}}
{"id": "1OQ90khuUGZ", "cdate": 1601308206052, "mdate": null, "content": {"title": "Action Guidance: Getting the Best of Sparse Rewards and Shaped Rewards for Real-time Strategy Games", "abstract": "Training agents using Reinforcement Learning in games with sparse rewards is a challenging problem, since large amounts of exploration are required to retrieve even the first reward. To tackle this problem, a common approach is to use reward shaping to help exploration. However, an important drawback of reward shaping is that agents sometimes learn to optimize the shaped reward instead of the true objective. In this paper, we present a novel technique that we call action guidance that successfully trains agents to eventually optimize the true objective in games with sparse rewards while maintaining most of the sample efficiency that comes with reward shaping. We evaluate our approach in a simplified real-time strategy (RTS) game simulator called $\\mu$RTS. "}}
{"id": "F_NXeXFAmiD9", "cdate": 1577836800000, "mdate": null, "content": {"title": "An Overview of Distance and Similarity Functions for Structured Data.", "abstract": "The notions of distance and similarity play a key role in many machine learning approaches, and artificial intelligence (AI) in general, since they can serve as an organizing principle by which individuals classify objects, form concepts and make generalizations. While distance functions for propositional representations have been thoroughly studied, work on distance functions for structured representations, such as graphs, frames or logical clauses, has been carried out in different communities and is much less understood. Specifically, a significant amount of work that requires the use of a distance or similarity function for structured representations of data usually employs ad-hoc functions for specific applications. Therefore, the goal of this paper is to provide an overview of this work to identify connections between the work carried out in different areas and point out directions for future work."}}
{"id": "3g8jZkG_NI", "cdate": 1577836800000, "mdate": null, "content": {"title": "ETC: Encoding Long and Structured Data in Transformers.", "abstract": "Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks. In this paper, we present a new Transformer architecture, Extended Transformer Construction (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs. To scale attention to longer inputs, we introduce a novel global-local attention mechanism between global tokens and regular input tokens. We also show that combining global-local attention with relative position encodings and a Contrastive Predictive Coding (CPC) pre-training objective allows ETC to encode structured inputs. We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs."}}
{"id": "qku4G6HdB2QO", "cdate": 1546300800000, "mdate": null, "content": {"title": "Experience Management in Multi-player Games.", "abstract": "Experience Management studies AI systems that automatically adapt interactive experiences such as games to tailor to specific players and to fulfill design goals. Although it has been explored for several decades, existing work in experience management has mostly focused on single-player experiences. This paper is a first attempt at identifying the main challenges to expand EM to multi-player/multi-user games or experiences. We also make connections to related areas where solutions for similar problems have been proposed (especially group recommender systems) and discusses the potential impact and applications of multi-player EM."}}
{"id": "dG6ir4h_TYoV", "cdate": 1546300800000, "mdate": null, "content": {"title": "RTS AI Problems and Techniques.", "abstract": "AI; Artificial intelligence; Game AI; Real-time strategy games; RTS games\nReal-time strategy (RTS) games is a subgenre of strategy games where players need to build an economy (gathering resources and building a base) and military power (training units and researching technologies) in order to defeat their opponents (destroying their army and base). Artificial intelligence problems related to RTS games deal with the behavior of an artificial player. This consists among others to learn how to play, to have an understanding about the game and its environment, and to predict and infer game situations from a context and sparse information.\nThe field of real-time strategy (RTS) game artificial intelligence (AI) has advanced significantly in the past few years, partially thanks to competitions such as the \u201cORTS RTS Game AI Competition\u201d (held from 2006 to 2009), the \u201cAIIDE StarCraft AI Competition\u201d (held since 2010), and the \u201cCIG StarCraft RTS AI Competition\u201d..."}}
{"id": "YE-i88sx5ivj", "cdate": 1546300800000, "mdate": null, "content": {"title": "Programming in game space: how to represent parallel programming concepts in an educational game.", "abstract": "Concurrent and parallel programming (CPP) skills are increasingly important in today's world of parallel hardware. However, the conceptual leap from deterministic sequential programming to CPP is notoriously challenging to make. Our educational game Parallel is designed to support the learning of CPP core concepts through a game-based learning approach, focusing on the connection between gameplay and CPP. Through a 10-week user study (n 25) in an undergraduate concurrent programming course, the first empirical study for a CPP educational game, our results show that Parallel offers both CPP knowledge and student engagement. Furthermore, we provide a new framework to describe the design space for programming games in general."}}
