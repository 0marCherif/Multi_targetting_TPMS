{"id": "SudRJjtGRz", "cdate": 1664928780874, "mdate": null, "content": {"title": "An Invariant Learning Characterization of Controlled Text Generation", "abstract": "Controlled generation refers to the problem of creating text that contains stylistic or semantic attributes of interest. \nMany approaches reduce this problem to building a predictor of the desired attribute.\nFor example, researchers hoping to deploy a large language model to produce non-toxic content may use a toxicity classifier to filter generated text. \nIn this paper, we show that the performance of controlled generation may be poor if the target distribution of text differs from the distribution the predictor was trained on. \nInstead, we take inspiration from causal representation learning and cast controlled generation under distribution shift as an invariant learning problem: the most effective predictor should be invariant across multiple text environments. Experiments demonstrate the promise and difficulty of adapting invariant learning methods, which have been primarily developed for vision, to text."}}
{"id": "zInaytkuzX", "cdate": 1664833378623, "mdate": null, "content": {"title": "An Invariant Learning Characterization of Controlled Text Generation", "abstract": "Controlled generation refers to the problem of creating text that contains stylistic or semantic attributes of interest. \nMany approaches reduce this problem to building a predictor of the desired attribute.\nFor example, researchers hoping to deploy a large language model to produce non-toxic content may use a toxicity classifier to filter generated text. \nIn this paper, we show that the performance of controlled generation may be poor if the target distribution of text differs from the distribution the predictor was trained on. \nInstead, we take inspiration from causal representation learning and cast controlled generation under distribution shift as an invariant learning problem: the most effective predictor should be invariant across multiple text environments. Experiments demonstrate the promise and difficulty of adapting invariant learning methods, which have been primarily developed for vision, to text."}}
{"id": "GzP0jvzVQB0", "cdate": 1649734631936, "mdate": 1649734631936, "content": {"title": "Conformal Sensitivity Analysis for Individual Treatment Effects", "abstract": "Estimating an individual treatment effect (ITE) is essential to personalized decision-making. However, existing methods for estimating the ITE often rely on unconfoundedness, an assumption that is fundamentally untestable with observed data. To this end, this paper proposes a method for sensitivity analysis of the ITE, a way to estimate a range of the ITE under unobserved confounding. The method we develop quantifies unmeasured confounding through a marginal sensitivity model and then adapts the framework of conformal inference to estimate an ITE interval at a given confounding strength. In particular, we formulate this sensitivity analysis problem as one of conformal inference under distribution shift, and we extend existing methods of covariate-shifted conformal inference to this more general setting. The result is a predictive interval that has guaranteed nominal coverage of the ITE, a method that provides coverage with distribution-free and nonasymptotic guarantees. We evaluate the method on synthetic data and illustrate its application in an observational study."}}
{"id": "SkeOIVBeUH", "cdate": 1567802447567, "mdate": null, "content": {"title": "Adapting Neural Networks for the Estimation of Treatment Effects", "abstract": "This paper addresses the use of neural networks for the estimation of treatment effects from observational data. Generally, estimation proceeds in two stages. First, we \ufb01t models for the expected outcome and the probability of treatment (propensity score). Second, we plug these \ufb01tted models into a downstream estimator. Neural networks are a natural choice for the models in the \ufb01rst step. Our question is: how can we adapt the design and training of the neural networks used in this \ufb01rst step in order to improve the quality of the \ufb01nal estimate of the treatment effect? We propose two adaptations based on insights from the statistical literature on the estimation of treatment effects. The \ufb01rst is a new architecture, the Dragonnet, that exploits the suf\ufb01ciency of the propensity score for estimation adjustment. The second is a regularization procedure, targeted regularization, that induces a bias towards models that have non-parametrically optimal asymptotic properties \u2018out-of-the-box\u2019. Studies on benchmark datasets for causal inference show these adaptations outperform existing methods."}}
