{"id": "nXzI5Xa9uLn", "cdate": 1684157835268, "mdate": 1684157835268, "content": {"title": "Least Squares Estimation of Weakly Convex Functions", "abstract": "Function estimation under shape restrictions, such as convexity, has many practical applications and has drawn a lot of recent interests. In this work we argue that convexity, as a global property, is too strict and prone to outliers. Instead, we propose to use weakly convex functions as a simple alternative to quantify \"approximate convexity\"---a notion that is perhaps more relevant in practice. We prove that, unlike convex functions, weakly convex functions can exactly interpolate any finite dataset and they are universal approximators. Through regularization using the modulus of convexity, we show that weakly convex functions can be efficiently estimated both statistically and algorithmically, requiring minimal modifications to existing algorithms and theory for estimating convex functions. Our numerical experiments confirm the class of weakly convex functions as a new competitive alternative for nonparametric estimation."}}
{"id": "9sW_kx0Ngmg", "cdate": 1683913962744, "mdate": 1683913962744, "content": {"title": "Revisiting flow generative models for Out-of-distribution detection", "abstract": "Deep generative models have been widely used in practical applications such as the detection of out-of-distribution (OOD) data. In this work,  we aim to re-examine the potential of generative flow models in OOD detection. We first propose a simple combination of univariate one-sample statistical test (e.g., Kolmogorov-Smirnov) and random projections in the latent space of flow models to perform OOD detection.  Then, we propose a two-sample version of our test to account for imperfect flow models. Quite distinctly, our method does not pose parametric assumptions on OOD data and is capable of exploiting any flow model. Experimentally, firstly we confirm the efficacy of our method against state-of-the-art baselines through extensive experiments on several image datasets; secondly we investigate the relationship between model accuracy (e.g., the generation quality) and the OOD detection performance, and found surprisingly that they are not always positively correlated; and thirdly we show that detection in the latent space of flow models generally outperforms detection in the sample space across various OOD datasets, hence highlighting the benefits of training a flow model."}}
{"id": "xqe64BxA9B", "cdate": 1683901251827, "mdate": 1683901251827, "content": {"title": "Symmetric Wasserstein Autoencoders", "abstract": "Leveraging the framework of Optimal Transport, we introduce a new family of generative autoencoders with a learnable prior, called Symmetric Wasserstein Autoencoders (SWAEs). We propose to symmetrically match the joint distributions of the observed data and the latent representation induced by the encoder and the decoder. The resultant algorithm jointly optimizes the modelling losses in both the data and the latent spaces with  the loss in the data space leading to the denoising effect. With the symmetric treatment of the data and the latent representation, the algorithm implicitly preserves the local structure of the data in the latent space. To further improve the latent representation, we incorporate a reconstruction loss into the objective, which significantly benefits both the generation and reconstruction. We empirically show the superior performance of SWAEs over several state-of-the-art generative autoencoders in terms of classification, reconstruction, and generation."}}
{"id": "N14DJ7L6be", "cdate": 1683899713581, "mdate": 1683899713581, "content": {"title": "SoftEdge: Combating Under-Fitting with Connectivity-Preserving Graph Augmentation ", "abstract": "Augmented graphs play a vital role in regularizing Graph Neural Networks (GNNs), which leverage information exchange along edges in  graphs,  in the form of message passing, for learning.   Due to their   effectiveness,  simple edge  and node manipulations (e.g., addition and deletion)  have been widely used in graph augmentation. Nevertheless, such  common augmentation techniques can  dramatically change the semantics of the original graph, causing overaggressive  augmentation and thus  under-fitting in the GNN learning. To address this problem, we propose  SoftEdge, which  assigns random  weights to a portion of the edges of a given graph, for graph augmentation. The synthetic graph generated by  SoftEdge maintains the same nodes and their connectivities as the original graph, thus mitigating the semantics changes of the original graph.  We empirically   show that  this simple method obtains  superior  accuracy to popular  node and edge manipulation approaches and notable resilience to the accuracy degradation with the  GNN depth."}}
{"id": "_OWsTYC7UgP", "cdate": 1683899405770, "mdate": 1683899405770, "content": {"title": "DP-LFlow: Differentially private latent flow for scalable sensitive image generation", "abstract": "Privacy concerns grow with the success of modern deep learning models, especially when the training set contains sensitive data. Differentially private generative model (DPGM) can serve as a solution to circumvent such concerns by generating data that are distributionally similar to the original data yet with differential privacy (DP) guarantees. While differentially private stochastic gradient descent (DP-SGD) is currently the leading algorithm for training a private deep learning model,  the norm of the stochastic noise introduced to the model by DP-SGD increases linearly with model dimensions, which is likely to spoil the model utility (particularly so for strong DP guarantees). Among various generative models, the flow generative model is exceptional for its capability of \\emph{exact} density estimation, and DP-flow in high dimensional space is rarely explored, thus it is of interest in this work. We will first show that it is challenging (or even infeasible) to train a DP-flow on image sets with acceptable utility, and then give an effective solution by reducing the generation from the pixel space to a lower dimensional latent space. We show the effectiveness and scalability of the proposed method via extensive experiments. Notably, our method is scalable to high-resolution image datasets, which is rarely studied in related works."}}
{"id": "Mtv25_UGTe", "cdate": 1650287995350, "mdate": 1650287995350, "content": {"title": "f-Mutual Information Contrastive Learning", "abstract": "Self-supervised contrastive learning is an emerging field due to its power in providing good data representations. Such learning paradigm widely adopts the InfoNCE loss, which is closely connected with maximizing the mutual information. In this work, we propose the -Mutual Information Contrastive Learning framework (-MICL) , which directly maximizes the -divergence-based generalization of mutual information. We theoretically prove that, under mild assumptions, our -MICL naturally attains the alignment for positive pairs and the uniformity for data representations, the two main factors for the success of contrastive learning.  We further provide theoretical guidance on designing the similarity function and choosing the effective -divergences for -MICL. Using several benchmark tasks from both vision and natural text, we empirically verify that our novel method outperforms or performs on par with state-of-the-art strategies."}}
{"id": "6y2KBh-0Fd9", "cdate": 1632875654708, "mdate": null, "content": {"title": "Revisiting flow generative models for Out-of-distribution detection", "abstract": "Deep generative models have been widely used in practical applications such as the detection of out-of-distribution (OOD) data. In this work,  we aim to re-examine the potential of generative flow models in OOD detection. We first propose a simple combination of univariate one-sample statistical test (e.g., Kolmogorov-Smirnov) and random projections in the latent space of flow models to perform OOD detection.  Then, we propose a two-sample version of our test to account for imperfect flow models. Quite distinctly, our method does not pose parametric assumptions on OOD data and is capable of exploiting any flow model. Experimentally, firstly we confirm the efficacy of our method against state-of-the-art baselines through extensive experiments on several image datasets; secondly we investigate the relationship between model accuracy (e.g., the generation quality) and the OOD detection performance, and found surprisingly that they are not always positively correlated; and thirdly we show that detection in the latent space of flow models generally outperforms detection in the sample space across various OOD datasets, hence highlighting the benefits of training a flow model."}}
{"id": "3kTt_W1_tgw", "cdate": 1632875499050, "mdate": null, "content": {"title": "$f$-Mutual Information Contrastive Learning", "abstract": "Self-supervised contrastive learning is an emerging field due to its power in providing good data representations. Such learning paradigm widely adopts the InfoNCE loss, which is closely connected with maximizing the mutual information. In this work, we propose the $f$-Mutual Information Contrastive Learning framework ($f$-MICL) , which directly maximizes the $f$-divergence-based generalization of mutual information. We theoretically prove that, under mild assumptions, our $f$-MICL naturally attains the alignment for positive pairs and the uniformity for data representations, the two main factors for the success of contrastive learning.  We further provide theoretical guidance on designing the similarity function and choosing the effective $f$-divergences for $f$-MICL. Using several benchmark tasks from both vision and natural text, we empirically verify that our novel method outperforms or performs on par with state-of-the-art strategies."}}
{"id": "tckGH8K9y6o", "cdate": 1601308114332, "mdate": null, "content": {"title": "Symmetric Wasserstein Autoencoders", "abstract": "Leveraging the framework of Optimal Transport, we introduce a new family of generative autoencoders with a learnable prior, called Symmetric Wasserstein Autoencoders (SWAEs). We propose to symmetrically match the joint distributions of the observed data and the latent representation induced by the encoder and the decoder. The resultant algorithm jointly optimizes the modelling losses in both the data and the latent spaces with  the loss in the data space leading to the denoising effect. With the symmetric treatment of the data and the latent representation, the algorithm implicitly preserves the local structure of the data in the latent space. To further improve the latent representation, we incorporate a reconstruction loss into the objective, which significantly benefits both the generation and reconstruction. We empirically show the superior performance of SWAEs over several state-of-the-art generative autoencoders in terms of classification, reconstruction, and generation."}}
