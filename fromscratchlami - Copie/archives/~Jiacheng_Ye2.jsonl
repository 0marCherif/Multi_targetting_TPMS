{"id": "buoL9Vd9VY", "cdate": 1672017938555, "mdate": 1672017938555, "content": {"title": "Self-adaptive In-context Learning", "abstract": "Despite the surprising few-shot performance of in-context learning (ICL), it is still a common practice to randomly sample examples to serve as context. This paper advocates a new principle for ICL: self-adaptive in-context learning. The self-adaption mechanism is introduced to help each sample find an in-context example permutation (i.e., selection and ordering) that can derive the correct prediction, thus maximizing performance. To validate the effectiveness of self-adaptive ICL, we propose a general select-then-rank framework and instantiate it with new selection and ranking algorithms. Upon extensive evaluation on eight different NLP datasets, our self-adaptive ICL method achieves a 40% relative improvement over the common practice setting. Further analysis reveals the enormous potential of self-adaptive ICL that it might be able to close the gap between ICL and finetuning given more advanced algorithms. Our code is released to facilitate future research in this area: https://github.com/ Shark-NLP/self-adaptive-ICL"}}
{"id": "BGZmoPuUovM", "cdate": 1664500778913, "mdate": 1664500778913, "content": {"title": "ZEROGEN: Efficient Zero-shot Learning via Dataset Generation", "abstract": "There is a growing interest in dataset generation recently due to the superior generative\ncapacity of large pre-trained language models\n(PLMs). In this paper, we study a flexible\nand efficient zero-short learning method, ZEROGEN. Given a zero-shot task, we first\ngenerate a dataset from scratch using PLMs\nin an unsupervised manner. Then, we train\na tiny task model (e.g., LSTM) under the\nsupervision of the synthesized dataset. This\napproach allows highly efficient inference as\nthe final task model only has orders of magnitude fewer parameters comparing to PLMs\n(e.g., GPT2-XL). Apart from being annotationfree and efficient, we argue that ZEROGEN can\nalso provide useful insights from the perspective of data-free model-agnostic knowledge\ndistillation, and unreferenced text generation\nevaluation. Experiments and analysis on different NLP tasks, namely, text classification,\nquestion answering, and natural language inference), show the effectiveness of ZEROGEN."}}
{"id": "h5OpjGd_lo6", "cdate": 1663850127018, "mdate": null, "content": {"title": "Self-Guided Noise-Free Data Generation for Efficient Zero-Shot Learning", "abstract": "There is a rising interest in further exploring the zero-shot learning potential of large pre-trained language models (PLMs). A new paradigm called data-generation-based zero-shot learning has achieved impressive success. In this paradigm, the synthesized data from the PLM acts as the carrier of knowledge, which is used to train a task-specific model with orders of magnitude fewer parameters than the PLM, achieving both higher performance and efficiency than prompt-based zero-shot learning methods on PLMs. The main hurdle of this approach is that the synthesized data from PLM usually contains a significant portion of low-quality samples. Fitting on such data will greatly hamper the performance of the task-specific model, making it unreliable for deployment. Previous methods remedy this issue mainly by filtering synthetic data using heuristic metrics(e.g., output confidence), or refining the data with the help of a human expert, which comes with excessive manual tuning or expensive costs. In this paper, we propose a novel noise-robust re-weighting framework SunGen to automatically construct high-quality data for zero-shot classification problems. Our framework features the ability to learn the sample weights indicating data quality without requiring any human annotation. We theoretically and empirically verify the ability of our method to help construct good-quality synthetic datasets. Notably, SunGen-LSTM yields a 9.8% relative improvement than the baseline on average accuracy across eight different established text classification tasks."}}
