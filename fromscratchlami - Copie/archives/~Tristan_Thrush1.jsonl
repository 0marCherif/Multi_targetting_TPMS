{"id": "FAj56B1REj", "cdate": 1672531200000, "mdate": 1679938899085, "content": {"title": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset", "abstract": ""}}
{"id": "UoEw6KigkUn", "cdate": 1654508526458, "mdate": null, "content": {"title": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset", "abstract": "As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing, especially in multilingual settings. The BigScience workshop, a 1-year international and multidisciplinary initiative, was formed with the goal of researching and training large language models as a values-driven undertaking, putting issues of ethics, harm, and governance in the foreground. This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience Large Open-science Open-access Multilingual (BLOOM) language model. We further release a large initial subset of the corpus and analyses thereof, and hope to empower large-scale monolingual and multilingual modeling projects with both the data and the processing tools, as well as stimulate research around this large multilingual corpus."}}
{"id": "xJJOF-9RC7", "cdate": 1640995200000, "mdate": 1684238771518, "content": {"title": "Hatemoji: A Test Suite and Adversarially-Generated Dataset for Benchmarking and Detecting Emoji-Based Hate", "abstract": "Hannah Kirk, Bertie Vidgen, Paul Rottger, Tristan Thrush, Scott Hale. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022."}}
{"id": "w42XySDJjNG", "cdate": 1640995200000, "mdate": 1683914133462, "content": {"title": "DataPerf: Benchmarks for Data-Centric AI Development", "abstract": "Machine learning research has long focused on models rather than datasets, and prominent datasets are used for common ML tasks without regard to the breadth, difficulty, and faithfulness of the underlying problems. Neglecting the fundamental importance of data has given rise to inaccuracy, bias, and fragility in real-world applications, and research is hindered by saturation across existing dataset benchmarks. In response, we present DataPerf, a community-led benchmark suite for evaluating ML datasets and data-centric algorithms. We aim to foster innovation in data-centric AI through competition, comparability, and reproducibility. We enable the ML community to iterate on datasets, instead of just architectures, and we provide an open, online platform with multiple rounds of challenges to support this iterative development. The first iteration of DataPerf contains five benchmarks covering a wide spectrum of data-centric techniques, tasks, and modalities in vision, speech, acquisition, debugging, and diffusion prompting, and we support hosting new contributed benchmarks from the community. The benchmarks, online evaluation platform, and baseline implementations are open source, and the MLCommons Association will maintain DataPerf to ensure long-term benefits to academia and industry."}}
{"id": "sStdSlP0XJ7", "cdate": 1640995200000, "mdate": 1681512762637, "content": {"title": "Dynatask: A Framework for Creating Dynamic AI Benchmark Tasks", "abstract": ""}}
{"id": "kNadkYsy94-", "cdate": 1640995200000, "mdate": 1681512762637, "content": {"title": "Models in the Loop: Aiding Crowdworkers with Generative Annotation Assistants", "abstract": ""}}
{"id": "g9hZ_GX4zcs", "cdate": 1640995200000, "mdate": 1682537579444, "content": {"title": "Evaluate & Evaluation on the Hub: Better Best Practices for Data and Model Measurements", "abstract": "Leandro Von Werra, Lewis Tunstall, Abhishek Thakur, Sasha Luccioni, Tristan Thrush, Aleksandra Piktus, Felix Marty, Nazneen Rajani, Victor Mustar, Helen Ngo. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2022."}}
{"id": "fEhvr3Xq6E9", "cdate": 1640995200000, "mdate": 1681512762632, "content": {"title": "Measuring Data", "abstract": ""}}
{"id": "ei1tSeL2Vo", "cdate": 1640995200000, "mdate": 1681512762631, "content": {"title": "Evaluate & Evaluation on the Hub: Better Best Practices for Data and Model Measurements", "abstract": ""}}
{"id": "acTBqiwLvJw", "cdate": 1640995200000, "mdate": 1650865337436, "content": {"title": "Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality", "abstract": "We present a novel task and dataset for evaluating the ability of vision and language models to conduct visio-linguistic compositional reasoning, which we call Winoground. Given two images and two captions, the goal is to match them correctly - but crucially, both captions contain a completely identical set of words, only in a different order. The dataset was carefully hand-curated by expert annotators and is labeled with a rich set of fine-grained tags to assist in analyzing model performance. We probe a diverse range of state-of-the-art vision and language models and find that, surprisingly, none of them do much better than chance. Evidently, these models are not as skilled at visio-linguistic compositional reasoning as we might have hoped. We perform an extensive analysis to obtain insights into how future work might try to mitigate these models' shortcomings. We aim for Winoground to serve as a useful evaluation set for advancing the state of the art and driving further progress in the field. The dataset is available at https://huggingface.co/datasets/facebook/winoground."}}
