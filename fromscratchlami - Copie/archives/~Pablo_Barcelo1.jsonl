{"id": "dAbFRJJLBgP", "cdate": 1672531200000, "mdate": 1681565118883, "content": {"title": "Three iterations of (d-1)-WL test distinguish non isometric clouds of d-dimensional points", "abstract": ""}}
{"id": "NT4Q5t8DIp-", "cdate": 1672531200000, "mdate": 1681565118881, "content": {"title": "A Theory of Link Prediction via Relational Weisfeiler-Leman", "abstract": ""}}
{"id": "wY_IYhh6pqj", "cdate": 1662812636734, "mdate": null, "content": {"title": "Weisfeiler and Leman Go Relational", "abstract": "Knowledge graphs, modeling multi-relational data, improve numerous applications such as question answering or graph logical reasoning. Many graph neural networks for such data emerged recently, often outperforming shallow architectures. However, the design of such multi-relational graph neural networks is ad-hoc, driven mainly by intuition and empirical insights. Up to now, their expressivity, their relation to each other, and their (practical) learning performance is poorly understood. Here, we initiate the study of deriving a more principled understanding of multi-relational graph neural networks. Namely, we investigate the limitations in the expressive power of the well-known Relational GCN and Compositional GCN architectures and shed some light on their practical learning performance. By aligning both architectures with a suitable version of the Weisfeiler-Leman test, we establish under which conditions both models have the same expressive power in distinguishing non-isomorphic (multi-relational) graphs or vertices with different structural roles. Further, by leveraging recent progress in designing expressive graph neural networks, we introduce the $k$-RN  architecture that provably overcomes the expressiveness limitations of the above two architectures. Empirically, we confirm our theoretical findings in a vertex classification setting over small and large multi-relational graphs."}}
{"id": "zD65Zdh6ZhI", "cdate": 1652737819976, "mdate": null, "content": {"title": "On Computing Probabilistic Explanations for Decision Trees", "abstract": "  Formal XAI (explainable AI) is a growing area that focuses on computing explanations with mathematical guarantees for the decisions made by ML models. Inside formal XAI, one of the most studied cases is that of explaining the choices taken by decision trees, as they are traditionally deemed as one of the most interpretable classes of models. Recent work has focused on studying the computation of sufficient reasons, a kind of explanation in which given a decision tree $T$ and an instance $x$, one explains the decision $T(x)$ by providing a subset $y$ of the features of $x$ such that for any other instance $z$ compatible with $y$, it holds that  $T(z) = T(x)$, intuitively meaning that the features in $y$ are already enough to fully justify the classification of $x$ by $T$. \nIt has been argued, however, that sufficient reasons constitute a restrictive notion of explanation. For such a reason, the community has started to study their probabilistic counterpart, in which one requires that the probability of $T(z) = T(x)$ must be at least some value $\\delta \\in (0, 1]$, where $z$ is a random instance that is compatible with $y$. Our paper settles the computational complexity of $\\delta$-sufficient-reasons over decision trees, showing that both (1) finding $\\delta$-sufficient-reasons  that are minimal in size, and (2) finding $\\delta$-sufficient-reasons that are minimal inclusion-wise, do not admit polynomial-time algorithms (unless P = NP).\n   This is in stark contrast with the deterministic case ($\\delta = 1$) where inclusion-wise minimal sufficient-reasons are easy to compute. By doing this, we answer two open problems originally raised by Izza et al., and extend the hardness of explanations for Boolean circuits presented by W{\\\"a}ldchen et al. to the more restricted case of decision trees. On the positive side, we identify structural restrictions of decision trees that make the problem tractable, and show how SAT solvers might be able to tackle these problems in practical settings."}}
{"id": "qaQbt9fAC2p", "cdate": 1640995200000, "mdate": 1681565118900, "content": {"title": "On Computing Probabilistic Explanations for Decision Trees", "abstract": ""}}
{"id": "nPqJFmGIXv", "cdate": 1640995200000, "mdate": 1681565118888, "content": {"title": "Weisfeiler and Leman Go Relational", "abstract": ""}}
{"id": "aNUOxRlP63C", "cdate": 1640995200000, "mdate": 1681565118888, "content": {"title": "On the expressiveness of Lara: A proposal for unifying linear and relational algebra", "abstract": ""}}
{"id": "9E-AARr69d", "cdate": 1640995200000, "mdate": 1681565119036, "content": {"title": "Weisfeiler and Leman Go Relational", "abstract": ""}}
{"id": "07RBzd91SM", "cdate": 1640995200000, "mdate": 1681565119024, "content": {"title": "No Agreement Without Loss: Learning and Social Choice in Peer Review", "abstract": ""}}
{"id": "yGKklt8wyV", "cdate": 1621630225550, "mdate": null, "content": {"title": "Graph Neural Networks with Local Graph  Parameters", "abstract": "Various recent proposals increase the distinguishing power of Graph Neural Networks (GNNs) by propagating features between k-tuples of vertices. The distinguishing power of these \u201chigher-order\u201d GNNs is known to be bounded by the k-dimensional Weisfeiler-Leman (WL) test, yet their O(n^k) memory requirements limit their applicability. Other proposals infuse GNNs with local higher-order graph structural information from the start, hereby inheriting the desirable O(n) memory requirement from GNNs at the cost of a one-time, possibly non-linear, preprocessing step. We propose local graph parameter enabled GNNs as a framework for studying the latter\u00a0kind of approaches and precisely characterize their distinguishing power, in terms of a variant of the WL test, and in terms of the graph structural properties that they can take into account. Local graph parameters can be added to any GNN\u00a0architecture, and are cheap to compute. In terms of expressive power, our proposal lies in the middle of GNNs and their higher-order counterparts. Further, we propose\u00a0several techniques to aide in choosing the right local graph parameters. Our results\u00a0connect GNNs with deep results in finite model theory and finite variable logics. Our experimental evaluation shows that adding local graph parameters often has a\u00a0positive effect for a variety of GNNs, datasets and graph learning tasks. \n"}}
