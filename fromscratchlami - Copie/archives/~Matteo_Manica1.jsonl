{"id": "m81R33gpUH", "cdate": 1680307200000, "mdate": 1699258927685, "content": {"title": "Regression Transformer enables concurrent sequence regression and generation for molecular language modelling", "abstract": "Transformer models are gaining increasing popularity in modelling natural language as they can produce human-sounding text by iteratively predicting the next word in a sentence. Born and Manica apply the idea of Transformer-based text completion to property prediction of chemical compounds by providing the context of a problem and having the model complete the missing information."}}
{"id": "iizdwurAcSw", "cdate": 1672531200000, "mdate": 1680014171019, "content": {"title": "Unifying Molecular and Textual Representations via Multi-task Language Modelling", "abstract": ""}}
{"id": "e6BV2VXI-3n", "cdate": 1672531200000, "mdate": 1695368868807, "content": {"title": "Unifying Molecular and Textual Representations via Multi-task Language Modelling", "abstract": "The recent advances in neural language models have also been successfully applied to the field of chemistry, offering generative solutions for classical problems in molecular design and synthesis p..."}}
{"id": "SGWImdxQlbN", "cdate": 1672531200000, "mdate": 1680014171008, "content": {"title": "Domain-agnostic and Multi-level Evaluation of Generative Models", "abstract": ""}}
{"id": "dLHtwZKvJmE", "cdate": 1654124927412, "mdate": null, "content": {"title": "PGT: a prompt based generative transformer for the patent domain", "abstract": "Patents are a valuable source of knowledge, but drafting them is a time-consuming and expensive task.\nMethods that assist patent generation can provide a two-fold improvement as they can speed up the generation process and\nsuggest to the inventor ideas and claims.\nHerein, influenced by recent advances in language modeling via multitask learning and prompt engineering, we present Patent Generative Transformer (PGT), a transformer-based language model trained to facilitate patent drafting.\nSpecifically, the model supports three tasks: part-of-patent generation, text infilling, and patent coherence evaluation.\nPGT complements inventors and assures the fast and successful transition from their input to a coherent patent disclosure taking advantage of its multitasking nature.\nWe show how the model outperforms a collection of task-specific baselines on relevant metrics. We further test the quality of the generated text via blind testing by subject matter experts.\nFinally, we explore a zero-shot extension of the model showing how to use PGT for generating domain-specific abstracts."}}
{"id": "Ag3OAUEH9rZ", "cdate": 1648731965364, "mdate": null, "content": {"title": "Regression Transformer: Concurrent Conditional Generation and Regression by Blending Numerical and Textual Tokens ", "abstract": "We report the Regression Transformer (RT), a method that abstracts regression as a conditional sequence modeling problem. \nThe RT casts continuous properties as sequences of numerical tokens and encodes them jointly with conventional tokens.\nThis yields a dichotomous model that concurrently excels at regression tasks and property-driven conditional generation.\ncan seamlessly transition between solving regression tasks and conditional generation tasks; solely governed by the mask location. \nWe propose several extensions to the XLNet objective and adopt an alternating training scheme to concurrently optimize property prediction and conditional text generation with on a self-consistency loss.\nOur experiments on both chemical and protein languages demonstrate that the performance of traditional regression models can be surpassed despite training with cross entropy loss.\nImportantly, priming the same model with continuous properties yields a highly competitive conditional generative models that outperforms specialized approaches in a constrained property optimization benchmark.\nIn sum, the Regression Transformer opens the door for \"swiss army knife\" models that excel at both regression and conditional generation.\nThis finds application particularly in property-driven, local exploration of the chemical or protein space.\nThe code to reproduce all experiments of the paper is available at: https://anonymous.4open.science/r/regression-transformer/\n"}}
{"id": "xbX8Fv8oF4Q", "cdate": 1640995200000, "mdate": 1662438849601, "content": {"title": "Multitask Prompted Training Enables Zero-Shot Task Generalization", "abstract": "Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models\u2019 pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely unseen tasks specified in natural language. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several datasets, often outperforming models 16\u00d7 its size. Further, our model attains strong performance on a subset of tasks from the BIG-Bench benchmark, outperforming models 6\u00d7 its size. All trained models are available at https://github.com/bigscience-workshop/t-zero, and all prompts are available at https://github.com/bigscience-workshop/promptsource."}}
{"id": "wBFoLfg9o4g", "cdate": 1640995200000, "mdate": 1680014171124, "content": {"title": "Z-BERT-A: a zero-shot Pipeline for Unknown Intent detection", "abstract": ""}}
{"id": "vGOxGF0EAi", "cdate": 1640995200000, "mdate": 1680014171015, "content": {"title": "PCfun: a hybrid computational framework for systematic characterization of protein complex function", "abstract": ""}}
{"id": "ormtr90D1Pk", "cdate": 1640995200000, "mdate": 1680014171016, "content": {"title": "On the Choice of Active Site Sequences for Kinase-Ligand Affinity Prediction", "abstract": ""}}
