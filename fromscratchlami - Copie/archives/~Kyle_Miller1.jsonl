{"id": "-cGqWOL24ty", "cdate": 1694631549468, "mdate": 1694631549468, "content": {"title": "Actionable Model-Centric Explanations", "abstract": "We recommend using a model-centric, Boolean Satisfiability (SAT) formalism to obtain useful explanations of trained model behavior, different and complementary to what can be gleaned from LIME and SHAP, popular data-centric explanation tools in Artificial Intelligence (AI).We compare and contrast these methods, and show that data-centric methods may yield brittle explanations of limited practical utility.The model-centric framework, however, can offer actionable insights into risks of using AI models in practice. For critical applications of AI, split-second decision making is best informed by robust explanations that are invariant to properties of data, the capability offered by model-centric frameworks."}}
{"id": "JQ1RLAEn-BO", "cdate": 1632875689547, "mdate": null, "content": {"title": "Kernel Density Decision Trees", "abstract": "We propose kernel density decision trees (KDDTs), a novel fuzzy decision tree (FDT) formalism based on kernel density estimation that achieves state-of-the-art prediction performance often matching or exceeding that of conventional tree ensembles. Ensembles of KDDTs achieve even better generalization. FDTs address the sensitivity and tendency to overfitting of decision trees by representing uncertainty through fuzzy partitions. However, compared to conventional, crisp decision trees, FDTs are generally complex to apply, sensitive to design choices, slow to fit and make predictions, and difficult to interpret. Moreover, finding the optimal threshold for a given fuzzy split is challenging, resulting in methods that discretize data, settle for near-optimal thresholds, or fuzzify crisp trees. Our KDDTs address these shortcomings using a fast algorithm for finding optimal partitions for FDTs with piecewise-linear splitting functions or KDDTs with piecewise-constant fitting kernels. Prediction can take place with or without fuzziness; without it, KDDTs are identical to standard decision trees, but with a more robust fitting algorithm. Using KDDTs simplifies the process of fitting a model, grounds design choices in the well-studied theory of density estimation, supports optional incorporation of expert knowledge about uncertainty in the data, and enables interpretation in the context of kernels. We demonstrate prediction performance against conventional decision trees and tree ensembles on 12 publicly available datasets."}}
{"id": "ERAQ5ZCP9t", "cdate": 1601308267242, "mdate": null, "content": {"title": "Robust Multi-view Representation Learning", "abstract": "Multi-view data has become ubiquitous, especially with multi-sensor systems like self-driving cars or medical patient-side monitors.\n\nWe look at modeling multi-view data through robust representation learning, with the goal of leveraging relationships between views and building resilience to missing information.\nWe propose a new flavor of multi-view AutoEncoders, the Robust Multi-view AutoEncoder, which explicitly encourages robustness to missing views.\nThe principle we use is straightforward: we apply the idea of drop-out to the level of views.\nDuring training, we leave out views as input to our model while forcing it to reconstruct all of them.\nWe also consider a flow-based generative modeling extension of our approach in the case where all the views are available.\n\nWe conduct experiments for different scenarios: directly using the learned representations for reconstruction, as well as a two-step process where the learned representation is subsequently used as features for the data for a down-stream application.\nOur synthetic and real-world experiments show promising results for the application of these models to robust representation learning."}}
{"id": "By-yhuZOWH", "cdate": 1483228800000, "mdate": null, "content": {"title": "Noise-Tolerant Interactive Learning Using Pairwise Comparisons", "abstract": "We study the problem of interactively learning a binary classifier using noisy labeling and pairwise comparison oracles, where the comparison oracle answers which one in the given two instances is more likely to be positive. Learning from such oracles has multiple applications where obtaining direct labels is harder but pairwise comparisons are easier, and the algorithm can leverage both types of oracles. In this paper, we attempt to characterize how the access to an easier comparison oracle helps in improving the label and total query complexity. We show that the comparison oracle reduces the learning problem to that of learning a threshold function. We then present an algorithm that interactively queries the label and comparison oracles and we characterize its query complexity under Tsybakov and adversarial noise conditions for the comparison and labeling oracles. Our lower bounds show that our label and total query complexity is almost optimal."}}
