{"id": "eQeAMCywoVM", "cdate": 1640995200000, "mdate": 1667569406775, "content": {"title": "MAT: Multianchor Visual Tracking With Selective Search Region", "abstract": "The core prerequisite of most modern trackers is a motion assumption, defined as predicting the current location in a limited search region centering at the previous prediction. For clarity, the central subregion of a search region is denoted as the tracking anchor (e.g., the location of the previous prediction in the current frame). However, providing accurate predictions in all frames is very challenging in the complex nature scenes. In addition, the target locations in consecutive frames often change violently under the attribute of fast motion. Both facts are likely to lead the previous prediction to an unbelievable tracking anchor, which will make the aforementioned prerequisite invalid and cause tracking drift. To enhance the reliability of tracking anchors, we propose a real-time multianchor visual tracking mechanism, called multianchor tracking (MAT). Instead of directly relying on the tracking anchor inherited from the previous prediction, MAT selects the best anchor from an anchor ensemble, which includes several objectness-based anchor proposals and the anchor inherited from the previous prediction. The objectness-based anchors provide several complementary selective search regions, and an entropy-minimization-based selection method is introduced to find the best anchor. Our approach offers two benefits: 1) selective search regions can increase the chance of tracking success with affordable computational load and 2) anchor selection introduces the best anchor for each frame, which breaks the limitation of solo depending on the previous prediction. The extensive experiments of nine base trackers upgraded by MAT on four challenging datasets demonstrate the effectiveness of MAT."}}
{"id": "GM75boN4oT", "cdate": 1640995200000, "mdate": 1667569406814, "content": {"title": "Real-time Embedded Demo System for Fall Detection under 15W Power", "abstract": ""}}
{"id": "CjX29TnP9T", "cdate": 1640995200000, "mdate": 1667569406783, "content": {"title": "Anomaly Detection With Bidirectional Consistency in Videos", "abstract": "The core component of most anomaly detectors is a self-supervised model, tasked with modeling patterns included in training samples and detecting unexpected patterns as the anomalies in testing samples. To cope with normal patterns, this model is typically trained with reconstruction constraints. However, the model has the risk of overfitting to training samples and being sensitive to hard normal patterns in the inference phase, which results in irregular responses at normal frames. To address this problem, we formulate anomaly detection as a mutual supervision problem. Due to collaborative training, the complementary information of mutual learning can alleviate the aforementioned problem. Based on this motivation, a SIamese generative network (SIGnet), including two subnetworks with the same architecture, is proposed to simultaneously model the patterns of the forward and backward frames. During training, in addition to traditional constraints on improving the reconstruction performance, a bidirectional consistency loss based on the forward and backward views is designed as the regularization term to improve the generalization ability of the model. Moreover, we introduce a consistency-based evaluation criterion to achieve stable scores at the normal frames, which will benefit detecting anomalies with fluctuant scores in the inference phase. The results on several challenging benchmark data sets demonstrate the effectiveness of our proposed method."}}
{"id": "BjT5yrNwi0q", "cdate": 1640995200000, "mdate": 1667569406774, "content": {"title": "Locality-Aware Crowd Counting", "abstract": "Imbalanced data distribution in crowd counting datasets leads to severe under-estimation and over-estimation problems, which has been less investigated in existing works. In this paper, we tackle this challenging problem by proposing a simple but effective locality-based learning paradigm to produce generalizable features by alleviating sample bias. Our proposed method is locality-aware in two aspects. First, we introduce a locality-aware data partition (LADP) approach to group the training data into different bins via locality-sensitive hashing. As a result, a more balanced data batch is then constructed by LADP. To further reduce the training bias and enhance the collaboration with LADP, a new data augmentation method called locality-aware data augmentation (LADA) is proposed where the image patches are adaptively augmented based on the loss. The proposed method is independent of the backbone network architectures, and thus could be smoothly integrated with most existing deep crowd counting approaches in an end-to-end paradigm to boost their performance. We also demonstrate the versatility of the proposed method by applying it for adversarial defense. Extensive experiments verify the superiority of the proposed method over the state of the arts."}}
{"id": "0x7JO86KkBb", "cdate": 1640995200000, "mdate": 1667569406781, "content": {"title": "Person Re-Identification With Hierarchical Discriminative Spatial Aggregation", "abstract": "Practically, person re-identification (re-ID) may suffer from the critical spatial misalignment problem due to inaccurate human detection, variation on human pose and camera viewpoint, etc. To address this, a hierarchical discriminative spatial aggregation method is proposed. The key idea is to conduct spatial aggregation on local human parts via global average-pooling to acquire the strong spatial misalignment tolerance, with VALD encoding on the local parts for facilitating discriminative power jointly. This proposition is built on NetVLAD to ensure end-to-end deep learning capacity. Due to the fine-grained property of person re-ID task that has not been well concerned by the original NetVLAD model for scene recognition, a feature refinement layer that consists of 1 fully-connected (FC) layer and 2 batch normalization (BN) layers is added on top of the raw NetVLAD layer to enhance the discriminative power and training convergence. And, a human body occlusion and background component dropout manner is also proposed to resist the effect of serious occlusion. Technically, a refined codeword initialization manner is proposed to alleviate the potential codeword imbalance problem caused by naive random initialization. The proposed discriminative spatial aggregation approach is then conducted on multi-resolution convolutional feature map layers hierarchically via early feature fusion, to involve richer semantic and fine-grained visual clues jointly. Wide-range experiments on 6 datasets (i.e., CUHK03, DukeMTMC-reID, Occluded-DukeMTMC, Market-1501, MSMT17 and Occluded-REID) verifies the effectiveness of our proposition. The source code and supporting material is available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/zmyme/HDSA-reID</uri> ."}}
{"id": "sv1mwJ9OzVH", "cdate": 1609459200000, "mdate": 1667569406817, "content": {"title": "Single-Image Dehazing via Compositional Adversarial Network", "abstract": "Single-image dehazing has been an important topic given the commonly occurred image degradation caused by adverse atmosphere aerosols. The key to haze removal relies on an accurate estimation of global air-light and the transmission map. Most existing methods estimate these two parameters using separate pipelines which reduces the efficiency and accumulates errors, thus leading to a suboptimal approximation, hurting the model interpretability, and degrading the performance. To address these issues, this article introduces a novel generative adversarial network (GAN) for single-image dehazing. The network consists of a novel compositional generator and a novel deeply supervised discriminator. The compositional generator is a densely connected network, which combines fine-scale and coarse-scale information. Benefiting from the new generator, our method can directly learn the physical parameters from data and recover clean images from hazy ones in an end-to-end manner. The proposed discriminator is deeply supervised, which enforces that the output of the generator to look similar to the clean images from low-level details to high-level structures. To the best of our knowledge, this is the first end-to-end generative adversarial model for image dehazing, which simultaneously outputs clean images, transmission maps, and air-lights. Extensive experiments show that our method remarkably outperforms the state-of-the-art methods. Furthermore, to facilitate future research, we create the HazeCOCO dataset which is currently the largest dataset for single-image dehazing."}}
{"id": "fg7pPM4IPQ", "cdate": 1609459200000, "mdate": 1667569406774, "content": {"title": "Multi-Encoder Towards Effective Anomaly Detection in Videos", "abstract": "Given normal training samples, anomaly detection in videos can be regarded as a challenging problem of identifying unexpected events. The state-of-the-art approaches generally resort to the autoencoder model by using a single encoder to capture the motion and content patterns jointly. Nevertheless, due to the lack of accurate labels of normal and abnormal samples, how to detect anomalies is decided by the subjective understanding of models. It infers that different models will prefer to mine different patterns according to the characteristics of models. We call this problem as a pattern bias problem. To alleviate this problem, a novel Multi-Encoder Single-Decoder network, termed as MESDnet, is proposed in the spirit of encoding motion and content cues individually with multiple encoders. MESDnet is of end-to-end learning ability and real-time running speed. Particularly, the differences between adjacent frames and the raw frames are used as the motion and content sources, respectively. Then, a decoder takes charge of detecting anomalies in the way of observing reconstructing error towards the video frames by using the multi-stream encoded motion and content features simultaneously. The experiments on the CUHK Avenue dataset, the UCSD Pedestrian dataset, and the ShanghaiTech Campus dataset verify the effectiveness of MESDnet."}}
{"id": "5Nn62MrSfPz", "cdate": 1609459200000, "mdate": 1667569406943, "content": {"title": "Abrupt-motion-aware lightweight visual tracking for unmanned aerial vehicles", "abstract": "Visual tracking for unmanned aerial vehicles (UAVs) is a hot research topic for the wide applications of UAVs. As UAVs are high-altitude and high-freedom platforms, small targets in UAV tracking sequences are often under the attribute of large-scale location change due to the abrupt motion of the platform. Currently, many visual tracking methods based on the local search hypothesis have been widely researched on low-speed moving platforms. However, these methods cannot be directly used on the UAV platform, because targets will appear in any position of the new frame. To address this problem, we propose an abrupt-motion-aware visual tracking method in this paper. Because of the high power consumption of deep learning models, the proposed method is a lightweight tracker for small UAVs without the deep learning framework. Our method consists of three major components: abrupt motion estimation, object tracking and model updating. Abrupt motion often leads to abnormal changes in the response map of trackers. Thus, by analyzing the changes of tracking response maps, the abrupt motion can be detected efficiently. When abrupt motion happens, keypoint matching will be adaptively implemented to estimate the ego-motion and skipped otherwise. Then, the target location is predicted by the correlation filter tracker in a local search region. Moreover, according to the confidence analysis, an adaptive model update strategy is designed to alleviate the model noise caused by the short-term occlusion. Experimental results confirm the robustness and the accuracy of our method on challenging sequences and show the comparative performance of the proposed method against several state-of-the-art lightweight methods."}}
{"id": "4m9RARUTtR", "cdate": 1609459200000, "mdate": 1667569406809, "content": {"title": "LPQ++: A discriminative blur-insensitive textural descriptor with spatial-channel interaction", "abstract": ""}}
{"id": "hyAU8QpFo1", "cdate": 1577836800000, "mdate": 1667569406931, "content": {"title": "Attention-Driven Loss for Anomaly Detection in Video Surveillance", "abstract": "Recent video anomaly detection methods focus on reconstructing or predicting frames. Under this umbrella, the long-standing inter-class data-imbalance problem resorts to the imbalance between foreground and stationary background objects in video anomaly detection and this has been less investigated by existing solutions. Naively optimizing the reconstructing loss yields a biased optimization towards background reconstruction rather than the objects of interest in the foreground. To solve this, we proposed a simple yet effective solution, termed attention-driven loss to alleviate the foreground-background imbalance problem in anomaly detection. Specifically, we compute a single mask map that summarizes the frame evolution of moving foreground regions and suppresses the background in the training video clips. After that, we construct an attention map through the combination of the mask map and background to give different weights to the foreground and background region respectively. The proposed attention-driven loss is independent of backbone networks and can be easily augmented in most existing anomaly detection models. Augmented with attention-driven loss, the model is able to achieve AUC 86.0% on Avenue, 83.9% on Ped1, 96% on Ped2 datasets. Extensive experimental results and ablation studies further validate the effectiveness of our model."}}
