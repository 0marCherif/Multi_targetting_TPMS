{"id": "ttTn8KfK5LX", "cdate": 1640995200000, "mdate": 1681288809298, "content": {"title": "Data Learning: Integrating Data Assimilation and Machine Learning", "abstract": ""}}
{"id": "StIQdxA8lVR", "cdate": 1609459200000, "mdate": 1681288809349, "content": {"title": "Submodular Kernels for Efficient Rankings", "abstract": ""}}
{"id": "DU8ICRUOqX", "cdate": 1609459200000, "mdate": 1681288809351, "content": {"title": "Sliced Multi-Marginal Optimal Transport", "abstract": ""}}
{"id": "kVfEmLx8x5H", "cdate": 1577836800000, "mdate": 1681288809335, "content": {"title": "High-dimensional Bayesian optimization using low-dimensional feature spaces", "abstract": ""}}
{"id": "cbFd-VsYX-", "cdate": 1577836800000, "mdate": 1681288809333, "content": {"title": "High-dimensional Bayesian optimization with projections using quantile Gaussian processes", "abstract": ""}}
{"id": "rkxSpEreUS", "cdate": 1567802556730, "mdate": null, "content": {"title": "Fast Decomposable Submodular Function Minimization using Constrained Total Variation", "abstract": "We consider the problem of minimizing the sum of submodular set functions assuming minimization oracles of each summand function. Most existing approaches reformulate the problem as the convex minimization of the sum of the corresponding Lov\\'asz extensions and the squared Euclidean norm, leading to algorithms requiring total variation oracles of the summand functions; without further assumptions, these more complex oracles require many calls to the simpler minimization oracles often available in practice. In this paper, we consider a modified convex problem requiring  constrained version of the total variation oracles that can be solved with significantly fewer calls to the simple minimization oracles.  We support our claims by showing results on graph cuts for 2D and 3D graphs."}}
{"id": "TwdMIUtUcP", "cdate": 1546300800000, "mdate": null, "content": {"title": "High-Dimensional Bayesian Optimization with Manifold Gaussian Processes", "abstract": "Bayesian optimization (BO) is a powerful approach for seeking the global optimum of expensive black-box functions and has proven successful for fine tuning hyper-parameters of machine learning models. However, BO is practically limited to optimizing 10--20 parameters. To scale BO to high dimensions, we usually make structural assumptions on the decomposition of the objective and\\slash or exploit the intrinsic lower dimensionality of the problem, e.g. by using linear projections. We could achieve a higher compression rate with nonlinear projections, but learning these nonlinear embeddings typically requires much data. This contradicts the BO objective of a relatively small evaluation budget. To address this challenge, we propose to learn a low-dimensional feature space jointly with (a) the response surface and (b) a reconstruction mapping. Our approach allows for optimization of BO's acquisition function in the lower-dimensional subspace, which significantly simplifies the optimization problem. We reconstruct the original parameter space from the lower-dimensional subspace for evaluating the black-box function. For meaningful exploration, we solve a constrained optimization problem."}}
{"id": "HJXO3yzG0i0", "cdate": 1546300800000, "mdate": null, "content": {"title": "Differentially Private Empirical Risk Minimization with Sparsity-Inducing Norms", "abstract": "Differential privacy is concerned about the prediction quality while measuring the privacy impact on individuals whose information is contained in the data. We consider differentially private risk minimization problems with regularizers that induce structured sparsity. These regularizers are known to be convex but they are often non-differentiable. We analyze the standard differentially private algorithms, such as output perturbation, Frank-Wolfe and objective perturbation. Output perturbation is a differentially private algorithm that is known to perform well for minimizing risks that are strongly convex. Previous works have derived excess risk bounds that are independent of the dimensionality. In this paper, we assume a particular class of convex but non-smooth regularizers that induce structured sparsity and loss functions for generalized linear models. We also consider differentially private Frank-Wolfe algorithms to optimize the dual of the risk minimization problem. We derive excess risk bounds for both these algorithms. Both the bounds depend on the Gaussian width of the unit ball of the dual norm. We also show that objective perturbation of the risk minimization problems is equivalent to the output perturbation of a dual optimization problem. This is the first work that analyzes the dual optimization problems of risk minimization problems in the context of differential privacy."}}
{"id": "j-mQkkEpT-", "cdate": 1483228800000, "mdate": 1681288809620, "content": {"title": "Active-set Methods for Submodular Minimization Problems", "abstract": ""}}
{"id": "5g0ZgWQA86J", "cdate": 1451606400000, "mdate": 1681288809344, "content": {"title": "On the Links between Probabilistic Graphical Models and Submodular Optimisation. (Liens entre mod\u00e8les graphiques probabilistes et optimisation sous-modulaire)", "abstract": ""}}
