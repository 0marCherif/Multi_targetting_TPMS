{"id": "SH-Wxc3bIN", "cdate": 1672531200000, "mdate": 1680013878768, "content": {"title": "TwERC: High Performance Ensembled Candidate Generation for Ads Recommendation at Twitter", "abstract": ""}}
{"id": "-Ct9quJzd0", "cdate": 1672531200000, "mdate": 1696254690891, "content": {"title": "Exposure to Marginally Abusive Content on Twitter", "abstract": "Social media platforms can help people find connection and entertainment, but they can also show potentially abusive content such as insults and targeted cursing. While platforms do remove some abusive content for rule violation, some is considered \"margin content\" that does not violate any rules and thus stays on the platform. This paper presents a focused analysis of exposure to such content on Twitter, asking (RQ1) how exposure to marginally abusive content varies across Twitter users, and (RQ2) how algorithmically-ranked timelines impact exposure to marginally abusive content. Based on one month of impression data from November 2021, descriptive analyses (RQ1) show significant variation in exposure, with more active users experiencing higher rates and higher volumes of marginal impressions. Experimental analyses (RQ2) show that users with algorithmically-ranked timelines experience slightly lower rates of marginal impressions. However, they tend to register more total impression activity and thus experience a higher cumulative volume of marginal impressions. The paper concludes by discussing implications of the observed concentration, the multifaceted impact of algorithmically-ranked timelines, and potential directions for future work."}}
{"id": "hM8P-Dvn7Sm", "cdate": 1640995200000, "mdate": 1648778850284, "content": {"title": "Measuring Disparate Outcomes of Content Recommendation Algorithms with Distributional Inequality Metrics", "abstract": "The harmful impacts of algorithmic decision systems have recently come into focus, with many examples of systems such as machine learning (ML) models amplifying existing societal biases. Most metrics attempting to quantify disparities resulting from ML algorithms focus on differences between groups, dividing users based on demographic identities and comparing model performance or overall outcomes between these groups. However, in industry settings, such information is often not available, and inferring these characteristics carries its own risks and biases. Moreover, typical metrics that focus on a single classifier's output ignore the complex network of systems that produce outcomes in real-world settings. In this paper, we evaluate a set of metrics originating from economics, distributional inequality metrics, and their ability to measure disparities in content exposure in a production recommendation system, the Twitter algorithmic timeline. We define desirable criteria for metrics to be used in an operational setting, specifically by ML practitioners. We characterize different types of engagement with content on Twitter using these metrics, and use these results to evaluate the metrics with respect to the desired criteria. We show that we can use these metrics to identify content suggestion algorithms that contribute more strongly to skewed outcomes between users. Overall, we conclude that these metrics can be useful tools for understanding disparate outcomes in online social networks."}}
{"id": "7pAuEHUNXVX", "cdate": 1640995200000, "mdate": 1680013878762, "content": {"title": "Random Isn't Always Fair: Candidate Set Imbalance and Exposure Inequality in Recommender Systems", "abstract": ""}}
{"id": "-2NUePYNfJ", "cdate": 1640995200000, "mdate": 1680013878764, "content": {"title": "Measuring disparate outcomes of content recommendation algorithms with distributional inequality metrics", "abstract": ""}}
{"id": "cy-E5qHxN7", "cdate": 1631641082759, "mdate": null, "content": {"title": "Does Deep Learning Have Politics?", "abstract": "Deep neural networks have become ubiquitous in applications throughout the tech industry, underlying everything from facial recognition to automated translation tools. Many examples have demonstrated the potential ethical pitfalls of their application, and analyses of such examples often cite two major causes: biased training data sets and lack of diversity in the institutions that produce deep learning systems. In this work, we examine how inherent qualities of deep learning itself can give rise to its misuse, using the framework outlined in Winner\u2019s Do Artifacts Have Politics?. First, we argue that the design paradigm advocated by the deep learning revolution, namely the shift to \"end-to-end\" systems, has opened the door to ignorance of the sensitive, context-specific qualities of some input data. Second, we assert that the reliance of deep learning on increasingly large data sets and compute resources centers the power of these algorithms in corporations or the government, which thus leaves its practice vulnerable to the institutional racism and sexism that is so often found there."}}
