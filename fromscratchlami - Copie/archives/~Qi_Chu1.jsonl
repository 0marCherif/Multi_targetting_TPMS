{"id": "FhECi0ydhZ", "cdate": 1683886434492, "mdate": 1683886434492, "content": {"title": "EVOPOSE: A RECURSIVE TRANSFORMER FOR 3D HUMAN POSE ESTIMATION WITH KINEMATIC STRUCTURE PRIORS", "abstract": "Transformer is popular in recent 3D human pose estimation, which utilizes long-term modeling to lift 2D keypoints into the 3D space. However, current transformer-based methods do not fully exploit the prior knowledge of the human skeleton provided by the kinematic structure. In this paper, we propose a novel transformer-based model EvoPose to introduce the human body prior knowledge for 3D human pose estimation effectively. Specifically, a Structural Priors Representation (SPR) module represents human priors as structural features carrying rich body patterns, e.g. joint relationships. The structural features are interacted with 2D pose sequences and help the model to achieve more informative spatiotemporal features. Moreover, a Recursive Refinement (RR) module is applied to refine the 3D pose outputs by utilizing estimated results and further injects human priors simultaneously. Ex- tensive experiments demonstrate the effectiveness of EvoPose which achieves a new state of the art on two most popular benchmarks, Human3.6M and MPI-INF-3DHP."}}
{"id": "XOKxLxnsX6h", "cdate": 1682899200000, "mdate": 1682671973298, "content": {"title": "Semantic Probability Distribution Modeling for Diverse Semantic Image Synthesis", "abstract": "Semantic image synthesis, translating semantic layouts to photo-realistic images, is a one-to-many mapping problem. Though impressive progress has been recently made, diverse semantic synthesis that can efficiently produce semantic-level or even instance-level multimodal results, still remains a challenge. In this article, we propose a novel diverse semantic image synthesis framework from the perspective of semantic class distributions, which naturally supports diverse generation at both semantics and instance level. We achieve this by modeling class-level conditional modulation parameters as continuous probability distributions instead of discrete values, and sampling per-instance modulation parameters through instance-adaptive stochastic sampling that is consistent across the network. Moreover, we propose prior noise remapping, through linear perturbation parameters encoded from paired references, to facilitate supervised training and exemplar-based instance style control at test time. To further extend the user interaction function of the proposed method, we also introduce sketches into the network. In addition, specially designed generator modules, Progressive Growing Module and Multi-Scale Refinement Module, can be used as a general module to improve the performance of complex scene generation. Extensive experiments on multiple datasets show that our method can achieve superior diversity and comparable quality compared to state-of-the-art methods. Codes are available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/tzt101/INADE.git</uri> ."}}
{"id": "zMq-gDTXmDA", "cdate": 1672531200000, "mdate": 1675758504671, "content": {"title": "F2Trans: High-Frequency Fine-Grained Transformer for Face Forgery Detection", "abstract": "In recent years, face forgery detectors have aroused great interest and achieved impressive performance, but they are still struggling with generalization and robustness. In this work, we explore taking full advantage of the fine-grained forgery traces in both spatial and frequency domains to alleviate this issue. Specifically, we propose a novel High-Frequency Fine-Grained Transformer (F2Trans) network which contains two important components, namely Central Difference Attention (CDA) and High-frequency Wavelet Sampler (HWS). The premier CDA module is capable of capturing invariant fine-grained manipulation patterns by aggregating both pixel-level intensity and gradient information of the query to generate key and value pairs. Subsequently, the proposed HWS discards the low-frequency components of wavelet transformation and hierarchically explores high-frequency forgery cues of feature maps, which prevents model confusion caused by low-frequency components and pays attention to local frequency information. In addition, HWS can be employed as a special pooling layer for the F2Trans architecture to produce hierarchical feature representations in the spatial-frequency domain. Extensive experiments on multiple popular benchmarks demonstrate the generalization and robustness of the specially designed F2Trans framework is well-tailored for face forgery detection when confronting the cross-dataset, cross-manipulation, and unseen perturbations."}}
{"id": "QhyRkf6wrC", "cdate": 1672531200000, "mdate": 1682671973749, "content": {"title": "AutoMA: Towards Automatic Model Augmentation for Transferable Adversarial Attacks", "abstract": "Recent adversarial attack works attempt to improve the transferability by applying various differentiable transformations on input images. Considering the differentiable transformations and the original model together as a new model, these methods can be regarded as model augmentation that effectively derives an ensemble of models from the single original model. Despite their impressive performance, the model augmentation policies used in these methods are manually designed by experimental attempts, leaving the design of model augmentation policy an open question. In this paper, we propose an Automatic Model Augmentation (AutoMA) approach to find a strong model augmentation policy for transferable adversarial attacks. Specifically, we design a discrete search space that contains various diffierentiable transformations with different parameters and adopt reinforcement learning to search for the strong augmentation policy. The sampled augmentation policies together with the rewards they obtain during the searching process reveal several valuable observations for designing more powerful attacks using model augmentation policy: <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1) Augmentation transformations on color space are less effective; 2) The transformation type diversity matters; and 3) Using small distortion for geometric transformations while larger distortion for intensity transformations.</i> Extensive experiments show that the augmentation policy found by AutoMA achieves superior performance than existing manually designed policies in a wide range of cases."}}
{"id": "tQ4e6e29UQO", "cdate": 1640995200000, "mdate": 1667562366474, "content": {"title": "Learning Forgery Region-Aware and ID-Independent Features for Face Manipulation Detection", "abstract": "Over the past several years, to solve the problem of malicious abuse of facial manipulation technology, face manipulation detection has obtained considerable attention. Although existing works achieved impressive performance on a hold-out test set, their methods suffered a significant performance drop on data from a different distribution than the training set used. In this paper, we conduct an in-depth analysis on existing typical models about poor generalization capability and propose a novel method for face manipulation detection, which can alleviate overfitting and improve the generalization ability by learning forgery region aware and ID-independent features. Specifically, a forgery region guided self-attention module (FR) is introduced to make the model focus on the forgery region and a landmark guided dropout module (LM) is designed to randomly remove features of structured informative regions for destroying identity features. These two regularization modules are then added to the basic classification network, e.g., Xception. With the help of the joint learning framework, both forgery region-aware and ID-independent features are learned for face manipulation detection with better generalization ability. Extensive experiments demonstrate that the proposed method can not only outperform competing state-of-the-art methods on the FaceForensics++ dataset but also achieve superior generalization performance on the Celeb-DF dataset."}}
{"id": "rx3imNRaLf", "cdate": 1640995200000, "mdate": 1682671973749, "content": {"title": "Multi-view Geometry Distillation for Cloth-Changing Person ReID", "abstract": "Most person re-identification (ReID) methods aim at retrieving people with unchanged clothes. Meanwhile, fewer studies work on the cloth-inconsistency problem, which is more challenging but useful in the real intelligent surveillance scenario. We propose a novel method, named Multi-View Geometry Distillation (MVGD), taking advantage of 3D priors to explore cloth-unrelated multi-view human information. Specifically, a 3D Grouping Geometry Graph Convolution Network (3DG $$^{3}$$ ) is proposed to extract ReID-specific geometry representation from the 3D reconstructed body mesh, which encodes shape, pose, and other geometry patterns from the 3D perspective. Then, we design a 3D-Guided Appearance Learning scheme to extract more accurate part features. Furthermore, we also adopt a Multi-View Interactive Learning module (MVIL) to fuse the different types of features together and extract high-level multi-view geometry representation. Finally, these discriminative features are treated as the teacher to guide the backbone by the distillation mechanism for better representations. Extensive experiments on three popular cloth-changing ReID datasets demonstrate the effectiveness of our method. The proposed method brings 9 $$\\%$$ and 7.5 $$\\%$$ gains in average in terms of rank-1 and mAP metrics against the baseline, respectively."}}
{"id": "qqRNskHC9uQ", "cdate": 1640995200000, "mdate": 1667562366685, "content": {"title": "Reduce Information Loss in Transformers for Pluralistic Image Inpainting", "abstract": "Transformers have achieved great success in pluralistic image inpainting recently. However, we find existing transformer based solutions regard each pixel as a token, thus suffer from information loss issue from two aspects: 1) They downsample the input image into much lower resolutions for efficiency consideration, incurring information loss and extra misalignment for the boundaries of masked regions. 2) They quantize 256 <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">3</sup> RGB pixels to a small number (such as 512) of quantized pixels. The indices of quantized pixels are used as tokens for the inputs and prediction targets of transformer. Although an extra CNN network is used to upsample and refine the low-resolution results, it is difficult to retrieve the lost information back. To keep input information as much as possible, we propose a new transformer based framework \u201cPUT\u201d. Specifically, to avoid input downsampling while maintaining the computation efficiency, we design a patch-based auto-encoder P-VQVAE, where the encoder converts the masked image into non-overlapped patch tokens and the decoder recovers the masked regions from the inpainted tokens while keeping the unmasked regions unchanged. To eliminate the information loss caused by quantization, an Un-Quantized Transformer (UQ-Transformer) is applied, which directly takes the features from P-VQVAE encoder as input without quantization and regards the quantized tokens only as prediction targets. Extensive experiments show that PUT greatly outperforms state-of-the-art methods on image fidelity, especially for large masked regions and complex large-scale datasets."}}
{"id": "l8HFpl8M38i", "cdate": 1640995200000, "mdate": 1667562366686, "content": {"title": "Efficient Semantic Image Synthesis via Class-Adaptive Normalization", "abstract": "Spatially-adaptive normalization (SPADE) is remarkably successful recently in conditional semantic image synthesis in T. Park <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">et al.</i> 2019 which modulates the normalized activation with spatially-varying transformations learned from semantic layouts, to prevent the semantic information from being washed away. Despite its impressive performance, a more thorough understanding of the advantages inside the box is still highly demanded to help reduce the significant computation and parameter overhead introduced by this novel structure. In this paper, from a return-on-investment point of view, we conduct an in-depth analysis of the effectiveness of this spatially-adaptive normalization and observe that its modulation parameters benefit more from semantic-awareness rather than spatial-adaptiveness, especially for high-resolution input masks. Inspired by this observation, we propose class-adaptive normalization (CLADE), a lightweight but equally-effective variant that is only adaptive to semantic class. In order to further improve spatial-adaptiveness, we introduce intra-class positional map encoding calculated from semantic layouts to modulate the normalization parameters of CLADE and propose a truly spatially-adaptive variant of CLADE, namely CLADE-ICPE. Through extensive experiments on multiple challenging datasets, we demonstrate that the proposed CLADE can be generalized to different SPADE-based methods while achieving comparable generation quality compared to SPADE, but it is much more efficient with fewer extra parameters and lower computational cost. The code and pretrained models are available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/tzt101/CLADE.git</uri> ."}}
{"id": "gNzn6LbH-y", "cdate": 1640995200000, "mdate": 1667562366683, "content": {"title": "Counterfactual Intervention Feature Transfer for Visible-Infrared Person Re-identification", "abstract": ""}}
{"id": "_orUFVo7t63", "cdate": 1640995200000, "mdate": 1668742031392, "content": {"title": "UIA-ViT: Unsupervised Inconsistency-Aware Method Based on Vision Transformer for Face Forgery Detection", "abstract": "Intra-frame inconsistency has been proved to be effective for the generalization of face forgery detection. However, learning to focus on these inconsistency requires extra pixel-level forged location annotations. Acquiring such annotations is non-trivial. Some existing methods generate large-scale synthesized data with location annotations, which is only composed of real images and cannot capture the properties of forgery regions. Others generate forgery location labels by subtracting paired real and fake images, yet such paired data is difficult to collected and the generated label is usually discontinuous. To overcome these limitations, we propose a novel Unsupervised Inconsistency-Aware method based on Vision Transformer, called UIA-ViT, which only makes use of video-level labels and can learn inconsistency-aware feature without pixel-level annotations. Due to the self-attention mechanism, the attention map among patch embeddings naturally represents the consistency relation, making the vision Transformer suitable for the consistency representation learning. Based on vision Transformer, we propose two key components: Unsupervised Patch Consistency Learning (UPCL) and Progressive Consistency Weighted Assemble (PCWA). UPCL is designed for learning the consistency-related representation with progressive optimized pseudo annotations. PCWA enhances the final classification embedding with previous patch embeddings optimized by UPCL to further improve the detection performance. Extensive experiments demonstrate the effectiveness of the proposed method."}}
