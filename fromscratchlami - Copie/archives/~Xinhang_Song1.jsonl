{"id": "iixcf8_9Uom", "cdate": 1640995200000, "mdate": 1668188320891, "content": {"title": "Generative Meta-Adversarial Network for Unseen Object Navigation", "abstract": ""}}
{"id": "dyIhJF9cBK", "cdate": 1640995200000, "mdate": 1668188320740, "content": {"title": "Amorphous Region Context Modeling for Scene Recognition", "abstract": ""}}
{"id": "NWYlZ5z8Q-R", "cdate": 1621630108338, "mdate": null, "content": {"title": "See More for Scene: Pairwise Consistency Learning for Scene Classification", "abstract": "Scene classification is a valuable classification subtask and has its own characteristics which still needs more in-depth studies. Basically, scene characteristics are distributed over the whole image, which cause the need of \u201cseeing\u201d comprehensive and informative regions. Previous works mainly focus on region discovery and aggregation, while rarely involves the inherent properties of CNN along with its potential ability to satisfy the requirements of scene classification. In this paper, we propose to understand scene images and the scene classification CNN models in terms of the focus area. From this new perspective, we find that large focus area is preferred in scene classification CNN models as a consequence of learning scene characteristics. Meanwhile, the analysis about existing training schemes helps us to understand the effects of focus area, and also raises the question about optimal training method for scene classification. Pursuing the better usage of scene characteristics, we propose a new learning scheme with a tailored loss in the goal of activating larger focus area on scene images. Since the supervision of the target regions to be enlarged is usually lacked, our alternative learning scheme is to erase already activated area, and allow the CNN models to activate more area during training. The proposed scheme is implemented by keeping the pairwise consistency between the output of  the erased image and its original one. In particular, a tailored loss is proposed to keep such pairwise consistency by leveraging category-relevance information. Experiments on Places365 show the significant improvements of our method with various CNNs. Our method shows an inferior result on the object-centric dataset, ImageNet, which experimentally indicates that it captures the unique characteristics of scenes."}}
{"id": "OlE9XvwusV", "cdate": 1609459200000, "mdate": 1668188320799, "content": {"title": "Hierarchical Object-to-Zone Graph for Object Navigation", "abstract": ""}}
{"id": "MJK1NlaNC62", "cdate": 1609459200000, "mdate": 1668188320815, "content": {"title": "See More for Scene: Pairwise Consistency Learning for Scene Classification", "abstract": ""}}
{"id": "FeyLcjuviFx", "cdate": 1609459200000, "mdate": 1668188320885, "content": {"title": "Hierarchical Object-to-Zone Graph for Object Navigation", "abstract": ""}}
{"id": "DgPwAj1x9tZ", "cdate": 1609459200000, "mdate": 1668188320822, "content": {"title": "ION: Instance-level Object Navigation", "abstract": ""}}
{"id": "iD05sr0SX5Y", "cdate": 1577836800000, "mdate": null, "content": {"title": "Image captioning via semantic element embedding", "abstract": "Image caption approaches that use the global Convolutional Neural Network (CNN) features are not able to represent and describe all the important elements in complex scenes. In this paper, we propose to enrich the semantic representations of images and update the language model by proposing semantic element embedding. For the semantic element discovery, an object detection module is used to predict regions of the image, and a captioning model, Long Short-Term Memory (LSTM), is employed to generate local descriptions for these regions. The predicted descriptions and categories are used to generate the semantic feature, which not only contains detailed information but also shares a word space with descriptions, and thus bridges the modality gap between visual images and semantic captions. We further integrate the CNN feature with the semantic feature into the proposed Element Embedding LSTM (EE-LSTM) model to predict a language description. Experiments on MS COCO datasets demonstrate that the proposed approach outperforms conventional caption methods and is flexible to combine with baseline models to achieve superior performance."}}
{"id": "WuzeEZjrIuo", "cdate": 1577836800000, "mdate": null, "content": {"title": "Image Representations With Spatial Object-to-Object Relations for RGB-D Scene Recognition", "abstract": "Scene recognition is challenging due to the intra-class diversity and inter-class similarity. Previous works recognize scenes either with global representations or with the intermediate representations of objects. In contrast, we investigate more discriminative image representations of object-to-object relations for scene recognition, which are based on the triplets of <;object, relation, object> obtained with detection techniques. Particularly, two types of representations, including co-occurring frequency of object-to-object relation (denoted as COOR) and sequential representation of object-to-object relation (denoted as SOOR), are proposed to describe objects and their relative relations in different forms. COOR is represented as the intermediate representation of co-occurring frequency of objects and their relations, with a three order tensor that can be fed to scene classifier without further embedding. SOOR is represented in a more explicit and freer form that sequentially describe image contents with local captions. And a sequence encoding model (e.g., recurrent neural network (RNN)) is implemented to encode SOOR to the features for feeding the classifiers. In order to better capture the spatial information, the proposed COOR and SOOR are adapted to RGB-D data, where a RGB-D proposal fusion method is proposed for RGB-D object detection. With the proposed approaches COOR and SOOR, we obtain the state-of-the-art results of RGB-D scene recognition on SUN RGB-D and NYUD2 datasets."}}
{"id": "Ghp8sFptbWj", "cdate": 1577836800000, "mdate": null, "content": {"title": "Scene Recognition With Prototype-Agnostic Scene Layout", "abstract": "Exploiting the spatial structure in scene images is a key research direction for scene recognition. Due to the large intra-class structural diversity, building and modeling flexible structural layout to adapt various image characteristics is a challenge. Existing structural modeling methods in scene recognition either focus on predefined grids or rely on learned prototypes, which all have limited representative ability. In this paper, we propose Prototype-agnostic Scene Layout (PaSL) construction method to build the spatial structure for each image without conforming to any prototype. Our PaSL can flexibly capture the diverse spatial characteristic of scene images and have considerable generalization capability. Given a PaSL, we build Layout Graph Network (LGN) where regions in PaSL are defined as nodes and two kinds of independent relations between regions are encoded as edges. The LGN aims to incorporate two topological structures (formed in spatial and semantic similarity dimensions) into image representations through graph convolution. Extensive experiments show that our approach achieves state-of-the-art results on widely recognized MIT67 and SUN397 datasets without multi-model or multi-scale fusion. Moreover, we also conduct the experiments on one of the largest scale datasets, Places365. The results demonstrate the proposed method can be well generalized and obtains competitive performance."}}
