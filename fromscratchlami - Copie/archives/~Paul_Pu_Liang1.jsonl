{"id": "I_GUngvVNz", "cdate": 1676591078237, "mdate": null, "content": {"title": "Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals", "abstract": "High sample complexity has long been a challenge for RL. On the other hand, human learn to perform tasks not only from interaction or demonstrations, but also by reading unstructured text documents, e.g., instruction manuals. Instruction manuals and wiki pages are among the most abundant data that could inform agents of valuable features and policies or task-specific environmental dynamics and reward structures. Therefore, we hypothesize that the ability to utilize human-written instruction manuals to assist learning policies for specific tasks should lead to a more efficient and better-performing agent.\n\nWe propose the Read and Reward framework. Read and Reward speeds up RL algorithms on Atari games by reading manuals released by the Atari game developers. Our framework consists of a QA Extraction module that extracts and summarizes relevant information from the manual and a Reasoning module that evaluates object-agent interactions based on information from the manual. Auxiliary reward is then provided to a standard A2C RL agent, when interaction is detected. When assisted by our design, A2C improves on 4 games in the Atari environment with sparse rewards, and requires 1000x less training frames compared to the previous SOTA Agent 57 on Skiing, the hardest game in Atari."}}
{"id": "m7CAKlu_Ocx", "cdate": 1672531200000, "mdate": 1699220650476, "content": {"title": "HIINT: Historical, Intra- and Inter- personal Dynamics Modeling with Cross-person Memory Transformer", "abstract": "Accurately modeling affect dynamics, which refers to the changes and fluctuations in emotions and affective displays during human conversations, is crucial for understanding human interactions. However, modeling affect dynamics is challenging due to contextual factors, such as the complex and nuanced nature of intra- and inter- personal dependencies. Intrapersonal dependencies refer to the influences and dynamics within an individual, including their affective states and how it evolves over time. Interpersonal dependencies, on the other hand, involve the interactions and dynamics between individuals, encompassing how affective displays are influenced by and influence others during conversations. To address these challenges, we propose a Cross-person Memory Transformer (CPM-T) framework which explicitly models intra- and inter- personal dependencies in multi-modal non-verbal cues. The CPM-T framework maintains memory modules to store and update dependencies between earlier and later parts of a conversation. Additionally, our framework employs cross-modal attention to effectively align information from multi-modalities and leverage cross-person attention to align behaviors in multi-party interactions. We evaluate the effectiveness and robustness of our approach on three publicly available datasets for joint engagement, rapport, and human belief prediction tasks. Our framework outperforms baseline models in average F1-scores by up to 22.6%, 15.1%, and 10.0% respectively on these three tasks. Finally, we demonstrate the importance of each component in the framework via ablation studies with respect to multimodal temporal behavior."}}
{"id": "aSXaEftfTU", "cdate": 1672531200000, "mdate": 1696442298518, "content": {"title": "Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control", "abstract": ""}}
{"id": "_pQxrrmbqd", "cdate": 1672531200000, "mdate": 1699220649830, "content": {"title": "Tutorial on Multimodal Machine Learning: Principles, Challenges, and Open Questions", "abstract": "Multimodal machine learning is a vibrant multi-disciplinary research field that aims to design computer agents capable of understanding, reasoning, and learning through integrating multiple communicative modalities, including linguistic, acoustic, visual, tactile, and physiological messages. With the recent interest in video understanding, embodied autonomous agents, text-to-image generation, and multisensor fusion in healthcare and robotics, multimodality has brought unique computational and theoretical challenges to the machine learning community given the heterogeneity of data sources and the interconnections often found between modalities. However, the breadth of progress in multimodal research has made it difficult to identify the common themes and open questions in the field. By synthesizing a broad range of application domains and theoretical frameworks from both historical and recent perspectives, this tutorial is designed to provide an overview of the computational and theoretical foundations of multimodal machine learning. Building upon a new edition of our survey paper on multimodal ML and academic courses at CMU, this tutorial will cover three topics: (1) what is multimodal: the principles in learning from heterogeneous, connected, and interacting data, (2) why is it hard: a taxonomy of six core technical challenges faced in multimodal ML but understudied in unimodal ML, and (3) what is next: major directions for future research as identified by our taxonomy."}}
{"id": "_To6S3y2lN", "cdate": 1672531200000, "mdate": 1683924633962, "content": {"title": "Face-to-Face Contrastive Learning for Social Intelligence Question-Answering", "abstract": "Creating artificial social intelligence \u2013 algorithms that can understand the nuances of multi-person interactions \u2013 is an exciting and emerging challenge in processing facial expressions and gestures from multimodal videos. Recent multimodal methods have set the state of the art on many tasks, but have difficulty modeling the complex face-to-face conversational dynamics across speaking turns in social interaction, particularly in a self-supervised setup. In this paper, we propose Face-to-Face Contrastive Learning (F2F-CL), a graph neural network designed to model social interactions using factorization nodes to contextualize the multimodal face-to-face interaction along the boundaries of the speaking turn. With the F2F-CL model, we propose to perform contrastive learning between the factorization nodes of different speaking turns within the same video. We experimentally evaluate our method on the challenging Social-IQ dataset and show state-of-the-art results."}}
{"id": "_3ybOzrTBLe", "cdate": 1672531200000, "mdate": 1696442298555, "content": {"title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions", "abstract": "Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, Louis-Philippe Morency. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 2023."}}
{"id": "XUiiKfDtnQ", "cdate": 1672531200000, "mdate": 1685565804336, "content": {"title": "MultiViz: Towards User-Centric Visualizations and Interpretations of Multimodal Models", "abstract": ""}}
{"id": "VjXYZTZW0G", "cdate": 1672531200000, "mdate": 1696442298624, "content": {"title": "Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment", "abstract": "Rohan Pandey, Rulin Shao, Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023."}}
{"id": "VXsmRHjyZhG", "cdate": 1672531200000, "mdate": 1699220650349, "content": {"title": "Multimodal Fusion Interactions: A Study of Human and Automatic Quantification", "abstract": "In order to perform multimodal fusion of heterogeneous signals, we need to understand their interactions: how each modality individually provides information useful for a task and how this information changes in the presence of other modalities. In this paper, we perform a comparative study of how humans annotate two categorizations of multimodal interactions: (1) partial labels, where different annotators annotate the label given the first, second, and both modalities, and (2) counterfactual labels, where the same annotator annotates the label given the first modality before asking them to explicitly reason about how their answer changes when given the second. We further propose an alternative taxonomy based on (3) information decomposition, where annotators annotate the degrees of redundancy: the extent to which modalities individually and together give the same predictions, uniqueness: the extent to which one modality enables a prediction that the other does not, and synergy: the extent to which both modalities enable one to make a prediction that one would not otherwise make using individual modalities. Through experiments and annotations, we highlight several opportunities and limitations of each approach and propose a method to automatically convert annotations of partial and counterfactual labels to information decomposition, yielding an accurate and efficient method for quantifying multimodal interactions."}}
{"id": "MaY8UmKj8n", "cdate": 1672531200000, "mdate": 1696442298482, "content": {"title": "MultiViz: Towards Visualizing and Understanding Multimodal Models", "abstract": ""}}
