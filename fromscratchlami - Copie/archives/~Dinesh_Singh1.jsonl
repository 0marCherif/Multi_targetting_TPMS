{"id": "BfMNn3rYUe9", "cdate": 1609459200000, "mdate": 1645807028436, "content": {"title": "Nys-Curve: Nystr\u00f6m-Approximated Curvature for Stochastic Optimization", "abstract": "Second-order optimization methods are among the most widely used optimization approaches for convex optimization problems, and have recently been used to optimize non-convex optimization problems such as deep learning models. The widely used second-order optimization methods such as quasi-Newton methods generally provide curvature information by approximating the Hessian using the secant equation. However, the secant equation becomes insipid in approximating the Newton step owing to its use of the first-order derivatives. In this study, we propose an approximate Newton sketch-based stochastic optimization algorithm for large-scale empirical risk minimization. Specifically, we compute a partial column Hessian of size ($d\\times m$) with $m\\ll d$ randomly selected variables, then use the \\emph{Nystr\\\"om method} to better approximate the full Hessian matrix. To further reduce the computational complexity per iteration, we directly compute the update step ($\\Delta\\boldsymbol{w}$) without computing and storing the full Hessian or its inverse. We then integrate our approximated Hessian with stochastic gradient descent and stochastic variance-reduced gradient methods. The results of numerical experiments on both convex and non-convex functions show that the proposed approach was able to obtain a better approximation of Newton\\textquotesingle s method, exhibiting performance competitive with that of state-of-the-art first-order and stochastic quasi-Newton methods. Furthermore, we provide a theoretical convergence analysis for convex functions."}}
{"id": "PvZqCDCen_E", "cdate": 1601308068983, "mdate": null, "content": {"title": "FsNet: Feature Selection Network on High-dimensional Biological Data ", "abstract": "Biological data including gene expression data are generally high-dimensional and require efficient, generalizable, and scalable machine-learning methods to discover their complex nonlinear patterns. The recent advances in machine learning can be attributed to deep neural networks (DNNs), which excel in various tasks in terms of computer vision and natural language processing. However, standard DNNs are not appropriate for high-dimensional datasets generated in biology because they have many parameters, which in turn require many samples. In this paper, we propose a DNN-based, nonlinear feature selection method, called the feature selection network (FsNet), for high-dimensional and small number of sample data. Specifically, FsNet comprises a selection layer that selects features and a reconstruction layer that stabilizes the training. Because a large number of parameters in the selection and reconstruction layers can easily result in overfitting under a limited number of samples, we use two tiny networks to predict the large, virtual weight matrices of the selection and reconstruction layers. Experimental results on several real-world, high-dimensional biological datasets demonstrate the efficacy of the proposed method."}}
{"id": "hSBG5A6not", "cdate": 1577836800000, "mdate": null, "content": {"title": "FsNet: Feature Selection Network on High-dimensional Biological Data", "abstract": "Biological data including gene expression data are generally high-dimensional and require efficient, generalizable, and scalable machine-learning methods to discover their complex nonlinear patterns. The recent advances in machine learning can be attributed to deep neural networks (DNNs), which excel in various tasks in terms of computer vision and natural language processing. However, standard DNNs are not appropriate for high-dimensional datasets generated in biology because they have many parameters, which in turn require many samples. In this paper, we propose a DNN-based, nonlinear feature selection method, called the feature selection network (FsNet), for high-dimensional and small number of sample data. Specifically, FsNet comprises a selection layer that selects features and a reconstruction layer that stabilizes the training. Because a large number of parameters in the selection and reconstruction layers can easily result in overfitting under a limited number of samples, we use two tiny networks to predict the large, virtual weight matrices of the selection and reconstruction layers. Experimental results on several real-world, high-dimensional biological datasets demonstrate the efficacy of the proposed method."}}
{"id": "5NfPKxfkWEv", "cdate": 1577836800000, "mdate": 1680077052835, "content": {"title": "Real-Time Detection of Motorcyclist without Helmet using Cascade of CNNs on Edge-device", "abstract": ""}}
{"id": "1eLL5yparcv", "cdate": 1577836800000, "mdate": null, "content": {"title": "GraphLIME: Local Interpretable Model Explanations for Graph Neural Networks", "abstract": "Graph structured data has wide applicability in various domains such as physics, chemistry, biology, computer vision, and social networks, to name a few. Recently, graph neural networks (GNN) were shown to be successful in effectively representing graph structured data because of their good performance and generalization ability. GNN is a deep learning based method that learns a node representation by combining specific nodes and the structural/topological information of a graph. However, like other deep models, explaining the effectiveness of GNN models is a challenging task because of the complex nonlinear transformations made over the iterations. In this paper, we propose GraphLIME, a local interpretable model explanation for graphs using the Hilbert-Schmidt Independence Criterion (HSIC) Lasso, which is a nonlinear feature selection method. GraphLIME is a generic GNN-model explanation framework that learns a nonlinear interpretable model locally in the subgraph of the node being explained. More specifically, to explain a node, we generate a nonlinear interpretable model from its $N$-hop neighborhood and then compute the K most representative features as the explanations of its prediction using HSIC Lasso. Through experiments on two real-world datasets, the explanations of GraphLIME are found to be of extraordinary degree and more descriptive in comparison to the existing explanation methods."}}
{"id": "uZ6dnCKlfW", "cdate": 1546300800000, "mdate": 1680077052904, "content": {"title": "Deep Spatio-Temporal Representation for Detection of Road Accidents Using Stacked Autoencoder", "abstract": ""}}
{"id": "D24Z1q3qU3", "cdate": 1514764800000, "mdate": 1680077052836, "content": {"title": "Fast-BoW: Scaling Bag-of-Visual-Words Generation", "abstract": ""}}
{"id": "3cKi8oxb7J-", "cdate": 1514764800000, "mdate": 1680077052824, "content": {"title": "Projection-SVM: Distributed Kernel Support Vector Machine for Big Data using Subspace Partitioning", "abstract": ""}}
{"id": "vsnXMONVON", "cdate": 1483228800000, "mdate": 1667312856447, "content": {"title": "DiP-SVM : Distribution Preserving Kernel Support Vector Machine for Big Data", "abstract": "In literature, the task of learning a support vector machine for large datasets has been performed by splitting the dataset into manageable sized \u201cpartitions\u201d and training a sequential support vector machine on each of these partitions separately to obtain local support vectors. However, this process invariably leads to the loss in classification accuracy as global support vectors may not have been chosen as local support vectors in their respective partitions. We hypothesize that retaining the original distribution of the dataset in each of the partitions can help solve this issue. Hence, we present DiP-SVM, a distribution preserving kernel support vector machine where the first and second order statistics of the entire dataset are retained in each of the partitions. This helps in obtaining local decision boundaries which are in agreement with the global decision boundary, thereby reducing the chance of missing important global support vectors. We show that DiP-SVM achieves a minimal loss in classification accuracy among other distributed support vector machine techniques on several benchmark datasets. We further demonstrate that our approach reduces communication overhead between partitions leading to faster execution on large datasets and making it suitable for implementation in cloud environments."}}
{"id": "jBCY0kICfi", "cdate": 1483228800000, "mdate": 1680077052975, "content": {"title": "Graph formulation of video activities for abnormal activity recognition", "abstract": ""}}
