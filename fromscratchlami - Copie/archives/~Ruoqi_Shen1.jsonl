{"id": "rnFOPhTMB0Y", "cdate": 1663850521825, "mdate": null, "content": {"title": "How to fine-tune vision models with SGD", "abstract": "SGD (with momentum) and AdamW are the two most commonly used optimizers for fine-tuning large neural networks in computer vision. When the two methods perform the same, SGD is preferable because it uses less memory and is more efficient than AdamW. However, when evaluating on  downstream tasks that differ significantly from pretraining, we find that across five popular benchmarks SGD fine-tuning gets substantially lower accuracies than AdamW on many modern vision models such as Vision Transformers and ConvNeXts---especially out-of-distribution (OOD). We find that such large gaps arise in instances where the fine-tuning gradients in the first (``embedding'') layer are much larger than the rest of the model. Our analysis suggests an easy fix: if we simply freeze the embedding layer (0.7\\% of the parameters), SGD performs competitively with AdamW while using less memory across a suite of benchmarks. Our insights lead to state-of-the-art accuracies on popular distribution shift benchmarks: WILDS-FMoW, WILDS-Camelyon, BREEDS-Living-17, Waterbirds, and DomainNet."}}
{"id": "tmer8WAEzV", "cdate": 1652737557498, "mdate": null, "content": {"title": "Sampling with Riemannian Hamiltonian Monte Carlo in a Constrained Space", "abstract": "We demonstrate for the first time that ill-conditioned, non-smooth, constrained distributions in very high dimension, upwards of 100,000, can be sampled efficiently \\emph{in practice}. Our algorithm incorporates constraints into the Riemannian version of Hamiltonian Monte Carlo and maintains sparsity. This allows us to achieve a mixing rate independent of smoothness and condition numbers. On benchmark data sets in systems biology and linear programming, our algorithm outperforms existing packages by orders of magnitude. In particular, we achieve a 1,000-fold speed-up for sampling from the largest published human metabolic network (RECON3D). Our package has been incorporated into a popular Bioinformatics library."}}
{"id": "q2nJyb3cvR9", "cdate": 1652737465073, "mdate": null, "content": {"title": "Near-Optimal Randomized Exploration for Tabular Markov Decision Processes", "abstract": "We study algorithms using randomized value functions for exploration in reinforcement learning. This type of algorithms enjoys appealing empirical performance. We show that when we use 1) a single random seed in each episode, and 2) a Bernstein-type magnitude of noise, we obtain a worst-case $\\widetilde{O}\\left(H\\sqrt{SAT}\\right)$ regret bound for episodic time-inhomogeneous Markov Decision Process where $S$ is the size of state space, $A$ is the size of action space, $H$ is the planning horizon and $T$ is the number of interactions. This bound polynomially improves all existing bounds for algorithms based on randomized value functions, and for the first time, matches the $\\Omega\\left(H\\sqrt{SAT}\\right)$ lower bound up to logarithmic factors. Our result highlights that randomized exploration can be near-optimal, which was previously achieved only by optimistic algorithms. To achieve the desired result, we develop 1) a new clipping operation to ensure both the probability of being optimistic and the probability of being pessimistic are lower bounded by a constant, and 2) a new recursive  formula for the absolute value of estimation errors to analyze the regret."}}
{"id": "SnEez8-mrl5", "cdate": 1645715785876, "mdate": 1645715785876, "content": {"title": "Analysis of Langevin Monte Carlo from Poincar\u00e9 to Log-Sobolev", "abstract": "Classically, the continuous-time Langevin diffusion converges exponentially fast to its stationary distribution \u03c0 under the sole assumption that \u03c0 satisfies a Poincar\u00e9 inequality. Using this fact to provide guarantees for the discrete-time Langevin Monte Carlo (LMC) algorithm, however, is considerably more challenging due to the need for working with chi-squared or R\u00e9nyi divergences, and prior works have largely focused on strongly log-concave targets. In this work, we provide the first convergence guarantees for LMC assuming that \u03c0 satisfies either a Lata\u0142a--Oleszkiewicz or modified log-Sobolev inequality, which interpolates between the Poincar\u00e9 and log-Sobolev settings. Unlike prior works, our results allow for weak smoothness and do not require convexity or dissipativity conditions."}}
{"id": "LQCUmLgFlR", "cdate": 1632875721577, "mdate": null, "content": {"title": "On Optimal Early Stopping: Overparametrization versus Underparametrization", "abstract": "Early stopping is a simple and widely used method to prevent over-training neural networks. We develop theoretical results to reveal the relationship between optimal early stopping time and model dimension as well as sample size of the dataset for certain linear regression models. Our results demonstrate two very different behaviors when the model dimension exceeds the number of features versus the opposite scenario. While most previous works on linear models focus on the latter setting, we observe that in common deep learning tasks, the dimension of the model often exceeds the number of features arising from data. We demonstrate experimentally that our theoretical results on optimal early stopping time corresponds to the training process of deep neural network. Moreover, we study the effect of early stopping on generalization and demonstrate that optimal early stopping can help mitigate ''descent'' in various settings."}}
{"id": "napaTaDQ0lY", "cdate": 1621630124254, "mdate": null, "content": {"title": "Lower Bounds on Metropolized Sampling Methods for Well-Conditioned Distributions", "abstract": "We give lower bounds on the performance of two of the most popular sampling methods in practice, the Metropolis-adjusted Langevin algorithm (MALA) and multi-step Hamiltonian Monte Carlo (HMC) with a leapfrog integrator, when applied to well-conditioned distributions. Our main result is a nearly-tight lower bound of $\\widetilde{\\Omega}(\\kappa d)$ on the mixing time of MALA from an exponentially warm start, matching a line of algorithmic results \\cite{DwivediCW018, ChenDWY19, LeeST20a} up to logarithmic factors and answering an open question of \\cite{ChewiLACGR20}. We also show that a polynomial dependence on dimension is necessary for the relaxation time of HMC under any number of leapfrog steps, and bound the gains achievable by changing the step count. Our HMC analysis draws upon a novel connection between leapfrog integration and Chebyshev polynomials, which may be of independent interest."}}
{"id": "B1geUNBgIr", "cdate": 1567802439702, "mdate": null, "content": {"title": "The Randomized Midpoint Method for Log-Concave Sampling", "abstract": "Sampling from log-concave distributions is a well researched problem that has many applications in statistics and machine learning. We study the distributions of the form $p^{*}\\propto\\exp(-f(x))$, where $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ has an $L$-Lipschitz gradient and is $m$-strongly convex. In our paper, we propose a Markov chain Monte Carlo (MCMC) algorithm based on the underdamped Langevin diffusion (ULD). It can achieve $\\epsilon\\cdot D$ error (in 2-Wasserstein distance) in $\\tilde{O}\\left(\\kappa^{7/6}/\\epsilon^{1/3}+\\kappa/\\epsilon^{2/3}\\right)$ steps, where $D\\defeq\\sqrt{\\frac{d}{m}}$ is the effective diameter of the problem and $\\kappa\\defeq\\frac{L}{m}$ is the condition number. Our algorithm performs significantly faster than the previously best ULD based one, which needs $\\tilde{O}\\left(\\kappa^{2}/\\epsilon\\right)$ steps [#cheng2017underdamped]. It also significantly outperforms the best known algorithm for solving this problem, which is based on Hamiltonian Monte Carlo (HMC) and requires $\\tilde{O}\\left(\\kappa^{1.5}/\\epsilon\\right)$ steps [#chen2019optimal]. Moreover, our algorithm can be easily parallelized to require only $O(\\kappa\\log\\frac{1}{\\epsilon})$ parallel steps.   To solve the sampling problem, we propose a new framework to discretize stochastic differential equations. We applies this framework to discretize and simulate ULD, which converges to the target distribution $p^{*}$. The framework can be used to solve not only the log-concave sampling problem, but any problems that involve simulating (stochastic) differential equations."}}
