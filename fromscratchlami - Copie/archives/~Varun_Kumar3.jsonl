{"id": "hcpMe4g88Ch", "cdate": 1686242011749, "mdate": 1686242011749, "content": {"title": "A Static Evaluation of Code Completion by Large Language Models", "abstract": "Large language models trained on code have shown great potential to increase productivity of software developers. Several execution-based benchmarks have been proposed to evaluate functional correctness of model-generated code on simple programming problems. Nevertheless, it is expensive to perform the same evaluation on complex real-world projects considering the execution cost. On the contrary, static analysis tools such as linters, which can detect errors without running the program, haven't been well explored for evaluating code generation models. In this work, we propose a static evaluation framework to quantify static errors in Python code completions, by leveraging Abstract Syntax Trees. Compared with execution-based evaluation, our method is not only more efficient, but also applicable to code in the wild. For experiments, we collect code context from open source repos to generate one million function bodies using public models. Our static analysis reveals that Undefined Name and Unused Variable are the most common errors among others made by language models. Through extensive studies, we also show the impact of sampling temperature, model size, and context on static errors in code completions."}}
{"id": "EiwlLLPgAsb", "cdate": 1674441130343, "mdate": 1674441130343, "content": {"title": "ReCode: Robustness Evaluation of Code Generation Models", "abstract": "Code generation models have achieved impressive performance. However, they tend to be brittle as slight edits to a prompt could lead to very different generations; these robustness properties, critical for user experience when deployed in real-life applications, are not well understood. Most existing works on robustness in text or code tasks have focused on classification, while robustness in generation tasks is an uncharted area and to date there is no comprehensive benchmark for robustness in code generation. In this paper, we propose ReCode, a comprehensive robustness evaluation benchmark for code generation models. We customize over 30 transformations specifically for code on docstrings, function and variable names, code syntax, and code format. They are carefully designed to be natural in real-life coding practice, preserve the original semantic meaning, and thus provide multifaceted assessments of a model's robustness performance. With human annotators, we verified that over 90% of the perturbed prompts do not alter the semantic meaning of the original prompt. In addition, we define robustness metrics for code generation models considering the worst-case behavior under each type of perturbation, taking advantage of the fact that executing the generated code can serve as objective evaluation. We demonstrate ReCode on SOTA models using HumanEval, MBPP, as well as function completion tasks derived from them. Interesting observations include: better robustness for CodeGen over InCoder and GPT-J; models are most sensitive to syntax perturbations; more challenging robustness evaluation on MBPP over HumanEval.\n"}}
{"id": "Bo7eeXm6An8", "cdate": 1663849880481, "mdate": null, "content": {"title": "Multi-lingual Evaluation of Code Generation Models", "abstract": "We present two new benchmarks, MBXP and Multilingual HumanEval, designed to evaluate code completion models in over 10 programming languages. These datasets are generated using a conversion framework that transpiles prompts and test cases from the original MBPP and HumanEval datasets into the corresponding data in the target language. By using these benchmarks, we are able to assess the performance of code generation models in a multi-lingual fashion, and discovered generalization ability of language models on out-of-domain languages, advantages of multi-lingual models over mono-lingual, the ability of  few-shot prompting to teach the model new languages, and zero-shot translation abilities. In addition, we use our code generation model to perform large-scale bootstrapping to obtain synthetic canonical solutions in several languages, which can be used for other code-related evaluations such as code insertion, robustness, or summarization tasks."}}
{"id": "vK5SYXibeqi", "cdate": 1640995200000, "mdate": 1654279439045, "content": {"title": "Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal", "abstract": "Umang Gupta, Jwala Dhamala, Varun Kumar, Apurv Verma, Yada Pruksachatkun, Satyapriya Krishna, Rahul Gupta, Kai-Wei Chang, Greg Ver Steeg, Aram Galstyan. Findings of the Association for Computational Linguistics: ACL 2022. 2022."}}
{"id": "hmXULiP_XV", "cdate": 1640995200000, "mdate": 1654279439043, "content": {"title": "Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal", "abstract": "Language models excel at generating coherent text, and model compression techniques such as knowledge distillation have enabled their use in resource-constrained settings. However, these models can be biased in multiple ways, including the unfounded association of male and female genders with gender-neutral professions. Therefore, knowledge distillation without any fairness constraints may preserve or exaggerate the teacher model's biases onto the distilled model. To this end, we present a novel approach to mitigate gender disparity in text generation by learning a fair model during knowledge distillation. We propose two modifications to the base knowledge distillation based on counterfactual role reversal$\\unicode{x2014}$modifying teacher probabilities and augmenting the training set. We evaluate gender polarity across professions in open-ended text generated from the resulting distilled and finetuned GPT$\\unicode{x2012}$2 models and demonstrate a substantial reduction in gender disparity with only a minor compromise in utility. Finally, we observe that language models that reduce gender polarity in language generation do not improve embedding fairness or downstream classification fairness."}}
{"id": "c5r5fLpfMh", "cdate": 1640995200000, "mdate": 1654279439043, "content": {"title": "On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations", "abstract": "Yang Cao, Yada Pruksachatkun, Kai-Wei Chang, Rahul Gupta, Varun Kumar, Jwala Dhamala, Aram Galstyan. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 2022."}}
{"id": "JJiA7A3iQK", "cdate": 1640995200000, "mdate": 1654279439039, "content": {"title": "On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations", "abstract": "Multiple metrics have been introduced to measure fairness in various natural language processing tasks. These metrics can be roughly categorized into two categories: 1) \\emph{extrinsic metrics} for evaluating fairness in downstream applications and 2) \\emph{intrinsic metrics} for estimating fairness in upstream contextualized language representation models. In this paper, we conduct an extensive correlation study between intrinsic and extrinsic metrics across bias notions using 19 contextualized language models. We find that intrinsic and extrinsic metrics do not necessarily correlate in their original setting, even when correcting for metric misalignments, noise in evaluation datasets, and confounding factors such as experiment configuration for extrinsic metrics. %al"}}
{"id": "71-wmuyAYkK", "cdate": 1633716134263, "mdate": 1633716134263, "content": {"title": "BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation", "abstract": "Recent advances in deep learning techniques have enabled machines to generate cohesive open-ended text when prompted with\na sequence of words as context. While these models now empower\nmany downstream applications from conversation bots to automatic storytelling, they have been shown to generate texts that\nexhibit social biases. To systematically study and benchmark social\nbiases in open-ended language generation, we introduce the Bias\nin Open-Ended Language Generation Dataset (BOLD), a large-scale\ndataset that consists of 23,679 English text generation prompts for\nbias benchmarking across five domains: profession, gender, race,\nreligion, and political ideology. We also propose new automated\nmetrics for toxicity, psycholinguistic norms, and text gender polarity to measure social biases in open-ended text generation from\nmultiple angles. An examination of text generated from three popular language models reveals that the majority of these models\nexhibit a larger social bias than human-written Wikipedia text\nacross all domains. With these results we highlight the need to\nbenchmark biases in open-ended language generation and caution\nusers of language generation models on downstream tasks to be\ncognizant of these embedded prejudices."}}
{"id": "GNY4ff0vhgx", "cdate": 1609459200000, "mdate": 1635212866231, "content": {"title": "Protoda: Efficient Transfer Learning for Few-Shot Intent Classification", "abstract": "Practical sequence classification tasks in natural language processing often suffer from low training data availability for target classes. Recent works towards mitigating this problem have focused on transfer learning using embeddings pre-trained on often unrelated tasks, for instance, language modeling. We adopt an alternative approach by transfer learning on an ensemble of related tasks using prototypical networks under the meta-learning paradigm. Using intent classification as a case study, we demonstrate that increasing variability in training tasks can significantly improve classification performance. Further, we apply data augmentation in conjunction with meta-learning to reduce sampling bias. We make use of a conditional generator for data augmentation that is trained directly using the meta-learning objective and simultaneously with prototypical networks, hence ensuring that data augmentation is customized to the task. We explore augmentation in the sentence embedding space as well as prototypical embedding space. Combining meta-learning with augmentation provides upto 6.49% and 8.53% relative F1-score improvements over the best performing systems in the 5-shot and 10-shot learning, respectively."}}
{"id": "AJXRF9gV-JE", "cdate": 1609459200000, "mdate": 1631298142963, "content": {"title": "BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation", "abstract": "Recent advances in deep learning techniques have enabled machines to generate cohesive open-ended text when prompted with a sequence of words as context. While these models now empower many downstream applications from conversation bots to automatic storytelling, they have been shown to generate texts that exhibit social biases. To systematically study and benchmark social biases in open-ended language generation, we introduce the Bias in Open-Ended Language Generation Dataset (BOLD), a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains: profession, gender, race, religion, and political ideology. We also propose new automated metrics for toxicity, psycholinguistic norms, and text gender polarity to measure social biases in open-ended text generation from multiple angles. An examination of text generated from three popular language models reveals that the majority of these models exhibit a larger social bias than human-written Wikipedia text across all domains. With these results we highlight the need to benchmark biases in open-ended language generation and caution users of language generation models on downstream tasks to be cognizant of these embedded prejudices."}}
