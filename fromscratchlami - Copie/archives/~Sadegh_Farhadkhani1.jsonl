{"id": "w58srxMIwb", "cdate": 1672531200000, "mdate": 1690897805414, "content": {"title": "On the Strategyproofness of the Geometric Median", "abstract": "The geometric median, an instrumental component of the secure machine learning toolbox, is known to be effective when robustly aggregating models (or gradients), gathered from potentially malicious..."}}
{"id": "KKfhszEi7T", "cdate": 1672531200000, "mdate": 1690897805418, "content": {"title": "Fixing by Mixing: A Recipe for Optimal Byzantine ML under Heterogeneity", "abstract": "Byzantine machine learning (ML) aims to ensure the resilience of distributed learning algorithms to misbehaving (or Byzantine) machines. Although this problem received significant attention, prior ..."}}
{"id": "7OLSwgBV4Iy", "cdate": 1672531200000, "mdate": 1679948936199, "content": {"title": "Fixing by Mixing: A Recipe for Optimal Byzantine ML under Heterogeneity", "abstract": ""}}
{"id": "ZRHcxl5U45r", "cdate": 1640995200000, "mdate": 1659630754416, "content": {"title": "An Equivalence Between Data Poisoning and Byzantine Gradient Attacks", "abstract": "To study the resilience of distributed learning, the \u201cByzantine\" literature considers a strong threat model where workers can report arbitrary gradients to the parameter server. Whereas this model helped obtain several fundamental results, it has sometimes been considered unrealistic, when the workers are mostly trustworthy machines. In this paper, we show a surprising equivalence between this model and data poisoning, a threat considered much more realistic. More specifically, we prove that every gradient attack can be reduced to data poisoning, in any personalized federated learning system with PAC guarantees (which we show are both desirable and realistic). This equivalence makes it possible to obtain new impossibility results on the resilience of any \u201crobust\u201d learning algorithm to data poisoning in highly heterogeneous applications, as corollaries of existing impossibility theorems on Byzantine machine learning. Moreover, using our equivalence, we derive a practical attack that we show (theoretically and empirically) can be very effective against classical personalized federated learning models."}}
{"id": "BnHLPGFTpe", "cdate": 1640995200000, "mdate": 1659630754414, "content": {"title": "Byzantine Machine Learning Made Easy By Resilient Averaging of Momentums", "abstract": "Byzantine resilience emerged as a prominent topic within the distributed machine learning community. Essentially, the goal is to enhance distributed optimization algorithms, such as distributed SGD..."}}
{"id": "7pZiaojaVGU", "cdate": 1632875536856, "mdate": null, "content": {"title": "An Equivalence Between Data Poisoning and Byzantine Gradient Attacks", "abstract": "To address the resilience of distributed learning, the ``Byzantine\" literature considers a strong threat model where workers can report arbitrary gradients to the parameter server. While this model helped generate several fundamental results, it has however sometimes been considered unrealistic, when the workers are mostly trustworthy machines. In this paper, we show a surprising equivalence between this model and data poisoning, a threat considered much more realistic. More specifically, we prove that any gradient attack can be reduced to data poisoning in a personalized federated learning system that provides PAC guarantees (which we show are both desirable and realistic in various personalized federated learning contexts such as linear regression and classification). Maybe most importantly, we derive a simple and practical attack that may be constructed against classical personalized federated learning models, and we show both theoretically and empirically the effectiveness of this attack."}}
{"id": "O8wI1avs4WF", "cdate": 1621629853003, "mdate": null, "content": {"title": "Collaborative Learning in the Jungle (Decentralized, Byzantine, Heterogeneous, Asynchronous and Nonconvex Learning)", "abstract": "We study \\emph{Byzantine collaborative learning}, where $n$ nodes seek to collectively learn from each others' local data. The data distribution may vary from one node to another. No node is trusted, and $f < n$ nodes can behave arbitrarily. We prove that collaborative learning is equivalent to a new form of agreement, which we call \\emph{averaging agreement}. In this problem, nodes start each with an initial vector and seek to approximately agree on a common vector, which is close to the average of honest nodes' initial vectors.  We present two asynchronous solutions to averaging agreement, each we prove optimal according to some dimension. The first, based on the minimum-diameter averaging, requires $n \\geq 6f+1$, but achieves asymptotically the best-possible averaging constant up to a multiplicative constant. The second, based on reliable broadcast and coordinate-wise trimmed mean, achieves optimal Byzantine resilience, i.e.,  $n \\geq 3f+1$. Each of these algorithms induces an optimal Byzantine collaborative learning protocol. In particular, our equivalence yields new impossibility theorems on what any collaborative learning algorithm can achieve in adversarial and heterogeneous environments."}}
{"id": "V5bTFIrPFlv", "cdate": 1609459200000, "mdate": 1659630754415, "content": {"title": "Collaborative Learning in the Jungle (Decentralized, Byzantine, Heterogeneous, Asynchronous and Nonconvex Learning)", "abstract": "We study \\emph{Byzantine collaborative learning}, where $n$ nodes seek to collectively learn from each others' local data. The data distribution may vary from one node to another. No node is trusted, and $f &lt; n$ nodes can behave arbitrarily. We prove that collaborative learning is equivalent to a new form of agreement, which we call \\emph{averaging agreement}. In this problem, nodes start each with an initial vector and seek to approximately agree on a common vector, which is close to the average of honest nodes' initial vectors. We present two asynchronous solutions to averaging agreement, each we prove optimal according to some dimension. The first, based on the minimum-diameter averaging, requires $n \\geq 6f+1$, but achieves asymptotically the best-possible averaging constant up to a multiplicative constant. The second, based on reliable broadcast and coordinate-wise trimmed mean, achieves optimal Byzantine resilience, i.e., $n \\geq 3f+1$. Each of these algorithms induces an optimal Byzantine collaborative learning protocol. In particular, our equivalence yields new impossibility theorems on what any collaborative learning algorithm can achieve in adversarial and heterogeneous environments."}}
