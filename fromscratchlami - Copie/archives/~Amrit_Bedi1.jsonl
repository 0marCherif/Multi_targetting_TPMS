{"id": "EvuAJ0wD98", "cdate": 1686324868244, "mdate": null, "content": {"title": "Intent-Aware Planning in Heterogeneous Traffic via Distributed Multi-Agent Reinforcement Learning", "abstract": "Navigating safely and efficiently in dense and heterogeneous traffic scenarios is challenging for autonomous vehicles (AVs) due to their inability to infer the behaviors or intentions of nearby drivers. In this work, we introduce a distributed multi-agent reinforcement learning (MARL) algorithm for joint trajectory and intent prediction for autonomous vehicles in dense and heterogeneous environments. Our approach for intent-aware planning, iPLAN, allows agents to infer nearby drivers' intents solely from their local observations. We model an explicit representation of agents' private incentives: Behavioral Incentive for high-level decision-making strategy that sets planning sub-goals and Instant Incentive for low-level motion planning to execute sub-goals. Our approach enables agents to infer their opponents' behavior incentives and integrate this inferred information into their decision-making and motion-planning processes. We perform experiments on two simulation environments, Non-Cooperative Navigation and Heterogeneous Highway. In Heterogeneous Highway, results show that, compared with centralized training decentralized execution (CTDE) MARL baselines such as QMIX and MAPPO, our method yields a $4.3\\%$ and $38.4\\%$ higher episodic reward in mild and chaotic traffic, with $48.1\\%$ higher success rate and $80.6\\%$ longer survival time in chaotic traffic. We also compare with a decentralized training decentralized execution (DTDE) baseline IPPO and demonstrate a higher episodic reward of $12.7\\%$ and $6.3\\%$ in mild traffic and chaotic traffic, $25.3\\%$ higher success rate, and $13.7\\%$ longer survival time."}}
{"id": "VJgxHKjj7e", "cdate": 1679929747077, "mdate": 1679929747077, "content": {"title": "Beyond Exponentially Fast Mixing in Average-Reward Reinforcement Learning via Multi-Level Monte Carlo Actor-Critic", "abstract": "Many existing reinforcement learning (RL) methods\nemploy stochastic gradient iteration on the\nback end, whose stability hinges upon a hypothesis\nthat the data-generating process mixes exponentially\nfast with a rate parameter that appears\nin the step-size selection. Unfortunately, this assumption\nis violated for large state spaces or settings\nwith sparse rewards, and the mixing time is\nunknown, making the step size inoperable. In this\nwork, we propose an RL methodology attuned\nto the mixing time by employing a multi-level\nMonte Carlo estimator for the critic, the actor,\nand the average reward embedded within an actor critic\n(AC) algorithm. This method, which we\ncall Multi-level Actor-Critic (MAC), is developed\nespecially for infinite-horizon average-reward settings\nand neither relies on oracle knowledge of\nthe mixing time in its parameter selection nor assumes\nits exponential decay; it, therefore, is readily\napplicable to applications with slower mixing\ntimes. Nonetheless, it achieves a convergence rate\ncomparable to the state-of-the-art AC algorithms.\nWe experimentally show that these alleviated restrictions\non the technical conditions required for\nstability translate to superior performance in practice\nfor RL problems with sparse rewards."}}
{"id": "IBpCWGpR4k", "cdate": 1667331959690, "mdate": null, "content": {"title": "STOCHASTIC BILEVEL PROJECTION-FREE OPTIMIZATION", "abstract": "Bi-level optimization is a powerful framework to solve a\nrich class of problems such as hyper-parameter optimization,\nmodel-agnostic meta-learning, data distillation, and matrix\ncompletion. The existing first-order solutions to bi-level\nproblems exhibit scalability limitations (for example, in matrix completion) because of the requirement of projecting\nsolutions onto the feasible set. In this work, we propose a\nnovel Stochastic Bi-level Frank-Wolfe (SBFW) algorithm\nto solve the stochastic bi-level optimization problems in a\nprojection-free manner. We utilize a momentum-based gradient tracker that results in a sample complexity of O(\u03f5\u22123) for\nconvex outer objectives with strongly convex inner objectives.\nWe formulate the matrix completion problem with denoising\nas a stochastic bilevel problem and show that SBFW outperforms the state-of-the-art methods for the problem of matrix\ncompletion with denoising and achieves improvements of up\nto 82% in terms of the wall-clock time required to achieve the\nsame level of accuracy"}}
{"id": "GdGpM3VWWXD", "cdate": 1664310940589, "mdate": null, "content": {"title": "Posterior Coreset Construction with Kernelized Stein Discrepancy for Model-Based Reinforcement Learning", "abstract": "Model-based reinforcement learning (MBRL) exhibits favorable performance in practice, but its theoretical guarantees are mostly restricted to the setting when the transition model is Gaussian or Lipschitz and demands a posterior estimate whose representational complexity grows unbounded with time. In this work, we develop a novel MBRL method (i) which relaxes the assumptions on the target transition model to belong to a generic family of mixture models; (ii) is applicable to large-scale training by incorporating a compression step such that the posterior estimate consists of a \\emph{Bayesian coreset} of only statistically significant past state-action pairs; and (iii) {exhibits a Bayesian regret of $\\mathcal{O}(dH^{1+({\\alpha}/{2})}T^{1-({\\alpha}/{2})})$ with coreset size of $\\Omega(\\sqrt{T^{1+\\alpha}})$, where $d$ is the aggregate dimension of state action space, $H$ is the episode length, $T$ is the total number of time steps experienced, and $\\alpha\\in (0,1]$ is the tuning parameter which is a novel introduction into the analysis of MBRL in this work}. To achieve these results, we adopt an approach based upon Stein's method, which allows distributional distance to be evaluated in closed form as the kernelized Stein discrepancy (KSD). Experimentally, we observe that this approach is competitive with several state-of-the-art RL methodologies, and can achieve up to $50\\%$ reduction in wall clock time in some continuous control environments."}}
{"id": "sakN4NI67sb", "cdate": 1663939402239, "mdate": null, "content": {"title": "SWIFT: Rapid Decentralized Federated Learning via Wait-Free Model Communication", "abstract": "The decentralized Federated Learning (FL) setting avoids the role of a potentially unreliable or untrustworthy central host by utilizing groups of clients to collaboratively train a model via localized training and model/gradient sharing. Most existing decentralized FL algorithms require synchronization of client models where the speed of synchronization depends upon the slowest client. In this work, we propose SWIFT: a novel wait-free decentralized FL algorithm that allows clients to conduct training at their own speed. Theoretically, we prove that SWIFT matches the gold-standard iteration convergence rate $\\mathcal{O}(1/\\sqrt{T})$ of parallel stochastic gradient descent for convex and non-convex smooth optimization (total iterations $T$). Furthermore, we provide theoretical results for IID and non-IID settings without any bounded-delay assumption for slow clients which is required by other asynchronous decentralized FL algorithms. Although SWIFT achieves the same iteration convergence rate with respect to $T$ as other state-of-the-art (SOTA) parallel stochastic algorithms, it converges faster with respect to run-time due to its wait-free structure. Our experimental results demonstrate that SWIFT's run-time is reduced due to a large reduction in communication time per epoch, which falls by an order of magnitude compared to synchronous counterparts. Furthermore, SWIFT produces loss levels for image classification, over IID and non-IID data settings, upwards of 50% faster than existing SOTA algorithms."}}
{"id": "jh1nCir1R3d", "cdate": 1663850056071, "mdate": null, "content": {"title": "SWIFT: Rapid Decentralized Federated Learning via Wait-Free Model Communication", "abstract": "The decentralized Federated Learning (FL) setting avoids the role of a potentially unreliable or untrustworthy central host by utilizing groups of clients to collaboratively train a model via localized training and model/gradient sharing. Most existing decentralized FL algorithms require synchronization of client models where the speed of synchronization depends upon the slowest client. In this work, we propose SWIFT: a novel wait-free decentralized FL algorithm that allows clients to conduct training at their own speed. Theoretically, we prove that SWIFT matches the gold-standard iteration convergence rate $\\mathcal{O}(1/\\sqrt{T})$ of parallel stochastic gradient descent for convex and non-convex smooth optimization (total iterations $T$). Furthermore, we provide theoretical results for IID and non-IID settings without any bounded-delay assumption for slow clients which is required by other asynchronous decentralized FL algorithms. Although SWIFT achieves the same iteration convergence rate with respect to $T$ as other state-of-the-art (SOTA) parallel stochastic algorithms, it converges faster with respect to runtime due to its wait-free structure. Our experimental results demonstrate that SWIFT's runtime is reduced due to a large reduction in communication time per epoch, which falls by an order of magnitude compared to synchronous counterparts. Furthermore, SWIFT produces loss levels for image classification, over IID and non-IID data settings, upwards of 50\\% faster than existing SOTA algorithms."}}
{"id": "uPWhEXjyvoo", "cdate": 1655376341012, "mdate": null, "content": {"title": "HTRON: Efficient Outdoor Navigation with Sparse Rewards via  Heavy Tailed Adaptive Reinforce Algorithm", "abstract": "We present a novel approach to improve the performance of deep reinforcement learning (DRL) based outdoor robot navigation systems. Most, existing DRL methods are based on carefully designed dense reward functions that learn the efficient behavior in an environment. \u00a0We circumvent this issue by working only with sparse rewards (which are easy to design) and propose a novel adaptive Heavy-Tailed Reinforce algorithm for Outdoor Navigation called HTRON. Our main idea is to utilize heavy-tailed policy parametrizations which implicitly induce exploration in sparse reward settings. We evaluate the performance of HTRON against Reinforce, PPO, and TRPO algorithms in three different outdoor scenarios: goal-reaching, obstacle avoidance, and uneven terrain navigation. We observe average an increase of 34.41% in terms of success rate, a 15.15% decrease in the average time steps taken to reach the goal, and a 24.9% decrease in the elevation cost compared to the navigation policies obtained by the other methods. Further, we demonstrate that our algorithm can be transferred directly into a Clearpath Husky robot to perform outdoor terrain navigation in real-world scenarios."}}
{"id": "N4kr2nCeiPv", "cdate": 1591624052972, "mdate": null, "content": {"title": "Efficient Large-Scale Gaussian Process Bandits by Believing only Informative Actions", "abstract": "In this work, we cast Bayesian optimization as a multi-armed bandit problem, where the payoff function is sampled from a Gaussian process (GP). Further, we focus on action selections via the GP upper confidence bound (UCB). While numerous prior works use GPs in bandit settings, they do not apply to settings where the total number of iterations T may be large-scale, as the complexity of computing the posterior parameters scales cubically with the number of past observations. To circumvent this computational burden, we propose a simple statistical test: only incorporate an action into the GP posterior when its conditional entropy exceeds an \u000f threshold. Doing so permits us to derive sublinear regret bounds of GP bandit algorithms up to factors depending on the compression parameter \u000f for both discrete and continuous action sets. Moreover, the complexity of the GP posterior remains provably finite. Experimentally, we observe state of the art accuracy and complexity tradeoffs for GP bandit algorithms on various hyper-parameter tuning tasks, suggesting\nthe merits of managing the complexity of GPs in bandit settings"}}
{"id": "H1lSFJ34FH", "cdate": 1571237756885, "mdate": null, "content": {"title": " A Projection Operator to Balance Consistency and Complexity in Importance Sampling", "abstract": "Importance sampling (IS) is a standard Monte Carlo (MC) tool to compute information about random variables such as moments or quantiles with unknown distributions.  IS is \nasymptotically consistent as the number of MC samples, and hence deltas (particles) that parameterize the density estimate, go to infinity. However, retaining infinitely many particles is intractable. We propose a scheme for only keeping a \\emph{finite representative subset} of particles and their augmented importance weights that is \\emph{nearly consistent}. To do so in {an online manner}, we approximate importance sampling in two ways.  First, we replace the deltas by kernels, yielding kernel density estimates (KDEs).  Second, we sequentially project KDEs onto nearby lower-dimensional subspaces. We characterize the asymptotic bias of this scheme as determined by a compression parameter and kernel bandwidth, which yields a tunable tradeoff between consistency and memory. In experiments,  we observe a favorable tradeoff between memory and accuracy, providing for the first time near-consistent compressions of arbitrary posterior distributions."}}
