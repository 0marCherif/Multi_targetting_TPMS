{"id": "cO1IH43yUF", "cdate": 1601308143558, "mdate": null, "content": {"title": "Revisiting Few-sample BERT Fine-tuning", "abstract": "This paper is a study of fine-tuning of BERT contextual representations, with focus on commonly observed instabilities in few-sample scenarios. We identify several factors that cause this instability: the common use of a non-standard optimization method with biased gradient estimation; the limited applicability of significant parts of the BERT network for down-stream tasks; and the prevalent practice of using a pre-determined, and small number of training iterations. We empirically test the impact of these factors, and identify alternative practices that resolve the commonly observed instability of the process. In light of these observations, we re-visit recently proposed methods to improve few-sample fine-tuning with BERT and re-evaluate their effectiveness. Generally, we observe the impact of these methods diminishes significantly with our modified process. "}}
{"id": "SkeHuCVFDr", "cdate": 1569439341081, "mdate": null, "content": {"title": "BERTScore: Evaluating Text Generation with BERT", "abstract": "We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task and show that BERTScore is more robust to challenging examples compared to existing metrics. "}}
{"id": "H1NpxjWdbB", "cdate": 1546300800000, "mdate": null, "content": {"title": "Simplifying Graph Convolutional Networks", "abstract": "Graph Convolutional Networks (GCNs) and their variants have experienced significant attention and have become the de facto methods for learning graph representations. GCNs derive inspiration primar..."}}
{"id": "SkVhlh09tX", "cdate": 1538087924174, "mdate": null, "content": {"title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU."}}
{"id": "Hk2aImxAb", "cdate": 1518730184646, "mdate": null, "content": {"title": "Multi-Scale Dense Networks for Resource Efficient Image Classification", "abstract": "In this paper we investigate image classification with computational resource limits at test time. Two such settings are: 1. anytime classification, where the network\u2019s prediction for a test example is progressively updated, facilitating the output of a prediction at any time; and 2. budgeted batch classification, where a fixed amount of computation is available to classify a set of examples that can be spent unevenly across \u201ceasier\u201d and \u201charder\u201d inputs. In contrast to most prior work, such as the popular Viola and Jones algorithm, our approach is based on convolutional neural networks. We train multiple classifiers with varying resource demands, which we adaptively apply during test time. To maximally re-use computation between the classifiers, we incorporate them as early-exits into a single deep convolutional neural network and inter-connect them with dense connectivity. To facilitate high quality classification early on, we use a two-dimensional multi-scale network architecture that maintains coarse and fine level features all-throughout the network. Experiments on three image-classification tasks demonstrate that our framework substantially improves the existing state-of-the-art in both settings."}}
{"id": "Sy1f0e-R-", "cdate": 1518730170534, "mdate": null, "content": {"title": "An empirical study on evaluation metrics of generative adversarial networks", "abstract": "Despite the widespread interest in generative adversarial networks (GANs), few works have studied the metrics that quantitatively evaluate GANs' performance. In this paper, we revisit several representative sample-based evaluation metrics for GANs, and address the important problem of \\emph{how to evaluate the evaluation metrics}. We start with a few necessary conditions for metrics to produce meaningful scores, such as distinguishing real from generated samples, identifying mode dropping and mode collapsing, and detecting overfitting. Then with a series of carefully designed experiments,  we are able to comprehensively investigate existing sample-based metrics and identify their strengths and limitations in practical settings. Based on these results, we observe that kernel Maximum Mean Discrepancy (MMD) and the 1-Nearest-Neighbour (1-NN) two-sample test seem to satisfy most of the desirable properties, provided that the distances between samples are computed in a suitable feature space. Our experiments also unveil interesting properties about the behavior of several popular GAN models, such as whether they are memorizing training samples, and how far these state-of-the-art GANs are from perfect."}}
{"id": "HJRV1ZZAW", "cdate": 1518730170416, "mdate": null, "content": {"title": "FAST READING COMPREHENSION WITH CONVNETS", "abstract": "State-of-the-art deep reading comprehension models are dominated by recurrent\nneural nets. Their sequential nature is a natural fit for language, but it also precludes\nparallelization within an instances and often becomes the bottleneck for\ndeploying such models to latency critical scenarios. This is particularly problematic\nfor longer texts. Here we present a convolutional architecture as an alternative\nto these recurrent architectures. Using simple dilated convolutional units in place\nof recurrent ones, we achieve results comparable to the state of the art on two\nquestion answering tasks, while at the same time achieving up to two orders of\nmagnitude speedups for question answering."}}
{"id": "SJQHZKbdZS", "cdate": 1483228800000, "mdate": null, "content": {"title": "On Fairness and Calibration", "abstract": "The machine learning community has become increasingly concerned with the potential for bias and discrimination in predictive models. This has motivated a growing line of work on what it means for a classification procedure to be \"fair.\" In this paper, we investigate the tension between minimizing error disparity across different population groups while maintaining calibrated probability estimates. We show that calibration is compatible only with a single error constraint (i.e. equal false-negatives rates across groups), and show that any algorithm that satisfies this relaxation is no better than randomizing a percentage of predictions for an existing classifier. These unsettling findings, which extend and generalize existing results, are empirically confirmed on several datasets."}}
{"id": "r1WZlrbubr", "cdate": 1356998400000, "mdate": null, "content": {"title": "Effective string processing and matching for author disambiguation", "abstract": "Track 2 in KDD Cup 2013 aims at determining duplicated authors in a data set from Microsoft Academic Search. This type of problems appears in many large-scale applications that compile information from different sources. This paper describes our solution developed at National Taiwan University to win the first prize of the competition. We propose an effective name matching framework and realize two implementations. An important strategy in our approach is to consider Chinese and non-Chinese names separately because of their different naming conventions. Post-processing including merging results of two predictions further boosts the performance. Our approach achieves F1-score 0.99202 on the private leader board, while 0.99195 on the public leader board."}}
{"id": "S1ZgiVZu-B", "cdate": 1356998400000, "mdate": null, "content": {"title": "Combination of feature engineering and ranking models for paper-author identification in KDD Cup 2013", "abstract": "The track 1 problem in KDD Cup 2013 is to discriminate between papers confirmed by the given authors from the other deleted papers. This paper describes the winning solution of team National Taiwan University for track 1 of KDD Cup 2013. First, we conduct the feature engineering to transform the various provided text information into 97 features. Second, we train classification and ranking models using these features. Last, we combine our individual models to boost the performance by using results on the internal validation set and the official Valid set. Some effective post-processing techniques have also been proposed. Our solution achieves 0.98259 MAP score and ranks the first place on the private leaderboard of Test set."}}
