{"id": "pxyA4SpbRM", "cdate": 1706745600000, "mdate": 1707903395829, "content": {"title": "Generalized Random Gilbert-Varshamov Codes: Typical Error Exponent and Concentration Properties", "abstract": "We find the exact typical error exponent of constant composition generalized random Gilbert-Varshamov (RGV) codes over discrete memoryless channels with generalized likelihood decoding. We show that the typical error exponent of the RGV ensemble is equal to the expurgated error exponent, provided that the RGV codebook parameters are chosen appropriately. We also prove that the random coding exponent converges in probability to the typical error exponent, and the corresponding non-asymptotic concentration rates are derived. Our results show that the decay rate of the lower tail is exponential while that of the upper tail is double exponential above the expurgated error exponent. The explicit dependence of the decay rates on the RGV distance functions is characterized."}}
{"id": "w8qyWsYnWfR", "cdate": 1701388800000, "mdate": 1705908742640, "content": {"title": "Concentration Properties of Random Codes", "abstract": "This paper shows that, for discrete memoryless channels, the error exponent of a randomly generated code with independent codewords converges in probability to its expectation\u2014the typical error exponent. For high rates, the result follows from the fact that the random-coding error exponent and the sphere-packing error exponent coincide. For low rates, instead, the convergence is based on the fact that the union bound accurately characterizes the error probability. The paper also zooms into the behavior at asymptotically low rates, and shows that the normalized error exponent converges in distribution to the standard Gaussian or a Gaussian-like distribution. We also state several results on the convergence of the error probability and error exponent for generic ensembles and channels."}}
{"id": "29xOc2WTLwO", "cdate": 1701388800000, "mdate": 1705908742663, "content": {"title": "Replica Analysis of the Linear Model With Markov or Hidden Markov Signal Priors", "abstract": "This paper estimates free energy, average mutual information, and minimum mean square error (MMSE) of a linear model under two assumptions: 1) the source is generated by a Markov chain; 2) the source is generated via a hidden Markov model. Our estimates are based on the replica method in statistical physics. We show that under the posterior mean estimator, the linear model with Markov sources or hidden Markov sources is decoupled into single-input AWGN channels with state information available at both encoder and decoder where the state distribution follows the left Perron-Frobenius eigenvector with unit Manhattan norm of the stochastic matrix of Markov chains. Numerical results show that the free energies and MSEs obtained via the replica method are closely approximate to their counterparts achieved by the Metropolis-Hastings algorithm or some well-known approximate message passing algorithms in the research literature."}}
{"id": "q8eyIUNcDC", "cdate": 1672531200000, "mdate": 1681746747021, "content": {"title": "Global Convergence Rate of Deep Equilibrium Models with General Activations", "abstract": "In a recent paper, Ling et al. investigated the over-parametrized Deep Equilibrium Model (DEQ) with ReLU activation and proved that the gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function. In this paper, we show that this fact still holds for DEQs with any general activation which has bounded first and second derivatives. Since the new activation function is generally non-linear, a general population Gram matrix is designed, and a new form of dual activation with Hermite polynomial expansion is developed."}}
{"id": "X6pyuOd0PY", "cdate": 1672531200000, "mdate": 1687285541150, "content": {"title": "Fundamental limits and algorithms for sparse linear regression with sublinear sparsity", "abstract": "We establish exact asymptotic expressions for the normalized mutual information and minimum mean-square-error (MMSE) of sparse linear regression in the sub-linear sparsity regime. Our result is achieved by a generalization of the adaptive interpolation method in Bayesian inference for linear regimes to sub-linear ones. A modification of the well-known approximate message passing algorithm to approach the MMSE fundamental limit is also proposed, and its state evolution is rigorously analysed. Our results show that the traditional linear assumption between the signal dimension and number of observations in the replica and adaptive interpolation methods is not necessary for sparse signals. They also show how to modify the existing well-known AMP algorithms for linear regimes to sub-linear ones."}}
{"id": "PsJW1JqjIt", "cdate": 1672531200000, "mdate": 1687285541151, "content": {"title": "Variable-Complexity Weighted-Tempered Gibbs Samplers for Bayesian Variable Selection", "abstract": "Subset weighted-Tempered Gibbs Sampler (wTGS) has been recently introduced by Jankowiak to reduce the computation complexity per MCMC iteration in high-dimensional applications where the exact calculation of the posterior inclusion probabilities (PIP) is not essential. However, the Rao-Backwellized estimator associated with this sampler has a high variance as the ratio between the signal dimension and the number of conditional PIP estimations is large. In this paper, we design a new subset weighted-Tempered Gibbs Sampler (wTGS) where the expected number of computations of conditional PIPs per MCMC iteration can be much smaller than the signal dimension. Different from the subset wTGS and wTGS, our sampler has a variable complexity per MCMC iteration. We provide an upper bound on the variance of an associated Rao-Blackwellized estimator for this sampler at a finite number of iterations, $T$, and show that the variance is $O\\big(\\big(\\frac{P}{S}\\big)^2 \\frac{\\log T}{T}\\big)$ for a given dataset where $S$ is the expected number of conditional PIP computations per MCMC iteration. Experiments show that our Rao-Blackwellized estimator can have a smaller variance than its counterpart associated with the subset wTGS."}}
{"id": "HtYyLN0D1y4", "cdate": 1672531200000, "mdate": 1692953018038, "content": {"title": "Concentration Properties of Generalized Random Gilbert-Varshamov Codes", "abstract": "We study the typical error exponent of constant composition generalized random Gilbert-Varshamov (RGV) codes over discrete memoryless channels (DMC) channels with generalized likelihood decoding. We show that the typical error exponent of the RGV ensemble is equal to the expurgated error exponent, provided that the RGV codebook parameters are chosen appropriately. We also prove that the exponent of a randomly chosen RGV code converges in probability to the typical error exponent; the lower tail is shown to decay exponentially while the upper tail decays double-exponentially above the expurgated exponent."}}
{"id": "jdsmBlsHGF2", "cdate": 1652737544134, "mdate": null, "content": {"title": "Generalization Error Bounds on Deep Learning with Markov Datasets", "abstract": "In this paper, we derive upper bounds on generalization errors for deep neural networks with Markov datasets. These bounds are developed based on Koltchinskii and Panchenko's approach for bounding the generalization error of combined classifiers with i.i.d. datasets. The development of new symmetrization inequalities in high-dimensional probability for Markov chains is a key element in our extension, where the spectral gap of the infinitesimal generator of the Markov chain plays a key parameter in these inequalities. We also propose a simple method to convert these bounds and other similar ones in traditional deep learning and machine learning to Bayesian counterparts for both i.i.d. and Markov datasets. Extensions to $m$-order homogeneous Markov chains such as AR and ARMA models and mixtures of several Markov data services are given."}}
{"id": "u5TFTxELpu", "cdate": 1640995200000, "mdate": 1681746747420, "content": {"title": "Convergence in Distribution of the Error Exponent of Random Codes at Zero Rate", "abstract": "We study the convergence in distribution of the error exponent of random codes, defined as the negative normalized logarithm of the probability of error, of both i.i.d. and constant-composition ensembles over discrete memoryless channels. For a constant number of messages, the distribution of the error exponent converges to that of the minimum of a set of independent normal random variables. For an increasing sub-exponential number of messages, the error exponent converges to a normal distribution, independent of the number of messages. As a byproduct, we provide a new method to prove the convergence to a normal distribution of an infinite number of random variables based on a modification of the Wasserstein metric."}}
{"id": "ooJrOUa84mS", "cdate": 1640995200000, "mdate": 1681746747113, "content": {"title": "Generalized Random Gilbert-Varshamov Codes: Typical Error Exponent and Concentration Properties", "abstract": "We find the exact typical error exponent of constant composition generalized random Gilbert-Varshamov (RGV) codes over DMCs channels with generalized likelihood decoding. We show that the typical error exponent of the RGV ensemble is equal to the expurgated error exponent, provided that the RGV codebook parameters are chosen appropriately. We also prove that the random coding exponent converges in probability to the typical error exponent, and the corresponding non-asymptotic concentration rates are derived. Our results show that the decay rate of the lower tail is exponential while that of the upper tail is double exponential above the expurgated error exponent. The explicit dependence of the decay rates on the RGV distance functions is characterized."}}
