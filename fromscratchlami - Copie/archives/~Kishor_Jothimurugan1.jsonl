{"id": "hWqo5rVcdx8", "cdate": 1665251218118, "mdate": null, "content": {"title": "Robust Option Learning for Compositional Generalization", "abstract": "Compositional reinforcement learning is a promising approach for training policies to perform complex long-horizon tasks. Typically, a high-level task is decomposed into a sequence of subtasks and a separate policy is trained to perform each subtask. In this paper, we focus on the problem of training subtask policies in a way that they can be used to perform any task; here, a task is given by a sequence of subtasks. We aim to maximize the worst-case performance over all tasks as opposed to the average-case performance. We formulate the problem as a two agent zero-sum game in which the adversary picks the sequence of subtasks. We propose two RL algorithms to solve this game: one is an adaptation of existing multi-agent RL algorithms to our setting and the other is an asynchronous version which enables parallel training of subtask policies. We evaluate our approach on two multi-task environments with continuous states and actions and demonstrate that our algorithms outperform state-of-the-art baselines."}}
{"id": "msFfpucKMf", "cdate": 1652737613935, "mdate": null, "content": {"title": "Robust Option Learning for Adversarial Generalization", "abstract": "Compositional reinforcement learning is a promising approach for training policies to perform complex long-horizon tasks. Typically, a high-level task is decomposed into a sequence of subtasks and a separate policy is trained to perform each subtask. In this paper, we focus on the problem of training subtask policies in a way that they can be used to perform any task; here, a task is given by a sequence of subtasks. We aim to maximize the worst-case performance over all tasks as opposed to the average-case performance. We formulate the problem as a two agent zero-sum game in which the adversary picks the sequence of subtasks. We propose two RL algorithms to solve this game: one is an adaptation of existing multi-agent RL algorithms to our setting and the other is an asynchronous version which enables parallel training of subtask policies. We evaluate our approach on two multi-task environments with continuous states and actions and demonstrate that our algorithms outperform state-of-the-art baselines."}}
{"id": "_gEGhe2L3q", "cdate": 1640995200000, "mdate": 1672632606907, "content": {"title": "Specification-Guided Learning of Nash Equilibria with High Social Welfare", "abstract": ""}}
{"id": "ion6Lo5tKtJ", "cdate": 1621629817981, "mdate": null, "content": {"title": "Compositional Reinforcement Learning from Logical Specifications", "abstract": "We study the problem of learning control policies for complex tasks given by logical specifications. Recent approaches automatically generate a reward function from a given specification and use a suitable reinforcement learning algorithm to learn a policy that maximizes the expected reward. These approaches, however, scale poorly to complex tasks that require high-level planning. In this work, we develop a compositional learning approach, called DIRL, that interleaves high-level planning and reinforcement learning. First, DIRL encodes the specification as an abstract graph; intuitively, vertices and edges of the graph correspond to regions of the state space and simpler sub-tasks, respectively. Our approach then incorporates reinforcement learning to learn neural network policies for each edge (sub-task) within a Dijkstra-style planning algorithm to compute a high-level plan in the graph. An evaluation of the proposed approach on a set of challenging control benchmarks with continuous state and action spaces demonstrates that it outperforms state-of-the-art baselines."}}
{"id": "_a8v1Ymh1c", "cdate": 1609459200000, "mdate": 1672632606952, "content": {"title": "Compositional Learning and Verification of Neural Network Controllers", "abstract": ""}}
{"id": "YhDu9NNEX1", "cdate": 1609459200000, "mdate": 1672632606957, "content": {"title": "Abstract Value Iteration for Hierarchical Reinforcement Learning", "abstract": ""}}
{"id": "SG9dX7yOrtb", "cdate": 1609459200000, "mdate": 1672632606956, "content": {"title": "Compositional Reinforcement Learning from Logical Specifications", "abstract": ""}}
{"id": "F46-J_5KUxu", "cdate": 1609459200000, "mdate": 1672632606957, "content": {"title": "A Framework for Transforming Specifications in Reinforcement Learning", "abstract": ""}}
{"id": "5qLZgLx3Yd", "cdate": 1609459200000, "mdate": 1672632606952, "content": {"title": "Learning Algorithms for Regenerative Stopping Problems with Applications to Shipping Consolidation in Logistics", "abstract": ""}}
{"id": "AnCcoJ1vPef", "cdate": 1577836800000, "mdate": 1672632606961, "content": {"title": "Space-efficient Query Evaluation over Probabilistic Event Streams", "abstract": ""}}
