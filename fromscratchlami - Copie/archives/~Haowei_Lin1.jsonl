{"id": "euxp53tkZaT", "cdate": 1673021625998, "mdate": null, "content": {"title": "A Probabilistic Explanation for VoE-based Evaluation", "abstract": "Visual grounded \\emph{Violation of expectations} (VoE) paradigm is widely used to evaluate the physics learning capability of both humans and machines. It does this by measuring the prediction error, or \\emph{surprise}, of a physics learning model in a given scene. Despite intuitive formulation and perfect alignment with developmental psychology, the design of evaluation protocol based on \\textit{surprise} score is empirical. We point out the potential risks behind the traditional \\textit{surprise} score design and provide a probabilistic explanation of VoE paradigm based on \\textit{likelihood ratio theory}. Guided by the theoretical framework, we propose two novel and extensible surprise scores that are theoretically sounded. Furthermore, we implement a simple yet novel baseline based on PredRNN~\\cite{wang2017predrnn} that demonstrates the ability to perform physical reasoning through direct \\emph{pixel-level prediction}. Our model outperforms a strong \\emph{object-level prediction} baseline PLATO, achieving an overall accuracy of 90.0\\% on the \\texttt{Probe} dataset, compared to 73.4\\% for PLATO (with overall accuracy $73.4\\%$). Additionally, we conduct experiments using our newly proposed metric."}}
{"id": "imJN0MUZkbA", "cdate": 1672531200000, "mdate": 1681153106149, "content": {"title": "Continual Learning of Language Models", "abstract": ""}}
{"id": "RiEnmzmK9_g", "cdate": 1672531200000, "mdate": 1691659209459, "content": {"title": "Continual Pre-training of Language Models", "abstract": ""}}
{"id": "-TKGZg77NA", "cdate": 1672531200000, "mdate": 1691659209501, "content": {"title": "Adapting a Language Model While Preserving its General Knowledge", "abstract": "Domain-adaptive pre-training (or DA-training for short), also known as post-training, aims to train a pre-trained general-purpose language model (LM) using an unlabeled corpus of a particular domain to adapt the LM so that end-tasks in the domain can give improved performances. However, existing DA-training methods are in some sense blind as they do not explicitly identify what knowledge in the LM should be preserved and what should be changed by the domain corpus. This paper shows that the existing methods are suboptimal and proposes a novel method to perform a more informed adaptation of the knowledge in the LM by (1) soft-masking the attention heads based on their importance to best preserve the general knowledge in the LM and (2) contrasting the representations of the general and the full (both general and domain knowledge) to learn an integrated representation with both general and domain-specific knowledge. Experimental results will demonstrate the effectiveness of the proposed approach."}}
{"id": "DA4Ay4rJfCL", "cdate": 1671498480303, "mdate": 1671498480303, "content": {"title": "CMG: A Class-Mixed Generation Approach to Out-of-Distribution Detection", "abstract": "Recently, contrastive learning with data and class augmentations has been shown to produce markedly better results for out-of-distribution (OOD) detection than previous approaches. However, a major shortcoming of this approach is that it is extremely slow due to the significant increase in data size and in the number of classes and the quadratic pairwise similarity computation. This paper shows that this heavy machinery is unnecessary. A novel approach, called CMG (Class-Mixed Generation), is proposed, which generates pseudo-OOD data by mixing class embeddings as abnormal conditions to CVAE (conditional variational Auto-Encoder) and then uses the data to fine-tune a classifier built using the given in-distribution (IND) data. To our surprise, the obvious approach of using the IND data and the pseudo-OOD data to directly train an OOD model is a very poor choice. The fine-tuning based approach turns out to be markedly better. Empirical evaluation shows that CMG not only produces new state-of-the-art results but also is much more efficient than contrastive learning, at least 10 times faster."}}
{"id": "m_GDIItaI3o", "cdate": 1663850463925, "mdate": null, "content": {"title": "Continual Pre-training of Language Models", "abstract": "Language models (LMs) have been instrumental for the rapid advance of natural language processing. This paper studies continual pre-training of LMs, in particular, continual domain-adaptive pre-training (or continual DAP-training). Existing research has shown that further pre-training an LM using a domain corpus to adapt the LM to the domain can improve the end-task performance in the domain. This paper proposes a novel method to continually DAP-train an LM with a sequence of unlabeled domain corpora to adapt the LM to these domains to improve their end-task performances. The key novelty of our method is a soft-masking mechanism that directly controls the update to the LM. A novel proxy is also proposed to preserve the general knowledge in the original LM. Additionally, it contrasts the representations of the previously learned domain knowledge (including the general knowledge in the pre-trained LM) and the knowledge from the current full network to achieve knowledge integration. The method not only overcomes catastrophic forgetting, but also achieves knowledge transfer to improve end-task performances. Empirical evaluation demonstrates the effectiveness of the proposed method."}}
{"id": "vl5FjvnutLd", "cdate": 1640995200000, "mdate": 1681691998349, "content": {"title": "Adapting a Language Model While Preserving its General Knowledge", "abstract": ""}}
{"id": "k-1RGc6kBG", "cdate": 1640995200000, "mdate": 1681691998244, "content": {"title": "Continual Training of Language Models for Few-Shot Learning", "abstract": ""}}
{"id": "bz-jJKgHhRh", "cdate": 1640995200000, "mdate": 1681691998357, "content": {"title": "CMG: A Class-Mixed Generation Approach to Out-of-Distribution Detection", "abstract": "Recently, contrastive learning with data and class augmentations has been shown to produce markedly better results for out-of-distribution (OOD) detection than previous approaches. However, a major shortcoming of this approach is that it is extremely slow due to the significant increase in data size and in the number of classes and the quadratic pairwise similarity computation. This paper shows that this heavy machinery is unnecessary. A novel approach, called CMG (Class-Mixed Generation), is proposed, which generates pseudo-OOD data by mixing class embeddings as abnormal conditions to CVAE (conditional variational Auto-Encoder) and then uses the data to fine-tune a classifier built using the given in-distribution (IND) data. To our surprise, the obvious approach of using the IND data and the pseudo-OOD data to directly train an OOD model is a very poor choice. The fine-tuning based approach turns out to be markedly better. Empirical evaluation shows that CMG not only produces new state-of-the-art results but also is much more efficient than contrastive learning, at least 10 times faster (Code is available at: https://github.com/shaoyijia/CMG )."}}
{"id": "Hv5JkzjoWJZ", "cdate": 1640995200000, "mdate": 1672931533105, "content": {"title": "Continual Training of Language Models for Few-Shot Learning", "abstract": ""}}
