{"id": "o4RHha_ZPl", "cdate": 1672531200000, "mdate": 1696307716669, "content": {"title": "Generative Autoencoders as Watermark Attackers: Analyses of Vulnerabilities and Threats", "abstract": "Invisible watermarks safeguard images' copyright by embedding hidden messages only detectable by owners. They also prevent people from misusing images, especially those generated by AI models. We propose a family of regeneration attacks to remove these invisible watermarks. The proposed attack method first adds random noise to an image to destroy the watermark and then reconstructs the image. This approach is flexible and can be instantiated with many existing image-denoising algorithms and pre-trained generative models such as diffusion models. Through formal proofs and empirical results, we show that all invisible watermarks are vulnerable to the proposed attack. For a particularly resilient watermark, RivaGAN, regeneration attacks remove 93-99% of the invisible watermarks while the baseline attacks remove no more than 3%. However, if we do not require the watermarked image to look the same as the original one, watermarks that keep the image semantically similar can be an alternative defense against our attack. Our finding underscores the need for a shift in research/industry emphasis from invisible watermarks to semantically similar ones. Code is available at https://github.com/XuandongZhao/WatermarkAttacker."}}
{"id": "jiKIykdE1nM", "cdate": 1672531200000, "mdate": 1696307716644, "content": {"title": "Large Language Models Are Partially Primed in Pronoun Interpretation", "abstract": ""}}
{"id": "YowgLi6OYe", "cdate": 1672531200000, "mdate": 1695411282847, "content": {"title": "ReDi: Efficient Learning-Free Diffusion Inference via Trajectory Retrieval", "abstract": "Diffusion models show promising generation capability for a variety of data. Despite their high generation quality, the inference for diffusion models is still time-consuming due to the numerous sa..."}}
{"id": "ReoiIUEOQN", "cdate": 1672531200000, "mdate": 1696307716661, "content": {"title": "Large Language Models Are Partially Primed in Pronoun Interpretation", "abstract": "While a large body of literature suggests that large language models (LLMs) acquire rich linguistic representations, little is known about whether they adapt to linguistic biases in a human-like way. The present study probes this question by asking whether LLMs display human-like referential biases using stimuli and procedures from real psycholinguistic experiments. Recent psycholinguistic studies suggest that humans adapt their referential biases with recent exposure to referential patterns; closely replicating three relevant psycholinguistic experiments from Johnson & Arnold (2022) in an in-context learning (ICL) framework, we found that InstructGPT adapts its pronominal interpretations in response to the frequency of referential patterns in the local discourse, though in a limited fashion: adaptation was only observed relative to syntactic but not semantic biases. By contrast, FLAN-UL2 fails to generate meaningful patterns. Our results provide further evidence that contemporary LLMs discourse representations are sensitive to syntactic patterns in the local context but less so to semantic patterns. Our data and code are available at \\url{https://github.com/zkx06111/llm_priming}."}}
{"id": "-VKqLVyO183F", "cdate": 1672531200000, "mdate": 1695411282880, "content": {"title": "ALGO: Synthesizing Algorithmic Programs with Generated Oracle Verifiers", "abstract": "Large language models (LLMs) excel at implementing code from functionality descriptions, but struggle with algorithmic problems that require not only implementation but also identification of the suitable algorithm. Moreover, LLM-generated programs lack guaranteed correctness and require human verification. To address these challenges, we propose ALGO, a framework that synthesizes Algorithmic programs with LLM-Generated Oracles to guide the creation and verify their correctness. ALGO first generates a probably correct but possibly slow reference oracle by prompting an LLM to exhaustively enumerate all the combinations of relevant variables. This oracle is then utilized to guide an arbitrary search strategy in exploring the algorithm space and to verify the algorithms synthesized. Our study shows that the LLM-generated oracles are correct for 88% of the cases. With the oracles as verifiers, ALGO can be integrated with any existing code generation model in a model-agnostic manner to enhance its performance. Experiments show that when equipped with ALGO, we achieve an 8x better one-submission pass rate over the Codex model and a 2.6x better one-submission pass rate over CodeT, the current state-of-the-art model on CodeContests. We can also get 1.3x better pass rate over the ChatGPT Code Interpreter on unseen problems."}}
{"id": "E41uHpgni1", "cdate": 1640995200000, "mdate": 1682317916743, "content": {"title": "A Study of Syntactic Multi-Modality in Non-Autoregressive Machine Translation", "abstract": "Kexun Zhang, Rui Wang, Xu Tan, Junliang Guo, Yi Ren, Tao Qin, Tie-Yan Liu. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022."}}
{"id": "-f1KbPBB3I", "cdate": 1640995200000, "mdate": 1682318194251, "content": {"title": "Focus on the Action: Learning to Highlight and Summarize Jointly for Email To-Do Items Summarization", "abstract": ""}}
{"id": "yyNJnQAok3W", "cdate": 1609459200000, "mdate": 1682318194210, "content": {"title": "WSRGlow: A Glow-Based Waveform Generative Model for Audio Super-Resolution", "abstract": "Audio super-resolution is the task of constructing a high-resolution (HR) audio from a low-resolution (LR) audio by adding the missing band. Previous methods based on convolutional neural networks and mean squared error training objective have relatively low performance, while adversarial generative models are difficult to train and tune. Recently, normalizing flow has attracted a lot of attention for its high performance, simple training and fast inference. In this paper, we propose WSRGlow, a Glow-based waveform generative model to perform audio super-resolution. Specifically, 1) we integrate WaveNet and Glow to directly maximize the exact likelihood of the target HR audio conditioned on LR information; and 2) to exploit the audio information from low-resolution audio, we propose an LR audio encoder and an STFT encoder, which encode the LR information from the time domain and frequency domain respectively. The experimental results show that the proposed model is easier to train and outperforms the previous works in terms of both objective and perceptual quality. WSRGlow is also the first model to produce 48kHz waveforms from 12kHz LR audio. Audio samples are publicly available."}}
