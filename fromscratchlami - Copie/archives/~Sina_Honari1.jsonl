{"id": "Un7vZGr-Z4", "cdate": 1668777930617, "mdate": 1668777930617, "content": {"title": "On Triangulation as a Form of Self-Supervision for 3D Human Pose Estimation", "abstract": "Supervised approaches to 3D pose estimation from single images are remarkably effective when labeled data is abundant. However, as the acquisition of ground-truth 3D labels is labor intensive and time consuming, recent attention has shifted towards semi- and weakly-supervised learning. Generating an effective form of supervision with little annotations still poses major challenge in crowded scenes. In this paper we propose to impose multi-view geometrical constraints by means of a weighted differentiable triangulation and use it as a form of self-supervision when no labels are available. We therefore train a 2D pose estimator in such a way that its predictions correspond to the re-projection of the triangulated 3D pose and train an auxiliary network on them to produce the final 3D poses. We complement the triangulation with a weighting mechanism that alleviates the impact of noisy predictions caused by self-occlusion or occlusion from other subjects. We demonstrate the effectiveness of our semi-supervised approach on Human3.6M and MPI-INF-3DHP datasets, as well as on a new multi-view multi-person dataset that features occlusion."}}
{"id": "OUtdV6MfIIc", "cdate": 1667468611041, "mdate": 1667468611041, "content": {"title": "SegmentMeIfYouCan: A Benchmark for Anomaly Segmentation", "abstract": "State-of-the-art semantic or instance segmentation deep neural networks (DNNs) are usually trained on a closed set of semantic classes. As such, they are ill-equipped to handle previously-unseen objects. However, detecting and localizing such objects is crucial for safety-critical applications such as perception for automated driving, especially if they appear on the road ahead. While some methods have tackled the tasks of anomalous or out-of-distribution object segmentation, progress remains slow, in large part due to the lack of solid benchmarks; existing datasets either consist of synthetic data, or suffer from label inconsistencies. In this paper, we bridge this gap by introducing the \"SegmentMeIfYouCan\" benchmark. Our benchmark addresses two tasks: Anomalous object segmentation, which considers any previously-unseen object category; and road obstacle segmentation, which focuses on any object on the road, may it be known or unknown. We provide two corresponding datasets together with a test suite performing an in-depth method analysis, considering both established pixel-wise performance metrics and recent component-wise ones, which are insensitive to object sizes. We empirically evaluate multiple state-of-the-art baseline methods, including several models specifically designed for anomaly / obstacle segmentation, on our datasets and on public ones, using our test suite. The anomaly and obstacle segmentation results show that our datasets contribute to the diversity and difficulty of both data landscapes. "}}
{"id": "OFiGmksrSz1", "cdate": 1629131176727, "mdate": null, "content": {"title": "SegmentMeIfYouCan: A Benchmark for Anomaly Segmentation", "abstract": "State-of-the-art semantic or instance segmentation deep neural networks (DNNs) are usually trained on a closed set of semantic classes. As such, they are ill-equipped to handle previously-unseen objects. \nHowever, detecting and localizing such objects is crucial for safety-critical applications such as perception for automated driving, especially if they appear on the road ahead. While some methods have tackled the tasks of anomalous or out-of-distribution object segmentation, progress remains slow, in large part due to the lack of solid benchmarks; existing datasets either consist of synthetic data, or suffer from label inconsistencies. In this paper, we bridge this gap by introducing the \"SegmentMeIfYouCan\" benchmark. Our benchmark addresses two tasks: Anomalous object segmentation, which considers any previously-unseen object category; and road obstacle segmentation, which focuses on any object on the road, may it be known or unknown.\nWe provide two corresponding datasets together with a test suite performing an in-depth method analysis, considering both established pixel-wise performance metrics and recent component-wise ones, which are insensitive to object sizes. We empirically evaluate multiple state-of-the-art baseline methods, including several models specifically designed for anomaly / obstacle segmentation, on our datasets and on public ones, using our test suite.\nThe anomaly and obstacle segmentation results show that our datasets contribute to the diversity and difficulty of both data landscapes."}}
{"id": "OTnqQUEwPKu", "cdate": 1623107124485, "mdate": null, "content": {"title": "Benchmarking Bias Mitigation Algorithms in Representation Learning through Fairness Metrics", "abstract": "With the recent expanding attention of machine learning researchers and practitioners to fairness, there is a void of a common framework to analyze and compare the capabilities of proposed models in deep representation learning. In this paper, we evaluate different fairness methods trained with deep neural networks on a common synthetic dataset and a real-world dataset to obtain better insights on how these methods work. In particular, we train about 3000 different models in various setups, including imbalanced and correlated data configurations, to verify the limits of the current models and better understand in which setups they are subject to failure. Our results show that the bias of models increase as datasets become more imbalanced or datasets attributes become more correlated, the level of dominance of correlated sensitive dataset features impact bias, and the sensitive information remains in the latent representation even when bias-mitigation algorithms are applied. Overall, we present a dataset, propose various challenging evaluation setups, and rigorously evaluate recent promising bias-mitigation algorithms in a common framework and publicly release this benchmark, hoping the research community would take it as a common entry point for fair deep learning."}}
{"id": "aYbCpFNnHdh", "cdate": 1601308376454, "mdate": null, "content": {"title": "Visual Question Answering From Another Perspective: CLEVR Mental Rotation Tests", "abstract": "Different types of \\emph{mental rotation tests} have been used extensively in psychology to understand human visual reasoning and perception. Understanding what an object or visual scene would look like from another viewpoint is a challenging problem that is made even harder if it must be performed from a single image. 3D computer vision has a long history of examining related problems. However, often what one is most interested in is the answer to a relatively simple question posed in another visual frame of reference -- as opposed to creating a full 3D reconstruction. \nMental rotations tests can also manifest as consequential questions in the real world such as: does the pedestrian that I see, see the car that I am driving?\nWe explore a controlled setting whereby questions are posed about the properties of a scene if the scene were observed from another viewpoint. To do this we have created a new version of the CLEVR VQA problem setup and dataset that we call CLEVR Mental Rotation Tests or CLEVR-MRT, where the goal is to answer questions about the original CLEVR viewpoint given a single image obtained from a different viewpoint of the same scene. Using CLEVR Mental Rotation Tests we examine standard state of the art methods, show how they fall short, then explore novel neural architectures that involve inferring representations encoded as feature volumes describing a scene. Our new methods use rigid transformations of feature volumes conditioned on the viewpoint camera. We examine the efficacy of different model variants through performing a rigorous ablation study. Furthermore, we examine the use of contrastive learning to infer a volumetric encoder in a self-supervised manner and find that this approach yields the best results of our study using CLEVR-MRT."}}
{"id": "xEpUl1um6V", "cdate": 1601308265044, "mdate": null, "content": {"title": "Benchmarking Bias Mitigation Algorithms in Representation Learning through Fairness Metrics", "abstract": "With the recent expanding attention of machine learning researchers and practitioners to fairness, there is a void of a common framework to analyze and compare the capabilities of proposed models in deep representation learning. In this paper, we evaluate different fairness methods trained with deep neural networks on a common synthetic dataset to obtain a better insight into the working of these methods. In particular, we train about 2000 different models in various setups, including unbalanced and correlated data configurations, to verify the limits of the current models and better understand in which setups they are subject to failure. In doing so we present a dataset, a large subset of proposed fairness metrics in the literature, and rigorously evaluate recent promising debiasing algorithms in a common framework hoping the research community would take this benchmark as a common entry point for fair deep learning."}}
{"id": "4E_909QFjz", "cdate": 1581944640358, "mdate": null, "content": {"title": "Unsupervised Depth Estimation, 3D Face Rotation and Replacement", "abstract": "We present an unsupervised approach for learning to estimate three dimensional\n(3D) facial structure from a single image while also predicting 3D viewpoint\ntransformations that match a desired pose and facial geometry. We achieve this\nby inferring the depth of facial keypoints of an input image in an unsupervised\nmanner, without using any form of ground-truth depth information. We show how\nit is possible to use these depths as intermediate computations within a new backpropable loss to predict the parameters of a 3D affine transformation matrix that\nmaps inferred 3D keypoints of an input face to the corresponding 2D keypoints on\na desired target facial geometry or pose. Our resulting approach, called DepthNets,\ncan therefore be used to infer plausible 3D transformations from one face pose\nto another, allowing faces to be frontalized, transformed into 3D models or even\nwarped to another pose and facial geometry. Lastly, we identify certain shortcomings with our formulation, and explore adversarial image translation techniques as\na post-processing step to re-synthesize complete head shots for faces re-targeted to\ndifferent poses or identities."}}
{"id": "G_xm7S5Bqe", "cdate": 1581936611168, "mdate": null, "content": {"title": "Improving Landmark Localization with Semi-Supervised Learning", "abstract": "We present two techniques to improve landmark localization in images from partially annotated datasets. Our\nprimary goal is to leverage the common situation where precise landmark locations are only provided for a small data\nsubset, but where class labels for classification or regression tasks related to the landmarks are more abundantly\navailable. First, we propose the framework of sequential\nmultitasking and explore it here through an architecture for\nlandmark localization where training with class labels acts\nas an auxiliary signal to guide the landmark localization on\nunlabeled data. A key aspect of our approach is that errors\ncan be backpropagated through a complete landmark localization model. Second, we propose and explore an unsupervised learning technique for landmark localization based on\nhaving a model predict equivariant landmarks with respect\nto transformations applied to the image. We show that these\ntechniques, improve landmark prediction considerably and\ncan learn effective detectors even when only a small fraction of the dataset has landmark labels. We present results\non two toy datasets and four real datasets, with hands and\nfaces, and report new state-of-the-art on two datasets in the\nwild, e.g. with only 5% of labeled images we outperform\nprevious state-of-the-art trained on the AFLW dataset."}}
{"id": "L93iy_iWAD", "cdate": 1581936385878, "mdate": null, "content": {"title": "Recombinator Networks: Learning Coarse-to-Fine Feature Aggregation", "abstract": "Deep neural networks with alternating convolutional,\nmax-pooling and decimation layers are widely used in state\nof the art architectures for computer vision. Max-pooling\npurposefully discards precise spatial information in order\nto create features that are more robust, and typically organized as lower resolution spatial feature maps. On some\ntasks, such as whole-image classification, max-pooling derived features are well suited; however, for tasks requiring\nprecise localization, such as pixel level prediction and segmentation, max-pooling destroys exactly the information required to perform well. Precise localization may be preserved by shallow convnets without pooling but at the expense of robustness. Can we have our max-pooled multilayered cake and eat it too? Several papers have proposed\nsummation and concatenation based methods for combining upsampled coarse, abstract features with finer features\nto produce robust pixel level predictions. Here we introduce another model \u2014 dubbed Recombinator Networks \u2014\nwhere coarse features inform finer features early in their\nformation such that finer features can make use of several\nlayers of computation in deciding how to use coarse features. The model is trained once, end-to-end and performs\nbetter than summation-based architectures, reducing the error from the previous state of the art on two facial keypoint\ndatasets, AFW and AFLW, by 30% and beating the current\nstate-of-the-art on 300W without using extra data. We improve performance even further by adding a denoising prediction model based on a novel convnet formulation."}}
{"id": "SJIA3pijM", "cdate": 1523469518112, "mdate": null, "content": {"title": "How to Cure Cancer (in images) with Unpaired Image Translation", "abstract": "We discuss how distribution matching losses, such as those used in CycleGAN, when used to translate images from one domain to another can lead to mis-diagnosis of medical conditions. It seems appealing to use these methods for image translation from the source domain to the target domain without requiring paired data. However, the way these models function is through matching the distribution of the translated images to the target domain. This can cause issues especially when the percentage of known and unknown labels (e.g. sick and healthy labels) differ between the source and target domains. When the output of the model is an image, current methods do not guarantee that the known and unknown labels have been preserved. Therefore until alternative solutions are proposed to maintain the accuracy of the translated features, such translated images should not be used for medical interpretation (e.g. by doctors). However, recent papers are using these models as if this is the goal."}}
