{"id": "jK02XX9ZpJkt", "cdate": 1663850146012, "mdate": null, "content": {"title": "CAMA: A New Framework for Safe Multi-Agent Reinforcement Learning  Using Constraint Augmentation", "abstract": "With the widespread application of multi-agent reinforcement learning (MARL) in real-life settings, the ability to meet safety constraints has become an urgent problem to solve. For example, it is necessary to avoid collisions to reach a common goal in controlling multiple drones. We address this problem by introducing the Constraint Augmented Multi-Agent framework --- CAMA. CAMA can serve as a plug-and-play module to the popular MARL algorithms, including centralized training, decentralized execution and independent learning frameworks. In our approach, we represent the safety constraint as the sum of discounted safety costs bounded by the predefined value, which we call the safety budget. Experiments demonstrate that CAMA can converge quickly to a high degree of constraint satisfaction and surpasses other state-of-the-art safety counterpart algorithms in both cooperative and competitive settings. "}}
{"id": "PTrcWS4aRnH", "cdate": 1652963729132, "mdate": 1652963729132, "content": {"title": "SAUTE RL: Almost Surely Safe Reinforcement Learning Using State Augmentation", "abstract": "Satisfying safety constraints almost surely (or with probability one) can be critical for deployment of Reinforcement Learning (RL) in real-life applications. For example, plane landing and take-off should ideally occur with probability one. We address the problem by introducing Safety Augmented (Saute) Markov Decision Processes (MDPs), where the safety constraints are eliminated by augmenting them into the state-space and reshaping the objective. We show that Saute MDP satisfies the Bellman equation and moves us closer to solving Safe RL with constraints satisfied almost surely. We argue that Saute MDP allows to view Safe RL problem from a different perspective enabling new features. For instance, our approach has a plug-and-play nature, i.e., any RL algorithm can be \"sauteed\". Additionally, state augmentation allows for policy generalization across safety constraints. We finally show that Saute RL algorithms can outperform their state-of-the-art counterparts when constraint satisfaction is of high importance."}}
{"id": "ht61oVsaya", "cdate": 1632875492621, "mdate": null, "content": {"title": "DESTA: A Framework for Safe Reinforcement Learning with Markov Games of Intervention", "abstract": "Exploring in an unknown system can place an agent in dangerous situations,\nexposing to potentially catastrophic hazards. Many current approaches for tackling\nsafe learning in reinforcement learning (RL) lead to a trade-off between safe\nexploration and fulfilling the task. Though these methods possibly incur fewer\nsafety violations they often also lead to reduced task performance. In this paper, we\ntake the first step in introducing a generation of RL solvers that learn to minimise\nsafety violations while maximising the task reward to the extend that can be\ntolerated by safe policies. Our approach uses a new two-player framework for safe\nRL called DESTA. The core of DESTA is a novel game between two RL agents:\nSafety Agent that is delegated the task of minimising safety violations and Task\nAgent whose goal is to maximise the reward set by the environment task. Safety\nAgent can selectively take control of the system at any given point to prevent\nsafety violations while Task Agent is free to execute its actions at all other states.\nThis framework enables Safety Agent to learn to take actions that minimise future\nsafety violations (during and after training) by performing safe actions at certain\nstates while Task Agent performs actions that maximise the task performance\neverywhere else. We demonstrate DESTA\u2019s ability to tackle challenging tasks and\ncompare against state-of-the-art RL methods in Safety Gym Benchmarks which\nsimulate real-world physical systems and OpenAI\u2019s Lunar Lander.\n"}}
{"id": "BlyXYc4wF2-", "cdate": 1632875480541, "mdate": null, "content": {"title": "Multi-Agent Constrained Policy Optimisation ", "abstract": "Developing reinforcement learning algorithms that satisfy safety constraints is becoming increasingly important in real-world applications. In multi-agent reinforcement learning (MARL) settings, policy optimisation with safety awareness is particularly challenging because each individual agent has to not only meet its own safety constraints, but also consider those of others so that their joint behaviour can be guaranteed safe. Despite its importance, the problem of safe multi-agent learning has not been rigorously studied; very few solutions have been proposed, nor a sharable testing environment or benchmarks. To fill these gaps, in this work, we formulate the safe MARL problem as a constrained Markov game and solve it with policy optimisation methods. Our solutions---Multi-Agent Constrained Policy Optimisation (MACPO) and MAPPO-Lagrangian---leverage the theories from both constrained policy optimisation and multi-agent trust region learning. Crucially, our methods enjoy theoretical guarantees of both monotonic improvement in reward and satisfaction of safety constraints at every iteration. To examine the effectiveness of our methods, we develop the benchmark suite of Safe Multi-Agent MuJoCo that involves a variety of  MARL baselines. Experimental results justify that MACPO/MAPPO-Lagrangian can consistently satisfy safety constraints, meanwhile achieving comparable performance to strong baselines."}}
