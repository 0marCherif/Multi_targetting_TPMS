{"id": "GWa2efGorZH", "cdate": 1684220922876, "mdate": 1684220922876, "content": {"title": "Faster Gradient-Free Algorithms for Nonsmooth Nonconvex Stochastic Optimization", "abstract": "We consider the optimization problem of the form \n$\\min_{x \\in \\mathbb{R}^d} f(x) \\triangleq \\mathbb{E}[F(x;\\xi)]$ \n, where the component $F(x;\\xi)$ is $L$-mean-squared Lipschitz but possibly nonconvex and nonsmooth.\nThe recently proposed gradient-free method requires at most $\\mathcal{O}( L^4 d^{3/2} \\epsilon^{-4} + \\Delta L^3 d^{3/2}  \\delta^{-1} \\epsilon^{-4})$ stochastic zeroth-order oracle complexity to find a $(\\delta,\\epsilon)$-Goldstein stationary point of objective function, where $\\Delta = f(x_0) - \\inf_{x \\in \\mathbb{R}^d} f(x)$ and $x_0$ is the initial point of the algorithm. This paper proposes a more efficient algorithm using stochastic recursive gradient estimators, which improves the complexity to $\\mathcal{O}(L^3 d^{3/2}  \\epsilon^{-3}+ \\Delta L^2 d^{3/2} \\delta^{-1} \\epsilon^{-3})$."}}
{"id": "PzNjdn9huN", "cdate": 1684220674643, "mdate": 1684220674643, "content": {"title": "Faster Gradient-Free Algorithms for Nonsmooth Nonconvex Stochastic Optimization", "abstract": "We consider the optimization problem of the form \n$\\min_{x \\in \\mathbb{R}^d} f(x) \\triangleq \\mathbb{E}[F(x;\\xi)]$ \n, where the component $F(x;\\xi)$ is $L$-mean-squared Lipschitz but possibly nonconvex and nonsmooth.\nThe recently proposed gradient-free method requires at most $\\mathcal{O}( L^4 d^{3/2} \\epsilon^{-4} + \\Delta L^3 d^{3/2}  \\delta^{-1} \\epsilon^{-4})$ stochastic zeroth-order oracle complexity to find a $(\\delta,\\epsilon)$-Goldstein stationary point of objective function, where $\\Delta = f(x_0) - \\inf_{x \\in \\mathbb{R}^d} f(x)$ and $x_0$ is the initial point of the algorithm. This paper proposes a more efficient algorithm using stochastic recursive gradient estimators, which improves the complexity to $\\mathcal{O}(L^3 d^{3/2}  \\epsilon^{-3}+ \\Delta L^2 d^{3/2} \\delta^{-1} \\epsilon^{-3})$."}}
{"id": "9JahY13NUDd", "cdate": 1672531200000, "mdate": 1674734802097, "content": {"title": "Faster Gradient-Free Algorithms for Nonsmooth Nonconvex Stochastic Optimization", "abstract": "We consider the optimization problem of the form $\\min_{x \\in \\mathbb{R}^d} f(x) \\triangleq \\mathbb{E}_{\\xi} [F(x; \\xi)]$, where the component $F(x;\\xi)$ is $L$-mean-squared Lipschitz but possibly nonconvex and nonsmooth. The recently proposed gradient-free method requires at most $\\mathcal{O}( L^4 d^{3/2} \\epsilon^{-4} + \\Delta L^3 d^{3/2} \\delta^{-1} \\epsilon^{-4})$ stochastic zeroth-order oracle complexity to find a $(\\delta,\\epsilon)$-Goldstein stationary point of objective function, where $\\Delta = f(x_0) - \\inf_{x \\in \\mathbb{R}^d} f(x)$ and $x_0$ is the initial point of the algorithm. This paper proposes a more efficient algorithm using stochastic recursive gradient estimators, which improves the complexity to $\\mathcal{O}(L^3 d^{3/2} \\epsilon^{-3}+ \\Delta L^2 d^{3/2} \\delta^{-1} \\epsilon^{-3})$."}}
{"id": "Jb-d9fZX14", "cdate": 1652737524985, "mdate": null, "content": {"title": "Finding Second-Order Stationary Points in Nonconvex-Strongly-Concave Minimax Optimization", "abstract": "We study the smooth minimax optimization problem $\\min_{\\bf x}\\max_{\\bf y} f({\\bf x},{\\bf y})$, where $f$ is $\\ell$-smooth, strongly-concave in ${\\bf y}$ but possibly nonconvex in ${\\bf x}$. Most of existing works focus on finding the first-order stationary point of the function $f({\\bf x},{\\bf y})$ or its primal function $P({\\bf x})\\triangleq \\max_{\\bf y} f({\\bf x},{\\bf y})$, but few of them focus on achieving the second-order stationary point, which is essential to nonconvex problems. In this paper, we propose a novel approach for minimax optimization, called Minimax Cubic Newton (MCN), which could find an ${\\mathcal O}\\left(\\varepsilon,\\kappa^{1.5}\\sqrt{\\rho\\varepsilon}\\right)$-second-order stationary point of $P({\\bf x})$ with calling ${\\mathcal O}\\left(\\kappa^{1.5}\\sqrt{\\rho}\\varepsilon^{-1.5}\\right)$ times of second-order oracles and $\\tilde{\\mathcal O}\\left(\\kappa^{2}\\sqrt{\\rho}\\varepsilon^{-1.5}\\right)$ times of first-order oracles, where $\\kappa$ is the condition number and $\\rho$ is the Lipschitz continuous constant for the Hessian of $f({\\bf x},{\\bf y})$. In addition, we propose an inexact variant of MCN for high-dimensional problems to avoid calling the expensive second-order oracles. Instead, our method solves the cubic sub-problem inexactly via gradient descent and matrix Chebyshev expansion. This strategy still obtains the desired approximate second-order stationary point with high probability but only requires $\\tilde{\\mathcal O}\\left(\\kappa^{1.5}\\ell\\varepsilon^{-2}\\right)$ Hessian-vector oracle calls and $\\tilde{\\mathcal O}\\left(\\kappa^{2}\\sqrt{\\rho}\\varepsilon^{-1.5}\\right)$ first-order oracle calls. To the best of our knowledge, this is the first work that considers the non-asymptotic convergence behavior of finding second-order stationary points for minimax problems without the convex-concave assumptions."}}
{"id": "pELM0QgWIjn", "cdate": 1652737502902, "mdate": null, "content": {"title": "Quasi-Newton Methods for Saddle Point Problems", "abstract": "This paper studies quasi-Newton methods for strongly-convex-strongly-concave  saddle point problems. \nWe propose random Broyden family updates, which have explicit local superlinear convergence rate of ${\\mathcal O}\\big(\\big(1-1/(d\\varkappa^2)\\big)^{k(k-1)/2}\\big)$, where $d$ is the dimension of the problem, $\\varkappa$ is the condition number and $k$ is the number of iterations. The design and analysis of proposed algorithm are based on estimating the square of indefinite Hessian matrix, which is different from classical quasi-Newton methods in convex optimization. We also present two specific Broyden family algorithms with BFGS-type and SR1-type updates, which enjoy the faster local convergence rate of $\\mathcal O\\big(\\big(1-1/d\\big)^{k(k-1)/2}\\big)$. Our numerical experiments show proposed algorithms outperform classical first-order methods."}}
{"id": "JSha3zfdmSo", "cdate": 1652737319922, "mdate": null, "content": {"title": "Faster Stochastic Algorithms for Minimax Optimization under Polyak-{\\L}ojasiewicz Condition", "abstract": "This paper considers stochastic first-order algorithms for minimax optimization under Polyak-{\\L}ojasiewicz (PL) conditions. \nWe propose SPIDER-GDA for solving the finite-sum problem of the form $\\min_x \\max_y f(x,y)\\triangleq \\frac{1}{n} \\sum_{i=1}^n f_i(x,y)$, where the objective function $f(x,y)$ is $\\mu_x$-PL in $x$ and $\\mu_y$-PL in $y$; and each $f_i(x,y)$ is $L$-smooth. We prove SPIDER-GDA could find an $\\epsilon$-approximate solution within ${\\mathcal O}\\left((n + \\sqrt{n}\\,\\kappa_x\\kappa_y^2)\\log (1/\\epsilon)\\right)$ stochastic first-order oracle (SFO) complexity, which is better than the state-of-the-art method whose SFO upper bound is ${\\mathcal O}\\big((n + n^{2/3}\\kappa_x\\kappa_y^2)\\log (1/\\epsilon)\\big)$, where $\\kappa_x\\triangleq L/\\mu_x$ and $\\kappa_y\\triangleq L/\\mu_y$.\nFor the ill-conditioned case, we provide an accelerated algorithm to reduce the computational cost further. It achieves $\\tilde{{\\mathcal O}}\\big((n+\\sqrt{n}\\,\\kappa_x\\kappa_y)\\log^2 (1/\\epsilon)\\big)$ SFO upper bound when $\\kappa_x\\geq\\sqrt{n}$. Our ideas also can be applied to the more general setting that the objective function only satisfies PL condition for one variable. Numerical experiments validate the superiority of proposed methods."}}
{"id": "YT5n8Y5d9MF", "cdate": 1640995200000, "mdate": 1674734802183, "content": {"title": "An Optimal Stochastic Algorithm for Decentralized Nonconvex Finite-sum Optimization", "abstract": "This paper studies the decentralized nonconvex optimization problem $\\min_{x\\in{\\mathbb R}^d} f(x)\\triangleq \\frac{1}{m}\\sum_{i=1}^m f_i(x)$, where $f_i(x)\\triangleq \\frac{1}{n}\\sum_{j=1}^n f_{i,j}(x)$ is the local function on the $i$-th agent of the network. We propose a novel stochastic algorithm called DEcentralized probAbilistic Recursive gradiEnt deScenT (\\DEAREST), which integrates the techniques of variance reduction, gradient tracking and multi-consensus. We construct a Lyapunov function that simultaneously characterizes the function value, the gradient estimation error and the consensus error for the convergence analysis. Based on this measure, we provide a concise proof to show DEAREST requires at most ${\\mathcal O}(mn+\\sqrt{mn}L\\varepsilon^{-2})$ incremental first-order oracle (IFO) calls and ${\\mathcal O}({L\\varepsilon^{-2}}/{\\sqrt{1-\\lambda_2(W)}}\\,)$ communication rounds to find an $\\varepsilon$-stationary point in expectation, where $L$ is the smoothness parameter and $\\lambda_2(W)$ is the second-largest eigenvalue of the gossip matrix $W$. We can verify both of the IFO complexity and communication complexity match the lower bounds. To the best of our knowledge, DEAREST is the first optimal algorithm for decentralized nonconvex finite-sum optimization."}}
{"id": "G8kkc_ginP", "cdate": 1640995200000, "mdate": 1674734802106, "content": {"title": "A Simple and Efficient Stochastic Algorithm for Decentralized Nonconvex-Strongly-Concave Minimax Optimization", "abstract": "This paper studies the stochastic optimization for decentralized nonconvex-strongly-concave minimax problem. We propose a simple and efficient algorithm, called Decentralized Recursive-gradient descEnt Ascent Method (\\texttt{DREAM}), which achieves the best-known theoretical guarantee for finding the $\\epsilon$-stationary point of the primal function. For the online setting, the proposed method requires $\\mathcal{O}(\\kappa^3\\epsilon^{-3})$ stochastic first-order oracle (SFO) calls and $\\mathcal{O}\\big(\\kappa^2\\epsilon^{-2}/\\sqrt{1-\\lambda_2(W)}\\,\\big)$ communication rounds to find an $\\epsilon$-stationary point, where $\\kappa$ is the condition number and $\\lambda_2(W)$ is the second-largest eigenvalue of the gossip matrix~$W$. For the offline setting with totally $N$ component functions, the proposed method requires $\\mathcal{O}\\big(\\kappa^2 \\sqrt{N} \\epsilon^{-2}\\big)$ SFO calls and the same communication complexity as the online setting."}}
{"id": "9k4_W0Hp8_", "cdate": 1640995200000, "mdate": 1674734802136, "content": {"title": "Partial-Quasi-Newton Methods: Efficient Algorithms for Minimax Optimization Problems with Unbalanced Dimensionality", "abstract": "This paper studies the strongly-convex-strongly-concave minimax optimization with unbalanced dimensionality. Such problems contain several popular applications in data science such as few shot learning and fairness-aware machine learning task. The design of conventional iterative algorithm for minimax optimization typically focuses on reducing the total number of oracle calls, which ignores the unbalanced computational cost for accessing the information from two different variables in minimax. We propose a novel second-order optimization algorithm, called Partial-Quasi-Newton (PQN) method, which takes the advantage of unbalanced structure in the problem to establish the Hessian estimate efficiently. We theoretically prove our PQN method converges to the saddle point faster than existing minimax optimization algorithms. The numerical experiments on real-world applications show the proposed PQN performs significantly better than the state-of-the-art methods."}}
{"id": "7t6hClT4er", "cdate": 1640995200000, "mdate": 1674734802257, "content": {"title": "Near-Optimal Algorithms for Making the Gradient Small in Stochastic Minimax Optimization", "abstract": "We study the problem of finding a near-stationary point for smooth minimax optimization. The recent proposed extra anchored gradient (EAG) methods achieve the optimal convergence rate for the convex-concave minimax problem in deterministic setting. However, the direct extension of EAG to stochastic optimization is not efficient.In this paper, we design a novel stochastic algorithm called Recursive Anchored IteratioN (RAIN). We show that the RAIN achieves near-optimal stochastic first-order oracle (SFO) complexity for stochastic minimax optimization in both convex-concave and strongly-convex-strongly-concave cases. In addition, we extend the idea of RAIN to solve structured nonconvex-nonconcave minimax problem and it also achieves near-optimal SFO complexity."}}
