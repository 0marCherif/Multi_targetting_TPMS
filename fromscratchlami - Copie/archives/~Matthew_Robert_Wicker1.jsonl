{"id": "Ea0fNpgMJh", "cdate": 1681833044890, "mdate": null, "content": {"title": "Individual Fairness in Bayesian Neural Networks", "abstract": "We study Individual Fairness (IF) for Bayesian neural networks (BNNs).\nSpecifically, we consider the $\\epsilon$-$\\delta$-individual fairness notion, which requires that, for any pair of input points that are $\\epsilon$-similar according to a given similarity metrics, the output of the BNN is within a given tolerance $\\delta>0.$\nWe leverage bounds on statistical sampling over the input space and the relationship between adversarial robustness and individual fairness to derive a framework for the systematic estimation of $\\epsilon$-$\\delta$-IF, designing Fair-FGSM and  Fair-PGD as global, fairness-aware extensions to gradient-based attacks for BNNs.  \nWe empirically study IF of a variety of approximately inferred BNNs with different architectures on fairness benchmarks, and compare against deterministic models learnt using frequentist techniques. Interestingly, we find that BNNs trained by means of approximate Bayesian inference consistently tend to be markedly more individually fair than their deterministic counterparts. "}}
{"id": "_hHYaKu0jcj", "cdate": 1663850287704, "mdate": null, "content": {"title": "Robust Explanation Constraints for Neural Networks", "abstract": "Post-hoc explanation methods are used with the intent of providing insights about neural networks and are sometimes said to help engender trust in their outputs. However, popular explanations methods have been found to be fragile to minor perturbations of input features or model parameters. Relying on constraint relaxation techniques from non-convex optimization, we develop a method that upper-bounds the largest change an adversary can make to a gradient-based explanation via bounded manipulation of either the input features or model parameters. By propagating a compact input or parameter set as symbolic intervals through the forwards and backwards computations of the neural network we can formally certify the robustness of gradient-based explanations. Our bounds are differentiable, hence we can incorporate provable explanation robustness into neural network training. Empirically, our method surpasses the robustness provided by previous heuristic approaches. We find that our training method is the only method able to learn neural networks with certificates of explanation robustness across all six datasets tested.  "}}
{"id": "jvZ7WZVFoot", "cdate": 1655029149369, "mdate": null, "content": {"title": "Tractable Uncertainty for Structure Learning", "abstract": "Bayesian structure learning allows one to capture uncertainty over the causal directed acyclic graph (DAG) responsible for generating given data. In this work, we present Tractable Uncertainty for STructure learning (TRUST), a framework for approximate posterior inference that relies on probabilistic circuits as the representation of our posterior belief. In contrast to sample-based posterior approximations, our representation can capture a much richer space of DAGs, while also being able to tractably reason about the uncertainty through a range of useful inference queries. We empirically show how probabilistic circuits can be used as an augmented representation for structure learning methods, leading to improvement in both the quality of inferred structures and posterior uncertainty. Experimental results on conditional query answering further demonstrate the practical utility of the representational capacity of TRUST."}}
{"id": "gIFNal5B6v", "cdate": 1606146137942, "mdate": null, "content": {"title": "Gradient-Free Adversarial Attacks for Bayesian Neural Networks", "abstract": "The existence of adversarial examples, i.e., small perturbations to their inputs that can cause a misclassification, underscores the importance of understanding the robustness of machine learning models. Bayesian neural networks (BNNs), due to their calibrated uncertainty,  have been shown to posses favorable adversarial robustness properties. However, when approximate Bayesian inference methods are employed, the adversarial robustness of BNNs is still not well understood.  In this work, we employ gradient-free optimization methods in order to find adversarial examples for BNNs. In particular, we consider genetic algorithms, surrogate models, as well as zeroth order optimization methods and adapt them to the goal of finding adversarial examples for BNNs.  In an empirical evaluation on the MNIST and Fashion MNIST datasets, we show that for various approximate Bayesian inference methods the usage of  gradient-free algorithms can greatly improve the rate of finding adversarial examples compared to state-of-the-art gradient-based methods."}}
{"id": "BJgyn1BFwS", "cdate": 1569439654808, "mdate": null, "content": {"title": "Global Adversarial Robustness Guarantees for Neural Networks", "abstract": "We investigate global adversarial robustness guarantees for machine learning models.  Specifically, given a trained model we consider the problem of computing the probability that its prediction at any point sampled from the (unknown) input distribution is susceptible to adversarial attacks.  Assuming continuity of the model, we prove measurability for a selection of local robustness properties used in the literature. We then show how concentration inequalities can be employed to compute global robustness with estimation error upper-bounded by $\\epsilon$, for any $\\epsilon > 0$ selected a priori. We utilise the methods to provide statistically sound analysis of the robustness/accuracy trade-off for a variety of neural networks architectures and training methods on MNIST, Fashion-MNIST and CIFAR. We empirically observe that robustness and accuracy tend to be negatively correlated for networks trained via stochastic gradient descent and with iterative pruning techniques, while a positive trend is observed between them in Bayesian settings."}}
