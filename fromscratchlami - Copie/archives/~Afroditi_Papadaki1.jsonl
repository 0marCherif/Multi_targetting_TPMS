{"id": "CmmxQQE6U60A", "cdate": 1663939405033, "mdate": null, "content": {"title": "Federated Fairness without Access to Demographics", "abstract": "Existing federated learning approaches address demographic group fairness assuming that clients are aware of the sensitive groups. Such approaches are not applicable in settings where sensitive groups are unidentified or unavailable. In this paper, we address this limitation by focusing on federated learning settings of fairness without demographics. We present a novel objective that allows trade-offs between (worst-case) group fairness and average utility performance through a hyper-parameter and a group size constraint. We show that the proposed objective recovers existing approaches as special cases and then provide an algorithm to efficiently solve the proposed optimization problem. We experimentally showcase the different solutions that can be achieved by our proposed approach and compare it against baselines on various standard datasets."}}
{"id": "sGKGJdgkgKM", "cdate": 1640995200000, "mdate": 1682341234141, "content": {"title": "Minimax Demographic Group Fairness in Federated Learning", "abstract": ""}}
{"id": "hbL5WNUjKe", "cdate": 1609459200000, "mdate": 1682341234119, "content": {"title": "Blind Pareto Fairness and Subgroup Robustness", "abstract": ""}}
{"id": "DRgBUgR5RO2", "cdate": 1609459200000, "mdate": 1682341234115, "content": {"title": "Federating for Learning Group Fair Models", "abstract": ""}}
{"id": "MMXhHXbNsa-", "cdate": 1601308269403, "mdate": null, "content": {"title": "Blind Pareto Fairness and Subgroup Robustness", "abstract": "With the wide adoption of machine learning algorithms across various application domains, there is a growing interest in the fairness properties of such algorithms. The vast majority of the activity in the field of group fairness addresses disparities between prede\ufb01ned groups based on protected features such as gender, age, and race, which need to be available at train, and often also at test, time. These approaches are static and retrospective, since algorithms designed to protect groups identified  a priori cannot anticipate and protect the needs of different at-risk groups in the future. In this work we analyze the space of solutions for worst-case fairness beyond demographics, and propose Blind Pareto Fairness (BPF), a method that leverages no-regret dynamics to recover a fair minimax classi\ufb01er that reduces worst-case risk of  any potential subgroup of suf\ufb01cient size, and guarantees that the remaining population receives the best possible level of service. BPF addresses  fairness beyond demographics, that is, it  does not rely on prede\ufb01ned notions of at-risk groups, neither at train nor at test time. Our experimental results show that the proposed framework improves worst-case risk in multiple standard datasets, while simultaneously providing better levels of service for the remaining population, in comparison to competing methods."}}
{"id": "Sy4sLoZOZr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Adversarially Learned Representations for Information Obfuscation and Inference", "abstract": "Data collection and sharing are pervasive aspects of modern society. This process can either be voluntary, as in the case of a person taking a facial image to unlock his/her phone, or incidental, s..."}}
{"id": "SJe2so0qF7", "cdate": 1538087844393, "mdate": null, "content": {"title": "Learning data-derived privacy preserving representations from information metrics", "abstract": "It is clear that users should own and control their data and privacy. Utility providers are also becoming more interested in guaranteeing data privacy. Therefore, users and providers can and should collaborate in privacy protecting challenges, and this paper addresses this new paradigm. We propose a framework where the user controls what characteristics of the data they want to share (utility) and what they want to keep private (secret), without necessarily asking the utility provider to change its existing machine learning algorithms. We first analyze the space of privacy-preserving representations and derive natural information-theoretic bounds on the utility-privacy trade-off when disclosing a sanitized version of the data X. We present explicit learning architectures to learn privacy-preserving representations that approach this bound in a data-driven fashion. We describe important use-case scenarios where the utility providers are willing to collaborate with the sanitization process. We study space-preserving transformations where the utility provider can use the same algorithm on original and sanitized data, a critical and novel attribute to help service providers accommodate varying privacy requirements with a single set of utility algorithms. We illustrate this framework through  the implementation of three use cases; subject-within-subject, where we tackle the problem of having a face identity detector that works only on a consenting subset of users, an important application, for example, for mobile devices activated by face recognition; gender-and-subject, where we preserve facial verification while hiding the gender attribute for users who choose to do so; and emotion-and-gender, where we hide independent variables, as is the case of hiding gender while preserving emotion detection."}}
