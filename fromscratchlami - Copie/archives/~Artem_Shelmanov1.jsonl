{"id": "JT-bRz_Sf9r", "cdate": 1672531200000, "mdate": 1682346059447, "content": {"title": "Active Learning for Abstractive Text Summarization", "abstract": "Construction of human-curated annotated datasets for abstractive text summarization (ATS) is very time-consuming and expensive because creating each instance requires a human annotator to read a long document and compose a shorter summary that would preserve the key information relayed by the original document. Active Learning (AL) is a technique developed to reduce the amount of annotation required to achieve a certain level of machine learning model performance. In information extraction and text classification, AL can reduce the amount of labor up to multiple times. Despite its potential for aiding expensive annotation, as far as we know, there were no effective AL query strategies for ATS. This stems from the fact that many AL strategies rely on uncertainty estimation, while as we show in our work, uncertain instances are usually noisy, and selecting them can degrade the model performance compared to passive annotation. We address this problem by proposing the first effective query strategy for AL in ATS based on diversity principles. We show that given a certain annotation budget, using our strategy in AL annotation helps to improve the model performance in terms of ROUGE and consistency scores. Additionally, we analyze the effect of self-learning and show that it can further increase the performance of the model."}}
{"id": "v6NNlubbSQ", "cdate": 1652737787850, "mdate": null, "content": {"title": "Nonparametric Uncertainty Quantification for Single Deterministic Neural Network", "abstract": "  This paper proposes a fast and scalable method for uncertainty quantification of machine learning models' predictions. First, we show the principled way to measure the uncertainty of predictions for a classifier based on Nadaraya-Watson's nonparametric estimate of the conditional label distribution. Importantly, the approach allows to disentangle explicitly \\textit{aleatoric} and \\textit{epistemic} uncertainties. The resulting method works directly in the feature space. However, one can apply it to any neural network by considering an embedding of the data induced by the network. We demonstrate the strong performance of the method in uncertainty estimation tasks on text classification problems and a variety of real-world image datasets, such as MNIST, SVHN, CIFAR-100 and several versions of ImageNet."}}
{"id": "zzAM28XwPB", "cdate": 1640995200000, "mdate": 1682346059433, "content": {"title": "NeuralSympCheck: A Symptom Checking and Disease Diagnostic Neural Model with Logic Regularization", "abstract": "The symptom checking systems inquire users for their symptoms and perform a rapid and affordable medical assessment of their condition. The basic symptom checking systems based on Bayesian methods, decision trees, or information gain methods are easy to train and do not require significant computational resources. However, their drawbacks are low relevance of proposed symptoms and insufficient quality of diagnostics. The best results on these tasks are achieved by reinforcement learning models. Their weaknesses are the difficulty of developing and training such systems and limited applicability to cases with large and sparse decision spaces. We propose a new approach based on the supervised learning of neural models with logic regularization that combines the advantages of the different methods. Our experiments on real and synthetic data show that the proposed approach outperforms the best existing methods in the accuracy of diagnosis when the number of diagnoses and symptoms is large."}}
{"id": "vXq5621G1u", "cdate": 1640995200000, "mdate": 1681833482030, "content": {"title": "Towards Computationally Feasible Deep Active Learning", "abstract": "Akim Tsvigun, Artem Shelmanov, Gleb Kuzmin, Leonid Sanochkin, Daniil Larionov, Gleb Gusev, Manvel Avetisian, Leonid Zhukov. Findings of the Association for Computational Linguistics: NAACL 2022. 2022."}}
{"id": "pcjBjpmhuS", "cdate": 1640995200000, "mdate": 1682346059438, "content": {"title": "Medical Crossing: a Cross-lingual Evaluation of Clinical Entity Linking", "abstract": "Anton Alekseev, Zulfat Miftahutdinov, Elena Tutubalina, Artem Shelmanov, Vladimir Ivanov, Vladimir Kokh, Alexander Nesterov, Manvel Avetisian, Andrei Chertok, Sergey Nikolenko. Proceedings of the Thirteenth Language Resources and Evaluation Conference. 2022."}}
{"id": "kD2RiDJnC5", "cdate": 1640995200000, "mdate": 1682346059432, "content": {"title": "Entity Linking over Nested Named Entities for Russian", "abstract": "Natalia Loukachevitch, Pavel Braslavski, Vladimir Ivanov, Tatiana Batura, Suresh Manandhar, Artem Shelmanov, Elena Tutubalina. Proceedings of the Thirteenth Language Resources and Evaluation Conference. 2022."}}
{"id": "jdHR2JPcyW", "cdate": 1640995200000, "mdate": 1681652388164, "content": {"title": "Active Learning for Abstractive Text Summarization", "abstract": ""}}
{"id": "czdj53l4Xs", "cdate": 1640995200000, "mdate": 1682346059401, "content": {"title": "Neural entity linking: A survey of models based on deep learning", "abstract": "This survey presents a comprehensive description of recent neural entity linking (EL) systems developed since 2015 as a result of the \u201cdeep learning revolution\u201d in natural language processing. Its goal is to systemize design features of neural entity linking systems and compare their performance to the remarkable classic methods on common benchmarks. This work distills a generic architecture of a neural EL system and discusses its components, such as candidate generation, mention-context encoding, and entity ranking, summarizing prominent methods for each of them. The vast variety of modifications of this general architecture are grouped by several common themes: joint entity mention detection and disambiguation, models for global linking, domain-independent techniques including zero-shot and distant supervision methods, and cross-lingual approaches. Since many neural models take advantage of entity and mention/context embeddings to represent their meaning, this work also overviews prominent entity embedding techniques. Finally, the survey touches on applications of entity linking, focusing on the recently emerged use-case of enhancing deep pre-trained masked language models based on the Transformer architecture."}}
{"id": "ZTpAgTZvNIc", "cdate": 1640995200000, "mdate": 1668592014841, "content": {"title": "Uncertainty Estimation of Transformer Predictions for Misclassification Detection", "abstract": "Artem Vazhentsev, Gleb Kuzmin, Artem Shelmanov, Akim Tsvigun, Evgenii Tsymbalov, Kirill Fedyanin, Maxim Panov, Alexander Panchenko, Gleb Gusev, Mikhail Burtsev, Manvel Avetisian, Leonid Zhukov. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022."}}
{"id": "TZVH9QVfk_", "cdate": 1640995200000, "mdate": 1682346059432, "content": {"title": "Medical Image Captioning via Generative Pretrained Transformers", "abstract": "The automatic clinical caption generation problem is referred to as proposed model combining the analysis of frontal chest X-Ray scans with structured patient information from the radiology records. We combine two language models, the Show-Attend-Tell and the GPT-3, to generate comprehensive and descriptive radiology records. The proposed combination of these models generates a textual summary with the essential information about pathologies found, their location, and the 2D heatmaps localizing each pathology on the original X-Ray scans. The proposed model is tested on two medical datasets, the Open-I, MIMIC-CXR, and the general-purpose MS-COCO. The results measured with the natural language assessment metrics prove their efficient applicability to the chest X-Ray image captioning."}}
