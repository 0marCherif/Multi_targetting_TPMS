{"id": "WN9AzJclNj", "cdate": 1675715128869, "mdate": null, "content": {"title": "The Role of Pre-training Data in Transfer Learning", "abstract": "We explore which pre-training dataset should be used to achieve the best transfer learning performance. We investigate the impact of pre-training on the few-shot and full fine-tuning performance using 7 pre-training datasets, and 9 downstream datasets. Through extensive controlled experiments, we find that the choice of the pre-training dataset is essential for the few-shot transfer, but its role decreases as more data is made available for fine-tuning. Additionally, we explore the role of data curation and examine the trade-offs between label noise and the size of the pre-training dataset. We find that using 2000\u00d7 more pre-training data from LAION can match the performance of supervised ImageNet pre-training."}}
{"id": "w-ak2sfl0-", "cdate": 1672531200000, "mdate": 1681496135293, "content": {"title": "The Role of Pre-training Data in Transfer Learning", "abstract": ""}}
{"id": "q_PkAzGFrmq", "cdate": 1663850514708, "mdate": null, "content": {"title": "The Role of Pre-training Data in Transfer Learning", "abstract": "The transfer learning paradigm of model pre-training and subsequent fine-tuning produces high accuracy models. However, a question remains: what data and method should be used for pre-training? We study the effect of the pre-training distribution on transfer learning in the context of image classification. Through controlled experiments, we find that the pre-training dataset is initially important for low-shot transfer. However, the differences between distributions is diminished as more data is made available for fine-tuning. Still, fine-tuning outperforms training from scratch. We also investigate dataset size and observe that larger pre-training datasets lead to better accuracy, however, the absolute accuracy difference is largest in the few-shot regime. Beyond data, we study the effect of the pre-training method, language-image contrastive vs. image-image contrastive, finding that the latter usually leads to better transfer accuracy"}}
{"id": "gU5sJ6ZggcX", "cdate": 1663850480535, "mdate": null, "content": {"title": "REPAIR: REnormalizing Permuted Activations for Interpolation Repair", "abstract": "In this paper we empirically investigate the conjecture from Entezari et al. (2021) which states that if permutation invariance is taken into account, then there should be no loss barrier to the linear interpolation between SGD solutions. We conduct our investigation using standard computer vision architectures trained on CIFAR-10 and ImageNet.  First, we observe a general phenomenon in which interpolated deep networks suffer a collapse in the variance of their activations. We demonstrate that an appropriate rescaling of the pre-activations of the interpolated networks ameliorates this problem and significantly reduces the barrier. Second, by combining this with an algorithm for finding permutations based on maximizing correlations between the activations of matched neurons, we are able to reduce the interpolation barrier for a standard ResNet18 trained on CIFAR-10 to 1.5% absolute test error. We explore the interaction between our method and the choice of normalization layer, and demonstrate its robustness across a variety of architectures and training sets."}}
{"id": "wS7lJi8NZg", "cdate": 1653595784884, "mdate": null, "content": {"title": "How well do contrastively trained models transfer?", "abstract": "There are two prevailing methods for pre-training on large datasets to learn transferable representations: 1) supervised pre-training on large but weakly-labeled datasets; 2) contrastively training on image only and image, text pairs. While supervised pre-training learns good representations that can be transferred to a wide range of tasks, contrastively models such as CLIP have demonstrated unprecedented zero-shot transfer. In this work, we compare the transferability of the two aforementioned methods to multiple downstream tasks. The pre-training distributions we consider include YFCC, Conceptual Captions, and ImageNet-21K while pre-training objectives range from supervised to SimCLR, CLIP, and SLIP. We observe that different pre-training methods with the same training source transfer similarly given their ImageNet accuracy."}}
{"id": "i4UjmpKyAB", "cdate": 1640995200000, "mdate": 1681496135302, "content": {"title": "Understanding the effect of sparsity on neural networks robustness", "abstract": ""}}
{"id": "XGfLLSsNaV-", "cdate": 1640995200000, "mdate": 1681496135300, "content": {"title": "REPAIR: REnormalizing Permuted Activations for Interpolation Repair", "abstract": ""}}
{"id": "GsThM0xl1S", "cdate": 1640995200000, "mdate": 1681496135299, "content": {"title": "The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks", "abstract": ""}}
{"id": "77nY_n-8YZ", "cdate": 1640995200000, "mdate": 1681714698579, "content": {"title": "Studying the impact of magnitude pruning on contrastive learning methods", "abstract": "We study the impact of different pruning techniques on the representation learned by deep neural networks trained with contrastive loss functions. Our work finds that at high sparsity levels, contrastive learning results in a higher number of misclassified examples relative to models trained with traditional cross-entropy loss. To understand this pronounced difference, we use metrics such as the number of PIEs (Hooker et al., 2019), Q-Score (Kalibhat et al., 2022), and PD-Score (Baldock et al., 2021) to measure the impact of pruning on the learned representation quality. Our analysis suggests the schedule of the pruning method implementation matters. We find that the negative impact of sparsity on the quality of the learned representation is the highest when pruning is introduced early on in the training phase."}}
{"id": "2Tvpjzz_yq3", "cdate": 1640995200000, "mdate": 1681887726179, "content": {"title": "SensorFormer: Efficient Many-to-Many Sensor Calibration With Learnable Input Subsampling", "abstract": "Accurate calibration of low-cost environmental sensors is a prerequisite for their successful use in many monitoring applications. State-of-the-art calibration methods vary from simple linear regression to sophisticated deep models based on LSTMs and GRUs. The latter take past measurements to improve calibration accuracy. In this article, we argue that both recent past and close future measurements help to achieve accurate calibration, whereas accuracy improvements beyond the past come with a delay introduced by the occurrence of the future. We propose a generalized many-to-many calibration scheme called SensorFormer based on the successful Transformer model which takes both past and future raw measurements into account. We show that the proposed approach: 1) outperforms other methods by improving calibration accuracy by 16.5%\u201320.4% on public data sets and own field data and 2) can efficiently run on low-power microcontrollers with very limited computational and storage capabilities. The latter is achieved by a novel optimization technique based on learnable input subsampling taking advantage of the properties of typical sensor data. We manage to reduce the model size by 20%\u201333% and minimize the overall floating point operations per second (FLOPs) by 65% while maintaining superior accuracy than state-of-the-art methods."}}
