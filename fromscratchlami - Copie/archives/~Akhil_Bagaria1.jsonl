{"id": "UMfRpi3CAt", "cdate": 1672531200000, "mdate": 1683909185809, "content": {"title": "Scaling Goal-based Exploration via Pruning Proto-goals", "abstract": "One of the gnarliest challenges in reinforcement learning (RL) is exploration that scales to vast domains, where novelty-, or coverage-seeking behaviour falls short. Goal-directed, purposeful behaviours are able to overcome this, but rely on a good goal space. The core challenge in goal discovery is finding the right balance between generality (not hand-crafted) and tractability (useful, not too many). Our approach explicitly seeks the middle ground, enabling the human designer to specify a vast but meaningful proto-goal space, and an autonomous discovery process to refine this to a narrower space of controllable, reachable, novel, and relevant goals. The effectiveness of goal-conditioned exploration with the latter is then demonstrated in three challenging environments."}}
{"id": "nAvBCvT5oA", "cdate": 1663850368322, "mdate": null, "content": {"title": "Learning Portable Skills by Identifying Generalizing Features with an Attention-Based Ensemble", "abstract": "The ability to rapidly generalize is crucial for reinforcement learning to be practical in real-world tasks. However, generalization is complicated by the fact that, in many settings, some state features reliably support generalization while others do not. We consider the problem of learning generalizable policies and skills (in the form of options) by identifying feature sets that generalize across instances. We propose an attention-ensemble approach, where a collection of minimally overlapping feature masks is learned, each of which individually maximizes performance on the source instance. Subsequent tasks are instantiated using the ensemble, and transfer performance is used to update the estimated probability that each feature set will generalize in the future. We show that our approach leads to fast policy generalization for eight tasks in the Procgen benchmark. We then show its use in learning portable options in Montezuma's Revenge, where it is able to generalize skills learned in the first screen to the remainder of the game. "}}
{"id": "M2b38g11qJl", "cdate": 1640995200000, "mdate": 1683909185788, "content": {"title": "Optimistic Initialization for Exploration in Continuous Control", "abstract": "Optimistic initialization underpins many theoretically sound exploration schemes in tabular domains; however, in the deep function approximation setting, optimism can quickly disappear if initialized naively. We propose a framework for more effectively incorporating optimistic initialization into reinforcement learning for continuous control. Our approach uses metric information about the state-action space to estimate which transitions are still unexplored, and explicitly maintains the initial Q-value optimism for the corresponding state-action pairs. We also develop methods for efficiently approximating these training objectives, and for incorporating domain knowledge into the optimistic envelope to improve sample efficiency. We empirically evaluate these approaches on a variety of hard exploration problems in continuous control, where our method outperforms existing exploration techniques."}}
{"id": "g3hG1bls1d", "cdate": 1609459200000, "mdate": 1683909185774, "content": {"title": "Skill Discovery for Exploration and Planning using Deep Skill Graphs", "abstract": "We introduce a new skill-discovery algorithm that builds a discrete graph representation of large continuous MDPs, where nodes correspond to skill subgoals and the edges to skill policies. The agen..."}}
{"id": "HKr8FUm4qRS", "cdate": 1609459200000, "mdate": 1683909185843, "content": {"title": "Robustly Learning Composable Options in Deep Reinforcement Learning", "abstract": "Hierarchical reinforcement learning (HRL) is only effective for long-horizon problems when high-level skills can be reliably sequentially executed. Unfortunately, learning reliably composable skills is difficult, because all the components of every skill are constantly changing during learning. We propose three methods for improving the composability of learned skills: representing skill initiation regions using a combination of pessimistic and optimistic classifiers; learning re-targetable policies that are robust to non-stationary subgoal regions; and learning robust option policies using model-based RL. We test these improvements on four sparse-reward maze navigation tasks involving a simulated quadrupedal robot. Each method successively improves the robustness of a baseline skill discovery method, substantially outperforming state-of-the-art flat and hierarchical methods."}}
{"id": "-mvAo5hWNp", "cdate": 1591922548644, "mdate": null, "content": {"title": "Skill Discovery for Exploration and Planning using Deep Skill Graphs", "abstract": "Temporal abstraction provides an opportunity to drastically lower the decision making burden facing reinforcement learning agents  in rich sensorimotor spaces. Well constructed hierarchies induce state and action abstractions that can reduce large continuous MDPs into small discrete ones, in which planning with a learned model is feasible. We propose a novel algorithm, Deep Skill Graphs, for acquiring such a minimal representation of an environment. Our algorithm seamlessly interleaves discovering skills and planning using them to gain unsupervised mastery over ever increasing portions of the state-space. The constructed skill graph can be used to drive the agent to novel goals at test time, requiring little-to-no additional learning. We test our algorithm on a series of continuous control tasks where it outperforms baseline flat and hierarchical RL methods alike."}}
{"id": "PjCptH5J6d", "cdate": 1577836800000, "mdate": 1683909185844, "content": {"title": "Option Discovery using Deep Skill Chaining", "abstract": "We present a new hierarchical reinforcement learning algorithm which can solve high-dimensional goal-oriented tasks more reliably than non-hierarchical agents and other state-of-the-art skill discovery techniques."}}
{"id": "B1grPT9GTH", "cdate": 1575296348703, "mdate": null, "content": {"title": "[Replication] A Unified Bellman Optimality Principle Combining Reward Maximization and Empowerment", "abstract": "Designing learning agents that gain broad competence in a self-motivated manner is a longstanding goal of reinforcement learning. Empowerment is a task-agnostic information-theoretic quantity that has recently been used to intrinsically motivate reinforcement learning agents. Leibfried et al. 2019 showed how to combine empowerment with traditional task-specific reward maximization. In this work, we replicate the main empirical results of their paper. In particular, we reproduce the main algorithm of the paper, empowered actor-critic (EAC) and compare its performance with state-of-the-art baselines: soft actor-critic (SAC), proximal policy optimization (PPO), and deep deterministic policy gradients (DDPG) on a series of continuous control tasks in the MuJoCo simulator. We find that the performance of our implementation of EAC closely follows that of the original paper. However, our empirical findings also suggest that EAC is unable to improve upon baseline actor-critic algorithms . We share our code, raw learning curves and the scripts used to produce the figures in this paper."}}
{"id": "B1gqipNYwH", "cdate": 1569439138389, "mdate": null, "content": {"title": "Option Discovery using Deep Skill Chaining", "abstract": "Autonomously discovering temporally extended actions, or skills, is a longstanding goal of hierarchical reinforcement learning. We propose a new algorithm that combines skill chaining with deep neural networks to autonomously discover skills in high-dimensional, continuous domains. The resulting algorithm, deep skill chaining, constructs skills with the property that executing one enables the agent to execute another. We demonstrate that deep skill chaining significantly outperforms both non-hierarchical agents and other state-of-the-art skill discovery techniques in challenging continuous control tasks."}}
{"id": "skscMoMbdS6", "cdate": 1388534400000, "mdate": 1683909185832, "content": {"title": "Sphynx: A Shared Instruction Cache Exporatory Study", "abstract": "The Sphynx project was an exploratory study to discover what might be done to improve the heavy replication of in- structions in independent instruction caches for a massively parallel machine where a single program is executing across all of the cores. While a machine with only many cores (fewer than 50) might not have any issues replicating the instructions for each core, as we approach the era where thousands of cores can be placed on one chip, the overhead of instruction replication may become unacceptably large. We believe that a large amount of sharing should be possible when the ma- chine is configured for all of the threads to issue from the same set of instructions. We propose a technique that allows sharing an instruction cache among a number of independent processor cores to allow for inter-thread sharing and reuse of instruction memory. While we do not have test cases to demonstrate the potential magnitude of performance gains that could be achieved, the potential for sharing reduces the die area required for instruction storage on chip."}}
