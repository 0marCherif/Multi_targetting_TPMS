{"id": "8xizR4144u7", "cdate": 1609459200000, "mdate": null, "content": {"title": "Efficient Encrypted Inference on Ensembles of Decision Trees", "abstract": "Data privacy concerns often prevent the use of cloud-based machine learning services for sensitive personal data. While homomorphic encryption (HE) offers a potential solution by enabling computations on encrypted data, the challenge is to obtain accurate machine learning models that work within the multiplicative depth constraints of a leveled HE scheme. Existing approaches for encrypted inference either make ad-hoc simplifications to a pre-trained model (e.g., replace hard comparisons in a decision tree with soft comparators) at the cost of accuracy or directly train a new depth-constrained model using the original training set. In this work, we propose a framework to transfer knowledge extracted by complex decision tree ensembles to shallow neural networks (referred to as DTNets) that are highly conducive to encrypted inference. Our approach minimizes the accuracy loss by searching for the best DTNet architecture that operates within the given depth constraints and training this DTNet using only synthetic data sampled from the training data distribution. Extensive experiments on real-world datasets demonstrate that these characteristics are critical in ensuring that DTNet accuracy approaches that of the original tree ensemble. Our system is highly scalable and can perform efficient inference on batched encrypted (134 bits of security) data with amortized time in milliseconds. This is approximately three orders of magnitude faster than the standard approach of applying soft comparison at the internal nodes of the ensemble trees."}}
{"id": "wpuQHfGtL4Y", "cdate": 1577836800000, "mdate": null, "content": {"title": "Privacy Enhanced Decision Tree Inference", "abstract": "In many areas in machine learning, decision trees play a crucial role in classification and regression. When a decision tree based classifier is hosted as a service in a critical application with the need for privacy protection of the service as well as the user data, fully homomorphic encrypted can be employed. However, a decision node in a decision tree can't be directly implemented in FHE. In this paper, we describe an end-to-end approach to support privacyenhanced decision tree classification using IBM supported open-source library HELib. Using several options for building a decision node and employing oblivious computations coupled with an argmax function in FHE we show that a highly secure and trusted decision tree service can be enabled."}}
{"id": "Lp60YNDpFr0", "cdate": 1577836800000, "mdate": null, "content": {"title": "Approximation algorithms for connected maximum cut and related problems", "abstract": "An instance of the Connected Maximum Cut problem consists of an undirected graph G = ( V , E ) and the goal is to find a subset of vertices S \u2286 V that maximizes the number of edges in the cut \u03b4 ( S ) such that the induced graph G [ S ] is connected. We present the first non-trivial \u03a9 ( 1 log \u2061 n ) approximation algorithm for the Connected Maximum Cut problem in general graphs using novel techniques. We then extend our algorithm to edge weighted case and obtain a poly-logarithmic approximation algorithm. Interestingly, in contrast to the classical Max-Cut problem that can be solved in polynomial time on planar graphs, we show that the Connected Maximum Cut problem remains NP-hard on unweighted, planar graphs. On the positive side, we obtain a polynomial time approximation scheme for the Connected Maximum Cut problem on planar graphs and more generally on bounded genus graphs."}}
{"id": "KXEkme8Ye_R", "cdate": 1577836800000, "mdate": null, "content": {"title": "Analyzing the Optimal Neighborhood: Algorithms for Partial and Budgeted Connected Dominating Set Problems", "abstract": "We study partial and budgeted versions of the well-studied connected dominating set problem. In the partial connected dominating set (PCDS) problem, we are given an undirected graph $G = (V,E)$ and an integer $n'$, and the goal is to find a minimum subset of vertices that induces a connected subgraph of $G$ and dominates at least $n'$ vertices. We obtain the first polynomial time algorithm with an $O(\\ln \\Delta)$ approximation guarantee for this problem, thereby significantly extending the results of Guha and Khuller [Algorithmica, 20(1998), pp. 374--387] for the connected dominating set problem. We note that none of the methods developed earlier can be applied directly to solve this problem. In the budgeted connected dominating set problem, there is a budget on the number of vertices we can select, and the goal is to dominate as many vertices as possible. We obtain a $\\frac{1}{12}(1-\\frac{1}{e})$ approximation algorithm for this problem. Finally, we show that our techniques extend to a more general setting where the profit function associated with a subset of vertices is a \u201cspecial\u201d submodular function. This generalization captures the connected dominating set problem with capacities and/or weighted profits as special cases. This implies an $O(\\ln q)$ approximation (where $q$ denotes the quota) and $O(1)$ approximation algorithms for the partial and budgeted versions of these problems. While the algorithms are simple, the results make a surprising use of the greedy set cover framework in defining a useful profit function. Finally, we prove that (both edge and node) weighted versions of the PCDS problem are as hard as the more general group Steiner tree problem."}}
{"id": "0LSrjB8IUkS", "cdate": 1577836800000, "mdate": null, "content": {"title": "Maximizing Throughput in Flow Shop Real-Time Scheduling", "abstract": "We consider scheduling real-time jobs in the classic flow shop model. The input is a set of n jobs, each consisting of m segments to be processed on m machines in the specified order, such that segment I_i of a job can start processing on machine M_i only after segment I_{i-1} of the same job completed processing on machine M_{i-1}, for 2 \u2264 i \u2264 m. Each job also has a release time, a due date, and a weight. The objective is to maximize the throughput (or, profit) of the n jobs, i.e., to find a subset of the jobs that have the maximum total weight and can complete processing on the m machines within their time windows. This problem has numerous real-life applications ranging from manufacturing to cloud and embedded computing platforms, already in the special case where m = 2. Previous work in the flow shop model has focused on makespan, flow time, or tardiness objectives. However, little is known for the flow shop model in the real-time setting. In this work, we give the first nontrivial results for this problem and present a pseudo-polynomial time (2m+1)-approximation algorithm for the problem on m \u2265 2 machines, where m is a constant. This ratio is essentially tight due to a hardness result of \u03a9(m/(log m)) for the approximation ratio. We further give a polynomial-time algorithm for the two-machine case, with an approximation ratio of (9+\u03b5) where \u03b5 = O(1/n). We obtain better bounds for some restricted subclasses of inputs with two machines. To the best of our knowledge, this fundamental problem of throughput maximization in the flow shop scheduling model is studied here for the first time."}}
{"id": "rX0VMyQe_TB", "cdate": 1546300800000, "mdate": null, "content": {"title": "Blockchain Enabled AI Marketplace: The Price You Pay for Trust.", "abstract": "There has been a considerable amount of interest in exploring blockchain technologies for enabling marketplaces of different kinds. In this work, we provide a blockchain implementation that enables an \"AI marketplace\": a platform where consumers and data providers can transact data and/or models and derive value. Preserving privacy and trust during these transactions is a paramount concern. As an enabling use case, we consider a transfer learning setting. In this setting, a consumer entity wants to acquire a large training set, from different private data providers, that matches a small validation dataset provided by the consumer. Data providers expect fair value for their contribution and the consumer also wants to maximize its benefit. We implement a distributed protocol on a blockchain that provides guarantees on privacy and consumer's benefit. We also demonstrate that our blockchain implementation plays a crucial role in addressing the issue of fair value attribution and privacy in a trustable way. We consider three different designs for a blockchain implementation that trades off trust requirements on different entities and the overhead in terms of time taken for completion of the task. The first design provides no trust guarantees. The second one guarantees trust with respect to other participants if the platform is trustworthy. The third one guarantees complete trust with no requirements. Our experiments show that the performance in the second and third cases, with partial/complete trust guarantees, degrade by roughly 2x and 5x respectively, compared to the baseline with no trust guarantees."}}
{"id": "ifjP8kJxIx", "cdate": 1546300800000, "mdate": null, "content": {"title": "Blockchain analytics and artificial intelligence", "abstract": "Blockchain records track information about financial payments, movements of products through supply chains, identity verification information, and many other assets. Analytics on this data can provide provenance histories, predictive planning, fraud identification, and regulatory compliance.\u00a0In this paper, we describe analytics engines connected to blockchains to provide easy-to-use configurable dashboards, predictive models, provenance histories, and compliance checking.\u00a0We also describe how blockchain data can be combined with external data sources for secure and private analytics, enable artificial intelligence (AI) model creation over geographically dispersed data, and create a history of model creation enabling provenance and lineage tracking for trusted AI."}}
{"id": "VCON7AlmPCP", "cdate": 1546300800000, "mdate": null, "content": {"title": "Constrained submodular maximization via greedy local search", "abstract": "We present a simple combinatorial 1 \u2212 e \u2212 2 2 -approximation algorithm for maximizing a monotone submodular function subject to a knapsack and a matroid constraint. This classic problem is known to be hard to approximate within factor better than 1 \u2212 1 \u2215 e . We extend the algorithm to yield 1 \u2212 e \u2212 ( k + 1 ) k + 1 approximation for submodular maximization subject to a single knapsack and k matroid constraints, for any fixed k > 1 . Our algorithms, which combine the greedy algorithm of Khuller et\u00a0al. (1999) and Sviridenko (2004) with local search, show the power of this natural framework in submodular maximization with combined constraints."}}
{"id": "TGMI8X1iIFl", "cdate": 1546300800000, "mdate": null, "content": {"title": "Differentially Private Distributed Data Summarization under Covariate Shift", "abstract": "We envision AI marketplaces to be platforms where consumers, with very less data for a target task, can obtain a relevant model by accessing many private data sources with vast number of data samples. One of the key challenges is to construct a training dataset that matches a target task without compromising on privacy of the data sources. To this end, we consider the following distributed data summarizataion problem. Given K private source datasets denoted by $[D_i]_{i\\in [K]}$ and a small target validation set $D_v$, which may involve a considerable covariate shift with respect to the sources, compute a summary dataset $D_s\\subseteq \\bigcup_{i\\in [K]} D_i$ such that its statistical distance from the validation dataset $D_v$ is minimized. We use the popular Maximum Mean Discrepancy as the measure of statistical distance. The non-private problem has received considerable attention in prior art, for example in prototype selection (Kim et al., NIPS 2016). Our work is the first to obtain strong differential privacy guarantees while ensuring the quality guarantees of the non-private version. We study this problem in a Parsimonious Curator Privacy Model, where a trusted curator coordinates the summarization process while minimizing the amount of private information accessed. Our central result is a novel protocol that (a) ensures the curator accesses at most $O(K^{\\frac{1}{3}}|D_s| + |D_v|)$ points (b) has formal privacy guarantees on the leakage of information between the data owners and (c) closely matches the best known non-private greedy algorithm. Our protocol uses two hash functions, one inspired by the Rahimi-Recht random features method and the second leverages state of the art differential privacy mechanisms. We introduce a novel \"noiseless\" differentially private auctioning protocol for winner notification and demonstrate the efficacy of our protocol using real-world datasets."}}
{"id": "QQMy0D_D6Jd", "cdate": 1546300800000, "mdate": null, "content": {"title": "Generalized Assignment via Submodular Optimization with Reserved Capacity", "abstract": "We study a variant of the generalized assignment problem (GAP) with group constraints. An instance of (Group GAP) is a set I of items, partitioned into L groups, and a set of m uniform (unit-sized) bins. Each item i in I has a size s_i >0, and a profit p_{i,j} >= 0 if packed in bin j. A group of items is satisfied if all of its items are packed. The goal is to find a feasible packing of a subset of the items in the bins such that the total profit from satisfied groups is maximized. We point to central applications of Group GAP in Video-on-Demand services, mobile Device-to-Device network caching and base station cooperation in 5G networks. Our main result is a 1/6-approximation algorithm for Group GAP instances where the total size of each group is at most m/2. At the heart of our algorithm lies an interesting derivation of a submodular function from the classic LP formulation of GAP, which facilitates the construction of a high profit solution utilizing at most half the total bin capacity, while the other half is reserved for later use. In particular, we give an algorithm for submodular maximization subject to a knapsack constraint, which finds a solution of profit at least 1/3 of the optimum, using at most half the knapsack capacity, under mild restrictions on element sizes. Our novel approach of submodular optimization subject to a knapsack with reserved capacity constraint may find applications in solving other group assignment problems."}}
