{"id": "IV3tbEYK0y", "cdate": 1640995200000, "mdate": 1674998064002, "content": {"title": "YM2413-MDB: A Multi-Instrumental FM Video Game Music Dataset with Emotion Annotations", "abstract": "Existing multi-instrumental datasets tend to be biased toward pop and classical music. In addition, they generally lack high-level annotations such as emotion tags. In this paper, we propose YM2413-MDB, an 80s FM video game music dataset with multi-label emotion annotations. It includes 669 audio and MIDI files of music from Sega and MSX PC games in the 80s using YM2413, a programmable sound generator based on FM. The collected game music is arranged with a subset of 15 monophonic instruments and one drum instrument. They were converted from binary commands of the YM2413 sound chip. Each song was labeled with 19 emotion tags by two annotators and validated by three verifiers to obtain refined tags. We provide the baseline models and results for emotion recognition and emotion-conditioned symbolic music generation using YM2413-MDB."}}
{"id": "DjXUMdBUrJ0", "cdate": 1609459200000, "mdate": 1650546591328, "content": {"title": "Tr\u00e4umerAI: Dreaming Music with StyleGAN", "abstract": "The goal of this paper to generate a visually appealing video that responds to music with a neural network so that each frame of the video reflects the musical characteristics of the corresponding audio clip. To achieve the goal, we propose a neural music visualizer directly mapping deep music embeddings to style embeddings of StyleGAN, named Tr\\\"aumerAI, which consists of a music auto-tagging model using short-chunk CNN and StyleGAN2 pre-trained on WikiArt dataset. Rather than establishing an objective metric between musical and visual semantics, we manually labeled the pairs in a subjective manner. An annotator listened to 100 music clips of 10 seconds long and selected an image that suits the music among the 200 StyleGAN-generated examples. Based on the collected data, we trained a simple transfer function that converts an audio embedding to a style embedding. The generated examples show that the mapping between audio and video makes a certain level of intra-segment similarity and inter-segment dissimilarity."}}
{"id": "mP9fP8LkHoR", "cdate": 1577836800000, "mdate": 1650546591328, "content": {"title": "Polyphonic Piano Transcription Using Autoregressive Multi-State Note Model", "abstract": ""}}
{"id": "6y1gI8WNMLo", "cdate": 1577836800000, "mdate": null, "content": {"title": "Polyphonic Piano Transcription Using Autoregressive Multi-State Note Model", "abstract": "Recent advances in polyphonic piano transcription have been made primarily by a deliberate design of neural network architectures that detect different note states such as onset or sustain and model the temporal evolution of the states. The majority of them, however, use separate neural networks for each note state, thereby optimizing multiple loss functions, and also they handle the temporal evolution of note states by abstract connections between the state-wise neural networks or using a post-processing module. In this paper, we propose a unified neural network architecture where multiple note states are predicted as a softmax output with a single loss function and the temporal order is learned by an auto-regressive connection within the single neural network. This compact model allows to increase note states without architectural complexity. Using the MAESTRO dataset, we examine various combinations of multiple note states including on, onset, sustain, re-onset, offset, and off. We also show that the autoregressive module effectively learns inter-state dependency of notes. Finally, we show that our proposed model achieves performance comparable to state-of-the-arts with fewer parameters."}}
{"id": "qOoFEik0niq", "cdate": 1546300800000, "mdate": 1650546591518, "content": {"title": "VirtuosoNet: A Hierarchical RNN-based System for Modeling Expressive Piano Performance", "abstract": ""}}
{"id": "S14AG2W_-r", "cdate": 1546300800000, "mdate": null, "content": {"title": "Graph Neural Network for Music Score Data and Modeling Expressive Piano Performance", "abstract": "Music score is often handled as one-dimensional sequential data. Unlike words in a text document, notes in music score can be played simultaneously by the polyphonic nature and each of them has its..."}}
{"id": "0GZSz-t5JGc", "cdate": 1546300800000, "mdate": null, "content": {"title": "A Cross-Scape Plot Representation for Visualizing Symbolic Melodic Similarity", "abstract": ""}}
{"id": "JkuqphOQWbb", "cdate": 1514764800000, "mdate": 1650546591328, "content": {"title": "A Timbre-based Approach to Estimate Key Velocity from Polyphonic Piano Recordings", "abstract": ""}}
{"id": "IphT9a16HuN", "cdate": 1483228800000, "mdate": null, "content": {"title": "Audio-to-score alignment of piano music using RNN-based automatic music transcription", "abstract": "We propose a framework for audio-to-score alignment on piano performance that employs automatic music transcription (AMT) using neural networks. Even though the AMT result may contain some errors, the note prediction output can be regarded as a learned feature representation that is directly comparable to MIDI note or chroma representation. To this end, we employ two recurrent neural networks that work as the AMT-based feature extractors to the alignment algorithm. One predicts the presence of 88 notes or 12 chroma in frame-level and the other detects note onsets in 12 chroma. We combine the two types of learned features for the audio-to-score alignment. For comparability, we apply dynamic time warping as an alignment algorithm without any additional post-processing. We evaluate the proposed framework on the MAPS dataset and compare it to previous work. The result shows that the alignment framework with the learned features significantly improves the accuracy, achieving less than 10 ms in mean onset error."}}
