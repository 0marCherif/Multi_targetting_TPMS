{"id": "fadoo8Xs6pH", "cdate": 1663850354767, "mdate": null, "content": {"title": "Towards Antisymmetric Neural Ansatz Separation", "abstract": "We study separations between two fundamental models (or \\emph{Ans\u00e4tze}) of antisymmetric functions, that is, functions $f$ of the form $f(x_{\\sigma(1)}, \\ldots, x_{\\sigma(N)}) = \\text{sign}(\\sigma)f(x_1, \\ldots, x_N)$, where $\\sigma$ is any permutation. \nThese arise in the context of quantum chemistry, and are the basic modeling tool for wavefunctions of Fermionic systems. \nSpecifically, we consider two popular antisymmetric Ans\u00e4tze: the Slater representation, which leverages the alternating structure of determinants, and the Jastrow ansatz, which augments Slater determinants with a product by an arbitrary symmetric function. We construct an antisymmetric function that can be more efficiently expressed in Jastrow form, yet provably cannot be approximated by Slater determinants unless there are exponentially (in $N^2$) many terms. This represents the first explicit quantitative separation between these two Ans\u00e4tze."}}
{"id": "jjlQkcHxkp0", "cdate": 1652737563683, "mdate": null, "content": {"title": "Exponential Separations in Symmetric Neural Networks", "abstract": "In this work we demonstrate a novel separation between symmetric neural network architectures.  Specifically, we consider the Relational Network~\\parencite{santoro2017simple} architecture as a natural generalization of the DeepSets~\\parencite{zaheer2017deep} architecture, and study their representational gap. Under the restriction to analytic activation functions, we construct a symmetric function acting on sets of size $N$ with elements in dimension $D$, which can be efficiently approximated by the former architecture, but provably requires width exponential in $N$ and $D$ for the latter. "}}
{"id": "sew79Me0W0c", "cdate": 1602926480227, "mdate": null, "content": {"title": "Neural Algorithms for Graph Navigation", "abstract": "The application of deep reinforcement learning (RL) to graph learning and meta-learning admits challenges from both topics.  We consider the task of one-shot, partially observed graph navigation, acknowledging and addressing the difficulties of partially observed graph environments.  In this work, we present a framework for graph meta-learning, and we propose an agent equipped with external memory and local action priors adapted to the underlying graphs.  We demonstrate the efficacy of our framework through partially-observed navigation on synthetic graphs, as well as application to partially-observed navigation on 3D meshes, showing substantially improvement in one-shot performance over baseline agents."}}
{"id": "HkNrfjbuZH", "cdate": 1546300800000, "mdate": null, "content": {"title": "Graphite: Iterative Generative Modeling of Graphs", "abstract": "Graphs are a fundamental abstraction for modeling relational data. However, graphs are discrete and combinatorial in nature, and learning representations suitable for machine learning tasks poses s..."}}
{"id": "H1eSS3CcKX", "cdate": 1538087997425, "mdate": null, "content": {"title": "Stochastic Optimization of Sorting Networks via Continuous Relaxations", "abstract": "Sorting input objects is an important step in many machine learning pipelines. However, the sorting operator is non-differentiable with respect to its inputs, which prohibits end-to-end gradient-based optimization. In this work, we propose NeuralSort, a general-purpose continuous relaxation of the output of the sorting operator from permutation matrices to the set of unimodal row-stochastic matrices, where every row sums to one and has a distinct argmax. This relaxation permits straight-through optimization of any computational graph involve a sorting operation. Further, we use this relaxation to enable gradient-based stochastic optimization over the combinatorially large space of permutations by deriving a reparameterized gradient estimator for the Plackett-Luce family of distributions over permutations. We demonstrate the usefulness of our framework on three tasks that require learning semantic orderings of high-dimensional objects, including a fully differentiable, parameterized extension of the k-nearest neighbors algorithm"}}
