{"id": "2CcZGB0ToA", "cdate": 1680531218604, "mdate": 1680531218604, "content": {"title": "Performative Prediction in a Stateful World", "abstract": "Deployed supervised machine learning models make predictions that interact with and influence the world. This phenomenon is called performative prediction by Perdomo et al. (ICML 2020). It is an ongoing challenge to understand the influence of such predictions as well as design tools so as to control that influence. We propose a theoretical framework where the response of a target population to the deployed classifier is modeled as a function of the classifier and the current state (distribution) of the population. We show necessary and sufficient conditions for convergence to an equilibrium of two retraining algorithms, repeated risk minimization and a lazier variant. Furthermore, convergence is near an optimal classifier. We thus generalize results of Perdomo et al., whose performativity framework does not assume any dependence on the state of the target population. A particular phenomenon captured by our model is that of distinct groups that acquire information and resources at different rates to be able to respond to the latest deployed classifier. We study this phenomenon theoretically and empirically. "}}
{"id": "MlZP_uCZ5P", "cdate": 1680531141868, "mdate": 1680531141868, "content": {"title": "Fast, Sample-Efficient, Affine-Invariant Private Mean and Covariance Estimation for Subgaussian Distributions", "abstract": "We present a fast, differentially private algorithm for high-dimensional covariance-aware mean estimation with nearly optimal sample complexity. Only exponential-time estimators were previously known to achieve this guarantee. Given $n$ samples from a (sub-)Gaussian distribution with unknown mean $\\mu$ and covariance $\\Sigma$, our $(\\varepsilon,\\delta)$-differentially private estimator produces $\\tilde{\\mu}$ such that $\\|\\mu - \\tilde{\\mu}\\|_{\\Sigma} \\leq \\alpha$ as long as $n \\gtrsim \\tfrac d {\\alpha^2} + \\tfrac{d \\sqrt{\\log 1/\\delta}}{\\alpha \\varepsilon}+\\frac{d\\log 1/\\delta}{\\varepsilon}$. The Mahalanobis error metric $\\|\\mu - \\hat{\\mu}\\|_{\\Sigma}$ measures the distance between $\\hat \\mu$ and $\\mu$ relative to $\\Sigma$; it characterizes the error of the sample mean. Our algorithm runs in time $\\tilde{O}(nd^{\\omega - 1} + nd/\\eps)$, where $\\omega < 2.38$ is the matrix multiplication exponent.\n\nWe  adapt an exponential-time approach of Brown, Gaboardi, Smith, Ullman, and Zakynthinou (2021), giving efficient variants of stable mean and covariance estimation subroutines that also improve the sample complexity to the nearly optimal bound above.\n\nOur stable covariance estimator can be turned to private covariance estimation for unrestricted subgaussian distributions. With $n\\gtrsim d^{3/2}$ samples, our estimate is accurate in spectral norm. This is the first such algorithm using $n= o(d^2)$ samples, answering an open question posed by Alabi et al. (2022). With $n\\gtrsim d^2$ samples, our estimate is accurate in Frobenius norm. This leads to a fast, nearly optimal algorithm for private learning of unrestricted Gaussian distributions in TV distance.\n\nDuchi, Haque, and Kuditipudi (2023) obtained similar results independently and concurrently."}}
{"id": "TScq50yPzJ", "cdate": 1680530915639, "mdate": 1680530915639, "content": {"title": "Strong Memory Lower Bounds for Learning Natural Models", "abstract": "We give lower bounds on the amount of memory required by one-pass streaming algorithms for solving several natural learning problems. In a setting where examples lie in $\\{0,1\\}^d$ and the optimal classifier can be encoded using $\\kappa$ bits, we show that algorithms which learn using a near-minimal number of examples, $\\tilde{O}(\\kappa)$, must use $\\tilde{\\Omega}(d\\kappa)$ bits of space. Our space bounds match the dimension of the ambient space of the problem's natural parametrization, even when it is quadratic in the size of examples and the final classifier. For instance, in the setting of $d$-sparse linear classifiers over degree-2 polynomial features, for which $\\kappa=\\Theta(d \\log d)$, our space lower bound is $\\tilde{\\Omega}(d^2)$. Our bounds degrade gracefully with the stream length $N$, generally having the form $\\tilde{\\Omega}(dk \\cdot \\frac{\\kappa}{N})$.\nBounds of the form $\\Omega(d\\kappa)$ were known for learning parity and other problems defined over finite fields. Bounds that apply in a narrow range of sample sizes are also known for linear regression. Ours are the first such bounds for problems of the type commonly seen in recent learning applications that apply for a large range of input sizes. "}}
{"id": "vHiCXW2HKwS", "cdate": 1680530728213, "mdate": 1680530728213, "content": {"title": "When is Memorization of Irrelevant Training Data Necessary for High-Accuracy Learning?", "abstract": "Modern machine learning models are complex and frequently encode surprising amounts of information about individual inputs. In extreme cases, complex models appear to memorize entire input examples, including seemingly irrelevant information (social security numbers from text, for example). In this paper, we aim to understand whether this sort of memorization is necessary for accurate learning. We describe natural prediction problems in which every sufficiently accurate training algorithm must encode, in the prediction model, essentially all the information about a large subset of its training examples. This remains true even when the examples are high-dimensional and have entropy much higher than the sample size, and even when most of that information is ultimately irrelevant to the task at hand. Further, our results do not depend on the training algorithm or the class of models used for learning.\nOur problems are simple and fairly natural variants of the next-symbol prediction and the cluster labeling tasks. These tasks can be seen as abstractions of text- and image-related prediction problems. To establish our results, we reduce from a family of one-way communication problems for which we prove new information complexity lower bounds. Additionally, we present synthetic-data experiments demonstrating successful attacks on logistic regression and neural network classifiers. "}}
{"id": "yeXzjR0SVZ", "cdate": 1672531200000, "mdate": 1706926957173, "content": {"title": "Metalearning with Very Few Samples Per Task", "abstract": "Metalearning and multitask learning are two frameworks for solving a group of related learning tasks more efficiently than we could hope to solve each of the individual tasks on their own. In multitask learning, we are given a fixed set of related learning tasks and need to output one accurate model per task, whereas in metalearning we are given tasks that are drawn i.i.d. from a metadistribution and need to output some common information that can be easily specialized to new, previously unseen tasks from the metadistribution. In this work, we consider a binary classification setting where tasks are related by a shared representation, that is, every task $P$ of interest can be solved by a classifier of the form $f_{P} \\circ h$ where $h \\in H$ is a map from features to some representation space that is shared across tasks, and $f_{P} \\in F$ is a task-specific classifier from the representation space to labels. The main question we ask in this work is how much data do we need to metalearn a good representation? Here, the amount of data is measured in terms of both the number of tasks $t$ that we need to see and the number of samples $n$ per task. We focus on the regime where the number of samples per task is extremely small. Our main result shows that, in a distribution-free setting where the feature vectors are in $\\mathbb{R}^d$, the representation is a linear map from $\\mathbb{R}^d \\to \\mathbb{R}^k$, and the task-specific classifiers are halfspaces in $\\mathbb{R}^k$, we can metalearn a representation with error $\\varepsilon$ using just $n = k+2$ samples per task, and $d \\cdot (1/\\varepsilon)^{O(k)}$ tasks. Learning with so few samples per task is remarkable because metalearning would be impossible with $k+1$ samples per task, and because we cannot even hope to learn an accurate task-specific classifier with just $k+2$ samples per task."}}
{"id": "Wt1qrTmvua", "cdate": 1672531200000, "mdate": 1682746126064, "content": {"title": "Fast, Sample-Efficient, Affine-Invariant Private Mean and Covariance Estimation for Subgaussian Distributions", "abstract": "We present a fast, differentially private algorithm for high-dimensional covariance-aware mean estimation with nearly optimal sample complexity. Only exponential-time estimators were previously known to achieve this guarantee. Given $n$ samples from a (sub-)Gaussian distribution with unknown mean $\\mu$ and covariance $\\Sigma$, our $(\\varepsilon,\\delta)$-differentially private estimator produces $\\tilde{\\mu}$ such that $\\|\\mu - \\tilde{\\mu}\\|_{\\Sigma} \\leq \\alpha$ as long as $n \\gtrsim \\tfrac d {\\alpha^2} + \\tfrac{d \\sqrt{\\log 1/\\delta}}{\\alpha \\varepsilon}+\\frac{d\\log 1/\\delta}{\\varepsilon}$. The Mahalanobis error metric $\\|\\mu - \\hat{\\mu}\\|_{\\Sigma}$ measures the distance between $\\hat \\mu$ and $\\mu$ relative to $\\Sigma$; it characterizes the error of the sample mean. Our algorithm runs in time $\\tilde{O}(nd^{\\omega - 1} + nd/\\varepsilon)$, where $\\omega < 2.38$ is the matrix multiplication exponent. We adapt an exponential-time approach of Brown, Gaboardi, Smith, Ullman, and Zakynthinou (2021), giving efficient variants of stable mean and covariance estimation subroutines that also improve the sample complexity to the nearly optimal bound above. Our stable covariance estimator can be turned to private covariance estimation for unrestricted subgaussian distributions. With $n\\gtrsim d^{3/2}$ samples, our estimate is accurate in spectral norm. This is the first such algorithm using $n= o(d^2)$ samples, answering an open question posed by Alabi et al. (2022). With $n\\gtrsim d^2$ samples, our estimate is accurate in Frobenius norm. This leads to a fast, nearly optimal algorithm for private learning of unrestricted Gaussian distributions in TV distance. Duchi, Haque, and Kuditipudi (2023) obtained similar results independently and concurrently."}}
{"id": "v__IeHpP9j", "cdate": 1640995200000, "mdate": 1682355808492, "content": {"title": "Strong Memory Lower Bounds for Learning Natural Models", "abstract": "We give lower bounds on the amount of memory required by a one-pass streaming algorithms for solving several natural learning problems. In a setting where examples lie in $\\{0,1\\}^d$ and the optima..."}}
{"id": "OnDTMnFoDL", "cdate": 1640995200000, "mdate": 1682355808099, "content": {"title": "Strong Memory Lower Bounds for Learning Natural Models", "abstract": "We give lower bounds on the amount of memory required by one-pass streaming algorithms for solving several natural learning problems. In a setting where examples lie in $\\{0,1\\}^d$ and the optimal classifier can be encoded using $\\kappa$ bits, we show that algorithms which learn using a near-minimal number of examples, $\\tilde O(\\kappa)$, must use $\\tilde \\Omega( d\\kappa)$ bits of space. Our space bounds match the dimension of the ambient space of the problem's natural parametrization, even when it is quadratic in the size of examples and the final classifier. For instance, in the setting of $d$-sparse linear classifiers over degree-2 polynomial features, for which $\\kappa=\\Theta(d\\log d)$, our space lower bound is $\\tilde\\Omega(d^2)$. Our bounds degrade gracefully with the stream length $N$, generally having the form $\\tilde\\Omega\\left(d\\kappa \\cdot \\frac{\\kappa}{N}\\right)$. Bounds of the form $\\Omega(d\\kappa)$ were known for learning parity and other problems defined over finite fields. Bounds that apply in a narrow range of sample sizes are also known for linear regression. Ours are the first such bounds for problems of the type commonly seen in recent learning applications that apply for a large range of input sizes."}}
{"id": "BjHCucPUkT", "cdate": 1640995200000, "mdate": 1682746126078, "content": {"title": "Performative Prediction in a Stateful World", "abstract": "Deployed supervised machine learning models make predictions that interact with and influence the world. This phenomenon is called performative prediction by Perdomo et al. (ICML 2020). It is an ongoing challenge to understand the influence of such predictions as well as design tools so as to control that influence. We propose a theoretical framework where the response of a target population to the deployed classifier is modeled as a function of the classifier and the current state (distribution) of the population. We show necessary and sufficient conditions for convergence to an equilibrium of two retraining algorithms, repeated risk minimization and a lazier variant. Furthermore, convergence is near an optimal classifier. We thus generalize results of Perdomo et al., whose performativity framework does not assume any dependence on the state of the target population. A particular phenomenon captured by our model is that of distinct groups that acquire information and resources at different rates to be able to respond to the latest deployed classifier. We study this phenomenon theoretically and empirically."}}
{"id": "INBO6h9gtG", "cdate": 1621629937005, "mdate": null, "content": {"title": "Covariance-Aware Private Mean Estimation Without Private Covariance Estimation", "abstract": "We present two sample-efficient differentially private mean estimators for $d$-dimensional (sub)Gaussian distributions with unknown covariance. Informally, given $n \\gtrsim d/\\alpha^2$ samples from such a distribution with mean $\\mu$ and covariance $\\Sigma$, our estimators output $\\tilde\\mu$ such that $\\| \\tilde\\mu - \\mu \\|_{\\Sigma} \\leq \\alpha$, where $\\| \\cdot \\|_{\\Sigma}$ is the \\emph{Mahalanobis distance}. All previous estimators with the same guarantee either require strong a priori bounds on the covariance matrix or require $\\Omega(d^{3/2})$ samples.  \n   \nEach of our estimators is based on a simple, general approach to designing differentially private mechanisms, but with novel technical steps to make the estimator private and sample-efficient. Our first estimator samples a point with approximately maximum Tukey depth using the exponential mechanism, but restricted to the set of points of large Tukey depth. Proving that this mechanism is private requires a novel analysis. Our second estimator perturbs the empirical mean of the data set with noise calibrated to the empirical covariance. Only the mean is released, however; the covariance is only used internally. Its sample complexity guarantees hold more generally for subgaussian distributions, albeit with a slightly worse dependence on the privacy parameter. For both estimators, careful preprocessing of the data is required to satisfy differential privacy."}}
