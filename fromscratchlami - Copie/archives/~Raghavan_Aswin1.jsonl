{"id": "SD7m4B3kGiQ", "cdate": 1591922549300, "mdate": null, "content": {"title": "Lifelong Learning using Eigentasks: Task Separation, Skill Acquisition and Selective Transfer", "abstract": "We introduce the eigentask framework for lifelong learning. An eigentask is a pairing of a skill that solves a set of related tasks with a generative model that can sample from the skill's input space. The model extends generative replay approaches, which have mainly been used to avoid catastrophic forgetting, to also address other lifelong learning goals such as forward knowledge transfer. We propose a wake-sleep cycle of alternating task learning and knowledge consolidation for learning in our framework, and instantiate it for lifelong supervised learning and lifelong RL. We achieve performance comparable to the state-of-the-art in supervised continual learning benchmarks, and show encouraging preliminary results for forward knowledge transfer in a lifelong RL application in the video game Starcraft~2."}}
{"id": "HJg1NTGZRZ", "cdate": 1518730159194, "mdate": null, "content": {"title": "Bit-Regularized Optimization of Neural Nets", "abstract": "We present a novel regularization strategy for training neural networks which we call ``BitNet''. The parameters of neural networks are usually unconstrained and have a dynamic range dispersed over a real valued range. Our key idea is to control the expressive power of the network by dynamically quantizing the range and set of values that the parameters can take. We formulate this idea using a novel end-to-end approach  that regularizes a typical classification loss function. Our regularizer is inspired by the Minimum Description Length (MDL) principle. For each layer of the network, our approach optimizes a translation and scaling factor along with integer-valued parameters. We empirically compare BitNet to an equivalent unregularized model on the MNIST and CIFAR-10 datasets. We show that BitNet converges faster to a superior quality solution. Additionally, the resulting model is significantly smaller in size due to the use of integer instead of floating-point parameters."}}
{"id": "rJW9DEMOZB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Emergency Response Optimization using Online Hybrid Planning", "abstract": "This paper poses the planning problem faced by the dispatcher responding to urban emergencies as a Hybrid (Discrete and Continuous) State and Action Markov Decision Process (HSA-MDP). We evaluate the performance of three online planning algorithms based on hindsight optimization for HSA- MDPs on real-world emergency data in the city of Corvallis, USA. The approach takes into account and respects the policy constraints imposed by the emergency department. We show that our algorithms outperform a heuristic policy commonly used by dispatchers by significantly reducing the average response time as well as lowering the fraction of unanswered calls. Our results give new insights into the problem such as withholding of resources for future emergencies in some situations."}}
{"id": "SJ-5VSfu-H", "cdate": 1514764800000, "mdate": null, "content": {"title": "Aesop: A Visual Storytelling Platform for Conversational AI", "abstract": "We present a new collaborative visual storytelling platform, Aesop, for direction and animation. Aesop consists of a language parser, human gesture monitoring, composition graphs, dialogue state manager, and an interactive 3D animation software. Aesop thus enables 3D spatial and temporal reasoning which are both essential for storytelling. Our key innovation is to enable conversational AI using both verbal and non-verbal communication, which enables research in language, vision, and planning."}}
{"id": "HkbrikZOZH", "cdate": 1483228800000, "mdate": null, "content": {"title": "Hindsight Optimization for Hybrid State and Action MDPs", "abstract": "Hybrid (mixed discrete and continuous) state and action Markov Decision Processes (HSA-MDPs) provide an expressive formalism for modeling stochastic and concurrent sequential decision-making problems. Existing solvers for HSA-MDPs are either limited to very restricted transition distributions, require knowledge of domain-specific basis functions to achieve good approximations, or do not scale. We explore a domain-independent approach based on the framework of hindsight optimization (HOP) for HSA-MDPs, which uses an upper bound on the finite-horizon action values for action selection. Our main contribution is a linear time reduction to a Mixed Integer Linear Program (MILP) that encodes the HOP objective, when the dynamics are specified as location-scale probability distributions parametrized by Piecewise Linear (PWL) functions of states and actions. In addition, we show how to use the same machinery to select actions based on a lower-bound generated by straight line plans. Our empirical results show that the HSA-HOP approach effectively scales to high-dimensional problems and outperforms baselines that are capable of scaling to such large hybrid MDPs."}}
{"id": "rJVGswb_ZS", "cdate": 1356998400000, "mdate": null, "content": {"title": "Symbolic Opportunistic Policy Iteration for Factored-Action MDPs", "abstract": "We address the scalability of symbolic planning under uncertainty with factored states and actions. Prior work has focused almost exclusively on factored states but not factored actions, and on value iteration (VI) compared to policy iteration (PI). Our \ufb01rst contribution is a novel method for symbolic policy backups via the application of constraints, which is used to yield a new ef\ufb01cient symbolic imple- mentation of modi\ufb01ed PI (MPI) for factored action spaces. While this approach improves scalability in some cases, naive handling of policy constraints comes with its own scalability issues. This leads to our second and main contribution, symbolic Opportunistic Policy Iteration (OPI), which is a novel convergent al- gorithm lying between VI and MPI. The core idea is a symbolic procedure that applies policy constraints only when they reduce the space and time complexity of the update, and otherwise performs full Bellman backups, thus automatically adjusting the backup per state. We also give a memory bounded version of this algorithm allowing a space-time tradeoff. Empirical results show signi\ufb01cantly improved scalability over the state-of-the-art."}}
{"id": "ByVKragdbr", "cdate": 1356998400000, "mdate": null, "content": {"title": "Relational Markov Decision Processes: Promise and Prospects", "abstract": "Relational Markov Decision Processes (RMDPs) offer an elegant formalism that combines probabilistic and relational knowledge representations with the decision-theoretic notions of action and utility. In this paper we motivate RMDPs to address a variety of problems in AI, including open world planning, transfer learning, and relational inference. We describe a symbolic dynamic programming approach via the 'template method' which addresses the problem of reasoning about exogenous events. We end with a discussion of the challenges involved and some promising future research directions."}}
{"id": "rJVq4lZdbH", "cdate": 1325376000000, "mdate": null, "content": {"title": "Planning in Factored Action Spaces with Symbolic Dynamic Programming", "abstract": "We consider symbolic dynamic programming (SDP) for solving Markov Decision Processes (MDP) with factored state and action spaces, where both states and actions are described by sets of discrete variables. Prior work on SDP has considered only the case of factored states and ignored structure in the action space, causing them to scale poorly in terms of the number of action variables. Our main contribution is to present the first SDP-based planning algorithm for leveraging both state and action space structure in order to compute compactly represented value functions and policies. Since our new algorithm can potentially require more space than when action structure is ignored, our second contribution is to describe an approach for smoothly trading-off space versus time via recursive conditioning. Finally, our third contribution is to introduce a novel SDP approximation that often significantly reduces planning time with little loss in quality by exploiting action structure in weakly coupled MDPs. We present empirical results in three domains with factored action spaces that show that our algorithms scale much better with the number of action variables as compared to state-of-the-art SDP algorithms."}}
