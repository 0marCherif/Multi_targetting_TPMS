{"id": "rwSWaS_tGgG", "cdate": 1632875625404, "mdate": null, "content": {"title": "Uncertainty Regularized Policy Learning for Offline Reinforcement Learning", "abstract": "Recent studies show the promising results of using online RL methods in the offline setting.\nHowever, such a learning diagram may suffer from an overtraining issue, that is, the performance of the policy degrades significantly as the training process continues when the dataset is not sufficiently large and diverse. \nIn this work, we propose an alternative approach to alleviate and avoid the overtraining issue: we explicitly take the learning stability into account in the policy learning objective, and adaptively select a good policy before the overtraining issue happens.\nTo do so, we develop an Uncertainty Regularized Policy Learning (URPL) method.\nURPL adds an uncertainty regularization term in the policy learning objective to enforce to learn a more stable policy under the offline setting.\nMoreover, we further use the uncertainty regularization term as a surrogate metric indicating the potential performance of a policy.\nBased on the low-valued region of the uncertainty term, we can select a good policy with considerable good performance and low computation requirements.\nOn standard offline RL benchmark D4RL, URPL achieves much better final performance over existing state-of-the-art baselines."}}
{"id": "zhynF6JnC4q", "cdate": 1632875624492, "mdate": null, "content": {"title": "Adaptive Q-learning for Interaction-Limited Reinforcement Learning", "abstract": "Conventional reinforcement learning (RL) needs an environment to collect fresh data, which is impractical when an online interaction is costly.\nOffline RL provides an alternative solution by directly learning from the logged dataset. However, it usually yields unsatisfactory performance due to a pessimistic update scheme or/and the low quality of logged datasets.\nMoreover, how to evaluate the policy under the offline setting is also a challenging problem.\nIn this paper, we propose a unified framework called Adaptive Q-learning for effectively taking advantage of offline and online learning.\nSpecifically, we explicitly consider the difference between the online and offline data and apply an adaptive update scheme accordingly, i.e., a pessimistic update strategy for the offline dataset and a greedy or no pessimistic update scheme for the online dataset.\nWhen combining both, we can apply very limited online exploration steps to achieve expert performance even when the offline dataset is poor, e.g., random dataset.\nSuch a framework provides a unified way to mix the offline and online RL and gain the best of both worlds. \nTo understand our framework better, we then provide an initialization following our framework's setting.\nExtensive experiments are done to verify the effectiveness of our proposed method."}}
{"id": "9Dye3MHFyvq", "cdate": 1621912795488, "mdate": null, "content": {"title": "Cooperative Heterogeneous Deep Reinforcement Learning", "abstract": "Numerous deep reinforcement learning agents have been proposed, and each\nof them has its strengths and flaws. In this work, we present a Cooperative\nHeterogeneous Deep Reinforcement Learning (CHDRL) framework that can learn\na policy by integrating the advantages of heterogeneous agents. Specifically, we\npropose a cooperative learning framework that classifies heterogeneous agents into\ntwo classes: global agents and local agents. Global agents are off-policy agents\nthat can utilize experiences from the other agents. Local agents are either on-policy\nagents or population-based evolutionary algorithms (EAs) agents that can explore\nthe local area effectively. We employ global agents, which are sample-efficient,\nto guide the learning of local agents so that local agents can benefit from sample efficient agents and simultaneously maintain their advantages, e.g., stability. Global\nagents also benefit from effective local searches. Experimental studies on a range of\ncontinuous control tasks from the Mujoco benchmark show that CHDRL achieves\nbetter performance compared with state-of-the-art baselines."}}
