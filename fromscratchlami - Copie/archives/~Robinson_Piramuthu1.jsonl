{"id": "u0gTRgpTGG1", "cdate": 1672531200000, "mdate": 1695964193447, "content": {"title": "RREx-BoT: Remote Referring Expressions with a Bag of Tricks", "abstract": "Household robots operate in the same space for years. Such robots incrementally build dynamic maps that can be used for tasks requiring remote object localization. However, benchmarks in robot learning often test generalization through inference on tasks in unobserved environments. In an observed environment, locating an object is reduced to choosing from among all object proposals in the environment, which may number in the 100,000s. Armed with this intuition, using only a generic vision-language scoring model with minor modifications for 3d encoding and operating in an embodied environment, we demonstrate an absolute performance gain of 9.84% on remote object grounding above state of the art models for REVERIE and of 5.04% on FAO. When allowed to pre-explore an environment, we also exceed the previous state of the art pre-exploration method on REVERIE. Additionally, we demonstrate our model on a real-world TurtleBot platform, highlighting the simplicity and usefulness of the approach. Our analysis outlines a \"bag of tricks\" essential for accomplishing this task, from utilizing 3d coordinates and context, to generalizing vision-language models to large 3d search spaces."}}
{"id": "qzuYKZcDQo", "cdate": 1672531200000, "mdate": 1695964203134, "content": {"title": "A Simple Approach for Visual Room Rearrangement: 3D Mapping and Semantic Search", "abstract": ""}}
{"id": "j7Hc-5v8rQ", "cdate": 1672531200000, "mdate": 1695964216289, "content": {"title": "A Simple Approach for Visual Room Rearrangement: 3D Mapping and Semantic Search", "abstract": ""}}
{"id": "ie7uYeypm-Z", "cdate": 1672531200000, "mdate": 1695964193382, "content": {"title": "Decision Making for Human-in-the-loop Robotic Agents via Uncertainty-Aware Reinforcement Learning", "abstract": "In a Human-in-the-Loop paradigm, a robotic agent is able to act mostly autonomously in solving a task, but can request help from an external expert when needed. However, knowing when to request such assistance is critical: too few requests can lead to the robot making mistakes, but too many requests can overload the expert. In this paper, we present a Reinforcement Learning based approach to this problem, where a semi-autonomous agent asks for external assistance when it has low confidence in the eventual success of the task. The confidence level is computed by estimating the variance of the return from the current state. We show that this estimate can be iteratively improved during training using a Bellman-like recursion. On discrete navigation problems with both fully- and partially-observable state information, we show that our method makes effective use of a limited budget of expert calls at run-time, despite having no access to the expert at training time."}}
{"id": "WNy4pjSueU", "cdate": 1672531200000, "mdate": 1695964216300, "content": {"title": "RREx-BoT: Remote Referring Expressions with a Bag of Tricks", "abstract": "Household robots operate in the same space for years. Such robots incrementally build dynamic maps that can be used for tasks requiring remote object localization. However, benchmarks in robot learning often test generalization through inference on tasks in unobserved environments. In an observed environment, locating an object is reduced to choosing from among all object proposals in the environment, which may number in the 100,000s. Armed with this intuition, using only a generic vision-language scoring model with minor modifications for 3d encoding and operating in an embodied environment, we demonstrate an absolute performance gain of 9.84% on remote object grounding above state of the art models for REVERIE and of 5.04% on FAO. When allowed to pre-explore an environment, we also exceed the previous state of the art pre-exploration method on REVERIE. Additionally, we demonstrate our model on a real-world TurtleBot platform, highlighting the simplicity and usefulness of the approach. Our analysis outlines a \"bag of tricks\" essential for accomplishing this task, from utilizing 3d coordinates and context, to generalizing vision-language models to large 3d search spaces."}}
{"id": "OCebMiNnHAp", "cdate": 1672531200000, "mdate": 1695964203235, "content": {"title": "Decision Making for Human-in-the-loop Robotic Agents via Uncertainty-Aware Reinforcement Learning", "abstract": "In a Human-in-the-Loop paradigm, a robotic agent is able to act mostly autonomously in solving a task, but can request help from an external expert when needed. However, knowing when to request such assistance is critical: too few requests can lead to the robot making mistakes, but too many requests can overload the expert. In this paper, we present a Reinforcement Learning based approach to this problem, where a semi-autonomous agent asks for external assistance when it has low confidence in the eventual success of the task. The confidence level is computed by estimating the variance of the return from the current state. We show that this estimate can be iteratively improved during training using a Bellman-like recursion. On discrete navigation problems with both fully- and partially-observable state information, we show that our method makes effective use of a limited budget of expert calls at run-time, despite having no access to the expert at training time."}}
{"id": "EcpLGiQiWN", "cdate": 1672531200000, "mdate": 1695964193380, "content": {"title": "A Simple Approach for Visual Room Rearrangement: 3D Mapping and Semantic Search", "abstract": ""}}
{"id": "9RK1BJEKkHG", "cdate": 1672531200000, "mdate": 1695964216302, "content": {"title": "Decision Making for Human-in-the-loop Robotic Agents via Uncertainty-Aware Reinforcement Learning", "abstract": "In a Human-in-the-Loop paradigm, a robotic agent is able to act mostly autonomously in solving a task, but can request help from an external expert when needed. However, knowing when to request such assistance is critical: too few requests can lead to the robot making mistakes, but too many requests can overload the expert. In this paper, we present a Reinforcement Learning based approach to this problem, where a semi-autonomous agent asks for external assistance when it has low confidence in the eventual success of the task. The confidence level is computed by estimating the variance of the return from the current state. We show that this estimate can be iteratively improved during training using a Bellman-like recursion. On discrete navigation problems with both fully- and partially-observable state information, we show that our method makes effective use of a limited budget of expert calls at run-time, despite having no access to the expert at training time."}}
{"id": "8rkQDAm4UJg", "cdate": 1672531200000, "mdate": 1695964203160, "content": {"title": "RREx-BoT: Remote Referring Expressions with a Bag of Tricks", "abstract": "Household robots operate in the same space for years. Such robots incrementally build dynamic maps that can be used for tasks requiring remote object localization. However, benchmarks in robot learning often test generalization through inference on tasks in unobserved environments. In an observed environment, locating an object is reduced to choosing from among all object proposals in the environment, which may number in the 100,000s. Armed with this intuition, using only a generic vision-language scoring model with minor modifications for 3d encoding and operating in an embodied environment, we demonstrate an absolute performance gain of 9.84% on remote object grounding above state of the art models for REVERIE and of 5.04% on FAO. When allowed to pre-explore an environment, we also exceed the previous state of the art pre-exploration method on REVERIE. Additionally, we demonstrate our model on a real-world TurtleBot platform, highlighting the simplicity and usefulness of the approach. Our analysis outlines a \"bag of tricks\" essential for accomplishing this task, from utilizing 3d coordinates and context, to generalizing vision-language models to large 3d search spaces."}}
{"id": "yEdWwYnQpD", "cdate": 1667699784969, "mdate": null, "content": {"title": "CLIP-Nav: Using CLIP for Zero-Shot Vision-and-Language Navigation", "abstract": "Household environments are visually diverse. Embodied agents performing Vision-and-Language Navigation (VLN) in the wild must be able to handle this diversity, while also following arbitrary language instructions.\nRecently, Vision-Language models like CLIP have shown great performance on the task of zero-shot object recognition. In this work, we ask if these models are also capable of zero-shot language grounding.\nIn particular, we utilize CLIP to tackle the novel problem of zero-shot VLN using natural language referring expressions that describe target objects, in contrast to past work that used simple language templates describing object classes.\nWe examine CLIP's capability in making sequential navigational decisions without any dataset-specific finetuning and study how it influences the path that an agent takes.\nOur results on the coarse-grained instruction following task of REVERIE demonstrate the navigational capability of CLIP, surpassing the supervised baseline in terms of both success rate (SR) and success weighted by path length (SPL). More importantly, we quantitatively show that our CLIP-based zero-shot approach generalizes better to show consistent performance across environments when compared to SOTA, fully supervised learning approaches when evaluated with the Relative Change in Success (RCS). "}}
