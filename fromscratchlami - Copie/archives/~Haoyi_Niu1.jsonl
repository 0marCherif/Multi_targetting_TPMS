{"id": "LEbkQ8qjsQQ", "cdate": 1672531200000, "mdate": 1681649830533, "content": {"title": "(Re)2H2O: Autonomous Driving Scenario Generation via Reversely Regularized Hybrid Offline-and-Online Reinforcement Learning", "abstract": ""}}
{"id": "RzhhFh4rkWu", "cdate": 1655376335035, "mdate": null, "content": {"title": "Discriminator-Guided Model-Based Offline Imitation Learning", "abstract": "Offline imitation learning (IL) is a powerful method to solve decision-making problems from expert demonstrations without reward labels. Existing offline IL methods suffer from severe performance degeneration under limited expert data. Including a learned dynamics model can potentially improve the state-action space coverage of expert data, however, it also faces challenging issues like model approximation/generalization errors and suboptimality of rollout data. In this paper, we propose the Discriminator-guided Model-based offline Imitation Learning (DMIL) framework, which introduces a discriminator to simultaneously distinguish the dynamics correctness and sub-optimality of model rollout data against real expert demonstrations. DMIL adopts a novel cooperative-yet-adversarial learning strategy, which uses the discriminator to guide and couple the learning process of the policy and dynamics model, resulting in improved model performance and robustness. Our framework can also be extended to the case when demonstrations contain a large proportion of suboptimal data. Experimental results show that DMIL and its extension achieve superior performance and robustness compared to state-of-the-art offline IL methods under small datasets."}}
{"id": "zXE8iFOZKw", "cdate": 1652737483962, "mdate": null, "content": {"title": "When to Trust Your Simulator: Dynamics-Aware Hybrid Offline-and-Online Reinforcement Learning", "abstract": "Learning effective reinforcement learning (RL) policies to solve real-world complex tasks can be quite challenging without a high-fidelity simulation environment. In most cases, we are only given imperfect simulators with simplified dynamics, which inevitably lead to severe sim-to-real gaps in RL policy learning. The recently emerged field of offline RL provides another possibility to learn policies directly from pre-collected historical data. However, to achieve reasonable performance, existing offline RL algorithms need impractically large offline data with sufficient state-action space coverage for training. This brings up a new question: is it possible to combine learning from limited real data in offline RL and unrestricted exploration through imperfect simulators in online RL to address the drawbacks of both approaches? In this study, we propose the Dynamics-Aware Hybrid Offline-and-Online Reinforcement Learning (H2O) framework to provide an affirmative answer to this question. H2O introduces a dynamics-aware policy evaluation scheme, which adaptively penalizes the Q function learning on simulated state-action pairs with large dynamics gaps, while also simultaneously allowing learning from a fixed real-world dataset. Through extensive simulation and real-world tasks, as well as theoretical analysis, we demonstrate the superior performance of H2O against other cross-domain online and offline RL algorithms. H2O provides a brand new hybrid offline-and-online RL paradigm, which can potentially shed light on future RL algorithm design for solving practical real-world tasks."}}
{"id": "znLaWN2llUs", "cdate": 1640995200000, "mdate": 1681649830554, "content": {"title": "Discriminator-Guided Model-Based Offline Imitation Learning", "abstract": ""}}
{"id": "Zws084m7fM", "cdate": 1640995200000, "mdate": 1658842451718, "content": {"title": "Discriminator-Guided Model-Based Offline Imitation Learning", "abstract": "Offline imitation learning (IL) is a powerful method to solve decision-making problems from expert demonstrations without reward labels. Existing offline IL methods suffer from severe performance degeneration under limited expert data due to covariate shift. Including a learned dynamics model can potentially improve the state-action space coverage of expert data, however, it also faces challenging issues like model approximation/generalization errors and suboptimality of rollout data. In this paper, we propose the Discriminator-guided Model-based offline Imitation Learning (DMIL) framework, which introduces a discriminator to simultaneously distinguish the dynamics correctness and suboptimality of model rollout data against real expert demonstrations. DMIL adopts a novel cooperative-yet-adversarial learning strategy, which uses the discriminator to guide and couple the learning process of the policy and dynamics model, resulting in improved model performance and robustness. Our framework can also be extended to the case when demonstrations contain a large proportion of suboptimal data. Experimental results show that DMIL and its extension achieve superior performance and robustness compared to state-of-the-art offline IL methods under small datasets."}}
{"id": "4rF-2VwRHy", "cdate": 1640995200000, "mdate": 1681649830536, "content": {"title": "When to Trust Your Simulator: Dynamics-Aware Hybrid Offline-and-Online Reinforcement Learning", "abstract": ""}}
{"id": "y5FecJGGcc", "cdate": 1609459200000, "mdate": 1681649830556, "content": {"title": "ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving", "abstract": ""}}
{"id": "txMmYdDFey", "cdate": 1609459200000, "mdate": 1681649830499, "content": {"title": "DR2L: Surfacing Corner Cases to Robustify Autonomous Driving via Domain Randomization Reinforcement Learning", "abstract": ""}}
{"id": "sOfhrH07bQs", "cdate": 1609459200000, "mdate": 1681649830534, "content": {"title": "DR2L: Surfacing Corner Cases to Robustify Autonomous Driving via Domain Randomization Reinforcement Learning", "abstract": ""}}
{"id": "btFpLXAwXK0", "cdate": 1577836800000, "mdate": 1681649830511, "content": {"title": "Tactical Decision Making for Emergency Vehicles based on a Combinational Learning Method", "abstract": ""}}
