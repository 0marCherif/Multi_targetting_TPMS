{"id": "tTHfzpIEByv", "cdate": 1672531200000, "mdate": 1684232754675, "content": {"title": "One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era", "abstract": "OpenAI has recently released GPT-4 (a.k.a. ChatGPT plus), which is demonstrated to be one small step for generative AI (GAI), but one giant leap for artificial general intelligence (AGI). Since its official release in November 2022, ChatGPT has quickly attracted numerous users with extensive media coverage. Such unprecedented attention has also motivated numerous researchers to investigate ChatGPT from various aspects. According to Google scholar, there are more than 500 articles with ChatGPT in their titles or mentioning it in their abstracts. Considering this, a review is urgently needed, and our work fills this gap. Overall, this work is the first to survey ChatGPT with a comprehensive review of its underlying technology, applications, and challenges. Moreover, we present an outlook on how ChatGPT might evolve to realize general-purpose AIGC (a.k.a. AI-generated content), which will be a significant milestone for the development of AGI."}}
{"id": "HsCpAjTdAT1", "cdate": 1672531200000, "mdate": 1684343603236, "content": {"title": "Learning representational invariances for data-efficient action recognition", "abstract": ""}}
{"id": "RR-zm-tizB", "cdate": 1640995200000, "mdate": 1684343603230, "content": {"title": "Self-Supervised Cross-Video Temporal Learning for Unsupervised Video Domain Adaptation", "abstract": "We address the task of unsupervised domain adaptation (UDA) for videos with self-supervised learning. While UDA for images is a widely studied problem, UDA for videos is relatively unexplored. In this paper, we propose a novel self-supervised loss for the task of video UDA. The method is motivated by inverted reasoning. Many works on video classification have shown success with representations based on events in videos, e.g., \u2018reaching\u2019, \u2018picking\u2019, and \u2018drinking\u2019 events for \u2018drinking coffee\u2019. We argue that if we have event-based representations, we should be able to predict the relative distances between clips in videos. Inverting that, we propose a self-supervised task to predict the difference of the distance between two clips from the source video and the distance between two clips from the target video. We hope that such a task would encourage learning event-based representations of the videos, which is known to be beneficial for classification. Since we predict the difference of clip distances between clips from source videos and target videos, we \u2018tie\u2019 the two domains and expect to achieve well-adapted representations. We combine this purely self-supervised loss and the source classification loss to learn the model parameters. We give extensive empirical results on challenging video UDA benchmarks, i.e., UCF-HMDB and EPIC-Kitchens. The presented qualitative and quantitative results support our motivations and method."}}
{"id": "w9I23iZwkYy", "cdate": 1609459200000, "mdate": 1668508043377, "content": {"title": "Learning Representational Invariances for Data-Efficient Action Recognition", "abstract": "Data augmentation is a ubiquitous technique for improving image classification when labeled data is scarce. Constraining the model predictions to be invariant to diverse data augmentations effectively injects the desired representational invariances to the model (e.g., invariance to photometric variations) and helps improve accuracy. Compared to image data, the appearance variations in videos are far more complex due to the additional temporal dimension. Yet, data augmentation methods for videos remain under-explored. This paper investigates various data augmentation strategies that capture different video invariances, including photometric, geometric, temporal, and actor/scene augmentations. When integrated with existing semi-supervised learning frameworks, we show that our data augmentation strategy leads to promising performance on the Kinetics-100/400, Mini-Something-v2, UCF-101, and HMDB-51 datasets in the low-label regime. We also validate our data augmentation strategy in the fully supervised setting and demonstrate improved performance."}}
{"id": "mYfuTnHTMm", "cdate": 1577836800000, "mdate": 1668508043360, "content": {"title": "Unsupervised and Semi-Supervised Domain Adaptation for Action Recognition from Drones", "abstract": "We address the problem of human action classification in drone videos. Due to the high cost of capturing and labeling large-scale drone videos with diverse actions, we present unsupervised and semi-supervised domain adaptation approaches that leverage both the existing fully annotated action recognition datasets and unannotated (or only a few annotated) videos from drones. To study the emerging problem of drone-based action recognition, we create a new dataset, NEC-DRONE, containing 5,250 videos to evaluate the task. We tackle both problem settings with 1) same and 2) different action label sets for the source (e.g., Kinectics dataset) and target domains (drone videos). We present a combination of video and instance-based adaptation methods, paired with either a classifier or an embedding-based framework to transfer the knowledge from source to target. Our results show that the proposed adaptation approach substantially improves the performance on these challenging and practical tasks. We further demonstrate the applicability of our method for learning cross-view action recognition on the Charades-Ego dataset. We provide qualitative analysis to understand the behaviors of our approaches."}}
{"id": "CJrMf4FvnSh", "cdate": 1577836800000, "mdate": 1668508043382, "content": {"title": "Shuffle and Attend: Video Domain Adaptation", "abstract": "We address the problem of domain adaptation in videos for the task of human action recognition. Inspired by image-based domain adaptation, we can perform video adaptation by aligning the features of frames or clips of source and target videos. However, equally aligning all clips is sub-optimal as not all clips are informative for the task. As the first novelty, we propose an attention mechanism which focuses on more discriminative clips and directly optimizes for video-level (cf.\u00a0clip-level) alignment. As the backgrounds are often very different between source and target, the source background-corrupted model adapts poorly to target domain videos. To alleviate this, as a second novelty, we propose to use the clip order prediction as an auxiliary task. The clip order prediction loss, when combined with domain adversarial loss, encourages learning of representations which focus on the humans and objects involved in the actions, rather than the uninformative and widely differing (between source and target) backgrounds. We empirically show that both components contribute positively towards adaptation performance. We report state-of-the-art performances on two out of three challenging public benchmarks, two based on the UCF and HMDB datasets, and one on Kinetics to NEC-Drone datasets. We also support the intuitions and the results with qualitative results."}}
{"id": "rJl_4EreUS", "cdate": 1567802415897, "mdate": null, "content": {"title": "Why Can't I Dance in the Mall? Learning to Mitigate Scene Bias in Action Recognition", "abstract": "Human activities often occur in specific scene contexts (e.g., playing basketball in a basketball court). A model trained on existing video datasets thus inevitably captures such bias and may not generalize well to new action classes or different tasks. In this paper, we propose to mitigate scene bias for video representation learning.  Specifically, we augment the standard cross-entropy loss for action classification with 1) an adversarial loss for scene types and 2) an entropy maximization loss for videos where the human actors are masked out. These two losses encourage learning representations that are unable to predict the scene types and the correct actions when there is no evidence. We validate the effectiveness of our method by transferring our pre-trained model to three different tasks, action classification, temporal localization, and spatio-temporal detection. Our results show consistent improvement over the baseline model without debiasing."}}
{"id": "-jYZEWstY4", "cdate": 1546300800000, "mdate": 1668508043320, "content": {"title": "Why Can't I Dance in the Mall? Learning to Mitigate Scene Bias in Action Recognition", "abstract": "Human activities often occur in specific scene contexts, e.g., playing basketball on a basketball court. Training a model using existing video datasets thus inevitably captures and leverages such bias (instead of using the actual discriminative cues). The learned representation may not generalize well to new action classes or different tasks. In this paper, we propose to mitigate scene bias for video representation learning. Specifically, we augment the standard cross-entropy loss for action classification with 1) an adversarial loss for scene types and 2) a human mask confusion loss for videos where the human actors are masked out. These two losses encourage learning representations that are unable to predict the scene types and the correct actions when there is no evidence. We validate the effectiveness of our method by transferring our pre-trained model to three different tasks, including action classification, temporal localization, and spatio-temporal action detection. Our results show consistent improvement over the baseline model without debiasing."}}
{"id": "HEyB-hMRck", "cdate": 1514764800000, "mdate": 1668508043360, "content": {"title": "Semi-automated home-based therapy for the upper extremity of stroke survivors", "abstract": "Technology assisted home based rehabilitation therapy offers a potentially cost-effective and convenient solution for those affected by neuro and musculoskeletal impairments. Home based solutions, however, face many challenges, the most significant of which is trying to reproduce a complex adaptive therapy experience in the home without the continuous presence of the therapist. Building on our prior work creating interactive systems for the clinic, we present our home-based system that integrates customized therapy objects, camera based movement capture and assessment techniques, and a flexible exercise protocol aimed at generalizing to variable daily life activities. We present findings from two pilot studies with unimpaired and impaired users and describe how insights from these studies will guide future work."}}
{"id": "t8BvH4ftbeB", "cdate": 1483228800000, "mdate": 1668508043315, "content": {"title": "HOMER: An Interactive System for Home Based Stroke Rehabilitation", "abstract": "Delivering long term, unsupervised stroke rehabilitation in the home is a complex challenge that requires robust, low cost, scalable, and engaging solutions. We present HOMER, an interactive system that uses novel therapy artifacts, a computer vision approach, and a tablet interface to provide users with a flexible solution suitable for home based rehabilitation. HOMER builds on our prior work developing systems for lightly supervised rehabilitation use in the clinic, by identifying key features for functional movement analysis, adopting a simplified classification assessment approach, and supporting transferability of therapy outcomes to daily living experiences through the design of novel rehabilitation artifacts. A small pilot study with unimpaired subjects indicates the potential of the system in effectively assessing movement and establishing a creative environment for training."}}
