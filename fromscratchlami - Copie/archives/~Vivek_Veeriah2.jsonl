{"id": "X8hp3yXns0Q", "cdate": 1634067445592, "mdate": null, "content": {"title": "GrASP: Gradient-Based Affordance Selection for Planning", "abstract": "The ability to plan using a learned model is arguably a key component of intelligence. There are several challenges in realising such a component in large-scale reinforcement learning (RL) problems. One such challenge is dealing effectively with continuous action spaces when using tree-search planning (e.g., it is not feasible to consider every action even at just the root node of the tree). In this paper, we present a method for discovering affordances useful for planning---for learning which a small number of actions/options from a continuous space of actions/options to consider in the tree-expansion process during planning. We consider affordances that are goal-and-state-conditional mappings to actions/options as well as unconditional affordances that simply select actions/options available in all states. Our discovery method is gradient-based: we compute gradients through the planning procedure to update the parameters of the function that represents affordances. Our empirical work shows that it is indeed feasible to learn both primitive-action and option affordances in this way and that model-based RL while simultaneously learning affordances and a value-equivalent model can outperform model-free RL. "}}
{"id": "zrdUVVAvcP2", "cdate": 1632875755490, "mdate": null, "content": {"title": "GrASP: Gradient-Based Affordance Selection for Planning", "abstract": "Planning with a learned model is arguably a key component of intelligence. There are several challenges in realizing such a component in large-scale reinforcement learning (RL) problems. One such challenge is dealing effectively with continuous action spaces when using tree-search planning (e.g., it is not feasible to consider every action even at just the root node of the tree). In this paper we present a method for \\emph{selecting} affordances useful for planning---for learning which small number of actions/options from a continuous space of actions/options to consider in the tree-expansion process during planning. We consider affordances that are goal-and-state-conditional mappings to actions/options as well as unconditional affordances that simply select actions/options available in all states. Our selection method is gradient based: we compute gradients through the planning procedure to update the parameters of the function that represents affordances. Our empirical work shows that it is feasible to learn to select both primitive-action and option  affordances, and that simultaneously learning to select affordances and planning with a learned value-equivalent model can outperform model-free RL. "}}
{"id": "nJqCQUzpvS", "cdate": 1621630276024, "mdate": null, "content": {"title": "Learning State Representations from Random Deep Action-conditional Predictions", "abstract": "Our main contribution in this work is an empirical finding that random General Value Functions (GVFs), i.e., deep action-conditional predictions---random both in what feature of observations they predict as well as in the sequence of actions the predictions are conditioned upon---form good auxiliary tasks for reinforcement learning (RL) problems. In particular, we show that random deep action-conditional predictions when used as auxiliary tasks yield state representations that produce control performance competitive with state-of-the-art hand-crafted auxiliary tasks like value prediction, pixel control, and CURL in both Atari and DeepMind Lab tasks. In another set of experiments we stop the gradients from the RL part of the network to the state representation learning part of the network and show, perhaps surprisingly, that the auxiliary tasks alone are sufficient to learn state representations good enough to outperform an end-to-end trained actor-critic baseline. We opensourced our code at https://github.com/Hwhitetooth/random_gvfs."}}
{"id": "AADxnPG-PR", "cdate": 1621630253645, "mdate": null, "content": {"title": "Discovery of Options via Meta-Learned Subgoals", "abstract": "Temporal abstractions in the form of options have been shown to help reinforcement learning (RL) agents learn faster. However, despite prior work on this topic, the problem of discovering options through interaction with an environment remains a challenge. In this paper, we introduce a novel meta-gradient approach for discovering useful options in multi-task RL environments. Our approach is based on a manager-worker decomposition of the RL agent, in which a manager maximises rewards from the environment by learning a task-dependent policy over both a set of task-independent discovered-options and primitive actions. The option-reward and termination functions that define a subgoal for each option are parameterised as neural networks and trained via meta-gradients to maximise their usefulness. Empirical analysis on gridworld and DeepMind Lab tasks show that: (1) our approach can discover meaningful and diverse temporally-extended options in multi-task RL domains, (2) the discovered options are frequently used by the agent while learning to solve the training tasks, and (3) that the discovered options help a randomly initialised manager learn faster in completely new tasks."}}
{"id": "H1gc04Be8B", "cdate": 1567802577605, "mdate": null, "content": {"title": "Discovery of Useful Questions as Auxiliary Tasks", "abstract": "A good representation of knowledge is one that can answer a set of useful questions about some aspects of future experience.  One use of questions is as auxiliary tasks; by learning to answer auxiliary task questions the agent rapidly builds a representation that supports a main reinforcement learning (RL) task. A major outstanding issue is how to discover useful questions directly from experience. We propose a solution to the discovery problem, based on a principled non-myopic meta-gradient procedure, that explicitly optimises the usefulness of the questions via the induced representation\u2019s effectiveness in solving the main RL task. We apply our meta-gradient discovery algorithm to the class of generalized value functions (GVFs). We directly evaluate the ability of the discovered questions to lead to good representations by stopping the gradient from the main reinforcement learning task from adapting to the representations. We show that the auxiliary tasks based on the discovered questions lead to representations that support main task learning, and that they do so better than hand-designed auxiliary tasks. "}}
