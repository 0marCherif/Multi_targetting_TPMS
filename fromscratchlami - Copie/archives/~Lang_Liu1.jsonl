{"id": "AxRUi1XESo", "cdate": 1664310941504, "mdate": null, "content": {"title": "Likelihood Score under Generalized Self-Concordance", "abstract": "We show how, under a generalized self-concordance assumption and possible model misspecification, we can establish non-asymptotic bounds on the normalized likelihood score when using maximum likelihood or score matching. The tail behavior is governed by an effective dimension corresponding to the trace of the sandwich covariance. We also show how our non-asymptotic approach allows us to obtain confidence bounds for the estimator and analyze Rao's score test."}}
{"id": "swAU40QC4Zw", "cdate": 1640995200000, "mdate": 1681756047189, "content": {"title": "MAUVE Scores for Generative Models: Theory and Practice", "abstract": "Generative AI has matured to a point where large-scale models can generate text that seems indistinguishable from human-written text and remarkably photorealistic images. Automatically measuring how close the distribution of generated data is to the target real data distribution is a key step in diagnosing existing models and developing better models. We present MAUVE, a family of comparison measures between pairs of distributions such as those encountered in the generative modeling of text or images. These scores are statistical summaries of divergence frontiers capturing two types of errors in generative modeling. We explore four approaches to statistically estimate these scores: vector quantization, non-parametric estimation, classifier-based estimation, and parametric Gaussian approximations. We provide statistical bounds for the vector quantization approach. Empirically, we find that the proposed scores paired with a range of $f$-divergences and statistical estimation methods can quantify the gaps between the distributions of human-written text and those of modern neural language models by correlating with human judgments and identifying known properties of the generated texts. We conclude the paper by demonstrating its applications to other AI domains and discussing practical recommendations."}}
{"id": "afAX7TIikmn", "cdate": 1640995200000, "mdate": 1682392552419, "content": {"title": "Entropy Regularized Optimal Transport Independence Criterion", "abstract": "We introduce an independence criterion based on entropy regularized optimal transport. Our criterion can be used to test for independence between two samples. We establish non-asymptotic bounds for our test statistic and study its statistical behavior under both the null hypothesis and the alternative hypothesis. The theoretical results involve tools from U-process theory and optimal transport theory. We also offer a random feature type approximation for large-scale problems, as well as a differentiable program implementation for deep learning applications. We present experimental results on existing benchmarks for independence testing, illustrating the interest of the proposed criterion to capture both linear and nonlinear dependencies in synthetic data and real data."}}
{"id": "_Fuhh3X40Jz", "cdate": 1640995200000, "mdate": 1682392552443, "content": {"title": "Orthogonal Statistical Learning with Self-Concordant Loss", "abstract": "Orthogonal statistical learning and double machine learning have emerged as general frameworks for two-stage statistical prediction in the presence of a nuisance component. We establish non-asympto..."}}
{"id": "XEBLY2li7hq", "cdate": 1640995200000, "mdate": 1682392552519, "content": {"title": "Stochastic Optimization for Spectral Risk Measures", "abstract": "Spectral risk objectives - also called $L$-risks - allow for learning systems to interpolate between optimizing average-case performance (as in empirical risk minimization) and worst-case performance on a task. We develop stochastic algorithms to optimize these quantities by characterizing their subdifferential and addressing challenges such as biasedness of subgradient estimates and non-smoothness of the objective. We show theoretically and experimentally that out-of-the-box approaches such as stochastic subgradient and dual averaging are hindered by bias and that our approach outperforms them."}}
{"id": "A5W7b9DTrK", "cdate": 1640995200000, "mdate": 1682392552468, "content": {"title": "Statistical and Computational Guarantees for Influence Diagnostics", "abstract": "Influence diagnostics such as influence functions and approximate maximum influence perturbations are popular in machine learning and in AI domain applications. Influence diagnostics are powerful statistical tools to identify influential datapoints or subsets of datapoints. We establish finite-sample statistical bounds, as well as computational complexity bounds, for influence functions and approximate maximum influence perturbations using efficient inverse-Hessian-vector product implementations. We illustrate our results with generalized linear models and large attention based models on synthetic and real data."}}
{"id": "9tLn4c6p-z", "cdate": 1640995200000, "mdate": 1682392552472, "content": {"title": "Distribution Embedding Networks for Meta-Learning with Heterogeneous Covariate Spaces", "abstract": "We propose Distribution Embedding Networks (DEN) for classification with small data. In the same spirit of meta-learning, DEN learns from a diverse set of training tasks with the goal to generalize to unseen target tasks. Unlike existing approaches which require the inputs of training and target tasks to have the same dimension with possibly similar distributions, DEN allows training and target tasks to live in heterogeneous input spaces. This is especially useful for tabular-data tasks where labeled data from related tasks are scarce. DEN uses a three-block architecture: a covariate transformation block followed by a distribution embedding block and then a classification block. We provide theoretical insights to show that this architecture allows the embedding and classification blocks to be fixed after pre-training on a diverse set of tasks; only the covariate transformation block with relatively few parameters needs to be fine-tuned for each new task. To facilitate training, we also propose an approach to synthesize binary classification tasks, and demonstrate that DEN outperforms existing methods in a number of synthetic and real tasks in numerical studies."}}
{"id": "Z_J5bCb4Rra", "cdate": 1621630133959, "mdate": null, "content": {"title": "Divergence Frontiers for Generative Models: Sample Complexity, Quantization Effects, and Frontier Integrals", "abstract": "The spectacular success of deep generative models calls for quantitative tools to measure their statistical performance. Divergence frontiers have recently been proposed as an evaluation framework for generative models, due to their ability to measure the quality-diversity trade-off inherent to deep generative modeling. We establish non-asymptotic bounds on the sample complexity of divergence frontiers. We also introduce frontier integrals which provide summary statistics of divergence frontiers. We show how smoothed estimators such as Good-Turing or Krichevsky-Trofimov can overcome the missing mass problem and lead to faster rates of convergence. We illustrate the theoretical results with numerical examples from natural language processing and computer vision."}}
{"id": "6NBH4ASTEwD", "cdate": 1609830285982, "mdate": null, "content": {"title": "Asymptotics of Entropy-Regularized Optimal Transport via Chaos Decomposition", "abstract": "Consider the problem of estimating the optimal coupling (i.e., matching) between $N$ i.i.d. data points sampled from two densities $\\rho_0$ and $\\rho_1$ in $\\mathbb{R}^d$. The cost of transport is an arbitrary continuous function that satisfies suitable growth and integrability assumptions. For both computational efficiency and smoothness, often a regularization term using entropy is added to this discrete problem. We introduce a modification of the commonly used discrete entropic regularization (Cuturi '13) such that the optimal coupling for the regularized problem can be thought of as the static Schr\u00f6dinger bridge with $N$ particles. This paper is on the asymptotic properties of this discrete Schr\u00f6dinger bridge as $N$ tends to infinity. We show that it converges to the continuum Schr\u00f6dinger bridge and derive the first two error terms of orders $N^{\u22121/2}$ and $N^{\u22121}$, respectively. This gives us functional CLT, including for the cost of transport, and second order Gaussian chaos limits when the limiting Gaussian variance is zero, extending similar recent results derived for finite state spaces and the quadratic cost. The proofs are based on a novel chaos decomposition of the discrete Schr\u00f6dinger bridge by polynomial functions of the pair of empirical distributions as a first and second order Taylor approximations in the space of measures. This is achieved by extending the Hoeffding decomposition from the classical theory of U-statistics. The kernels corresponding to the first and second order chaoses are given by Markov operators which have natural interpretations in the Sinkhorn algorithm."}}
{"id": "PmzB9gcjvtp", "cdate": 1609459200000, "mdate": 1682392552482, "content": {"title": "Divergence Frontiers for Generative Models: Sample Complexity, Quantization Effects, and Frontier Integrals", "abstract": "The spectacular success of deep generative models calls for quantitative tools to measure their statistical performance. Divergence frontiers have recently been proposed as an evaluation framework for generative models, due to their ability to measure the quality-diversity trade-off inherent to deep generative modeling. We establish non-asymptotic bounds on the sample complexity of divergence frontiers. We also introduce frontier integrals which provide summary statistics of divergence frontiers. We show how smoothed estimators such as Good-Turing or Krichevsky-Trofimov can overcome the missing mass problem and lead to faster rates of convergence. We illustrate the theoretical results with numerical examples from natural language processing and computer vision."}}
