{"id": "n5HL7AT5M8m", "cdate": 1704067200000, "mdate": 1708594676130, "content": {"title": "(Ir)rationality and Cognitive Biases in Large Language Models", "abstract": "Do large language models (LLMs) display rational reasoning? LLMs have been shown to contain human biases due to the data they have been trained on; whether this is reflected in rational reasoning remains less clear. In this paper, we answer this question by evaluating seven language models using tasks from the cognitive psychology literature. We find that, like humans, LLMs display irrationality in these tasks. However, the way this irrationality is displayed does not reflect that shown by humans. When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases. On top of this, the LLMs reveal an additional layer of irrationality in the significant inconsistency of the responses. Aside from the experimental results, this paper seeks to make a methodological contribution by showing how we can assess and compare different capabilities of these types of models, in this case with respect to rational reasoning."}}
{"id": "Q14NT29jQ4q", "cdate": 1704067200000, "mdate": 1708594676119, "content": {"title": "Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges", "abstract": "Generative Artificial Intelligence (AI) is one of the most exciting developments in Computer Science of the last decade. At the same time, Reinforcement Learning (RL) has emerged as a very successful paradigm for a variety of machine learning tasks. In this survey, we discuss the state of the art, opportunities and open research questions in applying RL to generative AI. In particular, we will discuss three types of applications, namely, RL as an alternative way for generation without specified objectives; as a way for generating outputs while concurrently maximizing an objective function; and, finally, as a way of embedding desired characteristics, which cannot be easily captured by means of an objective function, into the generative process. We conclude the survey with an in-depth discussion of the opportunities and challenges in this fascinating emerging area."}}
{"id": "2CquQthrO4", "cdate": 1704067200000, "mdate": 1708594676132, "content": {"title": "Information-Theoretic State Variable Selection for Reinforcement Learning", "abstract": "Identifying the most suitable variables to represent the state is a fundamental challenge in Reinforcement Learning (RL). These variables must efficiently capture the information necessary for making optimal decisions. In order to address this problem, in this paper, we introduce the Transfer Entropy Redundancy Criterion (TERC), an information-theoretic criterion, which determines if there is \\textit{entropy transferred} from state variables to actions during training. We define an algorithm based on TERC that provably excludes variables from the state that have no effect on the final performance of the agent, resulting in more sample efficient learning. Experimental results show that this speed-up is present across three different algorithm classes (represented by tabular Q-learning, Actor-Critic, and Proximal Policy Optimization (PPO)) in a variety of environments. Furthermore, to highlight the differences between the proposed methodology and the current state-of-the-art feature selection approaches, we present a series of controlled experiments on synthetic data, before generalizing to real-world decision-making tasks. We also introduce a representation of the problem that compactly captures the transfer of information from state variables to actions as Bayesian networks."}}
{"id": "epuAA5bX3e1", "cdate": 1677628800000, "mdate": 1681649758283, "content": {"title": "RLQ: Workload Allocation With Reinforcement Learning in Distributed Queues", "abstract": ""}}
{"id": "w2w-q7s6zFN", "cdate": 1672531200000, "mdate": 1708594676239, "content": {"title": "Tree Search in DAG Space with Model-based Reinforcement Learning for Causal Discovery", "abstract": "Identifying causal structure is central to many fields ranging from strategic decision-making to biology and economics. In this work, we propose CD-UCT, a model-based reinforcement learning method for causal discovery based on tree search that builds directed acyclic graphs incrementally. We also formalize and prove the correctness of an efficient algorithm for excluding edges that would introduce cycles, which enables deeper discrete search and sampling in DAG space. The proposed method can be applied broadly to causal Bayesian networks with both discrete and continuous random variables. We conduct a comprehensive evaluation on synthetic and real-world datasets, showing that CD-UCT substantially outperforms the state-of-the-art model-free reinforcement learning technique and greedy search, constituting a promising advancement for combinatorial methods."}}
{"id": "q4oZDPgDelp", "cdate": 1672531200000, "mdate": 1699153982957, "content": {"title": "8th International Workshop on Mental Health and Well-being: Sensing and Intervention", "abstract": "Mental health and well-being are critical components of overall health: suffering from a mental illness can be both debilitating and life-threatening for individuals experiencing symptoms. Detecting symptoms of mental illness early-on and delivering interventions to prevent and/or manage symptoms can improve health and well-being outcomes. Ubiquitous systems are increasingly playing a central role in uncovering clinically relevant contextual information on mental health. Research shows that these systems can passively measure symptoms and enable opportunities to deliver intervention. However, despite this potential, the uptake of ubiquitous technologies into mental healthcare has been slow, and a number of challenges need to be addressed towards the effective implementation of these tools. The goal of this workshop is to bring together researchers, practitioners, and industry professionals interested in identifying, articulating, and addressing such issues and opportunities. Following the success of this workshop for the last seven years, we aim to continue facilitating the UbiComp community in both the conceptualization, translation, and implementation of novel approaches for sensing and intervention in the context of mental health."}}
{"id": "psyDeJLQ1c6", "cdate": 1672531200000, "mdate": 1708594676226, "content": {"title": "Augmented Modular Reinforcement Learning based on Heterogeneous Knowledge", "abstract": "In order to mitigate some of the inefficiencies of Reinforcement Learning (RL), modular approaches composing different decision-making policies to derive agents capable of performing a variety of tasks have been proposed. The modules at the basis of these architectures are generally reusable, also allowing for \"plug-and-play\" integration. However, such solutions still lack the ability to process and integrate multiple types of information (knowledge), such as rules, sub-goals, and skills. We propose Augmented Modular Reinforcement Learning (AMRL) to address these limitations. This new framework uses an arbitrator to select heterogeneous modules and seamlessly incorporate different types of knowledge. Additionally, we introduce a variation of the selection mechanism, namely the Memory-Augmented Arbitrator, which adds the capability of exploiting temporal information. We evaluate the proposed mechanisms on established as well as new environments and benchmark them against prominent deep RL algorithms. Our results demonstrate the performance improvements that can be achieved by augmenting traditional modular RL with other forms of heterogeneous knowledge."}}
{"id": "pgTsAhTqEfx", "cdate": 1672531200000, "mdate": 1708594676141, "content": {"title": "Modeling Moral Choices in Social Dilemmas with Multi-Agent Reinforcement Learning", "abstract": "Practical uses of Artificial Intelligence (AI) in the real world have demonstrated the importance of embedding moral choices into intelligent agents. They have also highlighted that defining top-down ethical constraints on AI according to any one type of morality is extremely challenging and can pose risks. A bottom-up learning approach may be more appropriate for studying and developing ethical behavior in AI agents. In particular, we believe that an interesting and insightful starting point is the analysis of emergent behavior of Reinforcement Learning (RL) agents that act according to a predefined set of moral rewards in social dilemmas. In this work, we present a systematic analysis of the choices made by intrinsically-motivated RL agents whose rewards are based on moral theories. We aim to design reward structures that are simplified yet representative of a set of key ethical systems. Therefore, we first define moral reward functions that distinguish between consequence- and norm-based agents, between morality based on societal norms or internal virtues, and between single- and mixed-virtue (e.g., multi-objective) methodologies. Then, we evaluate our approach by modeling repeated dyadic interactions between learning moral agents in three iterated social dilemma games (Prisoner's Dilemma, Volunteer's Dilemma and Stag Hunt). We analyze the impact of different types of morality on the emergence of cooperation, defection or exploitation, and the corresponding social outcomes. Finally, we discuss the implications of these findings for the development of moral agents in artificial and mixed human-AI societies."}}
{"id": "jBEeWNjt4RL", "cdate": 1672531200000, "mdate": 1708594676225, "content": {"title": "On the Creativity of Large Language Models", "abstract": "Large Language Models (LLMs) are revolutionizing several areas of Artificial Intelligence. One of the most remarkable applications is creative writing, e.g., poetry or storytelling: the generated outputs are often of astonishing quality. However, a natural question arises: can LLMs be really considered creative? In this article we firstly analyze the development of LLMs under the lens of creativity theories, investigating the key open questions and challenges. In particular, we focus our discussion around the dimensions of value, novelty and surprise as proposed by Margaret Boden in her work. Then, we consider different classic perspectives, namely product, process, press and person. We discuss a set of ``easy'' and ``hard'' problems in machine creativity, presenting them in relation to LLMs. Finally, we examine the societal impact of these technologies with a particular focus on the creative industries, analyzing the opportunities offered by them, the challenges arising by them and the potential associated risks, from both legal and ethical points of view."}}
{"id": "ixNzXRnIPbc", "cdate": 1672531200000, "mdate": 1681649756002, "content": {"title": "Investigating the Impact of Direct Punishment on the Emergence of Cooperation in Multi-Agent Reinforcement Learning Systems", "abstract": ""}}
