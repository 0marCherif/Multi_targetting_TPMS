{"id": "xLsQln45ARf", "cdate": 1609459200000, "mdate": 1641822963478, "content": {"title": "Artificial Neural Networks as Feature Extractors in Continuous Evolutionary Optimization", "abstract": "Recent years have seen the advancement of data-driven paradigms in population-based and evolutionary optimization. This reflects on one hand the mere abundance of available data, but on the other hand also progresses in the refinement of previously available machine learning methods. Surprisingly, deep pattern recognition methods emerging from the studies of neural networks have only been sparingly applied. This comes unexpected, as the complex data generated by evolutionary search algorithms can be considered tedious and intractable for manual analysis with mere practical intuitions. In this work, we therefore explore opportunities to employ deep networks to directly learn problem characteristics of continuous optimization problems. Particularly, with data obtained during initial runs of an optimization algorithm. We find that a graph neural network, trained upon a graph representation of continuous search spaces, shows in comparison to more traditional approaches higher validation accuracy and retrieves characteristics within the latent space which are better at distinguishing different continuous optimization problems. We hope that our study can pave the way towards new approaches which allow us to learn problem-dependent algorithm components and recall these from predictions of inputs generated during the run-time of an optimization algorithm."}}
{"id": "K20zwCYz5th", "cdate": 1609459200000, "mdate": 1636019991590, "content": {"title": "Interpreting Node Embedding with Text-labeled Graphs", "abstract": "Graph neural networks have recently received increasing attention. These methods often map nodes into latent spaces and learn vector representations of the nodes for a variety of downstream tasks. To gain trust and to promote collaboration between AIs and humans, it would be better if those representations were interpretable for humans. However, most explainable AIs focus on a supervised learning setting and aim to answer the following question: \u201cWhy does the model predict y for an input x?\u201d. For an unsupervised learning setting as node embedding, interpretation can be more complicated since the embedding vectors are usually not understandable for humans. On the other hand, nodes and edges in a graph are often associated with texts in many real-world applications. A question naturally arises: could we integrate the human-understandable textural data into graph learning to facilitate interpretable node embedding? In this paper we present interpretable graph neural networks (iGNN), a model to learn textual explanations for node representations modeling the extra information contained in the associated textual data. To validate the performance of the proposed method, we investigate the learned interpretability of the embedding vectors and use functional interpretability to measure it. Experimental results on multiple text-labeled graphs show the effectiveness of the iGNN model on learning textual explanations of node embedding while performing well in downstream tasks."}}
{"id": "GURQRPAFwDV", "cdate": 1609459200000, "mdate": 1641374869065, "content": {"title": "Learning Sparsity of Representations with Discrete Latent Variables", "abstract": "Deep latent generative models have attracted increasing attention due to the capacity of combining the strengths of deep learning and probabilistic models in an elegant way. The data representations learned with the models are often continuous and dense. However in many applications, sparse representations are expected, such as learning sparse high dimensional embedding of data in an unsupervised setting, and learning multi-labels from thousands of candidate tags in a supervised setting. In some scenarios, there could be further restriction on degree of sparsity: the number of non-zero features of a representation cannot be larger than a pre-defined threshold <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$L_{0}$</tex> . In this paper we propose a sparse deep latent generative model SDLGM to explicitly model degree of sparsity and thus enable to learn the sparse structure of the data with the quantified sparsity constraint. The resulting sparsity of a representation is not fixed, but fits to the observation itself under the pre-defined restriction. In particular, we introduce to each observation <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$i$</tex> an auxiliary random variable <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$L_{i}$</tex> , which models the sparsity of its representation. The sparse representations are then generated with a two-step sampling process via two Gumbel-Softmax distributions. For inference and learning, we develop an amortized variational method based on MC gradient estimator. The resulting sparse representations are differentiable with backpropagation. The experimental evaluation on multiple datasets for unsupervised and supervised learning problems shows the benefits of the proposed method."}}
{"id": "EHW4o0XeUPr", "cdate": 1577836800000, "mdate": 1641374869067, "content": {"title": "Hierarchical Adversarial Training for Multi-domain Adaptive Sentiment Analysis", "abstract": "Extracting useful insights with sentiment analysis is of increasing importance due to the growing availability of user-generated content. Sentiment analysis usually involves multiple different domains, and the labeled data is often difficult to obtain. In this paper we propose a hierarchical adversarial neural network (HANN) for adaptiveXu, Zhao\u00a0sentiment analysis. Unlike most existing deep learning based methods, the proposed method HANN is able to share information between multiple domains bidirectionally, not justVon Ritter, Lorenzo\u00a0transfers information from source domain to target domain in one direction only. In particular, the HANN method is inspired by the ideas of hierarchical Bayesian modeling and generative adversarial networks. We introduce each domain a distinct encoder to model the domain-specific distribution of the latent features. The learning procedures onSerra, Giuseppe\u00a0different domains are coupled by a discriminator network to propagate the information, which can be viewed as adversarial networks in a supervised context by forcing the discriminator to identify domain labels. The proposed method HANN not only captures the distinct properties of each domain, but also shares common information across multiple domains. We demonstrate the superior performance of our method on real data including the Amazon review dataset and the Sanders Twitter sentiment dataset."}}
{"id": "z3rN1k4HzX", "cdate": 1483228800000, "mdate": 1641822963478, "content": {"title": "Graph Enhanced Memory Networks for Sentiment Analysis", "abstract": "Memory networks model information and knowledge as memories that can be manipulated for prediction and reasoning about questions of interest. In many cases, there exists complicated relational structure in the data, by which the memories can be linked together into graphs to propagate information. Typical examples include tree structure of a sentence and knowledge graph in a dialogue system. In this paper, we present a novel graph enhanced memory network GEMN to integrate relational information between memories for prediction and reasoning. Our approach introduces graph attentions to model the relations, and couples them with content-based attentions via an additional neural network layer. It thus can better identify and manipulate the memories related to a given question, and provides more accurate prediction about the final response. We demonstrate the effectiveness of the proposed approach with aspect based sentiment classification. The empirical analysis on real data shows the advantages of incorporating relational dependencies into the memory networks."}}
{"id": "eO3De1nh9F1", "cdate": 1483228800000, "mdate": null, "content": {"title": "Gaussian Process", "abstract": "Expectation propagation; Kernels; Laplace estimate; Nonparametric Bayesian\nGaussian processes generalize multivariate Gaussian distributions over finite-dimensional vectors to infinite dimensionality. Specifically, a Gaussian process is a stochastic process that has Gaussian-distributed finite-dimensional marginal distributions, hence the name. In doing so, it defines a distribution over functions, i.e., each draw from a Gaussian process is a function. Gaussian processes provide a principled, practical, and probabilistic approach to inference and learning in kernel machines.\nBayesian probabilistic approaches have many virtues, including their ability to incorporate prior knowledge and their ability to link related sources of information. Typically, we are given a set of data points sampled from an underlying but unknown distribution, each of which includes input x and output y, such as the ones shown in Fig. 1a. The task is to learn a..."}}
{"id": "B1bor4Gd-S", "cdate": 1483228800000, "mdate": null, "content": {"title": "Stochastic Online Anomaly Analysis for Streaming Time Series", "abstract": "Identifying patterns in time series that exhibit anomalous behavior is of increasing importance in many domains, such as financial and Web data analysis. In real applications, time series data often arrive continuously, and usually only a single scan is allowed through the data. Batch learning and retrospective segmentation methods would not be well applicable to such scenarios. In this paper, we present an online nonparametric Bayesian method OLAD for anomaly analysis in streaming time series. Moreover, we develop a novel and efficient online learning approach for the OLAD model based on stochastic gradient descent. The proposed method can effectively learn the underlying dynamics of anomaly-contaminated heavy-tailed time series and identify potential anomalous events. Empirical analysis on real-world datasets demonstrates the effectiveness of our method."}}
{"id": "I1DUjjigDRS", "cdate": 1325376000000, "mdate": 1641822963583, "content": {"title": "Infinite Hidden Relational Models", "abstract": "In many cases it makes sense to model a relationship symmetrically, not implying any particular directionality. Consider the classical example of a recommendation system where the rating of an item by a user should symmetrically be dependent on the attributes of both the user and the item. The attributes of the (known) relationships are also relevant for predicting attributes of entities and for predicting attributes of new relations. In recommendation systems, the exploitation of relational attributes is often referred to as collaborative filtering. Again, in many applications one might prefer to model the collaborative effect in a symmetrical way. In this paper we present a relational model, which is completely symmetrical. The key innovation is that we introduce for each entity (or object) an infinite-dimensional latent variable as part of a Dirichlet process (DP) model. We discuss inference in the model, which is based on a DP Gibbs sampler, i.e., the Chinese restaurant process. We extend the Chinese restaurant process to be applicable to relational modeling. Our approach is evaluated in three applications. One is a recommendation system based on the MovieLens data set. The second application concerns the prediction of the function of yeast genes/proteins on the data set of KDD Cup 2001 using a multi-relational model. The third application involves a relational medical domain. The experimental results show that our model gives significantly improved estimates of attributes describing relationships or entities in complex relational models."}}
{"id": "H1bVWe-OWS", "cdate": 1325376000000, "mdate": null, "content": {"title": "Pre-Symptomatic Prediction of Plant Drought Stress Using Dirichlet-Aggregation Regression on Hyperspectral Images", "abstract": "Pre-symptomatic drought stress prediction is of great relevance in precision plant protection, ultimately helping to meet the challenge of \"How to feed a hungry world?\". Unfortunately, it also presents unique computational problems in scale and interpretability: it is a temporal, large-scale prediction task, e.g., when monitoring plants over time using hyperspectral imaging, and features are 'things' with a 'biological' meaning and interpretation and not just mathematical abstractions computable for any data. In this paper we propose Dirichletaggregation regression (DAR) to meet the challenge. DAR represents all data by means of convex combinations of only few extreme ones computable in linear time and easy to interpret. Then, it puts a Gaussian process prior on the Dirichlet distributions induced on the simplex spanned by the extremes. The prior can be a function of any observed meta feature such as time, location, type of fertilization, and plant species. We evaluated DAR on two hyperspectral image series of plants over time with about 2 (resp. 5:8) Billion matrix entries. The results demonstrate that DAR can be learned efficiently and predicts stress well before it becomes visible to the human eye."}}
{"id": "GmJgYBBFXCj", "cdate": 1325376000000, "mdate": 1641822963478, "content": {"title": "Efficient Learning for Hashing Proportional Data", "abstract": "Spectral hashing (SH) seeks compact binary codes of data points so that Hamming distances between codes correlate with data similarity. Quickly learning such codes typically boils down to principle component analysis (PCA). However, this is only justified for normally distributed data. For proportional data (normalized histograms), this is not the case. Due to the sum-to-unity constraint, features that are as independent as possible will not all be uncorrelated. In this paper, we show that a linear-time transformation efficiently copes with sum-to-unity constraints: first, we select a small number K of diverse data points by maximizing the volume of the simplex spanned by these prototypes; second, we represent each data point by means of its cosine similarities to the K selected prototypes. This maximum volume hashing is sensible since each dimension in the transformed space is likely to follow a von Mises (vM) distribution, and, in very high dimensions, the vM distribution closely resembles a Gaussian distribution. This justifies to employ PCA on the transformed data. Our extensive experiments validate this: maximum volume hashing outperforms spectral hashing and other state of the art techniques."}}
