{"id": "mD_3SHMSCO", "cdate": 1668681031493, "mdate": 1668681031493, "content": {"title": "CGAN-TM: A Novel Domain-to-Domain Transferring Method for Person Re-Identification", "abstract": "Person re-identification (re-ID) is a technique aiming to recognize person cross different cameras. Although some supervised methods have achieved favorable performance, they are far from practical application owing to the lack of labeled data. Thus, unsupervised person re-ID methods are in urgent need. Generally, the commonly used approach in existing unsupervised methods is to first utilize the source image dataset for generating a model in supervised manner, and then transfer the source image domain to the target image domain. However, images may lose their identity information after translation, and the distributions between different domains are far away. To solve these problems, we propose an image domain-to-domain translation method by keeping pedestrian's identity information and pulling closer the domains' distributions for unsupervised person re-ID tasks. Our work exploits the CycleGAN to transfer the existing labeled image domain to the unlabeled image domain. Specially, a Self-labeled Triplet Net is proposed to maintain the pedestrian identity information, and maximum mean discrepancy is introduced to pull the domain distribution closer. Extensive experiments have been conducted and the results demonstrate that the proposed method performs superiorly than the state-of-the-art unsupervised methods on DukeMTMC-reID and Market-1501."}}
{"id": "TuV5emxYnP", "cdate": 1668589442873, "mdate": 1668589442873, "content": {"title": "Instance-Dependent Label-Noise Learning With Manifold-Regularized Transition Matrix Estimation", "abstract": "In label-noise learning, estimating the transition matrix has attracted more and more attention as the matrix plays an important role in building statistically consistent classifiers. However, it is very challenging to estimate the transition matrix T(x), where T(x) denotes the instance, because it is unidentifiable under the instance-dependent noise (IDN). To address this problem, we have noticed that, there are psychological and physiological evidences showing that we humans are more likely to annotate instances of similar appearances to the same classes, and thus poor-quality or ambiguous instances of similar appearances are easier to be mislabeled to the correlated or same noisy classes. Therefore, we propose assumption on the geometry of T(x) that \"the closer two instances are, the more similar their corresponding transition matrices should be\". More specifically, we formulate above assumption into the manifold embedding, to effectively reduce the degree of freedom of T(x) and make it stably estimable in practice. This proposed manifold-regularized technique works by directly reducing the estimation error without hurting the approximation error about the estimation problem of T(x) Experimental evaluations on four synthetic and two real-world datasets demonstrate our method is superior to state-of-the-art approaches for label-noise learning under the challenging IDN."}}
{"id": "Bvaekygzl2m", "cdate": 1663849948345, "mdate": null, "content": {"title": "Strength-Adaptive Adversarial Training", "abstract": "Adversarial training (AT) is proved to reliably improve network's robustness against adversarial data. However, current AT with a pre-specified perturbation budget has limitations in learning a robust network. Firstly, applying a pre-specified perturbation budget on networks of various model capacities will yield divergent degree of robustness disparity between natural and robust accuracies, which deviates from robust network's desideratum. Secondly, the attack strength of adversarial training data constrained by the pre-specified perturbation budget fails to upgrade as the growth of network robustness, which leads to robust overfitting and further degrades the adversarial robustness. To overcome these limitations, we propose Strength-Adaptive Adversarial Training (SAAT). Specifically, the adversary employs an adversarial loss constraint to generate adversarial training data. Under this constraint, the perturbation budget will be adaptively adjusted according to the training state of adversarial data, which can effectively avoid robust overfitting. Besides, SAAT explicitly constrains the attack strength of training data through the adversarial loss, which manipulates model capacity scheduling during training, and thereby can flexibly control the degree of robustness disparity and adjust the tradeoff between natural accuracy and robustness. Extensive experiments show that our proposal boosts the robustness of adversarial training. "}}
{"id": "IvnoGKQuXi", "cdate": 1652737336658, "mdate": null, "content": {"title": "Class-Dependent Label-Noise Learning with Cycle-Consistency Regularization", "abstract": "In label-noise learning, estimating the transition matrix plays an important role in building statistically consistent classifier. Current state-of-the-art consistent estimator for the transition matrix has been developed under the newly proposed sufficiently scattered assumption, through incorporating the minimum volume constraint of the transition matrix T into label-noise learning. To compute the volume of  T, it heavily relies on the estimated  noisy class posterior. However, the estimation error of the noisy class posterior could usually be large as deep learning methods tend to easily overfit the noisy labels. Then, directly minimizing the volume of such obtained T could lead the transition matrix to be poorly estimated.  Therefore, how to reduce the side-effects of the inaccurate noisy class posterior has become the bottleneck of such method. In this paper, we creatively propose to estimate the transition matrix under the forward-backward cycle-consistency regularization, of which we have greatly reduced the dependency of estimating the transition matrix T on the noisy class posterior. We show that the cycle-consistency regularization helps to minimize the volume of the transition matrix T indirectly without exploiting the estimated noisy class posterior, which could further encourage the estimated transition matrix T to converge to its optimal solution. Extensive experimental results consistently justify the effectiveness of the proposed method, on reducing the estimation error of the transition matrix and greatly boosting the classification performance."}}
{"id": "AD_NnvViZiY", "cdate": 1649643749817, "mdate": 1649643749817, "content": {"title": "Towards Semi-Supervised Deep Facial Expression Recognition with An Adaptive Confidence Margin", "abstract": "Only parts of unlabeled data are selected to train mod\u0002els for most semi-supervised learning methods, whose confi\u0002dence scores are usually higher than the pre-defined thresh\u0002old (i.e., the confidence margin). We argue that the recog\u0002nition performance should be further improved by making\nfull use of all unlabeled data. In this paper, we learn an\nAdaptive Confidence Margin (Ada-CM) to fully leverage all\nunlabeled data for semi-supervised deep facial expression\nrecognition. All unlabeled samples are partitioned into two\nsubsets by comparing their confidence scores with the adap\u0002tively learned confidence margin at each training epoch:\n(1) subset I including samples whose confidence scores are\nno lower than the margin; (2) subset II including samples\nwhose confidence scores are lower than the margin. For\nsamples in subset I, we constrain their predictions to match\npseudo labels. Meanwhile, samples in subset II participate\nin the feature-level contrastive objective to learn effective\nfacial expression features. We extensively evaluate Ada\u0002CM on four challenging datasets, showing that our method\nachieves state-of-the-art performance, especially surpass\u0002ing fully-supervised baselines in a semi-supervised man\u0002ner. Ablation study further proves the effectiveness of our\nmethod. The code will be publicly available."}}
{"id": "ndtLe66Saq6", "cdate": 1649643443088, "mdate": 1649643443088, "content": {"title": "Towards Defending against Adversarial Examples via Attack-Invariant Features", "abstract": "Deep neural networks (DNNs) are vulnerable to\nadversarial noise. Their adversarial robustness\ncan be improved by exploiting adversarial exam\u0002ples. However, given the continuously evolving at\u0002tacks, models trained on seen types of adversarial\nexamples generally cannot generalize well to un\u0002seen types of adversarial examples. To solve this\nproblem, in this paper, we propose to remove ad\u0002versarial noise by learning generalizable invariant\nfeatures across attacks which maintain semantic\nclassification information. Specifically, we intro\u0002duce an adversarial feature learning mechanism\nto disentangle invariant features from adversarial\nnoise. A normalization term has been proposed in\nthe encoded space of the attack-invariant features\nto address the bias issue between the seen and\nunseen types of attacks. Empirical evaluations\ndemonstrate that our method could provide better\nprotection in comparison to previous state-of-the\u0002art approaches, especially against unseen types of\nattacks and adaptive attacks"}}
{"id": "anWCFENEc5H", "cdate": 1632875486011, "mdate": null, "content": {"title": "Modeling Adversarial Noise for Adversarial Defense", "abstract": "Deep neural networks have been demonstrated to be vulnerable to adversarial noise, promoting the development of defense against adversarial attacks. Motivated by the fact that adversarial noise contains well-generalizing features and that the relationship between adversarial data and natural data can help infer natural data and make reliable predictions, in this paper, we study to model adversarial noise by learning the transition relationship between adversarial labels (i.e. the flipped labels used to generate adversarial data) and natural labels (i.e. the ground truth labels of the natural data). Specifically, we introduce an instance-dependent transition matrix to relate adversarial labels and natural labels, which can be seamlessly embedded with the target model (enabling us to model stronger adaptive adversarial noise). Empirical evaluations demonstrate that our method could effectively improve adversarial accuracy."}}
{"id": "I8nahMfPixC", "cdate": 1601308274042, "mdate": null, "content": {"title": "ADD-Defense: Towards Defending Widespread Adversarial Examples via Perturbation-Invariant Representation", "abstract": "Due to vulnerability of machine learning algorithms under adversarial examples, it is challenging to defend against them. Recently, various defenses have been proposed to mitigate negative effects of adversarial examples generated from known attacks. However, these methods have obvious limitations against unknown attacks. Cognitive science investigates that the brain can recognize the same person with any expression by extracting invariant information on the face. Similarly, different adversarial examples share the invariant information retained from original examples. Motivated by this observation, we propose a defense framework ADD-Defense, which extracts the invariant information called \\textit{perturbation-invariant representation} (PIR) to defend against widespread adversarial examples. Specifically, realized by adversarial training with additional ability to utilize perturbation-specific information, the PIR is invariant to known attacks and has no perturbation-specific information. Facing the imbalance between widespread unknown attacks and limited known attacks, the PIR is expected to generalize well on unknown attacks via being matched to a Gaussian prior distribution. In this way, the PIR is invariant to both known and unknown attacks. Once the PIR is learned, we can generate an example without malicious perturbations as the output. We evaluate our ADD-Defense using various pixel-constrained and spatially-constrained attacks, especially BPDA and AutoAttack. The empirical results illustrate that our ADD-Defense is robust to widespread adversarial examples."}}
{"id": "xW9zZm9qK0_", "cdate": 1601308159396, "mdate": null, "content": {"title": "Class2Simi: A New Perspective on Learning with Label Noise", "abstract": "Label noise is ubiquitous in the era of big data. Deep learning algorithms can easily fit the noise and thus cannot generalize well without properly modeling the noise. In this paper, we propose a new perspective on dealing with  label noise called ``\\textit{Class2Simi}''. Specifically, we transform the training examples with noisy class labels into pairs of examples with noisy similarity labels, and propose a deep learning framework to learn robust classifiers with the noisy similarity labels. Note that a class label shows the class that an instance belongs to; while a similarity label indicates whether or not two instances belong to the same class. It is worthwhile to perform the transformation: We prove that the noise rate for the noisy similarity labels is lower than that of the noisy class labels, because similarity labels themselves are robust to noise. For example, given two instances, even if both of their class labels are incorrect, their similarity label could be correct. Due to the lower noise rate, Class2Simi achieves remarkably better classification accuracy than its baselines that directly deals with the noisy class labels."}}
{"id": "Eql5b1_hTE4", "cdate": 1601308154798, "mdate": null, "content": {"title": "Robust early-learning: Hindering the memorization of noisy labels", "abstract": "The \\textit{memorization effects} of deep networks show that they will first memorize training data with clean labels and then those with noisy labels. The \\textit{early stopping} method therefore can be exploited for learning with noisy labels. However, the side effect brought by noisy labels will influence the memorization of clean labels before early stopping. In this paper, motivated by the \\textit{lottery ticket hypothesis} which shows that only partial parameters are important for generalization, we find that only partial parameters are important for fitting clean labels and generalize well, which we term as \\textit{critical parameters}; while the other parameters tend to fit noisy labels and cannot generalize well, which we term as \\textit{non-critical parameters}. Based on this, we propose \\textit{robust early-learning} to reduce the side effect of noisy labels before early stopping and thus enhance the memorization of clean labels. Specifically, in each iteration, we divide all parameters into the critical and non-critical ones, and then perform different update rules for different types of parameters. Extensive experiments on benchmark-simulated and real-world label-noise datasets demonstrate the superiority of the proposed method over the state-of-the-art label-noise learning methods."}}
