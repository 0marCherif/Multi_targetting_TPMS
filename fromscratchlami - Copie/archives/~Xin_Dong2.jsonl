{"id": "up7oTfzc1x", "cdate": 1676484087074, "mdate": 1676484087074, "content": {"title": "PAM: understanding product images in cross product category attribute extraction", "abstract": "Understanding product attributes plays an important role in improving online shopping experience for customers and serves asan integral part for constructing a product knowledge graph. Most existing methods focus on attribute extraction from text description or utilize visual information from product images such as shape and color. Compared to the inputs considered in prior works, a product image in fact contains more information, represented by a rich mixture of words and visual clues with a layout carefully designed to impress customers. This work proposes a more inclusive framework that fully utilizes these different modalities for attribute extraction.Inspired by recent works in visual question answering, we use a transformer based sequence to sequence model to fuse representations of product text, Optical Character Recognition (OCR) tokens and visual objects detected in the product image. The framework is further extended with the capability to extract attribute value across multiple product categories with a single model, by training the decoder to predict both product category and attribute value and conditioning its output on product category. The model provides a unified attribute extraction solution desirable at an e-commerce platform that offers numerous product categories with a diverse body of product attributes. We evaluated the model on two product attributes, one with many possible values and one with a small set of possible values, over 14 product categories and found the model could achieve 15% gain on the Recall and 10% gain on the F1 score compared to existing methods using text-only features."}}
{"id": "ARQ3_Y9Mis", "cdate": 1676483934584, "mdate": null, "content": {"title": "AdaTag: Multi-Attribute Value Extraction from Product Profiles with Adaptive Decoding", "abstract": "Automatic extraction of product attribute values is an important enabling technology in e-Commerce platforms. This task is usually modeled using sequence labeling architectures, with several extensions to handle multi-attribute extraction. One line of previous work constructs attribute-specific models, through separate decoders or entirely separate models. However, this approach constrains knowledge sharing across different attributes. Other contributions use a single multi-attribute model, with different techniques to embed attribute information. But sharing the entire network parameters across all attributes can limit the model's capacity to capture attribute-specific characteristics. In this paper we present AdaTag, which uses adaptive decoding to handle extraction. We parameterize the decoder with pretrained attribute embeddings, through a hypernetwork and a Mixture-of-Experts (MoE) module. This allows for separate, but semantically correlated, decoders to be generated on the fly for different attributes. This approach facilitates knowledge sharing, while maintaining the specificity of each attribute. Our experiments on a real-world e-Commerce dataset show marked improvements over previous methods."}}
{"id": "PsTgGy649Jo", "cdate": 1676483829881, "mdate": null, "content": {"title": "AutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types", "abstract": "Can one build a knowledge graph (KG) for all products in the world? Knowledge graphs have firmly established themselves as valuable sources of information for search and question answering, and it is natural to wonder if a KG can contain information about products offered at online retail sites. There have been several successful examples of generic KGs, but organizing information about products poses many additional challenges, including sparsity and noise of structured data for products, complexity of the domain with millions of product types and thousands of attributes, heterogeneity across large number of categories, as well as large and constantly growing number of products.\n\nWe describe AutoKnow, our automatic (self-driving) system that addresses these challenges. The system includes a suite of novel techniques for taxonomy construction, product property identification, knowledge extraction, anomaly detection, and synonym discovery. AutoKnow is (a) automatic, requiring little human intervention, (b) multi-scalable, scalable in multiple dimensions (many domains, many products, and many attributes), and (c) integrative, exploiting rich customer behavior logs. AutoKnow has been operational in collecting product knowledge for over 11K product types."}}
{"id": "6gCdoq4sG9k", "cdate": 1674892996881, "mdate": 1674892996881, "content": {"title": "Deep Transfer Learning for Multi-source Entity Linkage via Domain Adaptation", "abstract": "Multi-source entity linkage focuses on integrating knowledge from\nmultiple sources by linking the records that represent the same real\nworld entity. This is critical in high-impact applications such as\ndata cleaning and user stitching. The state-of-the-art entity linkage\npipelines mainly depend on supervised learning that requires abundant amounts of training data. However, collecting well-labeled\ntraining data becomes expensive when the data from many sources\narrives incrementally over time. Moreover, the trained models can\neasily overfit to specific data sources, and thus fail to generalize\nto new sources due to significant differences in data and label distributions. To address these challenges, we present AdaMEL, a\ndeep transfer learning framework that learns generic high-level\nknowledge to perform multi-source entity linkage. AdaMEL models\nthe attribute importance that is used to match entities through an\nattribute-level self-attention mechanism, and leverages the massive\nunlabeled data from new data sources through domain adaptation\nto make it generic and data-source agnostic. In addition, AdaMEL\nis capable of incorporating an additional set of labeled data to more\naccurately integrate data sources with different attribute importance. Extensive experiments show that our framework achieves\nstate-of-the-art results with 8.21% improvement on average over\nmethods based on supervised learning. Besides, it is more stable in\nhandling different sets of data sources in less runtime."}}
{"id": "pRWTqUk7TA1", "cdate": 1674892898163, "mdate": 1674892898163, "content": {"title": "CORDEL: A Contrastive Deep Learning Approach for Entity Linkage", "abstract": "Entity linkage (EL) is a critical problem in data\ncleaning and integration. In the past several decades, EL\nhas typically been done by rule-based systems or traditional\nmachine learning models with hand-curated features, both of\nwhich heavily depend on manual human inputs. With the\never-increasing growth of new data, deep learning (DL) based\napproaches have been proposed to alleviate the high cost of\nEL associated with the traditional models. Existing exploration\nof DL models for EL strictly follows the well-known twinnetwork architecture. However, we argue that the twin-network\narchitecture is sub-optimal to EL, leading to inherent drawbacks\nof existing models. In order to address the drawbacks, we\npropose a novel and generic contrastive DL framework for EL.\nThe proposed framework is able to capture both syntactic and\nsemantic matching signals and pays attention to subtle but critical\ndifferences. Based on the framework, we develop a contrastive DL\napproach for EL, called CORDEL, with three powerful variants.\nWe evaluate CORDEL with extensive experiments conducted\non both public benchmark datasets and a real-world dataset.\nCORDEL outperforms previous state-of-the-art models by 5.2%\non public benchmark datasets. Moreover, CORDEL yields a 2.4%\nimprovement over the current best DL model on the real-world\ndataset, while reducing the number of training parameters by\n97.6%."}}
{"id": "jiQDQdnD44g", "cdate": 1674892813995, "mdate": 1674892813995, "content": {"title": "AutoBlock: A Hands-off Blocking Framework for Entity Matching", "abstract": "Entity matching seeks to identify data records over one or multiple\ndata sources that refer to the same real-world entity. Virtually every\nentity matching task on large datasets requires blocking, a step that\nreduces the number of record pairs to be matched. However, most\nof the traditional blocking methods are learning-free and key-based,\nand their successes are largely built on laborious human effort in\ncleaning data and designing blocking keys.\nIn this paper, we propose AutoBlock, a novel hands-off blocking\nframework for entity matching, based on similarity-preserving representation learning and nearest neighbor search. Our contributions\ninclude: (a) Automation: AutoBlock frees users from laborious\ndata cleaning and blocking key tuning. (b) Scalability: AutoBlock\nhas a sub-quadratic total time complexity and can be easily deployed\nfor millions of records. (c) Effectiveness: AutoBlock outperforms\na wide range of competitive baselines on multiple large-scale, realworld datasets, especially when datasets are dirty and/or unstructured."}}
{"id": "meLzymorEz", "cdate": 1674892659688, "mdate": 1674892659688, "content": {"title": "Collective Multi-type Entity Alignment Between Knowledge Graphs", "abstract": "Knowledge graph (e.g. Freebase, YAGO) is a multi-relational graph\nrepresenting rich factual information among entities of various\ntypes. Entity alignment is the key step towards knowledge graph\nintegration from multiple sources. It aims to identify entities across\ndifferent knowledge graphs that refer to the same real world entity.\nHowever, current entity alignment systems overlook the sparsity\nof different knowledge graphs and can not align multi-type entities by one single model. In this paper, we present a Collective\nGraph neural network for Multi-type entity Alignment, called CGMuAlign. Different from previous work, CG-MuAlign jointly aligns\nmultiple types of entities, collectively leverages the neighborhood\ninformation and generalizes to unlabeled entity types. Specifically,\nwe propose novel collective aggregation function tailored for this\ntask, that (1) relieves the incompleteness of knowledge graphs via\nboth cross-graph and self attentions, (2) scales up efficiently with\nmini-batch training paradigm and effective neighborhood sampling\nstrategy. We conduct experiments on real world knowledge graphs\nwith millions of entities and observe the superior performance\nbeyond existing methods. In addition, the running time of our approach is much less than the current state-of-the-art deep learning\nmethods."}}
{"id": "S9YDbiy1tF", "cdate": 1673462875845, "mdate": 1673462875845, "content": {"title": "Error Diagnosis and Data Profiling with Data X-Ray", "abstract": "The problem of identifying and repairing data errors has been an area of persistent focus in data management research. However, while traditional data cleaning techniques can be effective at identifying several data discrepancies, they disregard the fact that many errors are systematic, inherent to the process that produces the data, and thus will keep occurring unless the root cause is identified and corrected. \n\nIn this demonstration, we will present a large-scale diagnostic framework called DATAXRAY. Like a medical X-ray that aids the diagnosis of medical conditions by revealing problems underneath the surface, DATAXRAY reveals hidden connections and common properties among data errors. Thus, in contrast to traditional cleaning methods, which treat the symptoms, our system investigates the underlying conditions that cause the errors. \n\nThe core of DATAXRAY combines an intuitive and principled cost model derived by Bayesian analysis, and an efficient, highly parallelizable diagnostic algorithm that discovers common properties among erroneous data elements in a top-down fashion. Our system has a simple interface that allows users to load different datasets, to interactively adjust key diagnostic parameters, to explore the derived diagnoses, and to compare with solutions produced by alternative algorithms. Through this demonstration, participants will understand (1) the characteristics of good diagnoses, (2) how and why errors occur in real-world datasets, and (3) the distinctions with other related problems and approaches."}}
{"id": "_xJcllRhLs", "cdate": 1601100593086, "mdate": null, "content": {"title": "P-Companion: A Principled Framework for Diversified Complementary Product Recommendation", "abstract": "If one customer buys a tennis racket, what are the best 3 complementary products to purchase together? 3 tennis ball packs, 3 headbands, 3 overgrips, or 1 of each respectively? Complementary product recommendation (CPR), aiming at providing product suggestions that are often bought together to serve a joint demand, forms a pivotal component of e-commerce service, however, existing methods are far from optimal. Given one product, how to recommend its complementary products of different types is the key problem we tackle in this work. We first conduct an extensive analysis to correct the inaccurate assumptions adopted by existing work to show that co-purchased products are not always complementary and further propose a new strategy to generate clean distant supervision labels for CPR modeling. Moreover, to bridge in the gap from existing work that CPR does not only need relevance modeling but also requires diversity to fulfill the whole purchase demand, we develop a deep learning framework, P-Companion to explicitly model both relevance and diversity. More specifically, given one product with its product type, P-Companion first uses an encoder-decoder network to predict multiple complementary product types, then a transfer metric learning network is developed to project the embedding of query product to each predicted complementary product type subspace and further learn the complementary relationship based on the distant supervision labels within each subspace. The whole framework can be trained from end-to-end and robust to cold-start products attributed to a novel pretrained product embedding module named Product2Vec, based on graph attention networks. Extensive offline experiments show that P-Companion outperforms state-of-the-art baselines by 7.1% increase on the Hit@10 score with well-controlled diversity. Production-wise, we deploy P-Companion to provide online recommendations for over 200M products at Amazon and observe significant gains on product sales and profit."}}
{"id": "m4-Zx-0JkNm", "cdate": 1594406387904, "mdate": null, "content": {"title": "Automatic Validation of Textual Attribute Values in E-commerce Catalog by Learning with Limited Labeled Data", "abstract": "Product catalogs are valuable resources for eCommerce website. In the catalog, a product is associated with multiple attributes whose values are short texts, such as product name, brand, functionality and flavor. Usually individual retailers self-report these key values, and thus the catalog information unavoidably contains noisy facts. Although existing deep neural network models have shown success in conducting cross-checking between two pieces of texts, their success has to be dependent upon a large set of quality labeled data, which are hard to obtain in this validation task: products span a variety of categories. To address the aforementioned challenges, we propose a novel meta-learning latent variable approach, called MetaBridge, which can learn transferable knowledge from a subset of categories with limited labeled data and capture the uncertainty of never-seen categories with unlabeled data. More specifically, we make the following contributions. (1) We formalize the problem of validating the textual attribute values of products from a variety of categories as a natural language inference task in the few-shot learning setting, and propose a meta-learning latent variable model to jointly process the signals obtained from product profiles and textual attribute values. (2) We propose to integrate meta learning and latent variable in a unified model to effectively capture the uncertainty of various categories. (3) We propose a novel objective function based on latent variable model in the few-shot learning setting, which ensures distribution consistency between unlabeled and labeled data and prevents overfitting by sampling from the learned distribution. Extensive experiments on real eCommerce datasets from hundreds of categories demonstrate the effectiveness of MetaBridge on textual attribute validation and its outstanding performance compared with state-of-the-art approaches."}}
