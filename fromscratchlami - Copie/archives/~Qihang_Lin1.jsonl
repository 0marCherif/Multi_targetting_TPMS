{"id": "YsTJNRkliNG", "cdate": 1683745965040, "mdate": 1683745965040, "content": {"title": "Hybrid Predictive Models: When an Interpretable ModelCollaborates with a Black-box Model", "abstract": "Interpretable machine learning has become a strong competitor for black-box models.  However, the possible loss of the predictive performance for gaining understandability is often inevitable, especially when it needs to satisfy users with diverse backgrounds or high standards for what is considered interpretable.  This tension puts practitioners in a dilemma of choosing between high accuracy (black-box models) and interpretability (interpretable models).   In this work, we propose a novel framework  for  building a  Hybrid Predictive Model that integrates an interpretable model with any pre-trained  black-box model  to combine their  strengths.   The  interpretable  model  substitutes the  black-box  model on  asubset  of  data  where  the  interpretable  model  is  most  competent,  gaining  transparency at a low cost of the predictive accuracy.  We design a principled objective function that considers predictive accuracy, model interpretability, and model transparency(defined as the percentage of data processed by the interpretable substitute.)  Under this framework, we propose two hybrid models, one substituting with association rules and the other with linear models,  and design  customized  training algorithms  for  both models.   We test  thehybrid  models on  structured  data and  text  data where  interpretable  models collaboratewith various state-of-the-art black-box models.  Results show that hybrid models obtain an efficient trade-off between transparency and predictive performance,  characterized by pareto frontiers.  Finally, we apply the proposed model on a real-world patients dataset forpredicting cardiovascular disease and propose multi-model Pareto frontiers to assist model selection in real applications."}}
{"id": "6aKcyoDJBaX", "cdate": 1663850339119, "mdate": null, "content": {"title": "Federated Learning on Adaptively Weighted Nodes by Bilevel Optimization", "abstract": "We propose a federated learning method with weighted nodes in which the weights can be modified to optimize the model\u2019s performance on a separate validation set. The problem is formulated as a bilevel optimization problem where the inner problem is a federated learning problem with weighted nodes and the outer problem focuses on optimizing the weights based on the validation performance of the model returned from the inner problem. A communication-efficient federated optimization algorithm is designed to solve this bilevel optimization problem. We analyze the generalization performance of the output model and identify the scenarios when our method is in theory superior to training a model locally and superior to federated learning with static and evenly distributed weights. "}}
{"id": "FFPcFtWJwsB", "cdate": 1652737570022, "mdate": null, "content": {"title": "Large-scale Optimization of Partial AUC in a Range of False Positive Rates", "abstract": "The area under the ROC curve (AUC) is one of the most widely used performance measures for classification models in machine learning. However, it summarizes the true positive rates (TPRs) over all false positive rates (FPRs) in the ROC space, which may include the FPRs with no practical relevance in some applications. The partial AUC, as a generalization of the AUC, summarizes only the TPRs over a specific range of the FPRs and is thus a more suitable performance measure in many real-world situations. Although partial AUC optimization in a range of FPRs had been studied, existing algorithms are not scalable to big data and not applicable to deep learning.  To address this challenge, we cast the problem into a non-smooth difference-of-convex (DC) program for any smooth predictive functions (e.g., deep neural networks), which allowed us to develop an efficient approximated gradient descent method based on the Moreau envelope smoothing technique, inspired by recent advances in non-smooth DC optimization. To increase the efficiency of large data processing, we used an efficient stochastic block coordinate update in our algorithm. Our proposed algorithm can also be used to minimize the sum of ranked range loss, which also lacks efficient solvers. We established a complexity of $\\tilde O(1/\\epsilon^6)$ for finding a nearly $\\epsilon$-critical solution. Finally, we numerically demonstrated the effectiveness of our proposed algorithms in training both linear models and deep neural networks for partial AUC maximization and sum of ranked range loss minimization. "}}
{"id": "nyBJcnhjAoy", "cdate": 1652737485685, "mdate": null, "content": {"title": "ProtoX: Explaining a Reinforcement Learning Agent via Prototyping", "abstract": "While deep reinforcement learning has proven to be successful in solving control tasks, the ``black-box'' nature of an agent has received increasing concerns. We propose a prototype-based post-hoc \\emph{policy explainer}, ProtoX, that explains a black-box agent by prototyping the agent's behaviors into scenarios, each represented by a prototypical state. When learning prototypes, ProtoX considers both visual similarity and scenario similarity. The latter is unique to the reinforcement learning context since it explains why the same action is taken in visually different states. To teach ProtoX about visual similarity, we pre-train an encoder using contrastive learning via self-supervised learning to recognize states as similar if they occur close together in time and receive the same action from the black-box agent. We then add an isometry layer to allow ProtoX to adapt scenario similarity to the downstream task. ProtoX is trained via imitation learning using behavior cloning, and thus requires no access to the environment or agent. In addition to explanation fidelity, we  design different prototype shaping terms in the objective function to encourage better interpretability. We conduct various experiments to test ProtoX. Results show that ProtoX achieved high fidelity to the original black-box agent while providing meaningful and understandable explanations."}}
{"id": "eREppDCqKSM", "cdate": 1621721654245, "mdate": null, "content": {"title": "Optimal Epoch Stochastic Gradient Descent Ascent Methods for Min-Max Optimization", "abstract": "Epoch gradient descent method (a.k.a. Epoch-GD) proposed by Hazan and Kale (2011) was deemed a breakthrough for stochastic strongly convex minimization, which achieves the optimal convergence rate of O(1/T) with T iterative updates for the {\\it objective gap}. However, its extension to solving stochastic min-max problems with strong convexity and strong concavity still remains open, and it is still unclear whether a fast rate of O(1/T) for the {\\it duality gap} is achievable for stochastic min-max optimization under strong convexity and strong concavity. Although some recent studies have proposed stochastic algorithms with fast convergence rates for min-max problems, they require additional assumptions about the problem, e.g., smoothness, bi-linear structure, etc. In this paper, we bridge this gap by providing a sharp analysis of epoch-wise stochastic gradient descent ascent method (referred to as Epoch-GDA) for solving strongly convex strongly concave (SCSC) min-max problems, without imposing any additional assumption about smoothness or the function's structure. To the best of our knowledge, our result is the first one that shows Epoch-GDA can achieve the optimal rate of O(1/T) for the duality gap of general SCSC min-max problems. We emphasize that such generalization of Epoch-GD for strongly convex minimization problems to Epoch-GDA for SCSC min-max problems is non-trivial and requires novel technical analysis. Moreover, we notice that the key lemma can also be used for proving the convergence of Epoch-GDA for weakly-convex strongly-concave min-max problems, leading to a nearly optimal complexity without resorting to smoothness or other structural conditions."}}
{"id": "SyNpLo-OWr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Stochastic Optimization for DC Functions and Non-smooth Non-convex Regularizers with Non-asymptotic Convergence", "abstract": "Difference of convex (DC) functions cover a broad family of non-convex and possibly non-smooth and non-differentiable functions, and have wide applications in machine learning and statistics. Altho..."}}
{"id": "ry831QWAb", "cdate": 1518730157656, "mdate": null, "content": {"title": "BLOCK-NORMALIZED GRADIENT METHOD: AN EMPIRICAL STUDY FOR TRAINING DEEP NEURAL NETWORK", "abstract": "In this paper, we propose a generic and simple strategy for utilizing stochastic gradient information in optimization. The technique essentially contains two consecutive steps in each iteration: 1) computing and normalizing each block (layer) of the mini-batch stochastic gradient; 2) selecting appropriate step size to update the decision variable (parameter) towards the negative of the block-normalized gradient. We conduct extensive empirical studies on various non-convex neural network optimization problems, including multilayer perceptron, convolution neural networks and recurrent neural networks. The results indicate the block-normalized gradient can help accelerate the training of neural networks.  In particular,\nwe observe that the normalized gradient methods having constant step size with occasionally decay, such as SGD with momentum, have better performance in the deep convolution neural networks, while those with adaptive step sizes, such as Adam, perform better in recurrent neural networks. Besides, we also observe this line of methods can lead to solutions with better generalization properties, which is confirmed by the performance improvement over strong baselines. "}}
{"id": "SkNrBXG_-r", "cdate": 1514764800000, "mdate": null, "content": {"title": "A Unified Analysis of Stochastic Momentum Methods for Deep Learning", "abstract": "Stochastic momentum methods have been widely adopted in training deep neural networks. However, their theoretical analysis of convergence of the training objective and the generalization error for prediction is still under-explored. This paper aims to bridge the gap between practice and theory by analyzing the stochastic gradient (SG) method, and the stochastic momentum methods including two famous variants, i.e., the stochastic heavy-ball (SHB) method and the stochastic variant of Nesterov?s accelerated gradient (SNAG) method. We propose a framework that unifies the three variants. We then derive the convergence rates of the norm of gradient for the non-convex optimization problem, and analyze the generalization performance through the uniform stability approach. Particularly, the convergence analysis of the training objective exhibits that SHB and SNAG have no advantage over SG. However, the stability analysis shows that the momentum term can improve the stability of the learned model and hence improve the generalization performance. These theoretical insights verify the common wisdom and are also corroborated by our empirical analysis on deep learning."}}
{"id": "B1-6n9ZuWH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Level-Set Methods for Finite-Sum Constrained Convex Optimization", "abstract": "We consider the constrained optimization where the objective function and the constraints are defined as summation of finitely many loss functions. This model has applications in machine learning s..."}}
{"id": "rJ4dJ_Wd-S", "cdate": 1483228800000, "mdate": null, "content": {"title": "ADMM without a Fixed Penalty Parameter: Faster Convergence with New Adaptive Penalization", "abstract": "Alternating direction method of multipliers (ADMM) has received tremendous interest for solving numerous problems in machine learning, statistics and signal processing. However, it is known that the performance of ADMM and many of its variants is very sensitive to the penalty parameter of a quadratic penalty applied to the equality constraints. Although several approaches have been proposed for dynamically changing this parameter during the course of optimization, they do not yield theoretical improvement in the convergence rate and are not directly applicable to stochastic ADMM. In this paper, we develop a new ADMM and its linearized variant with a new adaptive scheme to update the penalty parameter. Our methods can be applied under both deterministic and stochastic optimization settings for structured non-smooth objective function. The novelty of the proposed scheme lies at that it is adaptive to a local sharpness property of the objective function, which marks the key difference from previous adaptive scheme that adjusts the penalty parameter per-iteration based on certain conditions on iterates. On theoretical side, given the local sharpness characterized by an exponent $\\theta\\in(0, 1]$, we show that the proposed ADMM enjoys an improved iteration complexity of $\\widetilde O(1/\\epsilon^{1-\\theta})$\\footnote{$\\widetilde O()$ suppresses a logarithmic factor.} in the deterministic setting and an iteration complexity of $\\widetilde O(1/\\epsilon^{2(1-\\theta)})$ in the stochastic setting without smoothness and strong convexity assumptions. The complexity in either setting improves that of the standard ADMM which only uses a fixed penalty parameter. On the practical side, we demonstrate that the proposed algorithms converge comparably to, if not much faster than, ADMM with a fine-tuned fixed penalty parameter."}}
