{"id": "_ktSa2OW7G", "cdate": 1668478177731, "mdate": 1668478177731, "content": {"title": "Pyramidal Recurrent Unit for Language Modeling", "abstract": "LSTMs are powerful tools for modeling con- textual information, as evidenced by their suc- cess at the task of language modeling. How- ever, modeling contexts in very high dimen- sional space can lead to poor generalizability. We introduce the Pyramidal Recurrent Unit (PRU), which enables learning representations in high dimensional space with more gener- alization power and fewer parameters. PRUs replace the linear transformation in LSTMs with more sophisticated interactions including pyramidal and grouped linear transformations. This architecture gives strong results on word- level language modeling while reducing the number of parameters significantly. In partic- ular, PRU improves the perplexity of a recent state-of-the-art language model Merity et al. (2018) by up to 1.3 points while learning 15- 20% fewer parameters. For similar number of model parameters, PRU outperforms all previ- ous RNN models that exploit different gating mechanisms and transformations. We provide a detailed examination of the PRU and its be- havior on the language modeling tasks. Our code is open-source and available at https: //sacmehta.github.io/PRU/."}}
{"id": "fwMj9jWBx9Q", "cdate": 1668478121029, "mdate": 1668478121029, "content": {"title": "DiCENet: Dimension-wise Convolutions for Efficient Networks", "abstract": "We introduce a novel and generic convolutional unit, DiCE unit, that is built using dimension-wise convolutions and dimension-wise fusion. The dimension-wise convolutions apply light-weight convolutional filtering across each dimension of the input tensor while dimension-wise fusion efficiently combines these dimension-wise representations; allowing the DiCE unit to efficiently encode spatial and channel-wise information contained in the input tensor. The DiCE unit is simple and can be seamlessly integrated with any architecture to improve its efficiency and performance. Compared to depth-wise separable convolutions, the DiCE unit shows significant improvements across different architectures. When DiCE units are stacked to build the DiCENet model, we observe significant improvements over state-of-the-art models across various computer vision tasks including image classification, object detection, and semantic segmentation. On the ImageNet dataset, the DiCENet delivers 2-4% higher accuracy than state-of-the-art manually designed models (e.g., MobileNetv2 and ShuffleNetv2). Also, DiCENet generalizes better to tasks (e.g., object detection) that are often used in resource-constrained devices in comparison to state-of-the-art separable convolution-based efficient networks, including neural search-based methods (e.g., MobileNetv3 and MixNet)."}}
{"id": "ONboNnSUf0G", "cdate": 1668478058160, "mdate": 1668478058160, "content": {"title": "ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network", "abstract": "We introduce a light-weight, power efficient, and gen- eral purpose convolutional neural network, ESPNetv2, for modeling visual and sequential data. Our network uses group point-wise and depth-wise dilated separable convolu- tions to learn representations from a large effective recep- tive field with fewer FLOPs and parameters. The perfor- mance of our network is evaluated on four different tasks: (1) object classification, (2) semantic segmentation, (3) ob- ject detection, and (4) language modeling. Experiments on these tasks, including image classification on the Ima- geNet and language modeling on the PenTree bank dataset, demonstrate the superior performance of our method over the state-of-the-art methods. Our network outperforms ES- PNet by 4-5% and has 2\u22124\u00d7 fewer FLOPs on the PASCAL VOC and the Cityscapes dataset. Compared to YOLOv2 on the MS-COCO object detection, ESPNetv2 delivers 4.4% higher accuracy with 6\u00d7 fewer FLOPs. Our ex- periments show that ESPNetv2 is much more power effi- cient than existing state-of-the-art efficient methods includ- ing ShuffleNets and MobileNets."}}
{"id": "bKYniQEdMG", "cdate": 1668477994231, "mdate": 1668477994231, "content": {"title": "ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation", "abstract": "We introduce a fast and efficient convolutional neural network, ES- PNet, for semantic segmentation of high resolution images under resource con- straints. ESPNet is based on a new convolutional module, efficient spatial pyra- mid (ESP), which is efficient in terms of computation, memory, and power. ES- PNet is 22 times faster (on a standard GPU) and 180 times smaller than the state-of-the-art semantic segmentation network PSPNet, while its category-wise accuracy is only 8% less. We evaluated ESPNet on a variety of semantic segmen- tation datasets including Cityscapes, PASCAL VOC, and a breast biopsy whole slide image dataset. Under the same constraints on memory and computation, ESPNet outperforms all the current efficient CNN networks such as MobileNet, ShuffleNet, and ENet on both standard metrics and our newly introduced perfor- mance metrics that measure efficiency on edge devices. Our network can process high resolution images at a rate of 112 and 9 frames per second on a standard GPU and edge device, respectively. Our code is open-source and available at https://sacmehta.github.io/ESPNet/."}}
{"id": "ZbwqqxW2f-G", "cdate": 1663850131415, "mdate": null, "content": {"title": "RangeAugment:  Efficient Online Augmentation with Range Learning", "abstract": "State-of-the-art automatic augmentation methods (e.g., AutoAugment and RandAugment) for visual recognition tasks diversify training data using a large set of augmentation operations. The range of magnitudes of many augmentation operations (e.g., brightness and contrast) is continuous. Therefore, to make search computationally tractable, these methods use fixed and manually-defined magnitude ranges for each operation, which may lead to sub-optimal policies. To answer the open question on the importance of magnitude ranges for each augmentation operation,  we introduce RangeAugment that allows us to efficiently learn the range of magnitudes for individual as well as composite augmentation operations. RangeAugment uses an auxiliary loss based on image similarity as a measure to control the range of magnitudes of augmentation operations. As a result, RangeAugment has a single scalar parameter for search, image similarity, which we simply optimize via linear search. RangeAugment integrates seamlessly with any model and learns model- and task-specific augmentation policies. With extensive experiments on the ImageNet dataset across different networks, we show that RangeAugment achieves competitive performance to state-of-the-art automatic augmentation methods with 4-5 times fewer augmentation operations. Experimental results on semantic segmentation and contrastive learning further shows RangeAugment's effectiveness."}}
{"id": "qUcX0Zn5ROG", "cdate": 1640995200000, "mdate": 1668780307751, "content": {"title": "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer", "abstract": "Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different..."}}
{"id": "kN2SjrR3zE", "cdate": 1640995200000, "mdate": 1668780308212, "content": {"title": "SPIN: An Empirical Evaluation on Sharing Parameters of Isotropic Networks", "abstract": "Recent isotropic networks, such as ConvMixer and Vision Transformers, have found significant success across visual recognition tasks, matching or outperforming non-isotropic Convolutional Neural Networks. Isotropic architectures are particularly well-suited to cross-layer weight sharing, an effective neural network compression technique. In this paper, we perform an empirical evaluation on methods for sharing parameters in isotropic networks (SPIN). We present a framework to formalize major weight sharing design decisions and perform a comprehensive empirical evaluation of this design space. Guided by our experimental results, we propose a weight sharing strategy to generate a family of models with better overall efficiency, in terms of FLOPs and parameters versus accuracy, compared to traditional scaling methods alone, for example compressing ConvMixer by $$1.9\\times $$ while improving accuracy on ImageNet. Finally, we perform a qualitative study to further understand the behavior of weight sharing in isotropic architectures. The code is available at https://github.com/apple/ml-spin ."}}
{"id": "fekj0N51dGn", "cdate": 1640995200000, "mdate": 1668780307714, "content": {"title": "CVNets: High Performance Library for Computer Vision", "abstract": "We introduce CVNets, a high-performance open-source library for training deep neural networks for visual recognition tasks, including classification, detection, and segmentation. CVNets supports image and video understanding tools, including data loading, data transformations, novel data sampling methods, and implementations of several standard networks with similar or better performance than previous studies. Our source code is available at: https://github.com/apple/ml-cvnets."}}
{"id": "XmOuDuqgnp", "cdate": 1640995200000, "mdate": 1668780307696, "content": {"title": "DiCENet: Dimension-Wise Convolutions for Efficient Networks", "abstract": "We introduce a novel and generic convolutional unit, <monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">DiCE</monospace> unit, that is built using dimension-wise convolutions and dimension-wise fusion. The dimension-wise convolutions apply light-weight convolutional filtering across each dimension of the input tensor while dimension-wise fusion efficiently combines these dimension-wise representations; allowing the <monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">DiCE</monospace> unit to efficiently encode spatial and channel-wise information contained in the input tensor. The <monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">DiCE</monospace> unit is simple and can be seamlessly integrated with any architecture to improve its efficiency and performance. Compared to depth-wise separable convolutions, the <monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">DiCE</monospace> unit shows significant improvements across different architectures. When <monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">DiCE</monospace> units are stacked to build the <monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">DiCENet</monospace> model, we observe significant improvements over state-of-the-art models across various computer vision tasks including image classification, object detection, and semantic segmentation. On the ImageNet dataset, the <monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">DiCENet</monospace> delivers 2-4 percent higher accuracy than state-of-the-art manually designed models (e.g., MobileNetv2 and ShuffleNetv2). Also, <monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">DiCENet</monospace> generalizes better to tasks (e.g., object detection) that are often used in resource-constrained devices in comparison to state-of-the-art separable convolution-based efficient networks, including neural search-based methods (e.g., MobileNetv3 and MixNet)."}}
{"id": "Qs_qYUJfARH", "cdate": 1640995200000, "mdate": 1668780308075, "content": {"title": "SPIN: An Empirical Evaluation on Sharing Parameters of Isotropic Networks", "abstract": "Recent isotropic networks, such as ConvMixer and vision transformers, have found significant success across visual recognition tasks, matching or outperforming non-isotropic convolutional neural networks (CNNs). Isotropic architectures are particularly well-suited to cross-layer weight sharing, an effective neural network compression technique. In this paper, we perform an empirical evaluation on methods for sharing parameters in isotropic networks (SPIN). We present a framework to formalize major weight sharing design decisions and perform a comprehensive empirical evaluation of this design space. Guided by our experimental results, we propose a weight sharing strategy to generate a family of models with better overall efficiency, in terms of FLOPs and parameters versus accuracy, compared to traditional scaling methods alone, for example compressing ConvMixer by 1.9x while improving accuracy on ImageNet. Finally, we perform a qualitative study to further understand the behavior of weight sharing in isotropic architectures. The code is available at https://github.com/apple/ml-spin."}}
