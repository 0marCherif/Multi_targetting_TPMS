{"id": "3RJV5QiP_7d", "cdate": 1698796800000, "mdate": 1707200899080, "content": {"title": "Line Segment Detection Based on False Peak Suppression and Local Hough Transform and Application to Nuclear Emulsion", "abstract": ""}}
{"id": "NRFqkKSunF", "cdate": 1680513291112, "mdate": 1680513291112, "content": {"title": "Character Decomposition for Japanese-Chinese Character-Level Neural Machine Translation", "abstract": "After years of development, Neural Machine\nTranslation (NMT) has produced richer translation results\nthan ever over various language pairs, becoming a new\nmachine translation model with great potential. For the NMT\nmodel, it can only translate words/characters contained in\nthe training data. One problem on NMT is handling of the\nlow-frequency words/characters in the training data. In this\npaper, we propose a method for removing characters whose\nfrequencies of appearance are less than a given minimum\nthreshold by decomposing such characters into their components and/or pseudo-characters, using the Chinese character\ndecomposition table we made. Experiments of Japanese-to-Chinese and Chinese-to-Japanese NMT with ASPEC-JC\n(Asian Scientific Paper Excerpt Corpus, Japanese-Chinese)\ncorpus show that the BLEU scores, the training time and\nthe number of parameters are varied with the number of\nthe given minimum thresholds of decomposed characters."}}
{"id": "PDACMU_rQD", "cdate": 1680513231937, "mdate": 1680513231937, "content": {"title": "Corpus Augmentation for Neural Machine Translation with Chinese-Japanese Parallel Corpora", "abstract": "The translation quality of Neural Machine Translation (NMT) systems depends strongly on\nthe training data size. Sufficient amounts of parallel data are, however, not available for many language\npairs. This paper presents a corpus augmentation method, which has two variations: one is for all\nlanguage pairs, and the other is for the Chinese-Japanese language pair. The method uses both source\nand target sentences of the existing parallel corpus and generates multiple pseudo-parallel sentence\npairs from a long parallel sentence pair containing punctuation marks as follows: (1) split the sentence\npair into parallel partial sentences; (2) back-translate the target partial sentences; and (3) replace each\npartial sentence in the source sentence with the back-translated target partial sentence to generate\npseudo-source sentences. The word alignment information, which is used to determine the split points,\nis modified with \u201cshared Chinese character rates\u201d in segments of the sentence pairs. The experiment\nresults of the Japanese-Chinese and Chinese-Japanese translation with ASPEC-JC (Asian Scientific Paper\nExcerpt Corpus, Japanese-Chinese) show that the method substantially improves translation performance.\nWe also supply the code (see Supplementary Materials) that can reproduce our proposed method."}}
{"id": "vZwTCkAQMB", "cdate": 1680513195485, "mdate": 1680513195485, "content": {"title": "WCC-JC: A Web-Crawled Corpus for Japanese-Chinese Neural Machine Translation", "abstract": "Currently, there are only a limited number of Japanese-Chinese bilingual corpora of\na sufficient amount that can be used as training data for neural machine translation (NMT). In\nparticular, there are few corpora that include spoken language such as daily conversation. In this\nresearch, we attempt to construct a Japanese-Chinese bilingual corpus of a certain scale by crawling\nthe subtitle data of movies and TV series from the websites. We calculated the BLEU scores of the\nconstructed WCC-JC (Web Crawled Corpus\u2014Japanese and Chinese) and the other compared corpora.\nWe also manually evaluated the translation results using the translation model trained on the WCC-JC\nto confirm the quality and effectiveness."}}
{"id": "B0VHyxiFRq", "cdate": 1680513160230, "mdate": 1680513160230, "content": {"title": "WCC-JC 2.0: A Web-Crawled and Manually Aligned Parallel Corpus for Japanese-Chinese Neural Machine Translation", "abstract": "Movie and TV subtitles are frequently employed in natural language processing (NLP)\napplications, but there are limited Japanese-Chinese bilingual corpora accessible as a dataset to train\nneural machine translation (NMT) models. In our previous study, we effectively constructed a corpus\nof a considerable size containing bilingual text data in both Japanese and Chinese by collecting\nsubtitle text data from websites that host movies and television series. The unsatisfactory translation\nperformance of the initial corpus, Web-Crawled Corpus of Japanese and Chinese (WCC-JC 1.0), was\npredominantly caused by the limited number of sentence pairs. To address this shortcoming, we\nthoroughly analyzed the issues associated with the construction of WCC-JC 1.0 and constructed the\nWCC-JC 2.0 corpus by first collecting subtitle data from movie and TV series websites. Then, we\nmanually aligned a large number of high-quality sentence pairs. Our efforts resulted in a new corpus\nthat includes about 1.4 million sentence pairs, an 87% increase compared with WCC-JC 1.0. As a\nresult, WCC-JC 2.0 is now among the largest publicly available Japanese-Chinese bilingual corpora\nin the world. To assess the performance of WCC-JC 2.0, we calculated the BLEU scores relative to\nother comparative corpora and performed manual evaluations of the translation results generated by\ntranslation models trained on WCC-JC 2.0. We provide WCC-JC 2.0 as a free download for research\npurposes only."}}
{"id": "IQu20TaX5Uk", "cdate": 1672531200000, "mdate": 1707200899079, "content": {"title": "An Enhanced Method for Neural Machine Translation via Data Augmentation Based on the Self-Constructed English-Chinese Corpus, WCC-EC", "abstract": "In an era of increasing globalization, the imperative for understanding multilingual texts elevated the role of translation to an everyday necessity. The efficacy of contemporary Neural Machine Translation (NMT) systems was heavily dependent on the availability of substantial training data. As such, the creation of an expansive parallel corpus became a strategic focal point, providing a bedrock for the evolution of high-caliber NMT systems. However, building a parallel corpus often posed challenges due to its complexity and cost, especially for language pairs that lacked sufficient parallel data. To tackle this challenge, this study proposed a novel data augmentation method for the corpus. Bilingual news texts sourced from the KEKE English website were utilized, and SentenceBERT was employed to ensure accurate sentence alignment. Subsequently, the parallel partial sentences within the corpus were filtered and used to augment the dataset. Finally, the effectiveness of the method was assessed by calculating BLEU, chrF and METEOR scores based on both the original corpus and the data-augmented corpus using the base translation model. The experimental results indicated that compared to the baseline method, the optimal method showed an improvement of 0.4-2.1 points in BLEU scores, an improvement of 0.5-2.7 points in chrF scores, and an increase of 0.5-1.6 points in METEOR scores."}}
{"id": "siPIA6LsVV2", "cdate": 1546300800000, "mdate": 1681668452721, "content": {"title": "Character Decomposition for Japanese-Chinese Character-Level Neural Machine Translation", "abstract": "After years of development, Neural Machine Translation (NMT) has produced richer translation results than ever over various language pairs, becoming a new machine translation model with great potential. For the NMT model, it can only translate words/characters contained in the training data. One problem on NMT is handling of the low-frequency words/characters in the training data. In this paper, we propose a method for removing characters whose frequencies of appearance are less than a given minimum threshold by decomposing such characters into their components and/or pseudo-characters, using the Chinese character decomposition table we made. Experiments of Japanese-to-Chinese and Chinese-to-Japanese NMT with ASPEC-JC (Asian Scientific Paper Excerpt Corpus, Japanese-Chinese) corpus show that the BLEU scores, the training time and the number of parameters are varied with the number of the given minimum thresholds of decomposed characters."}}
{"id": "39Ld5FFzbJt", "cdate": 1546300800000, "mdate": 1681668452721, "content": {"title": "Corpus Augmentation by Sentence Segmentation for Low-Resource Neural Machine Translation", "abstract": "Neural Machine Translation (NMT) has been proven to achieve impressive results. The NMT system translation results depend strongly on the size and quality of parallel corpora. Nevertheless, for many language pairs, no rich-resource parallel corpora exist. As described in this paper, we propose a corpus augmentation method by segmenting long sentences in a corpus using back-translation and generating pseudo-parallel sentence pairs. The experiment results of the Japanese-Chinese and Chinese-Japanese translation with Japanese-Chinese scientific paper excerpt corpus (ASPEC-JC) show that the method improves translation performance."}}
{"id": "fPk1bGE2RT", "cdate": 1514764800000, "mdate": 1681668452678, "content": {"title": "Improving Character-level Japanese-Chinese Neural Machine Translation with Radicals as an Additional Input Feature", "abstract": "In recent years, Neural Machine Translation (NMT) has been proven to get impressive results. While some additional linguistic features of input words improve word-level NMT, any additional character features have not been used to improve character-level NMT so far. In this paper, we show that the radicals of Chinese characters (or kanji), as a character feature information, can be easily provide further improvements in the character-level NMT. In experiments on WAT2016 Japanese-Chinese scientific paper excerpt corpus (ASPEC-JP), we find that the proposed method improves the translation quality according to two aspects: perplexity and BLEU. The character-level NMT with the radical input feature's model got a state-of-the-art result of 40.61 BLEU points in the test set, which is an improvement of about 8.6 BLEU points over the best system on the WAT2016 Japanese-to-Chinese translation subtask with ASPEC-JP. The improvements over the character-level NMT with no additional input feature are up to about 1.5 and 1.4 BLEU points in the development-test set and the test set of the corpus, respectively."}}
{"id": "sLxzwJ2T5jq", "cdate": 1483228800000, "mdate": 1681668452683, "content": {"title": "Improving character-level Japanese-Chinese neural machine translation with radicals as an additional input feature", "abstract": ""}}
