{"id": "pbILUUf_hBN", "cdate": 1652737418452, "mdate": null, "content": {"title": "A Near-Optimal Best-of-Both-Worlds Algorithm for Online Learning with Feedback Graphs", "abstract": "We consider online learning with feedback graphs, a sequential decision-making framework where the learner's feedback is determined by a directed graph over the action set. We present a computationally-efficient algorithm for learning in this framework that simultaneously achieves near-optimal regret bounds in both stochastic and adversarial environments. The bound against oblivious adversaries is $\\tilde{O} (\\sqrt{\\alpha T})$, where $T$ is the time horizon and $\\alpha$ is the independence number of the feedback graph. The bound against stochastic environments is $O\\big((\\ln T)^2 \\max_{S\\in \\mathcal I(G)} \\sum_{i \\in S} \\Delta_i^{-1}\\big)$ where $\\mathcal I(G)$ is the family of all independent sets in a suitably defined undirected version of the graph and $\\Delta_i$ are the suboptimality gaps.\nThe algorithm combines ideas from the EXP3++ algorithm for stochastic and adversarial bandits and the EXP3.G algorithm for feedback graphs with a novel exploration scheme. The scheme, which exploits the structure of the graph to reduce exploration, is key to obtain best-of-both-worlds guarantees with feedback graphs. \nWe also extend our algorithm and results to a setting where the feedback graphs are allowed to change over time."}}
{"id": "COWb5y9eaPb", "cdate": 1609459200000, "mdate": null, "content": {"title": "An Algorithm for Stochastic and Adversarial Bandits with Switching Costs", "abstract": "We propose an algorithm for stochastic and adversarial multiarmed bandits with switching costs, where the algorithm pays a price $\\lambda$ every time it switches the arm being played. Our algorithm is based on adaptation of the Tsallis-INF algorithm of Zimmert and Seldin (2021) and requires no prior knowledge of the regime or time horizon. In the oblivious adversarial setting it achieves the minimax optimal regret bound of $O\\big((\\lambda K)^{1/3}T^{2/3} + \\sqrt{KT}\\big)$, where $T$ is the time horizon and $K$ is the number of arms. In the stochastically constrained adversarial regime, which includes the stochastic regime as a special case, it achieves a regret bound of $O\\left(\\big((\\lambda K)^{2/3} T^{1/3} + \\ln T\\big)\\sum_{i \\neq i^*} \\Delta_i^{-1}\\right)$, where $\\Delta_i$ are the suboptimality gaps and $i^*$ is a unique optimal arm. In the special case of $\\lambda = 0$ (no switching costs), both bounds are minimax optimal within constants. We also explore variants of the problem, where switching cost is allowed to change over time. We provide experimental evaluation showing competitiveness of our algorithm with the relevant baselines in the stochastic, stochastically constrained adversarial, and adversarial regimes with fixed switching cost."}}
{"id": "WDz6kCfa84i", "cdate": 1577836800000, "mdate": null, "content": {"title": "Tsallis-INF for Decoupled Exploration and Exploitation in Multi-armed Bandits", "abstract": "We consider a variation of the multi-armed bandit problem, introduced by Avner et al. (2012), in which the forecaster is allowed to choose one arm to explore and one arm to exploit at every round. The loss of the exploited arm is blindly suffered by the forecaster, while the loss of the explored arm is observed without being suffered. The goal of the learner is to minimize the regret. We derive a new algorithm using regularization by Tsallis entropy to achieve best of both worlds guarantees. In the adversarial setting we show that the algorithm achieves the minimax optimal $O(\\sqrt{KT})$ regret bound, slightly improving on the result of Avner et al.. In the stochastic regime the algorithm achieves a time-independent regret bound, significantly improving on the result of Avner et al.. The algorithm also achieves the same time-independent regret bound in the more general stochastically constrained adversarial regime introduced by Wei and Luo (2018)."}}
