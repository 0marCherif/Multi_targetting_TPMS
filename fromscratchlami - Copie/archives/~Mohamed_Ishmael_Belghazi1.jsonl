{"id": "k_iNqflnekU", "cdate": 1663850339714, "mdate": null, "content": {"title": "An ensemble view on mixup", "abstract": "Deep ensembles are widely used to improve the generalization, calibration, uncertainty estimates and adversarial robustness of neural networks. In parallel, the data augmentation technique of mixup has grown popular for the very same reasons. Could these two techniques be related? This work suggests that both implement a similar inductive bias to \u201clinearize\u201d decision boundaries. We show how to obtain diverse predictions from a single mixup machine by interpolating a test instance with multiple reference points. These \u201cmixup ensembles\u201d are cheap: one needs to train and store one single model, as opposed to the K independent members forming a deep ensemble. Motivated by the limitations of ensembles to model uncertainty far away from the training data, we propose a variant of mixup that builds augmented examples using both random interpolations and extrapolations of examples. We evaluate the efficacy of our proposed methods across a variety of in-domain and out-domain metrics on the CIFAR-10 and CIFAR-10-NEG datasets."}}
{"id": "f9AIc3mEprf", "cdate": 1632875641449, "mdate": null, "content": {"title": "What classifiers know what they don't know?", "abstract": "Being uncertain when facing the unknown is key to intelligent decision making. However, machine learning algorithms lack reliable estimates about their predictive uncertainty. This leads to wrong and overly-confident decisions when encountering classes unseen during training. Despite the importance of equipping classifiers with uncertainty estimates ready for the real world, prior work has focused on small datasets and little or no class discrepancy between training and testing data. To close this gap, we introduce UIMNET: a realistic, ImageNet-scale test-bed to evaluate predictive uncertainty estimates for deep image classifiers. Our benchmark provides implementations of eight state-of-the-art algorithms, six uncertainty measures, four in-domain metrics, three out-domain metrics, and a fully automated pipeline to train, calibrate, ensemble, select, and evaluate models. Our test-bed is open-source and all of our results are reproducible from a fixed commit in our repository. Adding new datasets, algorithms, measures, or metrics is a matter of a few lines of code-in so hoping that UIMNET becomes a stepping stone towards realistic, rigorous, and reproducible research in uncertainty estimation. Our results show that ensembles of ERM classifiers as well as single MIMO classifiers are the two best alternatives currently available to measure uncertainty about both in-domain and out-domain classe."}}
{"id": "aodjQXunVH9", "cdate": 1609459200000, "mdate": 1682324123970, "content": {"title": "What classifiers know what they don't?", "abstract": "Being uncertain when facing the unknown is key to intelligent decision making. However, machine learning algorithms lack reliable estimates about their predictive uncertainty. This leads to wrong and overly-confident decisions when encountering classes unseen during training. Despite the importance of equipping classifiers with uncertainty estimates ready for the real world, prior work has focused on small datasets and little or no class discrepancy between training and testing data. To close this gap, we introduce UIMNET: a realistic, ImageNet-scale test-bed to evaluate predictive uncertainty estimates for deep image classifiers. Our benchmark provides implementations of eight state-of-the-art algorithms, six uncertainty measures, four in-domain metrics, three out-domain metrics, and a fully automated pipeline to train, calibrate, ensemble, select, and evaluate models. Our test-bed is open-source and all of our results are reproducible from a fixed commit in our repository. Adding new datasets, algorithms, measures, or metrics is a matter of a few lines of code-in so hoping that UIMNET becomes a stepping stone towards realistic, rigorous, and reproducible research in uncertainty estimation. Our results show that ensembles of ERM classifiers as well as single MIMO classifiers are the two best alternatives currently available to measure uncertainty about both in-domain and out-domain classes."}}
{"id": "hymY63zzab", "cdate": 1596118818042, "mdate": null, "content": {"title": "Hierarchical Adversarially Learned Inference", "abstract": "We propose a novel hierarchical generative model with a simple Markovian structure and a corresponding inference model. Both the generative and inference model are trained using the adversarial learning paradigm. We demonstrate that the hierarchical structure supports the learning of progressively more abstract representations as well as providing semantically meaningful reconstructions with different levels of fidelity. Furthermore, we show that minimizing the Jensen-Shanon divergence between the generative and inference network is enough to minimize the reconstruction error. The resulting semantically meaningful hierarchical latent structure discovery is exemplified on the CelebA dataset. There, we show that the features learned by our model in an unsupervised way outperform the best handcrafted features. Furthermore, the extracted features remain competitive when compared to several recent deep supervised approaches on an attribute prediction task on CelebA. Finally, we leverage the model's inference network to achieve state-of-the-art performance on a semi-supervised variant of the MNIST digit classification task.\n"}}
{"id": "BE7J0XS4pUD", "cdate": 1596118581723, "mdate": null, "content": {"title": "Mutual Information Neural Estimation", "abstract": "We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent\nover neural networks. We present a Mutual Information Neural Estimator (MINE) that is linearly\nscalable in dimensionality as well as in sample\nsize, trainable through back-prop, and strongly\nconsistent. We present a handful of applications\non which MINE can be used to minimize or maximize mutual information. We apply MINE to improve adversarially trained generative models. We\nalso use MINE to implement the Information Bottleneck, applying it to supervised classification;\nour results demonstrate substantial improvement\nin flexibility and performance in these settings."}}
{"id": "yeR--mI0eoz", "cdate": 1596118472945, "mdate": null, "content": {"title": "Conservativeness of untied auto-encoders", "abstract": "We discuss necessary and sufficient conditions for an\nauto-encoder to define a conservative vector field, in\nwhich case it is associated with an energy function akin\nto the unnormalized log-probability of the data. We\nshow that the conditions for conservativeness are more\ngeneral than for encoder and decoder weights to be the\nsame (\u201ctied weights\u201d), and that they also depend on the\nform of the hidden unit activation function, but that contractive training criteria, such as denoising, will enforce\nthese conditions locally. Based on these observations,\nwe show how we can use auto-encoders to extract the\nconservative component of a vector field."}}
{"id": "BkMhorBeUr", "cdate": 1567802788400, "mdate": null, "content": {"title": "Learning about an exponential amount of conditional distributions", "abstract": "We introduce the Neural Conditioner (NC), a self-supervised machine able to learn about all the conditional distributions of a random vector X. The NC is a function NC(x\u22c5a,a,r) that leverages adversarial training to match each conditional distribution P(Xr|Xa=xa). After training, the NC generalizes to sample from conditional distributions never seen, including the joint distribution. The NC is also able to auto-encode examples, providing data representations useful for downstream classification tasks. In sum, the NC integrates different self-supervised tasks (each being the estimation of a conditional distribution) and levels of supervision (partially observed data) seamlessly into a single learning experience."}}
{"id": "X6wC4WhHqlv", "cdate": 1546300800000, "mdate": 1682324124017, "content": {"title": "Learning about an exponential amount of conditional distributions", "abstract": "We introduce the Neural Conditioner (NC), a self-supervised machine able to learn about all the conditional distributions of a random vector $X$. The NC is a function $NC(x \\cdot a, a, r)$ that leverages adversarial training to match each conditional distribution $P(X_r|X_a=x_a)$. After training, the NC generalizes to sample from conditional distributions never seen, including the joint distribution. The NC is also able to auto-encode examples, providing data representations useful for downstream classification tasks. In sum, the NC integrates different self-supervised tasks (each being the estimation of a conditional distribution) and levels of supervision (partially observed data) seamlessly into a single learning experience."}}
{"id": "rJHOuiqaf", "cdate": 1525491821225, "mdate": null, "content": {"title": "MINE: Mutual Information Neural Estimation", "abstract": "This paper presents a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size. MINE is  back-propable and we prove that it is strongly consistent. We illustrate a handful of applications in which MINE is succesfully applied  to enhance the property of generative models in both unsupervised and supervised settings. We apply our framework to estimate the information bottleneck, and apply it in tasks related to supervised classification problems. Our results  demonstrate substantial added flexibility and improvement in these settings.\n"}}
{"id": "HyXNCZbCZ", "cdate": 1518730166638, "mdate": null, "content": {"title": "Hierarchical Adversarially Learned Inference", "abstract": "We propose a novel hierarchical generative model with a simple Markovian structure and a corresponding inference model. Both the generative and inference model are trained using the adversarial learning paradigm. We demonstrate that the hierarchical structure supports the learning of progressively more abstract representations as well as providing semantically meaningful reconstructions with different levels of fidelity. Furthermore, we show that minimizing the Jensen-Shanon divergence between the generative and inference network is enough to minimize the reconstruction error.  The resulting semantically meaningful hierarchical latent structure discovery is exemplified on the CelebA dataset.  There, we show that the features learned by our model in an unsupervised way outperform the best handcrafted features. Furthermore, the extracted features remain competitive when compared to several recent deep supervised approaches on an attribute prediction task on CelebA. Finally, we leverage the model's inference network to achieve state-of-the-art performance on a semi-supervised variant of the MNIST digit classification task. "}}
