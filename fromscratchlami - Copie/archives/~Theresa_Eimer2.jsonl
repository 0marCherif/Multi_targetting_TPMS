{"id": "DJgHzXv61b", "cdate": 1686250303304, "mdate": null, "content": {"title": "Contextualize Me \u2013 The Case for Context in Reinforcement Learning", "abstract": "While Reinforcement Learning (RL) has shown successes in a variety of domains, including game playing, robot manipulation and nuclear fusion, modern RL algorithms are not designed with generalization in mind, making them brittle when faced with even slight variations of their environment. \nTo address this limitation, recent research has increasingly focused on the generalization capabilities of RL agents.\nIdeally, general agents should be capable of zero-shot transfer to previously unseen environments and robust to changes in the problem setting while interacting with an environment.\nSteps in this direction have been taken by proposing new problem settings where agents can test their transfer performance, e.g.~the Arcade Learning Environment's flavors or benchmarks utilizing Procedural Content Generation (PCG) to increase task variation, e.g. ProcGen, NetHack or Alchemy.\n\nWhile these extended problem settings in RL have expanded the possibilities for benchmarking agents in diverse environments, the degree of task variation is often either unknown or cannot be controlled precisely.\nWe believe that generalization in RL is held back by these factors, stemming in part from a lack of problem formalization.\nIn order to facilitate generalization in RL, contextual RL (cRL) proposes to explicitly take environment characteristics, the so-called context into account.\nThis inclusion enables precise design of train and test distributions with respect to this context.\nThus, cRL allows us to reason about the generalization capabilities of RL agents and to quantify their generalization performance.\nOverall, cRL provides a framework for both theoretical analysis and practical improvements.\n\nIn order to empirically study cRL, we introduce our benchmark library CARL, short for Context-Adaptive Reinforcement Learning.\nCARL collects well-established environments from the RL community and extends them with the notion of context.\nWe use our benchmark library to empirically show how different context variations can drastically increase the difficulty of training RL agents, even in simple environments.\nWe further verify the intuition that allowing RL agents access to context information is beneficial for generalization tasks in theory and practice."}}
{"id": "btdRY4lftF6", "cdate": 1686211287185, "mdate": 1686211287185, "content": {"title": "Self-Paced Context Evaluation for Contextual Reinforcement Learning", "abstract": " Reinforcement learning (RL) has made a lot of advances for solving a single problem in a given environment; but learning policies that generalize to unseen variations of a problem remains challenging. To improve sample efficiency for learning on such instances of a problem domain, we present Self-Paced Context Evaluation (SPaCE). Based on self-paced learning, \\spc automatically generates \\task curricula online with little computational overhead. To this end, SPaCE leverages information contained in state values during training to accelerate and improve training performance as well as generalization capabilities to new instances from the same problem domain. Nevertheless, SPaCE is independent of the problem domain at hand and can be applied on top of any RL agent with state-value function approximation. We demonstrate SPaCE's ability to speed up learning of different value-based RL agents on two environments, showing better generalization capabilities and up to 10x faster learning compared to naive approaches such as round robin or SPDRL, as the closest state-of-the-art approach. "}}
{"id": "1VUhOYjcqs", "cdate": 1686211193796, "mdate": 1686211193796, "content": {"title": "Hyperparameters in Reinforcement Learning and How To Tune Them", "abstract": "In order to improve reproducibility, deep reinforcement learning (RL) has been adopting better scientific practices such as standardized evaluation metrics and reporting. \nHowever, the process of hyperparameter optimization still varies widely across papers, which makes it challenging to compare RL algorithms fairly.  \nIn this paper, we show that hyperparameter choices in RL can significantly affect the agent's final performance and sample efficiency, and that the hyperparameter landscape can strongly depend on the tuning seed which may lead to overfitting. \nWe therefore propose adopting established best practices from AutoML, such as the separation of tuning and testing seeds, as well as principled hyperparameter optimization (HPO) across a broad search space. \nWe support this by comparing multiple state-of-the-art HPO tools on a range of RL algorithms and environments to their hand-tuned counterparts, demonstrating that HPO approaches often have higher performance and lower compute overhead. \nAs a result of our findings, we recommend a set of best practices for the RL community, which should result in stronger empirical results with fewer computational costs, better reproducibility, and thus faster progress.\nIn order to encourage the adoption of these practices, we provide plug-and-play implementations of the tuning algorithms used in this paper at https://github.com/facebookresearch/how-to-autorl."}}
{"id": "N3IDYxLxgtW", "cdate": 1685982300290, "mdate": null, "content": {"title": "Hyperparameters in Reinforcement Learning and How To Tune Them", "abstract": "Deep Reinforcement Learning (RL) has been adopting better scientific practices in order to improve reproducibility such as standardized evaluation metrics and reporting as well as greater attention to implementation details and design decisions. However, the process of hyperparameter optimization still varies widely across papers with inefficient grid searches being most commonly used. This makes fair comparisons between RL algorithms challenging. In this paper, we show that hyperparameter choices in RL can significantly affect the agent\u2019s final performance and sample efficiency, and that the hyperparameter landscape can strongly depend on the tuning seed which might lead to overfitting to single seeds. We therefore propose adopting established best practices from AutoML, such as the separation of tuning and testing seeds, as well as principled hyperparameter optimization (HPO) across a broad search space. We support this by comparing multiple state-of-the-art HPO tools on a range of RL algorithms and environments to their hand-tuned counterparts, demonstrating that HPO approaches often have higher performance and lower compute overhead. As a result of our findings, we recommend a set of best practices for the RL community going forward, which should result in stronger empirical results with fewer computational costs, better reproducibility, and thus faster progress in RL. In order to encourage the adoption of these practices, we provide plug-and-play implementations of the tuning algorithms used in this paper at https://anonymous.4open.science/r/how-to-autorl-DE67/README.md."}}
{"id": "qQghx7f6Qp", "cdate": 1661361914738, "mdate": 1661361914738, "content": {"title": "Automated Reinforcement Learning (AutoRL): A Survey and Open Problems", "abstract": "The combination of Reinforcement Learning (RL) with deep learning has led to a series of impressive feats, with many believing (deep) RL provides a path towards generally capable agents. However, the success of RL agents is often highly sensitive to design choices in the training process, which may require tedious and error-prone manual tuning. This makes it challenging to use RL for new problems, while also limits its full potential. In many other areas of machine learning, AutoML has shown it is possible to automate such design choices and has also yielded promising initial results when applied to RL. However, Automated Reinforcement Learning (AutoRL) involves not only standard applications of AutoML but also includes additional challenges unique to RL, that naturally produce a different set of methods. As such, AutoRL has been emerging as an important area of research in RL, providing promise in a variety of applications from RNA design to playing games such as Go. Given the diversity of methods and environments considered in RL, much of the research has been conducted in distinct subfields, ranging from meta-learning to evolution. In this survey we seek to unify the field of AutoRL, we provide a common taxonomy, discuss each area in detail and pose open problems which would be of interest to researchers going forward."}}
{"id": "g_2W9quM4U9", "cdate": 1642089007813, "mdate": 1642089007813, "content": {"title": "DACBench: A Benchmark Library for Dynamic Algorithm Configuration", "abstract": "Dynamic Algorithm Configuration (DAC) aims to\ndynamically control a target algorithm\u2019s hyperpa-\nrameters in order to improve its performance. Sev-\neral theoretical and empirical results have demon-\nstrated the benefits of dynamically controlling hy-\nperparameters in domains like evolutionary com-\nputation, AI Planning or deep learning. Replicat-\ning these results, as well as studying new methods\nfor DAC, however, is difficult since existing bench-\nmarks are often specialized and incompatible with\nthe same interfaces. To facilitate benchmarking and\nthus research on DAC, we propose DACBench, a\nbenchmark library that seeks to collect and stan-\ndardize existing DAC benchmarks from different\nAI domains, as well as provide a template for new\nones. For the design of DACBench, we focused on\nimportant desiderata, such as (i) flexibility, (ii) re-\nproducibility, (iii) extensibility and (iv) automatic\ndocumentation and visualization. To show the po-\ntential, broad applicability and challenges of DAC,\nwe explore how a set of six initial benchmarks com-\npare in several dimensions of difficulty."}}
