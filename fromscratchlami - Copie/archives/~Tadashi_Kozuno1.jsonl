{"id": "H6PGfgK_35", "cdate": 1672531200000, "mdate": 1681706664974, "content": {"title": "Avoiding Model Estimation in Robust Markov Decision Processes with a Generative Model", "abstract": "Robust Markov Decision Processes (MDPs) are getting more attention for learning a robust policy which is less sensitive to environment changes. There are an increasing number of works analyzing sample-efficiency of robust MDPs. However, most works study robust MDPs in a model-based regime, where the transition probability needs to be estimated and requires $\\mathcal{O}(|\\mathcal{S}|^2|\\mathcal{A}|)$ storage in memory. A common way to solve robust MDPs is to formulate them as a distributionally robust optimization (DRO) problem. However, solving a DRO problem is non-trivial, so prior works typically assume a strong oracle to obtain the optimal solution of the DRO problem easily. To remove the need for an oracle, we first transform the original robust MDPs into an alternative form, as the alternative form allows us to use stochastic gradient methods to solve the robust MDPs. Moreover, we prove the alternative form still preserves the role of robustness. With this new formulation, we devise a sample-efficient algorithm to solve the robust MDPs in a model-free regime, from which we benefit lower memory space $\\mathcal{O}(|\\mathcal{S}||\\mathcal{A}|)$ without using the oracle. Finally, we validate our theoretical findings via numerical experiments and show the efficiency to solve the alternative form of robust MDPs."}}
{"id": "GKsNIC_mQRG", "cdate": 1663850532782, "mdate": null, "content": {"title": "Emergence of Exploration in Policy Gradient Reinforcement Learning via Resetting", "abstract": "In reinforcement learning (RL), many exploration methods explicitly promote stochastic policies, e.g., by adding an entropy bonus. We argue that exploration only matters in RL because the agent repeatedly encounters the same or similar states, so that it is beneficial to gradually improve the performance over the encounters; otherwise, the greedy policy would be optimal. Based on this intuition, we propose ReMax, an objective for RL whereby stochastic exploration arises as an emergent property, without adding any explicit exploration bonus. In ReMax, an episode is modified so that the agent can reset to previous states in the trajectory, and the agent\u2019s goal is to maximize the best return in the trajectory tree. We show that this ReMax objective can be directly optimized with an unbiased policy gradient method. Experiments confirm that ReMax leads to the emergence of a stochastic exploration policy, and improves the performance compared to RL with no exploration bonus."}}
{"id": "Xq2J1kiZeHE", "cdate": 1663850027331, "mdate": null, "content": {"title": "KL-Entropy-Regularized RL with a Generative Model is Minimax Optimal", "abstract": "In this work, we consider and analyze the sample complexity of model-free reinforcement learning with a generative model. Particularly, we analyze mirror descent value iteration (MDVI) by Geist et al. (2019) and Vieillard et al. (2020a), which uses the Kullback-Leibler divergence and entropy regularization in its value and policy updates. Our analysis shows that it is nearly minimax-optimal for finding an \u03b5-optimal policy when \u03b5 is sufficiently small. This is the first theoretical result that demonstrates that a simple model-free algorithm without variance-reduction can be nearly minimax-optimal under the considered setting."}}
{"id": "Q_WPshXgGI9", "cdate": 1652737826222, "mdate": null, "content": {"title": "Confident Approximate Policy Iteration for Efficient Local Planning in $q^\\pi$-realizable MDPs", "abstract": "We consider approximate dynamic programming in $\\gamma$-discounted Markov decision processes and apply it to approximate planning with linear value-function approximation. Our first contribution is a new variant of Approximate Policy Iteration (API), called Confident Approximate Policy Iteration (CAPI), which computes a deterministic stationary policy with an optimal error bound scaling linearly with the product of the effective horizon $H$ and the worst-case approximation error  $\\epsilon$ of the action-value functions of stationary policies. This improvement over API (whose error scales with $H^2$) comes at the price of an $H$-fold increase in memory cost. Unlike Scherrer and Lesner [2012], who recommended computing a non-stationary policy to achieve a similar improvement (with the same memory overhead), we are able to stick to stationary policies. This allows for our second contribution, the application of CAPI to planning with local access to a simulator and $d$-dimensional linear function approximation. As such, we design a planning algorithm that applies CAPI to obtain a sequence of policies with successively refined accuracies on a dynamically evolving set of states. The algorithm outputs an $\\tilde O(\\sqrt{d}H\\epsilon)$-optimal policy after issuing $\\tilde O(dH^4/\\epsilon^2)$ queries to the simulator, simultaneously achieving the optimal accuracy bound and the best known query complexity bound, while earlier algorithms in the literature achieve only one of them. This query complexity is shown to be tight in all parameters except $H$. These improvements come at the expense of a mild (polynomial) increase in memory and computational costs of both the algorithm and its output policy."}}
{"id": "wgRVngcfMO0", "cdate": 1640995200000, "mdate": 1668508372574, "content": {"title": "Variational oracle guiding for reinforcement learning", "abstract": "How to make intelligent decisions is a central problem in machine learning and artificial intelligence. Despite recent successes of deep reinforcement learning (RL) in various decision making problems, an important but under-explored aspect is how to leverage oracle observation (the information that is invisible during online decision making, but is available during offline training) to facilitate learning. For example, human experts will look at the replay after a Poker game, in which they can check the opponents' hands to improve their estimation of the opponents' hands from the visible information during playing. In this work, we study such problems based on Bayesian theory and derive an objective to leverage oracle observation in RL using variational methods. Our key contribution is to propose a general learning framework referred to as variational latent oracle guiding (VLOG) for DRL. VLOG is featured with preferable properties such as its robust and promising performance and its versatility to incorporate with any value-based DRL algorithm. We empirically demonstrate the effectiveness of VLOG in online and offline RL domains with tasks ranging from video games to a challenging tile-based game Mahjong. Furthermore, we publish the Mahjong environment and an offline RL dataset as a benchmark to facilitate future research on oracle guiding (https://github.com/Agony5757/mahjong)."}}
{"id": "jtERqNnFmfB", "cdate": 1640995200000, "mdate": 1681706664969, "content": {"title": "Deep Learning-based Nonlinear Quantizer for Fronthaul Compression", "abstract": "This paper proposes an autoencoder-based nonlinear quantizer to conduct IQ data compression for the Fronthaul link employing digitized radio over fiber (D-RoF). The numerical simulation reports a 6-bit nonlinear quantizer to satisfy the EVM requirement."}}
{"id": "XZbKPddWbnW", "cdate": 1640995200000, "mdate": 1681706664969, "content": {"title": "No More Pesky Hyperparameters: Offline Hyperparameter Tuning for RL", "abstract": "The performance of reinforcement learning (RL) agents is sensitive to the choice of hyperparameters. In real-world settings like robotics or industrial control systems, however, testing different hyperparameter configurations directly on the environment can be financially prohibitive, dangerous, or time consuming. We propose a new approach to tune hyperparameters from offline logs of data, to fully specify the hyperparameters for an RL agent that learns online in the real world. The approach is conceptually simple: we first learn a model of the environment from the offline data, which we call a calibration model, and then simulate learning in the calibration model to identify promising hyperparameters. We identify several criteria to make this strategy effective, and develop an approach that satisfies these criteria. We empirically investigate the method in a variety of settings to identify when it is effective and when it fails."}}
{"id": "A_3GWD5VS4", "cdate": 1640995200000, "mdate": 1664470464897, "content": {"title": "KL-Entropy-Regularized RL with a Generative Model is Minimax Optimal", "abstract": "In this work, we consider and analyze the sample complexity of model-free reinforcement learning with a generative model. Particularly, we analyze mirror descent value iteration (MDVI) by Geist et al. (2019) and Vieillard et al. (2020a), which uses the Kullback-Leibler divergence and entropy regularization in its value and policy updates. Our analysis shows that it is nearly minimax-optimal for finding an $\\varepsilon$-optimal policy when $\\varepsilon$ is sufficiently small. This is the first theoretical result that demonstrates that a simple model-free algorithm without variance-reduction can be nearly minimax-optimal under the considered setting."}}
{"id": "480OYBYWgJ", "cdate": 1640995200000, "mdate": 1681706664969, "content": {"title": "Adapting to game trees in zero-sum imperfect information games", "abstract": "Imperfect information games (IIG) are games in which each player only partially observes the current game state. We study how to learn $\\epsilon$-optimal strategies in a zero-sum IIG through self-play with trajectory feedback. We give a problem-independent lower bound $\\widetilde{\\mathcal{O}}(H(A_{\\mathcal{X}}+B_{\\mathcal{Y}})/\\epsilon^2)$ on the required number of realizations to learn these strategies with high probability, where $H$ is the length of the game, $A_{\\mathcal{X}}$ and $B_{\\mathcal{Y}}$ are the total number of actions for the two players. We also propose two Follow the Regularized leader (FTRL) algorithms for this setting: Balanced FTRL which matches this lower bound, but requires the knowledge of the information set structure beforehand to define the regularization; and Adaptive FTRL which needs $\\widetilde{\\mathcal{O}}(H^2(A_{\\mathcal{X}}+B_{\\mathcal{Y}})/\\epsilon^2)$ realizations without this requirement by progressively adapting the regularization to the observations."}}
{"id": "-q4Voo0jtW", "cdate": 1640995200000, "mdate": 1681706664973, "content": {"title": "Confident Approximate Policy Iteration for Efficient Local Planning in q\u03c0-realizable MDPs", "abstract": "We consider approximate dynamic programming in $\\gamma$-discounted Markov decision processes and apply it to approximate planning with linear value-function approximation. Our first contribution is a new variant of Approximate Policy Iteration (API), called Confident Approximate Policy Iteration (CAPI), which computes a deterministic stationary policy with an optimal error bound scaling linearly with the product of the effective horizon $H$ and the worst-case approximation error $\\epsilon$ of the action-value functions of stationary policies. This improvement over API (whose error scales with $H^2$) comes at the price of an $H$-fold increase in memory cost. Unlike Scherrer and Lesner [2012], who recommended computing a non-stationary policy to achieve a similar improvement (with the same memory overhead), we are able to stick to stationary policies. This allows for our second contribution, the application of CAPI to planning with local access to a simulator and $d$-dimensional linear function approximation. As such, we design a planning algorithm that applies CAPI to obtain a sequence of policies with successively refined accuracies on a dynamically evolving set of states. The algorithm outputs an $\\tilde O(\\sqrt{d}H\\epsilon)$-optimal policy after issuing $\\tilde O(dH^4/\\epsilon^2)$ queries to the simulator, simultaneously achieving the optimal accuracy bound and the best known query complexity bound, while earlier algorithms in the literature achieve only one of them. This query complexity is shown to be tight in all parameters except $H$. These improvements come at the expense of a mild (polynomial) increase in memory and computational costs of both the algorithm and its output policy."}}
