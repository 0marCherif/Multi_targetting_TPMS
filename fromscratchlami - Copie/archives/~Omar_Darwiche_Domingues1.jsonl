{"id": "vRSY3L4Rlhp", "cdate": 1623604622026, "mdate": null, "content": {"title": "Density-Based Bonuses on Learned Representations for Reward-Free Exploration in Deep Reinforcement Learning", "abstract": "In this paper, we study the problem of representation learning and exploration in reinforcement learning. We propose a framework to compute exploration bonuses based on density estimation, that can be used with any representation learning method, and that allows the agent to explore without extrinsic rewards. In the special case of tabular Markov decision processes (MDPs), this approach mimics the behavior of theoretically sound algorithms. In continuous and partially observable MDPs, the same approach can be applied by learning a latent representation, on which a probability density is estimated."}}
{"id": "KP4F0YWt7P3", "cdate": 1617701806369, "mdate": null, "content": {"title": "UCB momentum Q-learning: Correcting the bias without forgetting", "abstract": "We propose UCBMQ, Upper Confidence Bound Momentum Q-learning, a new algorithm for reinforcement learning in tabular and possibly stage-dependent, episodic Markov decision process. UCBMQ is based on Q-learning where we add a momentum term and rely on the principle of optimism in face of uncertainty to deal with exploration. Our new technical ingredient of UCBMQ is the use of momentum to correct the bias that Q-learning suffers while, \\emph{at the same time}, limiting the impact it has on the second-order term of the regret. For UCBMQ, we are able to guarantee a regret of at most $\\tilde{O}(\\sqrt{H^3SAT}+ H^4 S A)$ where $H$ is the length of an episode, $S$ the number of states, $A$ the number of actions, $T$ the number of episodes and ignoring terms in poly$\\log(SAHT)$. Notably, UCBMQ is the first algorithm that simultaneously matches the lower bound of $\\Omega(\\sqrt{H^3SAT})$ for large enough $T$ and has a second-order term (with respect to $T$) that scales \\emph{only linearly} with the number of states $S$."}}
{"id": "Hye4qSSl8S", "cdate": 1567802763564, "mdate": null, "content": {"title": "Planning in entropy-regularized Markov decision processes and games", "abstract": "We propose a new planning algorithm for estimating the value function in entropy-regularized Markov decision processes and two-player games, given a generative model of the environment. We make use of the smoothness of the Bellman operator introduced by the regularization to provide an algorithm which has a problem-independent sample complexity of order $\\mathcal{O}(1/\\epsilon^4)$ for a desired accuracy $\\epsilon$, whereas non-regularized problems may not enjoy polynomial sample complexity in a worst-case sense."}}
