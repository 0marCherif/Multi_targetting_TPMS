{"id": "KAIyxWrP9-", "cdate": 1652737690705, "mdate": null, "content": {"title": "A Differentially Private Linear-Time fPTAS for the Minimum Enclosing Ball Problem", "abstract": "The Minimum Enclosing Ball (MEB) problem is one of the most fundamental problems in clustering, with applications in operations research, statistic and computational geometry. In this works, we give the first differentially private (DP) fPTAS for the Minimum Enclosing Ball problem, improving both on the runtime and the utility bound of the best known DP-PTAS for the problem, of Ghazi et al (2020). Given $n$ points in $\\mathbb{R}^d$ that are covered by the ball $B(\\theta_{opt},r_{opt})$, our simple iterative DP-algorithm returns a ball $B(\\theta,r)$ where $r\\leq (1+\\gamma)r_{opt}$ and which leaves at most $\\tilde O(\\frac{\\sqrt d}{\\gamma\\epsilon})$ points uncovered in $\\tilde O(n/\\gamma^2)$-time. We also give a local-model version of our algorithm, that leaves at most  $\\tilde O(\\frac{\\sqrt {nd}}{\\gamma\\epsilon})$ points uncovered, improving on the $n^{0.67}$-bound of Nissim and Stemmer (2018) (at the expense of other parameters). In addition, we test our algorithm empirically and discuss future open problems."}}
{"id": "r1ELKnb_WH", "cdate": 1546300800000, "mdate": null, "content": {"title": "An Optimal Private Stochastic-MAB Algorithm based on Optimal Private Stopping Rule", "abstract": "We present a provably optimal differentially private algorithm for the stochastic multi-arm bandit problem, as opposed to the private analogue of the UCB-algorithm (Mishra and Thakurta, 2015; Tosso..."}}
{"id": "rk-Uk_ZdZH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Differentially Private Contextual Linear Bandits", "abstract": "We study the contextual linear bandit problem, a version of the standard stochastic multi-armed bandit (MAB) problem where a learner sequentially selects actions to maximize a reward which depends also on a user provided per-round context. Though the context is chosen arbitrarily or adversarially, the reward is assumed to be a stochastic function of a feature vector that encodes the context and selected action. Our goal is to devise private learners for the contextual linear bandit problem. We first show that using the standard definition of differential privacy results in linear regret. So instead, we adopt the notion of joint differential privacy, where we assume that the action chosen on day t is only revealed to user t and thus needn't be kept private that day, only on following days. We give a general scheme converting the classic linear-UCB algorithm into a joint differentially private algorithm using the tree-based algorithm. We then apply either Gaussian noise or Wishart noise to achieve joint-differentially private algorithms and bound the resulting algorithms' regrets. In addition, we give the first lower bound on the additional regret any private algorithms for the MAB problem must incur."}}
{"id": "S1ZfE2ZuWr", "cdate": 1514764800000, "mdate": null, "content": {"title": "Locally Private Hypothesis Testing", "abstract": "We initiate the study of differentially private hypothesis testing in the local-model, under both the standard (symmetric) randomized-response mechanism (Warner 1965, Kasiviswanathan et al, 2008) a..."}}
{"id": "H1W3rjbuWH", "cdate": 1483228800000, "mdate": null, "content": {"title": "Differentially Private Ordinary Least Squares", "abstract": "Linear regression is one of the most prevalent techniques in machine learning; however, it is also common to use linear regression for its explanatory capabilities rather than label prediction. Ord..."}}
{"id": "ryW82OZObH", "cdate": 1388534400000, "mdate": null, "content": {"title": "Learning Mixtures of Ranking Models", "abstract": "This work concerns learning probabilistic models for ranking data in a heterogeneous population. The specific problem we study is learning the parameters of a {\\em Mallows Mixture Model}. Despite being widely studied, current heuristics for this problem do not have theoretical guarantees and can get stuck in bad local optima. We present the first polynomial time algorithm which provably learns the parameters of a mixture of two Mallows models. A key component of our algorithm is a novel use of tensor decomposition techniques to learn the top-$k$ prefix in both the rankings. Before this work, even the question of {\\em identifiability} in the case of a mixture of two Mallows models was unresolved."}}
{"id": "BkZsMo-d-S", "cdate": 1325376000000, "mdate": null, "content": {"title": "Predicting Consumer Behavior in Commerce Search", "abstract": ""}}
