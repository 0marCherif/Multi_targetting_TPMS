{"id": "WVpu8Wp4jZ", "cdate": 1685982300054, "mdate": null, "content": {"title": "Submodular Reinforcement Learning", "abstract": "In reinforcement learning (RL), rewards of states are typically considered additive, and following the Markov assumption, they are $\\textit{independent}$ of states visited previously. In many important applications, such as coverage control, experiment design and informative path planning, rewards naturally have diminishing returns, i.e., their value decreases in light of similar states visited previously. To tackle this, we propose $\\textit{submodular RL}$ (subRL), a paradigm which seeks to optimize more general, non-additive (and history-dependent) rewards modelled via submodular set functions, which capture diminishing returns. Unfortunately, in general, even in tabular settings, we show that the resulting optimization problem is hard to approximate. On the other hand, motivated by the success of greedy algorithms in classical submodular optimization, we propose subPO, a simple policy gradient-based algorithm for subRL that handles non-additive rewards by greedily maximizing marginal gains. Indeed, under some assumptions on the underlying Markov Decision Process (MDP), subPO recovers optimal constant factor approximations of submodular bandits. Moreover, we derive a natural policy gradient approach for locally optimizing subRL instances even in large state- and action- spaces. We showcase the versatility of our approach by applying subPO to several applications, such as biodiversity monitoring, Bayesian experiment design, informative path planning, and coverage maximization. Our results demonstrate sample efficiency, as well as scalability to high-dimensional state-action spaces."}}
{"id": "_L7f0ySKMWY", "cdate": 1652737392899, "mdate": null, "content": {"title": "Near-Optimal Multi-Agent Learning for Safe Coverage Control", "abstract": "In multi-agent coverage control problems, agents navigate their environment to reach locations that maximize the coverage of some density. In practice, the density is rarely known $\\textit{a priori}$, further complicating the original NP-hard problem. Moreover, in many applications, agents cannot visit arbitrary locations due to $\\textit{a priori}$ unknown safety constraints. In this paper, we aim to efficiently learn the density to approximately solve the coverage problem while preserving the agents' safety. We first propose a conditionally linear submodular coverage function that facilitates theoretical analysis. Utilizing this structure, we develop MacOpt, a novel algorithm that efficiently trades off the exploration-exploitation dilemma due to partial observability, and show that it achieves sublinear regret. Next, we extend results on single-agent safe exploration to our multi-agent setting and propose SafeMac for safe coverage and exploration. We analyze SafeMac and give first of its kind results: near optimal coverage in finite time while provably guaranteeing safety. We extensively evaluate our algorithms on synthetic and real problems, including a bio-diversity monitoring task under safety constraints, where SafeMac outperforms competing methods."}}
{"id": "YwywBEvMnX", "cdate": 1640995200000, "mdate": 1681117798836, "content": {"title": "Near-Optimal Multi-Agent Learning for Safe Coverage Control", "abstract": ""}}
{"id": "D9m3xwqKxT", "cdate": 1609459200000, "mdate": 1667374513626, "content": {"title": "Competitive policy optimization", "abstract": "A core challenge in policy optimization in competitive Markov decision processes is the design of efficient optimization methods with desirable convergence and stability properties. We propose comp..."}}
{"id": "0aZ4x2oW24", "cdate": 1577836800000, "mdate": 1667374513717, "content": {"title": "AMZ Driverless: The full autonomous racing system", "abstract": ""}}
{"id": "tmFewTbbet", "cdate": 1546300800000, "mdate": 1681117798842, "content": {"title": "Redundant Perception and State Estimation for Reliable Autonomous Racing", "abstract": ""}}
