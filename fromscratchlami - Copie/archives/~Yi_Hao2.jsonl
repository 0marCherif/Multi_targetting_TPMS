{"id": "URL1IhZvTfL", "cdate": 1640995200000, "mdate": 1672474965024, "content": {"title": "TURF: Two-Factor, Universal, Robust, Fast Distribution Learning Algorithm", "abstract": ""}}
{"id": "FWQXkwt9Au7", "cdate": 1609459200000, "mdate": 1670699797298, "content": {"title": "Compressed Maximum Likelihood", "abstract": "Maximum likelihood (ML) is one of the most fundamental and general statistical estimation techniques. Inspired by recent advances in estimating distribution functionals, we propose $\\textit{compres..."}}
{"id": "2ZyX8ZiGRzw", "cdate": 1609459200000, "mdate": 1670699797076, "content": {"title": "Competitive and Universal Learning", "abstract": "Author(s): Hao, Yi | Advisor(s): Orlitsky, Alon | Abstract: Modern data science calls for statistical inference algorithms that are both data-efficient and computation-efficient. We design and analyze methods that 1) outperform existing algorithms in the high-dimensional setting; 2) are as simple as possible but efficiently computable. One line of our work aims to bridge different fields and offer unified solutions to multiple questions. Consider the following canonical statistical inference tasks: distribution estimation, functional estimation, and property testing, sharing the model that provides sample access to an unknown discrete distribution. In a recent paper in NeurIPS '19, we showed that a single, simple, and unified estimator \u2013 profile maximum likelihood (PML), and its near-linear time computable variant are sample-optimal for estimating multiple distribution attributes. The result covers 1) any appropriately Lipschitz and additively separable functionals; 2) sorted distribution; 3) R<metaTags></metaTags>#x27;enyi entropy; 4) $\\ell_2$ distance to the uniform distribution, yielding an optimal tester for distributions' closeness. This work makes PML the first unified sample- and time-optimal method for the learning tasks mentioned above. A single algorithm with such broad applicability is \\emph{universal}. Another line of our work focuses on instance-optimal learning that designs algorithms with near-optimal guarantees for every possible data input. A flagship problem is distribution estimation over discrete or continuous domains, where ordering and geometry play an essential role. Going beyond worst-case guarantees, researchers designed algorithms that compete with a genie estimator that knows the actual distribution but is reasonably restricted. To obtain state-of-the-art algorithms for both tasks, we leveraged the simple but nontrivial idea of ``data binning''. For discrete settings, we group symbols that appear the same number of times. And for continuous settings, we partition the real domain and separate symbols according to pre-designed local quantiles. The respective algorithms run in near-linear-time, achieve the best-known estimation guarantees regarding the genie estimators, and appear in ICML'19 and NeurIPS '20. A genie-like algorithm adaptive to almost every data sample is \\emph{competitive}.We present a comprehensive understanding of universal and competitive algorithms for multiple fundamental learning problems. Our ideas and techniques may shed light on key challenges in modern data science and numerous applications beyond the scope of this dissertation."}}
{"id": "aiBlcE8Ou8z", "cdate": 1599353242933, "mdate": null, "content": {"title": "Bessel Smoothing and Multi-Distribution Property Estimation", "abstract": "We consider a basic problem in statistical learning: estimating properties of multiple discrete distributions. Denoting by $\\Delta_k$ the standard simplex over $[k]:=\\{0,1,\\ldots, k\\}$, a property of $d$ distributions is a mapping from $\\Delta_k^d$ to $\\mathbb R$. These properties include well-known distribution characteristics such as Shannon entropy and support size  ($d=1$), and many important divergence measures between distributions ($d=2$). The primary problem being considered is to learn the property value of an unknown $d$-tuple of distributions from its sample. The study of such problems dates back to the works of Efron and Thisted (1976b); Thisted and Efron (1987); Good (1953b); Carlton (1969), and has been pushed forward steadily during the past decades. Surprisingly, before our work, the general landscape of this fundamental learning problem was insufficiently understood, and nearly all the existing results are for the special case $d\\le 2$. \n\nOur first main result provides a near-linear-time computable algorithm that, given independent samples from any collection of distributions and for a broad class of multi-distribution properties, learns the property as well as the empirical plug-in estimator that uses samples with logarithmic-factor larger sizes. As a corollary of this, for any $\\varepsilon>0$ and fixed $d\\in \\mathbb Z^+$, a $d$-distribution property over $[k]$ that is Lipschitz and additively separable can be learned to an accuracy of $\\varepsilon$ using a sample of size $\\mathcal{O}(k/(\\varepsilon^3\\sqrt{\\log k}))$, with high probability. Our second result addresses a closely related problem-- tolerant independence testing: One receives samples from the unknown joint and marginal distributions, and attempts to infer the $\\ell_1$ distance between the joint distribution and the product distribution of the marginals. We show that this testing problem also admits a sample complexity sub-linear in the alphabet sizes, demonstrating the broad applicability of our approach."}}
{"id": "qi7sMTcrXld", "cdate": 1599352625908, "mdate": null, "content": {"title": "Data Amplification: Instance-Optimal Property Estimation", "abstract": "The best-known and most commonly used technique for distribution-property estimation uses a plug-in estimator, with empirical frequency replacing the underlying distribution. We present novel linear-time-computable estimators that significantly \u201camplify\u201d the effective amount of data available. For a large variety of distribution properties including four of the most popular ones and for every underlying distribution, they achieve the accuracy that the empirical-frequency plug-in estimators would attain using a logarithmic-factor more samples. Specifically, for Shannon entropy and a broad class of Lipschitz properties including the $L_1$ distance to a fixed distribution, the new estimators use $n$ samples to achieve the accuracy attained by the empirical estimators with $n \\log n$ samples. For support-size and coverage, the new estimators use $n$ samples to achieve the performance of empirical frequency with sample size $n$ times the logarithm of the property value. Significantly strengthening the traditional min-max formulation, these results hold not only for the worst distributions, but for each and every underlying distribution. Furthermore, the logarithmic amplification factors are optimal. Experiments on a wide variety of distributions show that the new estimators outperform the previous state-of-the-art estimators designed for each specific property."}}
{"id": "u-ELYzyfRdc", "cdate": 1577836800000, "mdate": null, "content": {"title": "SURF: A Simple, Universal, Robust, Fast Distribution Learning Algorithm", "abstract": "Sample- and computationally-efficient distribution estimation is a fundamental tenet in statistics and machine learning. We present $\\SURF$, an algorithm for approximating distributions by piecewise polynomials. $\\SURF$ is: simple, replacing prior complex optimization techniques by straight-forward empirical probability approximation of each potential polynomial piece through simple empirical-probability interpolation, and using plain divide-and-conquer to merge the pieces; universal, as well-known polynomial-approximation results imply that it accurately approximates a large class of common distributions; robust to distribution mis-specification as for any degree $d \\le 8$, it estimates any distribution to an $\\ell_1$ distance $&lt; 3$ times that of the nearest degree-$d$ piecewise polynomial, improving known factor upper bounds of 3 for single polynomials and 15 for polynomials with arbitrarily many pieces; fast, using optimal sample complexity, running in near sample-linear time, and if given sorted samples it may be parallelized to run in sub-linear time. In experiments, $\\SURF$ outperforms state-of-the art algorithms."}}
{"id": "gcC9ABNR5Sw", "cdate": 1577836800000, "mdate": null, "content": {"title": "SURF: A Simple, Universal, Robust, Fast Distribution Learning Algorithm", "abstract": "Sample- and computationally-efficient distribution estimation is a fundamental tenet in statistics and machine learning. We present SURF, an algorithm for approximating distributions by piecewise polynomials. SURF is: simple, replacing prior complex optimization techniques by straight-forward {empirical probability} approximation of each potential polynomial piece {through simple empirical-probability interpolation}, and using plain divide-and-conquer to merge the pieces; universal, as well-known polynomial-approximation results imply that it accurately approximates a large class of common distributions; robust to distribution mis-specification as for any degree $d \\le 8$, it estimates any distribution to an $\\ell_1$ distance $< 3$ times that of the nearest degree-$d$ piecewise polynomial, improving known factor upper bounds of 3 for single polynomials and 15 for polynomials with arbitrarily many pieces; fast, using optimal sample complexity, running in near sample-linear time, and if given sorted samples it may be parallelized to run in sub-linear time. In experiments, SURF outperforms state-of-the art algorithms."}}
{"id": "gYHRmLkkXJy", "cdate": 1577836800000, "mdate": null, "content": {"title": "Profile Entropy: A Fundamental Measure for the Learnability and Compressibility of Distributions", "abstract": "The profile of a sample is the multiset of its symbol frequencies. We show that for samples of discrete distributions, profile entropy is a fundamental measure unifying the concepts of estimation, inference, and compression. Specifically, profile entropy: a) determines the speed of estimating the distribution relative to the best natural estimator; b) characterizes the rate of inferring all symmetric properties compared with the best estimator over any label-invariant distribution collection; c) serves as the limit of profile compression, for which we derive optimal near-linear-time block and sequential algorithms. To further our understanding of profile entropy, we investigate its attributes, provide algorithms for approximating its value, and determine its magnitude for numerous structural distribution families."}}
{"id": "_Z5Jrbo5im_", "cdate": 1577836800000, "mdate": null, "content": {"title": "Profile Entropy: A Fundamental Measure for the Learnability and Compressibility of Discrete Distributions", "abstract": "The profile of a sample is the multiset of its symbol frequencies. We show that for samples of discrete distributions, profile entropy is a fundamental measure unifying the concepts of estimation, inference, and compression. Specifically, profile entropy a) determines the speed of estimating the distribution relative to the best natural estimator; b) characterizes the rate of inferring all symmetric properties compared with the best estimator over any label-invariant distribution collection; c) serves as the limit of profile compression, for which we derive optimal near-linear-time block and sequential algorithms. To further our understanding of profile entropy, we investigate its attributes, provide algorithms for approximating its value, and determine its magnitude for numerous structural distribution families."}}
{"id": "UQZVdaLUEZd", "cdate": 1577836800000, "mdate": null, "content": {"title": "Unsupervised Embedding of Hierarchical Structure in Euclidean Space", "abstract": "Deep embedding methods have influenced many areas of unsupervised learning. However, the best methods for learning hierarchical structure use non-Euclidean representations, whereas Euclidean geometry underlies the theory behind many hierarchical clustering algorithms. To bridge the gap between these two areas, we consider learning a non-linear embedding of data into Euclidean space as a way to improve the hierarchical clustering produced by agglomerative algorithms. To learn the embedding, we revisit using a variational autoencoder with a Gaussian mixture prior, and we show that rescaling the latent space embedding and then applying Ward's linkage-based algorithm leads to improved results for both dendrogram purity and the Moseley-Wang cost function. Finally, we complement our empirical results with a theoretical explanation of the success of this approach. We study a synthetic model of the embedded vectors and prove that Ward's method exactly recovers the planted hierarchical clustering with high probability."}}
