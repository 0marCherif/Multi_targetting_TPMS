{"id": "05g7mnKJyJ7", "cdate": 1683910859733, "mdate": 1683910859733, "content": {"title": "Is Local SGD Better than Minibatch SGD?", "abstract": "We study local SGD (also known as parallel SGD and federated averaging), a natural and frequently\nused stochastic distributed optimization method. Its theoretical foundations are currently lacking and\nwe highlight how all existing error guarantees in the convex setting are dominated by a simple baseline,\nminibatch SGD. (1) For quadratic objectives we prove that local SGD strictly dominates minibatch SGD\nand that accelerated local SGD is minimax optimal for quadratics; (2) For general convex objectives we\nprovide the first guarantee that at least sometimes improves over minibatch SGD; (3) We show that\nindeed local SGD does not dominate minibatch SGD by presenting a lower bound on the performance\nof local SGD that is worse than the minibatch SGD guarantee."}}
{"id": "CGH0ie-Rsec", "cdate": 1673405766392, "mdate": 1673405766392, "content": {"title": "Rank-constrained Hyperbolic Programming", "abstract": "We extend rank-constrained optimization to general hyperbolic programs (HP) using the notion of matroid rank. For LP and SDP respectively, this reduces to sparsity-constrained LP and rank-constrained SDP that are already well-studied. But for QCQP and SOCP, we obtain new interesting optimization problems. For example, rank-constrained SOCP includes weighted Max-Cut and nonconvex QP as special cases, and dropping the rank constraints yield the standard SOCP-relaxations of these problems. We will show (i) how to do rank reduction for SOCP and QCQP, (ii) that rank-constrained SOCP and rank-constrained QCQP are NP-hard, and (iii) an improved result for rank-constrained SDP showing that if the number of constraints is m and the rank constraint is less than 21/2\u2212\u03f5m\u203e\u203e\u221a for some \u03f5>0, then the problem is NP-hard. We will also study sparsity-constrained HP and extend results on LP sparsification to SOCP and QCQP. In particular, we show that there always exist (a) a solution to SOCP of cardinality at most twice the number of constraints and (b) a solution to QCQP of cardinality at most the sum of the number of linear constraints and the sum of the rank of the matrices in the quadratic constraints; and both (a) and (b) can be found efficiently."}}
{"id": "FpTEAng_TFH", "cdate": 1673405697265, "mdate": 1673405697265, "content": {"title": "Inverting a complex matrix", "abstract": "We analyze a complex matrix inversion algorithm proposed by Frobenius, which we call the Frobenius inversion. We show that the Frobenius inversion uses the least number of real matrix multiplications and inversions among all complex matrix inversion algorithms. We also analyze numerical properties of the Frobenius inversion. We prove that the Frobenius inversion runs faster than the widely used method based on LU decomposition if and only if the ratio of the running time of the real matrix inversion to that of the real matrix multiplication is greater than 5/4. We corroborate this theoretical result by numerical experiments. Moreover, we apply the Frobenius inversion to matrix sign function, Sylvester equation, and polar decomposition. In each of these examples, the Frobenius inversion is more efficient than inversion via LU-decomposition."}}
{"id": "moEDP0C75k", "cdate": 1673405555343, "mdate": 1673405555343, "content": {"title": "Numerical stability and tensor nuclear norm", "abstract": "We present a notion of bilinear stability, which is to numerical stability what bilinear complexity is to time complexity. In bilinear complexity, an algorithm for evaluating a bilinear operator \u03b2:\ud835\udd4c\u00d7\ud835\udd4d\u2192\ud835\udd4e is a decomposition \u03b2=\u03c61\u2297\u03c81\u2297w1+\u22ef+\u03c6r\u2297\u03c8r\u2297wr; the number of terms r captures the speed of the algorithm; and its smallest possible value, i.e., the tensor rank of \u03b2, quantifies the speed of a fastest algorithm. Bilinear stability introduces norms to the mix: The growth factor of the algorithm \u2016\u03c61\u2016\u2217\u2016\u03c81\u2016\u2217\u2016w1\u2016+\u22ef+\u2016\u03c6r\u2016\u2217\u2016\u03c8r\u2016\u2217\u2016wr\u2016 captures the accuracy of the algorithm; and its smallest possible value, i.e., the tensor nuclear norm of \u03b2, quantifies the accuracy of a stablest algorithm. To substantiate this notion, we establish a bound for the forward error in terms of the growth factor and present numerical evidence comparing various fast algorithms for matrix and complex multiplications, showing that larger growth factors correlate with less accurate results. Compared to similar studies of numerical stability, bilinear stability is more general, applying to any bilinear operators and not just matrix or complex multiplications; is more simplistic, bounding forward error in terms of a single (growth) factor; and is truly tensorial like bilinear complexity, invariant under any orthogonal change of coordinates. As an aside, we study a new algorithm for computing complex multiplication in terms of real, much like Gauss's, but is optimally fast and stable in that it attains both tensor rank and nuclear norm."}}
{"id": "Hblqhv4fQ5", "cdate": 1648670641856, "mdate": 1648670641856, "content": {"title": "Fair Representation Clustering with Several Protected Classes", "abstract": "We study the problem of fair $k$-median where each cluster is required to have a fair representation of individuals from different groups. In the fair representation $k$-median problem, we are given a set of points $X$ in a metric space. Each point $x\\in X$ belongs to one of $\\ell$ groups. Further, we are given fair representation parameters $\\alpha_j$ and $\\beta_j$ for each group $j\\in [\\ell]$. We say that a $k$-clustering $C_1, \\cdots, C_k$ fairly represents all groups if the number of points from group $j$ in cluster $C_i$ is between $\\alpha_j |C_i|$ and $\\beta_j |C_i|$ for every $j\\in[\\ell]$ and $i\\in [k]$. The goal is to find a set $\\sC$ of $k$ centers and an assignment $\\phi: X\\rightarrow \\sC$ such that the clustering defined by $(\\sC, \\phi)$ fairly represents all groups and minimizes the $\\ell_1$-objective $\\sum_{x\\in X} d(x, \\phi(x))$.\n\nWe present an $O(\\log k)$-approximation algorithm that runs in time $n^{O(\\ell)}$. Note that the known algorithms for the problem either (i) violate the fairness constraints by an additive term or (ii) run in time that is exponential in both $k$ and $\\ell$. We also consider an important special case of the problem where $\\alpha_j = \\beta_j = \\frac{f_j}{f}$ and $f_j, f \\in \\mathbb{N}$ for all $j\\in [\\ell]$. For this special case, we present an $O(\\log k)$-approximation algorithm that runs in $(kf)^{O(\\ell)}\\log n + \\poly(n)$ time."}}
{"id": "pSNs0PKx0Mw", "cdate": 1621630080060, "mdate": null, "content": {"title": "Representation Costs of Linear Neural Networks: Analysis and Design", "abstract": "For different parameterizations (mappings from parameters to predictors), we study the regularization cost in predictor space induced by $l_2$ regularization on the parameters (weights).  We focus on linear neural networks as parameterizations of linear predictors.  We identify the representation cost of certain sparse linear ConvNets and residual networks.  In order to get a better understanding of how the architecture and parameterization affect the representation cost, we also study the reverse problem, identifying which regularizers on linear predictors (e.g., $l_p$ norms, group norms, the $k$-support-norm, elastic net) can be the representation cost induced by simple $l_2$ regularization, and designing the parameterizations that do so."}}
{"id": "3oQyjABdbC8", "cdate": 1621630080060, "mdate": null, "content": {"title": "Representation Costs of Linear Neural Networks: Analysis and Design", "abstract": "For different parameterizations (mappings from parameters to predictors), we study the regularization cost in predictor space induced by $l_2$ regularization on the parameters (weights).  We focus on linear neural networks as parameterizations of linear predictors.  We identify the representation cost of certain sparse linear ConvNets and residual networks.  In order to get a better understanding of how the architecture and parameterization affect the representation cost, we also study the reverse problem, identifying which regularizers on linear predictors (e.g., $l_p$ norms, group norms, the $k$-support-norm, elastic net) can be the representation cost induced by simple $l_2$ regularization, and designing the parameterizations that do so."}}
