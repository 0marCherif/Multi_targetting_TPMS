{"id": "Us6BtPZGei3", "cdate": 1676591080968, "mdate": null, "content": {"title": "Learning to Modulate pre-trained Models in RL", "abstract": "Reinforcement Learning (RL) has experienced great success in complex games and simulations. However, RL agents are often highly specialized for a particular task, and it is difficult to adapt a trained agent to a new task. In supervised learning, an established paradigm is multi-task pre-training followed by fine-tuning. A similar trend is emerging in RL, where agents are pre-trained on data collections that comprise a multitude of tasks. Despite these developments, it remains an open challenge how to adapt such pre-trained agents to novel tasks while retaining performance on the pre-training tasks. In this regard, we pre-train an agent on a set of tasks from the Meta-World benchmark suite and adapt it to tasks from Continual-World. We conduct a comprehensive comparison of fine-tuning methods originating from supervised learning in our setup. Our findings show that fine-tuning is feasible, but for existing methods, performance on previously learned tasks often deteriorates. Therefore, we propose a novel approach that avoids forgetting by modulating the information flow of the pre-trained model. Our method outperforms existing fine-tuning approaches, and achieves state-of-the-art performance on the Continual-World benchmark. To facilitate future research in this direction, we collect datasets for all Meta-World tasks and make them publicly available."}}
{"id": "rEZ1Hxd51R", "cdate": 1665251231826, "mdate": null, "content": {"title": "Foundation Models for History Compression in Reinforcement Learning", "abstract": "Agents interacting under partial observability require access to past observations via a memory mechanism in order to approximate the true state of the environment.\nRecent work suggests that leveraging language as abstraction provides benefits for creating a representation of past events.\nHistory Compression via Language Models (HELM) leverages a pretrained Language Model (LM) for representing the past. \nIt relies on a randomized attention mechanism to translate environment observations to token embeddings.\nIn this work, we show that the representations resulting from this attention mechanism can collapse under certain conditions. \nThis causes blindness of the agent to subtle changes in the environment that may be crucial for solving a certain task. \nWe propose a solution to this problem consisting of two parts. \nFirst, we improve upon HELM by substituting the attention mechanism with a feature-wise centering-and-scaling operation. \nSecond, we take a step toward semantic history compression by leveraging foundation models, such as CLIP, to encode observations, which further improves performance. \nBy combining foundation models, our agent is able to solve the challenging MiniGrid-Memory environment.\nSurprisingly, however, our experiments suggest that this is not due to the semantic enrichment of the representation presented to the LM, but rather due to the discriminative power provided by CLIP. \nWe make our code publicly available at https://github.com/ml-jku/helm."}}
{"id": "KmxQ3v7nMmd", "cdate": 1664943347077, "mdate": null, "content": {"title": "Foundation Models for History Compression in Reinforcement Learning", "abstract": "Agents interacting under partial observability require access to past observations via a memory mechanism in order to approximate the true state of the environment.\nRecent work suggests that leveraging language as abstraction provides benefits for creating a representation of past events.\nHistory Compression via Language Models (HELM) leverages a pretrained Language Model (LM) for representing the past. \nIt relies on a randomized attention mechanism to translate environment observations to token embeddings.\nIn this work, we show that the representations resulting from this attention mechanism can collapse under certain conditions. \nThis causes blindness of the agent to subtle changes in the environment that may be crucial for solving a certain task. \nWe propose a solution to this problem consisting of two parts. \nFirst, we improve upon HELM by substituting the attention mechanism with a feature-wise centering-and-scaling operation. \nSecond, we take a step toward semantic history compression by leveraging foundation models, such as CLIP, to encode observations, which further improves performance. \nBy combining foundation models, our agent is able to solve the challenging MiniGrid-Memory environment.\nSurprisingly, however, our experiments suggest that this is not due to the semantic enrichment of the representation presented to the LM, but rather due to the discriminative power provided by CLIP. \nWe make our code publicly available at https://github.com/ml-jku/helm."}}
{"id": "97C6klf5shp", "cdate": 1664358386713, "mdate": null, "content": {"title": "Toward Semantic History Compression for Reinforcement Learning", "abstract": "Agents interacting under partial observability require access to past observations via a memory mechanism in order to approximate the true state of the environment.\nRecent work suggests that leveraging language as abstraction provides benefits for creating a representation of past events.\nHistory Compression via Language Models (HELM) leverages a pretrained Language Model (LM) for representing the past. \nIt relies on a randomized attention mechanism to translate environment observations to token embeddings.\nIn this work, we show that the representations resulting from this attention mechanism can collapse under certain conditions. \nThis causes blindness of the agent to subtle changes in the environment that may be crucial for solving a certain task. \nWe propose a solution to this problem consisting of two parts. \nFirst, we improve upon HELM by substituting the attention mechanism with a feature-wise centering-and-scaling operation. \nSecond, we take a step toward semantic history compression by leveraging foundation models, such as CLIP, to encode observations, which further improves performance. \nBy combining foundation models, our agent is able to solve the challenging MiniGrid-Memory environment.\nSurprisingly, however, our experiments suggest that this is not due to the semantic enrichment of the representation presented to the LM, but rather due to the discriminative power provided by CLIP. \nWe make our code publicly available at https://github.com/ml-jku/helm."}}
{"id": "r2S32AZyIu", "cdate": 1640995200000, "mdate": 1683641419190, "content": {"title": "Align-RUDDER: Learning From Few Demonstrations by Reward Redistribution", "abstract": "Reinforcement learning algorithms require many samples when solving complex hierarchical tasks with sparse and delayed rewards. For such complex tasks, the recently proposed RUDDER uses reward redi..."}}
{"id": "5RFwttmtOQA", "cdate": 1640995200000, "mdate": 1682320258904, "content": {"title": "A Dataset Perspective on Offline Reinforcement Learning", "abstract": "The application of Reinforcement Learning (RL) in real world environments can be expensive or risky due to sub-optimal policies during training. In Offline RL, this problem is avoided since interac..."}}
{"id": "t0PQSDcqAiy", "cdate": 1634067442486, "mdate": null, "content": {"title": "Modern Hopfield Networks for Return Decomposition for Delayed Rewards", "abstract": "Delayed rewards, which are separated from their causative actions by irrelevant actions, hamper learning in reinforcement learning (RL). Especially real world problems often contain such delayed and sparse rewards. Recently, return decomposition for delayed rewards (RUDDER) employed pattern recognition to remove or reduce delay in rewards, which dramatically simplifies the learning task of the underlying RL method. RUDDER was realized using a long short-term memory (LSTM). The LSTM was trained to identify important state-action pair patterns, responsible for the return. Reward was then redistributed to these important state-action pairs. However, training the LSTM is often difficult and requires a large number of episodes. In this work, we replace the LSTM with the recently proposed continuous modern Hopfield networks (MHN) and introduce Hopfield-RUDDER. MHN are powerful trainable associative memories with large storage capacity. They require only few training samples and excel at identifying and recognizing patterns. We use this property of MHN to identify important state-action pairs that are associated with low or high return episodes and directly redistribute reward to them. However, in partially observable environments, Hopfield-RUDDER requires additional information about the history of state-action pairs. Therefore, we evaluate several methods for compressing history and introduce reset-max history, a lightweight history compression using the max-operator in combination with a reset gate. We experimentally show that Hopfield-RUDDER is able to outperform LSTM-based RUDDER on various 1D environments with small numbers of episodes. Finally, we show in preliminary experiments that Hopfield-RUDDER scales to highly complex environments with the Minecraft ObtainDiamond task from the MineRL NeurIPS challenge."}}
{"id": "A4EWtf-TO3Y", "cdate": 1634067442232, "mdate": null, "content": {"title": "Understanding the Effects of Dataset Characteristics on Offline Reinforcement Learning", "abstract": "In real world, affecting the environment by a weak policy can be expensive or very risky, therefore hampers real world applications of reinforcement learning. Offline Reinforcement Learning (RL) can learn policies from a given dataset without interacting with the environment. However, the dataset is the only source of information for an Offline RL algorithm and determines the performance of the learned policy. We still lack studies on how dataset characteristics influence different Offline RL algorithms. Therefore, we conducted a comprehensive empirical analysis of how dataset characteristics effect the performance of Offline RL algorithms for discrete action environments. A dataset is characterized by two metrics: (1) the Trajectory Quality (TQ) measured by the average dataset return and (2) the State-Action Coverage (SACo) measured by the number of unique state-action pairs. We found that variants of the off-policy Deep Q-Network family require datasets with high SACo to perform well. Algorithms that constrain the learned policy towards the given dataset perform well for datasets with high TQ or SACo. For datasets with high TQ, Behavior Cloning outperforms or performs similarly to the best Offline RL algorithms."}}
{"id": "AlPBx2zq7Jt", "cdate": 1632875512081, "mdate": null, "content": {"title": "Align-RUDDER: Learning From Few Demonstrations by Reward Redistribution", "abstract": "Reinforcement Learning algorithms require a large number of samples to solve complex tasks with sparse and delayed rewards. Complex tasks are often hierarchically composed of sub-tasks. Solving a sub-task increases the return expectation and leads to a step in the $Q$-function. RUDDER identifies these steps and then redistributes reward to them, thus immediately giving reward if sub-tasks are solved. Since the delay of rewards is reduced, learning is considerably sped up. However, for complex tasks, current exploration strategies struggle with discovering episodes with high rewards. Therefore, we assume that episodes with high rewards are given as demonstrations and do not have to be discovered by exploration. Unfortunately, the number of demonstrations is typically small and RUDDER's LSTM as a deep learning model does not learn well on these few training samples. Hence, we introduce Align-RUDDER, which is RUDDER with two major modifications. First, Align-RUDDER assumes that episodes with high rewards are given as demonstrations, replacing RUDDER\u2019s safe exploration and lessons replay buffer. Second, we substitute RUDDER\u2019s LSTM model by a profile model that is obtained from multiple sequence alignment of demonstrations. Profile models can be constructed from as few as two demonstrations. Align-RUDDER uses reward redistribution to speed up learning by reducing the delay of rewards. Align-RUDDER outperforms competitors on complex artificial tasks with delayed rewards and few demonstrations. On the MineCraft ObtainDiamond task, Align-RUDDER is able to mine a diamond, though not frequently. "}}
{"id": "TRqBY1MUU1", "cdate": 1609459200000, "mdate": 1683641419185, "content": {"title": "Understanding the Effects of Dataset Characteristics on Offline Reinforcement Learning", "abstract": "The application of Reinforcement Learning (RL) in real world environments can be expensive or risky due to sub-optimal policies during training. In Offline RL, this problem is avoided since interactions with an environment are prohibited. Policies are learned from a given dataset, which solely determines their performance. Despite this fact, how dataset characteristics influence Offline RL algorithms is still hardly investigated. The dataset characteristics are determined by the behavioral policy that samples this dataset. Therefore, we define characteristics of behavioral policies as exploratory for yielding high expected information in their interaction with the Markov Decision Process (MDP) and as exploitative for having high expected return. We implement two corresponding empirical measures for the datasets sampled by the behavioral policy in deterministic MDPs. The first empirical measure SACo is defined by the normalized unique state-action pairs and captures exploration. The second empirical measure TQ is defined by the normalized average trajectory return and captures exploitation. Empirical evaluations show the effectiveness of TQ and SACo. In large-scale experiments using our proposed measures, we show that the unconstrained off-policy Deep Q-Network family requires datasets with high SACo to find a good policy. Furthermore, experiments show that policy constraint algorithms perform well on datasets with high TQ and SACo. Finally, the experiments show, that purely dataset-constrained Behavioral Cloning performs competitively to the best Offline RL algorithms for datasets with high TQ."}}
