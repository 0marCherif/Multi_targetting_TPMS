{"id": "2derKAVsXH", "cdate": 1698871148238, "mdate": 1698871148238, "content": {"title": "Deep Medial Fields", "abstract": "Implicit representations of geometry, such as occupancy fields or signed\ndistance fields (SDF), have recently re-gained popularity in encoding 3D\nsolid shape in a functional form. In this work, we introduce medial fields:\na field function derived from the medial axis transform (MAT) that makes\navailable information about the underlying 3D geometry that is immediately\nuseful for a number of downstream tasks. In particular, the medial field\nencodes the local thickness of a 3D shape, and enables O(1) projection of a\nquery point onto the medial axis. To construct the medial field we require\nnothing but the SDF of the shape itself, thus allowing its straightforward\nincorporation in any application that relies on signed distance fields. Working in unison with the O(1) surface projection supported by the SDF, the\nmedial field opens the door for an entirely new set of efficient, shape-aware\noperations on implicit representations. We present three such applications,\nincluding a modification to sphere tracing that renders implicit representations with better convergence properties, a fast construction method for\nmemory-efficient rigid-body collision proxies, and an efficient approximation of ambient occlusion that remains stable with respect to viewpoint\nvariations."}}
{"id": "AzQy8kwxlSg", "cdate": 1698709739735, "mdate": 1698709739735, "content": {"title": "Pointersect: Neural Rendering with Cloud-Ray Intersection", "abstract": "We propose a novel method that renders point clouds as if they are surfaces. The proposed method is differentiable and requires no scene-specific optimization. This unique capabil- ity enables, out-of-the-box, surface normal estimation, ren- dering room-scale point clouds, inverse rendering, and ray tracing with global illumination. Unlike existing work that focuses on converting point clouds to other representations\u2014 e.g., surfaces or implicit functions\u2014our key idea is to directly infer the intersection of a light ray with the underlying sur- face represented by the given point cloud. Specifically, we train a set transformer that, given a small number of local neighbor points along a light ray, provides the intersection point, the surface normal, and the material blending weights, which are used to render the outcome of this light ray. Lo- calizing the problem into small neighborhoods enables us to train a model with only 48 meshes and apply it to un- seen point clouds. Our model achieves higher estimation accuracy than state-of-the-art surface reconstruction and point-cloud rendering methods on three test sets. When ap- plied to room-scale point clouds, without any scene-specific optimization, the model achieves competitive quality with the state-of-the-art novel-view rendering methods. Moreover, we demonstrate ability to render and manipulate Lidar-scanned point clouds such as lighting control and object insertion."}}
{"id": "Gu7XtUSMoc", "cdate": 1695440963366, "mdate": 1695440963366, "content": {"title": "Kubric: A scalable dataset generator", "abstract": "Data is the driving force of machine learning, with the amount and quality of training data often being more important for the performance of a system than architecture and training details. But collecting, processing and annotating real data at scale is difficult, expensive, and frequently raises additional privacy, fairness and legal concerns. Synthetic data is a powerful tool with the potential to address these shortcomings: 1) it is cheap 2) supports rich ground-truth annotations 3) offers full control over data and 4) can circumvent or mitigate problems regarding bias, privacy and licensing. Unfortunately, software tools for effective data generation are less mature than those for architecture design and training, which leads to fragmented generation efforts. To address these problems we introduce Kubric, an open-source Python framework that interfaces with PyBullet and Blender to generate photo-realistic scenes, with rich annotations, and seamlessly scales to large jobs distributed over thousands of machines, and generating TBs of data. We demonstrate the effectiveness of Kubric by presenting a series of 11 different generated datasets for tasks ranging from studying 3D NeRF models to optical flow estimation. We release Kubric, the used assets, all of the generation code, as well as the rendered datasets for reuse and modification"}}
{"id": "VOPiHQUevh5", "cdate": 1652737311393, "mdate": null, "content": {"title": "TUSK: Task-Agnostic Unsupervised Keypoints", "abstract": "Existing unsupervised methods for keypoint learning rely heavily on the assumption that a specific keypoint type (e.g. elbow, digit, abstract geometric shape) appears only once in an image. This greatly limits their applicability, as each instance must be isolated before applying the method\u2014an issue that is never discussed or evaluated. We thus propose a novel method to learn Task-agnostic, UnSupervised Keypoints (TUSK) which can deal with multiple instances. To achieve this, instead of the commonly-used strategy of detecting multiple heatmaps, each dedicated to a specific keypoint type, we use a single heatmap for detection, and enable unsupervised learning of keypoint types through clustering. Specifically, we encode semantics into the keypoints by teaching them to reconstruct images from a sparse set of keypoints and their descriptors, where the descriptors are forced to form distinct clusters in feature space around learned prototypes. This makes our approach amenable to a wider range of tasks than any previous unsupervised keypoint method: we show experiments on multiple-instance detection and classification, object discovery, and landmark detection\u2014all unsupervised\u2014with performance on par with the state of the art, while also being able to deal with multiple instances."}}
{"id": "1EDRk-CyQou", "cdate": 1650495147432, "mdate": null, "content": {"title": "Representing 3D Ultrasound with Neural Fields", "abstract": "3D Ultrasound (3D-US) is a powerful imaging modality, but the high storage requirement and low spatial resolution challenge wider adoption. Recent advancements in Neural Fields suggest a potential for efficient storage and construction of 3D-US data. In this work, we show how to effectively represent 3D-US data with Neural Fields, where we first learn the 2D slices of the 3D ultrasound data and expand to 3D. This two-stage representation learning improves the quality of 3D-US in terms of Peak Signal-to-Noise Ratio (PSNR) to 31.84dB from 28.7dB, a significant improvement directly noticeable to the human eye."}}
{"id": "zAtfOHC2XNah", "cdate": 1640995200000, "mdate": 1663770365448, "content": {"title": "FlowNet-PET: Unsupervised Learning to Perform Respiratory Motion Correction in PET Imaging", "abstract": "To correct for respiratory motion in PET imaging, an interpretable and unsupervised deep learning technique, FlowNet-PET, was constructed. The network was trained to predict the optical flow between two PET frames from different breathing amplitude ranges. The trained model aligns different retrospectively-gated PET images, providing a final image with similar counting statistics as a non-gated image, but without the blurring effects. FlowNet-PET was applied to anthropomorphic digital phantom data, which provided the possibility to design robust metrics to quantify the corrections. When comparing the predicted optical flows to the ground truths, the median absolute error was found to be smaller than the pixel and slice widths. The improvements were illustrated by comparing against images without motion and computing the intersection over union (IoU) of the tumors as well as the enclosed activity and coefficient of variation (CoV) within the no-motion tumor volume before and after the corrections were applied. The average relative improvements provided by the network were 64%, 89%, and 75% for the IoU, total activity, and CoV, respectively. FlowNet-PET achieved similar results as the conventional retrospective phase binning approach, but only required one sixth of the scan duration. The code and data have been made publicly available (https://github.com/teaghan/FlowNet_PET)."}}
{"id": "mdvQDxogINv4", "cdate": 1640995200000, "mdate": 1663770365452, "content": {"title": "NeuralBF: Neural Bilateral Filtering for Top-down Instance Segmentation on Point Clouds", "abstract": "We introduce a method for instance proposal generation for 3D point clouds. Existing techniques typically directly regress proposals in a single feed-forward step, leading to inaccurate estimation. We show that this serves as a critical bottleneck, and propose a method based on iterative bilateral filtering with learned kernels. Following the spirit of bilateral filtering, we consider both the deep feature embeddings of each point, as well as their locations in the 3D space. We show via synthetic experiments that our method brings drastic improvements when generating instance proposals for a given point of interest. We further validate our method on the challenging ScanNet benchmark, achieving the best instance segmentation performance amongst the sub-category of top-down methods."}}
{"id": "c7ywiAbFpT", "cdate": 1640995200000, "mdate": 1663770365422, "content": {"title": "TUSK: Task-Agnostic Unsupervised Keypoints", "abstract": "Existing unsupervised methods for keypoint learning rely heavily on the assumption that a specific keypoint type (e.g. elbow, digit, abstract geometric shape) appears only once in an image. This greatly limits their applicability, as each instance must be isolated before applying the method-an issue that is never discussed or evaluated. We thus propose a novel method to learn Task-agnostic, UnSupervised Keypoints (TUSK) which can deal with multiple instances. To achieve this, instead of the commonly-used strategy of detecting multiple heatmaps, each dedicated to a specific keypoint type, we use a single heatmap for detection, and enable unsupervised learning of keypoint types through clustering. Specifically, we encode semantics into the keypoints by teaching them to reconstruct images from a sparse set of keypoints and their descriptors, where the descriptors are forced to form distinct clusters in feature space around learned prototypes. This makes our approach amenable to a wider range of tasks than any previous unsupervised keypoint method: we show experiments on multiple-instance detection and classification, object discovery, and landmark detection-all unsupervised-with performance on par with the state of the art, while also being able to deal with multiple instances."}}
{"id": "ZwLZdn7eFFh", "cdate": 1640995200000, "mdate": 1663770365332, "content": {"title": "Repurposing existing deep networks for caption and aesthetic-guided image cropping", "abstract": ""}}
{"id": "YkT9CKhO6svo", "cdate": 1640995200000, "mdate": 1663770365451, "content": {"title": "NeuMan: Neural Human Radiance Field from a Single Video", "abstract": "Photorealistic rendering and reposing of humans is important for enabling augmented reality experiences. We propose a novel framework to reconstruct the human and the scene that can be rendered with novel human poses and views from just a single in-the-wild video. Given a video captured by a moving camera, we train two NeRF models: a human NeRF model and a scene NeRF model. To train these models, we rely on existing methods to estimate the rough geometry of the human and the scene. Those rough geometry estimates allow us to create a warping field from the observation space to the canonical pose-independent space, where we train the human model in. Our method is able to learn subject specific details, including cloth wrinkles and accessories, from just a 10 seconds video clip, and to provide high quality renderings of the human under novel poses, from novel views, together with the background."}}
