{"id": "KufsD_c_BX", "cdate": 1668097105718, "mdate": 1668097105718, "content": {"title": "Improving Semantic Segmentation via Efficient Self-Training", "abstract": "Starting from the seminal work of Fully Convolutional Networks (FCN), there has been significant progress on semantic segmentation. However, deep learning models often require large amounts of pixelwise annotations to train accurate and robust models. Given the prohibitively expensive annotation cost of segmentation masks, we introduce a self-training framework in this paper to leverage pseudo labels generated from unlabeled data. In order to handle the data imbalance problem of semantic segmentation, we propose a centroid sampling strategy to uniformly select training samples from every class within each epoch. We also introduce a fast training schedule to alleviate the computational burden. This enables us to explore the usage of large amounts of pseudo labels. Our Centroid Sampling based Self-Training framework (CSST) achieves state-of-the-art results on Cityscapes and CamVid datasets. "}}
{"id": "62VZR_4KJu", "cdate": 1668096985484, "mdate": 1668096985484, "content": {"title": "ResNeSt: Split-Attention Networks", "abstract": "The ability to learn richer network representations generally boosts the performance of deep learning models. To improve representation-learning in convolutional neural networks, we present a multi-branch architecture, which applies channel-wise attention across different network branches to leverage the complementary strengths of both feature-map attention and multi-path representation. Our proposed Split-Attention module provides a simple and modular computation block that can serve as a drop-in replacement for the popular residual block, while producing more diverse representations via cross-feature interactions. Adding a Split-Attention module into the architecture design space of RegNet-Y and FBNetV2 directly improves the performance of the resulting network. Replacing residual blocks with our Split-Attention module, we further design a new variant of the ResNet model, named ResNeSt, which outperforms EfficientNet in terms of the accuracy/latency trade-off."}}
{"id": "pJ8Pbkuo9XB", "cdate": 1609459200000, "mdate": 1648850130112, "content": {"title": "Predicting ASD diagnosis in children with synthetic and image-based eye gaze data", "abstract": "Highlights \u2022 Atypical visual attention associated with autism spectrum disorders can be studied through eye gaze behavior of children at a very young age. \u2022 Autistic gaze viewing patterns in a free-viewing paradigm studied using computer vision. \u2022 Correlation between scan-path fixations and high-level image semantics extracted through deep learning result in ASD prediction accuracy up to 62% \u2022 Study finds significant class differences by applying machine learning to raw scan-path fixations and duration; verification on larger dataset needed. Abstract As early intervention is highly effective for young children with autism spectrum disorder (ASD), it is imperative to make accurate diagnosis as early as possible. ASD has often been associated with atypical visual attention and eye gaze data can be collected at a very early age. An automatic screening tool based on eye gaze data that could identify ASD risk offers the opportunity for intervention before the full set of symptoms is present. In this paper, we propose two machine learning methods, synthetic saccade approach and image based approach, to automatically classify ASD given children\u2019s eye gaze data collected from free-viewing tasks of natural images. The first approach uses a generative model of synthetic saccade patterns to represent the baseline scan-path from a typical non-ASD individual and combines it with the real scan-path as well as other auxiliary data as inputs to a deep learning classifier. The second approach adopts a more holistic image-based approach by feeding the input image and a sequence of fixation maps into a convolutional or recurrent neural network. Using a publicly-accessible collection of children\u2019s gaze data, our experiments indicate that the ASD prediction accuracy reaches 67.23% accuracy on the validation dataset and 62.13% accuracy on the test dataset."}}
{"id": "VG3tvRcDTSk", "cdate": 1577836800000, "mdate": 1648850130105, "content": {"title": "Machine Learning Based Autism Spectrum Disorder Detection from Videos", "abstract": "Early diagnosis of Autism Spectrum Disorder (ASD) is crucial for best outcomes to interventions. In this paper, we present a machine learning (ML) approach to ASD diagnosis based on identifying specific behaviors from videos of infants of ages 6 through 36 months. The behaviors of interest include directed gaze towards faces or objects of interest, positive affect, and vocalization. The dataset consists of 2000 videos of 3-minute duration with these behaviors manually coded by expert raters. Moreover, the dataset has statistical features including duration and frequency of the above mentioned behaviors in the video collection as well as independent ASD diagnosis by clinicians. We tackle the ML problem in a two-stage approach. Firstly, we develop deep learning models for automatic identification of clinically relevant behaviors exhibited by infants in a one-on-one interaction setting with parents or expert clinicians. We report baseline results of behavior classification using two methods: (1) image based model (2) facial behavior features based model. We achieve 70% accuracy for smile, 68% accuracy for look face, 67% for look object and 53% accuracy for vocalization. Secondly, we focus on ASD diagnosis prediction by applying a feature selection process to identify the most significant statistical behavioral features and a over and under sampling process to mitigate the class imbalance, followed by developing a baseline ML classifier to achieve an accuracy of 82% for ASD diagnosis."}}
{"id": "tzOiJxwuluZ", "cdate": 1546300800000, "mdate": 1648850130112, "content": {"title": "Recognizing road from satellite images by structured neural network", "abstract": "Highlights \u2022 We have collected a large labeled road extraction dataset with varying region distribution, which includes labels for both roads and their orientations. \u2022 We further designed a structured deep neural network for road extraction. The specially designed cascade network and direction module are quite effective to capture the linear structure of the road. The experimental results reveal the effectiveness of our structured road extraction network. \u2022 We designed a novel road performance evaluation metric that considers both the pixel-level aspect and number-level aspect. To our best knowledge, it is the first time to introduce the number-level metric for the road detection task. Abstract Recognizing and extracting roads accurately are significant for auto-driving cars and map providers. Thanks to the power of deep learning, it is possible to achieve high accuracy with a large amount of labeled data. However, as far as we know, there is not enough public data for road recognition from satellite images, especially for the urban scene. To provide sufficient data for training a neural network, we collect a large dataset for road recognition task, which covers varieties of road scenes and contains large-size images from the satellite view. Inspired by the unique road structure, we propose a structured deep neural network to obtain smooth and continuous road skeleton. The proposed network incorporates the road segmentation result and direction result together. Based on the shape prior of the road, the predicted direction information can facilitate road extraction in an end-to-end learning network. Then, a cascade skeleton network is proposed to achieve smooth, continuous and equal-width road skeleton. We also design an evaluation metric which measures both per pixel accuracy and per road accuracy. Our structured road extraction network outperforms the state-of-the-art approaches and the baseline without road prior."}}
{"id": "N6FKWZZkfdm", "cdate": 1546300800000, "mdate": 1648850130111, "content": {"title": "Predicting Autism Diagnosis using Image with Fixations and Synthetic Saccade Patterns", "abstract": "Signs of autism spectrum disorder (ASD) emerge in the first year of life in many children, but diagnosis is typically made much later, at an average age of 4 years in the United States. Early intervention is highly effective for young children with ASD, but is typically reserved for children with a formal diagnosis, making accurate identification as early as possible imperative. A screening tool that could identify ASD risk during infancy offers the opportunity for intervention before the full set of symptoms is present. In this paper, we propose two machine learning methods, synthetic saccade approach and image based approach, to automatically classify ASD given the scanpath data from children on free viewing of natural images. The first approach uses a generative model of synthetic saccade patterns to represent the baseline scan-path from a typical non-ASD individual and combines it with the input scanpath as well as other auxiliary data as inputs to a deep learning classifier. The second approach adopts a more holistic image based approach by feeding the input image and a sequence of fixation maps into a state-of-the-art convolutional neural network. Our experiments indicate that we can get 65.41% accuracy on the validation dataset."}}
{"id": "9AQQublvQBF", "cdate": 1546300800000, "mdate": 1648850130110, "content": {"title": "Not All Areas Are Equal: Transfer Learning for Semantic Segmentation via Hierarchical Region Selection", "abstract": "The success of deep neural networks for semantic segmentation heavily relies on large-scale and well-labeled datasets, which are hard to collect in practice. Synthetic data offers an alternative to obtain ground-truth labels for free. However, models directly trained on synthetic data often struggle to generalize to real images. In this paper, we consider transfer learning for semantic segmentation that aims to mitigate the gap between abundant synthetic data (source domain) and limited real data (target domain). Unlike previous approaches that either learn mappings to target domain or finetune on target images, our proposed method jointly learn from real images and selectively from realistic pixels in synthetic images to adapt to the target domain. Our key idea is to have weighting networks to score how similar the synthetic pixels are to real ones, and learn such weighting at pixel-, region- and image-levels. We jointly learn these hierarchical weighting networks and segmentation network in an end-to-end manner. Extensive experiments demonstrate that our proposed approach significantly outperforms other existing baselines, and is applicable to scenarios with extremely limited real images."}}
{"id": "rk4Qso0cKm", "cdate": 1538087835260, "mdate": null, "content": {"title": "Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network", "abstract": "We present a new algorithm to train a robust neural network against adversarial attacks. \nOur algorithm is motivated by the following two ideas. First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (Liu 2017), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. \nInstead, we model randomness under the framework of Bayesian Neural Network (BNN) to formally learn the posterior distribution of models in a scalable way. Second, we formulate the mini-max problem in BNN to learn the best model distribution under adversarial attacks, leading to an adversarial-trained Bayesian neural net. Experiment results demonstrate that the proposed algorithm achieves state-of-the-art performance under strong attacks. On CIFAR-10 with VGG network, our model leads to 14% accuracy improvement compared with adversarial training (Madry 2017) and random self-ensemble (Liu, 2017) under PGD attack with 0.035 distortion, and the gap becomes even larger on a subset of ImageNet."}}
