{"id": "qsA6gnT65q", "cdate": 1672531200000, "mdate": 1706014751485, "content": {"title": "Enhancing Visual Understanding by Removing Dithering with Global and Self-Conditioned Transformation", "abstract": "PNG-8 images are commonly used on the web due to their small size, but their limited color palette often leads to dithering artifacts. Unfortunately, restoring these images using a conventional convolutional neural network (CNN) often results in suboptimal performance since the spatial distribution of dithering is not uniform across the image. This is because the convolutional operator is spatially consistent, meaning it applies the same kernel to all pixels, which we refer to as a global transformation. To address this issue, we propose PNG8IRNet, one approach that combines global and self-conditioned transformations to remove dithering artifacts. Our method incorporates a multilayer perceptron (MLP) to generate diverse kernels for each pixel, taking into account the spatial non-uniformity of dithering, which we define as a self-conditioned transformation. PNG8IRNet demonstrates its performance on multiple datasets, substantially enhancing visual comprehension through a comprehensive set of experiments."}}
{"id": "ruuW5eqf50Y", "cdate": 1609459200000, "mdate": 1643944433783, "content": {"title": "Learning Representations for High-Dynamic-Range Image Color Transfer in a Self-Supervised Way", "abstract": "Reference-based color transfer between images has been a fundamental function in image editing. However, existing approaches pay less attention to high-dynamic-range (HDR) images. It is worth noting that designing an appropriate representation for HDR images to achieve satisfying color transfer is challenging. In this paper, we propose an innovative high-dynamic-range image color transfer generative adversarial network (HDRCTGAN) to encode the original image into fine representations that allow transfer of the color of the reference image to the target image. We propose to learn fine representations through a generative adversarial network (GAN) in a self-supervised way. Particularly, the proposed method is self-supervised learning that requires only unlabeled HDR images instead of supervised learning that requires lots of ground truth pairs. HDRCTGAN consists of a generator to transfer the color of the reference image to the target image over the feature domain and a discriminator to suppress the artifacts caused by the generator. We also design a loss function to ensure that HDRCTGAN possesses two required properties: (a) high fidelity and (b) self-identity. The proposed approach yields a pleasing visual result. We have carried out HDR specific evaluations including both objective quantitative experiments with HDR metrics and subjective user studies operated on HDR display devices to demonstrate the effectiveness of our method. Furthermore, we have verified the applicability of the proposed approach to several applications, such as color transfer of HDR images captured by smartphones, color transfer of fabric images, and reference-based grayscale image colorization."}}
{"id": "NpFGoqm687", "cdate": 1609459200000, "mdate": 1667358806983, "content": {"title": "FACIAL: Synthesizing Dynamic Talking Face with Implicit Attribute Learning", "abstract": "In this paper, we propose a talking face generation method that takes an audio signal as input and a short target video clip as reference, and synthesizes a photo-realistic video of the target face with natural lip motions, head poses, and eye blinks that are in-sync with the input audio signal. We note that the synthetic face attributes include not only explicit ones such as lip motions that have high correlations with speech, but also implicit ones such as head poses and eye blinks that have only weak correlation with the input audio. To model such complicated relationships among different face attributes with input audio, we propose a FACe Implicit Attribute Learning Generative Adversarial Network (FACIAL-GAN), which integrates the phonetics-aware, context-aware, and identity-aware information to synthesize the 3D face animation with realistic motions of lips, head poses, and eye blinks. Then, our Rendering-to-Video network takes the rendered face images and the attention map of eye blinks as input to generate the photorealistic output video frames. Experimental results and user studies show our method can generate realistic talking face videos with not only synchronized lip motions, but also natural head movements and eye blinks, with better qualities than the results of state-of-the-art methods."}}
{"id": "HtU-5lqf5Ct", "cdate": 1577836800000, "mdate": 1643944433779, "content": {"title": "DeSmoothGAN: Recovering Details of Smoothed Images via Spatial Feature-wise Transformation and Full Attention", "abstract": "Recently, generative adversarial networks (GAN) have been widely used to solve image-to-image translation problems such as edges to photos, labels to scenes, and colorizing grayscale images. However, how to recover details of smoothed images is still unexplored. Naively training a GAN like pix2pix causes insufficiently perfect results due to the fact that we ignore two main characteristics including spatial variability and spatial correlation as for this problem. In this work, we propose DeSmoothGAN to utilize both characteristics specifically. The spatial variability indicates that the details of different areas of smoothed images are distinct and they are supposed to be recovered differently. Therefore, we propose to perform spatial feature-wise transformation to recover individual areas differently. The spatial correlation represents that the details of different areas are related to each other. Thus, we propose to apply full attention to consider the relations between them. The proposed method generates satisfying results on several real-world datasets. We have conducted quantitative experiments including smooth consistency and image similarity to demonstrate the effectiveness of DeSmoothGAN. Furthermore, ablation studies are performed to illustrate the usefulness of our proposed feature-wise transformation and full attention."}}
{"id": "HWVW5l9zqAF", "cdate": 1546300800000, "mdate": 1643944433770, "content": {"title": "GBRTVis: online analysis of gradient boosting regression tree", "abstract": "Visualizations of machine learning models have developed rapidly during these days, attracting great interests of industry and researchers. However, a pipeline that visualizations are created from logged data is a time-consuming process. In this work, we adopt progressive visual analytics to propose a new pipeline to facilitate the visual analysis progress of gradient boosting regression tree (GBRT). Visualizations such as tree view, instances view, and cluster view are created according to different types of data in real time. Users can explore GBRT with different visualization components interactively through GBRTVis. Case studies demonstrate that our pipeline can improve the efficiency of the training process and understanding. Furthermore, we propose a mixed structure of GBRT to improve itself. Two tests on different datasets show the effectiveness of the improvement. Graphical Abstract"}}
{"id": "S4nZ9g5GcCF", "cdate": 1514764800000, "mdate": 1643944433770, "content": {"title": "Translucent Image Recoloring through Homography Estimation", "abstract": "Image color editing techniques are of great significance for users who wish to adjust the image color. However, previous works paid less attention to the translucent images. In this paper, we propose..."}}
