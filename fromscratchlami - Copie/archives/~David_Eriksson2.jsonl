{"id": "uKuZrFD2OV", "cdate": 1684350534549, "mdate": 1684350534549, "content": {"title": "Bayesian Optimization over High-Dimensional Combinatorial Spaces via Dictionary-based Embeddings", "abstract": "We consider the problem of optimizing expensive black-box functions over high-dimensional\ncombinatorial spaces which arises in many science, engineering, and ML applications. We use\nBayesian Optimization (BO) and propose a novel\nsurrogate modeling approach for efficiently handling a large number of binary and categorical\nparameters. The key idea is to select a number of discrete structures from the input space\n(the dictionary) and use them to define an ordinal embedding for high-dimensional combinatorial structures. This allows us to use existing Gaussian process models for continuous\nspaces. We develop a principled approach based\non binary wavelets to construct dictionaries for\nbinary spaces, and propose a randomized construction method that generalizes to categorical\nspaces. We provide theoretical justification to\nsupport the effectiveness of the dictionary-based\nembeddings. Our experiments on diverse realworld benchmarks demonstrate the effectiveness\nof our proposed surrogate modeling approach\nover state-of-the-art BO methods"}}
{"id": "WV1ZXTH0OIn", "cdate": 1652737389503, "mdate": null, "content": {"title": "Bayesian Optimization over Discrete and Mixed Spaces via Probabilistic Reparameterization", "abstract": "Optimizing expensive-to-evaluate black-box functions of discrete (and potentially continuous) design parameters is a ubiquitous problem in scientific and engineering applications. Bayesian optimization (BO) is a popular, sample-efficient method that leverages a probabilistic surrogate model and  an acquisition function (AF) to select promising designs to evaluate. However, maximizing the AF over mixed or high-cardinality discrete search spaces is challenging standard gradient-based methods cannot be used directly or evaluating the AF at every point in the search space would be computationally prohibitive. To address this issue, we propose using probabilistic reparameterization (PR). Instead of directly optimizing the AF over the search space containing discrete parameters, we instead maximize the expectation of the AF over a probability distribution defined by continuous parameters. We prove that under suitable reparameterizations, the BO policy that maximizes the probabilistic objective is the same as that which maximizes the AF, and therefore, PR enjoys the same regret bounds as the original BO policy using the underlying AF. Moreover, our approach provably converges to a stationary point of the probabilistic objective under gradient ascent using scalable, unbiased estimators of both the probabilistic objective and its gradient. Therefore, as the number of starting points and gradient steps increase, our approach will recover of a maximizer of the AF (an often-neglected requisite for commonly used BO regret bounds). We validate our approach empirically and demonstrate state-of-the-art optimization performance on a wide range of real-world applications. PR is complementary to (and benefits) recent work and naturally generalizes to settings with multiple objectives and black-box constraints."}}
{"id": "Od7OEyf8PcU", "cdate": 1652125708274, "mdate": 1652125708274, "content": {"title": "Fast Matrix Square Roots with Applications to Gaussian Processes and Bayesian Optimization", "abstract": "Matrix square roots and their inverses arise frequently in machine learning, eg, when sampling from high-dimensional Gaussians N (0, K) or \u201cwhitening\u201d a vector b against covariance matrix K. While existing methods typically require O (N^ 3) computation, we introduce a highly-efficient quadratic-time algorithm for computing K^{1/2} b, K^{-1/2} b, and their derivatives through matrix-vector multiplication (MVMs). Our method combines Krylov subspace methods with a rational approximation and typically achieves 4 decimal places of accuracy with fewer than 100 MVMs. Moreover, the backward pass requires little additional computation. We demonstrate our method's applicability on matrices as large as 50,000 by 50,000-well beyond traditional methods-with little approximation error. Applying this increased scalability to variational Gaussian processes, Bayesian optimization, and Gibbs sampling results in more powerful models with higher accuracy. In particular, we perform variational GP inference with up to 10,000 inducing points and perform Gibbs sampling on a 25,000-dimensional problem.\n"}}
{"id": "z7AHitqd_fZ", "cdate": 1652125609703, "mdate": 1652125609703, "content": {"title": "Bayesian optimization is superior to random search for machine learning hyperparameter tuning: Analysis of the black-box optimization challenge 2020", "abstract": "This paper presents the results and insights from the black-box optimization (BBO) challenge at NeurIPS2020 which ran from July\u2013October, 2020. The challenge emphasized the importance of evaluating derivative-free optimizers for tuning the hyperparameters of machine learning models. This was the first black-box optimization challenge with a machine learning emphasis. It was based on tuning (validation set) performance of standard machine learning models on real datasets. This competition has widespread impact as black-box optimization (eg, Bayesian optimization) is relevant for hyperparameter tuning in almost every machine learning project as well as many applications outside of machine learning. The final leaderboard was determined using the optimization performance on held-out (hidden) objective functions, where the optimizers ran without human intervention. Baselines were set using the default settings of several open source black-box optimization packages as well as random search.\n"}}
{"id": "MH2LX_CxinR", "cdate": 1652125493391, "mdate": 1652125493391, "content": {"title": "Scalable Constrained Bayesian Optimization", "abstract": "The global optimization of a high-dimensional black-box function under black-box constraints is a pervasive task in machine learning, control, and engineering. These problems are challenging since the feasible set is typically non-convex and hard to find, in addition to the curses of dimensionality and the heterogeneity of the underlying functions. In particular, these characteristics dramatically impact the performance of Bayesian optimization methods, that otherwise have become the defacto standard for sample-efficient optimization in unconstrained settings, leaving practitioners with evolutionary strategies or heuristics. We propose the scalable constrained Bayesian optimization (SCBO) algorithm that overcomes the above challenges and pushes the applicability of Bayesian optimization far beyond the state-of-the-art. A comprehensive experimental evaluation demonstrates that SCBO achieves excellent results on a variety of benchmarks. To this end, we propose two new control problems that we expect to be of independent value for the scientific community.\n"}}
{"id": "1WstkEYVcDz", "cdate": 1652125452491, "mdate": 1652125452491, "content": {"title": "Multi-objective bayesian optimization over high-dimensional search spaces", "abstract": "The ability to optimize multiple competing objective functions with high sample efficiency is imperative in many applied problems across science and industry. Multi-objective Bayesian optimization (BO) achieves strong empirical performance on such problems, but even with recent methodological advances, it has been restricted to simple, low-dimensional domains. Most existing BO methods exhibit poor performance on search spaces with more than a few dozen parameters. In this work we propose MORBO, a method for multi-objective Bayesian optimization over high-dimensional search spaces. MORBO performs local Bayesian optimization within multiple trust regions simultaneously, allowing it to explore and identify diverse solutions even when the objective functions are difficult to model globally. We show that MORBO significantly advances the state-of-the-art in sample-efficiency for several high-dimensional synthetic and real-world multi-objective problems, including a vehicle design problem with 222 parameters, demonstrating that MORBO is a practical approach for challenging and important problems that were previously out of reach for BO methods."}}
{"id": "B53PimTUnpX", "cdate": 1652125372265, "mdate": 1652125372265, "content": {"title": "High-Dimensional Bayesian Optimization with Sparse Axis-Aligned Subspaces", "abstract": "Bayesian optimization (BO) is a powerful paradigm for efficient optimization of black-box objective functions. High-dimensional BO presents a particular challenge, in part because the curse of dimensionality makes it difficult to define\u2014as well as do inference over\u2014a suitable class of surrogate models. We argue that Gaussian process surrogate models defined on sparse axis-aligned subspaces offer an attractive compromise between flexibility and parsimony. We demonstrate that our approach, which relies on Hamiltonian Monte Carlo for inference, can rapidly identify sparse subspaces relevant to modeling the unknown objective function, enabling sample-efficient high-dimensional BO. In an extensive suite of experiments comparing to existing methods for high-dimensional BO we demonstrate that our algorithm, Sparse AxisAligned Subspace BO (SAASBO), achieves excellent performance on several synthetic and real-world problems without the need to set problem-specific hyperparameters."}}
{"id": "r5IEvvIs9xq", "cdate": 1646077534827, "mdate": null, "content": {"title": "Multi-Objective Bayesian Optimization over High-Dimensional Search Spaces", "abstract": "Many real-world scientific and industrial applications require optimizing multiple competing black-box objectives. When the objectives are expensive-to-evaluate, multi-objective Bayesian optimization (BO) is a popular approach because of its high simple efficiency. However, even with recent methodological advances, most existing multi-objective BO methods perform poorly on search spaces with more than a few dozen parameters and rely on global surrogate models that scale cubically with the number of observations. In this work we propose MORBO, a scalable method for multi-objective BO over high-dimensional search spaces. MORBO identifies diverse globally optimal solutions by performing BO in multiple local regions of the design space in parallel using a coordinated strategy. We show that MORBO significantly advances the state-of-the-art in sample efficiency for several high-dimensional synthetic problems and real world applications, including an optical display design problem and a vehicle design problem with 146 and 222 parameters, respectively. On these problems, where existing BO algorithms fail to scale and perform well, MORBO provides practitioners with order-of-magnitude improvements in sample-efficiency over the current approach. "}}
{"id": "0ciyfd4SvbI", "cdate": 1621522019106, "mdate": null, "content": {"title": "Latency-Aware Neural Architecture Search with Multi-Objective Bayesian Optimization", "abstract": "When tuning the architecture and hyperparameters of large machine learning models for on-device deployment, it is desirable to understand the optimal trade-offs between on-device latency and model accuracy.  In this work, we leverage recent methodological advances in Bayesian optimization over high-dimensional search spaces and multi-objective Bayesian optimization to efficiently explore these trade-offs for a production-scale on-device natural language understanding model at Facebook."}}
{"id": "S1gW5ErgLS", "cdate": 1567802504767, "mdate": null, "content": {"title": "Scalable Global Optimization via Local Bayesian Optimization", "abstract": "Bayesian optimization has recently emerged as a popular method for the sample-efficient optimization of expensive black-box functions. However, the application to high-dimensional problems with several thousand observations remains challenging, and on difficult problems Bayesian optimization is often not competitive with other paradigms. In this paper we take the view that this is due to the implicit homogeneity of the global probabilistic models and an overemphasized exploration that results from global acquisition. This motivates the design of a local probabilistic approach for global optimization of large-scale high-dimensional problems. We propose the TuRBO algorithm that fits a collection of local models and performs a principled global allocation of samples across these models via an implicit bandit approach. A comprehensive evaluation demonstrates that TuRBO outperforms state-of-the-art methods from machine learning and operations research on problems spanning reinforcement learning, robotics, and the natural sciences."}}
