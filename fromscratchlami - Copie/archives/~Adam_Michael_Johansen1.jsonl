{"id": "eHOH-7sgb5", "cdate": 1684775579038, "mdate": 1684775579038, "content": {"title": "Particle algorithms for maximum likelihood training of latent variable models", "abstract": "(Neal and Hinton, 1998) recast maximum likelihood estimation of any given latent variable model as the minimization of a free energy functional F, and the EM algorithm as coordinate descent applied to F. Here, we explore alternative ways to optimize the functional. In particular, we identify various gradient flows associated with F and show that their limits coincide with F's stationary points. By discretizing the flows, we obtain practical particle-based algorithms for maximum likelihood estimation in broad classes of latent variable models. The novel algorithms scale to high-dimensional settings and perform well in numerical experiments."}}
{"id": "suV9OKB0kV", "cdate": 1640995200000, "mdate": 1649083190098, "content": {"title": "Product-form estimators: exploiting independence to scale up Monte Carlo", "abstract": "We introduce a class of Monte Carlo estimators that aim to overcome the rapid growth of variance with dimension often observed for standard estimators by exploiting the target\u2019s independence structure. We identify the most basic incarnations of these estimators with a class of generalized U-statistics and thus establish their unbiasedness, consistency, and asymptotic normality. Moreover, we show that they obtain the minimum possible variance amongst a broad class of estimators, and we investigate their computational cost and delineate the settings in which they are most efficient. We exemplify the merger of these estimators with other well known Monte Carlo estimators so as to better adapt the latter to the target\u2019s independence structure and improve their performance. We do this via three simple mergers: one with importance sampling, another with importance sampling squared, and a final one with pseudo-marginal Metropolis\u2013Hastings. In all cases, we show that the resulting estimators are well founded and achieve lower variances than their standard counterparts. Lastly, we illustrate the various variance reductions through several examples."}}
{"id": "dJrJ8M7ZBe", "cdate": 1640995200000, "mdate": 1681674484381, "content": {"title": "The node-wise Pseudo-marginal method: model selection with spatial dependence on latent graphs", "abstract": "Motivated by problems from neuroimaging in which existing approaches make use of \u201cmass univariate\u201d analysis which neglects spatial structure entirely, but the full joint modelling of all quantities of interest is computationally infeasible, a novel method for incorporating spatial dependence within a (potentially large) family of model-selection problems is presented. Spatial dependence is encoded via a Markov random field model for which a variant of the pseudo-marginal Markov chain Monte Carlo algorithm is developed and extended by a further augmentation of the underlying state space. This approach allows the exploitation of existing unbiased marginal likelihood estimators used in settings in which spatial independence is normally assumed thereby facilitating the incorporation of spatial dependence using non-spatial estimates with minimal additional development effort. The proposed algorithm can be realistically used for analysis of moderately sized data sets such as 2D slices of whole 3D dynamic PET brain images or other regions of interest. Principled approximations of the proposed method, together with simple extensions based on the augmented spaces, are investigated and shown to provide similar results to the full pseudo-marginal method. Such approximations and extensions allow the improved performance obtained by incorporating spatial dependence to be obtained at negligible additional cost. An application to measured PET image data shows notable improvements in revealing underlying spatial structure when compared to current methods that assume spatial independence."}}
{"id": "bJCqKvei0CA", "cdate": 1640995200000, "mdate": 1681674484311, "content": {"title": "Solving Fredholm Integral Equations of the First Kind via Wasserstein Gradient Flows", "abstract": "Solving Fredholm equations of the first kind is crucial in many areas of the applied sciences. In this work we adopt a probabilistic and variational point of view by considering a minimization problem in the space of probability measures with an entropic regularization. Contrary to classical approaches which discretize the domain of the solutions, we introduce an algorithm to asymptotically sample from the unique solution of the regularized minimization problem. As a result our estimators do not depend on any underlying grid and have better scalability properties than most existing methods. Our algorithm is based on a particle approximation of the solution of a McKean--Vlasov stochastic differential equation associated with the Wasserstein gradient flow of our variational formulation. We prove the convergence towards a minimizer and provide practical guidelines for its numerical implementation. Finally, our method is compared with other approaches on several examples including density deconvolution and epidemiology."}}
{"id": "7dwjXkM0IKN", "cdate": 1640995200000, "mdate": 1681674484433, "content": {"title": "Scalable particle-based alternatives to EM", "abstract": "Neal and Hinton, 1998) recast maximum likelihood estimation of any given latent variable model as the minimization of a free energy functional $F$, and the EM algorithm as coordinate descent applied to $F$. Here, we explore alternative ways to optimize the functional. In particular, we identify various gradient flows associated with $F$ and show that their limits coincide with $F$'s stationary points. By discretizing the flows, we obtain practical particle-based algorithms for maximum likelihood estimation in broad classes of latent variable models. The novel algorithms scale to high-dimensional settings and perform well in numerical experiments."}}
{"id": "tJoDodikCrJ", "cdate": 1609459200000, "mdate": 1681674484312, "content": {"title": "Global Consensus Monte Carlo", "abstract": "To conduct Bayesian inference with large datasets, it is often convenient or necessary to distribute the data across multiple machines. We consider a likelihood function expressed as a product of t..."}}
{"id": "AWvbaa-WOsf", "cdate": 1609459200000, "mdate": 1649083190094, "content": {"title": "A spatio-temporal model to reveal oscillator phenotypes in molecular clocks: Parameter estimation elucidates circadian gene transcription dynamics in single-cells", "abstract": "Author summary We develop a method for estimating and modelling transcriptional processes in living tissue samples using a model which approximates the end product of a series of complex chemical interactions using a density function\u2014and thus is amenable to parameter estimation\u2014but can realistically account for the intrinsic noise and rhythm generation inherent in the single-cell. The model incorporates a form of dependence between nearby cells using a spatial prior distribution over the parameters. The model thus describes the cyclical behaviour of the production of the population of some molecular species within cells, along with the spatial variation of the process across a network of cells. This approach is suitable for modelling circadian gene expression in the suprachiasmatic nucleus (SCN), the region of the brain which is responsible for the \u2018circadian master clock\u2019 which coordinates the bodies daily rhythms. This model is applied to three sample tissues from mice SCN. Based on the inferred dynamic behaviour of the cells, we are able to divide the organ into two regions: a central core in which the rhythm is intrinsic and resilient and the more entrainable outer region which is much more heavily influenced by external stimuli. The findings highlight a trade-off between resilient cyclic behaviour and ability to adapt to environmental cues."}}
{"id": "jwRmRyxY5ZM", "cdate": 1577836800000, "mdate": 1649083190107, "content": {"title": "Generalised Bayesian Filtering via Sequential Monte Carlo", "abstract": "We introduce a framework for inference in general state-space hidden Markov models (HMMs) under likelihood misspecification. In particular, we leverage the loss-theoretic perspective of Generalized Bayesian Inference (GBI) to define generalised filtering recursions in HMMs, that can tackle the problem of inference under model misspecification. In doing so, we arrive at principled procedures for robust inference against observation contamination by utilising the $\\beta$-divergence. Operationalising the proposed framework is made possible via sequential Monte Carlo methods (SMC), where the standard particle methods, and their associated convergence results, are readily adapted to the new setting. We demonstrate our approach to object tracking and Gaussian process regression problems, and observe improved performance over standard filtering algorithms."}}
{"id": "ADM-2xk2xPM", "cdate": 1577836800000, "mdate": 1649083190096, "content": {"title": "Generalized Bayesian Filtering via Sequential Monte Carlo", "abstract": "We introduce a framework for inference in general state-space hidden Markov models (HMMs) under likelihood misspecification. In particular, we leverage the loss-theoretic perspective of Generalized Bayesian Inference (GBI) to define generalised filtering recursions in HMMs, that can tackle the problem of inference under model misspecification. In doing so, we arrive at principled procedures for robust inference against observation contamination by utilising the $\\beta$-divergence. Operationalising the proposed framework is made possible via sequential Monte Carlo methods (SMC), where most standard particle methods, and their associated convergence results, are readily adapted to the new setting. We apply our approach to object tracking and Gaussian process regression problems, and observe improved performance over both standard filtering algorithms and other robust filters."}}
{"id": "n0oG1lqYmZD", "cdate": 1483228800000, "mdate": 1649083190104, "content": {"title": "Bayesian model comparison with un-normalised likelihoods", "abstract": "Models for which the likelihood function can be evaluated only up to a parameter-dependent unknown normalizing constant, such as Markov random field models, are used widely in computer science, statistical physics, spatial statistics, and network analysis. However, Bayesian analysis of these models using standard Monte Carlo methods is not possible due to the intractability of their likelihood functions. Several methods that permit exact, or close to exact, simulation from the posterior distribution have recently been developed. However, estimating the evidence and Bayes\u2019 factors for these models remains challenging in general. This paper describes new random weight importance sampling and sequential Monte Carlo methods for estimating BFs that use simulation to circumvent the evaluation of the intractable likelihood, and compares them to existing methods. In some cases we observe an advantage in the use of biased weight estimates. An initial investigation into the theoretical and empirical properties of this class of methods is presented. Some support for the use of biased estimates is presented, but we advocate caution in the use of such estimates."}}
