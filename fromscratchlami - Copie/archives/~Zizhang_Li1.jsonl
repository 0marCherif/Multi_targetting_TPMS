{"id": "wcJezmHj677", "cdate": 1672531200000, "mdate": 1695965572202, "content": {"title": "A Joint Modeling of Vision-Language-Action for Target-oriented Grasping in Clutter", "abstract": "We focus on the task of language-conditioned grasping in clutter, in which a robot is supposed to grasp the target object based on a language instruction. Previous works separately conduct visual grounding to localize the target object, and generate a grasp for that object. However, these works require object labels or visual attributes for grounding, which calls for handcrafted rules in planner and restricts the range of language instructions. In this paper, we propose to jointly model vision, language and action with object-centric representation. Our method is applicable under more flexible language instructions, and not limited by visual grounding error. Besides, by utilizing the powerful priors from the pre-trained multi-modal model and grasp model, sample efficiency is effectively improved and the sim2real problem is relived without additional data for transfer. A series of experiments carried out in simulation and real world indicate that our method can achieve better task success rate by less times of motion under more flexible language instructions. Moreover, our method is capable of generalizing better to scenarios with unseen objects and language instructions."}}
{"id": "pWMLkppft_", "cdate": 1672531200000, "mdate": 1695965572201, "content": {"title": "Failure-aware Policy Learning for Self-assessable Robotics Tasks", "abstract": "Self-assessment rules play an essential role in safe and effective real-world robotic applications, which verify the feasibility of the selected action before actual execution. But how to utilize the self-assessment results to re-choose actions remains a challenge. Previous methods eliminate the selected action evaluated as failed by the self-assessment rules, and re-choose one with the next-highest affordance (i.e. process-of-elimination strategy [1]), which ignores the dependency between the self-assessment results and the remaining untried actions. However, this dependency is important since the previous failures might help trim the remaining over-estimated actions. In this paper, we set to investigate this dependency by learning a failure-aware policy. We propose two architectures for the failure-aware policy by representing the self-assessment results of previous failures as the variable state, and leveraging recurrent neural networks to implicitly memorize the previous failures. Experiments conducted on three tasks demonstrate that our method can achieve better performances with higher task success rates by less trials. Moreover, when the actions are correlated, learning a failure-aware policy can achieve better performance than the process-of-elimination strategy."}}
{"id": "g0VFQjpucB", "cdate": 1672531200000, "mdate": 1681650923496, "content": {"title": "Learning a Room with the Occ-SDF Hybrid: Signed Distance Function Mingled with Occupancy Aids Scene Representation", "abstract": ""}}
{"id": "YrNYSXIJEsS", "cdate": 1672531200000, "mdate": 1681650923490, "content": {"title": "Failure-aware Policy Learning for Self-assessable Robotics Tasks", "abstract": ""}}
{"id": "424RCxMgBm", "cdate": 1672531200000, "mdate": 1681650923510, "content": {"title": "RICO: Regularizing the Unobservable for Indoor Compositional Reconstruction", "abstract": ""}}
{"id": "35XneE8AnY", "cdate": 1672531200000, "mdate": 1681650923502, "content": {"title": "A Joint Modeling of Vision-Language-Action for Target-oriented Grasping in Clutter", "abstract": ""}}
{"id": "nQA2CZzP1U", "cdate": 1640995200000, "mdate": 1681650923511, "content": {"title": "E-NeRV: Expedite Neural Video Representation with Disentangled Spatial-Temporal Context", "abstract": ""}}
{"id": "LHFZXPUK2s7", "cdate": 1640995200000, "mdate": 1667359264859, "content": {"title": "E-NeRV: Expedite Neural Video Representation with Disentangled Spatial-Temporal Context", "abstract": "Recently, the image-wise implicit neural representation of videos, NeRV, has gained popularity for its promising results and swift speed compared to regular pixel-wise implicit representations. However, the redundant parameters within the network structure can cause a large model size when scaling up for desirable performance. The key reason of this phenomenon is the coupled formulation of NeRV, which outputs the spatial and temporal information of video frames directly from the frame index input. In this paper, we propose E-NeRV, which dramatically expedites NeRV by decomposing the image-wise implicit neural representation into separate spatial and temporal context. Under the guidance of this new formulation, our model greatly reduces the redundant model parameters, while retaining the representation ability. We experimentally find that our method can improve the performance to a large extent with fewer parameters, resulting in a more than $8\\times$ faster speed on convergence. Code is available at https://github.com/kyleleey/E-NeRV."}}
{"id": "Do4DoYF79Pg", "cdate": 1640995200000, "mdate": 1667359264856, "content": {"title": "Learning Part Segmentation through Unsupervised Domain Adaptation from Synthetic Vehicles", "abstract": "Part segmentations provide a rich and detailed part-level description of objects. However, their annotation requires an enormous amount of work, which makes it difficult to apply standard deep learning methods. In this paper, we propose the idea of learning part segmentation through unsupervised domain adaptation (UDA) from synthetic data. We first introduce UDA-Part, a comprehensive part segmentation dataset for vehicles that can serve as an adequate benchmark for UDA <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> https://qliu24.github.io/udapart/. In UDA-Part, we label parts on 3D CAD models which enables us to generate a large set of annotated synthetic images. We also annotate parts on a number of real images to provide a real test set. Secondly, to advance the adaptation of part models trained from the synthetic data to the real images, we introduce a new UDA algorithm that leverages the object's spatial structure to guide the adaptation process. Our experimental results on two real test datasets confirm the superiority of our approach over existing works, and demonstrate the promise of learning part segmentation for general objects from synthetic data. We believe our dataset provides a rich testbed to study UDA for part segmentation and will help to significantly push forward research in this area."}}
{"id": "hLTZCN7f3M-", "cdate": 1621629979584, "mdate": null, "content": {"title": "Searching Parameterized AP Loss for Object Detection", "abstract": "Loss functions play an important role in training deep-network-based object detectors. The most widely used evaluation metric for object detection is Average Precision (AP), which captures the performance of localization and classification sub-tasks simultaneously. However, due to the non-differentiable nature of the AP metric, traditional object detectors adopt separate differentiable losses for the two sub-tasks. Such a mis-alignment issue may well lead to performance degradation. To address this, existing works seek to design surrogate losses for the AP metric manually, which requires expertise and may still be sub-optimal. In this paper, we propose Parameterized AP Loss, where parameterized functions are introduced to substitute the non-differentiable components in the AP calculation. Different AP approximations are thus represented by a family of parameterized functions in a unified formula. Automatic parameter search algorithm is then employed to search for the optimal parameters. Extensive experiments on the COCO benchmark with three different object detectors (i.e., RetinaNet, Faster R-CNN, and Deformable DETR) demonstrate that the proposed Parameterized AP Loss consistently outperforms existing handcrafted losses. Code shall be released."}}
