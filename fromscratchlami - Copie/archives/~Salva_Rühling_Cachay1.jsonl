{"id": "oqmtRXihYr", "cdate": 1690914834712, "mdate": 1690914834712, "content": {"title": "Accelerating Exploration of Marine Cloud Brightening Impacts on Tipping Points Using an AI Implementation of Fluctuation-Dissipation Theorem.", "abstract": "Marine cloud brightening (MCB) is a proposed climate intervention\ntechnology to partially offset greenhouse gas warming\nand possibly avoid crossing climate tipping points. The\nclimate impacts of MCB are typically estimated using computationally\nexpensive Earth System Model (ESM) simulations,\npreventing a thorough assessment of the large possibility\nspace of MCB intervention patterns. Here, we describe\nan AI model, named AiBEDO, that can be used to\nrapidly project climate responses via a novel application of\nthe Fluctuation-Dissipation Theorem (FDT). AiBEDO is a\nMultilayer Perceptron (MLP) model that maps from monthlymean\nradiation anomalies to surface climate anomalies at a\nrange of time lags. By leveraging a internal climate noise\nfrom a large existing dataset of ESM simulations, we use\nAiBEDO to construct an FDT operator that successfully\nprojects the pattern of MCB climate responses when evaluated\nagainst ESM simulations. We propose that AiBEDO\ncould be used to identify MCB forcing patterns to that reduce\ntipping point risks while minimizing negative side effects in\nother parts of the climate."}}
{"id": "MKJmYaxpIZ", "cdate": 1672531200000, "mdate": 1695961872228, "content": {"title": "DYffusion: A Dynamics-informed Diffusion Model for Spatiotemporal Forecasting", "abstract": "While diffusion models can successfully generate data and make predictions, they are predominantly designed for static images. We propose an approach for training diffusion models for dynamics forecasting that leverages the temporal dynamics encoded in the data, directly coupling it with the diffusion steps in the network. We train a stochastic, time-conditioned interpolator and a backbone forecaster network that mimic the forward and reverse processes of conventional diffusion models, respectively. This design choice naturally encodes multi-step and long-range forecasting capabilities, allowing for highly flexible, continuous-time sampling trajectories and the ability to trade-off performance with accelerated sampling at inference time. In addition, the dynamics-informed diffusion process imposes a strong inductive bias, allowing for improved computational efficiency compared to traditional Gaussian noise-based diffusion models. Our approach performs competitively on probabilistic skill score metrics in complex dynamics forecasting of sea surface temperatures, Navier-Stokes flows, and spring mesh systems."}}
{"id": "FZBtIpEAb5J", "cdate": 1629506255966, "mdate": null, "content": {"title": "ClimART: A Benchmark Dataset for Emulating Atmospheric Radiative Transfer in Weather and Climate Models", "abstract": "  Numerical simulations of Earth's weather and climate require substantial amounts of computation. This has led to a growing interest in replacing subroutines that explicitly compute physical processes with approximate machine learning (ML) methods that are fast at inference time. Within weather and climate models, atmospheric radiative transfer (RT) calculations are especially expensive. \n  This has made them a popular target for neural network-based emulators.  \n  However, prior work is hard to compare due to the lack of a comprehensive dataset and standardized best practices for ML benchmarking.\n  To fill this gap, we build a large dataset, ClimART, with more than 10 million samples from present, pre-industrial, and future climate conditions, based on the Canadian Earth System Model.\n  ClimART poses several methodological challenges for the ML community, such as multiple out-of-distribution test sets, underlying domain physics, and a trade-off between accuracy and inference speed.\n  We also present several novel baselines that indicate shortcomings of datasets and network architectures used in prior work."}}
{"id": "gbcsmD3Iznu", "cdate": 1621630086716, "mdate": null, "content": {"title": "End-to-End Weak Supervision", "abstract": "Aggregating multiple sources of weak supervision (WS) can ease the data-labeling bottleneck prevalent in many machine learning applications, by replacing the tedious manual collection of ground truth labels. \nCurrent state of the art approaches that do not use any labeled training data, however, require two separate modeling steps: Learning a probabilistic latent variable model based on the WS sources -- making assumptions that rarely hold in practice -- followed by downstream model training. \nImportantly, the first step of modeling does not consider the performance of the downstream model.\nTo address these caveats we propose an end-to-end approach for directly learning the downstream model by maximizing its agreement with probabilistic labels generated by reparameterizing previous probabilistic posteriors with a neural network. \nOur results show improved performance over prior work in terms of end model performance on downstream test sets, as well as in terms of improved robustness to dependencies among weak supervision sources. \n"}}
{"id": "hNhX8r5npbA", "cdate": 1609459200000, "mdate": 1667486153989, "content": {"title": "End-to-End Weak Supervision", "abstract": "Aggregating multiple sources of weak supervision (WS) can ease the data-labeling bottleneck prevalent in many machine learning applications, by replacing the tedious manual collection of ground truth labels. Current state of the art approaches that do not use any labeled training data, however, require two separate modeling steps: Learning a probabilistic latent variable model based on the WS sources -- making assumptions that rarely hold in practice -- followed by downstream model training. Importantly, the first step of modeling does not consider the performance of the downstream model.To address these caveats we propose an end-to-end approach for directly learning the downstream model by maximizing its agreement with probabilistic labels generated by reparameterizing previous probabilistic posteriors with a neural network. Our results show improved performance over prior work in terms of end model performance on downstream test sets, as well as in terms of improved robustness to dependencies among weak supervision sources."}}
{"id": "Tqi3vFqPTN2", "cdate": 1609459200000, "mdate": 1682642871681, "content": {"title": "The World as a Graph: Improving El Ni\u00f1o Forecasts with Graph Neural Networks", "abstract": "Deep learning-based models have recently outperformed state-of-the-art seasonal forecasting models, such as for predicting El Ni\\~no-Southern Oscillation (ENSO). However, current deep learning models are based on convolutional neural networks which are difficult to interpret and can fail to model large-scale atmospheric patterns. In comparison, graph neural networks (GNNs) are capable of modeling large-scale spatial dependencies and are more interpretable due to the explicit modeling of information flow through edge connections. We propose the first application of graph neural networks to seasonal forecasting. We design a novel graph connectivity learning module that enables our GNN model to learn large-scale spatial interactions jointly with the actual ENSO forecasting task. Our model, \\graphino, outperforms state-of-the-art deep learning-based models for forecasts up to six months ahead. Additionally, we show that our model is more interpretable as it learns sensible connectivity structures that correlate with the ENSO anomaly pattern."}}
{"id": "KcRyxRPhjyy", "cdate": 1609459200000, "mdate": 1681607059072, "content": {"title": "ClimART: A Benchmark Dataset for Emulating Atmospheric Radiative Transfer in Weather and Climate Models", "abstract": ""}}
