{"id": "z64kN1h1-rR", "cdate": 1652737632467, "mdate": null, "content": {"title": "Why So Pessimistic? Estimating Uncertainties for Offline RL through Ensembles, and Why Their Independence Matters", "abstract": "Motivated by the success of ensembles for uncertainty estimation in supervised learning, we take a renewed look at how ensembles of $Q$-functions can be leveraged as the primary source of pessimism for offline reinforcement learning (RL). We begin by identifying a critical flaw in a popular algorithmic choice used by many ensemble-based RL algorithms, namely the use of shared pessimistic target values when computing each ensemble member's Bellman error. Through theoretical analyses and construction of examples in toy MDPs, we demonstrate that shared pessimistic targets can paradoxically lead to value estimates that are effectively optimistic. Given this result, we propose MSG, a practical offline RL algorithm that trains an ensemble of $Q$-functions with independently computed targets based on completely separate networks, and optimizes a policy with respect to the lower confidence bound of predicted action values. Our experiments on the popular D4RL and RL Unplugged offline RL benchmarks demonstrate that on challenging domains such as antmazes, MSG with deep ensembles surpasses highly well-tuned state-of-the-art methods by a wide margin. Additionally, through ablations on benchmarks domains, we verify the critical significance of using independently trained $Q$-functions, and study the role of ensemble size. Finally, as using separate networks per ensemble member can become computationally costly with larger neural network architectures, we investigate whether efficient ensemble approximations developed for supervised learning can be similarly effective, and demonstrate that they do not match the performance and robustness of MSG with separate networks, highlighting the need for new efforts into efficient uncertainty estimation directed at RL."}}
{"id": "08Yk-n5l2Al", "cdate": 1652737360841, "mdate": null, "content": {"title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding", "abstract": "We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g., T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment."}}
{"id": "wQ7RCayXUSl", "cdate": 1632875759582, "mdate": null, "content": {"title": "Why so pessimistic? Estimating uncertainties for offline RL through ensembles, and why their independence matters.", "abstract": "In order to achieve strong performance in offline reinforcement learning (RL),  it is necessary to act conservatively with respect to confident lower-bounds on anticipated values of actions. Thus, a valuable approach would be to obtain high quality uncertainty estimates on action values. In current supervised learning literature, state-of-the-art approaches to uncertainty estimation and calibration rely on ensembling methods. In this work, we aim to transfer the success of ensembles from supervised learning to the setting of batch RL. We propose, MSG, a model-free dynamic programming based offline RL method that trains an ensemble of independent Q-functions, and updates a policy to act conservatively with respect to the uncertainties derived from the ensemble. Theoretically, by referring to the literature on infinite-width neural networks, we demonstrate the crucial dependence of the quality of uncertainty on the manner in which ensembling is performed, a phenomenon that arises due to the dynamic programming nature of RL and overlooked by existing offline RL methods. Our theoretical predictions are corroborated by pedagogical examples on toy MDPs, as well as empirical comparisons in benchmark continuous control domains. In the more challenging domains of the D4RL offline RL benchmark, MSG significantly surpasses highly well-tuned state-of-the-art methods in batch RL. Motivated by the success of MSG, we investigate whether efficient approximations to ensembles can be as effective. We demonstrate that while efficient variants outperform current state-of-the-art, they do not match MSG with deep ensembles. We hope our work engenders increased focus into deep network uncertainty estimation techniques directed for reinforcement learning."}}
{"id": "B8fp0LVMHa", "cdate": 1601308273454, "mdate": null, "content": {"title": "EMaQ: Expected-Max Q-Learning Operator for Simple Yet Effective Offline and Online RL", "abstract": "Off-policy reinforcement learning (RL) holds the promise of sample-efficient learning of decision-making policies by leveraging past experience. However, in the offline RL setting -- where a fixed collection of interactions are provided and no further interactions are allowed -- it has been shown that standard off-policy RL methods can significantly underperform. Recently proposed methods often aim to address this shortcoming by constraining learned policies to remain close to the given dataset of interactions. In this work, we closely investigate an important simplification of BCQ~\\citep{fujimoto2018off} -- a prior approach for offline RL -- which removes a heuristic design choice and naturally restrict extracted policies to remain \\emph{exactly} within the support of a given behavior policy. Importantly, in contrast to their original theoretical considerations, we derive this simplified algorithm through the introduction of a novel backup operator, Expected-Max Q-Learning (EMaQ), which is more closely related to the resulting practical algorithm. Specifically, in addition to the distribution support, EMaQ explicitly considers the number of samples and the proposal distribution, allowing us to derive new sub-optimality bounds which can serve as a novel measure of complexity for offline RL problems. In the offline RL setting -- the main focus of this work -- EMaQ matches and outperforms prior state-of-the-art in the D4RL benchmarks~\\citep{fu2020d4rl}. In the online RL setting, we demonstrate that EMaQ is competitive with Soft Actor Critic (SAC). The key contributions of our empirical findings are demonstrating the importance of careful generative model design for estimating behavior policies, and an intuitive notion of complexity for offline RL problems. With its simple interpretation and fewer moving parts, such as no explicit function approximator representing the policy, EMaQ serves as a strong yet easy to implement baseline for future work."}}
{"id": "Bbbdjb4aAbq", "cdate": 1600112128249, "mdate": null, "content": {"title": "A Divergence Minimization Perspective on Imitation Learning Methods", "abstract": "In many settings, it is desirable to learn decision-making and control policies through learning or bootstrapping from expert demonstrations. The\nmost common approaches under this Imitation Learning (IL) framework are Behavioural Cloning (BC), and Inverse Reinforcement Learning (IRL). Recent methods for IRL have demonstrated the capacity to learn effective policies with access to a very limited set of demonstrations, a scenario in which BC methods\noften fail. Unfortunately, due to multiple factors of variation, directly comparing\nthese methods does not provide adequate intuition for understanding this difference in performance. In this work, we present a unified probabilistic perspective\non IL algorithms based on divergence minimization. We present f-MAX, an fdivergence generalization of AIRL [1], a state-of-the-art IRL method. f-MAX\nenables us to relate prior IRL methods such as GAIL [2] and AIRL [1], and understand their algorithmic properties. Through the lens of divergence minimization we tease apart the differences between BC and successful IRL approaches,\nand empirically evaluate these nuances on simulated high-dimensional continuous control domains. Our findings conclusively identify that IRL\u2019s state-marginal\nmatching objective contributes most to its superior performance. Lastly, we apply our new understanding of IL method to the problem of state-marginal matching, where we demonstrate that in simulated arm pushing environments we can\nteach agents a diverse range of behaviours using simply hand-specified state distributions and no reward functions or expert demonstrations. For datasets and reproducing results please refer to https://github.com/KamyarGh/rl_swiss/\nblob/master/reproducing/fmax_paper.md."}}
{"id": "s9pzXLRdLwV", "cdate": 1600112023468, "mdate": null, "content": {"title": "EMaQ: Expected-Max Q-Learning Operator for Simple Yet Effective Offline and Online RL", "abstract": "Off-policy reinforcement learning (RL) holds the promise of sample-efficient\nlearning of decision-making policies by leveraging past experience. However, in\nthe offline RL setting \u2013 where a fixed collection of interactions are provided and\nno further interactions are allowed \u2013 it has been shown that standard off-policy\nRL methods can significantly underperform. Recently proposed methods aim to\naddress this shortcoming by regularizing learned policies to remain close to the\ngiven dataset of interactions. However, these methods involve several configurable\ncomponents such as learning a separate policy network on top of a behavior cloning\nactor, and explicitly constraining action spaces through clipping or reward penalties.\nStriving for simultaneous simplicity and performance, in this work we present\na novel backup operator, Expected-Max Q-Learning (EMaQ), which naturally\nrestricts learned policies to remain within the support of the offline dataset without\nany explicit regularization, while retaining desirable theoretical properties such\nas contraction. We demonstrate that EMaQ is competitive with Soft Actor Critic\n(SAC) in online RL, and surpasses SAC in the deployment-efficient setting. In\nthe offline RL setting \u2013 the main focus of this work \u2013 through EMaQ we are able\nto make important observations regarding key components of offline RL, and the\nnature of standard benchmark tasks. Lastly but importantly, we observe that EMaQ\nachieves state-of-the-art performance with fewer moving parts such as one less\nfunction approximation, making it a strong, yet easy to implement baseline for\nfuture work."}}
{"id": "Bkgya4HlUS", "cdate": 1567802550842, "mdate": null, "content": {"title": "SMILe: Scalable Meta Inverse Reinforcement Learning through Context-Conditional Policies", "abstract": "Imitation Learning (IL) has been successfully applied to complex sequential decision-making problems where standard Reinforcement Learning (RL) algorithms fail. A number of recent methods extend IL to few-shot learning scenarios, where a meta-trained policy learns to quickly master new tasks using limited demonstrations. However, although Inverse Reinforcement Learning (IRL) often outperforms Behavioral Cloning (BC) in terms of imitation quality, most of these approaches build on BC due to its simple optimization objective. In this work, we propose SMILe, a scalable framework for Meta Inverse Reinforcement Learning (Meta-IRL) based on maximum entropy IRL, which can learn high-quality policies from few demonstrations. We examine the efficacy of our method on a variety of high-dimensional simulated continuous control tasks and observe that SMILE significantly outperforms Meta-BC. To our knowledge, our approach is the first efficient method for Meta-IRL that scales to the intractable function approximator setting."}}
{"id": "rkeXrIIt_4", "cdate": 1553716795026, "mdate": null, "content": {"title": "Understanding the Relation Between Maximum-Entropy Inverse Reinforcement Learning and Behaviour Cloning", "abstract": "In many settings, it is desirable to learn decision-making and control policies through learning or from expert demonstrations. The most common approaches under this framework are Behaviour Cloning (BC), and Inverse Reinforcement Learning (IRL). Recent methods for IRL have demonstrated the capacity to learn effective policies with access to a very limited set of demonstrations, a scenario in which BC methods often fail. Unfortunately, directly comparing the algorithms for these methods does not provide adequate intuition for understanding this difference in performance. This is the motivating factor for our work. We begin by presenting $f$-MAX, a generalization of AIRL (Fu et al., 2018), a state-of-the-art IRL method. $f$-MAX provides grounds for more directly comparing the objectives for LfD. We demonstrate that $f$-MAX, and by inheritance AIRL, is a subset of the cost-regularized IRL framework laid out by Ho & Ermon (2016). We conclude by empirically evaluating the factors of difference between various LfD objectives in the continuous control domain."}}
{"id": "HkSm8t1PM", "cdate": 1518470684802, "mdate": null, "content": {"title": "Gradient-based Optimization of Neural Network Architecture", "abstract": "Neural networks can learn relevant features from data, but their predictive accuracy and propensity to overfit are sensitive to the values of the discrete hyperparameters that specify the network architecture (number of hidden layers, number of units per layer, etc.). Previous work optimized these hyperparmeters via grid search, random search, and black box optimization techniques such as Bayesian optimization. Bolstered by recent advances in gradient-based optimization of discrete stochastic objectives, we instead propose to directly model a distribution over possible architectures and use variational optimization to jointly optimize the network architecture and weights in one training pass. We discuss an implementation of this approach that estimates gradients via the Concrete relaxation, and show that it finds compact and accurate architectures for convolutional neural networks applied to the CIFAR10 and CIFAR100 datasets."}}
