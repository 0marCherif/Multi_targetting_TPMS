{"id": "YpyGV_i8Z_J", "cdate": 1652737344287, "mdate": null, "content": {"title": "Private Estimation with Public Data", "abstract": "We initiate the study of differentially private (DP) estimation with access to a small amount of public data. For private estimation of $d$-dimensional Gaussians, we assume that the public data comes from a Gaussian that may have vanishing similarity in total variation distance with the underlying Gaussian of the private data. We show that under the constraints of pure or concentrated DP, $d+1$ public data samples are sufficient to remove any dependence on the range parameters of the private data distribution from the private sample complexity, which is known to be otherwise necessary without public data. For separated Gaussian mixtures, we assume that the underlying public and private distributions are the same, and we consider two settings: (1) when given a dimension-independent amount of public data, the private sample complexity can be improved polynomially in terms of the number of mixture components, and any dependence on the range parameters of the distribution can be removed in the approximate DP case; (2) when given an amount of public data linear in the dimension, the private sample complexity can be made independent of range parameters even under concentrated DP, and additional improvements can be made to the overall sample complexity."}}
{"id": "c63eTNYh9Y", "cdate": 1652737344050, "mdate": null, "content": {"title": "New Lower Bounds for Private Estimation and a Generalized Fingerprinting Lemma", "abstract": "We prove new lower bounds for statistical estimation tasks under the constraint of $(\\varepsilon,\\delta)$-differential privacy. First, we provide tight lower bounds for private covariance estimation of Gaussian distributions. We show that estimating the covariance matrix in Frobenius norm requires $\\Omega(d^2)$ samples, and in spectral norm requires $\\Omega(d^{3/2})$ samples, both matching upper bounds up to logarithmic factors. We prove these bounds via our main technical contribution, a broad generalization of the fingerprinting method to exponential families. Additionally, using the private Assouad method of Acharya, Sun, and Zhang, we show a tight $\\Omega(d/(\\alpha^2 \\varepsilon))$ lower bound for estimating the mean of a distribution with bounded covariance to $\\alpha$-error in $\\ell_2$-distance. Prior known lower bounds for all these problems were either polynomially weaker or held under the stricter condition of $(\\varepsilon,0)$-differential privacy."}}
{"id": "YBanVDVEbVe", "cdate": 1621629822479, "mdate": null, "content": {"title": "Privately Learning Subspaces", "abstract": "Private data analysis suffers a costly curse of dimensionality. However, the data often has an underlying low-dimensional structure. For example, when optimizing via gradient descent, the gradients often lie in or near a low-dimensional subspace. If that low-dimensional structure can be identified, then we can avoid paying (in terms of privacy or accuracy) for the high ambient dimension. \n\nWe present differentially private algorithms that take input data sampled from a low-dimensional linear subspace (possibly with a small amount of error) and output that subspace (or an approximation to it). These algorithms can serve as a pre-processing step for other procedures."}}
{"id": "M69C7SCupYA", "cdate": 1621293629036, "mdate": null, "content": {"title": "Deterministic and Probabilistic Binary Search in Graphs", "abstract": "We consider the following natural generalization of Binary Search: in a given undirected, positively weighted graph, one vertex is a target. The algorithm's task is to identify the target by adaptively querying vertices. In response to querying a node q, the algorithm learns either that q is the target, or is given an edge out of q that lies on a shortest path from q to the target. We study this problem in a general noisy model in which each query independently receives a correct answer with probability p>12 (a known constant), and an (adversarial) incorrect one with probability 1\u2212p.\nOur main positive result is that when p=1 (i.e., all answers are correct), log2n queries are always sufficient. For general p, we give an (almost information-theoretically optimal) algorithm that uses, in expectation, no more than (1\u2212\u03b4)log2n1\u2212H(p)+o(logn)+O(log2(1/\u03b4)) queries, and identifies the target correctly with probability at leas 1\u2212\u03b4. Here, H(p)=\u2212(plogp+(1\u2212p)log(1\u2212p)) denotes the entropy. The first bound is achieved by the algorithm that iteratively queries a 1-median of the nodes not ruled out yet; the second bound by careful repeated invocations of a multiplicative weights algorithm.\nEven for p=1, we show several hardness results for the problem of determining whether a target can be found using K queries. Our upper bound of log2n implies a quasipolynomial-time algorithm for undirected connected graphs; we show that this is best-possible under the Strong Exponential Time Hypothesis (SETH). Furthermore, for directed graphs, or for undirected graphs with non-uniform node querying costs, the problem is PSPACE-complete. For a semi-adaptive version, in which one may query r nodes each in k rounds, we show membership in \u03a32k\u22121 in the polynomial hierarchy, and hardness for \u03a32k\u22125."}}
{"id": "7EjjGb7zmM", "cdate": 1577836800000, "mdate": null, "content": {"title": "Private Mean Estimation of Heavy-Tailed Distributions", "abstract": "We give new upper and lower bounds on the minimax sample complexity of differentially private mean estimation of distributions with bounded $k$-th moments. Roughly speaking, in the univariate case, we show that $$n = \\Theta\\left(\\frac{1}{\\alpha^2} + \\frac{1}{\\alpha^{\\frac{k}{k-1}}\\varepsilon}\\right)$$ samples are necessary and sufficient to estimate the mean to $\\alpha$-accuracy under $\\varepsilon$-differential privacy, or any of its common relaxations. This result demonstrates a qualitatively different behavior compared to estimation absent privacy constraints, for which the sample complexity is identical for all $k \\geq 2$. We also give algorithms for the multivariate setting whose sample complexity is a factor of $O(d)$ larger than the univariate case."}}
{"id": "UEiwhHd9jy", "cdate": 1546300800000, "mdate": null, "content": {"title": "Privately Learning High-Dimensional Distributions", "abstract": "We present novel, computationally efficient, and differentially private algorithms for two fundamental high-dimensional learning problems: learning a multivariate Gaussian and learning a product di..."}}
{"id": "8JG66PvEob5", "cdate": 1546300800000, "mdate": null, "content": {"title": "Differentially Private Algorithms for Learning Mixtures of Separated Gaussians", "abstract": "Learning the parameters of Gaussian mixture models is a fundamental and widely studied problem with numerous applications. In this work, we give new algorithms for learning the parameters of a high-dimensional, well separated, Gaussian mixture model subject to the strong constraint of differential privacy. In particular, we give a differentially private analogue of the algorithm of Achlioptas and McSherry. Our algorithm has two key properties not achieved by prior work: (1) The algorithm\u2019s sample complexity matches that of the corresponding non-private algorithm up to lower order terms in a wide range of parameters. (2) The algorithm requires very weak a priori bounds on the parameters of the mixture components."}}
