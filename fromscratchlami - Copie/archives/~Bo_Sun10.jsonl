{"id": "d_Q7YiGpkoz", "cdate": 1668539657960, "mdate": 1668539657960, "content": {"title": "Glissando-Net: Deep sinGLe vIew category level poSe eStimation ANd 3D recOnstruction", "abstract": "We present a deep learning model, dubbed Glissando-Net, to simultaneously estimate the pose and reconstruct the 3D\nshape of objects at the category level from a single RGB image. Previous works predominantly focused on either estimating poses\n(often at the instance level), or reconstructing shapes, but not both. Glissando-Net is composed of two auto-encoders that are jointly\ntrained, one for RGB images and the other for point clouds. We embrace two key design choices in Glissando-Net to achieve a more\naccurate prediction of the 3D shape and pose of the object given a single RGB image as input. First, we augment the feature maps of\nthe point cloud encoder and decoder with transformed feature maps from the image decoder, enabling effective 2D-3D interaction in\nboth training and prediction. Second, we predict both the 3D shape and pose of the object in the decoder stage. This way, we better\nutilize the information in the 3D point clouds presented only in the training stage to train the network for more accurate prediction. We\njointly train the two encoder-decoders for RGB and point cloud data to learn how to pass latent features to the point cloud decoder\nduring inference. In testing, the encoder of the 3D point cloud is discarded. The design of Glissando-Net is inspired by codeSLAM.\nUnlike codeSLAM, which targets 3D reconstruction of scenes, we focus on pose estimation and shape reconstruction of objects, and\ndirectly predict the object pose and a pose invariant 3D reconstruction without the need of the code optimization step. Extensive\nexperiments, involving both ablation studies and comparison with competing methods, demonstrate the efficacy of our proposed\nmethod, and compare favorably with the state-of-the-art."}}
{"id": "nfx93zELd_", "cdate": 1668536857438, "mdate": null, "content": {"title": "PRN: Panoptic Refinement Network", "abstract": "Panoptic segmentation is the task of uniquely assigning every pixel in an image to either a semantic label or an individual\nobject instance, generating a coherent and complete scene description. Many current panoptic segmentation methods, however, predict masks of semantic classes and object instances in separate branches, yielding inconsistent predictions. Moreover, because state-of-the-art panoptic segmentation models rely on box proposals, the instance masks predicted are often of low-resolution. To overcome these limitations, we propose the Panoptic Refinement Network (PRN), which takes masks from base panoptic segmentation models and refines them jointly to produce coherent results. PRN extends the offset map-based architecture of Panoptic-Deeplab with several novel ideas including a foreground mask and instance bounding box offsets, as well as coordinate convolutions for improved spatial prediction. Experimental results on COCO and Cityscapes show that PRN can significantly improve already accurate results from a variety of panoptic segmentation networks."}}
{"id": "DDdncbjRaA", "cdate": 1668528544222, "mdate": 1668528544222, "content": {"title": "Segmentation, Reconstruction and Recognition of Objects and Surfaces in 2D and 3D", "abstract": "Segmentation, reconstruction and recognition are classical problems in computer vision. In order to obtain an understanding of a given 2D image, we need to segment and recognize the objects through detection, instance and semantic segmentation. 2D information, however, is not enough for many computer vision applications. We need to reconstruct the 3D model of the corresponding objects in the 2D image to obtain shape information. To improve our awareness of a 3D scene, we also need the pose of each 3D object. Any 3D scene can be represented in terms of objects and surfaces, and planes are the most frequently seen type of surface, especially in man-made environments. In this dissertation, we present methods for segmentation, reconstruction and recognition in 2D and 3D data. First, we propose a method, named Oriented Point Sampling (OPS), for plane detection in 3D point clouds. Second, we present a deep learning model, dubbed Glissando-Net, to simultaneously estimate the pose and reconstruct the 3D shape of objects at the category level from a single RGB image. Last, we develop a mask refinement network for panoptic segmentation which can improve the predictions of existing panoptic segmentation methods."}}
{"id": "ezij4jGjD2r", "cdate": 1668528272944, "mdate": 1668528272944, "content": {"title": "Usability studies of an egocentric vision-based robotic wheelchair", "abstract": "Motivated by the need to improve the quality of life for the elderly and disabled individuals who rely on wheelchairs for mobility, and who may have limited or no hand functionality at all, we propose an egocentric computer vision based co-robot wheelchair to enhance their mobility without hand usage. The robot is built using a commercially available powered wheelchair modified to be controlled by head motion. Head motion is measured by tracking an egocentric camera mounted on the user\u2019s head and faces outward. Compared with previous approaches to hands-free mobility, our system provides a more natural human robot interface because it enables the user to control the speed and direction of motion in a continuous fashion, as opposed to providing a small number of discrete commands. This article presents three usability studies, which were conducted on 37 subjects. The first two usability studies focus on comparing the proposed control method with existing solutions while the third study was conducted to assess the effectiveness of training subjects to operate the wheelchair over several sessions. A limitation of our studies is that they have been conducted with healthy participants. Our findings, however, pave the way for further studies with subjects with disabilities."}}
{"id": "bqJ9ufqa9o", "cdate": 1668527868884, "mdate": 1668527868884, "content": {"title": "Learning to Navigate Robotic Wheelchairs from Demonstration: Is Training in Simulation Viable?", "abstract": "Learning from demonstration (LfD) enables robots to learn complex relationships between their state, perception and actions that are hard to express in an optimization framework. While people intuitively know what they would like to do in a given situation, they often have difficulty representing their decision process precisely enough to enable an implementation. Here, we are interested in robots that carry passengers, such as robotic wheelchairs, where user preferences, comfort and the feeling of safety are important for autonomous navigation. Balancing these requirements is not straightforward. While robots can be trained in an LfD framework in which users drive the robot according to their preferences, performing these demonstrations can be time-consuming, expensive, and possibly dangerous. Inspired by recent efforts for generating synthetic data for training autonomous driving systems, we investigate whether it is possible to train a robot based on simulations to reduce the time requirements, cost and potential risk. A key characteristic of our approach is that the input is not images, but the locations of people and obstacles relative to the robot. We argue that this allows us to transfer the classifier from the simulator to the physical world and to previously unseen environments that do not match the appearance of the training set. Experiments with 14 subjects providing physical and simulated demonstrations validate our claim."}}
{"id": "MnDEbF8R4Fw", "cdate": 1668527725939, "mdate": 1668527725939, "content": {"title": "A shared autonomy approach for wheelchair navigation based on learned user preferences", "abstract": "Research on robotic wheelchairs covers a broad range from complete autonomy to shared autonomy to manual navigation by a joystick or other means. Shared autonomy is valuable because it allows the user and the robot to complement each other, to correct each other's mistakes and to avoid collisions. In this paper, we present an approach that can learn to replicate path selection according to the wheelchair user's individual, often subjective, criteria in order to reduce the number of times the user has to intervene during automatic navigation. This is achieved by learning to rank paths using a support vector machine trained on selections made by the user in a simulator. If the classifier's confidence in the top ranked path is high, it is executed without requesting confirmation from the user. Otherwise, the choice is deferred to the user. Simulations and laboratory experiments using two path generation strategies demonstrate the effectiveness of our approach."}}
{"id": "-6eouvv2WY", "cdate": 1668527517429, "mdate": 1668527517429, "content": {"title": "Oriented point sampling for plane detection in unorganized point clouds", "abstract": "Plane detection in 3D point clouds is a crucial pre-processing step for applications such as point cloud segmentation, semantic mapping and SLAM. In contrast to many recent plane detection methods that are only applicable on organized point clouds, our work is targeted to unorganized point clouds that do not permit a 2D parametrization. We compare three methods for detecting planes in point clouds efficiently. One is a novel method proposed in this paper that generates plane hypotheses by sampling from a set of points with estimated normals. We named this method Oriented Point Sampling (OPS) to contrast with more conventional techniques that require the sampling of three unoriented points to generate plane hypotheses. We also implemented an efficient plane detection method based on local sampling of three unoriented points and compared it with OPS and the 3D-KHT algorithm, which is based on octrees, on the detection of planes on 10,000 point clouds from the SUN RGB-D dataset."}}
{"id": "vg4iopMUp3l", "cdate": 1668527295456, "mdate": 1668527295456, "content": {"title": "Shape-guided segmentation for fine-grained visual categorization", "abstract": "In this paper, we propose a shape-guided segmentation algorithm for fine-grained visual classification(FGVC). First, edge information is extracted from the query image and compared with each sample of training set, which can help us retrieve a subset of candidate proposals. These proposals are used to learn prior shape knowledge by separately estimating the foreground probabilities of corresponding pixels in the query image. Then, a redefined energy function is introduced to translate the minimum of energy to a good segmentation, with which we can dynamically pick out the most preferable proposal. After that, we obtain the label map of the image at the pixel level. Finally, the high-quality segmentation is used to aid locating semantic parts. We fine-tune one global model and two part models on Caffe to extract deep features and use a learned SVM classifier for categorization. We test three aspects in our experiment, including foreground segmentation, part localization and final classification. The results show that our method outperforms the state-of-the-art approaches on the famous Caltech-UCSD Birds 200-2011 dataset."}}
{"id": "tOPhq92QEg", "cdate": 1546300800000, "mdate": 1668660991976, "content": {"title": "Oriented Point Sampling for Plane Detection in Unorganized Point Clouds", "abstract": "Plane detection in 3D point clouds is a crucial pre-processing step for applications such as point cloud segmentation, semantic mapping and SLAM. In contrast to many recent plane detection methods that are only applicable on organized point clouds, our work is targeted to unorganized point clouds that do not permit a 2D parametrization. We compare three methods for detecting planes in point clouds efficiently. One is a novel method proposed in this paper that generates plane hypotheses by sampling from a set of points with estimated normals. We named this method Oriented Point Sampling (OPS) to contrast with more conventional techniques that require the sampling of three unoriented points to generate plane hypotheses. We also implemented an efficient plane detection method based on local sampling of three unoriented points and compared it with OPS and the 3D-KHT algorithm, which is based on octrees, on the detection of planes on 10,000 point clouds from the SUN RGB-D dataset."}}
{"id": "HcFLINQcIU-", "cdate": 1546300800000, "mdate": 1668660991981, "content": {"title": "Oriented Point Sampling for Plane Detection in Unorganized Point Clouds", "abstract": "Plane detection in 3D point clouds is a crucial pre-processing step for applications such as point cloud segmentation, semantic mapping and SLAM. In contrast to many recent plane detection methods that are only applicable on organized point clouds, our work is targeted to unorganized point clouds that do not permit a 2D parametrization. We compare three methods for detecting planes in point clouds efficiently. One is a novel method proposed in this paper that generates plane hypotheses by sampling from a set of points with estimated normals. We named this method Oriented Point Sampling (OPS) to contrast with more conventional techniques that require the sampling of three unoriented points to generate plane hypotheses. We also implemented an efficient plane detection method based on local sampling of three unoriented points and compared it with OPS and the 3D-KHT algorithm, which is based on octrees, on the detection of planes on 10,000 point clouds from the SUN RGB-D dataset."}}
