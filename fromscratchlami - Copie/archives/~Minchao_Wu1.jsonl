{"id": "96BH_uX8py_", "cdate": 1672531200000, "mdate": 1682964786692, "content": {"title": "Neural Network-Guided Synthesis of Recursive List Functions", "abstract": "Kobayashi et al. have recently proposed NeuGuS, a framework of neural-network-guided synthesis of logical formulas or simple program fragments, where a neural network is first trained based on data, and then a logical formula over integers is constructed by using the weights and biases of the trained network as hints. The previous method was, however, restricted the class of formulas of quantifier-free linear integer arithmetic. In this paper, we propose a NeuGuS method for the synthesis of recursive predicates over lists definable by using the left fold function. To this end, we design and train a special-purpose recurrent neural network (RNN), and use the weights of the trained RNN to synthesize a recursive predicate. We have implemented the proposed method and conducted preliminary experiments to confirm the effectiveness of the method."}}
{"id": "CBoRByFjMuy", "cdate": 1640995200000, "mdate": 1678011208300, "content": {"title": "A Bi-Directional Extensible Interface Between Lean and Mathematica", "abstract": ""}}
{"id": "edmYVRkYZv", "cdate": 1621629980366, "mdate": null, "content": {"title": "TacticZero: Learning to Prove Theorems from Scratch with Deep Reinforcement Learning", "abstract": "We propose a novel approach to interactive theorem-proving (ITP) using deep reinforcement learning. The proposed framework is able to learn proof search strategies as well as tactic and arguments prediction in an end-to-end manner. We formulate the process of ITP as a Markov decision process (MDP) in which each state represents a set of potential derivation paths. This structure allows us to introduce a novel backtracking mechanism which enables the agent to efficiently discard (predicted) dead-end derivations and restart the derivation from promising alternatives. We implement the framework in the HOL theorem prover. Experimental results show that the framework using learned search strategies outperforms existing automated theorem provers (i.e., hammers) available in HOL when evaluated on unseen problems. We further elaborate the role of key components of the framework using ablation studies.\n\n"}}
{"id": "-6xJn3LH05", "cdate": 1609459200000, "mdate": 1678011208272, "content": {"title": "TacticZero: Learning to Prove Theorems from Scratch with Deep Reinforcement Learning", "abstract": ""}}
{"id": "TGOVnhYs-RZ", "cdate": 1546300800000, "mdate": null, "content": {"title": "Verified Decision Procedures for Modal Logics", "abstract": "We describe a formalization of modal tableaux with histories for the modal logics K, KT and S4 in Lean. We describe how we formalized the static and transitional rules, the non-trivial termination and the correctness of loop-checks. The formalized tableaux are essentially executable decision procedures with soundness and completeness proved. Termination is also proved in order to define them as functions in Lean. All of these decision procedures return a concrete Kripke model in cases where the input set of formulas is satisfiable, and a proof constructed via the tableau rules witnessing unsatisfiability otherwise. We also describe an extensible formalization of backjumping and its verified implementation for the modal logic K. As far as we know, these are the first verified decision procedures for these modal logics."}}
