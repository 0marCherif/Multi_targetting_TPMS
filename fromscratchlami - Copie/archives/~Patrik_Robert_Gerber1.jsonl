{"id": "dWYy5wD0Dt", "cdate": 1672531200000, "mdate": 1682602212340, "content": {"title": "Fisher information lower bounds for sampling", "abstract": "We prove two lower bounds for the complexity of non-log-concave sampling within the framework of Balasubramanian et al. (2022), who introduced the use of Fisher information ($\\mathsf{FI}$) bounds as a notion of approximate first-order stationarity in sampling. Our first lower bound shows that averaged Langevin Monte Carlo (LMC) is optimal for the regime of large $\\mathsf{FI}$ by reducing the problem of finding stationary points in non-convex optimization to sampling. Our second lower bound shows that in the regime of small $\\mathsf{FI}$, obtaining a $\\mathsf{FI}$ of at most $\\varepsilon^2$ from the target distribution requires $\\text{poly}(1/\\varepsilon)$ queries, which is surprising as it rules out the existence of high-accuracy algorithms (e.g., algorithms using Metropolis{\u2013}Hastings filters) in this context."}}
{"id": "eaEFSjiv8-N", "cdate": 1640995200000, "mdate": 1682602212365, "content": {"title": "Gaussian discrepancy: A probabilistic relaxation of vector balancing", "abstract": ""}}
{"id": "_qdHPTMYNh", "cdate": 1640995200000, "mdate": 1682602212483, "content": {"title": "Fisher information lower bounds for sampling", "abstract": "We prove two lower bounds for the complexity of non-log-concave sampling within the framework of Balasubramanian et al. (2022), who introduced the use of Fisher information (FI) bounds as a notion of approximate first-order stationarity in sampling. Our first lower bound shows that averaged LMC is optimal for the regime of large FI by reducing the problem of finding stationary points in non-convex optimization to sampling. Our second lower bound shows that in the regime of small FI, obtaining a FI of at most $\\varepsilon^2$ from the target distribution requires $\\text{poly}(1/\\varepsilon)$ queries, which is surprising as it rules out the existence of high-accuracy algorithms (e.g., algorithms using Metropolis-Hastings filters) in this context."}}
{"id": "JqO8D1bxFW", "cdate": 1640995200000, "mdate": 1682602212325, "content": {"title": "Rejection sampling from shape-constrained distributions in sublinear time", "abstract": "We consider the task of generating exact samples from a target distribution, known up to normalization, over a finite alphabet. The classical algorithm for this task is rejection sampling, and although it has been used in practice for decades, there is surprisingly little study of its fundamental limitations. In this work, we study the query complexity of rejection sampling in a minimax framework for various classes of discrete distributions. Our results provide new algorithms for sampling whose complexity scales sublinearly with the alphabet size. When applied to adversarial bandits, we show that a slight modification of the EXP3 algorithm reduces the per-iteration complexity from O(K) to O(log(K) log(K/\\ensuremath{\\delta})) with probability 1-\\ensuremath{\\delta}, where K is the number of arms."}}
{"id": "6ziN6ybAL4k", "cdate": 1640995200000, "mdate": 1682602212429, "content": {"title": "Likelihood-free hypothesis testing", "abstract": "Consider the problem of binary hypothesis testing. Given $Z$ coming from either $\\mathbb P^{\\otimes m}$ or $\\mathbb Q^{\\otimes m}$, to decide between the two with small probability of error it is sufficient and in most cases necessary to have $m \\asymp 1/\\epsilon^2$, where $\\epsilon$ measures the separation between $\\mathbb P$ and $\\mathbb Q$ in total variation ($\\mathsf{TV}$). Achieving this, however, requires complete knowledge of the distributions and can be done, for example, using the Neyman-Pearson test. In this paper we consider a variation of the problem, which we call likelihood-free (or simulation-based) hypothesis testing, where access to $\\mathbb P$ and $\\mathbb Q$ is given through $n$ iid observations from each. In the case when $\\mathbb P,\\mathbb Q$ are assumed to belong to a non-parametric family $\\mathcal P$, we demonstrate the existence of a fundamental trade-off between $n$ and $m$ given by $nm \\asymp n^2_\\mathsf{GoF}(\\epsilon,\\cal P)$, where $n_\\mathsf{GoF}$ is the minimax sample complexity of testing between the hypotheses $H_0: \\mathbb P= \\mathbb Q$ vs $H_1: \\mathsf{TV}(\\mathbb P,\\mathbb Q) \\ge \\epsilon$. We show this for three families of distributions: $\\beta$-smooth densities supported on $[0,1]^d$, the Gaussian sequence model over a Sobolev ellipsoid, and the collection of distributions on alphabet $[k]=\\{1,2,\\dots,k\\}$ with pmfs bounded by $c/k$ for fixed $c$. For the larger family of all distributions on $[k]$ we obtain a more complicated trade-off that exhibits a phase-transition. The test that we propose, based on the $L^2$-distance statistic of Ingster, simultaneously achieves all points on the trade-off curve for the regular classes. This demonstrates the possibility of testing without fully estimating the distributions, provided $m\\gg1/\\epsilon^2$."}}
{"id": "6TlDrQ_1Ql", "cdate": 1640995200000, "mdate": 1682602212457, "content": {"title": "The query complexity of sampling from strongly log-concave distributions in one dimension", "abstract": "We establish the first tight lower bound of $\\Omega(\\log\\log\\kappa)$ on the query complexity of sampling from the class of strongly log-concave and log-smooth distributions with condition number $\\..."}}
{"id": "YV3uoawS5KK", "cdate": 1621630100542, "mdate": null, "content": {"title": "Averaging on the Bures-Wasserstein manifold: dimension-free convergence of gradient descent", "abstract": "We study first-order optimization algorithms for computing the barycenter of Gaussian distributions with respect to the optimal transport metric. Although the objective is geodesically non-convex, Riemannian gradient descent empirically converges rapidly, in fact faster than off-the-shelf methods such as Euclidean gradient descent and SDP solvers. This stands in stark contrast to the best-known theoretical results, which depend exponentially on the dimension. In this work, we prove new geodesic convexity results which provide stronger control of the iterates, yielding a dimension-free convergence rate. Our techniques also enable the analysis of two related notions of averaging, the entropically-regularized barycenter and the geometric median, providing the first convergence guarantees for these problems."}}
{"id": "vmO4wEm_Ak", "cdate": 1609459200000, "mdate": 1682602212460, "content": {"title": "The query complexity of sampling from strongly log-concave distributions in one dimension", "abstract": "We establish the first tight lower bound of $\\Omega(\\log\\log\\kappa)$ on the query complexity of sampling from the class of strongly log-concave and log-smooth distributions with condition number $\\kappa$ in one dimension. Whereas existing guarantees for MCMC-based algorithms scale polynomially in $\\kappa$, we introduce a novel algorithm based on rejection sampling that closes this doubly exponential gap."}}
{"id": "qQGEhoj9iYL", "cdate": 1609459200000, "mdate": 1682602212513, "content": {"title": "Rejection sampling from shape-constrained distributions in sublinear time", "abstract": "We consider the task of generating exact samples from a target distribution, known up to normalization, over a finite alphabet. The classical algorithm for this task is rejection sampling, and although it has been used in practice for decades, there is surprisingly little study of its fundamental limitations. In this work, we study the query complexity of rejection sampling in a minimax framework for various classes of discrete distributions. Our results provide new algorithms for sampling whose complexity scales sublinearly with the alphabet size. When applied to adversarial bandits, we show that a slight modification of the Exp3 algorithm reduces the per-iteration complexity from $\\mathcal O(K)$ to $\\mathcal O(\\log^2 K)$, where $K$ is the number of arms."}}
{"id": "S_4zMpiqhz", "cdate": 1609459200000, "mdate": 1682602212289, "content": {"title": "Averaging on the Bures-Wasserstein manifold: dimension-free convergence of gradient descent", "abstract": "We study first-order optimization algorithms for computing the barycenter of Gaussian distributions with respect to the optimal transport metric. Although the objective is geodesically non-convex, Riemannian GD empirically converges rapidly, in fact faster than off-the-shelf methods such as Euclidean GD and SDP solvers. This stands in stark contrast to the best-known theoretical results for Riemannian GD, which depend exponentially on the dimension. In this work, we prove new geodesic convexity results which provide stronger control of the iterates, yielding a dimension-free convergence rate. Our techniques also enable the analysis of two related notions of averaging, the entropically-regularized barycenter and the geometric median, providing the first convergence guarantees for Riemannian GD for these problems."}}
