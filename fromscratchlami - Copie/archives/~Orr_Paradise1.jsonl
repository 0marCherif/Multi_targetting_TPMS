{"id": "mtM5BCgN1Po", "cdate": 1668734792260, "mdate": null, "content": {"title": "Adversarial poisoning attacks on reinforcement learning-driven energy pricing", "abstract": "Reinforcement learning (RL) has emerged as a strong candidate for implementing complex controls in energy systems, such as energy pricing in microgrids. But what happens when some of the microgrid controllers are compromised by a malicious entity? We demonstrate a novel attack in RL.\nOur attack perturbs each trajectory to reverse the direction of the estimated gradient. We demonstrate that if data from a small fraction of microgrid controllers is adversarially perturbed, the learning of the RL agent can be significantly slowed or (with larger perturbations) caused to operate at a loss. Prosumers also face higher energy costs, use their batteries less, and suffer from higher peak demand when the pricing aggregator is adversarially poisoned.\n\nWe address this vulnerability with a \u201cdefense\u201d module; i.e., a ``robustification'' of RL algorithms against this attack. Our defense identifies the trajectories with the largest influence on the gradient and removes them from the training data."}}
{"id": "7CwGODn0gOZ", "cdate": 1664737657913, "mdate": null, "content": {"title": "A Theory of Unsupervised Translation for Understanding Animal Communication", "abstract": "Unsupervised translation refers to the challenging task of translating between two languages without parallel translations, i.e., from two separate monolingual corpora without a Rosetta stone. We propose an information-theoretic framework of unsupervised translation that models the case where the source language is that of highly intelligent animals, such as whales, and the target language is a human language, such as English. In particular, there may be limited quantities of source data, the source and target languages may be quite different in nature, and few assumptions are made on the source language syntax.\n\nWe apply our theory to a stylized setting of tree-based languages. Our analysis suggests that the amount of source data required for unsupervised translation is not significantly more than that of supervised translation. Our analysis is purely information-theoretic; issues of algorithmic efficiency are left for future work.\n\nWe are motivated by an ambitious initiative to translate whale communication using modern machine translation techniques. The recordings of whale communication that are being collected have no parallel human-language data."}}
{"id": "GisHNaleWiA", "cdate": 1652737861596, "mdate": null, "content": {"title": "Uni[MASK]: Unified Inference in Sequential Decision Problems", "abstract": "Randomly masking and predicting word tokens has been a successful approach in pre-training language models for a variety of downstream tasks. In this work, we observe that the same idea also applies naturally to sequential decision making, where many well-studied tasks like behavior cloning, offline RL, inverse dynamics, and waypoint conditioning correspond to different sequence maskings over a sequence of states, actions, and returns. We introduce the UniMASK framework, which provides a unified way to specify models which can be trained on many different sequential decision making tasks. We show that a single UniMASK model is often capable of carrying out many tasks with performance similar to or better than single-task models. Additionally, after fine-tuning, our UniMASK models consistently outperform comparable single-task models."}}
{"id": "rt-c0N6Vk-9", "cdate": 1646378294211, "mdate": null, "content": {"title": "Towards Flexible Inference in Sequential Decision Problems via Bidirectional Transformers", "abstract": "Randomly masking and predicting word tokens has been a successful approach in pre-training language models for a variety of downstream tasks. In this work, we observe that the same idea also applies naturally to sequential decision making, where many well-studied tasks like behavior cloning, offline RL, inverse dynamics, and waypoint conditioning correspond to different sequence maskings over a sequence of states, actions, and returns. We introduce the FlexiBiT framework, which provides a unified way to specify models which can be trained on many different sequential decision making tasks. We show that a single FlexiBiT model is simultaneously capable of carrying out many tasks with performance similar to or better than specialized models. Additionally, we show that performance can be further improved by fine-tuning our general model on specific tasks of interest."}}
{"id": "vKW9_1HTUbb", "cdate": 1640995200000, "mdate": 1681671132333, "content": {"title": "UniMASK: Unified Inference in Sequential Decision Problems", "abstract": "Randomly masking and predicting word tokens has been a successful approach in pre-training language models for a variety of downstream tasks. In this work, we observe that the same idea also applies naturally to sequential decision-making, where many well-studied tasks like behavior cloning, offline reinforcement learning, inverse dynamics, and waypoint conditioning correspond to different sequence maskings over a sequence of states, actions, and returns. We introduce the UniMASK framework, which provides a unified way to specify models which can be trained on many different sequential decision-making tasks. We show that a single UniMASK model is often capable of carrying out many tasks with performance similar to or better than single-task models. Additionally, after fine-tuning, our UniMASK models consistently outperform comparable single-task models. Our code is publicly available at https://github.com/micahcarroll/uniMASK."}}
{"id": "ok1_iyGnjE", "cdate": 1640995200000, "mdate": 1681760200834, "content": {"title": "Adversarial poisoning attacks on reinforcement learning-driven energy pricing", "abstract": "Complex controls are increasingly common in power systems. Reinforcement learning (RL) has emerged as a strong candidate for implementing various controllers. One common use of RL in this context is for prosumer pricing aggregations, where prosumers consist of buildings with both solar generation and energy storage. Specifically, supply and demand data serve as the observation space for many microgrid controllers acting based on a policy passed from a central RL agent. Each controller outputs an action space consisting of hourly \"buy\" and \"sell\" prices for energy throughout the day; in turn, each prosumer can choose whether to transact with the RL agent or the utility. The RL agent, who is learning online, is rewarded through its ability to generate a profit. We ask: what happens when some of the microgrid controllers are compromised by a malicious entity? We demonstrate a novel attack in RL and a simple defense against the attack. Our attack perturbs each trajectory to reverse the direction of the estimated gradient. We demonstrate that if data from a small fraction of microgrid controllers is adversarially perturbed, the learning of the RL agent can be significantly slowed. With larger perturbations, the RL aggregator can be manipulated to learn a catastrophic pricing policy that causes the RL agent to operate at a loss. Other environmental characteristics are worsened too: prosumers face higher energy costs, use their batteries less, and suffer from higher peak demand when the pricing aggregator is adversarially poisoned. We address this vulnerability with a \"defense\" module; i.e., a \"robustification\" of RL algorithms against this attack. Our defense identifies the trajectories with the largest influence on the gradient and removes them from the training data. It is computationally light and reasonable to include in any RL algorithm."}}
{"id": "kiJDNs1CoNX", "cdate": 1640995200000, "mdate": 1681760200896, "content": {"title": "A Theory of Unsupervised Translation Motivated by Understanding Animal Communication", "abstract": "Recent years have seen breakthroughs in neural language models that capture nuances of language, culture, and knowledge. Neural networks are capable of translating between languages -- in some cases even between two languages where there is little or no access to parallel translations, in what is known as Unsupervised Machine Translation (UMT). Given this progress, it is intriguing to ask whether machine learning tools can ultimately enable understanding animal communication, particularly that of highly intelligent animals. Our work is motivated by an ambitious interdisciplinary initiative, Project CETI, which is collecting a large corpus of sperm whale communications for machine analysis. We propose a theoretical framework for analyzing UMT when no parallel data are available and when it cannot be assumed that the source and target corpora address related subject domains or posses similar linguistic structure. The framework requires access to a prior probability distribution that should assign non-zero probability to possible translations. We instantiate our framework with two models of language. Our analysis suggests that accuracy of translation depends on the complexity of the source language and the amount of ``common ground'' between the source language and target prior. We also prove upper bounds on the amount of data required from the source language in the unsupervised setting as a function of the amount of data required in a hypothetical supervised setting. Surprisingly, our bounds suggest that the amount of source data required for unsupervised translation is comparable to the supervised setting. For one of the language models which we analyze we also prove a nearly matching lower bound. Our analysis is purely information-theoretic and as such can inform how much source data needs to be collected, but does not yield a computationally efficient procedure."}}
{"id": "AIS7pplKcf", "cdate": 1640995200000, "mdate": 1681671132534, "content": {"title": "Towards Flexible Inference in Sequential Decision Problems via Bidirectional Transformers", "abstract": "Randomly masking and predicting word tokens has been a successful approach in pre-training language models for a variety of downstream tasks. In this work, we observe that the same idea also applies naturally to sequential decision making, where many well-studied tasks like behavior cloning, offline RL, inverse dynamics, and waypoint conditioning correspond to different sequence maskings over a sequence of states, actions, and returns. We introduce the FlexiBiT framework, which provides a unified way to specify models which can be trained on many different sequential decision making tasks. We show that a single FlexiBiT model is simultaneously capable of carrying out many tasks with performance similar to or better than specialized models. Additionally, we show that performance can be further improved by fine-tuning our general model on specific tasks of interest."}}
{"id": "3481EI-kRKC", "cdate": 1640995200000, "mdate": 1684332538275, "content": {"title": "Uni[MASK]: Unified Inference in Sequential Decision Problems", "abstract": "Randomly masking and predicting word tokens has been a successful approach in pre-training language models for a variety of downstream tasks. In this work, we observe that the same idea also applies naturally to sequential decision making, where many well-studied tasks like behavior cloning, offline RL, inverse dynamics, and waypoint conditioning correspond to different sequence maskings over a sequence of states, actions, and returns. We introduce the UniMASK framework, which provides a unified way to specify models which can be trained on many different sequential decision making tasks. We show that a single UniMASK model is often capable of carrying out many tasks with performance similar to or better than single-task models. Additionally, after fine-tuning, our UniMASK models consistently outperform comparable single-task models."}}
{"id": "Xb_5APQ-rSc", "cdate": 1609459200000, "mdate": 1681760200894, "content": {"title": "Smooth and Strong PCPs", "abstract": "Probabilistically checkable proofs (PCPs) can be verified based only on a constant amount of random queries, such that any correct claim has a proof that is always accepted, and incorrect claims are rejected with high probability (regardless of the given alleged proof). We consider two possible features of PCPs: $$\\circ \\quad$$ \u2218 A PCP is strong if it rejects an alleged proof of a correct claimwith probability proportional to its distance from some correctproof of that claim. $$\\circ \\quad$$ \u2218 A PCP is smooth if each location in a proof is queried with equalprobability. We prove that all sets in $$\\mathcal{NP}$$ NP have PCPs that are both smooth andstrong, are of polynomial length and can be verified based on a constantnumber of queries. This is achieved by following the proof of thePCP theorem of Arora et al. (JACM 45(3):501\u2013555, 1998), providing astronger analysis of the Hadamard and Reed\u2013Muller based PCPs anda refined PCP composition theorem. In fact, we show that any set in $$\\mathcal{NP}$$ NP has a smooth strong canonical PCP of Proximity (PCPP), meaningthat there is an efficiently computable bijection of $$\\mathcal{NP}$$ NP witnesses to correct proofs. This improves on the recent construction of Dinur et al. (in: Blum (ed) 10th innovations in theoretical computer science conference, ITCS, San Diego, 2019) of PCPPs that are strong canonical but inherently non-smooth.Our result implies the hardness of approximating the satisfiability of \u201cstable\u201d 3CNF formulae with bounded variable occurrence, where stable means that the number of clauses violated by an assignment is proportional to its distance from a satisfying assignment (in the relative Hamming metric). This proves a hypothesis used in the work of Friggstad, Khodamoradi and Salavatipour (in: Chan (ed) Proceedings of the 30th annual ACM-SIAM symposium on discrete algorithms, SODA, San Diego, 2019), suggesting a connection between the hardness of these instances and other stable optimization problems."}}
