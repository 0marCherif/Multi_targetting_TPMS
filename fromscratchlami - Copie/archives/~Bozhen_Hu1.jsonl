{"id": "zGLH0fx6zAH", "cdate": 1672531200000, "mdate": 1699605541741, "content": {"title": "Deep Manifold Graph Auto-Encoder For Attributed Graph Embedding", "abstract": "Representing graph data in a low-dimensional space for subsequent tasks is the purpose of attributed graph embedding. Most existing neural network approaches learn latent representations by minimizing reconstruction errors. Rare work considers the data distribution and the topological structure of latent codes simultaneously, which often results in inferior embeddings in real-world graph data. This paper proposes a novel Deep Manifold (Variational) Graph Auto-Encoder (DMVGAE/DMGAE) method for attributed graph data to improve the stability and quality of learned representations to tackle the crowding problem. The node-to-node geodesic similarity is preserved between the original and latent space under a pre-defined distribution. The proposed method surpasses state-of-the-art baseline algorithms by a significant margin on different downstream tasks across popular datasets, which validates our solutions. We promise to release the code after acceptance."}}
{"id": "UKVT8y88g3r", "cdate": 1672531200000, "mdate": 1689144166424, "content": {"title": "Mole-BERT: Rethinking Pre-training Graph Neural Networks for Molecules", "abstract": ""}}
{"id": "OoWVaxfaa-5", "cdate": 1672531200000, "mdate": 1699605541714, "content": {"title": "Wordreg: Mitigating the Gap between Training and Inference with Worst-Case Drop Regularization", "abstract": "Dropout has emerged as one of the most frequently used techniques for training deep neural networks (DNNs). Although effective, the sampled sub-model by random dropout during training is inconsistent with the full model (without dropout) during inference. To mitigate this undesirable gap, we propose WordReg, a simple yet effective regularization built on dropout that enforces the consistency between the outputs of different sub-models sampled by dropout. Specifically, WordReg first obtains the worst-case dropout by maximizing the divergence between the outputs with two sub-models with different random dropouts. And then, it encourages the agreements between the outputs of the two sub-models with worstcase divergence. Extensive experiments on diverse DNNs and tasks reveal that WordReg can achieve notable and consistent improvements over non-regularized models and yields some state-of-the-art results. Theoretically, we verify that WordReg can reduce the gap between training and inference."}}
{"id": "Hfp0P_FHLt", "cdate": 1672531200000, "mdate": 1684040484824, "content": {"title": "Lightweight Contrastive Protein Structure-Sequence Transformation", "abstract": "Pretrained protein structure models without labels are crucial foundations for the majority of protein downstream applications. The conventional structure pretraining methods follow the mature natural language pretraining methods such as denoised reconstruction and masked language modeling but usually destroy the real representation of spatial structures. The other common pretraining methods might predict a fixed set of predetermined object categories, where a restricted supervised manner limits their generality and usability as additional labeled data is required to specify any other protein concepts. In this work, we introduce a novel unsupervised protein structure representation pretraining with a robust protein language model. In particular, we first propose to leverage an existing pretrained language model to guide structure model learning through an unsupervised contrastive alignment. In addition, a self-supervised structure constraint is proposed to further learn the intrinsic information about the structures. With only light training data, the pretrained structure model can obtain better generalization ability. To quantitatively evaluate the proposed structure models, we design a series of rational evaluation methods, including internal tasks (e.g., contact map prediction, distribution alignment quality) and external/downstream tasks (e.g., protein design). The extensive experimental results conducted on multiple tasks and specific datasets demonstrate the superiority of the proposed sequence-structure transformation framework."}}
{"id": "5GIA94XyA5", "cdate": 1672531200000, "mdate": 1699605541711, "content": {"title": "Global-Context Aware Generative Protein Design", "abstract": "The linear sequence of amino acids determines protein structure and function. Protein design, known as the inverse of protein structure prediction, aims to obtain a novel protein sequence that will fold into the defined structure. Recent works on computational protein design have studied designing sequences for the desired backbone structure with local positional information and achieved competitive performance. However, similar local environments in different backbone structures may result in different amino acids, which indicates the global context of protein structure matters. Thus, we propose the Global-Context Aware generative de novo protein design method (GCA), consisting of local modules and global modules. While local modules focus on relationships between neighbor amino acids, global modules explicitly capture non-local contexts. Experimental results demonstrate that the proposed GCA method achieves state-of-the-art performance on structure-based protein design. Our code and pretrained model have been released on Github <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> ."}}
{"id": "jevY-DtiZTR", "cdate": 1663850553297, "mdate": null, "content": {"title": "Mole-BERT: Rethinking Pre-training Graph Neural Networks for Molecules", "abstract": "Recent years have witnessed the prosperity of pre-training graph neural networks (GNNs) for molecules. Typically, atom types as node attributes are randomly masked, and GNNs are then trained to predict masked types as in AttrMask \\citep{hu2020strategies}, following the Masked Language Modeling (MLM) task of BERT~\\citep{devlin2019bert}. However, unlike MLM with a large vocabulary, the AttrMask pre-training does not learn informative molecular representations due to small and unbalanced atom `vocabulary'. To amend this problem, we propose a variant of VQ-VAE~\\citep{van2017neural} as a context-aware tokenizer to encode atom attributes into chemically meaningful discrete codes. This can enlarge the atom vocabulary size and mitigate the quantitative divergence between dominant (e.g., carbons) and rare atoms (e.g., phosphorus). With the enlarged atom `vocabulary', we propose a novel node-level pre-training task, dubbed Masked Atoms Modeling (\\textbf{MAM}), to mask some discrete codes randomly and then pre-train GNNs to predict them. MAM also mitigates another issue of AttrMask, namely the negative transfer. It can be easily combined with various pre-training tasks to improve their performance. Furthermore, we propose triplet masked contrastive learning (\\textbf{TMCL}) for graph-level pre-training to model the heterogeneous semantic similarity between molecules for effective molecule retrieval. MAM and TMCL constitute a novel pre-training framework, \\textbf{Mole-BERT}, which can match or outperform state-of-the-art methods in a fully data-driven manner. We release the code at \\textcolor{magenta}{\\url{https://github.com/junxia97/Mole-BERT}}."}}
{"id": "moPHq03kuVA", "cdate": 1648688114473, "mdate": 1648688114473, "content": {"title": "SimGRACE: A Simple Framework for Graph Contrastive Learning without Data Augmentation", "abstract": "Graph contrastive learning (GCL) has emerged as a dominant technique for graph representation learning which maximizes the mutual information between paired graph augmentations that share the same semantics. Unfortunately, it is difficult to preserve semantics well during augmentations in view of the diverse nature of graph data. Currently, data augmentations in GCL that are designed to preserve semantics broadly fall into three unsatisfactory ways. First, the augmentations can be manually picked per dataset\nby trial-and-errors. Second, the augmentations can be selected via cumbersome search. Third, the augmentations can be obtained by\nintroducing expensive domain-specific knowledge as guidance. All of these limit the efficiency and more general applicability of existing GCL methods. To circumvent these crucial issues, we propose a Simple framework for GRAph Contrastive lEarning, SimGRACE\nfor brevity, which does not require data augmentations. Specifically, we take original graph as input and GNN model with its perturbed\nversion as two encoders to obtain two correlated views for contrast. SimGRACE is inspired by the observation that graph data\ncan preserve their semantics well during encoder perturbations while not requiring manual trial-and-errors, cumbersome search\nor expensive domain knowledge for augmentations selection. Also, we explain why SimGRACE can succeed. Furthermore, we devise\nadversarial training scheme, dubbed AT-SimGRACE, to enhance the robustness of graph contrastive learning and theoretically explain the reasons. Albeit simple, we show that SimGRACE can yield competitive or better performance compared with state-of-the-art\nmethods in terms of generalizability, transferability and robustness, while enjoying unprecedented degree of flexibility and efficiency."}}
{"id": "o46DsZ1DNrA", "cdate": 1640995200000, "mdate": 1648688213039, "content": {"title": "SimGRACE: A Simple Framework for Graph Contrastive Learning without Data Augmentation", "abstract": "Graph contrastive learning (GCL) has emerged as a dominant technique for graph representation learning which maximizes the mutual information between paired graph augmentations that share the same semantics. Unfortunately, it is difficult to preserve semantics well during augmentations in view of the diverse nature of graph data. Currently, data augmentations in GCL that are designed to preserve semantics broadly fall into three unsatisfactory ways. First, the augmentations can be manually picked per dataset by trial-and-errors. Second, the augmentations can be selected via cumbersome search. Third, the augmentations can be obtained by introducing expensive domain-specific knowledge as guidance. All of these limit the efficiency and more general applicability of existing GCL methods. To circumvent these crucial issues, we propose a \\underline{Sim}ple framework for \\underline{GRA}ph \\underline{C}ontrastive l\\underline{E}arning, \\textbf{SimGRACE} for brevity, which does not require data augmentations. Specifically, we take original graph as input and GNN model with its perturbed version as two encoders to obtain two correlated views for contrast. SimGRACE is inspired by the observation that graph data can preserve their semantics well during encoder perturbations while not requiring manual trial-and-errors, cumbersome search or expensive domain knowledge for augmentations selection. Also, we explain why SimGRACE can succeed. Furthermore, we devise adversarial training scheme, dubbed \\textbf{AT-SimGRACE}, to enhance the robustness of graph contrastive learning and theoretically explain the reasons. Albeit simple, we show that SimGRACE can yield competitive or better performance compared with state-of-the-art methods in terms of generalizability, transferability and robustness, while enjoying unprecedented degree of flexibility and efficiency."}}
{"id": "dsoBXq4z06", "cdate": 1640995200000, "mdate": 1674005375564, "content": {"title": "SimGRACE: A Simple Framework for Graph Contrastive Learning without Data Augmentation", "abstract": ""}}
{"id": "TxPZeO8dAr", "cdate": 1640995200000, "mdate": 1674005375566, "content": {"title": "Protein Language Models and Structure Prediction: Connection and Progression", "abstract": ""}}
