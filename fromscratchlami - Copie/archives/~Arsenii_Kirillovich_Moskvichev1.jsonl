{"id": "p0idPKcUc8i", "cdate": 1672531200000, "mdate": 1689281351225, "content": {"title": "Narrative XL: A Large-scale Dataset For Long-Term Memory Models", "abstract": "Despite their tremendous successes, most large language models do not have any long-term memory mechanisms, which restricts their applications. Overcoming this limitation would not only require changes to the typical transformer architectures or training procedures, but also a dataset on which these new models could be trained and evaluated. We argue that existing resources lack a few key properties, and that at present, there are no naturalistic datasets of sufficient scale to train (and not only evaluate) long-term memory language models. We then present our solution that capitalizes on the advances in short-term memory language models to create such a dataset. Using GPT 3.5, we summarized each scene in 1500 hand-curated books from Project Gutenberg, which resulted in approximately 150 scene-level summaries per book. We then created a number of reading comprehension questions based on these summaries, including three types of multiple-choice scene recognition questions, as well as free-form narrative reconstruction questions. Each book is thus associated with more than 500 reading comprehension questions. Crucially, most questions have a known ``retention demand'', indicating how long-term of a memory is needed to answer it, which should aid long-term memory performance evaluation. We validate our data in three small-scale experiments: one with human labelers, and two with existing language models. We show that our questions 1) adequately represent the source material 2) can be used to diagnose the model's memory capacity 3) are not trivial for modern language models even when the memory demand does not exceed those models' context lengths. Lastly, we provide our code which can be used to further expand the dataset in an automated manner."}}
{"id": "Y8iNORh-ER", "cdate": 1672531200000, "mdate": 1689281351255, "content": {"title": "The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain", "abstract": "The abilities to form and abstract concepts is key to human intelligence, but such abilities remain lacking in state-of-the-art AI systems. There has been substantial research on conceptual abstraction in AI, particularly using idealized domains such as Raven's Progressive Matrices and Bongard problems, but even when AI systems succeed on such problems, the systems are rarely evaluated in depth to see if they have actually grasped the concepts they are meant to capture. In this paper we describe an in-depth evaluation benchmark for the Abstraction and Reasoning Corpus (ARC), a collection of few-shot abstraction and analogy problems developed by Chollet [2019]. In particular, we describe ConceptARC, a new, publicly available benchmark in the ARC domain that systematically assesses abstraction and generalization abilities on a number of basic spatial and semantic concepts. ConceptARC differs from the original ARC dataset in that it is specifically organized around \"concept groups\" -- sets of problems that focus on specific concepts and that are vary in complexity and level of abstraction. We report results on testing humans on this benchmark as well as three machine solvers: the top two programs from a 2021 ARC competition and OpenAI's GPT-4. Our results show that humans substantially outperform the machine solvers on this benchmark, showing abilities to abstract and generalize concepts that are not yet captured by AI systems. We believe that this benchmark will spur improvements in the development of AI systems for conceptual abstraction and in the effective evaluation of such systems."}}
{"id": "TUo6aFZIWkF", "cdate": 1640995200000, "mdate": 1682635871537, "content": {"title": "Language-Based Learning: Cognitive and Computational Perspective", "abstract": "Author(s): Moskvichev, Arsenii | Advisor(s): Steyvers, Mark | Abstract: This thesis focuses on a challenging and long-standing problem of learning from language, in other words, how humans or machines may use language to share and acquire knowledge. The work has three distinct parts. First, I review how different disciplines define and approach the problem of learning from language and argue that a number of areas in Cognitive Science and Computer Science research have recently advanced enough to begin to tackle this challenge. Second, I present a series of three behavioral experiments studying the problem of learning from language in the context of pedagogical category communication. The experiments demonstrate the flexibility of verbal communication as a means for sharing category knowledge, as well as the advantage of mixing communication media (verbal and exemplar-based) as opposed to relying on any one isolated channel. In the last part of the dissertation, I focus on the question of how modern AI architectures can be adapted and applied to the problem of lifelong learning from language. In particular, I identify the types of operations that the model should be able to make, and propose a training procedure and an architecture that support learning such operations in an end-to-end fashion. I test the architecture on a number of simulated non-linguistic domains, leaving its NLP applications to future research. Although it is only a small step towards creating a fully functioning learning from language model, I still believe that this step is important."}}
{"id": "Ndffz5uo6H", "cdate": 1632875767240, "mdate": null, "content": {"title": "Updater-Extractor Architecture for Inductive World State Representations", "abstract": "Developing sequential models traditionally involves two stages - training and application. Retention of information acquired after training (at application time) is architecturally limited by the size of the model's context window (in the case of transformers), or by the practical difficulties associated with long sequences (in the case of RNNs). In this paper, we propose a novel transformer-based Updater-Extractor architecture that can work with sequences of arbitrary length and refine its long-term knowledge about the world based on inputs at application time. We explicitly train the model to incorporate incoming information into its world state representation, obtaining strong inductive generalization and the ability to handle extremely long-range dependencies. We propose a novel one-step training procedure that makes such training feasible, and prove a lemma that provides theoretical justification for this training procedure. Empirically, we investigate the model performance on a variety of different tasks: we use two new simulated tasks tasks to study the model's ability to handle extremely long-range dependencies, we demonstrate competitive performance on the challenging Pathfinder problem using vanilla attention."}}
{"id": "lMc-SocTNM", "cdate": 1609459200000, "mdate": 1682635871536, "content": {"title": "Updater-Extractor Architecture for Inductive World State Representations", "abstract": "Developing NLP models traditionally involves two stages - training and application. Retention of information acquired after training (at application time) is architecturally limited by the size of the model's context window (in the case of transformers), or by the practical difficulties associated with long sequences (in the case of RNNs). In this paper, we propose a novel transformer-based Updater-Extractor architecture and a training procedure that can work with sequences of arbitrary length and refine its knowledge about the world based on linguistic inputs. We explicitly train the model to incorporate incoming information into its world state representation, obtaining strong inductive generalization and the ability to handle extremely long-range dependencies. We prove a lemma that provides a theoretical basis for our approach. The result also provides insight into success and failure modes of models trained with variants of Truncated Back-Propagation Through Time (such as Transformer XL). Empirically, we investigate the model performance on three different tasks, demonstrating its promise. This preprint is still a work in progress. At present, we focused on easily interpretable tasks, leaving the application of the proposed ideas to practical NLP applications for the future."}}
{"id": "K9YMQIu-eDx", "cdate": 1591975449890, "mdate": null, "content": {"title": "Reinforcement Communication Learning in Different Social Network Structures", "abstract": "Social network structure is one of the key determinants of human language evolution. Previous work has shown that the network of social interactions shapes decentralized learning in human groups, leading to the emergence of different kinds of communicative conventions. We examined the effects of social network organization on the properties of communication systems emerging in decentralized, multi-agent reinforcement learning communities. We found that the global connectivity of a social network drives the convergence of populations on shared and symmetric communication systems, preventing the agents from forming many local ``dialects\". Moreover, the agent's degree is inversely related to the consistency of its use of communicative conventions. These results show the importance of the basic properties of social network structure on reinforcement communication learning and suggest a new interpretation of findings on human convergence on word conventions."}}
{"id": "xOqrN3-ks6E", "cdate": 1577836800000, "mdate": 1682635871537, "content": {"title": "Reinforcement Communication Learning in Different Social Network Structures", "abstract": "Social network structure is one of the key determinants of human language evolution. Previous work has shown that the network of social interactions shapes decentralized learning in human groups, leading to the emergence of different kinds of communicative conventions. We examined the effects of social network organization on the properties of communication systems emerging in decentralized, multi-agent reinforcement learning communities. We found that the global connectivity of a social network drives the convergence of populations on shared and symmetric communication systems, preventing the agents from forming many local \"dialects\". Moreover, the agent's degree is inversely related to the consistency of its use of communicative conventions. These results show the importance of the basic properties of social network structure on reinforcement communication learning and suggest a new interpretation of findings on human convergence on word conventions."}}
{"id": "LIjz5SMemC", "cdate": 1577836800000, "mdate": 1682635871539, "content": {"title": "Effects of Supervision, Population Size, and Self-Play on Multi-Agent Reinforcement Learning to Communicate", "abstract": ""}}
{"id": "v13sxEuHqWB", "cdate": 1546300800000, "mdate": 1682635871557, "content": {"title": "Adaptation Aftereffects as a Result of Bayesian Categorization", "abstract": "We propose a unified explanation of contrastive and assimilative adaptation aftereffects from the perspective of higher-level cognitive processes: rational category learning and categorical perception. We replicate (twice) previously reported assimilative and contrastive effects (Uznadze illusion in visual modality), propose a rational computational model of the process, and evaluate our model performance against the Bayesian logistic regression baseline. We conclude by discussing theoretical implications of our study and directions for further research."}}
{"id": "fyU41Jb3Gy_", "cdate": 1546300800000, "mdate": null, "content": {"title": "Adaptation Aftereffects as a Result of Bayesian Categorization.", "abstract": "We propose a unified explanation of contrastive and assimilative adaptation aftereffects from the perspective of higher-level cognitive processes: rational category learning and categorical perception. We replicate (twice) previously reported assimilative and contrastive effects (Uznadze illusion in visual modality), propose a rational computational model of the process, and evaluate our model performance against the Bayesian logistic regression baseline. We conclude by discussing theoretical implications of our study and directions for further research."}}
