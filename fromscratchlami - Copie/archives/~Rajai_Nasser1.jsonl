{"id": "KhY2JCHj6UI", "cdate": 1672531200000, "mdate": 1680275593740, "content": {"title": "Higher degree sum-of-squares relaxations robust against oblivious outliers", "abstract": ""}}
{"id": "vspRe_Hsu3Y", "cdate": 1640995200000, "mdate": 1682939381517, "content": {"title": "Optimal SQ Lower Bounds for Learning Halfspaces with Massart Noise", "abstract": "We give tight statistical query (SQ) lower bounds for learnining halfspaces in the presence of Massart noise. In particular, suppose that all labels are corrupted with probability at most $\\eta$. We show that for arbitrary $\\eta \\in [0,1/2]$ every SQ algorithm achieving misclassification error better than $\\eta$ requires queries of superpolynomial accuracy or at least a superpolynomial number of queries. Further, this continues to hold even if the information-theoretically optimal error $\\mathrm{OPT}$ is as small as $\\exp\\left(-\\log^c(d)\\right)$, where $d$ is the dimension and $0 < c < 1$ is an arbitrary absolute constant, and an overwhelming fraction of examples are noiseless. Our lower bound matches known polynomial time algorithms, which are also implementable in the SQ framework. Previously, such lower bounds only ruled out algorithms achieving error $\\mathrm{OPT} + \\epsilon$ or error better than $\\Omega(\\eta)$ or, if $\\eta$ is close to $1/2$, error $\\eta - o_\\eta(1)$, where the term $o_\\eta(1)$ is constant in $d$ but going to 0 for $\\eta$ approaching $1/2$. As a consequence, we also show that achieving misclassification error better than $1/2$ in the $(A,\\alpha)$-Tsybakov model is SQ-hard for $A$ constant and $\\alpha$ bounded away from 1."}}
{"id": "_d6-qShwAXS", "cdate": 1640995200000, "mdate": 1682939381518, "content": {"title": "Optimal Age Over Erasure Channels", "abstract": "Previous works on age of information and erasure channels have dealt with specific models and computed the average age or average peak age for certain settings. In this paper, given a source that produces a letter every <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$T_{s}$ </tex-math></inline-formula> seconds and an erasure channel that can be used every <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$T_{c}$ </tex-math></inline-formula> seconds, we ask what is the coding strategy that minimizes the time-average \u201cage of information\u201d that an observer of the channel output incurs. We first analyze the case where the source alphabet and the channel-input alphabet have the same size. We show that a trivial coding strategy is optimal and a closed form expression for the age can be derived. We then analyze the case where the alphabets have different sizes. We use a random coding argument to bound the average age and show that the average age achieved using random codes converges to the optimal average age of linear block codes as the source alphabet becomes large."}}
{"id": "DpVF-jIX3I4", "cdate": 1640995200000, "mdate": 1682413153678, "content": {"title": "Higher degree sum-of-squares relaxations robust against oblivious outliers", "abstract": "We consider estimation models of the form $Y=X^*+N$, where $X^*$ is some $m$-dimensional signal we wish to recover, and $N$ is symmetrically distributed noise that may be unbounded in all but a small $\\alpha$ fraction of the entries. We introduce a family of algorithms that under mild assumptions recover the signal $X^*$ in all estimation problems for which there exists a sum-of-squares algorithm that succeeds in recovering the signal $X^*$ when the noise $N$ is Gaussian. This essentially shows that it is enough to design a sum-of-squares algorithm for an estimation problem with Gaussian noise in order to get the algorithm that works with the symmetric noise model. Our framework extends far beyond previous results on symmetric noise models and is even robust to adversarial perturbations. As concrete examples, we investigate two problems for which no efficient algorithms were known to work for heavy-tailed noise: tensor PCA and sparse PCA. For the former, our algorithm recovers the principal component in polynomial time when the signal-to-noise ratio is at least $\\tilde{O}(n^{p/4}/\\alpha)$, that matches (up to logarithmic factors) current best known algorithmic guarantees for Gaussian noise. For the latter, our algorithm runs in quasipolynomial time and matches the state-of-the-art guarantees for quasipolynomial time algorithms in the case of Gaussian noise. Using a reduction from the planted clique problem, we provide evidence that the quasipolynomial time is likely to be necessary for sparse PCA with symmetric noise. In our proofs we use bounds on the covering numbers of sets of pseudo-expectations, which we obtain by certifying in sum-of-squares upper bounds on the Gaussian complexities of sets of solutions. This approach for bounding the covering numbers of sets of pseudo-expectations may be interesting in its own right and may find other application in future works."}}
{"id": "8UPPtE9NTi", "cdate": 1640995200000, "mdate": 1682939381517, "content": {"title": "Optimal SQ Lower Bounds for Learning Halfspaces with Massart Noise", "abstract": "We give tight statistical query (SQ) lower bounds for learnining halfspaces in the presence of Massart noise. In particular, suppose that all labels are corrupted with probability at most $\\eta$. W..."}}
{"id": "5Na5ZP4QwSj", "cdate": 1640995200000, "mdate": 1682939381519, "content": {"title": "Age Distribution in Arbitrary Preemptive Memoryless Networks", "abstract": "We study the probability distribution of age of information (AoI) in arbitrary networks with memoryless service times. A source node generates packets following a Poisson process, and then the packets are forwarded across the network in such a way that newer updates preempt older ones. This model is equivalent to gossip networks that were recently studied by Yates, and for which he obtained a recursive formula allowing the computation for the average AoI. In this paper, we obtain a very simple characterization of the stationary distribution of AoI at every node in the network. This allows for the computation of the average of an arbitrary function of the age, such as the age-violation probabilities. Furthermore, we show how our simple characterization can yield substantial reductions in the computation time of average AoIs in some structured networks. Finally, we describe how it can yield faster and more accurate Monte Carlo simulations estimating the average AoI, or the average of an arbitrary function of the age."}}
{"id": "1s_8EljBdfM", "cdate": 1640995200000, "mdate": 1682939381522, "content": {"title": "Age Distribution in Arbitrary Preemptive Memoryless Networks", "abstract": "We study the probability distribution of age of information (AoI) in arbitrary networks with memoryless service times. A source node generates packets following a Poisson process, and then the packets are forwarded across the network in such a way that newer updates preempt older ones. This model is equivalent to gossip networks that was recently studied by Yates, and for which he obtained a recursive formula allowing the computation for the average AoI. In this paper, we obtain a very simple characterization of the stationary distribution of AoI at every node in the network. This allows for the computation of the average of an arbitrary function of the age. In particular, we can compute age-violation probabilities. Furthermore, we show how it is possible to use insights from our simple characterization in order to substantially reduce the computation time of average AoIs in some structured networks. Finally, we describe how it is possible to use our characterization in order to obtain faster and more accurate Monte Carlo simulations estimating the average AoI, or the average of an arbitrary function of the age."}}
{"id": "BaHth99Sp45", "cdate": 1621630045081, "mdate": null, "content": {"title": "Consistent Estimation for PCA and Sparse Regression with Oblivious Outliers", "abstract": "We develop machinery to design efficiently computable and \\emph{consistent} estimators, achieving estimation error approaching zero as the number of observations grows, when facing an oblivious adversary that may corrupt responses in all but an $\\alpha$ fraction of the samples.\nAs concrete examples, we investigate two problems: \nsparse regression and principal component analysis (PCA).\nFor sparse regression, we achieve consistency for optimal sample size $n\\gtrsim (k\\log d)/\\alpha^2$ \nand optimal error rate $O(\\sqrt{(k\\log d)/(n\\cdot \\alpha^2)})$\nwhere $n$ is the number of observations, $d$ is the number of dimensions and $k$ is the sparsity of the parameter vector, allowing the fraction of inliers to be inverse-polynomial in the number of samples.\nPrior to this work, no estimator was known to be consistent when the fraction of inliers $\\alpha$ is $o(1/\\log \\log n)$, even for (non-spherical) Gaussian design matrices.\nResults holding under weak design assumptions and in the presence of such general noise have only been shown in dense setting (i.e., general linear regression) very recently by d'Orsi et al.~\\cite{ICML-linear-regression}.\nIn the context of PCA, we attain optimal error guarantees under broad spikiness assumptions on the parameter matrix (usually used in matrix completion). \nPrevious works could obtain non-trivial guarantees only under the assumptions that the measurement noise corresponding to the inliers is polynomially small in $n$ (e.g., Gaussian with variance $1/n^2$).\n\nTo devise our estimators, we equip the Huber loss with non-smooth regularizers such as the $\\ell_1$ norm or the nuclear norm, and extend d'Orsi et al.'s approach~\\cite{ICML-linear-regression} in a novel way to analyze the loss function.\nOur machinery appears to be easily applicable to a wide range of estimation problems.\nWe complement these algorithmic results with statistical lower bounds showing that the fraction of inliers that our PCA estimator can deal with is optimal up to a constant factor."}}
{"id": "N2_3dAPMyMU", "cdate": 1609459200000, "mdate": 1682939381655, "content": {"title": "Robust recovery for stochastic block models", "abstract": "We develop an efficient algorithm for weak recovery in a robust version of the stochastic block model. The algorithm matches the statistical guarantees of the best known algorithms for the vanilla version of the stochastic block model. In this sense, our results show that there is no price of robustness in the stochastic block model. Our work is heavily inspired by recent work of Banks, Mohanty, and Raghavendra (SODA 2021) that provided an efficient algorithm for the corresponding distinguishing problem. Our algorithm and its analysis significantly depart from previous ones for robust recovery. A key challenge is the peculiar optimization landscape underlying our algorithm: The planted partition may be far from optimal in the sense that completely unrelated solutions could achieve the same objective value. This phenomenon is related to the push-out effect at the BBP phase transition for PCA. To the best of our knowledge, our algorithm is the first to achieve robust recovery in the presence of such a push-out effect in a non-asymptotic setting. Our algorithm is an instantiation of a framework based on convex optimization (related to but distinct from sum-of-squares), which may be useful for other robust matrix estimation problems. A by-product of our analysis is a general technique that boosts the probability of success (over the randomness of the input) of an arbitrary robust weak-recovery algorithm from constant (or slowly vanishing) probability to exponentially high probability."}}
{"id": "J6FgycMJVa", "cdate": 1609459200000, "mdate": 1682939381521, "content": {"title": "Robust recovery for stochastic block models", "abstract": "We develop an efficient algorithm for weak recovery in a robust version of the stochastic block model. The algorithm matches the statistical guarantees of the best known algorithms for the vanilla version of the stochastic block model. In this sense, our results show that there is no price of robustness in the stochastic block model. Our work is heavily inspired by recent work of Banks, Mohanty, and Raghavendra (SODA 2021) that provided an efficient algorithm for the corresponding distinguishing problem. Our algorithm and its analysis significantly depart from previous ones for robust recovery. A key challenge is the peculiar optimization landscape underlying our algorithm: The planted partition may be far from optimal in the sense that completely unrelated solutions could achieve the same objective value. This phenomenon is related to the push-out effect at the BBP phase transition for PCA. To the best of our knowledge, our algorithm is the first to achieve robust recovery in the presense of such a push-out effect in a non-asymptotic setting. Our algorithm is an instantiation of a framework based on convex optimization (related to but distinct from sum-of-squares), which may be useful for other robust matrix estimation problems. A by-product of our analysis is a general technique that boosts the probability of success (over the randomness of the input) of an arbitrary robust weak-recovery algorithm from constant (or slowly vanishing) probability to exponentially high probability."}}
