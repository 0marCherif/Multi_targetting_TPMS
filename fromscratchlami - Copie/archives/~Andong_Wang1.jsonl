{"id": "hnZ3FILHxaK", "cdate": 1672531200000, "mdate": 1679906750755, "content": {"title": "Transformed Low-Rank Parameterization Can Help Robust Generalization for Tensor Neural Networks", "abstract": ""}}
{"id": "bvM6qdbnsl5", "cdate": 1609459200000, "mdate": null, "content": {"title": "Tensor Recovery via $*_L$-Spectral $k$-Support Norm", "abstract": "Unlike traditional tensor decompositions which model low-rankness in the original domain, the recently proposed tensor <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">*L</sub> -Singular Value Decomposition ( <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">*L</sub> -SVD) casts a new light on tensor analysis by exploiting low-rankness in the spectral domain. For convexized rank minimization within the framework of <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">*L</sub> -SVD, the tensor <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">*L</sub> -Tubal Nuclear Norm ( <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">*L</sub> -TNN) out-performs classical tensorial extensions of matrix nuclear norm in many applications. In this paper, we first generalize <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">*L</sub> -TNN to the <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> *L</sub> -Spectral k-Support Norm ( <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">*L</sub> -SpSN-k)as a new low-rank regularizer, and then adopt it to formulate two estimators for tensor recovery from noisy linear observations. Further, statistical performance of the proposed estimators is analyzed by establishing both deterministic and non-asymptotic upper bounds on the estimation error, which indicates that the proposed tensor Dantzig selector enjoys near-optimal estimation error for tensor compressive sensing and tensor completion. Moreover, proximal operator of the proposed norm is derived and embeded in an efficient algorithm to compute the estimators. Experiments on synthetic datasets verify correctness of the proposed error bounds and image inpainting results demonstrate superiority of the proposed estimators."}}
{"id": "HFUZ4akmSeq", "cdate": 1609459200000, "mdate": 1645715387959, "content": {"title": "Guaranteed Robust Tensor Completion via \u2217L-SVD with Applications to Remote Sensing Data", "abstract": "This paper conducts a rigorous analysis for the problem of robust tensor completion, which aims at recovering an unknown three-way tensor from incomplete observations corrupted by gross sparse outliers and small dense noises simultaneously due to various reasons such as sensor dead pixels, communication loss, electromagnetic interferences, cloud shadows, etc. To estimate the underlying tensor, a new penalized least squares estimator is first formulated by exploiting the low rankness of the signal tensor within the framework of tensor \u2217L-Singular Value Decomposition (\u2217L-SVD) and leveraging the sparse structure of the outlier tensor. Then, an algorithm based on the Alternating Direction Method of Multipliers (ADMM) is designed to compute the estimator in an efficient way. Statistically, the non-asymptotic upper bound on the estimation error is established and further proved to be optimal (up to a log factor) in a minimax sense. Simulation studies on synthetic data demonstrate that the proposed error bound can predict the scaling behavior of the estimation error with problem parameters (i.e., tubal rank of the underlying tensor, sparsity of the outliers, and the number of uncorrupted observations). Both the effectiveness and efficiency of the proposed algorithm are evaluated through experiments for robust completion on seven different types of remote sensing data."}}
{"id": "huKjyVVkIeh", "cdate": 1577836800000, "mdate": null, "content": {"title": "Robust Tensor Decomposition via Orientation Invariant Tubal Nuclear Norms", "abstract": "Low-rank tensor recovery has been widely applied to computer vision and machine learning. Recently, tubal nuclear norm (TNN) based optimization is proposed with superior performance as compared to other tensor nuclear norms. However, one major limitation is its orientation sensitivity due to low-rankness strictly defined along tubal orientation and it cannot simultaneously model spectral low-rankness in multiple orientations. To this end, we introduce two new tensor norms called OITNN-O and OITNN-L to exploit multi-orientational spectral low-rankness for an arbitrary K-way (K \u2265 3) tensors. We further formulate two robust tensor decomposition models via the proposed norms and develop two algorithms as the solutions. Theoretically, we establish non-asymptotic error bounds which can predict the scaling behavior of the estimation error. Experiments on real-world datasets demonstrate the superiority and effectiveness of the proposed norms."}}
{"id": "bIkvKg-3TAz", "cdate": 1577836800000, "mdate": null, "content": {"title": "Classification of Epileptic IEEG Signals by CNN and Data Augmentation", "abstract": "Epileptic focus localization in patients with epileptic seizures is essential when surgery is needed. Recent studies show that this can be done automatically using machine learning approaches. However, well-designed feature extraction methods are often computationally demanding, requiring a large amount of data labeled by physicians, which is time consuming and impractical. In this paper, we firstly introduce a one-dimensional convolutional neural network (1D-CNN) model for epileptic seizure focus detection which avoids the manual, time-consuming feature extraction Moreover, to reduce the necessary number of training samples, we introduce an approach for data augmentation. The experimental results demonstrate the efficiency of the proposed method, with a nearly 3% improvement in performance using the data enhancement method compared to the best result obtained using the traditional feature extraction method."}}
{"id": "IcjNMIOAKCQ", "cdate": 1577836800000, "mdate": null, "content": {"title": "Robust tensor decomposition via t-SVD: Near-optimal statistical guarantee and scalable algorithms", "abstract": "Highlights \u2022 A TNN-based least square estimator is proposed for robust tensor decomposition. We only assume the underlying tensor to satisfy the l\u221e-norm boundedness condition, which is less strict than the tensor incoherence conditions. \u2022 On the statistical side, both deterministic and non-asymptotic upper bounds on the estimation error are established in Theorems\u00a01 and 2, respectively. The non-asymptotic upper bounds are then proved to be minimax near-optimal by Theorem\u00a03. Experiments on synthetic dataset verify that the proposed upper bounds can predict the scaling behavior of the estimation error. \u2022 On the computational side, two algorithms, i.e., an ADMM-based algorithm (Algorithm\u00a01) and an FW-based algorithm (Algorithm\u00a01), are proposed with convergence guarantees (Theorems\u00a04 and 5). The latter gets rid of the proximity operator of TNN, and has significantly cheaper one-iteration computational cost. Experiments on real dataset validate the effectiveness and the efficiency of the proposed algorithms. Abstract Aiming at recovering a signal tensor from its mixture with outliers and noises, robust tensor decomposition (RTD) arises frequently in many real-world applications. Recently, the low-tubal-rank model has shown more powerful performances than traditional tensor low-rank models in several tensor recovery tasks. Assuming the underlying tensor to be low-tubal-rank and the outliers sparse, this paper first proposes a penalized least squares estimator for RTD. Specifically, we adopt the tubal nuclear norm (TNN) and a sparsity inducing norm to regularize the underlying tensor and the outliers, respectively. Then, from a statistical standpoint, non-asymptotic upper bounds on the estimation error are established and proved to be near-optimal in a minimax sense. Further, two algorithms, namely, an ADMM-based algorithm and a Frank-Wolfe (FW) based algorithm are proposed to efficiently solve the proposed estimator from a computational standpoint. The sharpness of the proposed upper bound is verified on synthetic datasets. The superiority and efficiency of the proposed algorithms is demonstrated in experiments on real datasets."}}
{"id": "9vdlRKrJfcQ", "cdate": 1577836800000, "mdate": null, "content": {"title": "A faster tensor robust PCA via tensor factorization", "abstract": "Many kinds of real-world multi-way signal, like color images, videos, etc., are represented in tensor form and may often be corrupted by outliers. To recover an unknown signal tensor corrupted by outliers, tensor robust principal component analysis (TRPCA) serves as a robust tensorial modification of the fundamental PCA. Recently, a successful TRPCA model based on the tubal nuclear norm (TNN) (Lu et al. in IEEE Trans Pattern Anal Mach Intell 42:925\u2013938, 2019) has attracted much attention thanks to its superiority in many applications. However, TNN is computationally expensive due to the requirement of full singular value decompositions, seriously limiting its scalability to large tensors. To address this issue, we propose a new TRPCA model which adopts a factorization strategy. Algorithmically, an algorithm based on the non-convex augmented Lagrangian method is developed with convergence guarantee. Theoretically, we rigorously establish the sub-optimality of the proposed algorithm. We also extend the proposed model to the robust tensor completion problem. Both the effectiveness and efficiency of the proposed algorithm is demonstrated through extensive experiments on both synthetic and real data sets."}}
{"id": "Ykcv0wFvvxu", "cdate": 1546300800000, "mdate": null, "content": {"title": "Generalized Dantzig Selector for Low-tubal-rank Tensor Recovery", "abstract": "Due to the superiority in exploiting the ubiquitous \"spatial-shifting\" property in modern multi-way data, the recently proposed low-tubal-rank model has been successfully applied for tensor recovery in signal processing and computer vision. In this paper, we define the generalized tensor Dantzig selector to recover a low-tubal-rank tensor from noisy linear measurements. Algorithmically, we develop an efficient algorithm based on the ADMM framework. Statistically, we establish non-asymptotic upper bounds on the estimation error for the problems of tensor completion and compressive sensing. Numerical experiments illustrate that our bounds can predict the scaling behavior of the estimation error. Experiments on realword datasets show the effectiveness of the proposed model."}}
{"id": "VwJAyC0EWl2", "cdate": 1546300800000, "mdate": null, "content": {"title": "Noisy low-tubal-rank tensor completion", "abstract": "Highlights \u2022 An estimator based on the tubal nuclear norm is proposed for tensor completion. \u2022 A non-asymptotic upper bound on the estimation error is established and proved to be optimal (up to a logarithm factor). \u2022 An ADMM-based algorithm and a composite gradient descent (CGD) algorithm are proposed. \u2022 The CGD algorithm is proved to enjoy globally geometric rates of convergence up to the statistical error. Abstract In many applications of multi-dimensional signal processing, noisy tensor completion arises often where the acquired data suffers from miss values and noise. Recently, models based on the tubal nuclear norm (TNN) have been applied successfully in a number of tensor completion tasks. However, statistical analysis of these models is still insufficient. In this paper, a TNN-based estimator is proposed to estimate a tensor from its partial noisy observations. Statistically, a non-asymptotic upper bound on the estimation error is established and proved to be optimal (up to a logarithm factor) in a minimax sense. Algorithmically, an ADMM-based algorithm and a composite gradient descent (CGD) algorithm are proposed to compute the estimator. The CGD algorithm is further proved to enjoy globally geometric rates of convergence up to the statistical error. The sharpness of the proposed upper bound and the geometric convergence rate of the proposed CGD algorithm are verified in two experiments on synthetic datasets, respectively. The superiority of the proposed algorithms over several state-of-the-art convex algorithms is demonstrated in experiments on color images and a dataset from environment perception for unmanned ground vehicle."}}
{"id": "TIAzlWYSHH", "cdate": 1546300800000, "mdate": null, "content": {"title": "Latent Schatten TT Norm for Tensor Completion", "abstract": "Tensor completion arouses much attention in signal processing and machine learning. The tensor train (TT) decomposition has shown better performances than the Tucker decomposition in image and video inpainting. In this paper, we propose a novel tensor completion model based on a newly defined latent Schatten TT norm. Then, the statistical performance is analyzed by establishing a non-asymptotic upper bound on the estimation error. Further, a scalable algorithm is developed to efficiently solve the model. Experimental results of color image inpainting demonstrate that the proposed norm has promising performances compared to other variants of Schatten norm."}}
