{"id": "HFkxZ_V0sBQ", "cdate": 1652737767238, "mdate": null, "content": {"title": "Augmenting Online Algorithms with $\\varepsilon$-Accurate Predictions", "abstract": "The growing body of work in learning-augmented online algorithms studies how online algorithms can be improved when given access to ML predictions about the future. Motivated by ML models that give a confidence parameter for their predictions, we study online algorithms with predictions that are $\\epsilon$-accurate: namely, each prediction is correct with probability (at least) $\\epsilon$, but can be arbitrarily inaccurate with the remaining probability. We show that even with predictions that are accurate with a small probability and arbitrarily inaccurate otherwise, we can dramatically outperform worst-case bounds for a range of classical online problems including caching, online set cover, and online facility location. Our main results are an $O(\\log(1/\\varepsilon))$-competitive algorithm for caching, and a simple $O(1/\\varepsilon)$-competitive algorithm for a large family of covering problems, including set cover and facility location, with $\\epsilon$-accurate predictions."}}
{"id": "N-PiuVbkEpp", "cdate": 1652737635106, "mdate": null, "content": {"title": "Online Algorithms for the Santa Claus Problem", "abstract": "The Santa Claus problem is a fundamental problem in {\\em fair division}: the goal is to partition a set of {\\em heterogeneous} items among {\\em heterogeneous} agents so as to maximize the minimum value of items received by any agent. In this paper, we study the online version of this problem where the items are not known in advance and have to be assigned to agents as they arrive over time. If the arrival order of items is arbitrary, then no good assignment rule exists in the worst case. However, we show that, if the arrival order is random, then for $n$ agents and any $\\varepsilon > 0$, we can obtain a competitive ratio of $1-\\varepsilon$ when the optimal assignment gives value at least $\\Omega(\\log n / \\varepsilon^2)$ to every agent (assuming each item has at most unit value). We also show that this result is almost tight: namely, if the optimal solution has value at most $C \\ln n / \\varepsilon$ for some constant $C$, then there is no $(1-\\varepsilon)$-competitive algorithm even for random arrival order."}}
{"id": "GgS40Y04LxA", "cdate": 1621629727332, "mdate": null, "content": {"title": "A Regression Approach to Learning-Augmented Online Algorithms", "abstract": "The emerging field of learning-augmented online algorithms uses ML techniques to predict future input parameters and thereby improve the performance of online algorithms. Since these parameters are, in general, real-valued functions, a natural approach is to use regression techniques to make these predictions. We introduce this approach in this paper, and explore it in the context of a general online search framework that captures classic problems like (generalized) ski rental, bin packing, minimum makespan scheduling, etc. We show nearly tight bounds on the sample complexity of this regression problem, and extend our results to the agnostic setting. From a technical standpoint, we show that the key is to incorporate online optimization benchmarks in the design of the loss function for the regression problem, thereby diverging from the use of off-the-shelf regression tools with standard bounds on statistical error."}}
{"id": "oE7kFoeAjwX", "cdate": 1577836800000, "mdate": null, "content": {"title": "Customizing ML Predictions for Online Algorithms", "abstract": "A popular line of recent research incorporates ML advice in the design of online algorithms to improve their performance in typical instances. These papers treat the ML algorithm as a black-box, an..."}}
{"id": "nmq5FCFi20", "cdate": 1577836800000, "mdate": null, "content": {"title": "Online Algorithms for Weighted Paging with Predictions", "abstract": "In this paper, we initiate the study of the weighted paging problem with predictions. This continues the recent line of work in online algorithms with predictions, particularly that of Lykouris and Vassilvitski (ICML 2018) and Rohatgi (SODA 2020) on unweighted paging with predictions. We show that unlike unweighted paging, neither a fixed lookahead nor knowledge of the next request for every page is sufficient information for an algorithm to overcome existing lower bounds in weighted paging. However, a combination of the two, which we call the strong per request prediction (SPRP) model, suffices to give a 2-competitive algorithm. We also explore the question of gracefully degrading algorithms with increasing prediction error, and give both upper and lower bounds for a set of natural measures of prediction error."}}
{"id": "m6LzdBpgpOM", "cdate": 1577836800000, "mdate": null, "content": {"title": "Online Two-Dimensional Load Balancing", "abstract": "In this paper, we consider the problem of assigning 2-dimensional vector jobs to identical machines online so to minimize the maximum load on any dimension of any machine. For arbitrary number of dimensions d, this problem is known as vector scheduling, and recent research has established the optimal competitive ratio as O((log d)/(log log d)) (Im et al. FOCS 2015, Azar et al. SODA 2018). But, these results do not shed light on the situation for small number of dimensions, particularly for d = 2 which is of practical interest. In this case, a trivial analysis shows that the classic list scheduling greedy algorithm has a competitive ratio of 3. We show the following improvements over this baseline in this paper: - We give an improved, and tight, analysis of the list scheduling algorithm establishing a competitive ratio of 8/3 for two dimensions. - If the value of opt is known, we improve the competitive ratio to 9/4 using a variant of the classic best fit algorithm for two dimensions. - For any fixed number of dimensions, we design an algorithm that is provably the best possible against a fractional optimum solution. This algorithm provides a proof of concept that we can simulate the optimal algorithm online up to the integrality gap of the natural LP relaxation of the problem."}}
{"id": "jLlkbn53ma", "cdate": 1577836800000, "mdate": null, "content": {"title": "Aggregated Deletion Propagation for Counting Conjunctive Query Answers", "abstract": "We investigate the computational complexity of minimizing the source side-effect in order to remove a given number of tuples from the output of a conjunctive query. This is a variant of the well-studied {\\em deletion propagation} problem, the difference being that we are interested in removing the smallest subset of input tuples to remove a given number of output tuples} while deletion propagation focuses on removing a specific output tuple. We call this the {\\em Aggregated Deletion Propagation} problem. We completely characterize the poly-time solvability of this problem for arbitrary conjunctive queries without self-joins. This includes a poly-time algorithm to decide solvability, as well as an exact structural characterization of NP-hard instances. We also provide a practical algorithm for this problem (a heuristic for NP-hard instances) and evaluate its experimental performance on real and synthetic datasets."}}
{"id": "hdQR2vv8anC", "cdate": 1577836800000, "mdate": null, "content": {"title": "Deterministic Min-cut in Poly-logarithmic Max-flows", "abstract": "We give a deterministic (global) min-cut algorithm for weighted undirected graphs that runs in time O(m <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1+\u03b5</sup> ) plus polylog ( n) max-flow computations. Using the current best max-flow algorithms, this results in an overall running time of ~O(m\u00b7min(\u221am, n <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2/3</sup> )) for weighted graphs, and m <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">4/3+o(1)</sup> for unweighted (multi)-graphs. This is the first improvement in the running time of deterministic algorithms for the min-cut problem on general (weighted/multi) graphs since the early 1990s when a running time bound of ~O(mn) was established for this problem."}}
{"id": "dqTr_MyPtk", "cdate": 1577836800000, "mdate": null, "content": {"title": "Caching with Time Windows and Delays", "abstract": "We consider two generalizations of the classical weighted paging problem that incorporate the notion of delayed service of page requests. The first is the (weighted) Paging with Time Windows (PageTW) problem, which is like the classical weighted paging problem except that each page request only needs to be served before a given deadline. This problem arises in many practical applications of online caching, such as the \"deadline\" I/O scheduler in the Linux kernel and video-on-demand streaming. The second, and more general, problem is the (weighted) Paging with Delay (PageD) problem, where the delay in serving a page request results in a penalty being assessed to the objective. This problem generalizes the caching problem to allow delayed service, a line of work that has recently gained traction in online algorithms (e.g., Emek et al. STOC '16, Azar et al. STOC '17, Azar and Touitou FOCS '19). We give $O(\\log k\\log n)$-competitive algorithms for both the PageTW and PageD problems on $n$ pages with a cache of size $k$. This significantly improves on the previous best bounds of $O(k)$ for both problems (Azar et al. STOC '17). We also consider the offline PageTW and PageD problems, for which we give $O(1)$ approximation algorithms and prove APX-hardness. These are the first results for the offline problems; even NP-hardness was not known before our work. At the heart of our algorithms is a novel \"hitting-set\" LP relaxation of the PageTW problem that overcomes the $\\Omega(k)$ integrality gap of the natural LP for the problem. To the best of our knowledge, this is the first example of an LP-based algorithm for an online algorithm with delays/deadlines."}}
{"id": "abqIoi6ww3z", "cdate": 1577836800000, "mdate": null, "content": {"title": "Robust Algorithms for TSP and Steiner Tree", "abstract": "Robust optimization is a widely studied area in operations research, where the algorithm takes as input a range of values and outputs a single solution that performs well for the entire range. Specifically, a robust algorithm aims to minimize regret, defined as the maximum difference between the solution's cost and that of an optimal solution in hindsight once the input has been realized. For graph problems in P, such as shortest path and minimum spanning tree, robust polynomial-time algorithms that obtain a constant approximation on regret are known. In this paper, we study robust algorithms for minimizing regret in NP-hard graph optimization problems, and give constant approximations on regret for the classical traveling salesman and Steiner tree problems."}}
