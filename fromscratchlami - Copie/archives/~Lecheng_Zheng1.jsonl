{"id": "r_vnM5H9Fm", "cdate": 1663850013632, "mdate": null, "content": {"title": "DEEPER-GXX: DEEPENING ARBITRARY GNNS", "abstract": "Recently, motivated by real applications, a major research direction in graph neural networks (GNNs) is to explore deeper structures.\nFor instance, the graph connectivity is not always consistent with the label distribution (e.g., the closest neighbors of some nodes are not from the same category). In this case, GNNs need to stack more layers, in order to find the same categorical neighbors in a longer path for capturing the class-discriminative information. However, two major problems hinder the deeper GNNs to obtain satisfactory performance, i.e., vanishing gradient and over-smoothing. On one hand, stacking layers makes the neural network hard to train as the gradients of the first few layers vanish. Moreover, when simply addressing vanishing gradient in GNNs, we discover the shading neighbors effect (i.e., stacking layers inappropriately distorts the non-IID information of graphs and degrade the performance of GNNs). On the other hand, deeper GNNs aggregate much more information from common neighbors such that individual node representations share more overlapping features, which makes the final output representations not discriminative (i.e., overly smoothed). In this paper, for the first time, we address both problems to enable deeper GNNs, and propose Deeper-GXX, which consists of the Weight-Decaying Graph Residual Connection module (WDG-ResNet) and Topology-Guided Graph Contrastive Loss (TGCL). Extensive experiments on real-world data sets demonstrate that Deeper-GXX outperforms state-of-the-art deeper baselines."}}
{"id": "g6GZiTx60O", "cdate": 1640995200000, "mdate": 1673992696166, "content": {"title": "MentorGNN: Deriving Curriculum for Pre-Training GNNs", "abstract": ""}}
{"id": "8B3_KrF1wAO", "cdate": 1640995200000, "mdate": 1673992696166, "content": {"title": "Contrastive Learning with Complex Heterogeneity", "abstract": ""}}
{"id": "2JfwEqaEyPZ", "cdate": 1640995200000, "mdate": 1673992696165, "content": {"title": "MentorGNN: Deriving Curriculum for Pre-Training GNNs", "abstract": ""}}
{"id": "kQMXLDF_z20", "cdate": 1632875693544, "mdate": null, "content": {"title": "Tackling Oversmoothing of GNNs with Contrastive Learning", "abstract": "Graph neural networks (GNNs) integrate the comprehensive relation of graph data and the representation learning capability of neural networks, which is one of the most popular deep learning methods and achieves state-of-the-art performance in many applications, such as natural language processing and computer vision. In real-world scenarios, increasing the depth (i.e., the number of layers) of GNNs is sometimes necessary to capture more latent knowledge of the input data to mitigate the uncertainty caused by missing values.\nHowever, involving more complex structures and more parameters will decrease the performance of GNN models. One reason called oversmoothing is recently proposed, whose research still remains nascent. In general, oversmoothing makes the final representations of nodes indiscriminative to hurt the node classification and link prediction performance.\nIn this paper, we first survey the current de-oversmoothing methods and propose three major metrics to evaluate a de-oversmoothing method, i.e., constant divergence indicator, easy-to-determine divergence indicator, and model-agnostic strategy. Then, we propose the Topology-guided Graph Contrastive Layer, named TGCL, which is the first de-oversmoothing method maintaining the three mentioned metrics. With the contrastive learning manner, we provide the theoretical analysis of the effectiveness of the proposed method. Last but not least, we design extensive experiments to illustrate the empirical performance of TGCL comparing with state-of-the-art baselines."}}
{"id": "ogAC8uTrspx", "cdate": 1609459200000, "mdate": 1673992696169, "content": {"title": "Deep Co-Attention Network for Multi-View Subspace Learning", "abstract": ""}}
{"id": "f3Nd-fVzViN", "cdate": 1609459200000, "mdate": null, "content": {"title": "Deep Co-Attention Network for Multi-View Subspace Learning", "abstract": "Many real-world applications involve data from multiple modalities and thus exhibit the view heterogeneity. For example, user modeling on social media might leverage both the topology of the underlying social network and the content of the users' posts; in the medical domain, multiple views could be X-ray images taken at different poses. To date, various techniques have been proposed to achieve promising results, such as canonical correlation analysis based methods, etc. In the meanwhile, it is critical for decision-makers to be able to understand the prediction results from these methods. For example, given the diagnostic result that a model provided based on the X-ray images of a patient at different poses, the doctor needs to know why the model made such a prediction. However, state-of-the-art techniques usually suffer from the inability to utilize the complementary information of each view and to explain the predictions in an interpretable manner. To address these issues, in this paper, we propose a deep co-attention network for multi-view subspace learning, which aims to extract both the common information and the complementary information in an adversarial setting and provide robust interpretations behind the prediction to the end-users via the co-attention mechanism. In particular, it uses a novel cross reconstruction loss and leverages the label information to guide the construction of the latent representation by incorporating the classifier into our model. This improves the quality of latent representation and accelerates the convergence speed. Finally, we develop an efficient iterative algorithm to find the optimal encoders and discriminator, which are evaluated extensively on synthetic and real-world data sets. We also conduct a case study to demonstrate how the proposed method robustly interprets the predictions on an image data set."}}
{"id": "SvN8VvgkU_", "cdate": 1609459200000, "mdate": 1673992696167, "content": {"title": "Outlier Impact Characterization for Time Series Data", "abstract": ""}}
{"id": "Nx9Uruq7B7", "cdate": 1609459200000, "mdate": 1673992696166, "content": {"title": "Heterogeneous Contrastive Learning", "abstract": ""}}
{"id": "Iojridwo9ze", "cdate": 1609459200000, "mdate": 1673992696166, "content": {"title": "Tackling Oversmoothing of GNNs with Contrastive Learning", "abstract": ""}}
