{"id": "M2B1wrkYhKh", "cdate": 1609459200000, "mdate": 1682562955079, "content": {"title": "Larger-Context Neural Machine Translation", "abstract": ""}}
{"id": "HWf0aVBp1q", "cdate": 1577836800000, "mdate": 1645200582421, "content": {"title": "Log-Linear Reformulation of the Noisy Channel Model for Document-Level Neural Machine Translation", "abstract": "S\u00e9bastien Jean, Kyunghyun Cho. Proceedings of the Fourth Workshop on Structured Prediction for NLP. 2020."}}
{"id": "gtgkp7mA0tA", "cdate": 1546300800000, "mdate": null, "content": {"title": "Adaptive Scheduling for Multi-Task Learning", "abstract": "To train neural machine translation models simultaneously on multiple tasks (languages), it is common to sample each task uniformly or in proportion to dataset sizes. As these methods offer little control over performance trade-offs, we explore different task scheduling approaches. We first consider existing non-adaptive techniques, then move on to adaptive schedules that over-sample tasks with poorer results compared to their respective baseline. As explicit schedules can be inefficient, especially if one task is highly over-sampled, we also consider implicit schedules, learning to scale learning rates or gradients of individual tasks instead. These techniques allow training multilingual models that perform better for low-resource language pairs (tasks with small amount of data), while minimizing negative effects on high-resource tasks."}}
{"id": "TJPqLCXqDHN", "cdate": 1546300800000, "mdate": 1682562955049, "content": {"title": "Fill in the Blanks: Imputing Missing Sentences for Larger-Context Neural Machine Translation", "abstract": "Most neural machine translation systems still translate sentences in isolation. To make further progress, a promising line of research additionally considers the surrounding context in order to provide the model potentially missing source-side information, as well as to maintain a coherent output. One difficulty in training such larger-context (i.e. document-level) machine translation systems is that context may be missing from many parallel examples. To circumvent this issue, two-stage approaches, in which sentence-level translations are post-edited in context, have recently been proposed. In this paper, we instead consider the viability of filling in the missing context. In particular, we consider three distinct approaches to generate the missing context: using random contexts, applying a copy heuristic or generating it with a language model. In particular, the copy heuristic significantly helps with lexical coherence, while using completely random contexts hurts performance on many long-distance linguistic phenomena. We also validate the usefulness of tagged back-translation. In addition to improving BLEU scores as expected, using back-translated data helps larger-context machine translation systems to better capture long-range phenomena."}}
{"id": "Pn6KQ6crBj", "cdate": 1546300800000, "mdate": 1668099889025, "content": {"title": "Lingvo: a Modular and Scalable Framework for Sequence-to-Sequence Modeling", "abstract": "Lingvo is a Tensorflow framework offering a complete solution for collaborative deep learning research, with a particular focus towards sequence-to-sequence models. Lingvo models are composed of modular building blocks that are flexible and easily extensible, and experiment configurations are centralized and highly customizable. Distributed training and quantized inference are supported directly within the framework, and it contains existing implementations of a large number of utilities, helper functions, and the newest research ideas. Lingvo has been used in collaboration by dozens of researchers in more than 20 papers over the last two years. This document outlines the underlying design of Lingvo and serves as an introduction to the various pieces of the framework, while also offering examples of advanced features that showcase the capabilities of the framework."}}
{"id": "H87XJCErT1c", "cdate": 1546300800000, "mdate": 1645200582853, "content": {"title": "Context-Aware Learning for Neural Machine Translation", "abstract": "Interest in larger-context neural machine translation, including document-level and multi-modal translation, has been growing. Multiple works have proposed new network architectures or evaluation schemes, but potentially helpful context is still sometimes ignored by larger-context translation models. In this paper, we propose a novel learning algorithm that explicitly encourages a neural translation model to take into account additional context using a multilevel pair-wise ranking loss. We evaluate the proposed learning algorithm with a transformer-based larger-context translation system on document-level translation. By comparing performance using actual and random contexts, we show that a model trained with the proposed algorithm is more sensitive to the additional context."}}
{"id": "r1WZ5fG_br", "cdate": 1483228800000, "mdate": null, "content": {"title": "Adversarial Learning for Neural Dialogue Generation", "abstract": "In this paper, drawing intuition from the Turing test, we propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator---analagous to the human evaluator in the Turing test--- to distinguish between the human-generated dialogues and the machine-generated ones. The outputs from the discriminator are then used as rewards for the generative model, pushing the system to generate dialogues that mostly resemble human dialogues. \r\nIn addition to adversarial training we describe a model for adversarial {\\em evaluation} that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines."}}
{"id": "HrlH7R4STkc", "cdate": 1483228800000, "mdate": 1645200586945, "content": {"title": "Neural Machine Translation for Cross-Lingual Pronoun Prediction", "abstract": "Sebastien Jean, Stanislas Lauly, Orhan Firat, Kyunghyun Cho. Proceedings of the Third Workshop on Discourse in Machine Translation. 2017."}}
{"id": "BbVSkA4BTyq", "cdate": 1483228800000, "mdate": 1645200583028, "content": {"title": "The representational geometry of word meanings acquired by neural machine translation models", "abstract": "This work is the first comprehensive analysis of the properties of word embeddings learned by neural machine translation (NMT) models trained on bilingual texts. We show the word representations of NMT models outperform those learned from monolingual text by established algorithms such as Skipgram and CBOW on tasks that require knowledge of semantic similarity and/or lexical\u2013syntactic role. These effects hold when translating from English to French and English to German, and we argue that the desirable properties of NMT word embeddings should emerge largely independently of the source and target languages. Further, we apply a recently-proposed heuristic method for training NMT models with very large vocabularies, and show that this vocabulary expansion method results in minimal degradation of embedding quality. This allows us to make a large vocabulary of NMT embeddings available for future research and applications. Overall, our analyses indicate that NMT embeddings should be used in applications that require word concepts to be organised according to similarity and/or lexical function, while monolingual embeddings are better suited to modelling (nonspecific) inter-word relatedness."}}
{"id": "B6MiJREBakq", "cdate": 1483228800000, "mdate": 1645200583294, "content": {"title": "Does Neural Machine Translation Benefit from Larger Context?", "abstract": "We propose a neural machine translation architecture that models the surrounding text in addition to the source sentence. These models lead to better performance, both in terms of general translation quality and pronoun prediction, when trained on small corpora, although this improvement largely disappears when trained with a larger corpus. We also discover that attention-based neural machine translation is well suited for pronoun prediction and compares favorably with other approaches that were specifically designed for this task."}}
