{"id": "xLnfzQYSIue", "cdate": 1652737504602, "mdate": null, "content": {"title": "Top Two Algorithms Revisited", "abstract": "Top two algorithms arose as an adaptation of Thompson sampling to best arm identification in multi-armed bandit models for parametric families of arms. They select the next arm to sample from by randomizing among two candidate arms, a leader and a challenger. Despite their good empirical performance, theoretical guarantees for fixed-confidence best arm identification have only been obtained when the arms are Gaussian with known variances. In this paper, we provide a general analysis of top-two methods, which identifies desirable properties of the leader, the challenger, and the (possibly non-parametric) distributions of the arms. As a result, we obtain theoretically supported top-two algorithms for best arm identification with bounded distributions. Our proof method demonstrates in particular that the sampling step used to select the leader inherited from Thompson sampling can be replaced by other choices, like selecting the empirical best arm."}}
{"id": "ERzpLwEDOY", "cdate": 1621630173756, "mdate": null, "content": {"title": "Bandits with many optimal arms", "abstract": "We consider a stochastic bandit problem with a possibly infinite number of arms. We write $p^*$ for the proportion of optimal arms and $\\Delta$ for the minimal mean-gap between optimal and sub-optimal arms. We characterize the optimal learning rates both in the cumulative regret setting, and in the best-arm identification setting in terms of the problem parameters $T$ (the budget), $p^*$ and $\\Delta$. For the objective of minimizing the cumulative regret, we provide a lower bound of order $\\Omega(\\log(T)/(p^*\\Delta))$ and a UCB-style algorithm with matching upper bound up to a factor of $\\log(1/\\Delta)$. Our algorithm needs $p^*$ to calibrate its parameters, and we prove that this knowledge is necessary, since adapting to $p^*$ in this setting is impossible. For best-arm identification we also provide a lower bound of order $\\Omega(\\exp(-cT\\Delta^2p^*))$ on the probability of outputting a sub-optimal arm where $c>0$ is an absolute constant. We also provide an elimination algorithm with an upper bound matching the lower bound up to a factor of order $\\log(T)$ in the exponential, and that does not need $p^*$ or $\\Delta$ as parameter. Our results apply directly to the three related problems of competing against the $j$-th best arm, identifying an $\\epsilon$ good arm, and finding an arm with mean larger than a quantile of a known order."}}
{"id": "J_JnSvZOdN", "cdate": 1617703490894, "mdate": null, "content": {"title": "Fixed-confidence guarantees for Bayesian best-arm identification", "abstract": "We investigate and provide new insights on the sampling rule called Top-Two Thompson Sampling (TTTS). In particular, we justify its use for fixed-confidence best-arm identification. We further propose a variant of TTTS called Top-Two Transportation Cost (T3C), which disposes of the computational burden of TTTS. As our main contribution, we provide the first sample complexity analysis of TTTS and T3C when coupled with a very natural Bayesian stopping rule, for bandits with Gaussian rewards, solving one of the open questions raised by Russo (2016). We also provide new posterior convergence results for TTTS under two models that are commonly used in practice: bandits with Gaussian and Bernoulli rewards and conjugate priors. "}}
