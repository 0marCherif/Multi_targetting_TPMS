{"id": "H63MYeMHjn", "cdate": 1672704905155, "mdate": 1672704905155, "content": {"title": "A Review of Uncertainty for Deep Reinforcement Learning", "abstract": "Uncertainty is ubiquitous in games, both in the agents playing games and often in the games themselves. Working with uncertainty is therefore an important component of successful deep reinforcement learning agents. While there has been substantial effort and progress in understanding and working with uncertainty for supervised learning, the body of literature for uncertainty aware deep reinforcement learning is less developed. While many of the same problems regarding uncertainty in neural networks for supervised learning remain for reinforcement learning, there are additional sources of uncertainty due to the nature of an interactable environment. In this work, we provide an overview motivating and presenting existing techniques in uncertainty aware deep reinforcement learning. These works show empirical benefits on a variety of reinforcement learning tasks. This work serves to help to centralize the disparate results and promote future research in this area."}}
{"id": "HCMbGrA7M7q", "cdate": 1648668217793, "mdate": 1648668217793, "content": {"title": "An Empirical Review of Optimization Techniques for Quantum Variational Circuits", "abstract": "Quantum Variational Circuits (QVCs) are often claimed as one of the most potent uses of both near term and long term quantum hardware. The standard approaches to optimizing these circuits rely on a classical system to compute the new parameters at every optimization step. However, this process can be extremely challenging both in terms of navigating the exponentially scaling complex Hilbert space, barren plateaus, and the noise present in all foreseeable quantum hardware. Although a variety of optimization algorithms are employed in practice, there is often a lack of theoretical or empirical motivations for this choice. To this end we empirically evaluate the potential of many common gradient and gradient free optimizers on a variety of optimization tasks. These tasks include both classical and quantum data based optimization routines. Our evaluations were conducted in both noise free and noisy simulations. The large number of problems and optimizers yields strong empirical guidance for choosing optimizers for QVCs that is currently lacking."}}
{"id": "ruWWJ7AXfQq", "cdate": 1648668183366, "mdate": 1648668183366, "content": {"title": "Optimizing Quantum Variational Circuits with Deep Reinforcement Learning", "abstract": "Quantum Machine Learning (QML) is considered to be one of the most promising applications\nof near term quantum devices. However, the optimization of quantum machine learning models\npresents numerous challenges arising from the imperfections of hardware and the fundamental obstacles in navigating an exponentially scaling Hilbert space. In this work, we evaluate the potential\nof contemporary methods in deep reinforcement learning to augment gradient based optimization\nroutines in quantum variational circuits. We find that reinforcement learning augmented optimizers\nconsistently outperform gradient descent in noisy environments. All code and pretrained weights\nare available to replicate the results or deploy the models at github.com/lockwo/rl qvc opt."}}
{"id": "Lrg6ArFSphF", "cdate": 1631641383668, "mdate": 1631641383668, "content": {"title": "Playing Atari with Hybrid Quantum-Classical Reinforcement Learning", "abstract": "Despite the successes of recent works in quantum reinforcement learning, there are still severe limitations on its applications due to the challenge of encoding large observation spaces into quantum systems. To address this challenge, we propose using a neural network as a data encoder, with the Atari games as our testbed. Specifically, the neural network converts the pixel input from the games to quantum data for a Quantum Variational Circuit (QVC); this hybrid model is then used as a function approximator in the Double Deep Q Networks algorithm. We explore a number of variations of this algorithm and find that our proposed hybrid models do not achieve meaningful results on two Atari games \u2013 Breakout and Pong. We suspect this is due to the significantly reduced sizes of the hybrid quantum-classical systems."}}
{"id": "MUR1oxCnvNs", "cdate": 1616081792018, "mdate": null, "content": {"title": "In Defense of the Paper", "abstract": "The machine learning publication process is broken, of that there can be no doubt. Many of these flaws are attributed to the current workflow: LaTeX to PDF to reviewers to camera ready PDF. This has understandably resulted in the desire for new forms of publications; ones that can increase inclusively, accessibility and pedagogical strength. However, this venture fails to address the origins of these inadequacies in the contemporary paper workflow. The paper, being the basic unit of academic research, is merely how problems in the publication and research ecosystem manifest; but is not itself responsible for them. Not only will simply replacing or augmenting papers with different formats not fix existing problems; when used as a band-aid without systemic changes, will likely exacerbate the existing inequities. In this work, we argue that the root cause of hindrances in the accessibility of machine learning research lies not in the paper workflow but within the misaligned incentives behind the publishing and research processes. We discuss these problems and argue that the paper is the optimal workflow. We also highlight some potential solutions for the incentivization problems. "}}
{"id": "XhSN4Mm6fBM", "cdate": 1612102787829, "mdate": null, "content": {"title": "Replicating Softmax Deep Double Deterministic Policy Gradients", "abstract": "Reproducibility Summary\nScope of Reproducibility\nWe attempt to reproduce the claim that Softmax Deep Double Deterministic Policy Gradient (SD3) achieves superior performance over Twin Delayed Deep Double Deterministic Policy Gradient (TD3) on continuous control reinforcement learning tasks. We utilize both environments that were used by the paper and expand to include some not present. \n\nMethodology\nWe compare the performance of TD3 and SD3 on a variety of continuous control tasks. We use the author's PyTorch code but also provide Tensorflow implementations of SD3 and TD3 (which we did not use for optimization reasons). For the control tasks we utilize OpenAI Gym environments with PyBullet, as opposed to MuJoCo, in an effort to bolster claims of generalization and to avoid exclusionary research practices. Experiments are conducted both on similar environments in the original paper and those that were not mentioned.\n\nResults\nOverall we reach similar, albeit much milder, conclusions as the paper, specifically, that SD3 outperforms TD3 on some of continuous control tasks. However, the advantage is not always as readily apparent as in the original work. Algorithmic performance was comparable on most environments, with SD3 providing limited evidence of definitive superiority. Further investigation and improvements are warranted. The results are not directly comparable to the original paper due to differences in physics simulators. Additionally, we did not perform hyperparameter optimization, which could potentially bolster returns on some environments. \n\nWhat was easy\nThe author's made their code extremely easy to use, run, modify and rewrite in a different package. Because everything was available on their github and required only common reinforcement learning packages it was quick and painless to run. It was trivial to use the algorithms on different environments from different packages and collect their results for analysis.  \n\nWhat was difficult\nOne of the biggest difficulties was the time and resource consumption's of the experiments. Running each algorithm on each environment with a sufficient number of random seeds took the vast majority of the time. We had a total runtime of around 310 GPU hours (or 13 days). Time was our primary constraint and was the primary reason we did no investigate other environments. Simulator differences also proved to be somewhat challenging. \n\nCommunication with original authors\nOur contact with the authors was limited to a discussion we had at their poster presentation at NeurIPS 2020. "}}
{"id": "f8WokEhNhZ", "cdate": 1577836800000, "mdate": 1631641055753, "content": {"title": "Reinforcement Learning with Quantum Variational Circuits", "abstract": "The development of quantum computational techniques has advanced greatly in recent years, parallel to the advancements in techniques for deep reinforcement learning. This work explores the potential for quantum computing to facilitate reinforcement learning problems. Quantum computing approaches offer important potential improvements in time and space complexity over traditional algorithms because of its ability to exploit the quantum phenomena of superposition and entanglement. Specifically, we investigate the use of quantum variational circuits, a form of quantum machine learning. We present our techniques for encoding classical data for a quantum variational circuit, we further explore pure and hybrid quantum algorithms for DQN and Double DQN. Our results indicate both hybrid and pure quantum variational circuit have the ability to solve reinforcement learning tasks with a smaller parameter space. These comparison are conducted with two OpenAI Gym environments: CartPole and Blackjack, The success of this work is indicative of a strong future relationship between quantum machine learning and deep reinforcement learning."}}
