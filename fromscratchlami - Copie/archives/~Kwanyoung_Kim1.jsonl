{"id": "UI5YwCbPcEU", "cdate": 1672531200000, "mdate": 1685079913453, "content": {"title": "ZegOT: Zero-shot Segmentation Through Optimal Transport of Text Prompts", "abstract": "Recent success of large-scale Contrastive Language-Image Pre-training (CLIP) has led to great promise in zero-shot semantic segmentation by transferring image-text aligned knowledge to pixel-level classification. However, existing methods usually require an additional image encoder or retraining/tuning the CLIP module. Here, we propose a novel Zero-shot segmentation with Optimal Transport (ZegOT) method that matches multiple text prompts with frozen image embeddings through optimal transport. In particular, we introduce a novel Multiple Prompt Optimal Transport Solver (MPOT), which is designed to learn an optimal mapping between multiple text prompts and visual feature maps of the frozen image encoder hidden layers. This unique mapping method facilitates each of the multiple text prompts to effectively focus on distinct visual semantic attributes. Through extensive experiments on benchmark datasets, we show that our method achieves the state-of-the-art (SOTA) performance over existing Zero-shot Semantic Segmentation (ZS3) approaches."}}
{"id": "f-1fJagKTt", "cdate": 1640995200000, "mdate": 1685079913371, "content": {"title": "Noise Distribution Adaptive Self-Supervised Image Denoising using Tweedie Distribution and Score Matching", "abstract": "Tweedie distributions are a special case of exponential dispersion models, which are often used in classical statistics as distributions for generalized linear models. Here, we show that Tweedie distributions also play key roles in modern deep learning era, leading to a distribution adaptive self-supervised image denoising formula without clean reference images. Specifically, by combining with the recent Noise2Score self-supervised image denoising approach and the saddle point approximation of Tweedie distribution, we provide a general closed-form denoising formula that can be used for large classes of noise distributions without ever knowing the underlying noise distribution. Similar to the original Noise2Score, the new approach is composed of two successive steps: score matching using perturbed noisy images, followed by a closed form image denoising formula via distribution-independent Tweedie's formula. In addition, we reveal a systematic algorithm to estimate the noise model and noise parameters for a given noisy image data set. Through extensive experiments, we demonstrate that the proposed method can accurately estimate noise models and parameters, and provide the state-of-the-art self-supervised image denoising performance in the benchmark dataset and real-world dataset."}}
{"id": "ZqEUs3sTRU0", "cdate": 1621630003933, "mdate": null, "content": {"title": "Noise2Score: Tweedie\u2019s Approach to Self-Supervised Image Denoising without Clean Images", "abstract": "Recently, there has  been extensive research interest in training  deep networks to denoise images without clean reference.\nHowever, the representative approaches such as Noise2Noise, Noise2Void, Stein's unbiased risk estimator (SURE), etc.  seem to differ from one another and it is difficult to find the coherent mathematical structure. To address this, here we present a novel approach, called Noise2Score, which reveals a missing link in order to unite these seemingly different approaches.\nSpecifically, we  show that   image denoising  problems  without clean images can be addressed by finding the mode of the posterior distribution and that the Tweedie's formula offers an explicit solution through the score function (i.e. the gradient of loglikelihood). Our method then uses the  recent finding that  the score function  can be stably estimated from the noisy images using the amortized residual denoising autoencoder, the method of which is closely related to Noise2Noise or Nose2Void. Our Noise2Score approach is so universal  that the same network training can be used to remove noises from images that are corrupted by any exponential family distributions and noise parameters. Using extensive  experiments with Gaussian, Poisson, and Gamma noises, we show  that  Noise2Score significantly outperforms the state-of-the-art self-supervised denoising methods in the benchmark data set such as (C)BSD68, Set12, and Kodak, etc. \n"}}
{"id": "uQCJDN9Cuvx", "cdate": 1609459200000, "mdate": 1685079913439, "content": {"title": "Noise2Score: Tweedie's Approach to Self-Supervised Image Denoising without Clean Images", "abstract": "Recently, there has been extensive research interest in training deep networks to denoise images without clean reference.However, the representative approaches such as Noise2Noise, Noise2Void, Stein's unbiased risk estimator (SURE), etc. seem to differ from one another and it is difficult to find the coherent mathematical structure. To address this, here we present a novel approach, called Noise2Score, which reveals a missing link in order to unite these seemingly different approaches.Specifically, we show that image denoising problems without clean images can be addressed by finding the mode of the posterior distribution and that the Tweedie's formula offers an explicit solution through the score function (i.e. the gradient of loglikelihood). Our method then uses the recent finding that the score function can be stably estimated from the noisy images using the amortized residual denoising autoencoder, the method of which is closely related to Noise2Noise or Nose2Void. Our Noise2Score approach is so universal that the same network training can be used to remove noises from images that are corrupted by any exponential family distributions and noise parameters. Using extensive experiments with Gaussian, Poisson, and Gamma noises, we show that Noise2Score significantly outperforms the state-of-the-art self-supervised denoising methods in the benchmark data set such as (C)BSD68, Set12, and Kodak, etc."}}
{"id": "0xlYdWc0td", "cdate": 1609459200000, "mdate": 1681693017837, "content": {"title": "Task-Aware Variational Adversarial Active Learning", "abstract": "Often, labeling large amount of data is challenging due to high labeling cost limiting the application domain of deep learning techniques. Active learning (AL) tackles this by querying the most informative samples to be annotated among unlabeled pool. Two promising directions for AL that have been recently explored are task-agnostic approach to select data points that are far from the current labeled pool and task-aware approach that relies on the perspective of task model. Unfortunately, the former does not exploit structures from tasks and the latter does not seem to well-utilize overall data distribution. Here, we propose task-aware variational adversarial AL (TA-VAAL) that modifies task-agnostic VAAL, that considered data distribution of both label and unlabeled pools, by relaxing task learning loss prediction to ranking loss prediction and by using ranking conditional generative adversarial network to embed normalized ranking loss information on VAAL. Our proposed TA-VAAL outperforms state-of-the-arts on various benchmark datasets for classifications with balanced / imbalanced labels as well as semantic segmentation and its task-aware and task-agnostic AL properties were confirmed with our in-depth analyses."}}
{"id": "KL970o9HsLx", "cdate": 1577836800000, "mdate": null, "content": {"title": "Unsupervised Training of Denoisers for Low-Dose CT Reconstruction Without Full-Dose Ground Truth", "abstract": "Recently, deep neural network (DNN) based methods for low-dose CT have been investigated to achieve excellent performance in both image quality and computational speed. However, almost all methods using DNNs for low-dose CT require clean ground truth data with full radiation dose to train the DNNs. In this work, we attempt to train DNNs for low-dose CT reconstructions with reduced tube current by investigating unsupervised training of DNNs for denoising sensor measurements or sinograms without full-dose ground truth images. In other words, our proposed methods allow training of DNNs with only noisy low-dose CT measurements. First, the Poisson Unbiased Risk Estimator (PURE) is investigated to train a DNN for denoising CT measurements, and a method is proposed for reconstructing the CT image using filtered back-projection (FBP) and the DNN trained with PURE. Then, the CT forward model-based Weighted Stein's Unbiased Risk Estimator (WSURE) is proposed to train a DNN for denoising CT sinograms and to subsequently reconstruct the CT image using FBP. Our proposed methods achieve excellent performance in both fast computation and reconstructed image quality, which is more comparable to the results of the DNNs trained with full-dose ground truth data than other state-of-the-art denoising methods such as the BM3D, Deep Image Prior, and Deep Decoder."}}
