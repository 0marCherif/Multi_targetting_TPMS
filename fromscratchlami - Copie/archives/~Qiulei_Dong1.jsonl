{"id": "PtWh8Dy9tt_", "cdate": 1698796800000, "mdate": 1698556616199, "content": {"title": "Interactive piecewise planar building reconstruction from a single image based on geometric priors", "abstract": ""}}
{"id": "BpIt_kIldf", "cdate": 1698557822158, "mdate": 1698557822158, "content": {"title": "SMOC-Net: Leveraging Camera Pose for Self-Supervised Monocular Object Pose Estimation", "abstract": "Recently, self-supervised 6D object pose estimation,\nwhere synthetic images with object poses (sometimes jointly\nwith un-annotated real images) are used for training, has\nattracted much attention in computer vision. Some typical\nworks in literature employ a time-consuming differentiable\nrenderer for object pose prediction at the training stage, so\nthat (i) their performances on real images are generally limited\ndue to the gap between their rendered images and real\nimages and (ii) their training process is computationally expensive.\nTo address the two problems, we propose a novel\nNetwork for Self-supervised Monocular Object pose estimation\nby utilizing the predicted Camera poses from unannotated\nreal images, called SMOC-Net. The proposed\nnetwork is explored under a knowledge distillation framework,\nconsisting of a teacher model and a student model.\nThe teacher model contains a backbone estimation module\nfor initial object pose estimation, and an object pose refiner\nfor refining the initial object poses using a geometric constraint\n(called relative-pose constraint) derived from relative\ncamera poses. The student model gains knowledge for\nobject pose estimation from the teacher model by imposing\nthe relative-pose constraint. Thanks to the relative-pose\nconstraint, SMOC-Net could not only narrow the domain\ngap between synthetic and real data but also reduce the\ntraining cost. Experimental results on two public datasets\ndemonstrate that SMOC-Net outperforms several state-ofthe-\nart methods by a large margin while requiring much less\ntraining time than the differentiable-renderer-based methods."}}
{"id": "68s6mNkoGTV", "cdate": 1698557645465, "mdate": 1698557645465, "content": {"title": "Open-set Semantic Segmentation for Point Clouds via Adversarial Prototype Framework", "abstract": "much attention in computer vision. Most of the existing\nworks in literature assume that the training and testing\npoint clouds have the same object classes, but they are generally\ninvalid in many real-world scenarios for identifying\nthe 3D objects whose classes are not seen in the training\nset. To address this problem, we propose an Adversarial\nPrototype Framework (APF) for handling the open-set\n3D semantic segmentation task, which aims to identify 3D\nunseen-class points while maintaining the segmentation\nperformance on seen-class points. The proposed APF\nconsists of a feature extraction module for extracting point\nfeatures, a prototypical constraint module, and a feature\nadversarial module. The prototypical constraint module is\ndesigned to learn prototypes for each seen class from point\nfeatures. The feature adversarial module utilizes generative\nadversarial networks to estimate the distribution of unseenclass\nfeatures implicitly, and the synthetic unseen-class\nfeatures are utilized to prompt the model to learn more\neffective point features and prototypes for discriminating\nunseen-class samples from the seen-class ones. Experimental\nresults on two public datasets demonstrate that the\nproposed APF outperforms the comparative methods by a\nlarge margin in most cases."}}
{"id": "xazWcilcxbU", "cdate": 1688169600000, "mdate": 1698556616205, "content": {"title": "ASPPR: active single-image piecewise planar 3D reconstruction based on geometric priors", "abstract": ""}}
{"id": "kGUivpLYUs", "cdate": 1672531200000, "mdate": 1698556616196, "content": {"title": "Distilled Heterogeneous Feature Alignment Network for SAR Image Semantic Segmentation", "abstract": "Synthetic aperture radar (SAR) image semantic segmentation has attracted increasing attention in the remote-sensing community recently, due to SAR\u2019s all-time and all-weather imaging capability. However, SAR images are generally more difficult to be segmented than their electro-optical (EO) counterparts, since speckle noises and layovers are inevitably involved in SAR images. On the other hand, EO images could only be obtained under cloud-free conditions, which limits their applications. To this end, this letter investigates how to introduce EO features to assist the training of an SAR-segmentation model so that the model could segment SAR images without their EO counterparts in the application and proposes a distilled heterogeneous feature alignment network (DHFA-Net), where an SAR-segmentation student model learns and aligns the features from a pretrained EO-segmentation teacher model. In the proposed DHFA-Net, both the student and teacher models employ an identical architecture but different parameter configurations, and a heterogeneous feature distillation module (HFDM) is explored for transferring latent EO features from the teacher model to the student model through heterogeneous feature distillation and then supervising the training of the SAR-segmentation model. Moreover, a heterogeneous feature alignment module (HFAM) is designed to aggregate multiscale features for segmentation by the feature alignment approach in each of the student and teacher models. By enabling the multiscale heterogeneous feature aggregation, the SAR segmentation performance could be boosted. Experimental results on two public datasets demonstrate the superiority of the proposed DHFA-Net."}}
{"id": "VE2Oc-5zCP", "cdate": 1672531200000, "mdate": 1698556616222, "content": {"title": "MoEP-AE: Autoencoding Mixtures of Exponential Power Distributions for Open-Set Recognition", "abstract": "Open-set recognition aims to identify unknown classes while maintaining classification performance on known classes and has attracted increasing attention in the pattern recognition field. However, how to learn effective feature representations whose distributions are usually complex for classifying both known-class and unknown-class samples when only the known-class samples are available for training is an ongoing issue in open-set recognition. In contrast to methods implementing a single Gaussian, a mixture of Gaussians (MoG), or multiple MoGs, we propose a novel autoencoder that learns feature representations by modeling them as mixtures of exponential power distributions (MoEPs) in latent spaces called MoEP-AE. The proposed autoencoder considers that many real-world distributions are sub-Gaussian or super-Gaussian and can thus be represented by MoEPs rather than a single Gaussian or an MoG or multiple MoGs. We design a differentiable sampler that can sample from an MoEP to guarantee that the proposed autoencoder is trained effectively. Furthermore, we propose an MoEP-AE-based method for open-set recognition by introducing a discrimination strategy, where the MoEP-AE is used to model the distributions of the features extracted from the input known-class samples by minimizing a designed loss function at the training stage, called MoEP-AE-OSR. Extensive experimental results in both standard-dataset and cross-dataset settings demonstrate that the MoEP-AE-OSR method outperforms 14 existing open-set recognition methods in most cases in both open-set recognition and closed-set recognition tasks."}}
{"id": "gNKj_09yUu0", "cdate": 1667381289102, "mdate": 1667381289102, "content": {"title": "Self-distilled Feature Aggregation for Self-supervised Monocular Depth Estimation", "abstract": "Self-supervised monocular depth estimation has received much attention recently in computer vision. Most of the existing works in literature aggregate multi-scale features for depth prediction via either straightforward concatenation or element-wise addition, however, such feature aggregation operations generally neglect the contextual consistency between multi-scale features. Addressing this problem, we propose the Self-Distilled Feature Aggregation (SDFA) module for simultaneously aggregating a pair of low-scale and high-scale features and maintaining their contextual consistency. The SDFA employs three branches to learn three feature offset maps respectively: one offset map for refining the input low-scale feature and the other two for refining the input high-scale feature under a designed self-distillation manner. Then, we propose an SDFA-based network for self-supervised monocular depth estimation, and design a self-distilled training strategy to train the proposed network with the SDFA module. Experimental results on the KITTI dataset demonstrate that the proposed method outperforms the comparative state-of-the-art methods in most cases."}}
{"id": "xGTxdW8KR-T", "cdate": 1640995200000, "mdate": 1667381748303, "content": {"title": "Gated Feature Aggregation for Height Estimation From Single Aerial Images", "abstract": ""}}
{"id": "wrw50dXfSY", "cdate": 1640995200000, "mdate": 1667381748496, "content": {"title": "Superpoint-guided Semi-supervised Semantic Segmentation of 3D Point Clouds", "abstract": "3D point cloud semantic segmentation is a challenging topic in the computer vision field. Most of the existing methods in literature require a large amount of fully labeled training data, but it is extremely time-consuming to obtain these training data by manually labeling massive point clouds. Addressing this problem, we propose a superpoint-guided semi-supervised segmentation network for 3D point clouds, which jointly utilizes a small portion of labeled scene point clouds and a large number of unlabeled point clouds for network training. The proposed network is iteratively updated with its predicted pseudo labels, where a superpoint generation module is introduced for extracting superpoints from 3D point clouds, and a pseudo-label optimization module is explored for automatically assigning pseudo labels to the unlabeled points under the constraint of the extracted superpoints. Additionally, there are some 3D points without pseudo-label supervision. We propose an edge prediction module to constrain features of edge points. A superpoint feature aggregation module and a superpoint feature consistency loss function are introduced to smooth superpoint features. Extensive experimental results on two 3D public datasets demonstrate that our method can achieve better performance than several state-of-the-art point cloud segmentation networks and several popular semi-supervised segmentation methods with few labeled scenes."}}
{"id": "tiNQnoV4X5z", "cdate": 1640995200000, "mdate": 1667381748306, "content": {"title": "SAR-to-Optical Image Translation With Hierarchical Latent Features", "abstract": ""}}
