{"id": "rwfhkSNlKOn", "cdate": 1672531200000, "mdate": 1684137989831, "content": {"title": "Discovering Interpretable Directions in the Semantic Latent Space of Diffusion Models", "abstract": "Denoising Diffusion Models (DDMs) have emerged as a strong competitor to Generative Adversarial Networks (GANs). However, despite their widespread use in image synthesis and editing applications, their latent space is still not as well understood. Recently, a semantic latent space for DDMs, coined `$h$-space', was shown to facilitate semantic image editing in a way reminiscent of GANs. The $h$-space is comprised of the bottleneck activations in the DDM's denoiser across all timesteps of the diffusion process. In this paper, we explore the properties of h-space and propose several novel methods for finding meaningful semantic directions within it. We start by studying unsupervised methods for revealing interpretable semantic directions in pretrained DDMs. Specifically, we show that global latent directions emerge as the principal components in the latent space. Additionally, we provide a novel method for discovering image-specific semantic directions by spectral analysis of the Jacobian of the denoiser w.r.t. the latent code. Next, we extend the analysis by finding directions in a supervised fashion in unconditional DDMs. We demonstrate how such directions can be found by relying on either a labeled data set of real images or by annotating generated samples with a domain-specific attribute classifier. We further show how to semantically disentangle the found direction by simple linear projection. Our approaches are applicable without requiring any architectural modifications, text-based guidance, CLIP-based optimization, or model fine-tuning."}}
{"id": "xtbog7cfsr", "cdate": 1663850272178, "mdate": null, "content": {"title": "The Implicit Bias of Minima Stability in Multivariate Shallow ReLU Networks", "abstract": "We study the type of solutions to which stochastic gradient descent converges when used to train a single hidden-layer multivariate ReLU network with the quadratic loss. Our results are based on a dynamical stability analysis. In the univariate case, it was shown that linearly stable minima correspond to network functions (predictors), whose second derivative has a bounded weighted $L^1$ norm. Notably, the bound gets smaller as the step size increases, implying that training with a large step size leads to `smoother' predictors. Here we generalize this result to the multivariate case, showing that a similar result applies to the Laplacian of the predictor. We demonstrate the tightness of our bound on the MNIST dataset, and show that it accurately captures the behavior of the solutions as a function of the step size. Additionally, we prove a depth separation result on the approximation power of ReLU networks corresponding to stable minima of the loss. Specifically, although shallow ReLU networks are universal approximators, we prove that stable shallow networks are not. Namely, there is a function that cannot be well-approximated by stable single hidden-layer ReLU networks trained with a non-vanishing step size. This is while the same function can be realized as a stable two hidden-layer ReLU network. Finally, we prove that if a function is sufficiently smooth (in a Sobolev sense) then it can be approximated arbitrarily well using shallow ReLU networks that correspond to stable solutions of gradient descent."}}
{"id": "B8sDrKboA8", "cdate": 1640995200000, "mdate": 1683972881728, "content": {"title": "Spectral Discovery of Jointly Smooth Features for Multimodal Data", "abstract": ""}}
{"id": "2STmSnZAEt2", "cdate": 1621629860998, "mdate": null, "content": {"title": "The Implicit Bias of Minima Stability: A View from Function Space", "abstract": "The loss terrains of over-parameterized neural networks have multiple global minima. However, it is well known that stochastic gradient descent (SGD) can stably converge only to minima that are sufficiently flat w.r.t. SGD's step size. In this paper we study the effect that this mechanism has on the function implemented by the trained model. First, we extend the existing knowledge on minima stability to non-differentiable minima, which are common in ReLU nets. We then use our stability results to study a single hidden layer univariate ReLU network. In this setting, we show that SGD is biased towards functions whose second derivative (w.r.t the input) has a bounded weighted $L_1$ norm, and this is regardless of the initialization. In particular, we show that the function implemented by the network upon convergence gets smoother as the learning rate increases. The weight multiplying the second derivative is larger around the center of the support of the training distribution, and smaller towards its boundaries, suggesting that a trained model tends to be smoother at the center of the training distribution."}}
{"id": "BTSGCyTDPN", "cdate": 1577836800000, "mdate": 1684137989850, "content": {"title": "Unique Properties of Flat Minima in Deep Networks", "abstract": "It is well known that (stochastic) gradient descent has an implicit bias towards flat minima. In deep neural network training, this mechanism serves to screen out minima. However, the precise effec..."}}
{"id": "-_QfKx-RivK", "cdate": 1546300800000, "mdate": 1684137989903, "content": {"title": "On the Minimal Overcompleteness Allowing Universal Sparse Representation", "abstract": "Sparse representation over redundant dictionaries constitute a good model for many classes of signals (e.g., patches of natural images, segments of speech signals, and so on). However, despite its popularity, a very little are known about the representation capacity of this model. In this paper, we study how redundant a dictionary must be so as to allow any vector to admit a sparse approximation with a prescribed sparsity and a prescribed level of accuracy. We address this problem, both in a worst-case setting and in an average-case one. For each scenario, we derive lower and upper bounds on the minimal required overcompleteness. Our bounds have simple closed-form expressions that allow to easily deduce the asymptotic behavior in large dimensions. In particular, we find that the required overcompleteness grows exponentially with the sparsity level and polynomially with the allowed representation error. This implies that universal sparse representation is practical only at moderate sparsity levels, but can be achieved with a relatively high accuracy. As a side effect of our analysis, we obtain a tight lower bound on the regularized incomplete beta function, which may be interesting in its own right. We illustrate the validity of our results through numerical simulations, which support our findings."}}
{"id": "rkELDjWOWH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Revealing Common Statistical Behaviors in Heterogeneous Populations", "abstract": "In many areas of neuroscience and biological data analysis, it is desired to reveal common patterns among a group of subjects. Such analyses play important roles e.g., in detecting functional brain..."}}
{"id": "7cqI-s5R2iZ", "cdate": 1483228800000, "mdate": 1684137989868, "content": {"title": "Differential microphone arrays for the underwater acoustic channel", "abstract": "Design of underwater acoustic sensing and communication systems is a very challenging task due to several channel effects like multipath propagation and Doppler spread. In order to cope with these effects, beamforming techniques have been applied to the design of such systems. The broadband nature of acoustic systems motivates the use of beamformers with frequency-invariant beampattern. Moreover, in some cases, these systems are limited by their physical dimensions. Differential microphone arrays (DMAs) beamformers, which have been used extensively in recent years for broadband audio signals, may comply with these requirements. DMAs are small-size arrays which can provide almost frequency-invariant beampatterns and high directivity. In this paper, we present a pool experiment which shows the compatibility of DMAs for the underwater acoustic channel. Additionally, we show how to compensate for the array mismatch errors leading to much better performance level and robust beamformers."}}
