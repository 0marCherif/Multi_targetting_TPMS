{"id": "HYZlHnVNfQq", "cdate": 1648669869421, "mdate": 1648669869421, "content": {"title": "Wasserstein Distributionally Robust Optimization via Wasserstein Barycenters", "abstract": "In many applications in statistics and machine learning, the availability of data samples from multiple sources has become increasingly prevalent. On the other hand, in distributionally robust optimization, we seek data-driven decisions which perform well under the most adverse distribution from a nominal distribution constructed from data samples within a certain distance of probability distributions. However, it remains unclear how to achieve such distributional robustness when data samples from multiple sources are available. In this paper, we propose constructing the nominal distribution in Wasserstein distributionally robust optimization problems through the notion of Wasserstein barycenter as an aggregation of data samples from multiple sources. Under specific choices of the loss function, the proposed formulation admits a tractable reformulation as a finite convex program, with powerful finite-sample and asymptotic guarantees. We illustrate our proposed method through concrete examples with nominal distributions of location-scatter families and distributionally robust maximum likelihood estimation."}}
{"id": "nPG7aVYvGpF", "cdate": 1546300800000, "mdate": null, "content": {"title": "Global Convergence of Block Coordinate Descent in Deep Learning", "abstract": "Deep learning has aroused extensive attention due to its great empirical success. The efficiency of the block coordinate descent (BCD) methods has been recently demonstrated in deep neural network ..."}}
{"id": "QIRDWgyfRMj", "cdate": 1546300800000, "mdate": null, "content": {"title": "Optimal Multivariate Gaussian Fitting with Applications to PSF Modeling in Two-Photon Microscopy Imaging", "abstract": "Fitting Gaussian functions to empirical data is a crucial task in a variety of scientific applications, especially in image processing. However, most of the existing approaches for performing such fitting are restricted to two dimensions and they cannot be easily extended to higher dimensions. Moreover, they are usually based on alternating minimization schemes which benefit from few theoretical guarantees in the underlying nonconvex setting. In this paper, we provide a novel variational formulation of the multivariate Gaussian fitting problem, which is applicable to any dimension and accounts for possible nonzero background and noise in the input data. The block multiconvexity of our objective function leads us to propose a proximal alternating method to minimize it in order to estimate the Gaussian shape parameters. The resulting FIGARO algorithm is shown to converge to a critical point under mild assumptions. The algorithm shows a good robustness when tested on synthetic datasets. To demonstrate the versatility of FIGARO, we also illustrate its excellent performance in the fitting of the point spread functions of experimental raw data from a two-photon fluorescence microscope."}}
{"id": "HycIjFkPM", "cdate": 1518472018075, "mdate": null, "content": {"title": "A Proximal Block Coordinate Descent Algorithm for Deep Neural Network Training", "abstract": "Training deep neural networks (DNNs) efficiently is a challenge due to the associated highly nonconvex optimization. The backpropagation (backprop) algorithm has long been the most widely used algorithm for gradient computation of parameters of DNNs and is used along with gradient descent-type algorithms for this optimization task. Recent work have shown the efficiency of block coordinate descent (BCD) type methods empirically for training DNNs. In view of this, we propose a novel algorithm based on the BCD method for training DNNs and provide its global convergence results built upon the powerful framework of the Kurdyka-Lojasiewicz (KL) property. Numerical experiments on standard datasets demonstrate its competitive efficiency against standard optimizers with backprop. "}}
{"id": "ZY--z21wOb", "cdate": 1514764800000, "mdate": null, "content": {"title": "Optimal multivariate Gaussian fitting for PSF modeling in two-photon microscopy", "abstract": "Fitting multivariate Gaussian functions constitutes a fundamental task in many scientific fields. However, most of the existing approaches for performing such fitting are restricted to 2 dimensions and they cannot be easily extended to higher dimensions. One of the main applicative areas where it is necessary to go beyond the existing techniques is the modeling of Point Spread Functions in 3D imaging. In this paper, a novel variational approach is proposed to fit multivariate Gaussians from noisy data in arbitrary dimensions. The proposed FIGARO algorithm is applied to two-photon fluorescence microscopy where its excellent performance is demonstrated."}}
{"id": "jZBkhdxZDV", "cdate": 1483228800000, "mdate": null, "content": {"title": "Accelerated Block Coordinate Proximal Gradients with Applications in High Dimensional Statistics", "abstract": "Nonconvex optimization problems arise in different research fields and arouse lots of attention in signal processing, statistics and machine learning. In this work, we explore the accelerated proximal gradient method and some of its variants which have been shown to converge under nonconvex context recently. We show that a novel variant proposed here, which exploits adaptive momentum and block coordinate update with specific update rules, further improves the performance of a broad class of nonconvex problems. In applications to sparse linear regression with regularizations like Lasso, grouped Lasso, capped $\\ell_1$ and SCAP, the proposed scheme enjoys provable local linear convergence, with experimental justification."}}
