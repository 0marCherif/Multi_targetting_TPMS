{"id": "wRyvvlIF21", "cdate": 1680307200000, "mdate": 1681652558775, "content": {"title": "Self-taught cross-domain few-shot learning with weakly supervised object localization and task-decomposition", "abstract": ""}}
{"id": "PLf-SIFxmAr", "cdate": 1677628800000, "mdate": 1681652558783, "content": {"title": "G2LP-Net: Global to Local Progressive Video Inpainting Network", "abstract": ""}}
{"id": "wz7FDSXkh8", "cdate": 1672531200000, "mdate": 1681652557276, "content": {"title": "Conformal Loss-Controlling Prediction", "abstract": ""}}
{"id": "hNaYliwF5ZV", "cdate": 1672531200000, "mdate": 1681652559100, "content": {"title": "Memorizing Complementation Network for Few-Shot Class-Incremental Learning", "abstract": ""}}
{"id": "FRheDg5ou6", "cdate": 1672531200000, "mdate": 1681652559930, "content": {"title": "Complementary Calibration: Boosting General Continual Learning With Collaborative Distillation and Self-Supervision", "abstract": ""}}
{"id": "Eg5Np4J3hb", "cdate": 1672531200000, "mdate": 1681652556816, "content": {"title": "Lightweight MIMO-WNet for single image deblurring", "abstract": ""}}
{"id": "CmwMEieX50", "cdate": 1672531200000, "mdate": 1681652558444, "content": {"title": "USER: Unified Semantic Enhancement with Momentum Contrast for Image-Text Retrieval", "abstract": ""}}
{"id": "zmIw0nOGHOa", "cdate": 1640995200000, "mdate": 1668513717013, "content": {"title": "Heterogeneous memory enhanced graph reasoning network for cross-modal retrieval", "abstract": "Cross-modal retrieval (CMR) aims to retrieve the instances of a specific modality that are relevant to a given query from another modality, which has drawn much attention because of its importance in bridging vision with language. A key to the success of CMR is to learn more discriminative and robust representations for both visual and textual instances to further reduce the heterogeneous discrepancy existing in different modalities. In this paper, we address this challenging issue by proposing a heterogeneous memory enhanced graph reasoning network, named HMGR, to connect the semantic correlations between vision and language. On the one hand, we design a novel dual-path network architecture to generate relationship enhanced global representations by employing modality-specific graph reasoning on extracted local features for each instance. In this way, the topological interdependencies of both visual and textual intra-instance local fragments are fully mined to achieve a deeper semantic understanding of the relationships between them. On the other hand, we focus on utilizing inter-instance semantic correlated knowledge to enhance the discriminability of the final learned representations, which is achieved by introducing a joint heterogeneous memory network to iteratively restore both visual and textual instance-level information. Through interacting with long-term contextual multimodal knowledge, an encouraging shared latent feature space for mitigating the heterogeneous gap across different modalities can be learned. Extensive experiments under both image-text retrieval and video-text retrieval scenarios on three benchmark datasets demonstrate the effectiveness of our proposed method."}}
{"id": "wnj3wcMCoTe", "cdate": 1640995200000, "mdate": 1668040602061, "content": {"title": "Non-linear perceptual multi-scale network for single image super-resolution", "abstract": ""}}
{"id": "uE3XAqVFV7q", "cdate": 1640995200000, "mdate": 1667640377536, "content": {"title": "Local spatial alignment network for few-shot learning", "abstract": ""}}
