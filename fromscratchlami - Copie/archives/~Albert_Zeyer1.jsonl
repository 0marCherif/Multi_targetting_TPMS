{"id": "sC_dFPFUArG", "cdate": 1640995200000, "mdate": 1682319258493, "content": {"title": "Monotonic segmental attention for automatic speech recognition", "abstract": "We introduce a novel segmental-attention model for automatic speech recognition. We restrict the decoder attention to segments to avoid quadratic runtime of global attention, better generalize to long sequences, and eventually enable streaming. We directly compare global-attention and different segmental-attention modeling variants. We develop and compare two separate time-synchronous decoders, one specifically taking the segmental nature into account, yielding further improvements. Using time-synchronous decoding for segmental models is novel and a step towards streaming applications. Our experiments show the importance of a length model to predict the segment boundaries. The final best segmental-attention model using segmental decoding performs better than global-attention, in contrast to other monotonic attention approaches in the literature. Further, we observe that the segmental model generalizes much better to long sequences of up to several minutes."}}
{"id": "gusznRKCPI", "cdate": 1640995200000, "mdate": 1682319258288, "content": {"title": "Monotonic Segmental Attention for Automatic Speech Recognition", "abstract": "We introduce a novel segmental-attention model for automatic speech recognition. We restrict the decoder attention to segments to avoid quadratic runtime of global attention, better generalize to long sequences, and eventually enable streaming. We directly compare global-attention and different segmental-attention modeling variants. We develop and compare two separate time-synchronous decoders, one specifically taking the segmental nature into account, yielding further improvements. Using time-synchronous decoding for segmental models is novel and a step towards streaming applications. Our experiments show the importance of a length model to predict the segment boundaries. The final best segmental-attention model using segmental decoding performs better than global-attention, in contrast to other monotonic attention approaches in the literature. Further, we observe that the segmental model generalizes much better to long sequences of up to several minutes."}}
{"id": "JcsV_4ckVf", "cdate": 1640995200000, "mdate": 1682319258201, "content": {"title": "Neural network based modeling and architectures for automatic speech recognition and machine translation", "abstract": "Unsere Arbeit zielt darauf ab, das Feld und die Anwendung von neuronalen Netzwerken (NN) voranzubringen, Sequenz-zu-Sequenz-Modelle voranzutreiben durch Erweiterungen und Entwicklung neuer Modelle, und die Trainingsmethoden der Modelle zu verbessern. Wir f\u00fchren die erste umfassende Studie von Long Short-Term Memory (LSTM) akustischen Modellen durch und verbessern sie gegen\u00fcber unserem vorw\u00e4rtsgericheten (feed-forward) neuronalen Netzwerk (FFNN) um 16% relativ. Wir waren unter den Ersten, bidirektionale LSTMs (BLSTMs) f\u00fcr die Online-Erkennung eingesetzt haben. Wir haben erfolgreich konvolutionelle (convolutional) neuronale Netzwerk (CNN) Modelle trainiert, die mit unserem BLSTM-Modell konkurrenzf\u00e4hig sind. Wir sind die ersten, die verschiedene Layer-normalisierte (LN) LSTM Varianten vergleichen, um direkte und umfassende Studien durchzuf\u00fchren. Wir studieren die Auswirkungen auf Trainingsstabilit\u00e4t, Konvergenz und Varianz. Wir erhalten Verbesserungen um 10% relativ gegen\u00fcber der Standard BLSTM Baseline. Au\u00dferdem f\u00fchren wir eine umfassende Studie zu Transformer-Modellen im Vergleich zum LSTM durch. Wir untersuchen Transformer-Sprachmodelle und erreichen Stand-der-Technik-Ergebnisse mit 6% relativen Verbesserungen gegen\u00fcber dem besten LSTM. Als Alternative zu den hybriden neuronalen Netzwerk (NN)- hidden Markov Modelle (HMM) untersuchen wir Sequenz-zu-Sequenz-Modelle wie z.B. Attention-basierte Encoder-Decoder-Modelle. Wir entwickeln moderne Attention-basierte Modelle f\u00fcr maschinelle \u00dcbersetzung und Spracherkennung, die mit Byte-Pair encodierten (BPE) Teilw\u00f6rtern arbeiten. Monotonizit\u00e4t und online Erkennungliefern die Motivation f\u00fcr unsere einfache Variante der lokalen Attention. Wir erweitern diese Arbeit um einen prinzipiellen Ansatz mit expliziter latenten Variable, und f\u00fchren Latent-Attention-Modelle ein,mit Hard-Attention als Spezialfall, die eine neue Klasse von segmentellen Modellen darstellen. Wir zeigen die \u00c4quivalenz von segmentellen und Transducer-Modellen, und schlagen eine neue Klasse von verallgemeinerten und erweiterten Transducer-Modellen vor,die akkurater sind und besser generalisieren als unsere Attention-Modelle.Wir f\u00fchren eine umfassende Studie \u00fcber alle existierenden Varianten aus der Literatur als Spezialf\u00e4lle unseres verallgemeinerten und erweiterten Modells durch und zeigen die Effektivit\u00e4t unserer Erweiterungen. Wir beobachten, dass die Trainingsstrategien die wichtigste Rolle f\u00fcr eine gute Leistung spielen. Wir untersuchen auf Trainingskriterien, Optimierungstechniken, Lernratensteuerung, Vortrainierung, Regularisierung und Daten Augmentierung. Wir schlagen neuartige Vortrainierungsmethodenf\u00fcr LSTM und Ende-zu-Ende-Modelle vor, bei denen das neuronale Netzwerk in Tiefe und Breite vergr\u00f6\u00dfert wird. Wir untersuchen verschiedene Arten der Trainingsvarianz aufgrund von Zuf\u00e4lligkeiten beim Trainieren verursacht durch variierende Zufallsseeds und nicht-deterministische Trainingsalgorithmen. Wir sind unter den ersten, die einen hohen Einfluss der Anzahl der Trainingsepochen beobachten und dokumentieren. Wir schlagen ein neues verallgemeinertes Trainingsverfahren f\u00fcr hybride NN-HMM vor, bei dem die volle Summe \u00fcber alle Alignierungen durchgef\u00fchrt wird, und identifizieren Connectionist Temporal Classification (CTC) als einen Spezialfall davon. Wir erarbeiten eine mathematische Analyse \u00fcber das spitze Verhalten von CTC. Dies ist die erste Arbeit, welche das spitze Verhalten und Konvergenzeigenschaften von CTC auf einer mathematischen Ebene erkl\u00e4ren und herleiten kann. Wir entwickeln gro\u00dfe Anteile von RETURNN als effizientes und flexibles Software Framework einschlie\u00dflich einer effizienten Suchimplementierung um alle Experimente durchzuf\u00fchren. Dieses Framework und die meisten unserer Ergebnisse und Baselines werden innerhalb des Teams und dar\u00fcber hinaus benutzt. Alle unsere Arbeiten sind publiziert und der gesamte Code und alle Setups sind online verf\u00fcgbar. Our work aims to advance the field and application of neural networks, to advance sequence-to-sequence architectures by extending and developing new approaches, and to advance training methods. We perform the first comprehensive study of long short-term memory (LSTM) acoustic models and improve over our feed-forward neural network (FFNN) baseline by 16% relative. We are among the first to apply bidirectional LSTMs (BLSTMs) for online recognition. We successfully train convolutional neural network (CNN) models (ResNet and layer-wise context expansion with attention (LACE)) which are competitive with our BLSTM model. We are the first to compare different layer-normalized (LN) LSTM variants, to perform direct and comprehensive studies, and to study the effect on training stability, convergence and variance. We get improvements of 10% relative over the standard LSTM baseline. We further perform a comprehensive study on Transformer models in comparison to LSTMs, and we study Transformer language models and reach state-of-the-art results with 6% relative improvements over the best LSTM. We aim to advance the status quo which is the hybrid neural network (NN)-hidden Markov model (HMM) by investigating alternative sequence-to-sequence architectures such as attention-based encoder-decoder models. We develop state-of-the-art attention-based models for machine translation and speech recognition, operating on byte-pair encoding (BPE) subword labels. With the motivation to introduce monotonicity and potential streaming, we propose a simple local windowed attention variant. We extend this work further through a principled approach of having an explicit latent variable, and introduce latent attention models with hard attention as a special case, which are a novel class of segmental models. We discover the equivalence of segmental and transducer models, and propose a novel class of generalized and extended transducer models, which perform and generalize better than our attention models. We perform a comprehensive study on all existing variants from the literature as special cases of our generalized and extended model and show the effectiveness of our extensions. We observe that training strategies play the most important role in good performance. We investigate training criteria, optimization techniques, learning rate scheduling, pretraining, regularization and data augmentation. We propose novel pretraining schemes for LSTM and end-to-end models, where we grow the depth and width of the neural network. We investigate different types of training variance due to randomness in the training caused by varying random seeds and non-deterministic training algorithms. We are among the first to observe and document the high impact of the number of training epochs. We propose a novel generalized training procedure for hybrid NN-HMMs where we calculate the full sum over all alignments, and we identify connectionist temporal classification (CTC) as a special case of this. We further provide a mathematical analysis of the peaky behavior of CTC, making this the first work to explain the peaky behavior and convergence properties on a mathematical level. We develop large parts of RETURNN as an efficient and flexible software framework including beam search to perform all the experiments. This framework and most of our results and baselines are widely used among the team and beyond. All of our work is published and all code and setups are available online. Zeyer, Albert; Ney, Hermann; Watanabe, Shinji; Leibe, Bastian"}}
{"id": "D6lXOMhMR4D", "cdate": 1640995200000, "mdate": 1682319258569, "content": {"title": "Discrete Steps towards Approximate Computing", "abstract": "As long as a computational precision above 8 bits is preferred, digital design generally outperforms analog one incurring less hardware cost. This motivates our recent studies on digital approximate computing as presented in this paper. Rather than using fixed-point numbers, discrete steps of approximation using floating-point number representations such as BFloat16 and posit formats are explored particularly. Time-domain computing is addressed as well which starts in the digital domain with discrete delay values and moves towards the analog domain under increased delay uncertainties when pushed for energy efficiency by voltage scaling. The proposed approximate arithmetic and nonlinear activation functions are further evaluated in various artificial neural networks achieving competitive Quality-of-Service compared to the state-of-the-art with full-precision computing."}}
{"id": "yiltlxBNS6", "cdate": 1609459200000, "mdate": 1682319258627, "content": {"title": "Equivalence of Segmental and Neural Transducer Modeling: A Proof of Concept", "abstract": "With the advent of direct models in automatic speech recognition (ASR), the formerly prevalent frame-wise acoustic modeling based on hidden Markov models (HMM) diversified into a number of modeling architectures like encoder-decoder attention models, transducer models and segmental models (direct HMM). While transducer models stay with a frame-level model definition, segmental models are defined on the level of label segments directly. While (soft-)attention-based models avoid explicit alignment, transducer and segmental approach internally do model alignment, either by segment hypotheses or, more implicitly, by emitting so-called blank symbols. In this work, we prove that the widely used class of RNN-Transducer models and segmental models (direct HMM) are equivalent and therefore show equal modeling power. It is shown that blank probabilities translate into segment length probabilities and vice versa. In addition, we provide initial experiments investigating decoding and beam-pruning, comparing time-synchronous and label-/segment-synchronous search strategies and their properties using the same underlying model."}}
{"id": "WcALSdxeNYz", "cdate": 1609459200000, "mdate": 1682319258576, "content": {"title": "Investigating Methods to Improve Language Model Integration for Attention-Based Encoder-Decoder ASR Models", "abstract": "Attention-based encoder-decoder (AED) models learn an implicit internal language model (ILM) from the training transcriptions. The integration with an external LM trained on much more unpaired text usually leads to better performance. A Bayesian interpretation as in the hybrid autoregressive transducer (HAT) suggests dividing by the prior of the discriminative acoustic model, which corresponds to this implicit LM, similarly as in the hybrid hidden Markov model approach. The implicit LM cannot be calculated efficiently in general and it is yet unclear what are the best methods to estimate it. In this work, we compare different approaches from the literature and propose several novel methods to estimate the ILM directly from the AED model. Our proposed methods outperform all previous approaches. We also investigate other methods to suppress the ILM mainly by decreasing the capacity of the AED model, limiting the label context, and also by training the AED model together with a pre-existing LM."}}
{"id": "W57jBrYCHBI", "cdate": 1609459200000, "mdate": 1682319258639, "content": {"title": "Librispeech Transducer Model with Internal Language Model Prior Correction", "abstract": "We present our transducer model on Librispeech. We study variants to include an external language model (LM) with shallow fusion and subtract an estimated internal LM. This is justified by a Bayesian interpretation where the transducer model prior is given by the estimated internal LM. The subtraction of the internal LM gives us over 14% relative improvement over normal shallow fusion. Our transducer has a separate probability distribution for the non-blank labels which allows for easier combination with the external LM, and easier estimation of the internal LM. We additionally take care of including the end-of-sentence (EOS) probability of the external LM in the last blank probability which further improves the performance. All our code and setups are published."}}
{"id": "LPwhbjlLEU", "cdate": 1609459200000, "mdate": 1682319258644, "content": {"title": "Why does CTC result in peaky behavior?", "abstract": "The peaky behavior of CTC models is well known experimentally. However, an understanding about why peaky behavior occurs is missing, and whether this is a good property. We provide a formal analysis of the peaky behavior and gradient descent convergence properties of the CTC loss and related training criteria. Our analysis provides a deep understanding why peaky behavior occurs and when it is suboptimal. On a simple example which should be trivial to learn for any model, we prove that a feed-forward neural network trained with CTC from uniform initialization converges towards peaky behavior with a 100% error rate. Our analysis further explains why CTC only works well together with the blank label. We further demonstrate that peaky behavior does not occur on other related losses including a label prior model, and that this improves convergence."}}
{"id": "JazQFcGGvLX", "cdate": 1609459200000, "mdate": null, "content": {"title": "Investigating Methods to Improve Language Model Integration for Attention-based Encoder-Decoder ASR Models", "abstract": "Attention-based encoder-decoder (AED) models learn an implicit internal language model (ILM) from the training transcriptions. The integration with an external LM trained on much more unpaired text usually leads to better performance. A Bayesian interpretation as in the hybrid autoregressive transducer (HAT) suggests dividing by the prior of the discriminative acoustic model, which corresponds to this implicit LM, similarly as in the hybrid hidden Markov model approach. The implicit LM cannot be calculated efficiently in general and it is yet unclear what are the best methods to estimate it. In this work, we compare different approaches from the literature and propose several novel methods to estimate the ILM directly from the AED model. Our proposed methods outperform all previous approaches. We also investigate other methods to suppress the ILM mainly by decreasing the capacity of the AED model, limiting the label context, and also by training the AED model together with a pre-existing LM."}}
{"id": "HeqhaE6_o-2", "cdate": 1609459200000, "mdate": null, "content": {"title": "Librispeech Transducer Model with Internal Language Model Prior Correction", "abstract": "We present our transducer model on Librispeech. We study variants to include an external language model (LM) with shallow fusion and subtract an estimated internal LM. This is justified by a Bayesian interpretation where the transducer model prior is given by the estimated internal LM. The subtraction of the internal LM gives us over 14% relative improvement over normal shallow fusion. Our transducer has a separate probability distribution for the non-blank labels which allows for easier combination with the external LM, and easier estimation of the internal LM. We additionally take care of including the end-of-sentence (EOS) probability of the external LM in the last blank probability which further improves the performance. All our code and setups are published."}}
