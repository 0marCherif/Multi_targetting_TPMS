{"id": "Fj1S0SV8p3U", "cdate": 1663850459524, "mdate": null, "content": {"title": "Augmentation Curriculum Learning For Generalization in RL", "abstract": "Many Reinforcement Learning tasks rely solely on pixel-based observations of\nthe environment. During deployment, these observations can fall victim to visual\nperturbations and distortions, causing the agent\u2019s policy to significantly degrade\nin performance. This motivates the need for robust agents that can generalize in\nthe face of visual distribution shift. One common technique for doing this is to ap-\nply augmentations during training; however, it comes at the cost of performance.\nWe propose Augmentation Curriculum Learning a novel curriculum learning ap-\nproach that schedules augmentation into training into a weak augmentation phase\nand strong augmentation phase. We also introduce a novel visual augmentation\nstrategy that proves to aid in the benchmarks we evaluate on. Our method achieves\nstate-of-the-art performance on Deep Mind Control Generalization Benchmark."}}
{"id": "Ovnwe_sDQW", "cdate": 1663850442848, "mdate": null, "content": {"title": "BC-IRL: Learning Generalizable Reward Functions from Demonstrations", "abstract": "How well do reward functions learned with inverse reinforcement learning (IRL) generalize? We illustrate that state-of-the-art IRL algorithms, which maximize a maximum-entropy objective, learn rewards that overfit to the demonstrations. Such rewards struggle to provide meaningful rewards for states not covered by the demonstrations, a major detriment when using the reward to learn policies in new situations. We introduce BC-IRL a new inverse reinforcement learning method that learns reward functions that generalize better when compared to maximum-entropy IRL approaches. In contrast to the MaxEnt framework, which learns to maximize rewards around demonstrations, BC-IRL updates reward parameters such that the policy trained with the new reward matches the expert demonstrations better. We show that BC-IRL learns rewards that generalize better on an illustrative simple task and two continuous robotic control tasks, achieving over twice the success rate of baselines in challenging generalization settings."}}
{"id": "lp9foO8AFoD", "cdate": 1621629818100, "mdate": null, "content": {"title": "Generalizable Imitation Learning from Observation via Inferring Goal Proximity", "abstract": "Task progress is intuitive and readily available task information that can guide an agent closer to the desired goal. Furthermore, a task progress estimator can generalize to new situations. From this intuition, we propose a simple yet effective imitation learning from observation method for a goal-directed task using a learned goal proximity function as a task progress estimator for better generalization to unseen states and goals. We obtain this goal proximity function from expert demonstrations and online agent experience, and then use the learned goal proximity as a dense reward for policy training. We demonstrate that our proposed method can robustly generalize compared to prior imitation learning methods on a set of goal-directed tasks in navigation, locomotion, and robotic manipulation, even with demonstrations that cover only a part of the states."}}
{"id": "DPHsCQ8OpA", "cdate": 1621629721642, "mdate": null, "content": {"title": "Habitat 2.0: Training Home Assistants to Rearrange their Habitat", "abstract": "We introduce Habitat 2.0 (H2.0), a simulation platform for training virtual robots in interactive 3D environments and complex physics-enabled scenarios. We make comprehensive contributions to all levels of the embodied AI stack \u2013 data, simulation, and benchmark tasks. Specifically, we present: (i) ReplicaCAD: an artist-authored, annotated, reconfigurable 3D dataset of apartments (matching real spaces) with articulated objects (e.g. cabinets and drawers that can open/close); (ii) H2.0: a high-performance physics-enabled 3D simulator with speeds exceeding 25,000 simulation steps per second (850x real-time) on an 8-GPU node, representing 100x speed-ups over prior work; and, (iii) Home Assistant Benchmark (HAB): a suite of common tasks for assistive robots (tidy the house, stock groceries, set the table) that test a range of mobile manipulation capabilities. These large-scale engineering contributions allow us to systematically compare deep reinforcement learning (RL) at scale and classical sense-plan-act (SPA) pipelines in long-horizon structured tasks, with an emphasis on generalization to new objects, receptacles, and layouts. We find that (1) flat RL policies struggle on HAB compared to hierarchical ones; (2) a hierarchy with independent skills suffers from \u2018hand-off problems\u2019, and (3) SPA pipelines are more brittle than RL policies."}}
{"id": "L4v_5Qtshj7", "cdate": 1601308188793, "mdate": null, "content": {"title": "Goal-Driven Imitation Learning from Observation by Inferring Goal Proximity", "abstract": "Humans can effectively learn to estimate how close they are to completing a desired task simply by watching others fulfill the task. To solve the task, they can then take actions towards states with higher estimated proximity to the goal. From this intuition, we propose a simple yet effective method for imitation learning that learns a goal proximity function from expert demonstrations and online agent experience, and then uses the learned proximity to provide a dense reward signal for training a policy to solve the task. By predicting task progress as the temporal distance to the goal, the goal proximity function improves generalization to unseen states over methods that aim to directly imitate expert behaviors. We demonstrate that our proposed method efficiently learns a set of goal-driven tasks from state-only demonstrations in navigation, robotic arm manipulation, and locomotion tasks."}}
{"id": "rkx35lHKwB", "cdate": 1569439892347, "mdate": null, "content": {"title": "Generalizing Reinforcement Learning to Unseen Actions", "abstract": "A fundamental trait of intelligence is the ability to achieve goals in the face of novel circumstances. In this work, we address one such setting which requires solving a task with a novel set of actions. Empowering machines with this ability requires generalization in the way an agent perceives its available actions along with the way it uses these actions to solve tasks. Hence, we propose a framework to enable generalization over both these aspects: understanding an action\u2019s functionality, and using actions to solve tasks through reinforcement learning. Specifically, an agent interprets an action\u2019s behavior using unsupervised representation learning over a collection of data samples reflecting the diverse properties of that action. We employ a reinforcement learning architecture which works over these action representations, and propose regularization metrics essential for enabling generalization in a policy. We illustrate the generalizability of the representation learning method and policy, to enable zero-shot generalization to previously unseen actions on challenging sequential decision-making environments. Our results and videos can be found at sites.google.com/view/action-generalization/"}}
