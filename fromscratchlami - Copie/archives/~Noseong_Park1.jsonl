{"id": "ZnkYg2uOjWr", "cdate": 1672531200000, "mdate": 1681649692766, "content": {"title": "Regular Time-series Generation using SGM", "abstract": ""}}
{"id": "Nw_h97fgy65", "cdate": 1672531200000, "mdate": 1681649692894, "content": {"title": "Graph Neural Rough Differential Equations for Traffic Forecasting", "abstract": ""}}
{"id": "DWFwDLMNrv", "cdate": 1672531200000, "mdate": 1681649693297, "content": {"title": "Learnable Path in Neural Controlled Differential Equations", "abstract": ""}}
{"id": "9Vo61OWv_P", "cdate": 1672531200000, "mdate": 1681649693083, "content": {"title": "Enabling Hard Constraints in Differentiable Neural Network and Accelerator Co-Exploration", "abstract": ""}}
{"id": "rd7vVgmP3P", "cdate": 1671599769221, "mdate": 1671599769221, "content": {"title": "MONSTOR: An Inductive Approach for Estimating and Maximizing Influence over Unseen Networks", "abstract": "Influence maximization (IM) is one of the most important problems in social network analysis. Its objective is to find a given number of seed nodes that maximize the spread of information through a social network. Since it is an NP-hard problem, many approximate/heuristic methods have been developed, and a number of them repeat Monte Carlo (MC) simulations over and over to reliably estimate the influence (i.e., the number of infected nodes) of a seed set. In this work, we present an inductive machine learning method, called Monte Carlo Simulator (MONSTOR), for estimating the influence of given seed nodes in social networks unseen during training. To the best of our knowledge, MONSTOR is the first inductive method for this purpose. MONSTOR can greatly accelerate existing IM algorithms by replacing repeated MC simulations. In our experiments, MONSTOR provided highly accurate estimates, achieving 0.998 or higher Pearson and Spearman correlation coefficients in unseen real-world social networks. Moreover, IM algorithms equipped with MONSTOR are more accurate than state-of-the-art competitors in 63% of IM use cases."}}
{"id": "EOjCF9cA7q", "cdate": 1667492818155, "mdate": 1667492818155, "content": {"title": "Zero-Shot Quantization Brought Closer to the Teacher", "abstract": "Model quantization is considered as a promising method\nto greatly reduce the resource requirements of deep neural\nnetworks. To deal with the performance drop induced by\nquantization errors, a popular method is to use training data\nto fine-tune quantized networks. In real-world environments,\nhowever, such a method is frequently infeasible because\ntraining data is unavailable due to security, privacy, or confidentiality concerns. Zero-shot quantization addresses such\nproblems, usually by taking information from the weights of\na full-precision teacher network to compensate the performance drop of the quantized networks. In this paper, we first\nanalyze the loss surface of state-of-the-art zero-shot quantization techniques and provide several findings. In contrast\nto usual knowledge distillation problems, zero-shot quantization often suffers from 1) the difficulty of optimizing multiple\nloss terms together, and 2) the poor generalization capability\ndue to the use of synthetic samples. Furthermore, we observe\nthat many weights fail to cross the rounding threshold during\ntraining the quantized networks even when it is necessary\nto do so for better performance. Based on the observations,\nwe propose AIT, a simple yet powerful technique for zeroshot quantization, which addresses the aforementioned two\nproblems in the following way: AIT i) uses a KL distance\nloss only without a cross-entropy loss, and ii) manipulates\ngradients to guarantee that a certain portion of weights are\nproperly updated after crossing the rounding thresholds. Experiments show that AIT outperforms the performance of\nmany existing methods by a great margin, taking over the\noverall state-of-the-art position in the field"}}
{"id": "YurfS_kh5ib", "cdate": 1663850248876, "mdate": null, "content": {"title": "Partial Differential Equation-Regularized Neural Networks: An Application to Image Classification", "abstract": "Differential equations can be used to design neural networks. For instance, neural ordinary differential equations (neural ODEs) can be considered as a continuous generalization of residual networks. In this work, we present a novel partial differential equation (PDE)-based approach for image classification, where we construct a continuous-depth and continuous-width neural network as a form of solutions of PDEs, and the PDEs defining the evolution of the solutions also are learned from data. Owing to the recent advancement of identifying PDEs, the presented novel concept, called PR-Net, can be implemented. Our method shows comparable (or better) accuracy and robustness for various datasets and tasks in comparison with neural ODEs and Isometric MobileNet V3. Thanks to the efficient nature of PR-Net, it is suitable to be deployed in resource-scarce environments, e.g., deployed instead of MobileNet."}}
{"id": "1mNssCWt_v", "cdate": 1663850146611, "mdate": null, "content": {"title": "STaSy: Score-based Tabular data Synthesis", "abstract": "Tabular data synthesis is a long-standing research topic in machine learning. Many different methods have been proposed over the past decades, ranging from statistical methods to deep generative methods. However, it has not always been successful due to the complicated nature of real-world tabular data. In this paper, we present a new model named $\\textbf{S}$core-based $\\textbf{Ta}$bular data $\\textbf{Sy}$nthesis ($\\texttt{STaSy}$) and its training strategy based on the paradigm of score-based generative modeling. Despite the fact that score-based generative models have resolved many issues in generative models, there still exists room for improvement in tabular data synthesis. Our proposed training strategy includes a self-paced learning technique and a fine-tuning strategy, which further increases the sampling quality and diversity by stabilizing the denoising score matching training. Furthermore, we also conduct rigorous experimental studies in terms of the generative task trilemma: sampling quality, diversity, and time. In our experiments with 15 benchmark tabular datasets and 7 baselines, our method outperforms existing methods in terms of task-dependant evaluations and diversity.\n"}}
{"id": "9XFX-DdkGp9", "cdate": 1663850126286, "mdate": null, "content": {"title": "SPI-GAN: Denoising Diffusion GANs with Straight-Path Interpolations", "abstract": "Score-based generative models (SGMs) are a recently proposed paradigm for deep generative tasks and now show the state-of-the-art sampling performance. It is known that the original SGM design solves the two problems of the generative trilemma: i) sampling quality, and ii) sampling diversity. However, the last problem of the trilemma was not solved, i.e., their training/sampling complexity is notoriously high. To this end, combining SGMs with simpler models, e.g., generative adversarial networks (GANs), is gathering much attention currently. We present an enhanced denoising method using GANs, called straight-path interpolation GAN (SPI-GAN), which drastically reduces the sampling time while achieving as high sampling quality and diversity as SGMs. Our SPI-GAN can be compared to the state-of-the-art shortcut-based denoising method using GANs, called denoising diffusion GAN (DD-GAN). However, our method corresponds to an extreme method that does not use any intermediate shortcut information of the reverse SDE path, in which case DD-GAN ($K=1$) fails to obtain good results. Nevertheless, our straight-path interpolation method greatly stabilizes the overall training process. As a result, SPI-GAN is one of the best-balanced models in terms of the sampling quality/diversity/time for CIFAR-10, CelebA-HQ-256, and LSUN-Church-256."}}
{"id": "P5ZTXA7zy6", "cdate": 1663850120701, "mdate": null, "content": {"title": "When Neural ODEs meet Neural Operators", "abstract": "Differential equation-based neural networks perform well in a variety of deep learning fields. Among those many methods, neural ordinary differential equations (NODEs) are one of the most fundamental work. NODEs have been applied to general downstream tasks such as image classification, time series classification, and image generation. The ODE function of NODEs can be understood as a special type of differential operators, which had been overlooked before.  In this paper, therefore, we study the feasibility of modeling NODEs (or the ODE function of NODEs) as neural operators. Our neural operator-based methods are more rigorous than existing approaches when it comes to learning the differential operator (or the ODE function). To this end, we design a new neural operator structure called branched Fourier neural operator (BFNO), which is suitable for modeling the ODE function. It shows improved performance for several general machine learning tasks, as compared to existing various NODE models."}}
