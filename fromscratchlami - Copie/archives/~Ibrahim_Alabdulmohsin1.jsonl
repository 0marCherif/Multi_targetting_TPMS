{"id": "h3RYh6IBBS", "cdate": 1652737683644, "mdate": null, "content": {"title": "Revisiting Neural Scaling Laws in Language and Vision", "abstract": "The remarkable progress in deep learning in recent years is largely driven by improvements in scale, where bigger models are trained on larger datasets for longer schedules. To predict the benefit of scale empirically, we argue for a more rigorous methodology based on the extrapolation loss, instead of reporting the best-fitting (interpolating) parameters. We then present a recipe for estimating scaling law parameters reliably from learning curves. We demonstrate that it extrapolates more accurately than previous methods in a wide range of architecture families across several domains, including image classification, neural machine translation (NMT) and  language modeling, in addition to tasks from the BIG-Bench evaluation benchmark. Finally, we release a benchmark dataset comprising of 90 evaluation tasks to facilitate research in this domain. "}}
{"id": "ogNrYe9CJlH", "cdate": 1652737682565, "mdate": null, "content": {"title": "A Reduction to Binary Approach for Debiasing Multiclass Datasets", "abstract": "We propose a novel reduction-to-binary (R2B) approach that enforces demographic parity for multiclass classification with non-binary sensitive attributes via a reduction to a sequence of binary debiasing tasks. We prove that R2B satisfies optimality and bias guarantees and demonstrate empirically that it can lead to an improvement over two baselines: (1) treating multiclass problems  as multi-label by debiasing labels independently and (2) transforming the features instead of the labels. Surprisingly, we also demonstrate that independent label debiasing yields competitive results in most (but not all) settings. We validate these conclusions on synthetic and real-world datasets from social science, computer vision, and healthcare. \n"}}
{"id": "rxrLt7rTlAr", "cdate": 1652737495520, "mdate": null, "content": {"title": "Fair Wrapping for Black-box Predictions", "abstract": "We introduce a new family of techniques to post-process (``wrap\") a black-box classifier in order to reduce its bias. Our technique builds on the recent analysis of improper loss functions whose optimization can correct any twist in prediction, unfairness being treated as a twist. In the post-processing, we learn a wrapper function which we define as an $\\alpha$-tree, which modifies the prediction. We provide two generic boosting algorithms to learn $\\alpha$-trees. We show that our modification has appealing properties in terms of composition of $\\alpha$-trees, generalization, interpretability, and KL divergence between modified and original predictions. We exemplify the use of our technique in three fairness notions: conditional value-at-risk, equality of opportunity, and statistical parity; and provide experiments on several readily available datasets."}}
{"id": "K-A4tDJ6HHf", "cdate": 1652737411903, "mdate": null, "content": {"title": "Diagnosing failures of fairness transfer across distribution shift in real-world medical settings", "abstract": "Diagnosing and mitigating changes in model fairness under distribution shift is an important component of the safe deployment of machine learning in healthcare settings. Importantly, the success of any mitigation strategy strongly depends on the \\textit{structure} of the shift. Despite this, there has been little discussion of how to empirically assess the structure of a distribution shift that one is encountering in practice. In this work, we adopt a causal framing to motivate conditional independence tests as a key tool for characterizing distribution shifts. Using our approach in two medical applications, we show that this knowledge can help diagnose failures of fairness transfer, including cases where real-world shifts are more complex than is often assumed in the literature. Based on these results, we discuss potential remedies at each step of the machine learning pipeline."}}
{"id": "uqmgHNR-qs", "cdate": 1640995200000, "mdate": 1667589760903, "content": {"title": "Maintaining fairness across distribution shift: do we have viable solutions for real-world applications?", "abstract": "Fairness and robustness are often considered as orthogonal dimensions when evaluating machine learning models. However, recent work has revealed interactions between fairness and robustness, showing that fairness properties are not necessarily maintained under distribution shift. In healthcare settings, this can result in e.g. a model that performs fairly according to a selected metric in \"hospital A\" showing unfairness when deployed in \"hospital B\". While a nascent field has emerged to develop provable fair and robust models, it typically relies on strong assumptions about the shift, limiting its impact for real-world applications. In this work, we explore the settings in which recently proposed mitigation strategies are applicable by referring to a causal framing. Using examples of predictive models in dermatology and electronic health records, we show that real-world applications are complex and often invalidate the assumptions of such methods. Our work hence highlights technical, practical, and engineering gaps that prevent the development of robustly fair machine learning models for real-world applications. Finally, we discuss potential remedies at each step of the machine learning pipeline."}}
{"id": "qwBPHCsjW9", "cdate": 1640995200000, "mdate": 1667589760964, "content": {"title": "Revisiting Neural Scaling Laws in Language and Vision", "abstract": "The remarkable progress in deep learning in recent years is largely driven by improvements in scale, where bigger models are trained on larger datasets for longer schedules. To predict the benefit of scale empirically, we argue for a more rigorous methodology based on the extrapolation loss, instead of reporting the best-fitting (interpolating) parameters. We then present a recipe for estimating scaling law parameters reliably from learning curves. We demonstrate that it extrapolates more accurately than previous methods in a wide range of architecture families across several domains, including image classification, neural machine translation (NMT) and language modeling, in addition to tasks from the BIG-Bench evaluation benchmark. Finally, we release a benchmark dataset comprising of 90 evaluation tasks to facilitate research in this domain."}}
{"id": "b8v8HplzsZ", "cdate": 1640995200000, "mdate": 1667589760984, "content": {"title": "A Reduction to Binary Approach for Debiasing Multiclass Datasets", "abstract": "We propose a novel reduction-to-binary (R2B) approach that enforces demographic parity for multiclass classification with non-binary sensitive attributes via a reduction to a sequence of binary debiasing tasks. We prove that R2B satisfies optimality and bias guarantees and demonstrate empirically that it can lead to an improvement over two baselines: (1) treating multiclass problems as multi-label by debiasing labels independently and (2) transforming the features instead of the labels. Surprisingly, we also demonstrate that independent label debiasing yields competitive results in most (but not all) settings. We validate these conclusions on synthetic and real-world datasets from social science, computer vision, and healthcare."}}
{"id": "M9rteKWX7Yy", "cdate": 1640995200000, "mdate": 1667589760864, "content": {"title": "Fair Wrapping for Black-box Predictions", "abstract": "We introduce a new family of techniques to post-process (\"wrap\") a black-box classifier in order to reduce its bias. Our technique builds on the recent analysis of improper loss functions whose optimization can correct any twist in prediction, unfairness being treated as a twist. In the post-processing, we learn a wrapper function which we define as an $\\alpha$-tree, which modifies the prediction. We provide two generic boosting algorithms to learn $\\alpha$-trees. We show that our modification has appealing properties in terms of composition of $\\alpha$-trees, generalization, interpretability, and KL divergence between modified and original predictions. We exemplify the use of our technique in three fairness notions: conditional value-at-risk, equality of opportunity, and statistical parity; and provide experiments on several readily available datasets."}}
{"id": "PO-32ODWng", "cdate": 1632875671112, "mdate": null, "content": {"title": "Improving the Post-hoc Calibration of Modern Neural Networks with Probe Scaling", "abstract": "We present \"probe scaling\": a post-hoc recipe for calibrating the predictions of modern neural networks. Our recipe is inspired by several lines of work, which demonstrate that early layers in the neural network learn general rules whereas later layers specialize. We show how such observations can be utilized in a post-hoc manner to calibrate the predictions of trained neural networks by injecting linear probes on the network's intermediate representations. Similar to temperature scaling, probe scaling neither retrains the architecture nor requires significantly more parameters. Unlike temperature scaling, however, it utilizes intermediate layers in the neural network. We demonstrate that probe scaling improves performance over temperature scaling on benchmark datasets across all five metrics: expected calibration error (ECE), negative log-likelihood, Brier score, classification accuracy, and the area under the ROC curve."}}
{"id": "H5TBqNFPKSJ", "cdate": 1621629987665, "mdate": null, "content": {"title": "A Near-Optimal Algorithm for Debiasing Trained Machine Learning Models", "abstract": "We present a scalable post-processing algorithm for debiasing trained models, including deep neural networks (DNNs), which we prove to be near-optimal by bounding its excess Bayes risk.  We empirically validate its advantages on standard benchmark datasets across both classical algorithms as well as modern DNN architectures and demonstrate that it outperforms previous post-processing methods while performing on par with in-processing. In addition, we show that the proposed algorithm is particularly effective for models trained at scale where post-processing is a natural and practical choice."}}
