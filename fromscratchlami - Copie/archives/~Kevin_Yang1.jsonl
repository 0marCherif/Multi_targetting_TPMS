{"id": "DxDZBHLsWY", "cdate": 1683881101549, "mdate": 1683881101549, "content": {"title": "Generating Multi-Step Chemical Reaction Pathways with Black-Box Optimization", "abstract": "The practical usability of de novo small molecule generation depends heavily on the synthesizability of generated molecules.\nWe propose BBO-SYN, a generative framework based on black-box optimization (BBO), which predicts diverse molecules with desired properties together with corresponding synthesis pathways. Given an input molecule A, BBO-SYN employs a state-of-the-art BBO method operating on a latent space of molecules to find a reaction partner B, which maximizes the property score of the reaction product C, as determined by a pre-trained template-free reaction predictor. This single-step reaction (A+B\u2192C) forms the basis for an optimization loop, resulting in a synthesis tree yielding products with high property scores. Empirically, the sampling and search strategy of BBO-SYN outperforms comparable baselines on four synthesis-aware optimization tasks (QED, DRD2, GSK3, and JNK3), increasing product diversity by 37% and mean property score by 25% on our hardest JNK3 task. "}}
{"id": "rKfvMyWVO0L", "cdate": 1676077286239, "mdate": null, "content": {"title": "Generating Multi-Step Chemical Reaction Pathways with Black-Box Optimization", "abstract": "The practical usability of de novo small molecule generation depends heavily on the synthesizability of generated molecules.\nWe propose BBO-SYN, a generative framework based on black-box optimization (BBO), which predicts diverse molecules with desired properties together with corresponding synthesis pathways.\nGiven an input molecule A, BBO-SYN employs a state-of-the-art BBO method operating on a latent space of molecules to find a reaction partner B, which maximizes the property score of the reaction product C, as determined by a pre-trained template-free reaction predictor. \nThis single-step reaction (A+B\u2192C) forms the basis for an optimization loop, resulting in a synthesis tree yielding products with high property scores.\nEmpirically, the sampling and search strategy of BBO-SYN outperforms comparable baselines on four synthesis-aware optimization tasks (QED, DRD2, GSK3$\\beta$, and JNK3), increasing product diversity by 37% and mean property score by 25% on our hardest JNK3 task. "}}
{"id": "ohvcvLH3FR7", "cdate": 1672531200000, "mdate": 1695950232783, "content": {"title": "PREADD: Prefix-Adaptive Decoding for Controlled Text Generation", "abstract": ""}}
{"id": "i2znBtS8BM", "cdate": 1672531200000, "mdate": 1695950232700, "content": {"title": "PREADD: Prefix-Adaptive Decoding for Controlled Text Generation", "abstract": "We propose Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation. Unlike existing methods that use auxiliary expert models to control for attributes, PREADD does not require an external model, instead relying on linearly combining output logits from multiple prompts. Specifically, PREADD contrasts the output logits generated using a raw prompt against those generated using a prefix-prepended prompt, enabling both positive and negative control with respect to any attribute encapsulated by the prefix. We evaluate PREADD on three tasks -- toxic output mitigation, gender bias reduction, and sentiment control -- and find that PREADD outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on our main metrics for each task."}}
{"id": "bc85XuTYqLb", "cdate": 1672531200000, "mdate": 1695950232693, "content": {"title": "DOC: Improving Long Story Coherence With Detailed Outline Control", "abstract": ""}}
{"id": "_M7mw0DlUE", "cdate": 1672531200000, "mdate": 1695370973655, "content": {"title": "Modular Visual Question Answering via Code Generation", "abstract": "Sanjay Subramanian, Medhini Narasimhan, Kushal Khangaonkar, Kevin Yang, Arsha Nagrani, Cordelia Schmid, Andy Zeng, Trevor Darrell, Dan Klein. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 2023."}}
{"id": "Yz2rPlgte5", "cdate": 1672531200000, "mdate": 1688239592694, "content": {"title": "Modular Visual Question Answering via Code Generation", "abstract": "We present a framework that formulates visual question answering as modular code generation. In contrast to prior work on modular approaches to VQA, our approach requires no additional training and relies on pre-trained language models (LMs), visual models pre-trained on image-caption pairs, and fifty VQA examples used for in-context learning. The generated Python programs invoke and compose the outputs of the visual models using arithmetic and conditional logic. Our approach improves accuracy on the COVR dataset by at least 3% and on the GQA dataset by roughly 2% compared to the few-shot baseline that does not employ code generation."}}
{"id": "TZFsiRoFtDh", "cdate": 1672531200000, "mdate": 1695950232766, "content": {"title": "DOC: Improving Long Story Coherence With Detailed Outline Control", "abstract": ""}}
{"id": "NE6A9yp9mV", "cdate": 1672531200000, "mdate": 1695950232691, "content": {"title": "RLCD: Reinforcement Learning from Contrast Distillation for Language Model Alignment", "abstract": "We propose Reinforcement Learning from Contrast Distillation (RLCD), a method for aligning language models to follow natural language principles without using human feedback. RLCD trains a preference model using simulated preference pairs that contain both a high-quality and low-quality example, generated using contrasting positive and negative prompts. The preference model is then used to improve a base unaligned language model via reinforcement learning. Empirically, RLCD outperforms RLAIF (Bai et al., 2022b) and context distillation (Huang et al., 2022) baselines across three diverse alignment tasks--harmlessness, helpfulness, and story outline generation--and on both 7B and 30B model scales for preference data simulation."}}
{"id": "zBweVhrOJmi", "cdate": 1640995200000, "mdate": 1681499269425, "content": {"title": "Multi-objective Optimization by Learning Space Partition", "abstract": ""}}
