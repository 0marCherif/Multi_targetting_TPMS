{"id": "Wb265hXmwq", "cdate": 1686978771053, "mdate": 1686978771053, "content": {"title": "Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detector", "abstract": "The propensity of abstractive summarization models to make factual errors has been studied extensively, including design of metrics to detect factual errors and annotation of errors in current systems\u2019 outputs. However, the ever-evolving nature of summarization systems, metrics, and annotated benchmarks makes factuality evaluation a moving target, and drawing clear comparisons among metrics has become increasingly difficult. In this work, we aggregate factuality error annotations from nine existing datasets and stratify them according to the underlying summarization model. We compare performance of state-of-the-art factuality metrics, including recent ChatGPT-based metrics, on this stratified benchmark and show that their performance varies significantly across different types of summarization models. Critically, our analysis shows that much of the re- cent improvement in the factuality detection space has been on summaries from older (pre-Transformer) models instead of more relevant recent summarization models. We further per- form a finer-grained analysis per error-type and find similar performance variance across error types for different factuality metrics. Our results show that no one metric is superior in all settings or for all error types, and we provide recommendations for best practices given these insights."}}
{"id": "fC5ODB5BFqd", "cdate": 1672531200000, "mdate": 1681502594229, "content": {"title": "WiCE: Real-World Entailment for Claims in Wikipedia", "abstract": ""}}
{"id": "jmZmRdlOTQ", "cdate": 1640995200000, "mdate": 1664911710062, "content": {"title": "SNaC: Coherence Error Detection for Narrative Summarization", "abstract": "Progress in summarizing long texts is inhibited by the lack of appropriate evaluation frameworks. When a long summary must be produced to appropriately cover the facets of that text, that summary needs to present a coherent narrative to be understandable by a reader, but current automatic and human evaluation methods fail to identify gaps in coherence. In this work, we introduce SNaC, a narrative coherence evaluation framework rooted in fine-grained annotations for long summaries. We develop a taxonomy of coherence errors in generated narrative summaries and collect span-level annotations for 6.6k sentences across 150 book and movie screenplay summaries. Our work provides the first characterization of coherence errors generated by state-of-the-art summarization models and a protocol for eliciting coherence judgments from crowd annotators. Furthermore, we show that the collected annotations allow us to train a strong classifier for automatically localizing coherence errors in generated summaries as well as benchmarking past work in coherence modeling. Finally, our SNaC framework can support future work in long document summarization and coherence evaluation, including improved summarization modeling and post-hoc summary correction."}}
{"id": "jBEKiHW8jYa", "cdate": 1640995200000, "mdate": 1681502593976, "content": {"title": "Training Dynamics for Text Summarization Models", "abstract": ""}}
{"id": "i9VH3dftJZi", "cdate": 1640995200000, "mdate": 1682621016448, "content": {"title": "HydraSum: Disentangling Style Features in Text Summarization with Multi-Decoder Models", "abstract": ""}}
{"id": "YH3ZHJhVTVU", "cdate": 1640995200000, "mdate": 1681502594018, "content": {"title": "News Summarization and Evaluation in the Era of GPT-3", "abstract": ""}}
{"id": "XA8NVMeSBq", "cdate": 1640995200000, "mdate": 1671216226758, "content": {"title": "Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors", "abstract": "The propensity of abstractive summarization models to make factual errors has been studied extensively, including design of metrics to detect factual errors and annotation of errors in current systems' outputs. However, the ever-evolving nature of summarization systems, metrics, and annotated benchmarks makes factuality evaluation a moving target, and drawing clear comparisons among metrics has become increasingly difficult. In this work, we aggregate factuality error annotations from nine existing datasets and stratify them according to the underlying summarization model. We compare performance of state-of-the-art factuality metrics, including recent ChatGPT-based metrics, on this stratified benchmark and show that their performance varies significantly across different types of summarization models. Critically, our analysis shows that much of the recent improvement in the factuality detection space has been on summaries from older (pre-Transformer) models instead of more relevant recent summarization models. We further perform a finer-grained analysis per error-type and find similar performance variance across error types for different factuality metrics. Our results show that no one metric is superior in all settings or for all error types, and we provide recommendations for best practices given these insights."}}
{"id": "OMvDcklcnM", "cdate": 1640995200000, "mdate": 1681502594141, "content": {"title": "SNaC: Coherence Error Detection for Narrative Summarization", "abstract": ""}}
{"id": "FMnMxMmr94", "cdate": 1640995200000, "mdate": 1681502593998, "content": {"title": "FALTE: A Toolkit for Fine-grained Annotation for Long Text Evaluation", "abstract": ""}}
{"id": "-UWlkLPRxU", "cdate": 1640995200000, "mdate": 1681502594018, "content": {"title": "Shortcomings of Question Answering Based Factuality Frameworks for Error Localization", "abstract": ""}}
