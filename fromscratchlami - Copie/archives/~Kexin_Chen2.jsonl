{"id": "8VUy714QE4", "cdate": 1640995200000, "mdate": 1681680961056, "content": {"title": "CARLsim 6: An Open Source Library for Large-Scale, Biologically Detailed Spiking Neural Network Simulation", "abstract": "Mature simulation systems for Spiking Neural Networks (SNNs) become more relevant than ever for understanding the brain and supporting neuromorphic computing. The CARL-sim SNN platform is one of the first Open Source simulation systems that utilized CUDA GPUs to address the tremendous parallel processing demands of natural brains. It has evolved over almost a decade in numerous scientific research projects requiring efficient biologically plausible modeling at scale. With its sixth major release, CARLsim 6 respects this legacy by supporting the latest versions of operating systems, development tool chains, multi-core computers, and of course GPUs. It runs on a range of platforms; from Notebooks up to the NVIDIA DGX-A100 supercomputer, and is used in biologically plausible simulations of the hippocampus and neocortex. The latest version has added flexibility for incorporating long-term and short-term synaptic plasticity. Neuromodulation is an important property of neurobiology that can lead to rapid few shot learning, network rewiring, and neural activity modulation. Because of this, CARLsim 6 now supports four multiple neuromodulators for simulating neural excitability and synaptic plasticity."}}
{"id": "h99x5q2r10L", "cdate": 1609459200000, "mdate": 1681680961071, "content": {"title": "Domain Adaptation In Reinforcement Learning Via Latent Unified State Representation", "abstract": "Despite the recent success of deep reinforcement learning (RL), domain adaptation remains an open problem. Although the generalization ability of RL agents is critical for the real-world applicability of Deep RL, zero-shot policy transfer is still a challenging problem since even minor visual changes could make the trained agent completely fail in the new task. To address this issue, we propose a two-stage RL agent that first learns a latent unified state representation (LUSR) which is consistent across multiple domains in the first stage, and then do RL training in one source domain based on LUSR in the second stage. The cross-domain consistency of LUSR allows the policy acquired from the source domain to generalize to other target domains without extra training. We first demonstrate our approach in variants of CarRacing games with customized manipulations, and then verify it in CARLA, an autonomous driving simulator with more complex and realistic visual observations. Our results show that this approach can achieve state-of-the-art domain adaptation performance in related RL tasks and outperforms prior approaches based on latent-representation based RL and image-to-image translation."}}
{"id": "Gri8H_8aW7s", "cdate": 1609459200000, "mdate": 1681680961072, "content": {"title": "Domain Adaptation In Reinforcement Learning Via Latent Unified State Representation", "abstract": "Despite the recent success of deep reinforcement learning (RL), domain adaptation remains an open problem. Although the generalization ability of RL agents is critical for the real-world applicability of Deep RL, zero-shot policy transfer is still a challenging problem since even minor visual changes could make the trained agent completely fail in the new task. To address this issue, we propose a two-stage RL agent that first learns a latent unified state representation (LUSR) which is consistent across multiple domains in the first stage, and then do RL training in one source domain based on LUSR in the second stage. The cross-domain consistency of LUSR allows the policy acquired from the source domain to generalize to other target domains without extra training. We first demonstrate our approach in variants of CarRacing games with customized manipulations, and then verify it in CARLA, an autonomous driving simulator with more complex and realistic visual observations. Our results show that this approach can achieve state-of-the-art domain adaptation performance in related RL tasks and outperforms prior approaches based on latent-representation based RL and image-to-image translation."}}
{"id": "Aamn-4lzMU", "cdate": 1609459200000, "mdate": 1681680961073, "content": {"title": "Neuroevolution of a recurrent neural network for spatial and working memory in a simulated robotic environment", "abstract": "We evolved weights in a recurrent neural network (RNN) to replicate the behavior and neural activity observed in rats during a spatial and working memory task. The rat was simulated using a robot simulator to navigate a virtual maze. After evolving weights from sensory inputs to the RNN, within the RNN, and from the RNN to the robot's motors, the robot successfully navigated the space to reach four reward arms with minimal repeats before the timeout. Our current findings suggest that it is the RNN dynamics that are key to performance, and that performance is not dependent on any one sensory type, which suggests that neurons in the RNN are performing mixed selectivity and conjunctive coding. The RNN activity resembles spatial information and trajectory-dependent coding observed in the hippocampus. The evolved RNN exhibits navigation skills, spatial memory, and working memory."}}
{"id": "98czfNiO_8", "cdate": 1609459200000, "mdate": 1681680961073, "content": {"title": "Differential Spatial Representations in Hippocampal CA1 and Subiculum Emerge in Evolved Spiking Neural Networks", "abstract": "In rodent navigational studies, spatial responses have been identified in both the hippocampal subregion CA1 and the subiculum (SUB), but these two brain regions appear to encode spatial features differently. Place fields of SUB place cells are larger and less specific than CA1. Additionally, SUB neurons exhibit stronger directional modulation for heading and axes of travel. Based on neural and behavioral data recorded as rats perform a navigational task on a \u201ctriple-T\u201d maze, we present a spiking neural network modeling framework to replicate response properties observed in the CA1 and SUB. The parameters of Spike Timing Dependent Plasticity and homeostatic scaling (STDP-H) were evolved such that the response of the two different SNNs resembled recordings from CA1 and SUB when rats traversed the triple-T maze. Our results suggest that positional input may be more influential in forming CA1 place cells, while the SUB appears to integrate both allocentric positional information and self-motion cues to encode \u201ckinds of places\u201d. Furthermore, our results predict that the different spatial responses in these regions may be due in part to different STDP-H learning parameters. The framework presented here could be used as an automated parameter tuning system for replicating responses in other brain regions."}}
{"id": "64bm_YzNyF", "cdate": 1609459200000, "mdate": 1681680961124, "content": {"title": "Neuroevolution of a Recurrent Neural Network for Spatial and Working Memory in a Simulated Robotic Environment", "abstract": "Animals ranging from rats to humans can demonstrate cognitive map capabilities. We evolved weights in a biologically plausible recurrent neural network (RNN) using an evolutionary algorithm to replicate the behavior and neural activity observed in rats during a spatial and working memory task in a triple T-maze. The rat was simulated in the Webots robot simulator and used vision, distance and accelerometer sensors to navigate a virtual maze. After evolving weights from sensory inputs to the RNN, within the RNN, and from the RNN to the robot's motors, the Webots agent successfully navigated the space to reach all four reward arms with minimal repeats before time-out. Our current findings suggest that it is the RNN dynamics that are key to performance, and that performance is not dependent on any one sensory type, which suggests that neurons in the RNN are performing mixed selectivity and conjunctive coding. Moreover, the RNN activity resembles spatial information and trajectory-dependent coding observed in the hippocampus. Collectively, the evolved RNN exhibits navigation skills, spatial memory, and working memory. Our method demonstrates how the dynamic activity in evolved RNNs can capture interesting and complex cognitive behavior and may be used to create RNN controllers for robotic applications."}}
{"id": "GzPK0lvu6-", "cdate": 1577836800000, "mdate": 1631718215278, "content": {"title": "Neurorobots as a Means Toward Neuroethology and Explainable AI", "abstract": "Understanding why deep neural networks and machine learning algorithms act as they do is a difficult endeavor. Neuroscientists are faced with similar problems. One way biologists address this issue is by closely observing behavior while recording neurons or manipulating brain circuits. This has been called neuroethology. In a similar way, neurorobotics can be used to explain how neural network activity leads to behavior. In real world settings, neurorobots have been shown to perform behaviors analogous to animals. Moreover, a neuroroboticist has total control over the network, and by analyzing different neural groups or studying the effect of network perturbations (e.g., simulated lesions), they may be able to explain how the robot's behavior arises from artificial brain activity. In this paper, we review neurorobot experiments by focusing on how the robot's behavior leads to a qualitative and quantitative explanation of neural activity, and vice versa, that is, how neural activity leads to behavior. We suggest that using neurorobots as a form of computational neuroethology can be a powerful methodology for understanding neuroscience, as well as for artificial intelligence and machine learning."}}
