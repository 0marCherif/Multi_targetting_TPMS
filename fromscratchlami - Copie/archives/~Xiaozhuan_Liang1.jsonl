{"id": "cVAPebwn4b4", "cdate": 1683884012803, "mdate": 1683884012803, "content": {"title": "DeepKE: A deep learning based knowledge extraction toolkit for knowledge base population", "abstract": "We present an open-source and extensible knowledge extraction toolkit DeepKE, supporting complicated low-resource, document-level and multimodal scenarios in knowledge base population. DeepKE implements various information extraction tasks, including named entity recognition, relation extraction and attribute extraction. With a unified framework, DeepKE allows developers and researchers to customize datasets and models to extract information from unstructured data according to their requirements. Specifically, DeepKE not only provides various functional modules and model implementation for different tasks and scenarios but also organizes all components by consistent frameworks to maintain sufficient modularity and extensibility. We release the source code at GitHub with Google Colab tutorials and comprehensive documents for beginners. Besides, we present an online system for real-time extraction of various tasks, and a demo video."}}
{"id": "NRHajbzg8y0P", "cdate": 1663849861037, "mdate": null, "content": {"title": "Multimodal Analogical Reasoning over Knowledge Graphs", "abstract": "Analogical reasoning is fundamental to human cognition and holds an important place in various fields. However, previous studies mainly focus on single-modal analogical reasoning and ignore taking advantage of structure knowledge. Notably, the research in cognitive psychology has demonstrated that information from multimodal sources always brings more powerful cognitive transfer than single modality sources. To this end, we introduce the new task of multimodal analogical reasoning over knowledge graphs, which requires multimodal reasoning ability with the help of background knowledge. Specifically, we construct a Multimodal Analogical Reasoning dataSet (MARS) and a multimodal knowledge graph MarKG. We evaluate with multimodal knowledge graph embedding and pre-trained Transformer baselines, illustrating the potential challenges of the proposed task. We further propose a novel model-agnostic Multimodal analogical reasoning framework with Transformer (MarT) motivated by the structure mapping theory, which can obtain better performance. We hope our work can deliver benefits and inspire future research. Code and datasets are available in https://github.com/zjunlp/MKG_Analogy."}}
{"id": "Q8GnGqT-GTJ", "cdate": 1652737328542, "mdate": null, "content": {"title": "Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning", "abstract": "Prompt learning approaches have made waves in natural language processing by inducing better few-shot performance while they still follow a parametric-based learning paradigm; the oblivion and rote memorization problems in learning may encounter unstable generalization issues. Specifically, vanilla prompt learning may struggle to utilize atypical instances by rote during fully-supervised training or overfit shallow patterns with low-shot data. To alleviate such limitations, we develop RetroPrompt with the motivation of decoupling knowledge from memorization to help the model strike a balance between generalization and memorization. In contrast with vanilla prompt learning, RetroPrompt constructs an open-book knowledge-store from training instances and implements a retrieval mechanism during the process of input, training and inference, thus equipping the model with the ability to retrieve related contexts from the training corpus as cues for enhancement. Extensive experiments demonstrate that RetroPrompt can obtain better performance in both few-shot and zero-shot settings. Besides, we further illustrate that our proposed RetroPrompt can yield better generalization abilities with new datasets. Detailed analysis of memorization indeed reveals RetroPrompt can reduce the reliance of language models on memorization; thus, improving generalization for downstream tasks. Code is available in https://github.com/zjunlp/PromptKG/tree/main/research/RetroPrompt."}}
{"id": "izKFXZ5SJkw", "cdate": 1640995200000, "mdate": 1681650422504, "content": {"title": "Multimodal Analogical Reasoning over Knowledge Graphs", "abstract": ""}}
{"id": "fzteZutxN6Z", "cdate": 1640995200000, "mdate": 1681652151790, "content": {"title": "Relphormer: Relational Graph Transformer for Knowledge Graph Representation", "abstract": ""}}
{"id": "ZS-ucqhdOeT", "cdate": 1640995200000, "mdate": 1681652151674, "content": {"title": "OntoProtein: Protein Pretraining With Gene Ontology Embedding", "abstract": ""}}
{"id": "R2YrUhm87bH", "cdate": 1640995200000, "mdate": 1681652151741, "content": {"title": "OntoProtein: Protein Pretraining With Gene Ontology Embedding", "abstract": ""}}
{"id": "PJwvxejbvo5", "cdate": 1640995200000, "mdate": 1642813224646, "content": {"title": "DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population", "abstract": "We present an open-source and extensible knowledge extraction toolkit DeepKE, supporting complicated low-resource, document-level and multimodal scenarios in the knowledge base population. DeepKE implements various information extraction tasks, including named entity recognition, relation extraction and attribute extraction. With a unified framework, DeepKE allows developers and researchers to customize datasets and models to extract information from unstructured data according to their requirements. Specifically, DeepKE not only provides various functional modules and model implementation for different tasks and scenarios but also organizes all components by consistent frameworks to maintain sufficient modularity and extensibility. We release the source code at GitHub in https://github.com/zjunlp/DeepKE with Google Colab tutorials and comprehensive documents for beginners. Besides, we present an online system in http://deepke.openkg.cn/EN/re_doc_show.html for real-time extraction of various tasks, and a demo video."}}
{"id": "3hnuQAFeb8", "cdate": 1640995200000, "mdate": 1681652151720, "content": {"title": "Contrastive Demonstration Tuning for Pre-trained Language Models", "abstract": ""}}
{"id": "1weZCNxuHpu", "cdate": 1640995200000, "mdate": 1681650422498, "content": {"title": "Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning", "abstract": ""}}
