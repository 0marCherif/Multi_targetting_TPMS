{"id": "uGS7dlqGQU", "cdate": 1679994576488, "mdate": 1679994576488, "content": {"title": "Google's Next-Generation Real-Time Unit-Selection Synthesizer using Sequence-To-Sequence LSTM-based Autoencoders", "abstract": "A neural network model that significant improves unit-selection-based Text-To-Speech synthesis is presented. The model employs a sequence-to-sequence LSTM-based autoencoder that compresses the acoustic and linguistic features of each unit to a fixed-size vector referred to as an embedding. Unit-selection is facilitated by formulating the target cost as an L2 distance in the embedding space. In open-domain speech synthesis the method achieves a 0.2 improvement in the MOS, while for limited-domain it reaches the cap of 4.5 MOS. Furthermore, the new TTS system halves the gap between the previous unit-selection system and WaveNet in terms of quality while retaining low computational cost and latency."}}
{"id": "m23T4WKRrvl", "cdate": 1679994469347, "mdate": 1679994469347, "content": {"title": "AutoMOS - Learning a non-intrusive assessor of naturalness of speech", "abstract": "Developers of text-to-speech synthesizers (TTS) often make use of human raters to assess the quality of synthesized speech. We demonstrate that we can model human raters\u2019 mean opinion scores (MOS) of synthesized speech using a deep recurrent neural network whose inputs consist solely of a raw waveform. Our best models provide utterance-level estimates of MOS only moderately inferior to sampled human ratings, as shown by Pearson and Spearman correlations. When multiple utterances are scored and averaged, a scenario common in synthesizer quality assessment, AutoMOS achieves correlations approaching those of human raters. The AutoMOS model has a number of applications, such as the ability to explore the parameter space of a speech synthesizer without requiring a human-in-the-loop."}}
{"id": "IFThsw6enUB", "cdate": 1679994074697, "mdate": null, "content": {"title": "Vocaine the vocoder and applications in speech synthesis", "abstract": "Vocoders received renewed attention recently as basic components in speech synthesis applications such as voice transformation, voice conversion and statistical parametric speech synthesis. This paper presents a new vocoder synthesizer, referred to as Vocaine, that features a novel Amplitude Modulated-Frequency Modulated (AM-FM) speech model, a new way to synthesize non-stationary sinusoids using quadratic phase splines and a super fast cosine generator. Extensive evaluations are made against several state-of-the-art methods in Copy-Synthesis and Text-To-Speech synthesis experiments. Vocaine matches or outperforms STRAIGHT in Copy-Synthesis experiments and outperforms our baseline real-time optimized Mixed-Excitation vocoder with the same computational cost. We report that Vocaine considerably improves our statistical TTS synthesizers and that our new statistical parametric synthesizer [1] matched the quality of our mature production Unit-Selection system with uncompressed waveforms."}}
{"id": "s7T2rBRumGi", "cdate": 1679993994537, "mdate": null, "content": {"title": "Wrapped Gaussian Mixture Models for Modeling and High-Rate Quantization of Phase Data of Speech", "abstract": "The harmonic representation of speech signals has found many applications in speech processing. This paper presents a novel statistical approach to model the behavior of harmonic phases. Phase information is decomposed into three parts: a minimum phase part, a translation term, and a residual term referred to as dispersion phase. Dispersion phases are modeled by wrapped Gaussian mixture models (WGMMs) using an expectation-maximization algorithm suitable for circular vector data. A multivariate WGMM-based phase quantizer is then proposed and constructed using novel scalar quantizers for circular random variables. The proposed phase modeling and quantization scheme is evaluated in the context of a narrowband harmonic representation of speech. Results indicate that it is possible to construct a variable-rate harmonic codec that is equivalent to iLBC at approximately 13 kbps."}}
{"id": "EGr8lL2PP9", "cdate": 1679993935390, "mdate": 1679993935390, "content": {"title": "Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric Speech Synthesizers for Mobile Devices", "abstract": "Acoustic models based on long short-term memory recurrent neural networks (LSTM-RNNs) were applied to statistical parametric speech synthesis (SPSS) and showed significant improvements in naturalness and latency over those based on hidden Markov models (HMMs). This paper describes further optimizations of LSTM-RNN-based SPSS for deployment on mobile devices; weight quantization, multi-frame inference, and robust inference using an {\\epsilon}-contaminated Gaussian loss function. Experimental results in subjective listening tests show that these optimizations can make LSTM-RNN-based SPSS comparable to HMM-based SPSS in runtime speed while maintaining naturalness. Evaluations between LSTM-RNN- based SPSS and HMM-driven unit selection speech synthesis are also presented."}}
{"id": "d5hbhPu3tW", "cdate": 1679993709077, "mdate": 1679993709077, "content": {"title": "NATURAL TTS SYNTHESIS BY CONDITIONING WAVENET ON MEL SPECTROGRAM PREDICTIONS", "abstract": "This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean \nopinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present \nablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet \ninstead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a\nsignificant reduction in the size of the WaveNet architecture."}}
{"id": "9YlbjkbCZdx", "cdate": 1679993551407, "mdate": 1679993551407, "content": {"title": "TACOTRON: TOWARDS END-TO-END SPEECH SYNTHESIS", "abstract": "A text-to-speech synthesis system typically consists of multiple stages, such as a\ntext analysis frontend, an acoustic model and an audio synthesis module. Build-\ning these components often requires extensive domain expertise and may contain\nbrittle design choices. In this paper, we present Tacotron, an end-to-end genera-\ntive text-to-speech model that synthesizes speech directly from characters. Given\n<text, audio> pairs, the model can be trained completely from scratch with ran-\ndom initialization. We present several key techniques to make the sequence-to-\nsequence framework perform well for this challenging task. Tacotron achieves a\n3.82 subjective 5-scale mean opinion score on US English, outperforming a pro-\nduction parametric system in terms of naturalness. In addition, since Tacotron\ngenerates speech at the frame level, it\u2019s substantially faster than sample-level au-\ntoregressive methods"}}
