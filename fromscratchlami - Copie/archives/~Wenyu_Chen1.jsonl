{"id": "gCVboEOfOOJ", "cdate": 1681429565204, "mdate": 1681429565204, "content": {"title": "Multivariate Convex Regression at Scale", "abstract": "We present new large-scale algorithms for fitting a subgradient regularized multivariate convex regression function to $n$ samples in $d$ dimensions---a key problem in shape constrained nonparametric regression with widespread applications in statistics, engineering and the applied sciences. \nThe infinite-dimensional learning task can be expressed via a convex quadratic program (QP) with $O(nd)$ decision variables and $O(n^2)$ constraints. While instances with $n$ in the lower thousands can be addressed with current algorithms within reasonable runtimes, solving larger problems (e.g., $n\\approx 10^4$ or $10^5$) is computationally challenging. \nTo this end, we present an active set type algorithm on the dual QP. For computational scalability, we allow for approximate optimization of the reduced sub-problems; and propose randomized augmentation rules for expanding the active set.\n{{We derive novel computational guarantees for our algorithms.}}\nWe demonstrate that our framework can approximately solve instances of the subgradient regularized convex regression problem with $n=10^5$ and $d=10$ within minutes; and shows strong computational performance compared to earlier approaches.  "}}
{"id": "MYJm9rnaQj", "cdate": 1672531200000, "mdate": 1681653621400, "content": {"title": "Fast as CHITA: Neural Network Pruning with Combinatorial Optimization", "abstract": ""}}
{"id": "e9EARLwp2Yw", "cdate": 1664731450867, "mdate": null, "content": {"title": "Network Pruning at Scale: A Discrete Optimization Approach", "abstract": "Due to the ever-growing size of neural network models, there has been an emerging interest in compressing (i.e.,  pruning) neural networks by sparsifying weights in a pre-trained neural network, while maintaining the performance of dense model as much as possible. In this work, we focus on a neural network pruning framework based on local quadratic models of the loss function. We present an optimization-based approach with an $\\ell_0$-regression formulation, and propose novel algorithms to obtain good solutions to the combinatorial optimization problem. In practice, our basic (single-stage) approach, based on one local quadratic model approximation, is up to $10^3$ times faster than existing methods while achieving similar accuracy. We also propose a multi-stage method that outperforms other methods in terms of accuracy for a given sparsity constraint while remaining computationally efficient. In particular, our approach results in a 98\\% sparse (i.e., 98\\% of weights in dense model are set to zero) MLPNet with 90\\% test accuracy (i.e., 3\\% reduction in test accuracy compared to the dense model), which is an improvement over the previous best accuracy (55\\%). "}}
{"id": "KSRIjqx9Qiy", "cdate": 1640995200000, "mdate": 1681490806341, "content": {"title": "Knowledge Graph Guided Simultaneous Forecasting and Network Learning for Multivariate Financial Time Series", "abstract": ""}}
{"id": "xoMoCrt8tb0", "cdate": 1609459200000, "mdate": 1681653621497, "content": {"title": "Probabilistic framework for modeling event shocks to financial time series", "abstract": ""}}
{"id": "ByW5w3WO-S", "cdate": 1546300800000, "mdate": null, "content": {"title": "Unifying Orthogonal Monte Carlo Methods", "abstract": "Many machine learning methods making use of Monte Carlo sampling in vector spaces have been shown to be improved by conditioning samples to be mutually orthogonal. Exact orthogonal coupling of samp..."}}
