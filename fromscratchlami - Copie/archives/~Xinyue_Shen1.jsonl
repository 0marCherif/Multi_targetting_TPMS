{"id": "p1cA1jK_dpP", "cdate": 1620365698271, "mdate": null, "content": {"title": "Confidence bands for a log-concave density", "abstract": "We present a new approach for inference about a log-concave distribution: Instead of using the method of maximum likelihood, we propose to incorporate the log-concavity constraint in an appropriate nonparametric confidence set for the cdf F. This approach has the advantage that it automatically provides a measure of statistical uncertainty and it thus overcomes a marked limitation of the maximum likelihood estimate. In particular, we show how to construct confidence bands for the density that have a finite sample guaranteed confidence level. The nonparametric confidence set for F which we introduce here has attractive computational and statistical properties: It allows to bring modern tools from optimization to bear on this problem via difference of convex programming, and it results in optimal statistical inference. We show that the width of the resulting confidence bands converges at nearly the parametric n\u221212 rate when the log density is k-affine."}}
{"id": "ERzxEi7pCL", "cdate": 1620365604602, "mdate": null, "content": {"title": "Minimizing Oracle-Structured Composite Functions", "abstract": "We consider the problem of minimizing a composite convex function with two different access methods: an oracle, for which we can evaluate the value and gradient, and a structured function, which we access only by solving a convex optimization problem. We are motivated by two associated technological developments. For the oracle, systems like PyTorch or TensorFlow can automatically and efficiently compute gradients, given a computation graph description. For the structured function, systems like CVXPY accept a high level domain specific language description of the problem, and automatically translate it to a standard form for efficient solution. We develop a method that makes minimal assumptions about the two functions, does not require the tuning of algorithm parameters, and works well in practice across a variety of problems. Our algorithm combines a number of well-known ideas, including a low-rank quasi-Newton approximation of curvature, piecewise affine lower bounds from bundle-type methods, and two types of damping to ensure stability. We illustrate the method on stochastic optimization, utility maximization, and risk-averse programming problems."}}
{"id": "Vn65gz_Dx1", "cdate": 1546300800000, "mdate": null, "content": {"title": "Linearized ADMM for Nonconvex Nonsmooth Optimization With Convergence Analysis", "abstract": "Linearized alternating direction method of multipliers (ADMM) as an extension of ADMM has been widely used to solve linearly constrained problems in signal processing, machine learning, communications, and many other fields. Despite its broad applications in nonconvex optimization, for a great number of nonconvex and nonsmooth objective functions, its theoretical convergence guarantee is still an open problem. In this paper, we propose a two-block linearized ADMM and a multi-block parallel linearized ADMM for problems with nonconvex and nonsmooth objectives. Mathematically, we present that the algorithms can converge for a broader class of objective functions under less strict assumptions compared with previous works. Furthermore, our proposed algorithm can update coupled variables in parallel and work for less restrictive nonconvex problems, where the traditional ADMM may have difficulties in solving subproblems."}}
{"id": "HhiWx2ngSM9", "cdate": 1546300800000, "mdate": null, "content": {"title": "A Distributed Method for Optimal Capacity Reservation", "abstract": "We consider the problem of reserving link capacity in a network in such a way that any of a given set of flow scenarios can be supported. In the optimal capacity reservation problem, we choose the reserved link capacities to minimize the reservation cost. This problem reduces to a large linear program, with the number of variables and constraints on the order of the number of links times the number of scenarios. We develop a scalable, distributed algorithm for the problem that alternates between solving (in parallel) one-flow problem per scenario, and coordination steps, which connect the individual flows and the reservation capacities."}}
{"id": "61MMHIqd1wT", "cdate": 1546300800000, "mdate": null, "content": {"title": "Weakly Convex Regularized Robust Sparse Recovery Methods With Theoretical Guarantees", "abstract": "Robust sparse signal recovery against impulsive noise is a core issue in many applications. Numerous methods have been proposed to recover the sparse signal from measurements corrupted by various impulsive noises, but most of them either lack theoretical guarantee for robust sparse recovery or are not efficient enough for large-scale problems. To this end, a general optimization problem for robust sparse signal recovery, which includes many existing works as concrete instances, is analyzed by a freshly defined Double Null Space Property (DNSP), and its solution is proved to be able to robustly reconstruct the sparse signal under mild conditions. Moreover, for computational tractability, weakly convex sparsity-inducing penalties are applied to the general problem, and properties of the solution to the resultant non-convex problem are further studied. Based on these properties, an algorithm named Robust Projected Generalized Gradient (RPGG) is devised to solve the weakly convex problem. Theoretical results prove that the sparse signal can be precisely reconstructed by RPGG from compressive measurements with sparse noise or robustly recovered from those with impulsive noise. Meanwhile, simulations demonstrate that RPGG with tuned parameters outperforms other robust sparse recovery algorithms."}}
{"id": "yKnOlcerkPY", "cdate": 1514764800000, "mdate": null, "content": {"title": "Nonconvex Sparse Logistic Regression via Proximal Gradient Descent", "abstract": "In this work we propose to fit a sparse logistic regression model by a weakly convex regularized nonconvex optimization problem. The idea is based on the finding that a weakly convex function as an approximation of the \u2113 <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">0</sub> pseudo norm is able to better induce sparsity than the commonly used \u2113 <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sub> norm. For a class of weakly convex sparsity inducing functions, despite the nonconvexity, the algorithm proposed to solve the problem is based on proximal gradient descent, which allows the use of convergence acceleration techniques and stochastic gradient. Then the general framework is applied to a specific weakly convex function, and the solution method is instantiated as an iterative firm-shrinkage algorithm, of which the effectiveness is demonstrated in numerical experiments."}}
{"id": "GBKeTskf9IX", "cdate": 1514764800000, "mdate": null, "content": {"title": "Sparse Recovery Conditions and Performance Bounds for \u2113p-Minimization", "abstract": "In sparse recovery, a sparse signal <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">${\\mathbf x}\\in \\mathbb {R}^N$</tex-math></inline-formula> with <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$K$</tex-math></inline-formula> nonzero entries is to be reconstructed from a compressed measurement <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\mathbf y=Ax$</tex-math></inline-formula> with <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">${\\mathbf A}\\in \\mathbb {R}^{M\\times N}$</tex-math></inline-formula> ( <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$M&lt; N$</tex-math></inline-formula> ). The <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\ell _p$</tex-math></inline-formula> <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$(0\\leq p &lt; 1)$</tex-math></inline-formula> pseudonorm has been found to be a sparsity inducing function superior to the <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\ell _1$</tex-math></inline-formula> norm, and the null space constant (NSC) and restricted isometry constant (RIC) have been used as key notions in the performance analyses of the corresponding <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\ell _p$</tex-math></inline-formula> -minimization. In this paper, we study sparse recovery conditions and performance bounds for the <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\ell _p$</tex-math></inline-formula> -minimization. We devise a new NSC upper bound that outperforms the state-of-the-art result. Based on the improved NSC upper bound, we provide a new RIC upper bound dependent on the sparsity level <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$K$</tex-math></inline-formula> as a sufficient condition for precise recovery, and it is tighter than the existing bound for small <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$K$</tex-math></inline-formula> . Then, we study the largest choice of <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$p$</tex-math></inline-formula> for the <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\ell _p$</tex-math></inline-formula> -minimization problem to recover any <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$K$</tex-math></inline-formula> -sparse signal, and the largest recoverable <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$K$</tex-math></inline-formula> for a fixed <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$p$</tex-math></inline-formula> . Numerical experiments demonstrate the improvement of the proposed bounds in the recovery conditions over the up-to-date counterparts."}}
{"id": "FkTxbjTPlm", "cdate": 1514764800000, "mdate": null, "content": {"title": "Subspace Data Visualization with Dissimilarity Based on Principal Angle", "abstract": "In this work, we aim to visualize data points distributing on a union of subspaces in a two or three dimensional plot to demonstrate both the pairwise angles in the dataset and the relation among the latent subspaces. Our main contribution is a definition of dissimilarity among data, which is based on both the angles between every pair of data points and the principal angles between every data point and each of these intrinsic subspaces. The defined dissimilarity depicts both the inter-class and the intra-class angular structures, and is proven to be a semi-metric with a relaxed triangle inequality. The intrinsic subspaces are obtained in the preprocessing step by subspace clustering, and the defined dissimilarity matrix can be directly visualized by multidimensional scaling. The effectiveness is verified in numerical experiments on both synthetic data and real-world datasets of facial images and human motions, which are visualized into several subspace clusters, and both the sequence order within every cluster and the relation among different clusters are well illustrated."}}
{"id": "CO7s_w5TL46", "cdate": 1514764800000, "mdate": null, "content": {"title": "Subspace Principal Angle Preserving Property of Gaussian Random Projection", "abstract": "With the increase of the dimension of data, dimensionality reduction, such as random projection, becomes necessary. In many cases, the underlying structure of data is a union of subspaces, and the relative position between subspaces can be well described by principal angles. Motivated by the conjecture whether the principal angles between subspaces can remain almost unchanged after Gaussian random projection, in this work, we prove that the principal angle preserving property holds with probability exponentially close to 1. We use more advanced techniques compared with our previous work, and the improved conclusion is more rigorous in theory and more useful in practice. Principal angles are so essential in the relation between subspaces that many works on algorithms for data mining regarding subspaces are based on principal angles, so this work may contribute to extending them to compressive scenarios, in which the computational complexity can be fundamentally reduced due to the random dimensionality reduction. Experiments on real datasets verify that with a compression ratio as small as 0.05 the principal angles between subspaces after random projection can be sufficiently close to the original principal angles."}}
{"id": "QgOM8km_9Ca", "cdate": 1483228800000, "mdate": null, "content": {"title": "A Distributed Method for Optimal Capacity Reservation", "abstract": "We consider the problem of reserving link capacity in a network in such a way that any of a given set of flow scenarios can be supported. In the optimal capacity reservation problem, we choose the reserved link capacities to minimize the reservation cost. This problem reduces to a large linear program, with the number of variables and constraints on the order of the number of links times the number of scenarios. Small and medium size problems are within the capabilities of generic linear program solvers. We develop a more scalable, distributed algorithm for the problem that alternates between solving (in parallel) one flow problem per scenario, and coordination steps, which connect the individual flows and the reservation capacities."}}
