{"id": "OaZktJBVpUy", "cdate": 1677713797301, "mdate": null, "content": {"title": "MULTI-AGENT REINFORCEMENT LEARNING FOR COALITIONAL BARGAINING GAMES", "abstract": "In recent years, there has been growing attention to the application of MARL to coalition formation problems, in particular, on coalitional bargaining games as a means of negotiation. However, the lack of theoretical principles for using MARL in coalitional bargaining games remain less explored. This paper aims to address this gap by providing an examination of the theoretical link between coalition formation, coalitional bargaining games, and MARL through the link of stochastic games. Through this analysis, the paper seeks to shed light on the underlying principles that support the use of MARL in coalitional bargaining and to explore the contributions of this approach and its limitations in comparison to traditional game theoretical methods."}}
{"id": "uWb99HlLqmU", "cdate": 1672531200000, "mdate": 1683831232318, "content": {"title": "Reward Design with Language Models", "abstract": "Reward design in reinforcement learning (RL) is challenging since specifying human notions of desired behavior may be difficult via reward functions or require many expert demonstrations. Can we instead cheaply design rewards using a natural language interface? This paper explores how to simplify reward design by prompting a large language model (LLM) such as GPT-3 as a proxy reward function, where the user provides a textual prompt containing a few examples (few-shot) or a description (zero-shot) of the desired behavior. Our approach leverages this proxy reward function in an RL framework. Specifically, users specify a prompt once at the beginning of training. During training, the LLM evaluates an RL agent's behavior against the desired behavior described by the prompt and outputs a corresponding reward signal. The RL agent then uses this reward to update its behavior. We evaluate whether our approach can train agents aligned with user objectives in the Ultimatum Game, matrix games, and the DealOrNoDeal negotiation task. In all three tasks, we show that RL agents trained with our framework are well-aligned with the user's objectives and outperform RL agents trained with reward functions learned via supervised learning"}}
{"id": "10uNUgI5Kl", "cdate": 1663850413372, "mdate": null, "content": {"title": "Reward Design with Language Models", "abstract": "Reward design in reinforcement learning (RL) is challenging since specifying human notions of desired behavior may be difficult via reward functions or require many expert demonstrations. Can we instead cheaply design rewards using a natural language interface? This paper explores how to simplify reward design by using a large language model (LLM) such as GPT-3 as a proxy reward function, where the user provides a textual prompt containing a few examples (few-shot) or a description (zero-shot) of desired behavior. Our approach leverages this proxy reward function in an RL framework. Specifically, users specify a prompt once at the beginning of training. During training, the LLM evaluates an RL agent's behavior against the desired behavior described by the prompt and outputs a corresponding reward signal. The RL agent then uses this reward to update its behavior. We evaluate whether our approach can train agents aligned with user objectives in the Ultimatum Game, matrix games, and the DealOrNoDeal negotiation task. In all three tasks, we show that RL agents trained with our framework are well-aligned with the user's objectives and outperforms RL agents trained with reward functions learned via supervised learning. "}}
{"id": "btbL0aFqdV", "cdate": 1640995200000, "mdate": 1672765495286, "content": {"title": "Developing, evaluating and scaling learning agents in multi-agent environments", "abstract": ""}}
{"id": "Z0pun0GP5Vy", "cdate": 1609459200000, "mdate": 1681512762709, "content": {"title": "Quasi-Equivalence Discovery for Zero-Shot Emergent Communication", "abstract": ""}}
{"id": "Fblk4_Fd7ao", "cdate": 1601308218347, "mdate": null, "content": {"title": "Exploring Zero-Shot Emergent Communication in Embodied Multi-Agent Populations", "abstract": "Effective communication is an important skill for enabling information exchange and cooperation in multi-agent settings. Indeed, emergent communication is now a vibrant field of research, with common settings involving discrete cheap-talk channels. One limitation of this setting is that it does not allow for the emergent protocols to generalize beyond the training partners. \n Furthermore, so far emergent communication has primarily focused on the use of symbolic channels. In this work, we extend this line of work to a new modality, by studying agents that learn to communicate via actuating their joints in a 3D environment. We show that under realistic assumptions, a non-uniform distribution of intents and a common-knowledge energy cost, these agents can find protocols that generalize to novel partners. We also explore and analyze specific difficulties associated with finding these solutions in practice. Finally, we propose and evaluate initial training improvements to address these challenges, involving both specific training curricula and providing the latent feature that can be coordinated on during training."}}
{"id": "mWkzLMmIN2", "cdate": 1577836800000, "mdate": 1683831232364, "content": {"title": "Managing Learning Interactions for Collaborative Robot Learning", "abstract": "Robotic assistants should be able to actively engage their human partner(s) to generalize knowledge about relevant tasks within their shared environment. Yet a key challenge is not all human partners will be proficient at teaching; furthermore, humans should not be held accountable for tracking a robot\u2019s knowledge over time in a dynamically changing environment, across multiple tasks. Thus, it is important to enable these interactive robots to characterize their own uncertainty and equip them with an information gathering policy for asking the appropriate questions of their human partners to resolve that uncertainty. In this way, the robot shares the responsibility in guiding its own learning process and is a collaborator in the learning. Additionally, given the robot requires some tutelage from its partner, awareness of constraints on the teacher\u2019s time and cognitive resources available for devoting to the interaction could help the agent to use the time allotted more wisely. This thesis examines the problem of enabling a robotic agent to leverage structured interaction with a human partner for acquiring concepts relevant to a task it must later perform. To equip the agent with the desired concept knowledge, we first explore the paradigm of Learning from Demonstration for the acquisition of (1) training instances as examples of task-relevant concepts and (2) informative features for appropriately representing and discriminating between task-relevant concepts. Given empirical evidence that a human partner can be helpful to the agent in solving the concept learning problem, we subsequently investigate the design of algorithms that enable the robot learner to autonomously manage interaction with its human partner, using a questioning policy to actively gather both instance and feature information. This thesis seeks to investigate the following hypothesis: In the context of robot learning from human demonstrations in changeable and resource-constrained environments, enabling the robot to actively elicit multiple types of information through questions, and to reason about what question to ask and when, leads to improved learning performance."}}
{"id": "lOemT6PqLB", "cdate": 1577836800000, "mdate": 1681512762759, "content": {"title": "Exploring Zero-Shot Emergent Communication in Embodied Multi-Agent Populations", "abstract": ""}}
{"id": "GwFjmGRKOX1", "cdate": 1546300800000, "mdate": 1624220997396, "content": {"title": "Active Learning within Constrained Environments through Imitation of an Expert Questioner", "abstract": "Active learning agents typically employ a query selection algorithm which solely considers the agent's learning objectives. However, this may be insufficient in more realistic human domains.\u00a0 This work uses imitation learning to enable an agent in a constrained environment to concurrently reason about both its internal learning goals and environmental constraints externally imposed, all within its objective function. Experiments are conducted on a concept learning task to test generalization of the proposed algorithm to different environmental conditions and analyze how time and resource constraints impact efficacy of solving the learning problem. Our findings show the environmentally-aware learning agent is able to statistically outperform all other active learners explored under most of the constrained conditions. A key implication is adaptation for active learning agents to more realistic human environments, where constraints are often externally imposed on the learner."}}
{"id": "fJ0SLc6Z5AN", "cdate": 1514764800000, "mdate": 1624220930928, "content": {"title": "Human-Driven Feature Selection for a Robotic Agent Learning Classification Tasks from Demonstration", "abstract": "The state features available to a robot define the variables on which the learning computation depends. However, little prior work considers feature selection in the context of deploying a general-purpose robot able to learn new tasks. In this work, we explore human-driven feature selection in which a robotic agent can identify useful features with the aid of a human user, by extracting information from users about which features are most informative for discriminating between classes of objects needed for a given task (e.g. sorting groceries). The research questions examine (a) whether a domain expert is able to identify a subset of informative task features, (b) whether human selected features will enable the agent to classify unseen examples as accurately as using computational feature selection, and (c) if the interaction strategy used to elicit the information from the user impacts the quality of resulting feature selection. Toward that end, we conducted a user study with 30 participants on campus, given a multi-class classification task and one of five different approaches for conveying information about informative features to a robot learner. Our findings show that when features are semantically interpretable, human feature selection is effective in LfD scenarios because it is able to outperform computational methods when there is limited training data, yet still remains on-par with computational methods as the training sample size increases."}}
