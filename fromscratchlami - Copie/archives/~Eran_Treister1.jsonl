{"id": "zoQZxGHO6WF", "cdate": 1665013556319, "mdate": null, "content": {"title": "PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations", "abstract": "Graph neural networks are have shown their efficacy in fields such as computer vision, computational biology and chemistry, where data are naturally explained by graphs. However, unlike convolutional neural networks, deep graph networks do not necessarily yield better performance than shallow networks. \nThis behaviour usually stems from the over-smoothing phenomenon. In this work, we propose a family of architectures\nto control this behaviour by design. Our networks are motivated by numerical methods for solving Partial Differential Equations (PDEs) on manifolds, and as such, their behaviour can be explained by similar analysis."}}
{"id": "fwn2Mqpy4pS", "cdate": 1663850277302, "mdate": null, "content": {"title": "$\\omega$GNNs: Deep Graph Neural Networks Enhanced by Multiple Propagation Operators", "abstract": "Graph Neural Networks (GNNs) are limited in their propagation operators. These operators often contain non-negative elements only and are shared across channels and layers, limiting the expressiveness of GNNs. Moreover, some GNNs suffer from over-smoothing, limiting their depth. On the other hand, Convolutional Neural Networks (CNNs) can learn diverse propagation filters, and phenomena like over-smoothing are typically not apparent in CNNs.\nIn this paper, we bridge this gap by incorporating trainable channel-wise weighting factors $\\omega$ to learn and mix multiple smoothing and sharpening propagation operators at each layer. Our generic method is called $\\omega$GNN, and we study two variants: $\\omega$GCN and $\\omega$GAT.\nFor $\\omega$GCN, we theoretically analyse its behaviour and the impact of $\\omega$ on the obtained node features. Our experiments confirm these findings, demonstrating and explaining how both variants do not over-smooth.\nAdditionally, we experiment with 15 real-world datasets on node- and graph-classification tasks, where our $\\omega$GCN and $\\omega$GAT perform better or on par with state-of-the-art methods. "}}
{"id": "9gfir3fSy3J", "cdate": 1663849959556, "mdate": null, "content": {"title": "NeRN: Learning Neural Representations for Neural Networks", "abstract": "Neural Representations have recently been shown to effectively reconstruct a wide range of signals from 3D meshes and shapes to images and videos. We show that, when adapted correctly, neural representations can be used to directly represent the weights of a pre-trained convolutional neural network, resulting in a Neural Representation for Neural Networks (NeRN). Inspired by coordinate inputs of previous neural representation methods, we assign a coordinate to each convolutional kernel in our network based on its position in the architecture, and optimize a predictor network to map coordinates to their corresponding weights. Similarly to the spatial smoothness of visual scenes, we show that incorporating a smoothness constraint over the original network's weights aids NeRN towards a better reconstruction. In addition, since slight perturbations in pre-trained model weights can result in a considerable accuracy loss, we employ techniques from the field of knowledge distillation to stabilize the learning process. We demonstrate the effectiveness of NeRN in reconstructing widely used architectures on CIFAR-10, CIFAR-100, and ImageNet. Finally, we present two applications using NeRN, demonstrating the capabilities of the learned representations."}}
{"id": "YCgwkDo56q", "cdate": 1662812639850, "mdate": null, "content": {"title": "Global-Local Graph Neural Networks for Node-Classification", "abstract": "The task of graph node-classification is often approached using a \\emph{local} Graph Neural Network (GNN), that learns only local information from the node input features and their adjacency. In this paper we propose to benefit from global and local information through the form of learning \\emph{label}- and \\emph{node}- features to improve node-classification accuracy. We therefore call our method Global-Local-GNN (GLGNN).\nTo learn proper label features, for each label, we maximize the similarity between its features and nodes features that belong to the label, while maximizing the distance between nodes that do not belong to the considered label. We then use the learnt label features to predict the node-classification map. We demonstrate our GLGNN using GCN and GAT as GNN backbones, and show that our GLGNN approach improves baseline performance on the node-classification task."}}
{"id": "5g7l7EJoZT", "cdate": 1652737580839, "mdate": null, "content": {"title": "Wavelet Feature Maps Compression for Image-to-Image CNNs", "abstract": "Convolutional Neural Networks (CNNs) are known for requiring extensive computational resources, and quantization is among the best and most common methods for compressing them. While aggressive quantization (i.e., less than 4-bits) performs well for classification, it may cause severe performance degradation in image-to-image tasks such as semantic segmentation and depth estimation. In this paper, we propose Wavelet Compressed Convolution (WCC)---a novel approach for high-resolution activation maps compression integrated with point-wise convolutions, which are the main computational cost of modern architectures. To this end, we use an efficient and hardware-friendly Haar-wavelet transform, known for its effectiveness in image compression, and define the convolution on the compressed activation map. We experiment with various tasks that benefit from high-resolution input. By combining WCC with light quantization, we achieve compression rates equivalent to 1-4bit activation quantization with relatively small and much more graceful degradation in performance. Our code is available at https://github.com/BGUCompSci/WaveletCompressedConvolution."}}
{"id": "R3Y9yq49seb", "cdate": 1632875629028, "mdate": null, "content": {"title": "Wavelet Feature Maps Compression for Low Bandwidth Convolutional Neural Networks", "abstract": "Quantization is one of the most effective techniques for compressing Convolutional Neural Networks (CNNs), which are known for requiring extensive computational resources. However, aggressive quantization may cause severe degradation in the prediction accuracy of such networks, especially in image-to-image tasks such as semantic segmentation and depth prediction. In this paper, we propose Wavelet Compressed Convolution (WCC)---a novel approach for activation maps compression for $1\\times1$ convolutions (the workhorse of modern CNNs). WCC achieves compression ratios and computational savings that are equivalent to low bit quantization rates at a relatively minimal loss of accuracy. To this end, we use a hardware-friendly Haar-wavelet transform, known for its effectiveness in image compression, and define the convolution on the compressed activation map. WCC can be utilized with any $1\\times1$ convolution in an existing network architecture. By combining WCC with light quantization, we show that we achieve compression rates equal to 2-bit and 1-bit with minimal degradation in image-to-image tasks."}}
{"id": "qxmVF_PK5o", "cdate": 1632781958868, "mdate": null, "content": {"title": "Multigrid-augmented deep learning preconditioners for the Helmholtz equation", "abstract": "We present a data-driven approach to iteratively solve the discrete heterogeneous Helmholtz equation at high wavenumbers. We combine multigrid ingredients with convolutional neural networks (CNNs) to form a preconditioner which is applied within a Krylov solver. Two types of preconditioners are proposed 1) U-Net as a coarse grid solver, and 2) U-Net as a deflation operator with shifted Laplacian V-cycles. The resulting CNN preconditioner can generalize over residuals and a relatively general set of wave slowness models. On top of that, we offer an encoder-solver framework where an ``encoder'' network generalizes over the medium and sends context vectors to another ``solver'' network, which generalizes over the right-hand-sides. We show that this option is more efficient than the stand-alone variant. Lastly, we suggest a mini-retraining procedure, to improve the solver after the model is known. We demonstrate the efficiency and generalization abilities of our approach on a variety of 2D problems."}}
{"id": "Qb3Jpm0jAt4", "cdate": 1632781958330, "mdate": null, "content": {"title": "MGIC: Multigrid-in-Channels Neural Network Architectures", "abstract": "Multigrid (MG) methods are effective at solving numerical PDEs in linear complexity. In this work we present a multigrid-in-channels (MGIC) approach that tackles the quadratic growth of the number of parameters with respect to the number of channels in standard convolutional neural networks (CNNs). Indeed, lightweight CNNs can achieve comparable accuracy to standard CNNs with fewer parameters; however, the number of weights still scales quadratically with the CNN's width. Our MGIC architectures replace each CNN block with an MGIC counterpart that utilizes a hierarchy of nested grouped convolutions of small group size to address this. \nHence, our proposed architectures scale linearly with respect to the network's width while retaining full coupling of the channels as in standard CNNs.\nOur extensive experiments on image classification, segmentation, and point cloud classification show that applying this strategy to different architectures reduces the number of parameters while obtaining similar or better accuracy."}}
{"id": "_dIqV256AT5", "cdate": 1632781957971, "mdate": null, "content": {"title": "Quantized convolutional neural networks through the lens of partial differential equations", "abstract": "Quantization of Convolutional Neural Networks (CNNs) is a common approach to ease the computational burden involved in the deployment of CNNs. However, fixed-point arithmetic is not natural to the type of computations involved in neural networks.  \nIn our work, we consider symmetric and stable variants of common CNNs for image classification, and Graph Convolutional Networks (GCNs) for graph node-classification. We demonstrate through several experiments that the property of forward stability preserves the action of a network under different quantization rates, allowing stable quantized networks to behave similarly to their non-quantized counterparts while using fewer parameters. We also find that at times, stability aids in improving accuracy. These properties are of particular interest for sensitive, resource-constrained or real-time applications. "}}
{"id": "wWtk6GxJB2x", "cdate": 1621630055302, "mdate": null, "content": {"title": "PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations", "abstract": "Graph neural networks are increasingly becoming the go-to approach in various fields such as computer vision, computational biology and chemistry, where data are naturally explained by graphs. However, unlike traditional convolutional neural networks, deep graph networks do not necessarily yield better performance than shallow graph networks. \nThis behavior usually stems from the over-smoothing phenomenon. In this work, we propose a family of architectures\nto control this behavior by design. Our networks are motivated by numerical methods for solving Partial Differential Equations (PDEs) on manifolds, and as such, their behavior can be explained by similar analysis. Moreover, as we demonstrate using an extensive set of experiments, our PDE-motivated networks can generalize and be effective for various types of problems from different fields. Our architectures obtain better or on par with the current state-of-the-art results for problems that are typically approached using different architectures."}}
