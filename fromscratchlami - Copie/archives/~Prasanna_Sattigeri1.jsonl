{"id": "yMWT-D-eLo", "cdate": 1672531200000, "mdate": 1680112441562, "content": {"title": "Group Fairness with Uncertainty in Sensitive Attributes", "abstract": ""}}
{"id": "u_VsuGRrOH", "cdate": 1672531200000, "mdate": 1681791128888, "content": {"title": "Who Should Predict? Exact Algorithms For Learning to Defer to Humans", "abstract": "Automated AI classifiers should be able to defer the prediction to a human decision maker to ensure more accurate predictions. In this work, we jointly train a classifier with a rejector, which decides on each data point whether the classifier or the human should predict. We show that prior approaches can fail to find a human-AI system with low misclassification error even when there exists a linear classifier and rejector that have zero error (the realizable setting). We prove that obtaining a linear pair with low error is NP-hard even when the problem is realizable. To complement this negative result, we give a mixed-integer-linear-programming (MILP) formulation that can optimally solve the problem in the linear setting. However, the MILP only scales to moderately-sized problems. Therefore, we provide a novel surrogate loss function that is realizable-consistent and performs well empirically. We test our approaches on a comprehensive set of datasets and compare to a wide range of baselines."}}
{"id": "nkTZesYyvih", "cdate": 1672531200000, "mdate": 1683888611940, "content": {"title": "Reliable Gradient-free and Likelihood-free Prompt Tuning", "abstract": ""}}
{"id": "d_tIyPz8KiF", "cdate": 1672531200000, "mdate": 1683888614110, "content": {"title": "Reliable Gradient-free and Likelihood-free Prompt Tuning", "abstract": "Due to privacy or commercial constraints, large pre-trained language models (PLMs) are often offered as black-box APIs. Fine-tuning such models to downstream tasks is challenging because one can neither access the model's internal representations nor propagate gradients through it. This paper addresses these challenges by developing techniques for adapting PLMs with only API access. Building on recent work on soft prompt tuning, we develop methods to tune the soft prompts without requiring gradient computation. Further, we develop extensions that in addition to not requiring gradients also do not need to access any internal representation of the PLM beyond the input embeddings. Moreover, instead of learning a single prompt, our methods learn a distribution over prompts allowing us to quantify predictive uncertainty. Ours is the first work to consider uncertainty in prompts when only having API access to the PLM. Finally, through extensive experiments, we carefully vet the proposed methods and find them competitive with (and sometimes even improving on) gradient-based approaches with full access to the PLM."}}
{"id": "Rvi21kcSIBF", "cdate": 1652959366531, "mdate": 1652959366531, "content": {"title": "AR-Net: Adaptive Frame Resolution for Efficient Action Recognition", "abstract": "Action recognition is an open and challenging problem in computer vision. While current state-of-the-art models offer excellent recognition results, their computational expense limits their impact for many real-world applications. In this paper, we propose a novel approach, called AR-Net (Adaptive Resolution Network), that selects on-the-fly the optimal resolution for each frame conditioned on the input for efficient action recognition in long untrimmed videos. Specifically, given a video frame, a policy network is used to decide what input resolution should be used for processing by the action recognition model, with the goal of improving both accuracy and efficiency. We efficiently train the policy network jointly with the recognition model using standard back-propagation. Extensive experiments on several challenging action recognition benchmark datasets well demonstrate the efficacy of our proposed approach over state-of-the-art methods."}}
{"id": "7cL46kHUu4", "cdate": 1652737604375, "mdate": null, "content": {"title": "Fair Infinitesimal Jackknife: Mitigating the Influence of Biased Training Data Points Without Refitting", "abstract": "In consequential decision-making applications, mitigating unwanted biases in machine learning models that yield systematic disadvantage to members of groups delineated by sensitive attributes such as race and gender is one key intervention to strive for equity. Focusing on demographic parity and equality of opportunity, in this paper we propose an algorithm that improves the fairness of a pre-trained classifier by simply dropping carefully selected training data points. We select instances based on their influence on the fairness metric of interest, computed using an infinitesimal jackknife-based approach. The dropping of training points is done in principle, but in practice does not require the model to be refit. Crucially, we find that such an intervention does not substantially reduce the predictive performance of the model but drastically improves the fairness metric. Through careful experiments, we evaluate the effectiveness of the proposed approach on diverse tasks and find that it consistently improves upon existing alternatives. "}}
{"id": "SWeAMcbMXc", "cdate": 1648658966071, "mdate": 1648658966071, "content": {"title": "Causal Feature Selection for Algorithmic Fairness", "abstract": "The use of machine learning (ML) in high-stakes societal decisions\nhas encouraged the consideration of fairness throughout the ML\nlifecycle. Although data integration is one of the primary steps to\ngenerate high-quality training data, most of the fairness literature\nignores this stage. In this work, we consider fairness in the integra-\ntion component of data management, aiming to identify features\nthat improve prediction without adding any bias to the dataset.\nWe work under the causal fairness paradigm [45 ]. Without requir-\ning the underlying structural causal model a priori, we propose\nan approach to identify a sub-collection of features that ensure\nfairness of the dataset by performing conditional independence\ntests between different subsets of features. We use group testing to\nimprove the complexity of the approach. We theoretically prove the\ncorrectness of the proposed algorithm and show that sublinear con-\nditional independence tests are sufficient to identify these variables.\nA detailed empirical evaluation is performed on real-world datasets\nto demonstrate the efficacy and efficiency of our technique"}}
{"id": "B0l_lDLs9gq", "cdate": 1646077528388, "mdate": null, "content": {"title": "Intervention Target Estimation in the Presence of Latent Variables", "abstract": "This paper considers the problem of estimating unknown intervention targets in causal directed acyclic graphs from observational and interventional data in the presence of latent variables. The focus is on linear structural equation models with soft interventions. The existing approaches to this problem involve extensive conditional independence tests, and they estimate the unknown intervention targets alongside learning the structure of the causal model in its entirety. This joint learning approach to estimating the intervention targets results in algorithms that are not scalable as graph sizes grow. This paper proposes an approach that does not necessitate learning the entire causal model and focuses on learning only the intervention targets. The key idea of this approach is the property that interventions impose sparse changes in the precision matrix of a linear model. Leveraging this property, the proposed framework consists of a sequence of precision difference estimation steps. Furthermore, the necessary knowledge to refine an observational Markov equivalence class (MEC) to an interventional MEC is inferred. Simulation results are provided to illustrate the scalability of the proposed algorithm and compare it with those of the existing approaches."}}
{"id": "w4YoVQlP4I", "cdate": 1640995200000, "mdate": 1683888613534, "content": {"title": "Causal Bandits for Linear Structural Equation Models", "abstract": "This paper studies the problem of designing an optimal sequence of interventions in a causal graphical model to minimize cumulative regret with respect to the best intervention in hindsight. This is, naturally, posed as a causal bandit problem. The focus is on causal bandits for linear structural equation models (SEMs) and soft interventions. It is assumed that the graph's structure is known and has $N$ nodes. Two linear mechanisms, one soft intervention and one observational, are assumed for each node, giving rise to $2^N$ possible interventions. Majority of the existing causal bandit algorithms assume that at least the interventional distributions of the reward node's parents are fully specified. However, there are $2^N$ such distributions (one corresponding to each intervention), acquiring which becomes prohibitive even in moderate-sized graphs. This paper dispenses with the assumption of knowing these distributions or their marginals. Two algorithms are proposed for the frequentist (UCB-based) and Bayesian (Thompson Sampling-based) settings. The key idea of these algorithms is to avoid directly estimating the $2^N$ reward distributions and instead estimate the parameters that fully specify the SEMs (linear in $N$) and use them to compute the rewards. In both algorithms, under boundedness assumptions on noise and the parameter space, the cumulative regrets scale as $\\tilde{\\cal O} (d^{L+\\frac{1}{2}} \\sqrt{NT})$, where $d$ is the graph's maximum degree, and $L$ is the length of its longest causal path. Additionally, a minimax lower of $\\Omega(d^{\\frac{L}{2}-2}\\sqrt{T})$ is presented, which suggests that the achievable and lower bounds conform in their scaling behavior with respect to the horizon $T$ and graph parameters $d$ and $L$."}}
{"id": "van7pK0Wi4", "cdate": 1640995200000, "mdate": 1683812708479, "content": {"title": "Fair Infinitesimal Jackknife: Mitigating the Influence of Biased Training Data Points Without Refitting", "abstract": "In consequential decision-making applications, mitigating unwanted biases in machine learning models that yield systematic disadvantage to members of groups delineated by sensitive attributes such as race and gender is one key intervention to strive for equity. Focusing on demographic parity and equality of opportunity, in this paper we propose an algorithm that improves the fairness of a pre-trained classifier by simply dropping carefully selected training data points. We select instances based on their influence on the fairness metric of interest, computed using an infinitesimal jackknife-based approach. The dropping of training points is done in principle, but in practice does not require the model to be refit. Crucially, we find that such an intervention does not substantially reduce the predictive performance of the model but drastically improves the fairness metric. Through careful experiments, we evaluate the effectiveness of the proposed approach on diverse tasks and find that it consistently improves upon existing alternatives."}}
