{"id": "erHHjuxwojZ", "cdate": 1672765400216, "mdate": 1672765400216, "content": {"title": "EDICT: Exact Diffusion Inversion via Coupled Transformations", "abstract": "Finding an initial noise vector that produces an input\nimage when fed into the diffusion process (known as inversion) is an important problem in denoising diffusion models (DDMs), with applications for real image editing. The\nstate-of-the-art approach for real image editing with inversion uses denoising diffusion implicit models (DDIMs [28])\nto deterministically noise the image to the intermediate\nstate along the path that the denoising would follow given\nthe original conditioning. However, DDIM inversion for\nreal images is unstable as it relies on local linearization assumptions, which result in the propagation of errors, leading to incorrect image reconstruction and loss of content. To\nalleviate these problems, we propose Exact Diffusion Inversion via Coupled Transformations (EDICT), an inversion\nmethod that draws inspiration from affine coupling layers.\nEDICT enables mathematically exact inversion of real and\nmodel-generated images by maintaining two coupled noise\nvectors which are used to invert each other in an alternating fashion. Using Stable Diffusion [24], a state-of-the-art\nlatent diffusion model, we demonstrate that EDICT successfully reconstructs real images with high fidelity. On complex\nimage datasets like MS-COCO, EDICT reconstruction significantly outperforms DDIM, improving the mean square\nerror of reconstruction by a factor of two. Using noise\nvectors inverted from real images, EDICT enables a wide\nrange of image edits\u2014from local and global semantic edits to image stylization\u2014while maintaining fidelity to the\noriginal image structure. EDICT requires no model training/finetuning, prompt tuning, or extra data and can be\ncombined with any pretrained DDM. Code is available at\nhttps://github.com/salesforce/EDICT."}}
{"id": "-e9ZgxrbWV", "cdate": 1672765339569, "mdate": 1672765339569, "content": {"title": "Can We Characterize Tasks Without Labels or Features?", "abstract": "The problem of expert model selection deals with choosing the appropriate pretrained network (\u201cexpert\u201d) to transfer to a target task. Methods, however, generally depend on\ntwo separate assumptions: the presence of labeled images\nand access to powerful \u201cprobe\u201d networks that yield useful\nfeatures. In this work, we demonstrate the current reliance\non both of these aspects and develop algorithms to operate\nwhen either of these assumptions fail. In the unlabeled case,\nwe show that pseudolabels from the probe network provide\ndiscriminative enough gradients to perform nearly-equal\ntask selection even when the probe network is trained on\nimagery unrelated to the tasks. To compute the embedding\nwith no probe network at all, we introduce the Task Tangent\nKernel (TTK) which uses a kernelized distance across multiple random networks to achieve performance over double that of other methods with randomly initialized models.\nCode is available at https://github.com/BramSW/\ntask_characterization_cvpr_2021/."}}
{"id": "J_T78k6ch7", "cdate": 1672765281113, "mdate": 1672765281113, "content": {"title": "Few-Shot Generalization for Single-Image 3D Reconstruction via Priors", "abstract": "Recent work on single-view 3D reconstruction shows impressive results, but has been restricted to a few fixed categories where extensive training data is available. The problem of generalizing these models to new classes with limited training data is largely open. To address this problem,\nwe present a new model architecture that reframes singleview 3D reconstruction as learnt, category agnostic refinement of a provided, category-specific prior. The provided\nprior shape for a novel class can be obtained from as few\nas one 3D shape from this class. Our model can start reconstructing objects from the novel class using this prior\nwithout seeing any training image for this class and without\nany retraining. Our model outperforms category-agnostic\nbaselines and remains competitive with more sophisticated\nbaselines that finetune on the novel categories. Additionally, our network is capable of improving the reconstruction given multiple views despite not being trained on task\nof multi-view reconstruction."}}
{"id": "Dlu6z3XGYE", "cdate": 1672765195286, "mdate": 1672765195286, "content": {"title": "Extending and analyzing self-supervised learning across domains", "abstract": "Self-supervised representation learning has achieved impressive results in recent years, with experiments primarily coming on ImageNet or other similarly large internet imagery datasets. There has\nbeen little to no work with these methods on other smaller domains,\nsuch as satellite, textural, or biological imagery. We experiment with\nseveral popular methods on an unprecedented variety of domains. We\ndiscover, among other findings, that Rotation is the most semantically meaningful task, while much of the performance of Jigsaw is\nattributable to the nature of its induced distribution rather than semantic understanding. Additionally, there are several areas, such as\nfine-grain classification, where all tasks underperform. We quantitatively\nand qualitatively diagnose the reasons for these failures and successes\nvia novel experiments studying pretext generalization, random labelings, and implicit dimensionality. Code and models are available at\nhttps://github.com/BramSW/Extending_SSRL_Across_Domains/."}}
{"id": "Nc7EsfpZ7C", "cdate": 1665285245313, "mdate": null, "content": {"title": "Designing active and thermostable enzymes with sequence-only predictive models", "abstract": "Data-driven models of fitness can be useful in designing novel proteins with desired properties, but many questions remain regarding how and in what settings they should be used.  Here, we ask: How can we use predictive models of protein fitness, whose predictions we might not always trust, to design protein sequences enhanced for multiple fitness functions?  We propose a general approach for doing so, and apply it to design novel variants of eight different acylphosphatase and lysozyme wild types, intended to be more thermostable and at least as catalytically active as the wild types.  Our method does not require a structure, experimental measurements of activity, curation of homologous sequences, or family-specific thermostability data.  Experimental characterizations of our designed sequences, as well as sequences designed by PROSS, a competitive baseline method for improving protein thermostability, are currently underway and forthcoming."}}
{"id": "mKsMcL8FfsV", "cdate": 1632875480060, "mdate": null, "content": {"title": "Learning Rich Nearest Neighbor Representations from Self-supervised Ensembles", "abstract": "Pretraining convolutional neural networks via self-supervision, and applying them in transfer learning, is an incredibly fast-growing field that is rapidly and iteratively improving performance across practically all image domains. \nMeanwhile, model ensembling is one of the most universally applicable techniques in supervised learning literature and practice, offering a simple solution to reliably improve performance. But how to optimally combine self-supervised models to maximize representation quality has largely remained unaddressed.\nIn this work, we provide a framework to perform self-supervised model ensembling via a novel method of learning representations directly through gradient descent at inference time.\nThis technique improves representation quality, as measured by k-nearest neighbors, both on the in-domain dataset and in the transfer setting, with models transferable from the former setting to the latter.\nAdditionally, this direct learning of feature through backpropagation improves representations from even a single model, echoing the improvements found in self-distillation."}}
