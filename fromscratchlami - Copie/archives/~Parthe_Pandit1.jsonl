{"id": "jb2Gp5peGU", "cdate": 1672531200000, "mdate": 1680416709308, "content": {"title": "Toward Large Kernel Models", "abstract": ""}}
{"id": "kBR2bM0tmwe", "cdate": 1663849976533, "mdate": null, "content": {"title": "Learning large-scale Kernel Networks", "abstract": "    This paper concerns large-scale training of *Kernel Networks*, a generalization of kernel machines that allows the model to have arbitrary centers. We propose a scalable training algorithm -- EigenPro 3.0 -- based on alternating projections with preconditioned SGD for the alternating steps. In contrast to classical kernel machines, but similar to neural networks, our algorithm enables decoupling the learned model from the training set. This empowers kernel models to take advantage of modern methodologies in deep learning, such as data augmentation. We demonstrate the promise of EigenPro 3.0 on several experiments over large datasets. We also show data augmentation can improve performance of kernel models."}}
{"id": "5oS20NUCJEX", "cdate": 1652737840414, "mdate": null, "content": {"title": "Benign, Tempered, or Catastrophic: Toward a Refined Taxonomy of Overfitting", "abstract": "The practical success of overparameterized neural networks has motivated the recent scientific study of \\emph{interpolating methods}-- learning methods which are able fit their training data perfectly. Empirically, certain interpolating methods can fit noisy training data without catastrophically bad test performance, which defies standard intuitions from statistical learning theory. Aiming to explain this, a large body of recent work has studied \\emph{benign overfitting}, a behavior seen in certain asymptotic settings under which interpolating methods approach Bayes-optimality, even in the presence of noise. In this work, we argue that, while benign overfitting has been instructive to study, real interpolating methods like deep networks do not fit benignly. That is, noise in the train set leads to suboptimal generalization, suggesting that these methods fall in an intermediate regime between benign and catastrophic overfitting, in which asymptotic risk is neither is neither Bayes-optimal nor unbounded, with the confounding effect of the noise being ``tempered\" but non-negligible. We call this behavior \\textit{tempered overfitting}. We first provide broad empirical evidence for our three-part taxonomy, demonstrating that deep neural networks and kernel machines fit to noisy data can be reasonably well classified as benign, tempered, or catastrophic. We then specialize to kernel (ridge) regression (KR), obtaining conditions on the ridge parameter and kernel eigenspectrum under which KR exhibits each of the three behaviors, demonstrating the consequences for KR with common kernels and trained neural networks of infinite width using experiments on natural and synthetic datasets."}}
{"id": "GyWsthkJ1E2", "cdate": 1652737819645, "mdate": null, "content": {"title": "Instability and Local Minima in GAN Training with Kernel Discriminators", "abstract": "Generative Adversarial Networks (GANs) are a widely-used tool for generative modeling of complex data.  Despite their empirical success, the training of GANs is not fully understood due to the joint training of the generator and discriminator. This paper analyzes these joint dynamics when the true samples, as well as the generated samples, are discrete, finite sets, and the discriminator is kernel-based. A simple yet expressive framework for analyzing training called the $\\textit{Isolated Points Model}$ is introduced. In the proposed model, the distance between true samples greatly exceeds the kernel width so that each generated point is influenced by at most one true point. The model enables precise characterization of the conditions for convergence both to good and bad minima. In particular, the analysis explains two common failure modes: (i) an approximate mode collapse and (ii) divergence. Numerical simulations are provided that predictably replicate these behaviors."}}
{"id": "sWpi58N5m4T", "cdate": 1640995200000, "mdate": 1680416709307, "content": {"title": "Kernel Ridgeless Regression is Inconsistent for Low Dimensions", "abstract": ""}}
{"id": "kY1CjX8u1L", "cdate": 1640995200000, "mdate": 1680416709307, "content": {"title": "Kernel Methods and Multi-layer Perceptrons Learn Linear Models in High Dimensions", "abstract": ""}}
{"id": "ZmtJTOhpEUT", "cdate": 1640995200000, "mdate": 1680416709318, "content": {"title": "Benign, Tempered, or Catastrophic: A Taxonomy of Overfitting", "abstract": ""}}
{"id": "GHN-tJ5rUcb", "cdate": 1640995200000, "mdate": 1680416709319, "content": {"title": "Instability and Local Minima in GAN Training with Kernel Discriminators", "abstract": ""}}
{"id": "7Xd4-R-Zt3e", "cdate": 1640995200000, "mdate": 1680416709308, "content": {"title": "A note on Linear Bottleneck networks and their Transition to Multilinearity", "abstract": ""}}
{"id": "2Lv8mM7rTXf", "cdate": 1640995200000, "mdate": 1680416709319, "content": {"title": "Feature learning in neural networks and kernel machines that recursively learn features", "abstract": ""}}
