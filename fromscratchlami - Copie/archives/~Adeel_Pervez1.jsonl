{"id": "aUl_XKqA-tM", "cdate": 1672531200000, "mdate": 1699187769315, "content": {"title": "Differentiable Mathematical Programming for Object-Centric Representation Learning", "abstract": ""}}
{"id": "FRSQ03vUOe", "cdate": 1672531200000, "mdate": 1699187769284, "content": {"title": "Scalable Subset Sampling with Neural Conditional Poisson Networks", "abstract": ""}}
{"id": "1J-ZTr7aypY", "cdate": 1663850000689, "mdate": null, "content": {"title": "Differentiable Mathematical Programming for Object-Centric Representation Learning", "abstract": "We propose topology-aware feature partitioning into $k$ disjoint partitions for given scene features as a method for object-centric representation learning. To this end, we propose to use minimum $s$-$t$ graph cuts as a partitioning method which is represented as a linear program. The method is topologically aware since it explicitly encodes neighborhood relationships in the image graph. To solve the graph cuts our solution relies on an efficient, scalable, and differentiable quadratic programming approximation. Optimizations specific to cut problems allow us to solve the quadratic programs and compute their gradients significantly more efficiently compared with the general quadratic programming approach. Our results show that our approach is scalable and outperforms existing methods on object discovery tasks with textured scenes and objects."}}
{"id": "p8hMBcPtvju", "cdate": 1663849999969, "mdate": null, "content": {"title": "Scalable Subset Sampling with Neural Conditional Poisson Networks", "abstract": "A number of problems in learning can be formulated in terms of the basic primitive of sampling $k$ elements out of a universe of $n$ elements. This subset sampling operation cannot directly be included in differentiable models and approximations are essential. Current approaches take an \\emph{order sampling} approach to sampling subsets and depend on differentiable approximations of the Top-$k$ operator for selecting the largest $k$ elements from a set. We present a simple alternative method for sampling subsets based on \\emph{conditional Poisson sampling}. Unlike order sampling approaches, the parallel complexity of the proposed method is independent of the subset size which makes the method scalable to large subset sizes. We adapt the procedure to make it efficient and amenable to discrete gradient approximations for use in differentiable models. Furthermore, the method also allows the subset size parameter $k$ to be differentiable. We demonstrate our approach on model explanation, image sub-sampling and stochastic $k$-nearest neighbor tasks outperforming existing methods in accuracy, efficiency and scalability."}}
{"id": "7K-YiRP_Ir", "cdate": 1640995200000, "mdate": 1683145289901, "content": {"title": "Stability Regularization for Discrete Representation Learning", "abstract": "We present a method for training neural network models with discrete stochastic variables. The core of the method is \\emph{stability regularization}, which is a regularization procedure based on the idea of noise stability developed in Gaussian isoperimetric theory in the analysis of Gaussian functions. Stability regularization is method to make the output of continuous functions of Gaussian random variables close to discrete, that is binary or categorical, without the need for significant manual tuning. The method allows control over the extent to which a Gaussian function's output is close to discrete, thus allowing for continued flow of gradient. The method can be used standalone or in combination with existing continuous relaxation methods. We validate the method in a broad range of experiments using discrete variables including neural relational inference, generative modeling, clustering and conditional computing."}}
{"id": "6tmjoym9LR6", "cdate": 1632875641793, "mdate": null, "content": {"title": "Stability Regularization for Discrete Representation Learning", "abstract": "We present a method for training neural network models with discrete stochastic variables.\nThe core of the method is \\emph{stability regularization}, which is a regularization procedure based on the idea of noise stability developed in Gaussian isoperimetric theory in the analysis of Gaussian functions.\nStability regularization is method to make the output of continuous functions of Gaussian random variables close to discrete, that is binary or categorical, without the need for significant manual tuning.\nThe method allows control over the extent to which a Gaussian function's output is close to discrete, thus allowing for continued flow of gradient.\nThe method can be used standalone or in combination with existing continuous relaxation methods.\nWe validate the method in a broad range of experiments using discrete variables including neural relational inference, generative modeling, clustering and conditional computing."}}
{"id": "-307meF_GXp", "cdate": 1609459200000, "mdate": 1631642280243, "content": {"title": "Spectral Smoothing Unveils Phase Transitions in Hierarchical Variational Autoencoders", "abstract": "Variational autoencoders with deep hierarchies of stochastic layers have been known to suffer from the problem of posterior collapse, where the top layers fall back to the prior and become independ..."}}
{"id": "uvEgLKYMBF9", "cdate": 1601308213369, "mdate": null, "content": {"title": "Variance Reduction in Hierarchical Variational Autoencoders", "abstract": "Variational autoencoders with deep hierarchies of stochastic layers have been known to suffer from the problem of posterior collapse, where the top layers fall back to the prior and become independent of input.\nWe suggest that the hierarchical VAE objective explicitly includes the variance of the function parameterizing the mean and variance of the latent Gaussian distribution which itself is often a high variance function.\nBuilding on this we generalize VAE neural networks by incorporating a smoothing parameter motivated by Gaussian analysis to reduce variance in parameterizing functions and show that this can help to solve the problem of posterior collapse.\nWe further show that under such smoothing the VAE loss exhibits a phase transition, where the top layer KL divergence sharply drops to zero at a critical value of the smoothing parameter.\nWe validate the phenomenon across model configurations and datasets."}}
{"id": "Y-g9SDkjAHo", "cdate": 1577836800000, "mdate": 1631642280266, "content": {"title": "Low Bias Low Variance Gradient Estimates for Boolean Stochastic Networks", "abstract": "Stochastic neural networks with discrete random variables are an important class of models for their expressiveness and interpretability. Since direct differentiation and backpropagation is not pos..."}}
{"id": "DijRgY7dd2J", "cdate": 1577836800000, "mdate": 1699187769307, "content": {"title": "Co-clustering optimization using Artificial Bee Colony (ABC) algorithm", "abstract": ""}}
