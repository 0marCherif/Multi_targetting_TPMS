{"id": "gaVogNewvNJ", "cdate": 1546300800000, "mdate": 1663872593574, "content": {"title": "Deep Gaussian Processes for Multi-fidelity Modeling", "abstract": "Multi-fidelity methods are prominently used when cheaply-obtained, but possibly biased and noisy, observations must be effectively combined with limited or expensive true data in order to construct reliable models. This arises in both fundamental machine learning procedures such as Bayesian optimization, as well as more practical science and engineering applications. In this paper we develop a novel multi-fidelity model which treats layers of a deep Gaussian process as fidelity levels, and uses a variational inference scheme to propagate uncertainty across them. This allows for capturing nonlinear correlations between fidelities with lower risk of overfitting than existing methods exploiting compositional structure, which are conversely burdened by structural assumptions and constraints. We show that the proposed approach makes substantial improvements in quantifying and propagating uncertainty in multi-fidelity set-ups, which in turn improves their effectiveness in decision making pipelines."}}
{"id": "4WpERzwqKU", "cdate": 1546300800000, "mdate": 1663872593570, "content": {"title": "Broadening the scope of gaussian processes for large-scale learning. (\u00c9largir la port\u00e9e des processus gaussiens pour l'apprentissage \u00e0 grande \u00e9chelle)", "abstract": "The renewed importance of decision making under uncertainty calls for a re-evaluation of Bayesian inference techniques targeting this goal in the big data regime. Gaussian processes (GPs) are a fundamental building block of many probabilistic kernel machines; however, the computational and storage complexity of GPs hinders their scaling to large modern datasets. The contributions presented in this thesis are two-fold. We first investigate the effectiveness of exact GP inference on a computational budget by proposing a novel scheme for accelerating regression and classification by way of preconditioning. In the spirit of probabilistic numerics, we also show how the numerical uncertainty introduced by approximate linear algebra should be adequately evaluated and incorporated. Bridging the gap between GPs and deep learning techniques remains a pertinent research goal, and the second broad contribution of this thesis is to establish and reinforce the role of GPs, and their deep counterparts (DGPs), in this setting. Whereas GPs and DGPs were once deemed unfit to compete with alternative state-of-the-art methods, we demonstrate how such models can also be adapted to the large-scale and complex tasks to which machine learning is now being applied."}}
{"id": "rJW2Nn-dbS", "cdate": 1483228800000, "mdate": null, "content": {"title": "Random Feature Expansions for Deep Gaussian Processes", "abstract": "The composition of multiple Gaussian Processes as a Deep Gaussian Process DGP enables a deep probabilistic nonparametric approach to flexibly tackle complex machine learning problems with sound qua..."}}
{"id": "M0UmPu_aGE_", "cdate": 1483228800000, "mdate": 1663872593570, "content": {"title": "AutoGP: Exploring the Capabilities and Limitations of Gaussian Process Models", "abstract": ""}}
{"id": "B1-if3ZO-H", "cdate": 1451606400000, "mdate": null, "content": {"title": "Preconditioning Kernel Matrices", "abstract": "The computational and storage complexity of kernel machines presents the primary barrier to their scaling to large, modern, datasets. A common way to tackle the scalability issue is to use the conj..."}}
