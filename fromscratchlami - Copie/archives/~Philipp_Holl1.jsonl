{"id": "F2Gk6Vr3wu", "cdate": 1652737551187, "mdate": null, "content": {"title": "Scale-invariant Learning by Physics Inversion", "abstract": "Solving inverse problems, such as parameter estimation and optimal control, is a vital part of science. Many experiments repeatedly collect data and rely on machine learning algorithms to quickly infer solutions to the associated inverse problems. We find that state-of-the-art training techniques are not well-suited to many problems that involve physical processes. The highly nonlinear behavior, common in physical processes, results in strongly varying gradients that lead first-order optimizers like SGD or Adam to compute suboptimal optimization directions.\nWe propose a novel hybrid training approach that combines higher-order optimization methods with machine learning techniques. We take updates from a scale-invariant inverse problem solver and embed them into the gradient-descent-based learning pipeline, replacing the regular gradient of the physical process.\nWe demonstrate the capabilities of our method on a variety of canonical physical systems, showing that it yields significant improvements on a wide range of optimization and learning problems."}}
{"id": "QYDP6dJUsh", "cdate": 1640995200000, "mdate": 1681650334361, "content": {"title": "Simulating Liquids with Graph Networks", "abstract": ""}}
{"id": "OUD44FfIKUo", "cdate": 1640995200000, "mdate": 1681650334482, "content": {"title": "Half-Inverse Gradients for Physical Deep Learning", "abstract": ""}}
{"id": "-yIaYCqm62L", "cdate": 1640995200000, "mdate": 1681650334481, "content": {"title": "Half-Inverse Gradients for Physical Deep Learning", "abstract": ""}}
{"id": "HTx7vrlLBEj", "cdate": 1632875545982, "mdate": null, "content": {"title": "Half-Inverse Gradients for Physical Deep Learning", "abstract": "Recent works in deep learning have shown that integrating differentiable physics simulators into the training process can greatly improve the quality of results. Although this combination represents a more complex optimization task than usual neural network training, the same gradient-based optimizers are used to minimize the loss function. However, the integrated physics solvers have a profound effect on the gradient flow as manipulating scales in magnitude and direction is an inherent property of many physical processes. Consequently, the gradient flow is often highly unbalanced and creates an environment in which existing gradient-based optimizers perform poorly. In this work, we analyze the characteristics of both physical and neural network optimizations separately to derive a new method based on a half-inversion of the Jacobian. Our approach combines principles of both classical network and physics optimizers to solve the combined optimization task. Compared to state-of-the-art neural network optimizers, our method converges more quickly and to better solutions, which we demonstrate on three complex learning problems involving nonlinear oscillators, the Schroedinger equation and the Poisson problem."}}
{"id": "famc03Gg231", "cdate": 1632875468594, "mdate": null, "content": {"title": "Physical Gradients for Deep Learning", "abstract": "Solving inverse problems, such as parameter estimation and optimal control, is a vital part of science. Many experiments repeatedly collect data and employ machine learning algorithms to quickly infer solutions to the associated inverse problems. We find that state-of-the-art training techniques are not well-suited to many problems that involve physical processes since the magnitude and direction of the gradients can vary strongly. We propose a novel hybrid training approach that combines higher-order optimization methods with machine learning techniques. We replace the gradient of the physical process by a new construct, referred to as the physical gradient. This also allows us to introduce domain knowledge into training by incorporating priors about the solution space into the gradients. We demonstrate the capabilities of our method on a variety of canonical physical systems, showing that physical gradients yield significant improvements on a wide range of optimization and learning problems."}}
{"id": "OvshRb6f_9G", "cdate": 1609459200000, "mdate": 1681650334345, "content": {"title": "Physics-based Deep Learning", "abstract": ""}}
{"id": "4Q2IHyJnam", "cdate": 1609459200000, "mdate": 1681650334336, "content": {"title": "Physical Gradients for Deep Learning", "abstract": ""}}
{"id": "uHIapEDegh0", "cdate": 1577836800000, "mdate": 1681650334381, "content": {"title": "Learning to Control PDEs with Differentiable Physics", "abstract": ""}}
{"id": "tJqyNaFyEyk", "cdate": 1577836800000, "mdate": null, "content": {"title": "Solver-in-the-Loop: Learning from Differentiable Physics to Interact with Iterative PDE-Solvers", "abstract": "Finding accurate solutions to partial differential equations (PDEs) is a crucial task in all scientific and engineering disciplines. It has recently been shown that machine learning methods can improve the solution accuracy by correcting for effects not captured by the discretized PDE. We target the problem of reducing numerical errors of iterative PDE solvers and compare different learning approaches for finding complex correction functions. We find that previously used learning approaches are significantly outperformed by methods that integrate the solver into the training loop and thereby allow the model to interact with the PDE during training. This provides the model with realistic input distributions that take previous corrections into account, yielding improvements in accuracy with stable rollouts of several hundred recurrent evaluation steps and surpassing even tailored supervised variants. We highlight the performance of the differentiable physics networks for a wide variety of PDEs, from non-linear advection-diffusion systems to three-dimensional Navier-Stokes flows."}}
