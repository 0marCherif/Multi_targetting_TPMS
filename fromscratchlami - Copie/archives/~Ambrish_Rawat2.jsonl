{"id": "sOc0SRQ1WY", "cdate": 1681461078954, "mdate": 1681461078954, "content": {"title": "Certified Federated Adversarial Training", "abstract": "In federated learning (FL), robust aggregation schemes have been developed to protect against malicious clients. Many robust aggregation schemes rely on certain numbers of benign clients being present in a quorum of workers. This can be hard to guarantee when clients can join at will, or join based on factors such as idle system status, and connected to power and WiFi. We tackle the scenario of securing FL systems conducting adversarial training when a quorum of workers could be completely malicious. We model an attacker who poisons the model to insert a weakness into the adversarial training such that the model displays apparent adversarial robustness, while the attacker can exploit the inserted weakness to bypass the adversarial training and force the model to misclassify adversarial examples. We use abstract interpretation techniques to detect such stealthy attacks and block the corrupted model updates. We show that this defence can preserve adversarial robustness even against an adaptive attacker."}}
{"id": "3orPTYbqK82", "cdate": 1681460924324, "mdate": 1681460924324, "content": {"title": "FAT: Federated Adversarial Training", "abstract": "Federated learning (FL) is one of the most important paradigms addressing privacy and data governance issues in machine learning (ML). Adversarial training has emerged, so far, as the most promising approach against evasion threats on ML models. In this paper, we take the first known steps towards federated adversarial training (FAT) combining both methods to reduce the threat of evasion during inference while preserving the data privacy during training. We investigate the effectiveness of the FAT protocol for idealised federated settings using MNIST, Fashion-MNIST, and CIFAR10, and provide first insights on stabilising the training on the LEAF benchmark dataset which specifically emulates a federated learning environment. We identify challenges with this natural extension of adversarial training with regards to achieved adversarial robustness and further examine the idealised settings in the presence of clients undermining model convergence. We find that Trimmed Mean and Bulyan defences can be compromised and we were able to subvert Krum with a novel distillation based attack which presents an apparently \"robust\" model to the defender while in fact the model fails to provide robustness against simple attack modifications."}}
{"id": "b7vu9ukdpdL", "cdate": 1663939400503, "mdate": null, "content": {"title": "Federated Continual Learning with Differentially Private Data Sharing", "abstract": "In Federated Learning (FL) many types of skews can occur, including uneven class distributions, or varying client participation. In addition, new tasks and data modalities can be encountered as time passes, which leads us to the problem domain of Federated Continual Learning (FCL). \nIn this work we study how we can adapt some of the simplest, but often most effective, Continual Learning approaches based on replay to FL. We focus on temporal shifts in client behaviour, and show that direct application of replay methods leads to poor results. To address these shortcomings, we explore data sharing between clients employing differential privacy. This alleviates the shortcomings in current baselines, resulting in performance gains in a wide range of cases, with our method achieving maximum gains of 49%."}}
{"id": "RZai7UKX7Oo", "cdate": 1621522019343, "mdate": null, "content": {"title": "Bandit Limited Discrepancy Search and Application to Machine Learning Pipeline Optimization", "abstract": "Optimizing a machine learning (ML) pipeline has been an important topic of AI and ML.  Despite recent progress, this topic remains a challenging problem, due to potentially many combinations to consider as well as slow training and validation.  We present the BLDS algorithm for optimized algorithm selection (ML operations) in a fixed ML pipeline structure.  BLDS performs multi-fidelity optimization for selecting ML algorithms trained with smaller computational overhead, while controlling its pipeline search based on multi-armed bandit and limited discrepancy search.  Our experiments on well-known benchmarks show that BLDS is superior to competing algorithms.\n"}}
{"id": "k61vjRgiFDM", "cdate": 1620636082247, "mdate": null, "content": {"title": "Adversarial Robustness Toolbox v1.0.0", "abstract": "Adversarial Robustness Toolbox (ART) is a Python library supporting developers and researchers in defending Machine Learning models (Deep Neural Networks, Gradient Boosted Decision Trees, Support Vector Machines, Random Forests, Logistic Regression, Gaussian Processes, Decision Trees, Scikit-learn Pipelines, etc.) against adversarial threats and helps making AI systems more secure and trustworthy. Machine Learning models are vulnerable to adversarial examples, which are inputs (images, texts, tabular data, etc.) deliberately modified to produce a desired response by the Machine Learning model. ART provides the tools to build and deploy defences and test them with adversarial attacks. Defending Machine Learning models involves certifying and verifying model robustness and model hardening with approaches such as pre-processing inputs, augmenting training data with adversarial samples, and leveraging runtime detection methods to flag any inputs that might have been modified by an adversary. The attacks implemented in ART allow creating adversarial attacks against Machine Learning models which is required to test defenses with state-of-the-art threat models. Supported Machine Learning Libraries include TensorFlow (v1 and v2), Keras, PyTorch, MXNet, Scikit-learn, XGBoost, LightGBM, CatBoost, and GPy. The source code of ART is released with MIT license. The release includes code examples, notebooks with tutorials and documentation."}}
{"id": "bUulS5MTa-", "cdate": 1609459200000, "mdate": 1629295348394, "content": {"title": "The Devil is in the GAN: Defending Deep Generative Models Against Backdoor Attacks", "abstract": "Deep Generative Models (DGMs) allow users to synthesize data from complex, high-dimensional manifolds. Industry applications of DGMs include data augmentation to boost performance of (semi-)supervised machine learning, or to mitigate fairness or privacy concerns. Large-scale DGMs are notoriously hard to train, requiring expert skills, large amounts of data and extensive computational resources. Thus, it can be expected that many enterprises will resort to sourcing pre-trained DGMs from potentially unverified third parties, e.g.~open source model repositories. As we show in this paper, such a deployment scenario poses a new attack surface, which allows adversaries to potentially undermine the integrity of entire machine learning development pipelines in a victim organization. Specifically, we describe novel training-time attacks resulting in corrupted DGMs that synthesize regular data under normal operations and designated target outputs for inputs sampled from a trigger distribution. Depending on the control that the adversary has over the random number generation, this imposes various degrees of risk that harmful data may enter the machine learning development pipelines, potentially causing material or reputational damage to the victim organization. Our attacks are based on adversarial loss functions that combine the dual objectives of attack stealth and fidelity. We show its effectiveness for a variety of DGM architectures (Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs)) and data domains (images, audio). Our experiments show that - even for large-scale industry-grade DGMs - our attack can be mounted with only modest computational efforts. We also investigate the effectiveness of different defensive approaches (based on static/dynamic model and output inspections) and prescribe a practical defense strategy that paves the way for safe usage of DGMs."}}
{"id": "O8g4g1wIrS", "cdate": 1609459200000, "mdate": 1629295348383, "content": {"title": "Searching for Machine Learning Pipelines Using a Context-Free Grammar", "abstract": "AutoML automatically selects, composes and parameterizes machine learning algorithms into a workflow or pipeline of operations that aims at maximizing performance on a given dataset. Although current methods for AutoML achieved impressive results they mostly concentrate on optimizing fixed linear workflows. In this paper, we take a different approach and focus on generating and optimizing pipelines of complex directed acyclic graph shapes. These complex pipeline structure may lead to discovering hidden features and thus boost performance considerably. We explore the power of heuristic search and context-free grammars to search and optimize these kinds of pipelines. Experiments on various benchmark datasets show that our approach is highly competitive and often outperforms existing AutoML systems."}}
{"id": "Do2idYwrdRK", "cdate": 1599598780000, "mdate": null, "content": {"title": "IBM Federated Learning: an Enterprise Framework White Paper V0.1", "abstract": "Federated Learning (FL) is an approach to conduct machine learning without centralizing\ntraining data in a single place, for reasons of privacy, confidentiality or data volume. However,\nsolving federated machine learning problems raises issues above and beyond those of centralized\nmachine learning. These issues include setting up communication infrastructure between parties,\ncoordinating the learning process, integrating party results, understanding the characteristics\nof the training data sets of different participating parties, handling data heterogeneity, and\noperating with the absence of a verification data set.\nIBM Federated Learning provides infrastructure and coordination for federated learning.\nData scientists can design and run federated learning jobs based on existing, centralized machine\nlearning models and can provide high-level instructions on how to run the federation. The\nframework applies to both Deep Neural Networks as well as \u201ctraditional\u201d approaches for the\nmost common machine learning libraries. IBM Federated Learning enables data scientists to\nexpand their scope from centralized to federated machine learning, minimizing the learning curve\nat the outset while also providing the flexibility to deploy to different compute environments\nand design custom fusion algorithms."}}
{"id": "uS2WZzYGBbq", "cdate": 1577836800000, "mdate": null, "content": {"title": "Automation of Deep Learning - Theory and Practice", "abstract": "The growing interest in both the automation of machine learning and deep learning has inevitably led to the development of a wide variety of methods to automate deep learning. The choice of network architecture has proven critical, and many improvements in deep learning are due to new structuring of it. However, deep learning techniques are computationally intensive and their use requires a high level of domain knowledge. Even a partial automation of this process therefore helps to make deep learning more accessible for everyone. In this tutorial we present a uniform formalism that enables different methods to be categorized and compare the different approaches in terms of their performance. We achieve this through a comprehensive discussion of the commonly used architecture search spaces and architecture optimization algorithms based on reinforcement learning and evolutionary algorithms as well as approaches that include surrogate and one-shot models. In addition, we discuss approaches to accelerate the search for neural architectures based on early termination and transfer learning and address the new research directions, which include constrained and multi-objective architecture search as well as the automated search for data augmentation, optimizers, and activation functions."}}
{"id": "RqmbMYYKOn", "cdate": 1577836800000, "mdate": 1629295357830, "content": {"title": "FAT: Federated Adversarial Training", "abstract": "Federated learning (FL) is one of the most important paradigms addressing privacy and data governance issues in machine learning (ML). Adversarial training has emerged, so far, as the most promising approach against evasion threats on ML models. In this paper, we take the first known steps towards federated adversarial training (FAT) combining both methods to reduce the threat of evasion during inference while preserving the data privacy during training. We investigate the effectiveness of the FAT protocol for idealised federated settings using MNIST, Fashion-MNIST, and CIFAR10, and provide first insights on stabilising the training on the LEAF benchmark dataset which specifically emulates a federated learning environment. We identify challenges with this natural extension of adversarial training with regards to achieved adversarial robustness and further examine the idealised settings in the presence of clients undermining model convergence. We find that Trimmed Mean and Bulyan defences can be compromised and we were able to subvert Krum with a novel distillation based attack which presents an apparently \"robust\" model to the defender while in fact the model fails to provide robustness against simple attack modifications."}}
