{"id": "5LZGXJSlDim", "cdate": 1698699755423, "mdate": 1698699755423, "content": {"title": "RC-MVSNet: Unsupervised Multi-View Stereo with Neural Rendering", "abstract": "Finding accurate correspondences among different views is the Achilles' heel of unsupervised Multi-View Stereo (MVS). Existing methods are built upon the assumption that corresponding pixels share similar photometric features. However, multi-view images in real scenarios observe non-Lambertian surfaces and experience occlusions. In this work, we propose a novel approach with neural rendering (RC-MVSNet) to solve such ambiguity issues of correspondences among views. Specifically, we impose a depth rendering consistency loss to constrain the geometry features close to the object surface to alleviate occlusions. Concurrently, we introduce a reference view synthesis loss to generate consistent supervision, even for non-Lambertian surfaces. Extensive experiments on DTU and Tanks&Temples benchmarks demonstrate that our approach achieves state-of-the-art performance over unsupervised MVS frameworks and competitive performance to many supervised methods."}}
{"id": "c2uV0aj_eN", "cdate": 1651651253766, "mdate": 1651651253766, "content": {"title": "Adaptive sampling towards fast graph representation learning", "abstract": "Graph Convolutional Networks (GCNs) have become a crucial tool on learning representations of graph vertices. The main challenge of adapting GCNs on large-scale graphs is the scalability issue that it incurs heavy cost both in computation and memory due to the uncontrollable neighborhood expansion across layers. In this paper, we accelerate the training of GCNs through developing an adaptive layer-wise sampling method. By constructing the network layer by layer in a top-down passway, we sample the lower layer conditioned on the top one, where the sampled neighborhoods are shared by different parent nodes and the over expansion is avoided owing to the fixed-size sampling. More importantly, the proposed sampler is adaptive and applicable for explicit variance reduction, which in turn enhances the training of our method. Furthermore, we propose a novel and economical approach to promote the message passing over distant nodes by applying skip connections. Intensive experiments on several benchmarks verify the effectiveness of our method regarding the classification accuracy while enjoying faster convergence speed."}}
{"id": "p3aRH0Cw_KS", "cdate": 1651650894034, "mdate": 1651650894034, "content": {"title": "Leverage Your Local and Global Representations: A New Self-Supervised Learning Strategy", "abstract": "Self-supervised learning (SSL) methods aim to learn view-invariant representations by maximizing the similarity between the features extracted from different crops of the same image regardless of cropping size and content. In essence, this strategy ignores the fact that two crops may truly contain different image information, e.g., background and small objects, and thus tends to restrain the diversity of the learned representations. In this work, we address this issue by introducing a new self-supervised learning strategy, LoGo, that explicitly reasons about Local and Global crops. To achieve view invariance, LoGo encourages similarity between global crops from the same image, as well as between a global and a local crop. However, to correctly encode the fact that the content of smaller crops may differ entirely, LoGo promotes two local crops to have dissimilar representations, while being close to global crops. Our LoGo strategy can easily be applied to existing SSL methods. Our extensive experiments on a variety of datasets and using different self-supervised learning frameworks validate its superiority over existing approaches. Noticeably, we achieve better results than supervised models on transfer learning when using only 1/10 of the data."}}
{"id": "qd5cAu2FG6Y", "cdate": 1640995200000, "mdate": 1667342441982, "content": {"title": "Deep Non-rigid Structure-from-Motion: A Sequence-to-Sequence Translation Perspective", "abstract": "Directly regressing the non-rigid shape and camera pose from the individual 2D frame is ill-suited to the Non-Rigid Structure-from-Motion (NRSfM) problem. This frame-by-frame 3D reconstruction pipeline overlooks the inherent spatial-temporal nature of NRSfM, i.e., reconstructing the whole 3D sequence from the input 2D sequence. In this paper, we propose to model deep NRSfM from a sequence-to-sequence translation perspective, where the input 2D frame sequence is taken as a whole to reconstruct the deforming 3D non-rigid shape sequence. First, we apply a shape-motion predictor to estimate the initial non-rigid shape and camera motion from a single frame. Then we propose a context modeling module to model camera motions and complex non-rigid shapes. To tackle the difficulty in enforcing the global structure constraint within the deep framework, we propose to impose the union-of-subspace structure by replacing the self-expressiveness layer with multi-head attention and delayed regularizers, which enables end-to-end batch-wise training. Experimental results across different datasets such as Human3.6M, CMU Mocap and InterHand prove the superiority of our framework. The code will be made publicly available"}}
{"id": "qaGIRWz5mZ", "cdate": 1640995200000, "mdate": 1667342441919, "content": {"title": "Semi-supervised Active Salient Object Detection", "abstract": ""}}
{"id": "gqIMDky6vDk", "cdate": 1640995200000, "mdate": 1667342441927, "content": {"title": "MuIT: An End-to-End Multitask Learning Transformer", "abstract": "We propose an end-to-end Multitask Learning Transformer framework, named MulT, to simultaneously learn multiple high-level vision tasks, including depth estimation, semantic segmentation, reshading, surface normal estimation, 2D keypoint detection, and edge detection. Based on the Swin transformer model, our framework encodes the input image into a shared representation and makes predictions for each vision task using task-specific transformer-based decoder heads. At the heart of our approach is a shared attention mechanism modeling the dependencies across the tasks. We evaluate our model on several multitask benchmarks, showing that our MulT framework outperforms both the state-of-the art multitask convolutional neural network models and all the respective single task transformer models. Our experiments further highlight the benefits of sharing attention across all the tasks, and demonstrate that our MulT model is robust and generalizes well to new domains. Our project website is at https://ivrl.github.io/MulT/."}}
{"id": "GA8vS_2Svv", "cdate": 1640995200000, "mdate": 1667342441919, "content": {"title": "Leverage Your Local and Global Representations: A New Self-Supervised Learning Strategy", "abstract": "Self-supervised learning (SSL) methods aim to learn view-invariant representations by maximizing the similar-ity between the features extracted from different crops of the same image regardless of cropping size and content. In essence, this strategy ignores the fact that two crops may truly contain different image information, e.g., background and small objects, and thus tends to restrain the diversity of the learned representations. In this work, we address this issue by introducing a new self-supervised learning strat-egy, LoGo, that explicitly reasons about Local and Global crops. To achieve view invariance, LoGo encourages similarity between global crops from the same image, as well as between a global and a local crop. However, to correctly encode the fact that the content of smaller crops may differ entirely, LoGo promotes two local crops to have dissimi-lar representations, while being close to global crops. Our LoGo strategy can easily be applied to existing SSL meth-ods. Our extensive experiments on a variety of datasets and using different self-supervised learning frameworks vali-date its superiority over existing approaches. Noticeably, we achieve better results than supervised models on trans-fer learning when using only 1/10 of the data. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> Our code and pretrained models can be found at https://github.com/ztt1024/LoGo-SSL."}}
{"id": "86uA7R3lb0Z", "cdate": 1623574598745, "mdate": 1623574598745, "content": {"title": "Optimal Feature Transport for Cross-View Image Geo-Localization", "abstract": "This paper addresses the problem of cross-view image geo-\nlocalization, where the geographic location of a ground-level\nstreet-view query image is estimated by matching it against\na large scale aerial map (e.g., a high-resolution satellite im-\nage). State-of-the-art deep-learning based methods tackle this\nproblem as deep metric learning which aims to learn global\nfeature representations of the scene seen by the two different\nviews. Despite promising results are obtained by such deep\nmetric learning methods, they, however, fail to exploit a cru-\ncial cue relevant for localization, namely, the spatial layout\nof local features. Moreover, little attention is paid to the ob-\nvious domain gap (between aerial view and ground view) in\nthe context of cross-view localization. This paper proposes\na novel Cross-View Feature Transport (CVFT) technique to\nexplicitly establish cross-view domain transfer that facilitates\nfeature alignment between ground and aerial images. Specif-\nically, we implement the CVFT as network layers, which\ntransports features from one domain to the other, leading to\nmore meaningful feature similarity comparison. Our model is\ndifferentiable and can be learned end-to-end. Experiments on\nlarge-scale datasets have demonstrated that our method has\nremarkably boosted the state-of-the-art cross-view localiza-\ntion performance, e.g., on the CVUSA dataset, with signifi-\ncant improvements for top-1 recall from 40.79% to 61.43%,\nand for top-10 from 76.36% to 90.49%. We expect the key in-\nsight of the paper (i.e., explicitly handling domain difference\nvia domain transport) will prove to be useful for other similar\nproblems in computer vision as well."}}
{"id": "H2UJcaAX2bG", "cdate": 1620959342722, "mdate": null, "content": {"title": "UC-Net: Uncertainty Inspired RGB-D Saliency Detection via Conditional Variational Autoencoders", "abstract": "In this paper, we propose the first framework (UCNet) to employ uncertainty for RGB-D saliency detection by learning from the data labeling process. Existing RGB-D saliency detection methods treat the saliency detection task as a point estimation problem, and produce a single saliency map following a deterministic learning pipeline. Inspired by the saliency data labeling process, we propose a probabilistic RGB-D saliency detection network via conditional variational autoencoders to model human annotation uncertainty and generate multiple saliency maps for each input image by sampling in the latent space. With the proposed saliency consensus process, we are able to generate an accurate saliency map based on these multiple predictions. Quantitative and qualitative evaluations on six challenging benchmark datasets against 18 competing algorithms demonstrate the effectiveness of our approach in learning the distribution of saliency maps, leading to a new state-of-the-art in RGB-D saliency detection."}}
{"id": "shEAPmqSywD", "cdate": 1617670309308, "mdate": null, "content": {"title": "Learning Saliency from Single Noisy Labelling: A Robust Model Fitting Perspective", "abstract": "The advances made in predicting visual saliency using deep neural networks come at the expense of collecting large-scale annotated data. However, pixel-wise annotation is labor-intensive and overwhelming. In this paper, we propose to learn saliency prediction from a single noisy labelling, which is easy to obtain (e.g., from imperfect human annotation or from unsupervised saliency prediction methods). With this goal, we address a natural question: can we learn saliency prediction while identifying clean labels in a unified framework? To answer this question, we call on the theory of robust model fitting and formulate deep saliency prediction from a single noisy labelling as robust network learning and exploit model consistency across iterations to identify inliers and outliers (i.e., noisy labels). Extensive experiments on different benchmark datasets demonstrate the superiority of our proposed framework, which can learn comparable saliency prediction with state-of-the-art fully supervised saliency methods. Furthermore, we show that simply by treating ground truth annotations as noisy labelling, our framework achieves tangible improvements over state-of-the-art methods."}}
