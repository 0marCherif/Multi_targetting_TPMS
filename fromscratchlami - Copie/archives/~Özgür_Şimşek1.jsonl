{"id": "cE9hYVZW6Cv", "cdate": 1672531200000, "mdate": 1682437208050, "content": {"title": "Resource-Constrained Station-Keeping for Helium Balloons using Reinforcement Learning", "abstract": "High altitude balloons have proved useful for ecological aerial surveys, atmospheric monitoring, and communication relays. However, due to weight and power constraints, there is a need to investigate alternate modes of propulsion to navigate in the stratosphere. Very recently, reinforcement learning has been proposed as a control scheme to maintain the balloon in the region of a fixed location, facilitated through diverse opposing wind-fields at different altitudes. Although air-pump based station keeping has been explored, there is no research on the control problem for venting and ballasting actuated balloons, which is commonly used as a low-cost alternative. We show how reinforcement learning can be used for this type of balloon. Specifically, we use the soft actor-critic algorithm, which on average is able to station-keep within 50\\;km for 25\\% of the flight, consistent with state-of-the-art. Furthermore, we show that the proposed controller effectively minimises the consumption of resources, thereby supporting long duration flights. We frame the controller as a continuous control reinforcement learning problem, which allows for a more diverse range of trajectories, as opposed to current state-of-the-art work, which uses discrete action spaces. Furthermore, through continuous control, we can make use of larger ascent rates which are not possible using air-pumps. The desired ascent-rate is decoupled into desired altitude and time-factor to provide a more transparent policy, compared to low-level control commands used in previous works. Finally, by applying the equations of motion, we establish appropriate thresholds for venting and ballasting to prevent the agent from exploiting the environment. More specifically, we ensure actions are physically feasible by enforcing constraints on venting and ballasting."}}
{"id": "fR-etXLGEw", "cdate": 1609459200000, "mdate": 1682437208043, "content": {"title": "RL4HCI: Reinforcement Learning for Humans, Computers, and Interaction", "abstract": ""}}
{"id": "Zeu-YGxK882", "cdate": 1577836800000, "mdate": 1682437208106, "content": {"title": "Fast and frugal heuristics for portfolio decisions with positive project interactions", "abstract": ""}}
{"id": "K97MsQz7MAg", "cdate": 1546300800000, "mdate": null, "content": {"title": "Iterative Policy-Space Expansion in Reinforcement Learning", "abstract": "Humans and animals solve a difficult problem much more easily when they are presented with a sequence of problems that starts simple and slowly increases in difficulty. We explore this idea in the context of reinforcement learning. Rather than providing the agent with an externally provided curriculum of progressively more difficult tasks, the agent solves a single task utilizing a decreasingly constrained policy space. The algorithm we propose first learns to categorize features into positive and negative before gradually learning a more refined policy. Experimental results in Tetris demonstrate superior learning rate of our approach when compared to existing algorithms."}}
{"id": "5vt04iySQv", "cdate": 1546300800000, "mdate": null, "content": {"title": "The Game of Tetris in Machine Learning", "abstract": "The game of Tetris is an important benchmark for research in artificial intelligence and machine learning. This paper provides a historical account of the algorithmic developments in Tetris and discusses open challenges. Handcrafted controllers, genetic algorithms, and reinforcement learning have all contributed to good solutions. However, existing solutions fall far short of what can be achieved by expert players playing without time pressure. Further study of the game has the potential to contribute to important areas of research, including feature discovery, autonomous learning of action hierarchies, and sample-efficient reinforcement learning."}}
{"id": "4OTzmHracB", "cdate": 1546300800000, "mdate": null, "content": {"title": "Regularization in directable environments with application to Tetris", "abstract": "Learning from small data sets is difficult in the absence of specific domain knowledge. We present a regularized linear model called STEW that benefits from a generic and prevalent form of prior kn..."}}
{"id": "dJv-HYP7wIE", "cdate": 1451606400000, "mdate": null, "content": {"title": "Simple Regression Models", "abstract": "Developing theories of when and why simple predictive models perform well is a key step in understanding decisions of cognitively bounded humans and intelligent machines. We are interested in how well simple models predict in regression. We list and review existing simple regression models and define new ones. We identify the lack of a large-scale empirical comparison of these models with state-of-the-art regression models in a predictive regression context. We report the results of such an empirical analysis on 60 real-world data sets. Simple regression models such as equal-weights regression routinely outperformed state-of-the-art regression models, especially on small training-set sizes. There was no simple model that predicted well in all data sets, but in nearly all data sets, there was at least one simple model that predicted well. The supplementary material contains learning curves for individual data sets that have not been presented in the main article. It also contains detailed descriptions and source descriptions of all used data sets."}}
{"id": "cCHuy4ILLMJ", "cdate": 1451606400000, "mdate": null, "content": {"title": "On Learning Decision Heuristics", "abstract": "Decision heuristics are simple models of human and animal decision making that use few pieces of information and combine the pieces in simple ways, for example, by giving them equal weight or by considering them sequentially. We examine how decision heuristics can be learned\u2014and modified\u2014as additional training examples become available. In particular, we examine how additional training examples change the variance in parameter estimates of the heuristic. Our analysis suggests new decision heuristics, including a family of heuristics that generalizes two well-known families: lexicographic heuristics and tallying. We evaluate the empirical performance of these heuristics in a large, diverse collection of data sets. The supplementary material provides details on the random forest implementation and describes the 56 public data sets used in the empirical analysis."}}
{"id": "7Rx9UBnUlKe", "cdate": 1451606400000, "mdate": null, "content": {"title": "Why Most Decisions Are Easy in Tetris - And Perhaps in Other Sequential Decision Problems, As Well", "abstract": "We examined the sequence of decision problems that are encountered in the game of Tetris and found that most of the problems are easy in the following sense: One can choose well among the available..."}}
{"id": "7N363VrKPD", "cdate": 1451606400000, "mdate": null, "content": {"title": "Decision Heuristics for Comparison: How Good Are They?", "abstract": "Simple decision heuristics are cognitive models of human and animal decision making. They examine few pieces of information and combine the pieces in simple ways, for example, by considering them sequentially or giving them equal weight. They have been studied most extensively for the problem of \\textitcomparison, where the objective is to identify which of a given number of alternatives has the highest value on a specified (unobserved) criterion. We present the most comprehensive empirical evaluation of decision heuristics to date on the comparison problem. In a diverse collection of 56 real-world data sets, we compared heuristics to powerful statistical learning methods, including support vector machines and random forests. Heuristics performed surprisingly well. On average, they were only a few percentage points behind the best-performing algorithm. In many data sets, they yielded the highest accuracy in all or parts of the learning curve. The first part of the supplement describes implementation details of the algorithms tested; the second part describes the 56 public data sets used in the empirical analysis."}}
