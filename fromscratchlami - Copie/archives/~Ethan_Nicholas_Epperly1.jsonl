{"id": "jOTt0xNCSL", "cdate": 1672531200000, "mdate": 1683903167187, "content": {"title": "Robust, randomized preconditioning for kernel ridge regression", "abstract": "This paper introduces two randomized preconditioning techniques for robustly solving kernel ridge regression (KRR) problems with a medium to large number of data points ($10^4 \\leq N \\leq 10^7$). The first method, RPCholesky preconditioning, is capable of accurately solving the full-data KRR problem in $O(N^2)$ arithmetic operations, assuming sufficiently rapid polynomial decay of the kernel matrix eigenvalues. The second method, KRILL preconditioning, offers an accurate solution to a restricted version of the KRR problem involving $k \\ll N$ selected data centers at a cost of $O((N + k^2) k \\log k)$ operations. The proposed methods solve a broad range of KRR problems and overcome the failure modes of previous KRR preconditioners, making them ideal for practical applications."}}
{"id": "KEdO8Y1m39", "cdate": 1672531200000, "mdate": 1683903167151, "content": {"title": "XTrace: Making the most of every sample in stochastic trace estimation", "abstract": "The implicit trace estimation problem asks for an approximation of the trace of a square matrix, accessed via matrix-vector products (matvecs). This paper designs new randomized algorithms, XTrace and XNysTrace, for the trace estimation problem by exploiting both variance reduction and the exchangeability principle. For a fixed budget of matvecs, numerical experiments show that the new methods can achieve errors that are orders of magnitude smaller than existing algorithms, such as the Girard-Hutchinson estimator or the Hutch++ estimator. A theoretical analysis confirms the benefits by offering a precise description of the performance of these algorithms as a function of the spectrum of the input matrix. The paper also develops an exchangeable estimator, XDiag, for approximating the diagonal of a square matrix using matvecs."}}
{"id": "aOAZG3TgwC", "cdate": 1661990400000, "mdate": 1683903167170, "content": {"title": "A Theory of Quantum Subspace Diagonalization", "abstract": "Quantum subspace diagonalization methods are an exciting new class of algorithms for solving large-scale eigenvalue problems using quantum computers. Unfortunately, these methods require the solution of an ill-conditioned generalized eigenvalue problem, with a matrix pencil corrupted by a nonnegligible amount of noise that is far above the machine precision. Despite pessimistic predictions from classical worst-case perturbation theories, these methods can perform reliably well if the generalized eigenvalue problem is solved using a standard truncation strategy. By leveraging and advancing classical results in matrix perturbation theory, we provide a theoretical analysis of this surprising phenomenon, proving that under certain natural conditions, a quantum subspace diagonalization algorithm can accurately compute the smallest eigenvalue of a large Hermitian matrix. We give numerical experiments demonstrating the effectiveness of the theory and providing practical guidance for the choice of truncation level. Our new results can also be of independent interest to solving eigenvalue problems outside the context of quantum computation."}}
{"id": "vFujTQdaoOZ", "cdate": 1640995200000, "mdate": 1683903167200, "content": {"title": "Jackknife Variability Estimation For Randomized Matrix Computations", "abstract": "Randomized matrix algorithms have become workhorse tools in scientific computing and machine learning. To use these algorithms safely in applications, they should be coupled with posterior error estimates to assess the quality of the output. To meet this need, this paper proposes two diagnostics: a leave-one-out error estimator for randomized low-rank approximations and a jackknife resampling method to estimate the variance of the output of a randomized matrix computation. Both of these diagnostics are rapid to compute for randomized low-rank approximation algorithms such as the randomized SVD and Nystr\\\"om, and they provide useful information that can be used to assess the quality of the computed output and guide algorithmic parameter choices."}}
{"id": "LEeAuKo3h3", "cdate": 1640995200000, "mdate": 1683903167198, "content": {"title": "Randomly pivoted Cholesky: Practical approximation of a kernel matrix with few entry evaluations", "abstract": "Randomly pivoted Cholesky (RPCholesky) is a natural algorithm for computing a rank-k approximation of an N x N positive semidefinite (psd) matrix. RPCholesky can be implemented with just a few lines of code. It requires only (k+1)N entry evaluations and O(k^2 N) additional arithmetic operations. This paper offers the first serious investigation of its experimental and theoretical behavior. Empirically, RPCholesky matches or improves on the performance of alternative algorithms for low-rank psd approximation. Furthermore, RPCholesky provably achieves near-optimal approximation guarantees. The simplicity, effectiveness, and robustness of this algorithm strongly support its use in scientific computing and machine learning applications."}}
{"id": "9H-XuLwlCo", "cdate": 1640995200000, "mdate": 1683903167156, "content": {"title": "$(L_r, L_r, 1)$-Decompositions, Sparse Component Analysis, and the Blind Separation of Sums of Exponentials", "abstract": "We derive new uniqueness results for $(L_r,L_r,1)$-type block-term decompositions of third-order tensors by drawing connections to sparse component analysis. It is shown that our uniqueness results have a natural application in the context of the blind source separation problem, since they ensure uniqueness even among $(L_r,L_r,1)$-decompositions with incomparable rank profiles, allowing for stronger separation results for signals consisting of sums of exponentials in the presence of common poles among the source signals. As a byproduct, this line of ideas also suggests a new approach for computing $(L_r,L_r,1)$-decompositions, which proceeds by sequentially computing a canonical polyadic decomposition of the input tensor, followed by performing a sparse factorization on the third factor matrix."}}
{"id": "hY44I2KzkgN", "cdate": 1609459200000, "mdate": 1683903167192, "content": {"title": "Minimal Rank Completions for Overlapping Blocks", "abstract": "We consider the multi-objective optimization problem of choosing the bottom left block-entry of a block lower triangular matrix to minimize the ranks of all block sub-matrices. We provide a proof that there exists a simultaneous rank-minimizer by constructing the complete set of all minimizers."}}
{"id": "JY1jwmJ_YR", "cdate": 1609459200000, "mdate": 1683903167200, "content": {"title": "A Theory of Quantum Subspace Diagonalization", "abstract": "Quantum subspace diagonalization methods are an exciting new class of algorithms for solving large\\rev{-}scale eigenvalue problems using quantum computers. Unfortunately, these methods require the solution of an ill-conditioned generalized eigenvalue problem, with a matrix pair corrupted by a non-negligible amount of noise that is far above the machine precision. Despite pessimistic predictions from classical \\rev{worst-case} perturbation theories, these methods can perform reliably well if the generalized eigenvalue problem is solved using a standard truncation strategy. By leveraging and advancing classical results in matrix perturbation theory, we provide a theoretical analysis of this surprising phenomenon, proving that under certain natural conditions, a quantum subspace diagonalization algorithm can accurately compute the smallest eigenvalue of a large Hermitian matrix. We give numerical experiments demonstrating the effectiveness of the theory and providing practical guidance for the choice of truncation level. Our new results can also be of independent interest to solving eigenvalue problems outside the context of quantum computation."}}
{"id": "ZzEqTgQoWXT", "cdate": 1546300800000, "mdate": 1683903167203, "content": {"title": "Graph-Induced Rank Structures and their Representations", "abstract": "A new framework is proposed to study rank-structured matrices arising from discretizations of 2D and 3D elliptic operators. In particular, we introduce the notion of a graph-induced rank structure (GIRS) which aims to capture the fine low rank structures which appear in sparse matrices and their inverses in terms of the adjacency graph $\\mathbb{G}$. We show that the GIRS property is invariant under inversion, and hence any effective representation of the inverse of GIRS matrices would lead to effective solvers. Starting with the observation that sequentially semi-separable (SSS) matrices form a good candidate for representing GIRS matrices on the line graph, we propose two extensions of SSS matrices to arbitrary graphs: Dewilde-van der Veen (DV) representations and $\\mathbb{G}$-semi-separable ($\\mathbb{G}$-SS) representations. It is shown that both these representations come naturally equipped with fast solvers where the solve complexity is commensurate to fast sparse Gaussian elimination on the graph $\\mathbb{G}$, and $\\mathbb{G}$-SS representations have a linear time multiplication algorithm. We show the construction of these representations to be highly nontrivial by determining the minimal $\\mathbb{G}$-SS representation for the cycle graph $\\mathbb{G}$. To obtain a minimal representation, we solve an exotic variant of a low-rank completion problem."}}
