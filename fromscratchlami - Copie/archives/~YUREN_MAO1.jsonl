{"id": "v_IRly5Q4H", "cdate": 1672531200000, "mdate": 1682325062960, "content": {"title": "SparDL: Distributed Deep Learning Training with Efficient Sparse Communication", "abstract": "Top-$k$ sparsification has recently been widely used to reduce the communication volume in distributed deep learning; however, due to Gradient Accumulation (GA) dilemma, the performance of top-$k$ sparsification is still limited. Several methods have been proposed to handle the GA dilemma but have two drawbacks: (1) they are frustrated by the high communication complexity as they introduce a large amount of extra transmission; (2) they are not flexible for non-power-of-two numbers of workers. To solve these two problems, we propose a flexible and efficient sparse communication framework, dubbed SparDL. SparDL uses the Spar-Reduce-Scatter algorithm to solve the GA dilemma without additional communication operations and is flexible to any number of workers. Besides, to further reduce the communication complexity and adjust the proportion of latency and bandwidth cost in communication complexity, we propose the Spar-All-Gather algorithm as part of SparDL. Extensive experiments validate the superiority of SparDL."}}
{"id": "wUUutywJY6", "cdate": 1652737747942, "mdate": null, "content": {"title": "SoLar: Sinkhorn Label Refinery for Imbalanced Partial-Label Learning", "abstract": "Partial-label learning (PLL) is a peculiar weakly-supervised learning task where the training samples are generally associated with a set of candidate labels instead of single ground truth. While a variety of label disambiguation methods have been proposed in this domain, they normally assume a class-balanced scenario that may not hold in many real-world applications. Empirically, we observe degenerated performance of the prior methods when facing the combinatorial challenge from the long-tailed distribution and partial-labeling. In this work, we first identify the major reasons that the prior work failed. We subsequently propose SoLar, a novel Optimal Transport-based framework that allows to refine the disambiguated labels towards matching the marginal class prior distribution. SoLar additionally incorporates a new and systematic mechanism for estimating the long-tailed class prior distribution under the PLL setup. Through extensive experiments, SoLar exhibits substantially superior results on standardized benchmarks compared to the previous state-of-the-art PLL methods. Code and data are available at: https://github.com/hbzju/SoLar."}}
{"id": "7vmyjUHgm9_", "cdate": 1652737403179, "mdate": null, "content": {"title": "Less-forgetting Multi-lingual Fine-tuning", "abstract": "Multi-lingual fine-tuning (MLF), which fine-tunes a multi-lingual language model (MLLM) with multiple source languages, aims to gain good zero-shot performance on target languages. In MLF, the fine-tuned model tends to fit the source languages while forgetting its cross-lingual knowledge obtained from the pre-training stage. This forgetting phenomenon degenerates the zero-shot performance of MLF, which remains under-explored. To fill this gap, this paper proposes a multi-lingual fine-tuning method, dubbed Less-forgetting Multi-lingual Fine-tuning (LF-MLF). In LF-MLF, we cast multi-lingual fine-tuning as a constrained optimization problem, where the optimization objective is to minimize forgetting, and constraints are reducing the fine-tuning loss. The proposed method has superior zero-shot performance; furthermore, it can achieve the Pareto stationarity. Extensive experiments on Named Entity Recognition, Question Answering and Natural Language Inference back up our theoretical analysis and validate the superiority of our proposals."}}
{"id": "laWLYMYDEVe", "cdate": 1640995200000, "mdate": 1681585705282, "content": {"title": "MetaWeighting: Learning to Weight Tasks in Multi-Task Learning", "abstract": ""}}
{"id": "OchtAhRAQA", "cdate": 1640995200000, "mdate": 1668670058609, "content": {"title": "SoLar: Sinkhorn Label Refinery for Imbalanced Partial-Label Learning", "abstract": "Partial-label learning (PLL) is a peculiar weakly-supervised learning task where the training samples are generally associated with a set of candidate labels instead of single ground truth. While a variety of label disambiguation methods have been proposed in this domain, they normally assume a class-balanced scenario that may not hold in many real-world applications. Empirically, we observe degenerated performance of the prior methods when facing the combinatorial challenge from the long-tailed distribution and partial-labeling. In this work, we first identify the major reasons that the prior work failed. We subsequently propose SoLar, a novel Optimal Transport-based framework that allows to refine the disambiguated labels towards matching the marginal class prior distribution. SoLar additionally incorporates a new and systematic mechanism for estimating the long-tailed class prior distribution under the PLL setup. Through extensive experiments, SoLar exhibits substantially superior results on standardized benchmarks compared to the previous state-of-the-art PLL methods. Code and data are available at: https://github.com/hbzju/SoLar ."}}
{"id": "K9eCHluJKqm", "cdate": 1640995200000, "mdate": 1682325063079, "content": {"title": "PromptEM: Prompt-tuning for Low-resource Generalized Entity Matching", "abstract": "Entity Matching (EM), which aims to identify whether two entity records from two relational tables refer to the same real-world entity, is one of the fundamental problems in data management. Traditional EM assumes that two tables are homogeneous with the aligned schema, while it is common that entity records of different formats (e.g., relational, semi-structured, or textual types) involve in practical scenarios. It is not practical to unify their schemas due to the different formats. To support EM on format-different entity records, Generalized Entity Matching (GEM) has been proposed and gained much attention recently. To do GEM, existing methods typically perform in a supervised learning way, which relies on a large amount of high-quality labeled examples. However, the labeling process is extremely labor-intensive, and frustrates the use of GEM. Low-resource GEM, i.e., GEM that only requires a small number of labeled examples, becomes an urgent need. To this end, this paper, for the first time, focuses on the low-resource GEM and proposes a novel low-resource GEM method, termed as PromptEM. PromptEM has addressed three challenging issues (i.e., designing GEM-specific prompt-tuning, improving pseudo-labels quality, and running efficient self-training) in low-resource GEM. Extensive experimental results on eight real benchmarks demonstrate the superiority of PromptEM in terms of effectiveness and efficiency."}}
{"id": "17SvbK7ySDG", "cdate": 1640995200000, "mdate": 1682325062648, "content": {"title": "PromptEM: Prompt-tuning for Low-resource Generalized Entity Matching", "abstract": ""}}
{"id": "Xb8AjCh_rgq", "cdate": 1609459200000, "mdate": 1682325062606, "content": {"title": "Towards Improving Generalization of Multi-Task Learning", "abstract": "Multi-task Learning (MTL), which involves the simultaneous learning of multiple tasks, can achieve better performance than learning each task independently. It has achieved great success in various applications, ranging from computer vision to bioinformatics. However, involving multiple tasks in a single learning process is complicated, for both cooperation and competition exist across the including tasks; furthermore, the cooperation boosts the generalization of MTL while the competition degenerates it. There lacks of a systematic study on how to improve MTL's generalization by handling the cooperation and competition. This thesis systematically studies this problem and proposed four novel MTL methods to enhance the between-task cooperation or reduce the between-task competition. Specifically, for the between-task cooperation, adversarial multi-task representation learning (AMTRL) and semi-supervised multi-task learning (Semi-MTL) are studied; furthermore, a novel adaptive AMTRL method and a novel representation consistency regularization-based Semi-MTL method are proposed respectively. As to the between-task competition, this thesis analyzes the task variance and task imbalance; furthermore, a novel task variance regularization-based MTL method and a novel task-imbalance-aware MTL method are proposed respectively. The above proposed methods can improve the generalization of MTL and achieve state-of-the-art performance in real-word MTL applications."}}
{"id": "Ga8wPHD4YKt", "cdate": 1609459200000, "mdate": 1682325063097, "content": {"title": "Robust Deep Multi-task Learning Framework for Cancer Survival Analysis", "abstract": "Utilizing genomic characterizations and clinical information of patients across different cancer types, Multi-task Learning (MTL) based cancer survival analysis methods achieve good generalization performance and become prevalent. However, due to the high dimensionality and scarcity of the genomic sample set, as well as the inaccuracy of clinical data, the existing MTL-based survival analysis methods are limited to adopt the traditional statistical learning methods. The deep Multi-task Learning schema for cancer survival analysis has been less explored. To fill this gap, this paper proposes a novel robust deep MTL framework for cancer survival analysis. The proposed deep MTL framework is capable of handling low sample sized data with high dimension. Besides, the framework is robust to class-imbalance and task-imbalance of the cancer survival data. Experiments conducted on The Cancer Genome Atlas data sets validate the superiority of our deep MTL framework."}}
{"id": "BXpn6ryPpFh", "cdate": 1609459200000, "mdate": 1649848592166, "content": {"title": "BanditMTL: Bandit-based Multi-task Learning for Text Classification", "abstract": "Yuren Mao, Zekai Wang, Weiwei Liu, Xuemin Lin, Wenbin Hu. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021."}}
