{"id": "npAucdhIvib", "cdate": 1672531200000, "mdate": 1681542551426, "content": {"title": "SceneDreamer: Unbounded 3D Scene Generation from 2D Image Collections", "abstract": ""}}
{"id": "I1rN-_4oXXS", "cdate": 1672531200000, "mdate": 1681542551629, "content": {"title": "SparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthesis", "abstract": ""}}
{"id": "DiQl_A4aEn", "cdate": 1672531200000, "mdate": 1681542551669, "content": {"title": "SynBody: Synthetic Dataset with Layered Human Models for 3D Human Perception and Modeling", "abstract": ""}}
{"id": "g7U9jD_2CUr", "cdate": 1663849856847, "mdate": null, "content": {"title": "EVA3D: Compositional 3D Human Generation from 2D Image Collections", "abstract": "Inverse graphics aims to recover 3D models from 2D observations. Utilizing differentiable rendering, recent 3D-aware generative models have shown impressive results of rigid object generation using 2D images. However, it remains challenging to generate articulated objects, like human bodies, due to their complexity and diversity in poses and appearances. In this work, we propose, EVA3D, an unconditional 3D human generative model learned from 2D image collections only. EVA3D can sample 3D humans with detailed geometry and render high-quality images (up to 512x256) without bells and whistles (e.g. super resolution). At the core of EVA3D is a compositional human NeRF representation, which divides the human body into local parts. Each part is represented by an individual volume. This compositional representation enables 1) inherent human priors, 2) adaptive allocation of network parameters, 3) efficient training and rendering. Moreover, to accommodate for the characteristics of sparse 2D human image collections (e.g. imbalanced pose distribution), we propose a pose-guided sampling strategy for better GAN learning. Extensive experiments validate that EVA3D achieves state-of-the-art 3D human generation performance regarding both geometry and texture quality. Notably, EVA3D demonstrates great potential and scalability to \"inverse-graphics\" diverse human bodies with a clean framework."}}
{"id": "Z8OVyGZhjBj", "cdate": 1640995200000, "mdate": 1668694571975, "content": {"title": "Relighting4D: Neural Relightable Human from Videos", "abstract": "Human relighting is a highly desirable yet challenging task. Existing works either require expensive one-light-at-a-time (OLAT) captured data using light stage or cannot freely change the viewpoints of the rendered body. In this work, we propose a principled framework, Relighting4D, that enables free-viewpoints relighting from only human videos under unknown illuminations. Our key insight is that the space-time varying geometry and reflectance of the human body can be decomposed as a set of neural fields of normal, occlusion, diffuse, and specular maps. These neural fields are further integrated into reflectance-aware physically based rendering, where each vertex in the neural field absorbs and reflects the light from the environment. The whole framework can be learned from videos in a self-supervised manner, with physically informed priors designed for regularization. Extensive experiments on both real and synthetic datasets demonstrate that our framework is capable of relighting dynamic human actors with free-viewpoints. Codes are available at https://github.com/FrozenBurning/Relighting4D ."}}
{"id": "NH_6zY5nSx", "cdate": 1640995200000, "mdate": 1681542551661, "content": {"title": "Text2Light: Zero-Shot Text-Driven HDR Panorama Generation", "abstract": ""}}
{"id": "EsB5FuZV9hP", "cdate": 1640995200000, "mdate": 1668694571973, "content": {"title": "EVA3D: Compositional 3D Human Generation from 2D Image Collections", "abstract": "Inverse graphics aims to recover 3D models from 2D observations. Utilizing differentiable rendering, recent 3D-aware generative models have shown impressive results of rigid object generation using 2D images. However, it remains challenging to generate articulated objects, like human bodies, due to their complexity and diversity in poses and appearances. In this work, we propose, EVA3D, an unconditional 3D human generative model learned from 2D image collections only. EVA3D can sample 3D humans with detailed geometry and render high-quality images (up to 512x256) without bells and whistles (e.g. super resolution). At the core of EVA3D is a compositional human NeRF representation, which divides the human body into local parts. Each part is represented by an individual volume. This compositional representation enables 1) inherent human priors, 2) adaptive allocation of network parameters, 3) efficient training and rendering. Moreover, to accommodate for the characteristics of sparse 2D human image collections (e.g. imbalanced pose distribution), we propose a pose-guided sampling strategy for better GAN learning. Extensive experiments validate that EVA3D achieves state-of-the-art 3D human generation performance regarding both geometry and texture quality. Notably, EVA3D demonstrates great potential and scalability to \"inverse-graphics\" diverse human bodies with a clean framework."}}
{"id": "mAUj-dYgDb", "cdate": 1609459200000, "mdate": 1666321863415, "content": {"title": "Adaptive Focus for Efficient Video Recognition", "abstract": "In this paper, we explore the spatial redundancy in video recognition with the aim to improve the computational efficiency. It is observed that the most informative region in each frame of a video is usually a small image patch, which shifts smoothly across frames. Therefore, we model the patch localization problem as a sequential decision task, and propose a reinforcement learning based approach for efficient spatially adaptive video recognition (AdaFocus). In specific, a light-weighted ConvNet is first adopted to quickly process the full video sequence, whose features are used by a recurrent policy network to localize the most task-relevant regions. Then the selected patches are inferred by a high-capacity network for the final prediction. During offline inference, once the informative patch sequence has been generated, the bulk of computation can be done in parallel, and is efficient on modern GPU devices. In addition, we demonstrate that the proposed method can be easily extended by further considering the temporal redundancy, e.g., dynamically skipping less valuable frames. Extensive experiments on five benchmark datasets, i.e., ActivityNet, FCVID, MiniKinetics, Something-Something V1&V2, demonstrate that our method is significantly more efficient than the competitive baselines. Code is available at https://github.com/blackfeather-wang/AdaFocus."}}
