{"id": "8g0-YQZgXU", "cdate": 1698743638395, "mdate": 1698743638395, "content": {"title": "S3Aug: Segmentation, Sampling, and Shift for Action Recognition", "abstract": "Action recognition is a well-established area of research in computer vision. In this paper, we propose S3Aug, a video data augmenatation for action recognition. Unlike conventional video data augmentation methods that involve cutting and pasting regions from two videos, the proposed method generates new videos from a single training video through segmentation and label-to-image transformation. Furthermore, the proposed method modifies certain categories of label images by sampling to generate a variety of videos, and shifts intermediate features to enhance the temporal coherency between frames of the generate videos. Experimental results on the UCF101, HMDB51, and Mimetics datasets demonstrate the effectiveness of the proposed method, paricularlly for out-of-context videos of the Mimetics dataset."}}
{"id": "XSDpZOqB9Bw", "cdate": 1677628800000, "mdate": 1695971776183, "content": {"title": "Object-ABN: Learning to Generate Sharp Attention Maps for Action Recognition", "abstract": ""}}
{"id": "LyHEuLy_tvC", "cdate": 1672531200000, "mdate": 1695971776167, "content": {"title": "Joint learning of images and videos with a single Vision Transformer", "abstract": "In this study, we propose a method for jointly learning of images and videos using a single model. In general, images and videos are often trained by separate models. We propose in this paper a method that takes a batch of images as input to Vision Transformer IV-ViT, and also a set of video frames with temporal aggregation by late fusion. Experimental results on two image datasets and two action recognition datasets are presented."}}
{"id": "IYf0aEYxrK", "cdate": 1672531200000, "mdate": 1695971776155, "content": {"title": "Joint learning of images and videos with a single Vision Transformer", "abstract": "In this study, we propose a method for jointly learning of images and videos using a single model. In general, images and videos are often trained by separate models. We propose in this paper a method that takes a batch of images as input to Vision Transformer (IV-ViT), and also a set of video frames with temporal aggregation by late fusion. Experimental results on two image datasets and two action recognition datasets are presented."}}
{"id": "UX2E6kfbQVt", "cdate": 1669852800000, "mdate": 1695971776142, "content": {"title": "Model-Agnostic Multi-Domain Learning with Domain-Specific Adapters for Action Recognition", "abstract": ""}}
{"id": "n8fYa0JN9tL", "cdate": 1668036325707, "mdate": null, "content": {"title": "Weakly supervised temporal action localization  with additional sub networks for local spatial information", "abstract": "In this paper, we propose a weakly supervised learning model for temporal action localization using spatial information. Our proposed model extends STPN by adding sub networks inspired by the patch classification branches. While STPN uses only feature vectors based on 3D CNN, the proposed model also uses frame-level features to reduce the false positives of actions. The effectiveness of the proposed model is demonstrated by experimental results with quantitative and qualitative comparisons to the STPN model."}}
{"id": "F6awJ1TxhMu", "cdate": 1668036247931, "mdate": null, "content": {"title": "p-TSM: Learning to shift temporal features point-wise for action recognition", "abstract": "We propose a method of temporal shifting of features for video image recognition. Compared to the TSM module whose shifting channels are fixed and spatially uniform, the amount of temporal shift is learnt and spatially variable. The effectiveness of the proposed method is demonstrated by comparing the network model with the proposed shift module with the conventional method."}}
{"id": "BlutmJjOLy", "cdate": 1668036202210, "mdate": 1668036202210, "content": {"title": "ObjectMix: Data Augmentation by Copy-Pasting Objects in Videos for Action Recognition", "abstract": "In this paper, we propose a data augmentation method for action recognition using instance segmentation. Although many data augmentation methods have been proposed for image recognition, few of them are tailored for action recognition. Our proposed method, ObjectMix, extracts each object region from two videos using instance segmentation and combines them to create new videos. Experiments on two action recognition datasets, UCF101 and HMDB51, demonstrate the effectiveness of the proposed method and show its superiority over VideoMix, a prior work.\n"}}
{"id": "7XHjNNc61s2", "cdate": 1668036113912, "mdate": null, "content": {"title": "Object-ABN: Learning to Generate Sharp Attention Maps for Action Recognition", "abstract": "In this paper we propose an extension of the Attention Branch Network (ABN) by using instance segmentation for generating sharper attention maps for action recognition. Methods for visual explanation such as Grad-CAM usually generate blurry maps which are not intuitive for humans to understand, particularly in recognizing actions of people in videos. Our proposed method, Object-ABN, tackles this issue by introducing a new mask loss that makes the generated attention maps close to the instance segmentation result. Further the Prototype Conformity (PC) loss and multiple attention maps are introduced to enhance the sharpness of the maps and improve the performance of classification. Experimental results with UCF101 and SSv2 shows that the generated maps by the proposed method are much clearer qualitatively and quantitatively than those of the original ABN."}}
{"id": "KElgFbMQ-UH", "cdate": 1668035924895, "mdate": 1668035924895, "content": {"title": "Model-agnostic Multi-Domain Learning with Domain-Specific Adapters for Action Recognition", "abstract": "In this paper, we propose a multi-domain learning model for action recognition. The proposed method inserts domain-specific adapters between layers of domain-independent layers of a backbone network. Unlike a multi-head network that switches classification heads only, our model switches not only the heads, but also the adapters for facilitating to learn feature representations universal to multiple domains. Unlike prior works, the proposed method is model-agnostic and doesn't assume model structures unlike prior works. Experimental results on three popular action recognition datasets (HMDB51, UCF101, and Kinetics-400) demonstrate that the proposed method is more effective than a multi-head architecture and more efficient than separately training models for each domain.\n"}}
