{"id": "d_kjqKFKcA9", "cdate": 1599355281758, "mdate": null, "content": {"title": "Non-negative Independent Factor Analysis for single cell RNA-seq", "abstract": "Single-cell RNA sequencing (scRNA-seq) datasets contain discrete types of cells as well as  continuous pathway activities; yet, currently there is no matrix factorization approach that enables the joint inference of both. We present a new  probabilistic single-cell factor analysis model, Non-negative Independent Factor Analysis (NIFA), that combines features of Principal Component Analysis (PCA), NMF, and ICA. NIFA simultaneously models uni- and multi-modal latent factors, and it isolates discrete cell-type identity and variation in continuous, pathway-level expression activity into separate components with non-negative loadings. We apply our approach to a range of datasets where cell-type identity is known, and we show that NIFA-derived factors outperform results from ICA, PCA, NMF and scCoGAPS (an NMF method designed for single-cell data) in terms of disentangling biological sources of variation.  Studying an immunotherapy dataset in detail, we show that NIFA is able to reproduce and refine previous findings  and enables the discovery of new clinically relevant cell states."}}
{"id": "S4Ql1gLnYq", "cdate": 1599355075148, "mdate": null, "content": {"title": "DataRemix: a universal data transformation for optimal inference from gene expression datasets", "abstract": "RNAseq technology provides unprecedented power in the assessment of the transcription abundance and can be used to perform a variety of downstream tasks such as inference of gene-correlation network and eQTL discovery. However, raw gene expression values have to be normalized for nuisance biological variation and technical covariates, and different normalization strategies can lead to dramatically different results in the downstream study. We describe a generalization of SVD-based reconstruction for which the common techniques of whitening, rank-k approximation, and removing the top k principle components are special cases. Our simple three-parameter transformation, DataRemix, can be tuned to reweight the contribution of hidden factors and reveal otherwise hidden biological signals. In particular, we demonstrate that the method can effectively prioritize biological signals over noise without leveraging external dataset-specific knowledge, and can outperform normalization methods that make explicit use of known technical factors. We also show that DataRemix can be efficiently optimized via Thompson Sampling approach, which makes it feasible for computationally expensive objectives such as eQTL analysis. Finally, we apply our method to the ROSMAP dataset and we report what to our knwoledge is the first replicable trans-eQTL effect in human brain.\n"}}
{"id": "MCWJ-kWVtsg", "cdate": 1599354594847, "mdate": null, "content": {"title": "Modeling Enhancer-Promoter Interactions with Attention-Based Neural Networks", "abstract": "Gene regulatory sequences play critical roles in ensuring tightly controlled RNA expression patterns that are essential in a large variety of biological processes. Specifically, enhancer sequences drive expression of their target genes, and the availability of genome-wide maps of enhancer-promoter interactions has opened up the possibility to use machine learning approaches to extract and interpret features that define these interactions in different biological contexts. Inspired by machine translation models we develop an attention-based neural network model, EPIANN, to predict enhancer-promoter interactions based on DNA sequences. Codes and data are available at https://github.com/wgmao/EPIANN. Our approach accurately predicts enhancer-promoter interactions across six cell lines. In addition, our method generates pairwise attention scores at the sequence level, which specify how short regions in the enhancer and promoter pair-up to drive the interaction prediction. This allows us to identify over-represented transcription factors (TF) binding sites and TF-pair interactions in the context of enhancer function."}}
{"id": "HXbsEGr-WiW", "cdate": 1599354414980, "mdate": null, "content": {"title": "Pathway-level information extractor (PLIER) for gene expression data", "abstract": "A major challenge in gene expression analysis is to accurately infer relevant biological insights, such as variation in cell-type proportion or pathway activity, from global gene expression studies. We present pathway-level information extractor (PLIER) (https://github.com/wgmao/PLIER and http://gobie.csb.pitt.edu/PLIER), a broadly applicable solution for this problem that outperforms available cell proportion inference algorithms and can automatically identify specific pathways that regulate gene expression. Our method improves inter-study replicability and reveals biological insights when applied to trans-eQTL (expression quantitative trait loci) identification."}}
{"id": "By9iRkWA-", "cdate": 1518730173981, "mdate": null, "content": {"title": "Phase Conductor on Multi-layered Attentions for Machine Comprehension", "abstract": "Attention models have been intensively studied to improve NLP tasks such as machine comprehension via both question-aware passage attention model and self-matching attention model. Our research proposes phase conductor (PhaseCond) for attention models in two meaningful ways. First, PhaseCond, an architecture of multi-layered attention models, consists of multiple phases each implementing a stack of attention layers producing passage representations and a stack of inner or outer fusion layers regulating the information flow.  Second, we extend and improve the dot-product attention function for PhaseCond by simultaneously encoding multiple question and passage embedding layers from different perspectives. We demonstrate the effectiveness of our proposed model PhaseCond on the SQuAD dataset, showing that our model significantly outperforms both state-of-the-art single-layered and multiple-layered attention models. We deepen our results with new findings via both detailed qualitative analysis and visualized examples showing the dynamic changes through multi-layered attention models."}}
{"id": "zw4wrBZbwn", "cdate": 1514764800000, "mdate": 1681673558483, "content": {"title": "An Incentive Mechanism for Crowd Sensing with Colluding Agents", "abstract": "Vehicular mobile crowd sensing is a fast-emerging paradigm to collect data about the environment by mounting sensors on vehicles such as taxis. An important problem in vehicular crowd sensing is to design payment mechanisms to incentivize drivers (agents) to collect data, with the overall goal of obtaining the maximum amount of data (across multiple vehicles) for a given budget. Past works on this problem consider a setting where each agent operates in isolation---an assumption which is frequently violated in practice. In this paper, we design an incentive mechanism to incentivize agents who can engage in arbitrary collusions. We then show that in a \"homogeneous\" setting, our mechanism is optimal, and can do as well as any mechanism which knows the agents' preferences a priori. Moreover, if the agents are non-colluding, then our mechanism automatically does as well as any other non-colluding mechanism. We also show that our proposed mechanism has strong (and asymptotically optimal) guarantees for a more general \"heterogeneous\" setting. Experiments based on synthesized data and real-world data reveal gains of over 30\\% attained by our mechanism compared to past literature."}}
{"id": "kQ3AiwGdcbH", "cdate": 1483228800000, "mdate": 1681673558483, "content": {"title": "Phase Conductor on Multi-layered Attentions for Machine Comprehension", "abstract": "Attention models have been intensively studied to improve NLP tasks such as machine comprehension via both question-aware passage attention model and self-matching attention model. Our research proposes phase conductor (PhaseCond) for attention models in two meaningful ways. First, PhaseCond, an architecture of multi-layered attention models, consists of multiple phases each implementing a stack of attention layers producing passage representations and a stack of inner or outer fusion layers regulating the information flow. Second, we extend and improve the dot-product attention function for PhaseCond by simultaneously encoding multiple question and passage embedding layers from different perspectives. We demonstrate the effectiveness of our proposed model PhaseCond on the SQuAD dataset, showing that our model significantly outperforms both state-of-the-art single-layered and multiple-layered attention models. We deepen our results with new findings via both detailed qualitative analysis and visualized examples showing the dynamic changes through multi-layered attention models."}}
