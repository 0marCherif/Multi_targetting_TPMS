{"id": "OXXyRROGiLF", "cdate": 1661572372231, "mdate": 1661572372231, "content": {"title": "Generalized Few-shot Semantic Segmentation", "abstract": "Training semantic segmentation models requires a large amount of finely annotated data, making it hard to quickly adapt to novel classes not satisfying this condition. Few-Shot Segmentation (FS-Seg) tackles this problem with many constraints. In this paper, we introduce a new benchmark, called Generalized Few-Shot Semantic Segmentation (GFS-Seg), to analyze the generalization ability of simultaneously segmenting the novel categories with very few examples and the base categories with sufficient examples. It is the first study showing that previous representative state-of-the-art FS-Seg methods fall short in GFS-Seg and the performance discrepancy mainly comes from the constrained setting of FS-Seg. To make GFS-Seg tractable, we set up a GFS-Seg baseline that achieves decent performance without structural change on the original model. Then, since context is essential for semantic segmentation, we propose the Context-Aware Prototype Learning (CAPL) that significantly improves performance by 1) leveraging the co-occurrence prior knowledge from support samples, and 2) dynamically enriching contextual information to the classifier, conditioned on the content of each query image. Both two contributions are experimentally shown to have substantial practical merit. Extensive experiments on Pascal-VOC and COCO manifest the effectiveness of CAPL, and CAPL generalizes well to FS-Seg by achieving competitive performance"}}
{"id": "QqWqFLbllZh", "cdate": 1652737326896, "mdate": null, "content": {"title": "Spatial Pruned Sparse Convolution for Efficient 3D Object Detection", "abstract": "3D scenes are dominated by a large number of background points, which is redundant for the detection task that mainly needs to focus on foreground objects. In this paper, we analyze major components of existing sparse 3D CNNs and find that 3D CNNs ignores the redundancy of data and further amplifies it in the down-sampling process, which brings a huge amount of extra and unnecessary computational overhead. Inspired by this, we propose a new convolution operator named spatial pruned sparse convolution (SPS-Conv), which includes two variants, spatial pruned submanifold sparse convolution (SPSS-Conv) and spatial pruned regular sparse convolution (SPRS-Conv), both of which are based on the idea of dynamically determine crucial areas for performing computations to reduce redundancy. We empirically find that magnitude of features can serve as an important cues to determine crucial areas which get rid of the heavy computations of learning-based methods. The proposed modules can easily be incorporated into existing sparse  3D CNNs without extra architectural modifications. Extensive experiments on the KITTI and nuScenes datasets demonstrate that our method can achieve more than 50% reduction in GFLOPs without compromising the performance."}}
{"id": "m648UoqQ2Cj", "cdate": 1630048844004, "mdate": 1630048844004, "content": {"title": "Prior Guided Feature Enrichment Network for Few-Shot Segmentation", "abstract": "State-of-the-art semantic segmentation methods require sufficient labeled data to achieve good results and hard work on\nunseen classes without fine-tuning. Few-shot segmentation is thus proposed to tackle this problem by learning a model that quickly\nadapts to new classes with a few labeled support samples. These frameworks still face the challenge of generalization ability\nreduction on unseen classes due to inappropriate use of high-level semantic information of training classes and spatial inconsistency\nbetween query and support targets. To alleviate these issues, we propose the Prior Guided Feature Enrichment Network (PFENet). It\nconsists of novel designs of (1) a training-free prior mask generation method that not only retains generalization power but also\nimproves model performance and (2) Feature Enrichment Module (FEM) that overcomes spatial inconsistency by adaptively enriching\nquery features with support features and prior masks. Extensive experiments on PASCAL-5i and COCO prove that the proposed prior\ngeneration method and FEM both improve the baseline method significantly. Our PFENet also outperforms state-of-the-art methods by\na large margin without efficiency loss. It is surprising that our model even generalizes to cases without labeled support samples. Our\ncode is available at https://github.com/Jia-Research-Lab/PFENet/.\n"}}
{"id": "rslgVgWXx_6B", "cdate": 1546300800000, "mdate": null, "content": {"title": "Homomorphic Latent Space Interpolation for Unpaired Image-To-Image Translation.", "abstract": "Generative adversarial networks have achieved great success in unpaired image-to-image translation. Cycle consistency allows modeling the relationship between two distinct domains without paired data. In this paper, we propose an alternative framework, as an extension of latent space interpolation, to consider the intermediate region between two domains during translation. It is based on the fact that in a flat and smooth latent space, there exist many paths that connect two sample points. Properly selecting paths makes it possible to change only certain image attributes, which is useful for generating intermediate images between the two domains. We also show that this framework can be applied to multi-domain and multi-modal translation. Extensive experiments manifest its generality and applicability to various tasks."}}
{"id": "rsUWyNQeO6S", "cdate": 1546300800000, "mdate": null, "content": {"title": "Learning Shape-Aware Embedding for Scene Text Detection.", "abstract": "We address the problem of detecting scene text in arbitrary shapes, which is a challenging task due to the high variety and complexity of the scene. Specifically, we treat text detection as instance segmentation and propose a segmentation-based framework, which extracts each text instance as an independent connected component. To distinguish different text instances, our method maps pixels onto an embedding space where pixels belonging to the same text are encouraged to appear closer to each other and vise versa. In addition, we introduce a Shape-Aware Loss to make training adaptively accommodate various aspect ratios of text instances and the tiny gaps among them, and a new post-processing pipeline to yield precise bounding box predictions. Experimental results on three challenging datasets (ICDAR15, MSRA-TD500 and CTW1500) demonstrate the effectiveness of our work."}}
