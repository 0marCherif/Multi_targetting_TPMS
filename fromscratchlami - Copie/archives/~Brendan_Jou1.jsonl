{"id": "VtCMaH54AEp", "cdate": 1680361032050, "mdate": 1680361032050, "content": {"title": "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers", "abstract": "This technical report presents the modeling approaches used in our submission to the ICML Expressive Vocalizations Workshop & Competition multitask track (ExVo-MultiTask). We first applied image classification models of various sizes on mel-spectrogram representations of the vocal bursts, as is standard in sound event detection literature. Results from these models show an increase of 21.24% over the baseline system with respect to the harmonic mean of the task metrics, and comprise our team's main submission to the MultiTask track. We then sought to characterize the headroom in the MultiTask track by applying a large pre-trained Conformer model that previously achieved state-of-the-art results on paralinguistic tasks like speech emotion recognition and mask detection. We additionally investigated the relationship between the sub-tasks of emotional expression, country of origin, and age prediction, and discovered that the best performing models are trained as single-task models, questioning whether the problem truly benefits from a multitask setting."}}
{"id": "2CRJeoAjva", "cdate": 1680360830060, "mdate": 1680360830060, "content": {"title": "Federated Learning for Affective Computing Tasks", "abstract": "Federated learning mitigates the need to store user data in a central datastore for machine learning tasks, and is particularly beneficial when working with sensitive user data or tasks. Although successfully used for applications such as improving keyboard query suggestions, it is not studied systematically for modeling affective computing tasks which are often laden with subjective labels and high variability across individuals/raters or even by the same participant. In this paper, we study the federated averaging algorithm FedAvg to model self-reported emotional experience and perception labels on a variety of speech, video and text datasets. We identify two learning paradigms that commonly arise in affective computing tasks: modeling of self-reports (user-as-client), and modeling perceptual judgments such as labeling sentiment of online comments (rater-as-client). In the user-as-client setting, we show that FedAvg generally performs on-par with a non-federated model in classifying self-reports. In the rater-as-client setting, FedAvg consistently performed poorer than its non-federated counterpart. We found that the performance of FedAvg degraded for classes where the inter-rater agreement was moderate to low. To address this finding, we propose an algorithm FedRater that learns client-specific label distributions in federated settings. Our experimental results show that FedRater not only improves the overall classification performance compared to FedAvg but also provides insights for estimating proxies of inter-rater agreement in distributed settings."}}
{"id": "qY79G8jGsep", "cdate": 1632875734869, "mdate": null, "content": {"title": "DISSECT: Disentangled Simultaneous Explanations via Concept Traversals", "abstract": "Explaining deep learning model inferences is a promising venue for scientific understanding, improving safety, uncovering hidden biases, evaluating fairness, and beyond, as argued by many scholars. One of the principal benefits of counterfactual explanations is allowing users to explore \"what-if\" scenarios through what does not and cannot exist in the data, a quality that many other forms of explanation such as heatmaps and influence functions are inherently incapable of doing. However, most previous work on generative explainability cannot disentangle important concepts effectively, produces unrealistic examples, or fails to retain relevant information. We propose a novel approach, DISSECT, that jointly trains a generator, a discriminator, and a concept disentangler to overcome such challenges using little supervision. DISSECT generates Concept Traversals (CTs), defined as a sequence of generated examples with increasing degrees of concepts that influence a classifier's decision. By training a generative model from a classifier's signal, DISSECT offers a way to discover a classifier's inherent \"notion\" of distinct concepts automatically rather than rely on user-predefined concepts. We show that DISSECT produces CTs that (1) disentangle several concepts, (2) are influential to a classifier's decision and are coupled to its reasoning due to joint training (3), are realistic, (4) preserve relevant information, and (5) are stable across similar inputs. We validate DISSECT on several challenging synthetic and realistic datasets where previous methods fall short of satisfying desirable criteria for interpretability and show that it performs consistently well. Finally, we present experiments showing applications of DISSECT for detecting potential biases of a classifier and identifying spurious artifacts that impact predictions."}}
{"id": "H2WMqiaFZbJ", "cdate": 1620493957415, "mdate": null, "content": {"title": "Going Deeper for Multilingual Visual Sentiment Detection", "abstract": "This technical report details several improvements to the visual concept detector banks built on images from the Multilingual Visual Sentiment Ontology (MVSO). The detector banks are trained to detect a total of 9,918 sentiment-biased visual concepts from six major languages: English, Spanish, Italian, French, German and Chinese. In the original MVSO release, adjective-noun pair (ANP) detectors were trained for the six languages using an AlexNet-styled architecture by fine-tuning from DeepSentiBank. Here, through a more extensive set of experiments, parameter tuning, and training runs, we detail and release higher accuracy models for detecting ANPs across six languages from the same image pool and setting as in the original release using a more modern architecture, GoogLeNet, providing comparable or better performance with reduced network parameter cost.\n\nIn addition, since the image pool in MVSO can be corrupted by user noise from social interactions, we partitioned out a sub-corpus of MVSO images based on tag-restricted queries for higher fidelity labels. We show that as a result of these higher fidelity labels, higher performing AlexNet-styled ANP detectors can be trained using the tag-restricted image subset as compared to the models in full corpus. We release all these newly trained models for public research use along with the list of tag-restricted images from the MVSO dataset."}}
{"id": "M9I_INNOHWG", "cdate": 1620493908378, "mdate": null, "content": {"title": "Tamp: A Library for Compact Deep Neural Networks with Structured Matrices", "abstract": "We introduce Tamp, an open source C++ library for reducing the space and time costs of deep neural network models. In particular, Tamp implements several recent works which use structured matrices to replace unstructured matrices which are often bottlenecks in neural networks. Tamp is also designed to serve as a unified development platform with several supported optimization back-ends and abstracted data types. This paper introduces the design and API and also demonstrates the effectiveness with experiments on public datasets."}}
{"id": "x20Xi9DBfwr", "cdate": 1620493835175, "mdate": null, "content": {"title": "A survey of multimodal sentiment analysis", "abstract": "Sentiment analysis aims to automatically uncover the underlying attitude that we hold towards an entity. The aggregation of these sentiments over a population represents opinion polling and has numerous applications. Current text-based sentiment analysis relies on the construction of dictionaries and machine learning models that learn sentiment from large text corpora. Sentiment analysis from text is currently widely used for customer satisfaction assessment and brand perception analysis, among others. With the proliferation of social media, multimodal sentiment analysis is set to bring new opportunities with the arrival of complementary data streams for improving and going beyond text-based sentiment analysis. Since sentiment can be detected through affective traces it leaves, such as facial and vocal displays, multimodal sentiment analysis offers promising avenues for analyzing facial and vocal expressions in addition to the transcript or textual content. These approaches leverage emotion recognition and context inference to determine the underlying polarity and scope of an individual\u2019s sentiment. In this survey, we define sentiment and the problem of multimodal sentiment analysis and review recent developments in multimodal sentiment analysis in different domains, including spoken reviews, images, video blogs, human\u2013machine and human\u2013human interactions. Challenges and opportunities of this emerging field are also discussed, leading to our thesis that multimodal sentiment analysis holds a significant untapped potential."}}
{"id": "JPZd__44RL", "cdate": 1620491763815, "mdate": null, "content": {"title": "More cat than cute? Interpretable Prediction of Adjective-Noun Pairs", "abstract": "The increasing availability of affect-rich multimedia resources has bolstered interest in understanding sentiment and emotions in and from visual content. Adjective-noun pairs (ANP) are a popular mid-level semantic construct for capturing affect via visually detectable concepts such as \"cute dog\" or \"beautiful landscape\". Current state-of-the-art methods approach ANP prediction by considering each of these compound concepts as individual tokens, ignoring the underlying relationships in ANPs. This work aims at disentangling the contributions of the `adjectives' and `nouns' in the visual prediction of ANPs. Two specialised classifiers, one trained for detecting adjectives and another for nouns, are fused to predict 553 different ANPs. The resulting ANP prediction model is more interpretable as it allows us to study contributions of the adjective and noun components."}}
{"id": "xkqGYiu7WIT", "cdate": 1620491565357, "mdate": null, "content": {"title": "Sentiment concept embedding for visual affect recognition", "abstract": "Automatic sentiment and emotion understanding of general visual content has recently garnered much research attention. However, the large visual variance associated with high-level affective concepts presents a challenge when designing systems with high-performance requirements. One popular approach to bridge the \u201caffective gap\u201d between low-level visual features and affective semantics consists of using Adjective Noun Pair (ANP) semantic constructs for concepts, e.g. \u201cbeautiful landscape\u201d or \u201cscary face,\u201d which act as a mid-level representation that can be recognized by visual classifiers while still carrying an affective bias. In this work, we formulate the ANP detection task in images over a continuous space defined over an embedding that captures the inter-concept relationships between ANPs. We show how the compact representations obtained from the embedding extend the discrete concepts in the ontology and can be used for improved visual sentiment and emotion prediction, as well as new applications such as zero-shot ANP detection."}}
{"id": "SSFuTq511d1", "cdate": 1620491506589, "mdate": null, "content": {"title": "Uncertainty modeling in affective computing", "abstract": "This disclosure describes techniques that capture the uncertainty in machine-vision based affect (emotion) perception. The techniques are capable of predicting aleatoric, epistemic, and annotation uncertainty. Measures of uncertainty are important to safety-critical and subjective assessment tasks such as those found in the perception of affective expressions."}}
{"id": "2r1EMT8VcGv", "cdate": 1620491452828, "mdate": null, "content": {"title": "Sixteen facial expressions occur in similar contexts worldwide", "abstract": "Understanding the degree to which human facial expressions co-vary with specific social contexts across cultures is central to the theory that emotions enable adaptive responses to important challenges and opportunities. Concrete evidence linking social context to specific facial expressions is sparse and is largely based on survey-based approaches, which are often constrained by language and small sample sizes. Here, by applying machine-learning methods to real-world, dynamic behaviour, we ascertain whether naturalistic social contexts (for example, weddings or sporting competitions) are associated with specific facial expressions across different cultures. In two experiments using deep neural networks, we examined the extent to which 16 types of facial expression occurred systematically in thousands of contexts in 6 million videos from 144 countries. We found that each kind of facial expression had distinct associations with a set of contexts that were 70% preserved across 12 world regions. Consistent with these associations, regions varied in how frequently different facial expressions were produced as a function of which contexts were most salient. Our results reveal fine-grained patterns in human facial expressions that are preserved across the modern world."}}
