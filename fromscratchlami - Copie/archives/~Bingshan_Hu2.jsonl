{"id": "XfpmehHGo2", "cdate": 1676827100184, "mdate": null, "content": {"title": "Optimistic Thompson Sampling-based Algorithms for Episodic Reinforcement Learning", "abstract": "We propose two  Thompson Sampling-like, model-based learning algorithms for episodic Markov decision processes (MDPs) with a finite time horizon. Our proposed algorithms are inspired by Optimistic Thompson Sampling (O-TS), empirically studied in Chapelle and Li [2011], May et al. [2012] for stochastic multi-armed bandits. The key idea for the original O-TS is to clip the posterior distribution in an optimistic way to ensure that the sampled models are always better than the empirical models. Both of our proposed algorithms are easy to implement and only need one posterior sample to construct an episode-dependent model. Our first algorithm, Optimistic Thompson Sampling for MDPs (O-TS-MDP), achieves a $\\widetilde{O} \\left(\\sqrt{AS^2H^4T} \\right)$ regret bound, where $S$ is the size of the state space, $A$ is the size of the action space, $H$ is the number of time-steps per episode and $T$ is the number of episodes. Our second algorithm, Optimistic Thompson Sampling plus for MDPs (O-TS-MDP$^+$),  achieves the (near)-optimal $\\widetilde{O} \\left(\\sqrt{ASH^3T} \\right)$ regret bound by taking a more aggressive clipping strategy.  Since O-TS was only empirically studied previously, we derive regret bounds of O-TS for stochastic bandits. In addition, we propose,  O-TS-Bandit$^+$, a randomized version of UCB1 [Auer et al., 2002], for stochastic bandits. Both O-TS and O-TS-Bandit$^+$ achieve the optimal $O\\left(\\frac{A\\ln(T)}{\\Delta} \\right)$ problem-dependent regret bound, where $\\Delta$ denotes the sub-optimality gap."}}
{"id": "UShwCh8dEp", "cdate": 1672628674242, "mdate": 1672628674242, "content": {"title": "Problem-dependent regret bounds for online learning with feedback graphs", "abstract": "This paper addresses the stochastic multi-\narmed bandit problem with an undirected feed-\nback graph. We devise a UCB-based al-\ngorithm, UCB-NE, to provide a problem-\ndependent regret bound that depends on a\nclique covering. Our algorithm obtains re-\ngret which provably scales linearly with the\nclique covering number. Additionally, we pro-\nvide problem-dependent regret bounds for a\nThompson Sampling-based algorithm, TS-N,\nwhere again the bounds are linear in the clique\ncovering number. Finally, we present experi-\nmental results to see how UCB-NE, TS-N, and\na few related algorithms perform practically."}}
{"id": "Rx0cPoGEKR", "cdate": 1672531200000, "mdate": 1708541510751, "content": {"title": "Differentially Private Algorithms for Efficient Online Matroid Optimization", "abstract": "A matroid bandit is the online version of combinatorial optimization on a matroid, in which the learner chooses $K$ actions from a set of $L$ actions that can form a matroid basis. Many real-world ..."}}
{"id": "Bfzg8d8j9x5", "cdate": 1646077549530, "mdate": null, "content": {"title": "Near-Optimal Thompson Sampling-based Algorithms for Differentially Private Stochastic Bandits", "abstract": "We address differentially private stochastic bandits. We present two (near)-optimal  Thompson Sampling-based learning algorithms: DP-TS and Lazy-DP-TS. The core idea in achieving optimality  is  the principle of optimism in the face of uncertainty. We reshape the posterior distribution in an optimistic way as compared to the  non-private Thompson Sampling. Our DP-TS achieves a $\\sum\\limits_{j \\in \\mathcal{A}: \\Delta_j > 0} O \\left(\\frac{\\log(T)}{\\min \\left\\{\\epsilon, \\Delta_j \\right\\} )} \\log \\left(\\frac{\\log(T)}{\\epsilon \\cdot \\Delta_j} \\right) \\right)$ regret bound, where $\\mathcal{A}$ is the arm set, $\\Delta_j$ is the sub-optimality gap of a sub-optimal arm $j$, and $\\epsilon$ is the  privacy parameter. Our Lazy-DP-TS gets rid of the extra $\\log$ factor by using the idea of dropping observations. The regret of Lazy-DP-TS  is  $ \\sum\\limits_{j \\in \\mathcal{A}: \\Delta_j > 0} O \\left(\\frac{\\log(T)}{\\min \\left\\{\\epsilon, \\Delta_j \\right\\}} \\right)$, which matches the  regret lower bound. Additionally, we conduct experiments to compare the empirical performance of our proposed  algorithms with the existing optimal  algorithms for differentially private stochastic bandits."}}
{"id": "yvIhrSnrCf1", "cdate": 1640995200000, "mdate": 1688657887949, "content": {"title": "Near-optimal Thompson sampling-based algorithms for differentially private stochastic bandits", "abstract": "We address differentially private stochastic bandits. We present two (near)-optimal Thompson Sampling-based learning algorithms: DP-TS and Lazy-DP-TS. The core idea in achieving optimality is the principle of optimism in the face of uncertainty. We reshape the posterior distribution in an optimistic way as compared to the non-private Thompson Sampling. Our DP-TS achieves a $\\sum\\limits_{j \\in \\mathcal{A}: \\Delta_j > 0} O \\left(\\frac{\\log(T)}{\\min \\left\\{\\epsilon, \\Delta_j \\right\\} )} \\log \\left(\\frac{\\log(T)}{\\epsilon \\cdot \\Delta_j} \\right) \\right)$ regret bound, where $\\mathcal{A}$ is the arm set, $\\Delta_j$ is the sub-optimality gap of a sub-optimal arm $j$, and $\\epsilon$ is the privacy parameter. Our Lazy-DP-TS gets rid of the extra $\\log$ factor by using the idea of dropping observations. The regret of Lazy-DP-TS is $ \\sum\\limits_{j \\in \\mathcal{A}: \\Delta_j > 0} O \\left(\\frac{\\log(T)}{\\min \\left\\{\\epsilon, \\Delta_j \\right\\}} \\right)$, which matches the regret lower bound. Additionally, we conduct experiments to compare the empirical performance of our proposed algorithms with the existing optimal algorithms for differentially private stochastic bandits."}}
{"id": "zDbft0DjMSs", "cdate": 1609459200000, "mdate": null, "content": {"title": "Optimal Algorithms for Private Online Learning in a Stochastic Environment", "abstract": "We consider two variants of private stochastic online learning. The first variant is differentially private stochastic bandits. Previously, Sajed and Sheffet (2019) devised the DP Successive Elimination (DP-SE) algorithm that achieves the optimal $ O \\biggl(\\sum\\limits_{1\\le j \\le K: \\Delta_j >0} \\frac{ \\log T}{ \\Delta_j} + \\frac{ K\\log T}{\\epsilon} \\biggr)$ problem-dependent regret bound, where $K$ is the number of arms, $\\Delta_j$ is the mean reward gap of arm $j$, $T$ is the time horizon, and $\\epsilon$ is the required privacy parameter. However, like other elimination style algorithms, it is not an anytime algorithm. Until now, it was not known whether UCB-based algorithms could achieve this optimal regret bound. We present an anytime, UCB-based algorithm that achieves optimality. Our experiments show that the UCB-based algorithm is competitive with DP-SE. The second variant is the full information version of private stochastic online learning. Specifically, for the problems of decision-theoretic online learning with stochastic rewards, we present the first algorithm that achieves an $ O \\left( \\frac{ \\log K}{ \\Delta_{\\min}} + \\frac{ \\log K}{\\epsilon} \\right)$ regret bound, where $\\Delta_{\\min}$ is the minimum mean reward gap. The key idea behind our good theoretical guarantees in both settings is the forgetfulness, i.e., decisions are made based on a certain amount of newly obtained observations instead of all the observations obtained from the very beginning."}}
{"id": "XoiKIvVLTdB", "cdate": 1609459200000, "mdate": 1708541510752, "content": {"title": "Poster: Multi-agent Combinatorial Bandits with Moving Arms", "abstract": "In this paper, we study a distributed stochastic multi-armed bandit problem that can address many real-world problems such as task assignment for multiple crowdsourcing platforms, traffic scheduling in wireless networks with multiple access points and caching at cellular network edge. We propose an efficient algorithm called multi-agent combinatorial upper confidence bound (MACUCB) with provable performance guarantees and low communication overhead. Furthermore, we perform extensive experiments to show the effectiveness of the proposed algorithm."}}
{"id": "GGIziEPMDW", "cdate": 1609459200000, "mdate": 1708541510780, "content": {"title": "Caching by User Preference With Delayed Feedback for Heterogeneous Cellular Networks", "abstract": ""}}
{"id": "3R8xx-4pIEa", "cdate": 1609459200000, "mdate": 1708541510739, "content": {"title": "Bandit algorithms with graphical feedback models and privacy awareness", "abstract": ""}}
{"id": "QRtmRvK16ev", "cdate": 1577836800000, "mdate": null, "content": {"title": "Thompson Sampling for Combinatorial Semi-bandits with Sleeping Arms and Long-Term Fairness Constraints", "abstract": "We study the combinatorial sleeping multi-armed semi-bandit problem with long-term fairness constraints~(CSMAB-F). To address the problem, we adopt Thompson Sampling~(TS) to maximize the total rewards and use virtual queue techniques to handle the fairness constraints, and design an algorithm called \\emph{TS with beta priors and Bernoulli likelihoods for CSMAB-F~(TSCSF-B)}. Further, we prove TSCSF-B can satisfy the fairness constraints, and the time-averaged regret is upper bounded by $\\frac{N}{2\\eta} + O\\left(\\frac{\\sqrt{mNT\\ln T}}{T}\\right)$, where $N$ is the total number of arms, $m$ is the maximum number of arms that can be pulled simultaneously in each round~(the cardinality constraint) and $\\eta$ is the parameter trading off fairness for rewards. By relaxing the fairness constraints (i.e., let $\\eta \\rightarrow \\infty$), the bound boils down to the first problem-independent bound of TS algorithms for combinatorial sleeping multi-armed semi-bandit problems. Finally, we perform numerical experiments and use a high-rating movie recommendation application to show the effectiveness and efficiency of the proposed algorithm."}}
