{"id": "sgmDSzt-H2", "cdate": 1664310936969, "mdate": null, "content": {"title": "Diffusion Prior for Online Decision Making: A Case Study of Thompson Sampling", "abstract": "In this work, we investigate the possibility of using denoising diffusion models to learn priors for online decision making problems. Our special focus is on the meta-learning for bandit framework, with the goal of learning a strategy that performs well across bandit tasks of a same class. To this end, we train a diffusion model that learns the underlying task distribution and combine Thompson sampling with the learned prior to deal with new task at  test time. Our posterior sampling algorithm is designed to carefully balance between the learned prior and the noisy observations that come from the learner's interaction with the environment. Preliminary experiments clearly demonstrate the potential of the considered approach."}}
{"id": "TIPyxNbzeB8", "cdate": 1652737560779, "mdate": null, "content": {"title": "Uplifting Bandits", "abstract": "We introduce a new multi-armed bandit model where the reward is a sum of multiple random variables, and each action only alters the distributions of some of these variables. Upon taking an action, the agent observes the realizations of all variables. This model is motivated by marketing campaigns and recommender systems, where the variables represent outcomes on individual customers, such as clicks. We propose UCB-style algorithms that estimate the uplifts of the actions over a baseline. We study multiple variants of the problem, including when the baseline and affected variables are unknown, and prove sublinear regret bounds for all of these. In addition, we provide regret lower bounds that justify the necessity of our modeling assumptions. Experiments on synthetic and real-world datasets demonstrate the benefit of methods that estimate the uplifts over policies that do not use this structure.\n"}}
{"id": "dpYhDYjl4O", "cdate": 1652737469369, "mdate": null, "content": {"title": "No-regret learning in games with noisy feedback: Faster rates and adaptivity via learning rate separation", "abstract": "We examine the problem of regret minimization when the learner is involved in a continuous game with other optimizing agents: in this case, if all players follow a no-regret algorithm, it is possible to achieve significantly lower regret relative to fully adversarial environments. We study this problem in the context of variationally stable games (a class of continuous games which includes all convex-concave and monotone games), and when the players only have access to noisy estimates of their individual payoff gradients. If the noise is additive, the game-theoretic and purely adversarial settings enjoy similar regret guarantees; however, if the noise is \\emph{multiplicative}, we show that the learners can, in fact, achieve \\emph{constant} regret. We achieve this faster rate via an optimistic gradient scheme with \\emph{learning rate separation} \\textendash\\ that is, the method's extrapolation and update steps are tuned to different schedules, depending on the noise profile. Subsequently, to eliminate the need for delicate hyperparameter tuning, we propose a fully adaptive method that smoothly interpolates between worst- and best-case regret guarantees."}}
{"id": "EM4sLizlTeR", "cdate": 1622125814515, "mdate": null, "content": {"title": "Optimization in Open Networks via Dual Averaging", "abstract": "In networks of autonomous agents (e.g., fleets of vehicles, scattered sensors), the problem of minimizing the sum of the agents\u2019 local functions has received a lot of interest. We tackle here this distributed optimization problem in the case of open networks when agents can join and leave the network at any time. Leveraging recent online optimization techniques, we propose and analyze the convergence of a decentralized asynchronous optimization method for open networks."}}
{"id": "nz82BYsygEj", "cdate": 1622068937836, "mdate": null, "content": {"title": "Adaptive Learning in Continuous Games: Optimal Regret Bounds and Convergence to Nash Equilibrium", "abstract": "In game-theoretic learning, several agents are learning simultaneously following their individual\ninterests, so the environment is non-stationary from each player\u2019s perspective. In these circumstances,\nthe performance of a learning algorithm is often measured by its regret. However, no-regret\nalgorithms are not all created equal in terms of game-theoretic guarantees: depending on how they\nare tuned, some of them may drive the system to an equilibrium, while others could produce cyclic,\nchaotic, or otherwise divergent trajectories. To account for this, we propose a range of no-regret\npolicies based on optimistic mirror descent, with the following desirable properties: i) they do \u221a\nnot require any prior tuning or knowledge of the game; ii) they all achieve O( T ) regret against arbitrary, adversarial opponents; iii) they converge to the best response against convergent opponents; and, if employed by all players, iv) they guarantee O(1) social regret; and v) the induced sequence of play converges to Nash equilibrium while guaranteeing O(1) individual regret in all variationally stable games (a class of games that includes all monotone and convex-concave zero-sum games)."}}
{"id": "-OdCBsZFFBU", "cdate": 1609459200000, "mdate": null, "content": {"title": "Adaptive Learning in Continuous Games: Optimal Regret Bounds and Convergence to Nash Equilibrium", "abstract": "In game-theoretic learning, several agents are simultaneously following their individual interests, so the environment is non-stationary from each player's perspective. In this context, the performance of a learning algorithm is often measured by its regret. However, no-regret algorithms are not created equal in terms of game-theoretic guarantees: depending on how they are tuned, some of them may drive the system to an equilibrium, while others could produce cyclic, chaotic, or otherwise divergent trajectories. To account for this, we propose a range of no-regret policies based on optimistic mirror descent, with the following desirable properties: i) they do not require any prior tuning or knowledge of the game; ii) they all achieve O(\\sqrt{T}) regret against arbitrary, adversarial opponents; and iii) they converge to the best response against convergent opponents. Also, if employed by all players, then iv) they guarantee O(1) social regret; while v) the induced sequence of play converges to Nash equilibrium with O(1) individual regret in all variationally stable games (a class of games that includes all monotone and convex-concave zero-sum games)."}}
{"id": "x-d-f5zGBc", "cdate": 1577836800000, "mdate": null, "content": {"title": "Explore Aggressively, Update Conservatively: Stochastic Extragradient Methods with Variable Stepsize Scaling", "abstract": "Owing to their stability and convergence speed, extragradient methods have become a staple for solving large-scale saddle-point problems in machine learning. The basic premise of these algorithms is the use of an extrapolation step before performing an update; thanks to this exploration step, extra-gradient methods overcome many of the non-convergence issues that plague gradient descent/ascent schemes. On the other hand, as we show in this paper, running vanilla extragradient with stochastic gradients may jeopardize its convergence, even in simple bilinear models. To overcome this failure, we investigate a double stepsize extragradient algorithm where the exploration step evolves at a more aggressive time-scale compared to the update step. We show that this modification allows the method to converge even with stochastic gradients, and we derive sharp convergence rates under an error bound condition."}}
{"id": "VhquNaWyTT3", "cdate": 1577836800000, "mdate": null, "content": {"title": "Multi-Agent Online Optimization with Delays: Asynchronicity, Adaptivity, and Optimism", "abstract": "In this paper, we provide a general framework for studying multi-agent online learning problems in the presence of delays and asynchronicities. Specifically, we propose and analyze a class of adaptive dual averaging schemes in which agents only need to accumulate gradient feedback received from the whole system, without requiring any between-agent coordination. In the single-agent case, the adaptivity of the proposed method allows us to extend a range of existing results to problems with potentially unbounded delays between playing an action and receiving the corresponding feedback. In the multi-agent case, the situation is significantly more complicated because agents may not have access to a global clock to use as a reference point; to overcome this, we focus on the information that is available for producing each prediction rather than the actual delay associated with each feedback. This allows us to derive adaptive learning strategies with optimal regret bounds, even in a fully decentralized, asynchronous environment. Finally, we also analyze an \"optimistic\" variant of the proposed algorithm which is capable of exploiting the predictability of problems with a slower variation and leads to improved regret bounds."}}
{"id": "SJVSqj-_bB", "cdate": 1546300800000, "mdate": null, "content": {"title": "Classification from Positive, Unlabeled and Biased Negative Data", "abstract": "In binary classification, there are situations where negative (N) data are too diverse to be fully labeled and we often resort to positive-unlabeled (PU) learning in these scenarios. However, colle..."}}
{"id": "Jlw8QBMu_mw", "cdate": 1546300800000, "mdate": null, "content": {"title": "On the convergence of single-call stochastic extra-gradient methods", "abstract": "Variational inequalities have recently attracted considerable interest in machine learning as a flexible paradigm for models that go beyond ordinary loss function minimization (such as generative adversarial networks and related deep learning systems). In this setting, the optimal O(1/t) convergence rate for solving smooth monotone variational inequalities is achieved by the Extra-Gradient (EG) algorithm and its variants. Aiming to alleviate the cost of an extra gradient step per iteration (which can become quite substantial in deep learning), several algorithms have been proposed as surrogates to Extra-Gradient with a single oracle call per iteration. In this paper, we develop a synthetic view of such algorithms, and we complement the existing literature by showing that they retain a $O(1/t)$ ergodic convergence rate in smooth, deterministic problems. Subsequently, beyond the monotone deterministic case, we also show that the last iterate of single-call, stochastic extra-gradient methods still enjoys a $O(1/t)$ local convergence rate to solutions of non-monotone variational inequalities that satisfy a second-order sufficient condition."}}
