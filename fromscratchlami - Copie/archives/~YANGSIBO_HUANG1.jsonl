{"id": "21qf2LbB8e", "cdate": 1684258090431, "mdate": 1684258090431, "content": {"title": "A Dataset Auditing Method for Collaboratively Trained Machine Learning Models", "abstract": "Dataset auditing for machine learning (ML)\nmodels is a method to evaluate if a given dataset is used\nin training a model. In a Federated Learning setting where\nmultiple institutions collaboratively train a model with their\ndecentralized private datasets, dataset auditing can facilitate\nthe enforcement of regulations, which provide rules for\npreserving privacy, but also allow users to revoke authorizations\nand remove their data from collaboratively trained\nmodels. This paper first proposes a set of requirements for a\npractical dataset auditing method, and then present a novel\ndataset auditing method called Ensembled Membership Auditing\n(EMA). Its key idea is to leverage previously proposed\nMembership Inference Attack methods and to aggregate\ndata-wise membership scores using statistic testing to audit\na dataset for a ML model. We have experimentally evaluated\nthe proposed approach with benchmark datasets,\nas well as 4 X-ray datasets (CBIS-DDSM, COVIDx, Child-\nXRay, and CXR-NIH) and 3 dermatology datasets (DERM7pt,\nHAM10000, and PAD-UFES-20). Our results show that EMA\nmeet the requirements substantially better than the previous\nstate-of-the-art method."}}
{"id": "oWJPPMc07E", "cdate": 1672531200000, "mdate": 1683641438145, "content": {"title": "Matching-based Data Valuation for Generative Model", "abstract": "Data valuation is critical in machine learning, as it helps enhance model transparency and protect data properties. Existing data valuation methods have primarily focused on discriminative models, neglecting deep generative models that have recently gained considerable attention. Similar to discriminative models, there is an urgent need to assess data contributions in deep generative models as well. However, previous data valuation approaches mainly relied on discriminative model performance metrics and required model retraining. Consequently, they cannot be applied directly and efficiently to recent deep generative models, such as generative adversarial networks and diffusion models, in practice. To bridge this gap, we formulate the data valuation problem in generative models from a similarity-matching perspective. Specifically, we introduce Generative Model Valuator (GMValuator), the first model-agnostic approach for any generative models, designed to provide data valuation for generation tasks. We have conducted extensive experiments to demonstrate the effectiveness of the proposed method. To the best of their knowledge, GMValuator is the first work that offers a training-free, post-hoc data valuation strategy for deep generative models."}}
{"id": "YwaI-KFU66U", "cdate": 1672531200000, "mdate": 1682331381626, "content": {"title": "kNN-Adapter: Efficient Domain Adaptation for Black-Box Language Models", "abstract": "Fine-tuning a language model on a new domain is standard practice for domain adaptation. However, it can be infeasible when it comes to modern large-scale language models such as GPT-3, which can only be accessed through APIs, making it difficult to access the internal parameters of the model. In this paper, we propose $k$NN-Adapter, a method to effectively adapt these black-box large language models (LLMs) to a new domain. The $k$NN-Adapter builds on top of the retrieval-augmented language model, and adaptively learns to interpolate the output of the language model with retrieval results from a datastore consisting of the target domain data. Our experiments on four different domains demonstrate that $k$NN-Adapter significantly improves perplexity, and works particularly well in settings with limited access to LLMs. Additionally, we show that $k$NN-Adapter is more effective than fine-tuning when the amount of training data is limited. We also release a dataset to encourage further study."}}
{"id": "88ofSYGzye", "cdate": 1672531200000, "mdate": 1682347889962, "content": {"title": "Challenges towards the Next Frontier in Privacy", "abstract": "In July 2022, we organized a workshop (with the title Differential privacy (DP): Challenges towards the next frontier) with experts from industry, academia, and the public sector to seek answers to broad questions pertaining to privacy and its implications in the design of industry-grade systems. This document is the only public summary of the conversations from the workshop. There are two potential purposes of this document, which we envision: i) it serves as a standing reference for algorithmic/design decisions that are taken in the space of privacy, and ii) it provides guidance on future research directions. The document covers a broad array of topics, from infrastructure needs for designing private systems, to achieving better privacy/utility trade-offs, to conveying privacy guarantees to a broad audience. Finally, the document also looks at attacking and auditing these systems."}}
{"id": "dqgzfhHd2-", "cdate": 1652737760009, "mdate": null, "content": {"title": "Recovering Private Text in Federated Learning of Language Models", "abstract": "Federated learning allows distributed users to collaboratively train a model while keeping each user\u2019s data private. Recently, a growing body of work has demonstrated that an eavesdropping attacker can effectively recover image data from gradients transmitted during federated learning. However, little progress has been made in recovering text data. In this paper, we present a novel attack method FILM for federated learning of language models (LMs). For the first time, we show the feasibility of recovering text from large batch sizes of up to 128 sentences. Unlike image-recovery methods that are optimized to match gradients, we take a distinct approach that first identifies a set of words from gradients and then directly reconstructs sentences based on beam search and a prior-based reordering strategy. \nWe conduct the FILM attack on several large-scale datasets and show that it can successfully reconstruct single sentences with high fidelity for large batch sizes and even multiple sentences if applied iteratively.\nWe evaluate three defense methods: gradient pruning, DPSGD, and a simple approach to freeze word embeddings that we propose.  We show that both gradient pruning and DPSGD lead to a significant drop in utility. However, if we fine-tune a public pre-trained LM on private text without updating word embeddings, it can effectively defend the attack with minimal data utility loss. Together, we hope that our results can encourage the community to rethink the privacy concerns of LM training and its standard practices in the future. Our code is publicly available at https://github.com/Princeton-SysML/FILM ."}}
{"id": "u0n78thUJd4", "cdate": 1640995200000, "mdate": 1683641438147, "content": {"title": "Recovering Private Text in Federated Learning of Language Models", "abstract": "Federated learning allows distributed users to collaboratively train a model while keeping each user\u2019s data private. Recently, a growing body of work has demonstrated that an eavesdropping attacker can effectively recover image data from gradients transmitted during federated learning. However, little progress has been made in recovering text data. In this paper, we present a novel attack method FILM for federated learning of language models (LMs). For the first time, we show the feasibility of recovering text from large batch sizes of up to 128 sentences. Unlike image-recovery methods that are optimized to match gradients, we take a distinct approach that first identifies a set of words from gradients and then directly reconstructs sentences based on beam search and a prior-based reordering strategy. We conduct the FILM attack on several large-scale datasets and show that it can successfully reconstruct single sentences with high fidelity for large batch sizes and even multiple sentences if applied iteratively.We evaluate three defense methods: gradient pruning, DPSGD, and a simple approach to freeze word embeddings that we propose. We show that both gradient pruning and DPSGD lead to a significant drop in utility. However, if we fine-tune a public pre-trained LM on private text without updating word embeddings, it can effectively defend the attack with minimal data utility loss. Together, we hope that our results can encourage the community to rethink the privacy concerns of LM training and its standard practices in the future. Our code is publicly available at https://github.com/Princeton-SysML/FILM ."}}
{"id": "1hmMY0RZMx", "cdate": 1640995200000, "mdate": 1682342792215, "content": {"title": "Recovering Private Text in Federated Learning of Language Models", "abstract": "Federated learning allows distributed users to collaboratively train a model while keeping each user's data private. Recently, a growing body of work has demonstrated that an eavesdropping attacker can effectively recover image data from gradients transmitted during federated learning. However, little progress has been made in recovering text data. In this paper, we present a novel attack method FILM for federated learning of language models (LMs). For the first time, we show the feasibility of recovering text from large batch sizes of up to 128 sentences. Unlike image-recovery methods that are optimized to match gradients, we take a distinct approach that first identifies a set of words from gradients and then directly reconstructs sentences based on beam search and a prior-based reordering strategy. We conduct the FILM attack on several large-scale datasets and show that it can successfully reconstruct single sentences with high fidelity for large batch sizes and even multiple sentences if applied iteratively. We evaluate three defense methods: gradient pruning, DPSGD, and a simple approach to freeze word embeddings that we propose. We show that both gradient pruning and DPSGD lead to a significant drop in utility. However, if we fine-tune a public pre-trained LM on private text without updating word embeddings, it can effectively defend the attack with minimal data utility loss. Together, we hope that our results can encourage the community to rethink the privacy concerns of LM training and its standard practices in the future."}}
{"id": "0CDKgyYaxC8", "cdate": 1621629930002, "mdate": null, "content": {"title": "Evaluating Gradient Inversion Attacks and Defenses in Federated Learning", "abstract": "Gradient inversion attack (or input recovery from gradient) is an emerging threat to the security and privacy preservation of Federated learning, whereby malicious eavesdroppers or participants in the protocol can recover (partially) the clients' private data. This paper evaluates existing attacks and defenses. We find that some attacks make strong assumptions about the setup. Relaxing such assumptions can substantially weaken these attacks. We then evaluate the benefits of three proposed defense mechanisms against gradient inversion attacks. We show the trade-offs of privacy leakage and data utility of these defense methods, and find that combining them in an appropriate manner makes the attack less effective, even under the original strong assumptions. We also estimate the computation cost of end-to-end recovery of a single image under each evaluated defense. Our findings suggest that the state-of-the-art attacks can currently be defended against with minor data utility loss, as summarized in a list of potential strategies."}}
{"id": "z0FQdoaV9u", "cdate": 1609459200000, "mdate": 1683641438147, "content": {"title": "EMA: Auditing Data Removal from Trained Models", "abstract": "Data auditing is a process to verify whether certain data have been removed from a trained model. A recently proposed method\u00a0[10] uses Kolmogorov-Smirnov (KS) distance for such data auditing. However, it fails under certain practical conditions. In this paper, we propose a new method called Ensembled Membership Auditing ( $$\\mathsf {EMA}$$ ) for auditing data removal to overcome these limitations. We compare both methods using benchmark datasets (MNIST and SVHN) and Chest X-ray datasets with multi-layer perceptrons (MLP) and convolutional neural networks (CNN). Our experiments show that $$\\mathsf {EMA}$$ is robust under various conditions, including the failure cases of the previously proposed method. Our code is available at: https://github.com/Hazelsuko07/EMA ."}}
{"id": "wPp_eVSjW5EN", "cdate": 1609459200000, "mdate": 1664445388201, "content": {"title": "Evaluating Gradient Inversion Attacks and Defenses in Federated Learning", "abstract": "Gradient inversion attack (or input recovery from gradient) is an emerging threat to the security and privacy preservation of Federated learning, whereby malicious eavesdroppers or participants in the protocol can recover (partially) the clients' private data. This paper evaluates existing attacks and defenses. We find that some attacks make strong assumptions about the setup. Relaxing such assumptions can substantially weaken these attacks. We then evaluate the benefits of three proposed defense mechanisms against gradient inversion attacks. We show the trade-offs of privacy leakage and data utility of these defense methods, and find that combining them in an appropriate manner makes the attack less effective, even under the original strong assumptions. We also estimate the computation cost of end-to-end recovery of a single image under each evaluated defense. Our findings suggest that the state-of-the-art attacks can currently be defended against with minor data utility loss, as summarized in a list of potential strategies. Our code is available at: https://github.com/Princeton-SysML/GradAttack."}}
