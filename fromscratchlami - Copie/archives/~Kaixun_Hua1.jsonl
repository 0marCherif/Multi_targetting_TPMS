{"id": "-Oh_TKISy89", "cdate": 1652737665227, "mdate": null, "content": {"title": "A Scalable Deterministic Global Optimization Algorithm for Training Optimal Decision Tree", "abstract": "The training of optimal decision tree via mixed-integer programming (MIP) has attracted much attention in recent literature. However, for large datasets, state-of-the-art approaches struggle to solve the optimal decision tree training problems to a provable global optimal solution within a reasonable time. In this paper, we reformulate the optimal decision tree training problem as a two-stage optimization problem and propose a tailored reduced-space branch and bound algorithm to train optimal decision tree for the classification tasks with continuous features. We present several structure-exploiting lower and upper bounding methods. The computation of bounds can be decomposed into the solution of many small-scale subproblems and can be naturally parallelized. With these bounding methods, we prove that our algorithm can converge by branching only on variables representing the optimal decision tree structure, which is invariant to the size of datasets. Moreover, we propose a novel sample reduction method that can predetermine the cost of part of samples at each BB node. Combining the sample reduction method with the parallelized bounding strategies, our algorithm can be extremely scalable. Our algorithm can find global optimal solutions on dataset with over 245,000 samples (1000 cores, less than 1% optimality gap, within 2 hours). We test 21 real-world datasets from UCI Repository. The results reveal that for datasets with over 7,000 samples, our algorithm can, on average, improve the training accuracy by 3.6% and testing accuracy by 2.8%, compared to the current state-of-the-art."}}
{"id": "SrwrRP3yfq8", "cdate": 1652737663607, "mdate": null, "content": {"title": "Global Optimal K-Medoids Clustering of One Million Samples", "abstract": "We study the deterministic global optimization of the K-Medoids clustering problem. This work proposes a branch and bound (BB) scheme, in which a tailored Lagrangian relaxation method proposed in the 1970s is used to provide a lower bound at each BB node. The lower bounding method already guarantees the maximum gap at the root node. A closed-form solution to the lower bound can be derived analytically without explicitly solving any optimization problems, and its computation can be easily parallelized. Moreover, with this lower bounding method, finite convergence to the global optimal solution can be guaranteed by branching only on the regions of medoids. We also present several tailored bound tightening techniques to reduce the search space and computational cost. Extensive computational studies on 28 machine learning datasets demonstrate that our algorithm can provide a provable global optimal solution with an optimality gap of 0.1\\% within 4 hours on datasets with up to one million samples. Besides, our algorithm can obtain better or equal objective values than the heuristic method. A theoretical proof of global convergence for our algorithm is also presented.\n"}}
{"id": "SM6eEfZUTXZ", "cdate": 1640995200000, "mdate": 1660072033817, "content": {"title": "Deep Learning-based Predictive Control of Battery Management for Frequency Regulation", "abstract": "This paper proposes a deep learning-based optimal battery management scheme for frequency regulation (FR) by integrating model predictive control (MPC), supervised learning (SL), reinforcement learning (RL), and high-fidelity battery models. By taking advantage of deep neural networks (DNNs), the derived DNN-approximated policy is computationally efficient in online implementation. The design procedure of the proposed scheme consists of two sequential processes: (1) the SL process, in which we first run a simulation with an MPC embedding a low-fidelity battery model to generate a training data set, and then, based on the generated data set, we optimize a DNN-approximated policy using SL algorithms; and (2) the RL process, in which we utilize RL algorithms to improve the performance of the DNN-approximated policy by balancing short-term economic incentives and long-term battery degradation. The SL process speeds up the subsequent RL process by providing a good initialization. By utilizing RL algorithms, one prominent property of the proposed scheme is that it can learn from the data generated by simulating the FR policy on the high-fidelity battery simulator to adjust the DNN-approximated policy, which is originally based on low-fidelity battery model. A case study using real-world data of FR signals and prices is performed. Simulation results show that, compared to conventional MPC schemes, the proposed deep learning-based scheme can effectively achieve higher economic benefits of FR participation while maintaining lower online computational cost."}}
{"id": "S0M1-D6nAaR", "cdate": 1640995200000, "mdate": 1660072033841, "content": {"title": "Global Optimization of K-Center Clustering", "abstract": "k$-center problem is a well-known clustering method and can be formulated as a mixed-integer nonlinear programming problem. This work provides a practical global optimization algorithm for this task based on a reduced-space spatial branch and bound scheme. This algorithm can guarantee convergence to the global optimum by only branching on the centers of clusters, which is independent of the dataset\u2019s cardinality. In addition, a set of feasibility-based bounds tightening techniques are proposed to narrow down the domain of centers and significantly accelerate the convergence. To demonstrate the capacity of this algorithm, we present computational results on 32 datasets. Notably, for the dataset with 14 million samples and 3 features, the serial implementation of the algorithm can converge to an optimality gap of 0.1% within 2 hours. Compared with a heuristic method, the global optimum obtained by our algorithm can reduce the objective function on average by 30.4%."}}
{"id": "0DXfW2ioS_5", "cdate": 1640995200000, "mdate": 1660072033812, "content": {"title": "Optimization of compressor standby schemes for gas transmission pipeline systems based on gas delivery reliability", "abstract": ""}}
{"id": "L4H4_pfOQAF", "cdate": 1609459200000, "mdate": 1648740509684, "content": {"title": "A Scalable Deterministic Global Optimization Algorithm for Clustering Problems", "abstract": "The minimum sum-of-squares clustering (MSSC) task, which can be treated as a Mixed Integer Second Order Cone Programming (MISOCP) problem, is rarely investigated in the literature through determini..."}}
{"id": "_3o1gaYYsmP", "cdate": 1546300800000, "mdate": 1648740509679, "content": {"title": "Data ultrametricity and clusterability", "abstract": "The increasing needs of clustering massive datasets and the high cost of running clustering algorithms poses difficult problems for users. In this context it is important to determine if a data set is clusterable, that is, it may be partitioned efficiently into well-differentiated groups containing similar objects. We approach data clusterability from an ultrametric-based perspective. A novel approach to determine the ultrametricity of a dataset is proposed via a special type of matrix product, which allows us to evaluate the clusterability of the dataset. Furthermore, we show that by applying our technique to a dissimilarity space will generate the sub-dominant ultrametric of the dissimilarity."}}
{"id": "9By9_k6B09", "cdate": 1514764800000, "mdate": 1648740509680, "content": {"title": "Dual Criteria Determination of the Number of Clusters in Data", "abstract": "We present a method for determining the number of clusters existent in a data set involving a bi-criteria optimization that makes use of the entropy and the cohesion of a partition. The results are promising and may be applicable in dealing with clusterings of imbalanced data."}}
{"id": "zfQweBvGTH7", "cdate": 1451606400000, "mdate": 1648740509684, "content": {"title": "Long-lead term precipitation forecasting by Hierarchical Clustering-based Bayesian Structural Vector Autoregression", "abstract": "Heavy precipitation for several days and weeks always leads to some extreme nature disasters. Long-lead term precipitation forecasting plays an important role on the prevision of such calamities. Most works focus on the generation of training labels with allocation of the proper corresponding spatio-temporal information. In this paper, we will provide a different path by performing regression analysis using the precipitation amounts at particular locations. This method is called Hierarchical Clustering based Bayesian Structural Vector Autoregression (HC-BSVAR). The approach for HC-BSVAR is divided into two steps. First, we apply a hierarchical clustering algorithm to identify the Elite locations and then transfer the 3-dimensional data space into a new traditional 2-dimensional data space. Every column of the new data frame is a hydro-meteorological feature of the original data and each row represents a time point (day) in the original space. Secondly, an economic-based multivariate time series model called Bayesian-based Structural Vector Autoregression (BSVAR) is exploited to perform the final prediction result. The prediction quality will be vary for different cut of tree structure which generated by hierarchical clustering. The coefficient for determination of each location by each level of cut is applied to quantize the quality of prediction. The relationship between the cut level of clustering geographic locations and the regression model performance are also discussed, based on the result of prediction quality."}}
{"id": "8uaA7GkAvRl", "cdate": 1420070400000, "mdate": 1648740509660, "content": {"title": "Ultrametricity of Dissimilarity Spaces and Its Significance for Data Mining", "abstract": "We introduce a measureHua, Kaixun of ultrametricitySimovici, Dan A. for dissimilarityVetro, Rosanne spaces and examine transformations of dissimilarities that impact this measure. Then, we study the influence of ultrametricity on the behavior of two classes of data mining algorithms (kNN classification and PAM clustering) applied on dissimilarity spaces. We show that there is an inverse variation between ultrametricity and performance of classifiers. For clustering, increased ultrametricity generate clusterings with better separation. Lowering ultrametricity produces more compact clusters."}}
