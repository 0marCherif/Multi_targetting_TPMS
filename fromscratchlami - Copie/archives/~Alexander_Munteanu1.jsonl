{"id": "tcPojO4lDxD", "cdate": 1672531200000, "mdate": 1681805524819, "content": {"title": "Almost Linear Constant-Factor Sketching for \ud835\udcc11 and Logistic Regression", "abstract": "We improve upon previous oblivious sketching and turnstile streaming results for $\\ell_1$ and logistic regression, giving a much smaller sketching dimension achieving $O(1)$-approximation and yielding an efficient optimization problem in the sketch space. Namely, we achieve for any constant $c>0$ a sketching dimension of $\\tilde{O}(d^{1+c})$ for $\\ell_1$ regression and $\\tilde{O}(\\mu d^{1+c})$ for logistic regression, where $\\mu$ is a standard measure that captures the complexity of compressing the data. For $\\ell_1$-regression our sketching dimension is near-linear and improves previous work which either required $\\Omega(\\log d)$-approximation with this sketching dimension, or required a larger $\\operatorname{poly}(d)$ number of rows. Similarly, for logistic regression previous work had worse $\\operatorname{poly}(\\mu d)$ factors in its sketching dimension. We also give a tradeoff that yields a $1+\\varepsilon$ approximation in input sparsity time by increasing the total size to $(d\\log(n)/\\varepsilon)^{O(1/\\varepsilon)}$ for $\\ell_1$ and to $(\\mu d\\log(n)/\\varepsilon)^{O(1/\\varepsilon)}$ for logistic regression. Finally, we show that our sketch can be extended to approximate a regularized version of logistic regression where the data-dependent regularizer corresponds to the variance of the individual logistic losses."}}
{"id": "WZGvaLM6tC", "cdate": 1672531200000, "mdate": 1681805524836, "content": {"title": "Optimal Sketching Bounds for Sparse Linear Regression", "abstract": "We study oblivious sketching for $k$-sparse linear regression under various loss functions such as an $\\ell_p$ norm, or from a broad class of hinge-like loss functions, which includes the logistic and ReLU losses. We show that for sparse $\\ell_2$ norm regression, there is a distribution over oblivious sketches with $\\Theta(k\\log(d/k)/\\varepsilon^2)$ rows, which is tight up to a constant factor. This extends to $\\ell_p$ loss with an additional additive $O(k\\log(k/\\varepsilon)/\\varepsilon^2)$ term in the upper bound. This establishes a surprising separation from the related sparse recovery problem, which is an important special case of sparse regression. For this problem, under the $\\ell_2$ norm, we observe an upper bound of $O(k \\log (d)/\\varepsilon + k\\log(k/\\varepsilon)/\\varepsilon^2)$ rows, showing that sparse recovery is strictly easier to sketch than sparse regression. For sparse regression under hinge-like loss functions including sparse logistic and sparse ReLU regression, we give the first known sketching bounds that achieve $o(d)$ rows showing that $O(\\mu^2 k\\log(\\mu n d/\\varepsilon)/\\varepsilon^2)$ rows suffice, where $\\mu$ is a natural complexity parameter needed to obtain relative error bounds for these loss functions. We again show that this dimension is tight, up to lower order terms and the dependence on $\\mu$. Finally, we show that similar sketching bounds can be achieved for LASSO regression, a popular convex relaxation of sparse regression, where one aims to minimize $\\|Ax-b\\|_2^2+\\lambda\\|x\\|_1$ over $x\\in\\mathbb{R}^d$. We show that sketching dimension $O(\\log(d)/(\\lambda \\varepsilon)^2)$ suffices and that the dependence on $d$ and $\\lambda$ is tight."}}
{"id": "gu-SC0dpkvw", "cdate": 1663850268638, "mdate": null, "content": {"title": "Almost Linear Constant-Factor Sketching for $\\ell_1$ and Logistic Regression", "abstract": "We improve upon previous oblivious sketching and turnstile streaming results for $\\ell_1$ and logistic regression, giving a much smaller sketching dimension achieving $O(1)$-approximation and yielding an efficient optimization problem in the sketch space. Namely, we achieve for any constant $c>0$ a sketching dimension of $\\tilde{O}(d^{1+c})$ for $\\ell_1$ regression and $\\tilde{O}(\\mu d^{1+c})$ for logistic regression, where $\\mu$ is a standard measure that captures the complexity of compressing the data. For $\\ell_1$-regression our sketching dimension is near-linear and improves previous work which either required $\\Omega(\\log d)$-approximation with this sketching dimension, or required a larger $\\operatorname{poly}(d)$ number of rows. Similarly, for logistic regression previous work had worse $\\operatorname{poly}(\\mu d)$ factors in its sketching dimension. We also give a tradeoff that yields a $1+\\varepsilon$ approximation in input sparsity time by increasing the total size to $(d\\log(n)/\\varepsilon)^{O(1/\\varepsilon)}$ for $\\ell_1$ and to $(\\mu d\\log(n)/\\varepsilon)^{O(1/\\varepsilon)}$ for logistic regression. Finally, we show that our sketch can be extended to approximate a regularized version of logistic regression where the data-dependent regularizer corresponds to the variance of the individual logistic losses."}}
{"id": "foWGWTWK7v7", "cdate": 1640995200000, "mdate": 1680259983219, "content": {"title": "p-Generalized Probit Regression and Scalable Maximum Likelihood Estimation via Sketching and Coresets", "abstract": ""}}
{"id": "fLoVP5Q_J6a", "cdate": 1640995200000, "mdate": 1681805524841, "content": {"title": "Bounding the Width of Neural Networks via Coupled Initialization - A Worst Case Analysis", "abstract": "A common method in training neural networks is to initialize all the weights to be independent Gaussian vectors. We observe that by instead initializing the weights into independent pairs, where each pair consists of two identical Gaussian vectors, we can significantly improve the convergence analysis. While a similar technique has been studied for random inputs [Daniely, NeurIPS 2020], it has not been analyzed with arbitrary inputs. Using this technique, we show how to significantly reduce the number of neurons required for two-layer ReLU networks, both in the under-parameterized setting with logistic loss, from roughly $\\gamma^{-8}$ [Ji and Telgarsky, ICLR 2020] to $\\gamma^{-2}$, where $\\gamma$ denotes the separation margin with a Neural Tangent Kernel, as well as in the over-parameterized setting with squared loss, from roughly $n^4$ [Song and Yang, 2019] to $n^2$, implicitly also improving the recent running time bound of [Brand, Peng, Song and Weinstein, ITCS 2021]. For the under-parameterized setting we also prove new lower bounds that improve upon prior work, and that under certain assumptions, are best possible."}}
{"id": "XlZjuQ2qsoY", "cdate": 1640995200000, "mdate": 1680259983218, "content": {"title": "Bounding the Width of Neural Networks via Coupled Initialization A Worst Case Analysis", "abstract": ""}}
{"id": "HrzD9BpEkxE", "cdate": 1640995200000, "mdate": 1652596901964, "content": {"title": "p-Generalized Probit Regression and Scalable Maximum Likelihood Estimation via Sketching and Coresets", "abstract": "We study the $p$-generalized probit regression model, which is a generalized linear model for binary responses. It extends the standard probit model by replacing its link function, the standard normal cdf, by a $p$-generalized normal distribution for $p\\in[1, \\infty)$. The $p$-generalized normal distributions \\citep{Sub23} are of special interest in statistical modeling because they fit much more flexibly to data. Their tail behavior can be controlled by choice of the parameter $p$, which influences the model's sensitivity to outliers. Special cases include the Laplace, the Gaussian, and the uniform distributions. We further show how the maximum likelihood estimator for $p$-generalized probit regression can be approximated efficiently up to a factor of $(1+\\varepsilon)$ on large data by combining sketching techniques with importance subsampling to obtain a small data summary called coreset."}}
{"id": "xy_pUEm0_SH", "cdate": 1609459200000, "mdate": 1652596901963, "content": {"title": "Oblivious Sketching for Logistic Regression", "abstract": "What guarantees are possible for solving logistic regression in one pass over a data stream? To answer this question, we present the first data oblivious sketch for logistic regression. Our sketch ..."}}
{"id": "jO8uXStvg_", "cdate": 1609459200000, "mdate": 1681805524851, "content": {"title": "Oblivious sketching for logistic regression", "abstract": "What guarantees are possible for solving logistic regression in one pass over a data stream? To answer this question, we present the first data oblivious sketch for logistic regression. Our sketch can be computed in input sparsity time over a turnstile data stream and reduces the size of a $d$-dimensional data set from $n$ to only $\\operatorname{poly}(\\mu d\\log n)$ weighted points, where $\\mu$ is a useful parameter which captures the complexity of compressing the data. Solving (weighted) logistic regression on the sketch gives an $O(\\log n)$-approximation to the original problem on the full data set. We also show how to obtain an $O(1)$-approximation with slight modifications. Our sketches are fast, simple, easy to implement, and our experiments demonstrate their practicality."}}
{"id": "nwYOzEGdTKC", "cdate": 1577836800000, "mdate": null, "content": {"title": "Streaming statistical models via Merge & Reduce", "abstract": "Merge & Reduce is a general algorithmic scheme in the theory of data structures. Its main purpose is to transform static data structures\u2014that support only queries\u2014into dynamic data structures\u2014that allow insertions of new elements\u2014with as little overhead as possible. This can be used to turn classic offline algorithms for summarizing and analyzing data into streaming algorithms. We transfer these ideas to the setting of statistical data analysis in streaming environments. Our approach is conceptually different from previous settings where Merge & Reduce has been employed. Instead of summarizing the data, we combine the Merge & Reduce framework directly with statistical models. This enables performing computationally demanding data analysis tasks on massive data sets. The computations are divided into small tractable batches whose size is independent of the total number of observations n. The results are combined in a structured way at the cost of a bounded $$O(\\log n)$$ O ( log n ) factor in their memory requirements. It is only necessary, though nontrivial, to choose an appropriate statistical model and design merge and reduce operations on a casewise basis for the specific type of model. We illustrate our Merge & Reduce schemes on simulated and real-world data employing (Bayesian) linear regression models, Gaussian mixture models and generalized linear models."}}
