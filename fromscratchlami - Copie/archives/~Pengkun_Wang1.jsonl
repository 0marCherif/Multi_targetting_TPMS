{"id": "Bjf8wE55mpe", "cdate": 1683906482021, "mdate": 1683906482021, "content": {"title": "A Knowledge-Driven Memory System for Traffic Flow Prediction", "abstract": "Traffic flow prediction is critical for intelligent transportation systems. Recent studies indicate that performance improvement by designing new models is becoming marginal. Instead, we argue that the improvement can be achieved by using traffic-related facts or laws, which is termed exogenous knowledge. To this end, we propose a knowledge-driven memory system that can be seamlessly integrated into GCN-based traffic forecasting models. Specifically, the memory system includes three components: access interface, memory module, and feedback interface. The access interface based on the attention mechanism and the feedback interface based on the gate mechanism are used to guide the model to extract useful patterns and integrate these patterns into the model to enhance spatiotemporal representation respectively. The memory module is used to learn specific knowledge-based patterns, and this is achieved by constraining the learning process with unsupervised loss functions formulated inspired by exogenous knowledge. We construct three kinds of memory modules driven by different exogenous knowledge: the long-term trend memory to learn periodic patterns, the hierarchical effect memory to capture coarse-grained region patterns, and the representative pattern memory to extract representative patterns. Experiments combined with multiple existing models demonstrate the effectiveness of the memory system."}}
{"id": "lIW4pyWFkX3", "cdate": 1683882515608, "mdate": 1683882515608, "content": {"title": "Knowledge Expansion and Consolidation for Continual Traffic Prediction With Expanding Graphs", "abstract": "Accurate traffic prediction plays a vital role in intelligent transport managements and applications. However, in the vast majority of existing works, the focus is mainly on modeling spatiotemporal correlations in static traffic networks. Thus, the continuous expansion and evolution of traffic networks are ignored. In this work, we study the problem of traffic prediction with expanding road network structures under the continual learning paradigm. Considering the model prediction performance, efficiency, and data accessibility, a SpatioTemporal Knowledge Expansion and Consolidation (STKEC) framework is proposed. This framework contains an influence-based knowledge expansion strategy to help the spatiotemporal learning model integrate new spatiotemporal traffic patterns and a memory-augmented knowledge consolidation mechanism to preserve the learned spatiotemporal patterns without accessing the data in previous graphs. Extensive experiments are conducted on a large-scale dataset and verify the superior performance of STKEC in continual traffic prediction."}}
{"id": "T1HGLZC8W5J", "cdate": 1681881728503, "mdate": 1681881728503, "content": {"title": "Long-tailed Time Series Classification via Feature Space Rebalancing", "abstract": "Learning unbiased decision boundaries is crucial for time series classification. Real-world datasets typically exhibit long-tailed natures of class distributions, which results in an imbalanced feature space after training, i.e., decision boundaries will be easily biased towards dominant classes that dominate the feature space. However, existing methods mostly train models from artificially balanced datasets, making it still unclear how to deal with the long-tailed natures of time series data in real-world scenarios. Motivated by this question, we analyze the similarities and differences between long-tailed time series classification and general long-tailed recognition, and propose a Feature Space Rebalancing (FSR) strategy for time series classification, which works jointly from both representation and data perspectives. Specifically, from the representation perspective, we design Balanced Contrastive Learning (BCL), which avoids excessive intra-class compaction of tail classes by introducing a balanced supervised contrastive loss with hierarchical prototypes, resulting in a balanced feature space and better generalization. From the data perspective, we explore the effectiveness of traditional data augmentation on long-tailed distributions and propose an Adaptive Temporal Augmentation (ATA) to rebalance the potential feature space at the temporal level. Extensive experiments on multiple long-tailed time series datasets demonstrate its superiority, including different class distributions and imbalance ratios."}}
{"id": "vvfuoe6TOKT", "cdate": 1672531200000, "mdate": 1681880925198, "content": {"title": "Joint Gated Co-Attention Based Multi-Modal Networks for Subregion House Price Prediction", "abstract": "Urban housing price is widely accepted as an economic indicator which is of both business and research interest in urban computing. However, due to the complex nature of influencing factors and the sparse property of transaction records, to implement such a model is still challenging. To address these challenges, in this work, we study an effective and fine-grained model for urban subregion housing price predictions. Compared to existing works, our proposal improves the forecasting granularity from city-level to mile-level, with only publicly released transaction data. We employ a feature selection mechanism to select more relevant features. Then, we propose an integrated model, JGC_MMN (Joint Gated Co-attention Based Multi-modal Network), to learn all-level features and capture spatiotemporal correlations in all-time stages with a modified densely connected convolutional network as well as current ingredients and future expectations. Next, we devise a novel JGC based fusion method to better fuse the heterogeneous data of multi-stage models by considering their interactions in temporal dimension. Finally, extensive empirical studies on real datasets demonstrate the effectiveness of our proposal, and this fine-grained housing price forecasting has the potential to support a broad scope of applications, ranging from urban planning to housing market recommendations."}}
{"id": "lB0C3FjEDr2", "cdate": 1672531200000, "mdate": 1681880925165, "content": {"title": "A Multi-graph Fusion Based Spatiotemporal Dynamic Learning Framework", "abstract": "Spatiotemporal data forecasting is a fundamental task in the field of graph data mining. Typical spatiotemporal data prediction methods usually capture spatial dependencies by directly aggregating features of local neighboring vertices in a fixed graph. However, this kind of aggregators can only capture localized correlations between vertices, and while been stacked for larger receptive field, they fall into the dilemma of over-smoothing. Additional, in temporal perspective, traditional methods focus on fixed graphs, while the correlations among vertexes can be dynamic. And time series components integrated strategies in traditional spatiotemporal learning methods can hardly handle frequently and drastically changed sequences. To overcome those limitations of existing works, in this paper, we propose a novel multi-graph based dynamic learning framework. First, a novel Dynamic Neighbor Search (DNS) mechanism is introduced to model global dynamic correlations between vertices by constructing a feature graph (FG), where the adjacency matrix is dynamically determined by DNS. Then we further alleviate the over-smoothing issue with our newly designed Adaptive Heterogeneous Representation (AHR) module. Both FG and origin graph (OG) are fed into the AHR modules and fused in our proposed Multi-graph Fusion block. Additionally, we design a Differential Vertex Representation (DVR) module which takes advantage of differential information to model temporal trends. Extensive experiments illustrate the superior forecasting performances of our proposed multi-graph based dynamic learning framework on six real-world spatiotemporal datasets from different cities and domains, and this corroborates the solid effectiveness of our proposed framework and its superior generalization ability."}}
{"id": "KddwoUZduV6", "cdate": 1672531200000, "mdate": 1681880925163, "content": {"title": "Graph-Free Learning in Graph-Structured Data: A More Efficient and Accurate Spatiotemporal Learning Perspective", "abstract": "Spatiotemporal learning, which aims at extracting spatiotemporal correlations from the collected spatiotemporal data, is a research hotspot in recent years. And considering the inherent graph structure of spatiotemporal data, recent works focus on capturing spatial dependencies by utilizing Graph Convolutional Networks (GCNs) to aggregate vertex features with the guidance of adjacency matrices. In this paper, with extensive and deep-going experiments, we comprehensively analyze existing spatiotemporal graph learning models and reveal that extracting adjacency matrices with carefully design strategies, which are viewed as the key of enhancing performance on graph learning, are largely ineffective. Meanwhile, based on these experiments, we also discover that the aggregation itself is more important than the way that how vertices are aggregated. With these preliminary, a novel efficient Graph-Free Spatial (GFS) learning module based on layer normalization for capturing spatial correlations in spatiotemporal graph learning. The proposed GFS module can be easily plugged into existing models for replacing all graph convolution components. Rigorous theoretical proof demonstrates that the time complexity of GFS is significantly better than that of graph convolution operation. Extensive experiments verify the superiority of GFS in both the perspectives of efficiency and learning effect in processing graph-structured data especially extreme large scale graph data."}}
{"id": "Dvs-a3aymPe", "cdate": 1663850091548, "mdate": null, "content": {"title": "Searching Lottery Tickets in Graph Neural Networks: A Dual Perspective", "abstract": "Graph Neural Networks (GNNs) have shown great promise in various graph learning tasks. However, the computational overheads of fitting GNNs to large-scale graphs grow rapidly, posing obstacles to GNNs from scaling up to real-world applications. To tackle this issue, Graph Lottery Ticket (GLT) hypothesis articulates that there always exists a sparse subnetwork/subgraph with admirable performance in GNNs with random initialization. Such a pair of core subgraph and sparse subnetwork (called graph lottery tickets) can be uncovered by iteratively applying a novel sparsification method. While GLT provides new insights for GNN compression, it requires a full pretraining process to obtain graph lottery tickets, which is not universal and friendly to real-world applications. Moreover, the graph sparsification in GLT utilizes sampling techniques, which may result in massive information loss and aggregation failure. In this paper, we explore the searching of graph lottery tickets from a complementary perspective -- transforming a random ticket into a graph lottery ticket, which allows us to more comprehensively explore the relationships between the original network/graph and their sparse counterpart. To achieve this, we propose regularization-based network pruning and hierarchical graph sparsification, leading to our Dual Graph Lottery Ticket (DGLT) framework for a joint sparsification of network and graph. Compared to GLT, our DGLT helps achieve a triple-win situation of graph lottery tickets with high sparsity, admirable performance, and good explainability. More importantly, we rigorously prove that our model can eliminate noise and maintain reliable information in substructures using the graph information bottleneck theory. Extensive experimental results on various graph-related tasks validate the effectiveness of our framework."}}
{"id": "CJl2S0w1mbq", "cdate": 1663849874281, "mdate": null, "content": {"title": "A UNIFIED VIEW OF FINDING AND TRANSFORMING WINNING LOTTERY TICKETS", "abstract": "While over-parameterized deep neural networks obtain prominent results on various machine learning tasks, their superfluous parameters usually make model training and inference notoriously inefficient. Lottery Ticket Hypothesis (LTH) addresses this issue from a novel perspective: it articulates that there always exist sparse and admirable subnetworks in a randomly initialized dense network, which can be realized by an iterative pruning strategy. Dual Lottery Ticket Hypothesis (DLTH) further investigates sparse network training from a complementary view. Concretely, it introduces a gradually increased regularization term to transform a dense network to an ultra-light subnetwork without sacrificing learning capacity. After revisiting the success of LTH and DLTH, we unify these two research lines by coupling the stability of iterative pruning and the excellent performance of increased regularization, resulting in two new algorithms (UniLTH and UniDLTH) for finding and transforming winning tickets, respectively. Unlike either LTH without regularization or DLTH which applies regularization across the training, our methods first train the network without any regularization force until the model reaches a certain point (i.e., the validation loss does not decrease for several epochs), and then employ increased regularization for information extrusion and iteratively perform magnitude pruning till the end. We theoretically prove that the early stopping mechanism acts analogously as regularization and can help the optimization trajectory stop at a particularly better point in space than regularization. This not only prevent the parameters from being excessively skewed to the training distribution (over-fitting), but also better stimulate the network potential to obtain more powerful subnetworks. Extensive experiments are conducted to show the superiority of our methods in terms of accuracy and sparsity. "}}
{"id": "n4BtjIffuN", "cdate": 1640995200000, "mdate": 1674000830013, "content": {"title": "Inferring Intersection Traffic Patterns With Sparse Video Surveillance Information: An ST-GAN Method", "abstract": ""}}
{"id": "D1ssJFBaan", "cdate": 1640995200000, "mdate": 1681880925178, "content": {"title": "Countering Modal Redundancy and Heterogeneity: A Self-Correcting Multimodal Fusion", "abstract": "Fusing multimodal heterogeneous data plays a vital role in recognition and prediction tasks in various fields, e.g., action recognition and traffic accident forecast. Yet, there remain some key challenges, such as heterogeneous feature interaction and feature redundancies, that significantly affect the performance of multimodal fusion. To tackle these challenges, we first devise a Unified Feature Interaction Module (UFIM) in which a novel orthogonal attention component is designed to obtain fine-grained inter-modal interaction information among heterogeneous features. Then, we propose a novel Self-Correcting Transformer Module (SCTM) which employs a modified transformer to obtain the one-to-many correlation information between the current modal feature and the merged features of other modalities to alleviate the redundancy problem. Extensive experiments on four cross-domain tasks demonstrate the effectiveness and generalization ability of our proposed method."}}
