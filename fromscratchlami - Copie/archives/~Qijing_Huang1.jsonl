{"id": "OHlhibN1Cr", "cdate": 1682227364020, "mdate": null, "content": {"title": "DOSA: One-Loop DSE for DNN Accelerators Using Differentiable Models", "abstract": "The process of hardware design space exploration requires both hardware parameters and mappings from the algorithm onto the target hardware to be discovered and optimized. Previous work has largely approached this simultaneous optimization problem by separately exploring the hardware design space and the mapspace - both individually large and highly nonconvex spaces - independently. The resulting combinatorial explosion has created significant difficulties for optimizers. We introduce DOSA, which consists of differentiable latency and energy models, as well as a gradient descent-based optimization technique to simultaneously explore both spaces and identify high-performing design points. Experimental results demonstrate that DOSA outperforms random search and Bayesian optimization by 2.80x and 12.59x, respectively, in improving DNN model energy-delay product, given a similar number of samples. In particular, we demonstrate DOSA's modularity and flexibility via transfer to a real DNN accelerator setting, where we achieve a 2.07x improvement in energy-delay product by augmenting our analytical model with a learned model."}}
{"id": "GtyQbLUUagE", "cdate": 1682227363320, "mdate": null, "content": {"title": "Full Stack Optimization of Transformer Inference", "abstract": "Recent advances in state-of-the-art neural network architecture design have been moving toward Transformer models. These models achieve superior accuracy across a wide range of applications in computer vision, natural language processing, and speech recognition. This trend has been consistent over the past several years since Transformer models were originally introduced. However, the amount of compute and bandwidth required for inference of recent Transformer models is growing at a significant rate, and this has made their deployment in latency-sensitive applications challenging. As such, there has been an increased focus on making Transformer models more efficient, with methods that range from changing the architecture design, all the way to developing dedicated domain-specific accelerators.\n\nIn this work, we pursue a full-stack approach to optimizing Transformer inference. We analyze the implications of the Transformer architecture on hardware, including the impact of nonlinear operations such as Layer Normalization, Softmax, and GELU, as well as linear operations, and we use this analysis to optimize a fixed Transformer architecture. We assess the challenges with finding the right mapping and scheduling of operations for Transformer models, and pursue neural architecture search to further optimize the Transformer network. We find that a full-stack co-design approach with the aforementioned methods can result in up to 88.7x end-to-end speedup with minimal performance degradation for Transformer inference. More details can be found in our full paper, which includes (1) a comprehensive analysis of Transformer workloads, (2) an extensive survey of the current hardware and software solutions on efficient Transformer inference, and (3) case studies to quantify the advantages of co-design and co-optimization techniques across the stack on full-stack Transformer inference."}}
{"id": "9BwwK4pDcHW", "cdate": 1672531200000, "mdate": 1682317921785, "content": {"title": "Full Stack Optimization of Transformer Inference: a Survey", "abstract": "Recent advances in state-of-the-art DNN architecture design have been moving toward Transformer models. These models achieve superior accuracy across a wide range of applications. This trend has been consistent over the past several years since Transformer models were originally introduced. However, the amount of compute and bandwidth required for inference of recent Transformer models is growing at a significant rate, and this has made their deployment in latency-sensitive applications challenging. As such, there has been an increased focus on making Transformer models more efficient, with methods that range from changing the architecture design, all the way to developing dedicated domain-specific accelerators. In this work, we survey different approaches for efficient Transformer inference, including: (i) analysis and profiling of the bottlenecks in existing Transformer architectures and their similarities and differences with previous convolutional models; (ii) implications of Transformer architecture on hardware, including the impact of non-linear operations such as Layer Normalization, Softmax, and GELU, as well as linear operations, on hardware design; (iii) approaches for optimizing a fixed Transformer architecture; (iv) challenges in finding the right mapping and scheduling of operations for Transformer models; and (v) approaches for optimizing Transformer models by adapting the architecture using neural architecture search. Finally, we perform a case study by applying the surveyed optimizations on Gemmini, the open-source, full-stack DNN accelerator generator, and we show how each of these approaches can yield improvements, compared to previous benchmark results on Gemmini. Among other things, we find that a full-stack co-design approach with the aforementioned methods can result in up to 88.7x speedup with a minimal performance degradation for Transformer inference."}}
{"id": "HEWao_L2IP_", "cdate": 1651846518136, "mdate": null, "content": {"title": "Autophase V2: Towards Function Level Phase Ordering Optimization", "abstract": "Compilers are equipped with optimization passes that can be applied to improve the quality of a program. The selection and ordering of these passes is a classic NP-hard problem known as the phase ordering problem. Traditionally, compilers use expert-picked sequences to optimize for performance (e.g. -O2, -O3), or code size (e.g. -Os, -Oz). However, not all programs respond positively to all optimizations, and prior work has shown that these expert-picked phase orders can be outperformed by tailoring the phase order decisions to individual programs.\nIn this work, we propose further specializing the phase ordering for each function. We investigate the impact of various function and module passes and show that function-specific phase ordering provides an extra 2.3% improvement in code size reduction compared to program-specific phase ordering using a cheap search budget. Compared to using the Oz flag, the deep reinforcement learning method achieves up to 9% more code size reduction in the same dataset, and up to 6% improvement when transferring to new unseen data. Our exhaustive search experiment shows that searching on different levels of abstraction will be beneficial for solving the phase ordering problem. However, we note several limitations with our reinforcement learning approach, the observation features are not sufficient to give better generalization, the reward has high variance and the datasets are not well representative for all programs."}}
{"id": "blzDmHa2fJ", "cdate": 1640995200000, "mdate": 1682317921804, "content": {"title": "Learning A Continuous and Reconstructible Latent Space for Hardware Accelerator Design", "abstract": "The hardware design space is high-dimensional and discrete. Systematic and efficient exploration of this space has been a significant challenge. Central to this problem is the intractable search complexity that grows exponentially with the design choices and the discrete nature of the search space. This work investigates the feasibility of learning a meaningful low-dimensional continuous representation for hardware designs to reduce such complexity and facilitate the search process. We devise a variational autoencoder (VAE)-based design space exploration framework called VAESA, to encode the hardware design space in a compact and continuous representation. We show that black-box and gradient-based design space exploration algorithms can be applied to the latent space, and design points optimized in the latent space can be reconstructed to high-performance realistic hardware designs. Our experiments show that performing the design space search on the latent space consistently leads to the optimal design point under a fixed number of samples. In addition, the latent space can improve the sample efficiency of the original algorithm by 6.8$\\times$ and can discover hardware designs that are up to 5% more efficient than the optimal design searched directly in the high-dimensional input space."}}
{"id": "y2XPEbq64r", "cdate": 1609459200000, "mdate": 1682317922049, "content": {"title": "CoSA: Scheduling by Constrained Optimization for Spatial Accelerators", "abstract": "Recent advances in Deep Neural Networks (DNNs) have led to active development of specialized DNN accelerators, many of which feature a large number of processing elements laid out spatially, together with a multi-level memory hierarchy and flexible interconnect. While DNN accelerators can take advantage of data reuse and achieve high peak throughput, they also expose a large number of runtime parameters to the programmers who need to explicitly manage how computation is scheduled both spatially and temporally. In fact, different scheduling choices can lead to wide variations in performance and efficiency, motivating the need for a fast and efficient search strategy to navigate the vast scheduling space.To address this challenge, we present CoSA, a constrained-optimization-based approach for scheduling DNN accelerators. As opposed to existing approaches that either rely on designers\u2019 heuristics or iterative methods to navigate the search space, CoSA expresses scheduling decisions as a constrained-optimization problem that can be deterministically solved using mathematical optimization techniques. Specifically, CoSA leverages the regularities in DNN operators and hardware to formulate the DNN scheduling space into a mixed-integer programming (MIP) problem with algorithmic and architectural constraints, which can be solved to automatically generate a highly efficient schedule in one shot. We demonstrate that CoSA-generated schedules significantly outperform state-of-the-art approaches by a geometric mean of up to 2.5\u00d7 across a wide range of DNN networks while improving the time-to-solution by 90\u00d7."}}
{"id": "bFU694ZVk_f", "cdate": 1609459200000, "mdate": null, "content": {"title": "CoDeNet: Efficient Deployment of Input-Adaptive Object Detection on Embedded FPGAs", "abstract": "Deploying deep learning models on embedded systems for computer vision tasks has been challenging due to limited compute resources and strict energy budgets. The majority of existing work focuses on accelerating image classification, while other fundamental vision problems, such as object detection, have not been adequately addressed. Compared with image classification, detection problems are more sensitive to the spatial variance of objects, and therefore, require specialized convolutions to aggregate spatial information. To address this need, recent work introduces dynamic deformable convolution to augment regular convolutions. Regular convolutions process a fixed grid of pixels across all the spatial locations in an image, while dynamic deformable convolution may access arbitrary pixels in the image with the access pattern being input-dependent and varying with spatial location. These properties lead to inefficient memory accesses of inputs with existing hardware. In this work, we harness the flexibility of FPGAs to develop a novel object detection pipeline with deformable convolutions. We show the speed-accuracy tradeoffs for a set of algorithm modifications including irregular-access versus limited-range and fixed-shape on a flexible hardware accelerator. We evaluate these algorithmic changes with corresponding hardware optimizations and show a 1.36x and 9.76x speedup respectively for the full and depthwise deformable convolution on hardware with minor accuracy loss. We then co-design a network called CoDeNet with the modified deformable convolution for object detection and quantize the network to 4-bit weights and 8-bit activations. With our high-efficiency implementation, our solution reaches 26.9 frames per second with a tiny model size of 0.76 MB while achieving 61.7 AP50 on the standard object detection dataset, Pascal VOC. With our higher-accuracy implementation, our model gets to 67.1 AP50 on Pascal VOC with only 2.9 MB of parameters--20.9x smaller but 10% more accurate than Tiny-YOLO."}}
{"id": "NtP0V-HX8U", "cdate": 1609459200000, "mdate": 1668211345035, "content": {"title": "HAO: Hardware-aware Neural Architecture Optimization for Efficient Inference", "abstract": "Automatic algorithm-hardware co-design for DNN has shown great success in improving the performance of DNNs on FPGAs. However, this process remains challenging due to the intractable search space of neural network architectures and hardware accelerator implementation. Differing from existing hardware-aware neural architecture search (NAS) algorithms that rely solely on the expensive learning-based approaches, our work incorporates integer programming into the search algorithm to prune the design space. Given a set of hardware resource constraints, our integer programming formulation directly outputs the optimal accelerator configuration for mapping a DNN subgraph that minimizes latency. We use an accuracy predictor for different DNN subgraphs with different quantization schemes and generate accuracy-latency pareto frontiers. With low computational cost, our algorithm can generate quantized networks that achieve state-of-the-art accuracy and hardware performance on Xilinx Zynq (ZU3EG) FPGA for image classification on ImageNet dataset. The solution searched by our algorithm achieves 72.5% top-1 accuracy on ImageNet at framerate 50, which is 60% faster than MnasNet [37] and 135% faster than FBNet [43] with comparable accuracy."}}
{"id": "9CO5pp0c5t", "cdate": 1609459200000, "mdate": 1682317922005, "content": {"title": "HAWQ-V3: Dyadic Neural Network Quantization", "abstract": "Current low-precision quantization algorithms often have the hidden cost of conversion back and forth from floating point to quantized integer values. This hidden cost limits the latency improvemen..."}}
{"id": "qu8kT4K5Ws", "cdate": 1577836800000, "mdate": 1682317921796, "content": {"title": "AutoPhase: Juggling HLS Phase Orderings in Random Forests with Deep Reinforcement Learning", "abstract": ""}}
