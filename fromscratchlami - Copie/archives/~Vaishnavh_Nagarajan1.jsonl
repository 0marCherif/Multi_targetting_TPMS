{"id": "QcA9iGaLpH4", "cdate": 1663850481146, "mdate": null, "content": {"title": "What do large networks memorize?", "abstract": "The success of modern neural models has prompted renewed study of the connection between memorisation and generalisation: such models typically generalise well, despite being able to perfectly fit (\"memorise\") completely random labels.\nTo more carefully study this issue, Feldman (2019); Feldman & Zhang (2020) provided a simple metric to quantify the degree of memorisation of a specific training example, and empirically quantified the corresponding memorisation profile of a ResNet model on image classification benchmarks.\nWhile an exciting first glimpse into how real-world models memorise, these studies leave open several questions about memorisation of practical networks.\nIn particular, how is memorisation affected by increasing model size, and by distilling a large model into a smaller one?\nWe present a systematic empirical analysis of these questions.\nOn standard image classification benchmarks, we find that training examples exhibit a diverse set of memorisation trajectories across model sizes, with some samples having increased memorisation under larger models.\nFurther, we find that distillation tends to inhibit memorisation of the student model, while also improving generalisation.\nFinally, we show that computationally tractable measures of memorisation do not capture the properties we identify for memorisation in the sense of Feldman (2019), despite highly correlating to the latter. "}}
{"id": "xJz9LTHP0K", "cdate": 1663850262770, "mdate": null, "content": {"title": "On student-teacher deviations in distillation: does it pay to disobey?", "abstract": "Knowledge distillation has been widely-used to improve the performance of a ``student'' network by hoping to mimic the soft probabilities of a  ``teacher'' network. Yet, for self-distillation to work, the student {\\em must} deviate from the teacher in some manner \\citep{stanton21does}. We conduct a variety of experiments across image and language classification datasets to more precisely understand the nature of student-teacher deviations and how they relate to accuracy gains. Our first key empirical observation is that in a majority of our settings, the student underfits points that the teacher finds hard. Next, we find that student-teacher deviations during the \\textit{initial} phase training are \\textit{not} crucial to get the benefits of distillation --- simply switching to distillation in the middle of training can recover a significant fraction of distillation's accuracy gains.\nWe then provide two parallel theoretical perspectives of student-teacher deviations, one casting distillation as a regularizer in eigenspace, and another as a denoiser of gradients. In both these views, we argue how our empirically reported student-teacher deviations may emerge, and how they may relate to generalization. Importantly, our analysis bridges key gaps between existing theory and practice by focusing on gradient descent and avoiding label noise assumptions."}}
{"id": "xifR-LmUHC7", "cdate": 1633790969676, "mdate": null, "content": {"title": "Avoiding Spurious Correlations: Bridging Theory and Practice", "abstract": "Distribution shifts in the wild jeopardize the performance of machine learning models as they tend to pick up spurious correlations during training. Recent work (Nagarajan et al., 2020) has characterized two specific failure modes of out-of-distribution (OOD) generalization, and we extend this theoretical framework by interpreting existing algorithms as solutions to these failure modes. We then evaluate them on different image classification datasets, and in the process surface two issues that are central to existing robustness techniques. For the algorithms that require access to group information, we demonstrate how the existing annotations included in standard OOD benchmarks are unable to fully capture the spurious correlations present. For methods that don't rely on group annotations during training, the validation set they utilize for model selection carries assumptions that are not realistic in real-world settings. This leads us to explore how the choice of distribution shifts represented by validation data would affect the effectiveness of different OOD robustness algorithms."}}
{"id": "WvOGCEAQhxl", "cdate": 1632875471178, "mdate": null, "content": {"title": "Assessing Generalization of SGD via Disagreement", "abstract": "We empirically show that the test error of deep networks can be estimated by training the same architecture on the same training set but with two different runs of Stochastic Gradient Descent (SGD), and then measuring the disagreement rate between the two networks on unlabeled test data. This builds on -- and is a stronger version of -- the observation in Nakkiran&Bansal 20, which requires the runs to be on separate training sets. We further theoretically show that this peculiar phenomenon arises from the well-calibrated nature of ensembles of SGD-trained models. This finding not only provides a simple empirical measure to directly predict the test error using unlabeled test data, but also establishes a new conceptual connection between generalization and calibration."}}
{"id": "fSTD6NFIW_b", "cdate": 1601308250388, "mdate": null, "content": {"title": "Understanding the failure modes of out-of-distribution generalization", "abstract": "Empirical studies suggest that machine learning models often rely on features, such as the background, that may be spuriously correlated with the label only during training time, resulting in poor accuracy during test-time. In this work, we identify the fundamental factors that give rise to this behavior, by explaining why models fail this way even in easy-to-learn tasks where one would expect these models to succeed. In particular, through a theoretical study of gradient-descent-trained linear classifiers on some easy-to-learn tasks, we uncover two complementary failure modes. These modes arise from how spurious correlations induce two kinds of skews in the data: one geometric in nature and another, statistical. Finally, we construct natural modifications of image classification datasets to understand when these failure modes can arise in practice. We also design experiments to isolate the two failure modes when training modern neural networks on these datasets."}}
{"id": "7aL-OtQrBWD", "cdate": 1601308211045, "mdate": null, "content": {"title": "A Learning Theoretic Perspective on Local Explainability", "abstract": "In this paper, we explore connections between interpretable machine learning and learning theory through the lens of local approximation explanations. First, we tackle the traditional problem of performance generalization and bound the test-time predictive accuracy of a model using a notion of how locally explainable it is.  Second, we explore the novel problem of explanation generalization which is an important concern for a growing class of finite sample-based local approximation explanations. Finally, we validate our theoretical results empirically and show that they reflect what can be seen in practice."}}
{"id": "FravveXBzzv", "cdate": 1577836800000, "mdate": null, "content": {"title": "Provably Safe PAC-MDP Exploration Using Analogies", "abstract": "A key challenge in applying reinforcement learning to safety-critical domains is understanding how to balance exploration (needed to attain good performance on the task) with safety (needed to avoid catastrophic failure). Although a growing line of work in reinforcement learning has investigated this area of \"safe exploration,\" most existing techniques either 1) do not guarantee safety during the actual exploration process; and/or 2) limit the problem to a priori known and/or deterministic transition dynamics with strong smoothness assumptions. Addressing this gap, we propose Analogous Safe-state Exploration (ASE), an algorithm for provably safe exploration in MDPs with unknown, stochastic dynamics. Our method exploits analogies between state-action pairs to safely learn a near-optimal policy in a PAC-MDP sense. Additionally, ASE also guides exploration towards the most task-relevant states, which empirically results in significant improvements in terms of sample efficiency, when compared to existing methods."}}
{"id": "rygHFBSg8S", "cdate": 1567802748904, "mdate": null, "content": {"title": "Uniform convergence may be unable to explain generalization in deep learning", "abstract": "We cast doubt on the power of uniform convergence-based generalization bounds to provide a complete picture of why overparameterized deep networks generalize well.  While it is well-known that many existing uniform convergence-based bounds are numerically large, through a variety of  experiments, we first bring to light another crucial and more concerning aspect of these bounds:  in practice,  these bounds can {\\em increase} with the dataset size. Guided by our observations, we then present examples of overparameterized linear classifiers and neural networks trained by  gradient descent (GD) where uniform convergence provably cannot ``explain generalization,'' even if we take into account  implicit regularization by GD {\\em to the fullest extent possible}. More precisely, even if we consider only the set of classifiers output by GD that have test errors less than some small $\\epsilon$, applying (two-sided) uniform convergence on this set of classifiers yields a generalization guarantee  that is larger than $1-\\epsilon$ and is therefore nearly vacuous. "}}
{"id": "JgfTeGa7ZFs", "cdate": 1546300800000, "mdate": null, "content": {"title": "Generalization in Deep Networks: The Role of Distance from Initialization", "abstract": "Why does training deep neural networks using stochastic gradient descent (SGD) result in a generalization error that does not worsen with the number of parameters in the network? To answer this question, we advocate a notion of effective model capacity that is dependent on {\\em a given random initialization of the network} and not just the training algorithm and the data distribution. We provide empirical evidences that demonstrate that the model capacity of SGD-trained deep networks is in fact restricted through implicit regularization of {\\em the $\\ell_2$ distance from the initialization}. We also provide theoretical arguments that further highlight the need for initialization-dependent notions of model capacity. We leave as open questions how and why distance from initialization is regularized, and whether it is sufficient to explain generalization."}}
{"id": "Hygn2o0qKX", "cdate": 1538087859734, "mdate": null, "content": {"title": "Deterministic PAC-Bayesian generalization bounds for deep networks via generalizing noise-resilience", "abstract": "The ability of overparameterized deep networks to generalize well has been linked to the fact that stochastic gradient descent (SGD) finds solutions that lie in flat, wide minima in the training loss -- minima where the output of the network is resilient to small random noise added to its parameters. \nSo far this observation has been used to provide generalization guarantees only for neural networks whose parameters are either \\textit{stochastic} or \\textit{compressed}. In this work, we present a general PAC-Bayesian framework that leverages this observation to provide a bound on the original network learned -- a network that is deterministic and uncompressed.  What enables us to do this is a key novelty in our approach: our framework allows us to show that if on training data, the interactions between the weight matrices satisfy certain conditions that imply a wide training loss minimum, these conditions themselves {\\em generalize} to the interactions between the matrices on test data, thereby implying a wide test loss minimum. We then apply our general framework in a setup where we assume that the pre-activation values of the network are not too small (although we assume this only on the training data). In this setup, we provide a generalization guarantee for the original (deterministic, uncompressed) network, that does not scale with product of the spectral norms of the weight matrices -- a guarantee that would not have been possible with prior approaches."}}
