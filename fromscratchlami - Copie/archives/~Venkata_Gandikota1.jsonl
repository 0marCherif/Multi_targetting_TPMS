{"id": "weWbPUIMSq", "cdate": 1664731444035, "mdate": null, "content": {"title": "Optimization using Parallel Gradient Evaluations on Multiple Parameters", "abstract": "We propose a first-order method for convex optimization, where instead of being restricted to the gradient from a single parameter, gradients from multiple parameters can be used during each step of gradient descent. This setup is particularly useful when a few processors are available that can be used in parallel for optimization. Our method uses gradients from multiple parameters in synergy to update these parameters together towards the optima. While doing so, it is ensured that the computational and memory complexity is of the same order as that of gradient descent. Empirical results demonstrate that even using gradients from as low as \\textit{two} parameters, our method can often obtain significant acceleration and provide robustness to hyper-parameter settings. We remark that the primary goal of this work is less theoretical, and is instead aimed at exploring the understudied case of using multiple gradients during each step of optimization."}}
{"id": "8zeX-8yLeRb", "cdate": 1621630091007, "mdate": null, "content": {"title": "Support Recovery of Sparse Signals from a Mixture of Linear Measurements", "abstract": "Recovery of support of a sparse vector from simple measurements is a widely studied problem, considered under the frameworks of compressed sensing, 1-bit compressed sensing, and more general single index models. We consider generalizations of this problem: mixtures of linear regressions, and mixtures of linear classifiers, where the goal is to recover supports of multiple sparse vectors using only a small number of possibly noisy linear, and 1-bit measurements respectively. The key challenge is that the measurements from different vectors are randomly mixed. Both of these problems have also received attention recently. In mixtures of linear classifiers, an observation corresponds to the side of the queried hyperplane a random unknown vector lies in; whereas in mixtures of linear regressions we observe the projection of a random unknown vector on the queried hyperplane. The primary step in recovering the unknown vectors from the mixture is to first identify the support of all the individual component vectors. In this work, we study the number of measurements sufficient for recovering the supports of all the component vectors in a mixture in both these models. We provide algorithms that use a number of measurements polynomial in $k, \\log n$ and quasi-polynomial in $\\ell$, to recover the support of all the $\\ell$ unknown vectors in the mixture with high probability when each individual component is a $k$-sparse $n$-dimensional vector."}}
{"id": "-ioMuxJ6ud9", "cdate": 1621630091007, "mdate": null, "content": {"title": "Support Recovery of Sparse Signals from a Mixture of Linear Measurements", "abstract": "Recovery of support of a sparse vector from simple measurements is a widely studied problem, considered under the frameworks of compressed sensing, 1-bit compressed sensing, and more general single index models. We consider generalizations of this problem: mixtures of linear regressions, and mixtures of linear classifiers, where the goal is to recover supports of multiple sparse vectors using only a small number of possibly noisy linear, and 1-bit measurements respectively. The key challenge is that the measurements from different vectors are randomly mixed. Both of these problems have also received attention recently. In mixtures of linear classifiers, an observation corresponds to the side of the queried hyperplane a random unknown vector lies in; whereas in mixtures of linear regressions we observe the projection of a random unknown vector on the queried hyperplane. The primary step in recovering the unknown vectors from the mixture is to first identify the support of all the individual component vectors. In this work, we study the number of measurements sufficient for recovering the supports of all the component vectors in a mixture in both these models. We provide algorithms that use a number of measurements polynomial in $k, \\log n$ and quasi-polynomial in $\\ell$, to recover the support of all the $\\ell$ unknown vectors in the mixture with high probability when each individual component is a $k$-sparse $n$-dimensional vector."}}
{"id": "OOhLBCrZQww", "cdate": 1577836800000, "mdate": null, "content": {"title": "Reliable Distributed Clustering with Redundant Data Assignment.", "abstract": "In this paper, we present distributed generalized clustering algorithms that can handle large scale data across multiple machines in spite of straggling or unreliable machines. We propose a novel data assignment scheme that enables us to obtain global information about the entire data even when some machines fail to respond with the results of the assigned local computations. The assignment scheme leads to distributed algorithms with good approximation guarantees for a variety of clustering and dimensionality reduction problems."}}
{"id": "c_cilJm8Vr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Superset Technique for Approximate Recovery in One-Bit Compressed Sensing.", "abstract": "One-bit compressed sensing (1bCS) is a method of signal acquisition under extreme measurement quantization that gives important insights on the limits of signal compression and analog-to-digital conversion. The setting is also equivalent to the problem of learning a sparse hyperplane-classifier. In this paper, we propose a generic approach for signal recovery in nonadaptive 1bCS that leads to improved sample complexity for approximate recovery for a variety of signal models, including nonnegative signals and binary signals. We construct 1bCS matrices that are universal - i.e. work for all signals under a model - and at the same time recover very general random sparse signals with high probability. In our approach, we divide the set of samples (measurements) into two parts, and use the first part to recover the superset of the support of a sparse vector. The second set of measurements is then used to approximate the signal within the superset. While support recovery in 1bCS is well-studied, recovery of superset of the support requires fewer samples, which then leads to an overall reduction in sample complexity for approximate recovery."}}
{"id": "WyK9rQqYy2", "cdate": 1546300800000, "mdate": null, "content": {"title": "Superset Technique for Approximate Recovery in One-Bit Compressed Sensing.", "abstract": "One-bit compressed sensing (1bCS) is a method of signal acquisition under extreme measurement quantization that gives important insights on the limits of signal compression and analog-to-digital conversion. The setting is also equivalent to the problem of learning a sparse hyperplane-classifier. In this paper, we propose a novel approach for signal recovery in nonadaptive 1bCS that matches the sample complexity of the current best methods. We construct 1bCS matrices that are universal - i.e. work for all signals under a model - and at the same time recover very general random sparse signals with high probability. In our approach, we divide the set of samples (measurements) into two parts, and use the first part to recover the superset of the support of a sparse vector. The second set of measurements is then used to approximate the signal within the superset. While support recovery in 1bCS is well-studied, recovery of superset of the support requires fewer samples, and to our knowledge has not been previously considered for the purpose of approximate recovery of signals."}}
{"id": "2vOq9fBW9hm", "cdate": 1546300800000, "mdate": null, "content": {"title": "Relaxed Locally Correctable Codes in Computationally Bounded Channels.", "abstract": "Error-correcting codes that admit local decoding and correcting algorithms have been the focus of much recent research due to their numerous applications. An important goal is to obtain the best possible tradeoffs between the number of symbols of the codeword that the local decoding algorithm must examine (the locality of the task), and the amount of redundancy in the encoding (the information rate).In Hamming\u2019s classical adversarial channel model, the current tradeoffs are dramatic, allowing either small locality, but superpolynomial blocklength, or small blocklength, but high locality. However, in the computationally bounded, adversarial channel model, proposed by Lipton (STACS 1994), constructions of locally decodable codes suddenly exhibit small locality and small blocklength, but these constructions require strong trusted setup assumptions e.g., Ostrovsky, Pandey and Sahai (ICALP 2007) construct private locally decodable codes in the setting where the sender and receiver already share a symmetric key.We study variants of locally decodable and locally correctable codes in computationally bounded, adversarial channels, in a setting with no public-key or private-key cryptographic setup. The only setup assumption we require is the selection of the public parameters (seed) for a collision-resistant hash function. Specifically, we provide constructions of relaxed locally correctable and relaxed locally decodable codes over the binary alphabet, with constant information rate, and poly-logarithmic locality.Our constructions, which compare favorably with their classical analogs, crucially employ collision-resistant hash functions and local expander graphs, extending ideas from recent cryptographic constructions of memory-hard functions."}}
{"id": "2RauFMrJhW7", "cdate": 1546300800000, "mdate": null, "content": {"title": "vqSGD: Vector Quantized Stochastic Gradient Descent.", "abstract": "In this work, we present a family of vector quantization schemes \\emph{vqSGD} (Vector-Quantized Stochastic Gradient Descent) that provide an asymptotic reduction in the communication cost with convergence guarantees in first-order distributed optimization. In the process we derive the following fundamental information theoretic fact: $\\Theta(\\frac{d}{R^2})$ bits are necessary and sufficient to describe an unbiased estimator ${\\hat{g}}({g})$ for any ${g}$ in the $d$-dimensional unit sphere, under the constraint that $\\|{\\hat{g}}({g})\\|_2\\le R$ almost surely. In particular, we consider a randomized scheme based on the convex hull of a point set, that returns an unbiased estimator of a $d$-dimensional gradient vector with almost surely bounded norm. We provide multiple efficient instances of our scheme, that are near optimal, and require only $o(d)$ bits of communication at the expense of tolerable increase in error. The instances of our quantization scheme are obtained using the properties of binary error-correcting codes and provide a smooth tradeoff between the communication and the estimation error of quantization. Furthermore, we show that \\emph{vqSGD} also offers strong privacy guarantees."}}
{"id": "28fIj0DD_W", "cdate": 1546300800000, "mdate": null, "content": {"title": "Nearly Optimal Sparse Group Testing.", "abstract": "Group testing is the process of pooling arbitrary subsets from a set of n items so as to identify, with a minimal number of tests, a \u201csmall\u201d subset of d defective items. In \u201cclassical\u201d non-adaptive group testing, it is known that when d is substantially smaller than n, \u0398(dlog(n)) tests are both information-theoretically necessary and sufficient to guarantee recovery with high probability. Group testing schemes in the literature that meet this bound require most items to be tested \u03a9(log(n)) times, and most tests to incorporate \u03a9(n/d) items. Motivated by physical considerations, we study group testing models in which the testing procedure is constrained to be \u201csparse.\u201d Specifically, we consider (separately) scenarios in which 1) items are finitely divisible and hence may participate in at most \u03b3 \u2208 o(log(n)) tests; or 2) tests are size-constrained to pool no more than \u03c1 \u2208 o(n/d) items per test. For both scenarios, we provide information-theoretic lower bounds on the number of tests required to guarantee high probability recovery. In particular, one of our main results shows that \u03b3-finite divisibility of items forces any non-adaptive group testing algorithm with the probability of recovery error at most \u03f5 to perform at least \u03b3d(n/d) <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">(1-5\u03f5)/\u03b3</sup> tests. Analogously, for \u03c1-sized constrained tests, we show an information-theoretic lower bound of \u03a9(n/\u03c1) tests for high-probability recovery-hence in both settings the number of tests required grows dramatically (relative to the classical setting) as a function of n. In both scenarios, we provide both randomized constructions and explicit constructions of designs with computationally efficient reconstruction algorithms that require a number of tests that is optimal up to constant or small polynomial factors in some regimes of n, d, \u03b3, and \u03c1. The randomized design/reconstruction algorithm in the \u03c1-sized test scenario is universal-independent of the value of d, as long as \u03c1 \u2208 o(n/d). We also investigate the effect of unreliability/noise in test outcomes, and show that whereas the impact of noise in test outcomes can be obviated with a small (constant factor) penalty in the number of tests in the \u03c1-sized tests scenario, there is no group-testing procedure, regardless of the number of tests, that can combat noise in the \u03b3-divisible scenario."}}
{"id": "wGLbT6RxQ9G", "cdate": 1514764800000, "mdate": null, "content": {"title": "NP-Hardness of Reed-Solomon Decoding, and the Prouhet-Tarry-Escott Problem.", "abstract": "Establishing the complexity of bounded distance decoding for Reed--Solomon codes is a fundamental open problem in coding theory, explicitly asked by Guruswami and Vardy [IEEE Trans. Inform. Theory, 51 (2005), pp. 2249--2256]. The problem is motivated by the large current gap between the regime when it is NP-hard and the regime when it is efficiently solvable (i.e., the Johnson radius). We show the first NP-hardness results for asymptotically smaller decoding radii than the maximum likelihood decoding radius of Guruswami and Vardy. Specifically, for Reed--Solomon codes of length $N$ and dimension $K=\\Theta(N)$, we show that it is NP-hard to decode more than $ N-K- c\\frac{\\log N}{\\log\\log N}$ errors (with $c>0$ an absolute constant). Moreover, we show that the problem is NP-hard under quasi-polynomial-time reductions for an error amount $> N-K- c\\log{N}$ (with $c>0$ an absolute constant). An alternative natural reformulation of the bounded distance decoding problem for Reed--Solomon codes is as a polynomial reconstruction problem. In this view, our results show that it is NP-hard to decide whether there exists a degree $K$ polynomial passing through $K+ c\\frac{\\log N}{\\log\\log N}$ points from a given set of points $(a_1, b_1), (a_2, b_2)\\ldots, (a_N, b_N)$. Furthermore, it is NP-hard under quasi-polynomial-time reductions to decide whether there is a degree $K$ polynomial passing through $K+c\\log{N}$ many points. These results follow from the NP-hardness of a generalization of the classical subset sum problem to higher moments, called moments subset sum, which has been a known open problem, and which may be of independent interest. We further reveal a strong connection with the well-studied Prouhet--Tarry--Escott problem in number theory, which turns out to capture a main barrier in extending our techniques. We believe the Prouhet--Tarry--Escott problem deserves further study in the theoretical computer science community."}}
