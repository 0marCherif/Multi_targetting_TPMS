{"id": "u0vhguBOgvt", "cdate": 1672531200000, "mdate": 1680531683208, "content": {"title": "The unreasonable effectiveness of few-shot learning for machine translation", "abstract": ""}}
{"id": "1mLs3Sm6WeI", "cdate": 1672531200000, "mdate": 1681958688213, "content": {"title": "Document Flattening: Beyond Concatenating Context for Document-Level Neural Machine Translation", "abstract": "Existing work in document-level neural machine translation commonly concatenates several consecutive sentences as a pseudo-document, and then learns inter-sentential dependencies. This strategy limits the model's ability to leverage information from distant context. We overcome this limitation with a novel Document Flattening (DocFlat) technique that integrates Flat-Batch Attention (FBA) and Neural Context Gate (NCG) into Transformer model to utilize information beyond the pseudo-document boundaries. FBA allows the model to attend to all the positions in the batch and learns the relationships between positions explicitly and NCG identifies the useful information from the distant context. We conduct comprehensive experiments and analyses on three benchmark datasets for English-German translation, and validate the effectiveness of two variants of DocFlat. Empirical results show that our approach outperforms strong baselines with statistical significance on BLEU, COMET and accuracy on the contrastive test set. The analyses highlight that DocFlat is highly effective in capturing the long-range information."}}
{"id": "ulgP33gmCD", "cdate": 1640995200000, "mdate": 1681958688175, "content": {"title": "Toward More Effective Human Evaluation for Machine Translation", "abstract": "Improvements in text generation technologies such as machine translation have necessitated more costly and time-consuming human evaluation procedures to ensure an accurate signal. We investigate a simple way to reduce cost by reducing the number of text segments that must be annotated in order to accurately predict a score for a complete test set. Using a sampling approach, we demonstrate that information from document membership and automatic metrics can help improve estimates compared to a pure random sampling baseline. We achieve gains of up to 20% in average absolute error by leveraging stratified sampling and control variates. Our techniques can improve estimates made from a fixed annotation budget, are easy to implement, and can be applied to any problem with structure similar to the one we study."}}
{"id": "RogmJoQxao", "cdate": 1640995200000, "mdate": 1680531683251, "content": {"title": "Prompting PaLM for Translation: Assessing Strategies and Performance", "abstract": ""}}
{"id": "NpmUz0Hzvd5", "cdate": 1640995200000, "mdate": 1680531683243, "content": {"title": "A Natural Diet: Towards Improving Naturalness of Machine Translation Output", "abstract": ""}}
{"id": "--vTgtgQTaA", "cdate": 1640995200000, "mdate": 1681958687968, "content": {"title": "Results of WMT22 Metrics Shared Task: Stop Using BLEU - Neural Metrics Are Better and More Robust", "abstract": "Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, Andr\u00e9 F. T. Martins. Proceedings of the Seventh Conference on Machine Translation (WMT). 2022."}}
{"id": "BsQ-dfw_Y3", "cdate": 1623855924494, "mdate": 1623855924494, "content": {"title": "Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation", "abstract": "Human evaluation of modern high-quality machine translation systems is a difficult problem, and there is increasing evidence that inadequate evaluation procedures can lead to erroneous conclusions. While there has been considerable research on human evaluation, the field still lacks a commonly-accepted standard procedure. As a step toward this goal, we propose an evaluation methodology grounded in explicit error analysis, based on the Multidimensional Quality Metrics (MQM) framework. We carry out the largest MQM research study to date, scoring the outputs of top systems from the WMT 2020 shared task in two language pairs using annotations provided by professional translators with access to full document context. We analyze the resulting data extensively, finding among other results a substantially different ranking of evaluated systems from the one established by the WMT crowd workers, exhibiting a clear preference for human over machine output. Surprisingly, we also find that automatic metrics based on pre-trained embeddings can outperform human crowd workers. We make our corpus publicly available for further research."}}
{"id": "bJZfkI-hXd-", "cdate": 1619888371574, "mdate": null, "content": {"title": "Assessing Reference-Free Peer Evaluation for Machine Translation", "abstract": "Reference-free evaluation has the potential to make machine translation evaluation substantially more scalable, allowing us to pivot easily to new languages or domains. It has been recently shown that the probabilities given by a large, multilingual model can achieve state of the art results when used as a reference-free metric. We experiment with various modifications to this model and demonstrate that by scaling it up we can match the performance of BLEU. We analyze various potential weaknesses of the approach and find that it is surprisingly robust and likely to offer reasonable performance across a broad spectrum of domains and different system qualities."}}
{"id": "xlXE11L0kHi", "cdate": 1609459200000, "mdate": null, "content": {"title": "Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation", "abstract": "Human evaluation of modern high-quality machine translation systems is a difficult problem, and there is increasing evidence that inadequate evaluation procedures can lead to erroneous conclusions. While there has been considerable research on human evaluation, the field still lacks a commonly-accepted standard procedure. As a step toward this goal, we propose an evaluation methodology grounded in explicit error analysis, based on the Multidimensional Quality Metrics (MQM) framework. We carry out the largest MQM research study to date, scoring the outputs of top systems from the WMT 2020 shared task in two language pairs using annotations provided by professional translators with access to full document context. We analyze the resulting data extensively, finding among other results a substantially different ranking of evaluated systems from the one established by the WMT crowd workers, exhibiting a clear preference for human over machine output. Surprisingly, we also find that automatic metrics based on pre-trained embeddings can outperform human crowd workers. We make our corpus publicly available for further research."}}
{"id": "vM6vzJv9FB", "cdate": 1609459200000, "mdate": 1681958688320, "content": {"title": "Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation", "abstract": "Human evaluation of modern high-quality machine translation systems is a difficult problem, and there is increasing evidence that inadequate evaluation procedures can lead to erroneous conclusions. While there has been considerable research on human evaluation, the field still lacks a commonly accepted standard procedure. As a step toward this goal, we propose an evaluation methodology grounded in explicit error analysis, based on the Multidimensional Quality Metrics (MQM) framework. We carry out the largest MQM research study to date, scoring the outputs of top systems from the WMT 2020 shared task in two language pairs using annotations provided by professional translators with access to full document context. We analyze the resulting data extensively, finding among other results a substantially different ranking of evaluated systems from the one established by the WMT crowd workers, exhibiting a clear preference for human over machine output. Surprisingly, we also find that automatic metrics based on pre-trained embeddings can outperform human crowd workers. We make our corpus publicly available for further research."}}
