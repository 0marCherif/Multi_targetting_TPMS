{"id": "uQzv2070At4", "cdate": 1640995200000, "mdate": 1664502935719, "content": {"title": "Computational-Statistical Gap in Reinforcement Learning", "abstract": "Reinforcement learning with function approximation has recently achieved tremendous results in applications with large state spaces. This empirical success has motivated a growing body of theoretic..."}}
{"id": "kK5uTE73lna", "cdate": 1640995200000, "mdate": 1664502935719, "content": {"title": "Realizable Learning is All You Need", "abstract": "The equivalence of realizable and agnostic learnability is a fundamental phenomenon in learning theory. With variants ranging from classical settings like PAC learning and regression to recent tren..."}}
{"id": "OSunXnn9T6", "cdate": 1640995200000, "mdate": 1664502935719, "content": {"title": "Learning what to remember", "abstract": "We consider a lifelong learning scenario in which a learner faces a neverending and arbitrary stream of facts and has to decide which ones to retain in its limited memory. We introduce a mathematic..."}}
{"id": "0VqaVU__tDb", "cdate": 1640995200000, "mdate": 1664502935716, "content": {"title": "Convergence of online k-means", "abstract": "We prove asymptotic convergence for a general class of k-means algorithms performed over streaming data from a distribution\u2013the centers asymptotically converge to the set of stationary points of the k-means objective function. To do so, we show that online k-means over a distribution can be interpreted as stochastic gradient descent with a stochastic learning rate schedule. Then, we prove convergence by extending techniques used in optimization literature to handle settings where center-specific learning rates may depend on the past trajectory of the centers."}}
{"id": "LKflsw1SGmb", "cdate": 1609459200000, "mdate": 1664502935720, "content": {"title": "On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift", "abstract": "Policy gradient methods are among the most effective methods in challenging reinforcement learning problems with large state and/or action spaces. However, little is known about even their most basic theoretical convergence properties, including: if and how fast they converge to a globally optimal solution or how they cope with approximation error due to using a restricted class of parametric policies. This work provides provable characterizations of the computational, approximation, and sample size properties of policy gradient methods in the context of discounted Markov Decision Processes (MDPs). We focus on both: \"tabular\" policy parameterizations, where the optimal policy is contained in the class and where we show global convergence to the optimal policy; and parametric policy classes (considering both log-linear and neural policy classes), which may not contain the optimal policy and where we provide agnostic learning results. One central contribution of this work is in providing approximation guarantees that are average case --- which avoid explicit worst-case dependencies on the size of state space --- by making a formal connection to supervised learning under distribution shift. This characterization shows an important interplay between estimation error, approximation error, and exploration (as characterized through a precisely defined condition number)."}}
{"id": "AEp0UE7sL3g", "cdate": 1609459200000, "mdate": 1664502935721, "content": {"title": "Bilinear Classes: A Structural Framework for Provable Generalization in RL", "abstract": "This work introduces Bilinear Classes, a new structural framework, which permit generalization in reinforcement learning in a wide variety of settings through the use of function approximation. The..."}}
{"id": "heVFRZokhTF", "cdate": 1577836800000, "mdate": null, "content": {"title": "Noise-tolerant, Reliable Active Classification with Comparison Queries", "abstract": "With the explosion of massive, widely available unlabeled data in the past years, finding label and time efficient, robust learning algorithms has become ever more important in theory and in practice. We study the paradigm of active learning, in which algorithms with access to large pools of data may adaptively choose what samples to label in the hope of exponentially increasing efficiency. By introducing comparisons, an additional type of query comparing two points, we provide the first time and query efficient algorithms for learning non-homogeneous linear separators robust to bounded (Massart) noise. We further provide algorithms for a generalization of the popular Tsybakov low noise condition, and show how comparisons provide a strong reliability guarantee that is often impractical or impossible with only labels - returning a classifier that makes no errors with high probability."}}
{"id": "Fyz4OluMbek", "cdate": 1577836800000, "mdate": 1664502935719, "content": {"title": "Optimality and Approximation with Policy Gradient Methods in Markov Decision Processes", "abstract": "Policy gradient (PG) methods are among the most effective methods in challenging reinforcement learning problems with large state and/or action spaces. However, little is known about even their most basic theoretical convergence properties, including: if and how fast they converge to a globally optimal solution (say with a sufficiently rich policy class); how they cope with approximation error due to using a restricted class of parametric policies; or their finite sample behavior. Such characterizations are important not only to compare these methods to their approximate value function counterparts (where such issues are relatively well understood, at least in the worst case), but also to help with more principled approaches to algorithm design. This work provides provable characterizations of computational, approximation, and sample size issues with regards to policy gradient methods in the context of discounted Markov Decision Processes (MDPs). We focus on both: 1) \u201ctabular\u201d policy parameterizations, where the optimal policy is contained in the class and where we show global convergence to the optimal policy, and 2) restricted policy classes, which may not contain the optimal policy and where we provide agnostic learning results. In the \\emph{tabular setting}, our main results are: 1) convergence rate to global optimum for direct parameterization and projected gradient ascent 2) an asymptotic convergence to global optimum for softmax policy parameterization and PG; and a convergence rate with additional entropy regularization, and 3) dimension-free convergence to global optimum for softmax policy parameterization and Natural Policy Gradient (NPG) method with exact gradients. In \\emph{function approximation}, we further analyze NPG with exact as well as inexact gradients under certain smoothness assumptions on the policy parameterization and establish rates of convergence in terms of the quality of the initial state distribution. One insight of this work is in formalizing how a favorable initial state distribution provides a means to circumvent worst-case exploration issues. Overall, these results place PG methods under a solid theoretical footing, analogous to the global convergence guarantees of iterative value function based algorithms."}}
{"id": "EqLPS1oJtjV", "cdate": 1577836800000, "mdate": null, "content": {"title": "Agnostic $Q$-learning with Function Approximation in Deterministic Systems: Near-Optimal Bounds on Approximation Error and Sample Complexity", "abstract": "The current paper studies the problem of agnostic $Q$-learning with function approximation in deterministic systems where the optimal $Q$-function is approximable by a function in the class $\\mathcal{F}$ with approximation error $\\delta \\ge 0$. We propose a novel recursion-based algorithm and show that if $\\delta = O\\left(\\rho/\\sqrt{\\dim_E}\\right)$, then one can find the optimal policy using $O(\\dim_E)$ trajectories, where $\\rho$ is the gap between the optimal $Q$-value of the best actions and that of the second-best actions and $\\dim_E$ is the Eluder dimension of $\\mathcal{F}$. Our result has two implications: \\begin{enumerate} \\item In conjunction with the lower bound in [Du et al., 2020], our upper bound suggests that the condition $\\delta = \\widetilde{\\Theta}\\left(\\rho/\\sqrt{\\dim_E}\\right)$ is necessary and sufficient for algorithms with polynomial sample complexity. \\item In conjunction with the obvious lower bound in the tabular case, our upper bound suggests that the sample complexity $\\widetilde{\\Theta}\\left(\\dim_E\\right)$ is tight in the agnostic setting. \\end{enumerate} Therefore, we help address the open problem on agnostic $Q$-learning proposed in [Wen and Van Roy, 2013]. We further extend our algorithm to the stochastic reward setting and obtain similar results."}}
{"id": "D9IPotkQUx0", "cdate": 1577836800000, "mdate": 1664502935718, "content": {"title": "Point Location and Active Learning: Learning Halfspaces Almost Optimally", "abstract": "Given a finite set X \u2282 R <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">d</sup> and a binary linear classifier c: R <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">d</sup> \u2192 {0,1}, how many queries of the form c(x) are required to learn the label of every point in X? Known as point location, this problem has inspired over 35 years of research in the pursuit of an optimal algorithm. Building on the prior work of Kane, Lovett, and Moran (ICALP 2018), we provide the first nearly optimal solution, a randomized linear decision tree of depth \u00d5(dlog(|X|)), improving on the previous best of \u00d5(d <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup> log(|X|)) from Ezra and Sharir (Discrete and Computational Geometry, 2019). As a corollary, we also provide the first nearly optimal algorithm for actively learning halfspaces in the membership query model. En route to these results, building on the work of Carlen, Lieb, and Loss (J. Geometric Analysis 2004), as well as Dvir, Saraf, and Wigderson (STOC 2014), we prove a novel characterization of Barthe's Theorem (Inventiones Mathematicae, 1998) of independent interest. In particular, we show that X may be transformed into approximate isotropic position if and only if there exists no k-dimensional subspace with more than a k/d-fraction of X, and provide a similar characterization for exact isotropic position. The below is an extended abstract. The full work can be found at https://arxiv.org/abs/2004.11380."}}
