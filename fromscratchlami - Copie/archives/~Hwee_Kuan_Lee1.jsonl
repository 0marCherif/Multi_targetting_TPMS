{"id": "E3gF8L-mmS3", "cdate": 1632875648487, "mdate": null, "content": {"title": "Use of small auxiliary networks and scarce data to improve the adversarial robustness of deep learning models", "abstract": "Deep Learning models for image classification are known to be vulnerable to adversarial examples. \nAdversarial training is one of the most effective ways to provide defense against such threats, however it is a cumbersome process which requires many data points and long computation times. \nIn a setting where only small amounts of data are available for this process, adversarial training may negatively impact the classification performance on clean images by overfitting on the small amount of data.\nThis would be undesirable, especially when a large pre-trained model with satisfactory performance on clean data is already available.\nWe propose a new strategy to make a previously-trained model more robust against adversarial attacks, using scarce data and without degrading its performance on clean samples.\nThe proposed strategy consists in freezing the parameters of the originally trained base model and adding small auxiliary networks along the architecture, which process the features to reduce the effect of any adversarial perturbation.\nThis method can be used to defend a model against any arbitrary attack.\nA practical advantage of using auxiliary networks is that no modifications on the originally trained base model is required. \nTherefore, it can serve as a patch or add on to fix large and expensive existing deep learning models with little additional resources.\nExperiments on the CIFAR10 dataset showed that using only $10\\%$ of the full training set, the proposed method was able to adequately defend the model against the AutoPGD attack while maintaining a classification accuracy on clean images outperforming the model with adversarial training by $7\\%$. Indeed, the proposed method still performs reasonably well compared to adversarial training using $1\\%$ of the full training set."}}
{"id": "KRKGJrbPcKE", "cdate": 1601308125806, "mdate": null, "content": {"title": "Distribution Based MIL Pooling Filters are Superior to Point Estimate Based Counterparts", "abstract": "Multiple instance learning (MIL) is a machine learning paradigm which learns the mapping between bags of instances and bag labels. There are different MIL tasks which can be solved by different MIL methods. One common component of all MIL methods is the MIL pooling filter, which obtains bag level representations from extracted features of instances. Here, we recommend and discuss a grouping scheme for MIL pooling filters: point estimate based pooling filters and distribution based pooling filters. The point estimate based pooling filters include the standard pooling filters, such as \u2018max\u2019, \u2018mean\u2019 and \u2018attention\u2019 pooling. The distribution based pooling filters include recently proposed \u2018distribution\u2019 pooling and newly designed \u2018distribution with attention\u2019 pooling. In this paper, we perform the first systematic analysis of different pooling filters. We theoretically showed that the distribution based pooling filters are superior to the point estimate based counterparts in terms of amount of information captured while obtaining bag level representations from extracted features. Then, we empirically study the performance of the 5 pooling filters, namely \u2018max\u2019, \u2018mean\u2019, \u2018attention\u2019, \u2018distribution\u2019 and \u2018distribution with attention\u2019, on distinct real world MIL tasks. We showed that the performance of different pooling filters are different for different MIL tasks. Moreover, consistent with our theoretical analysis, models with distribution based pooling filters almost always performed equal or better than that with point estimate based pooling filters."}}
{"id": "BkgWahEFvr", "cdate": 1569438904963, "mdate": null, "content": {"title": "Enhancing Transformation-Based Defenses Against Adversarial Attacks with a Distribution Classifier", "abstract": "Adversarial attacks on convolutional neural networks (CNN) have gained significant attention and there have been active research efforts on defense mechanisms. Stochastic input transformation methods have been proposed, where the idea is to recover the image from adversarial attack by random transformation, and to take the majority vote as consensus among the random samples. However, the transformation improves the accuracy on adversarial images at the expense of the accuracy on clean images. While it is intuitive that the accuracy on clean images would deteriorate, the exact mechanism in which how this occurs is unclear. In this paper, we study the distribution of softmax induced by stochastic transformations. We observe that with random transformations on the clean images, although the mass of the softmax distribution could shift to the wrong class, the resulting distribution of softmax could be used to correct the prediction. Furthermore, on the adversarial counterparts, with the image transformation, the resulting shapes of the distribution of softmax are similar to the distributions from the clean images. With these observations, we propose a method to improve existing transformation-based defenses. We train a separate lightweight distribution classifier to recognize distinct features in the distributions of softmax outputs of transformed images. Our empirical studies show that our distribution classifier, by training on distributions obtained from clean images only, outperforms majority voting for both clean and adversarial images. Our method is generic and can be integrated with existing transformation-based defenses."}}
{"id": "B1xIj3VYvr", "cdate": 1569438878096, "mdate": null, "content": {"title": "Weakly Supervised Clustering by Exploiting Unique Class Count", "abstract": "A weakly supervised learning based clustering framework is proposed in this paper. As the core of this framework, we introduce a novel multiple instance learning task based on a bag level label called unique class count (ucc), which is the number of unique classes among all instances inside the bag. In this task, no annotations on individual instances inside the bag are needed during training of the models. We mathematically prove that with a perfect ucc classifier, perfect clustering of individual instances inside the bags is possible even when no annotations on individual instances are given during training. We have constructed a neural network based ucc classifier and experimentally shown that the clustering performance of our framework with our weakly supervised ucc classifier is comparable to that of fully supervised learning models where labels for all instances are known. Furthermore, we have tested the applicability of our framework to a real world task of semantic segmentation of breast cancer metastases in histological lymph node sections and shown that the performance of our weakly supervised framework is comparable to the performance of a fully supervised Unet model."}}
{"id": "SJlp8sA5Y7", "cdate": 1538087764893, "mdate": null, "content": {"title": "An Efficient Network for Predicting Time-Varying Distributions", "abstract": "While deep neural networks have achieved groundbreaking prediction results in many tasks, there is a class of data where existing architectures are not optimal -- sequences of probability distributions. Performing forward prediction on sequences of distributions has many important applications. However, there are two main challenges in designing a network model for this task. First, neural networks are unable to encode distributions compactly as each node encodes just a real value. A recent work of Distribution Regression Network (DRN) solved this problem with a novel network that encodes an entire distribution in a single node, resulting in improved accuracies while using much fewer parameters than neural networks. However, despite its compact distribution representation, DRN does not address the second challenge, which is the need to model time dependencies in a sequence of distributions. In this paper, we propose our Recurrent Distribution Regression Network (RDRN) which adopts a recurrent architecture for DRN. The combination of compact distribution representation and shared weights architecture across time steps makes RDRN suitable for modeling the time dependencies in a distribution sequence. Compared to neural networks and DRN, RDRN achieves the best prediction performance while keeping the network compact."}}
{"id": "ByYPLJA6W", "cdate": 1518730189747, "mdate": null, "content": {"title": "Distribution Regression Network", "abstract": "We introduce our Distribution Regression Network (DRN) which performs regression from input probability distributions to output probability distributions. Compared to existing methods, DRN learns with fewer model parameters and easily extends to multiple input and multiple output distributions. On synthetic and real-world datasets, DRN performs similarly or better than the state-of-the-art. Furthermore, DRN generalizes the conventional multilayer perceptron (MLP). In the framework of MLP, each node encodes a real number, whereas in DRN, each node encodes a probability distribution. "}}
