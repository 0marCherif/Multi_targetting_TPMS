{"id": "a6NvoZ5DLoe", "cdate": 1663850230759, "mdate": null, "content": {"title": "DYNAMIC ENSEMBLE FOR PROBABILISTIC TIME- SERIES FORECASTING VIA DEEP REINFORCEMENT LEARNING", "abstract": "Ensembles from given base learners are known to be indispensable in improving accuracy for most of the prediction tasks, leading to numerous methods. However, the only ensembling strategies that have been considered for time series forecasting in the past have been static methods, ones that have access to the predictions of the base learners but not to the base learners themselves. In this paper, we propose a novel  \\textit{dynamic ensemble policy}, which, unlike static methods, uses the power of the ensemble to improve each of the base learners being ensembled by reducing the error accumulation of each base learner via consecutively feeding an ensembled sample to each base learner. To do so, we adopt a deep Reinforcement Learning (RL) framework with a Markov Decision Process (MDP) designed where the ensemble agent interacts with our environment (\\textit{TS-GYM}) from offline data. The output of our ensemble strategy is a single autoregressive forecaster that supports several desirable properties of uncertainty quantification and sample path, along with notable performance gain. The effectiveness of the proposed framework is demonstrated in multiple synthetic and real-world experiments."}}
{"id": "mbxz9Cjehr", "cdate": 1663850183504, "mdate": null, "content": {"title": "A CMDP-within-online framework for Meta-Safe Reinforcement Learning", "abstract": "Meta-reinforcement learning has widely been used as a learning-to-learn framework to solve unseen tasks with limited experience. However, the aspect of constraint violations has not been adequately addressed in the existing works, making their application restricted in real-world settings. In this paper, we study the problem of meta-safe reinforcement learning (meta-SRL) through the CMDP-within-online framework. We obtain task-averaged regret guarantees for the reward maximization (optimality gap) and constraint violations using gradient-based meta-learning and show that the task-averaged optimality gap and constraint satisfaction improve with task-similarity in the static environment, or task-relatedness in the changing environment. Several technical challenges arise when making this framework practical while still having strong theoretical guarantees. To address these challenges, we propose a meta-algorithm that performs inexact online learning on the upper bounds of intra-task optimality gap and constraint violations estimated by off-policy stationary distribution corrections. Furthermore, we enable the learning rates to be adapted for every task and extend our approach to settings with the dynamically changing task environments. Finally, experiments are conducted to demonstrate the effectiveness of our approach. The proposed theoretical framework is the first to handle the nonconvexity and stochastic nature of within-task CMDPs, while exploiting inter-task dependency for multi-task safe learning.\n"}}
{"id": "Fs1A2SE0tF6", "cdate": 1652662291281, "mdate": null, "content": {"title": "Beyond Exact Gradients: Convergence of Stochastic Soft-Max Policy Gradient Methods with Entropy Regularization", "abstract": "Entropy regularization is an efficient technique for encouraging exploration and preventing a premature convergence of (vanilla) policy gradient methods in reinforcement learning (RL). However, the theoretical understanding of entropy regularized RL algorithms has been limited. In this paper, we revisit the classical entropy regularized policy gradient methods with the soft-max policy parametrization, whose convergence has so far only been established assuming access to exact gradient oracles. To go beyond this scenario, we propose the first set of (nearly) unbiased stochastic policy gradient estimators with trajectory-level entropy regularization, with one being an unbiased visitation measure-based estimator and the other one being a nearly unbiased yet more practical trajectory-based estimator. We prove that although the estimators themselves are unbounded in general due to the additional logarithmic policy rewards introduced by the entropy term, the variances are uniformly bounded. We then propose a two-phase stochastic policy gradient (PG) algorithm that uses a large batch size in the first phase to overcome the challenge of the stochastic approximation due to the non-coercive landscape, and uses a small batch size in the second phase by leveraging the curvature information around the optimal policy. We establish a global optimality convergence result and a sample complexity of $\\tilde{O}(\\epsilon^{-2})$ for the proposed algorithm. Our result is the first global convergence and sample complexity results for the stochastic entropy-regularized vanilla PG method."}}
{"id": "4CpVbU1qTXu", "cdate": 1652662089595, "mdate": null, "content": {"title": "On the Global Convergence of Momentum-based Policy Gradient", "abstract": "Policy gradient (PG) methods are popular and efficient for large-scale reinforcement learning due to their relative stability and incremental nature. In recent years, the empirical success of PG methods has led to the development of a theoretical foundation for these methods. In this work, we generalize this line of research by studying the global convergence of stochastic PG methods with momentum terms, which have been demonstrated to be efficient recipes for improving PG methods. We study both the soft-max and the Fisher-non-degenerate policy parametrizations, and show that adding a momentum improves the global optimality sample complexity of vanilla PG methods by $\\tilde{O}(\\epsilon^{-1.5})$ and $\\tilde{O}(\\epsilon^{-1})$, respectively, where $\\epsilon>0$ is the target tolerance. Our work is the first one that obtains global convergence results for the momentum-based PG methods. For the generic Fisher-non-degenerate policy parametrizations, our result is the first single-loop and finite-batch PG algorithm achieving $\\tilde{O}(\\epsilon^{-3})$ global optimality sample complexity. Finally, as a by-product, our methods also provide general framework for analyzing the global convergence rates of stochastic PG methods, which can be easily applied and extended to different PG estimators."}}
