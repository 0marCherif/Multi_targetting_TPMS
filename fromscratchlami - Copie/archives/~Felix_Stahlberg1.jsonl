{"id": "bCr0sEG6LSk", "cdate": 1640995200000, "mdate": 1682399384014, "content": {"title": "Jam or Cream First? Modeling Ambiguity in Neural Machine Translation with SCONES", "abstract": ""}}
{"id": "aDtmUo2or-", "cdate": 1640995200000, "mdate": 1682399384044, "content": {"title": "Uncertainty Determines the Adequacy of the Mode and the Tractability of Decoding in Sequence-to-Sequence Models", "abstract": ""}}
{"id": "B5Q_v0R20bR", "cdate": 1640995200000, "mdate": 1682399384063, "content": {"title": "Conciseness: An Overlooked Language Task", "abstract": "We report on novel investigations into training models that make sentences concise. We define the task and show that it is different from related tasks such as summarization and simplification. For evaluation, we release two test sets, consisting of 2000 sentences each, that were annotated by two and five human annotators, respectively. We demonstrate that conciseness is a difficult task for which zero-shot setups with large neural language models often do not perform well. Given the limitations of these approaches, we propose a synthetic data generation method based on round-trip translations. Using this data to either train Transformers from scratch or fine-tune T5 models yields our strongest baselines that can be further improved by fine-tuning on an artificial conciseness dataset that we derived from multi-annotator machine translation test sets."}}
{"id": "aOKqA4B8wRn", "cdate": 1577836800000, "mdate": null, "content": {"title": "Seq2Edits: Sequence Transduction Using Span-level Edit Operations", "abstract": "We propose Seq2Edits, an open-vocabulary approach to sequence editing for natural language processing (NLP) tasks with a high degree of overlap between input and output texts. In this approach, each sequence-to-sequence transduction is represented as a sequence of edit operations, where each operation either replaces an entire source span with target tokens or keeps it unchanged. We evaluate our method on five NLP tasks (text normalization, sentence fusion, sentence splitting & rephrasing, text simplification, and grammatical error correction) and report competitive results across the board. For grammatical error correction, our method speeds up inference by up to 5.2x compared to full sequence models because inference time depends on the number of edits rather than the number of target tokens. For text normalization, sentence fusion, and grammatical error correction, our approach improves explainability by associating each edit operation with a human-readable tag."}}
{"id": "yMZiuDmj8gM", "cdate": 1546300800000, "mdate": 1636727329453, "content": {"title": "On NMT Search Errors and Model Errors: Cat Got Your Tongue?", "abstract": "Felix Stahlberg, Bill Byrne. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019."}}
{"id": "HyVOyXW_-S", "cdate": 1546300800000, "mdate": null, "content": {"title": "Neural Grammatical Error Correction with Finite State Transducers", "abstract": "Felix Stahlberg, Christopher Bryant, Bill Byrne. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019."}}
{"id": "2y-vtDg6uET", "cdate": 1546300800000, "mdate": null, "content": {"title": "Neural Models of Text Normalization for Speech Applications", "abstract": "Machine learning, including neural network techniques, have been applied to virtually every domain in natural language processing. One problem that has been somewhat resistant to effective machine learning solutions is text normalization for speech applications such as text-to-speech synthesis (TTS). In this application, one must decide, for example, that 123 is verbalized as one hundred twenty three in 123 pages but as one twenty three in 123 King Ave. For this task, state-of-the-art industrial systems depend heavily on hand-written language-specific grammars.We propose neural network models that treat text normalization for TTS as a sequence-to-sequence problem, in which the input is a text token in context, and the output is the verbalization of that token. We find that the most effective model, in accuracy and efficiency, is one where the sentential context is computed once and the results of that computation are combined with the computation of each token in sequence to compute the verbalization. This model allows for a great deal of flexibility in terms of representing the context, and also allows us to integrate tagging and segmentation into the process.These models perform very well overall, but occasionally they will predict wildly inappropriate verbalizations, such as reading 3 cm as three kilometers. Although rare, such verbalizations are a major issue for TTS applications. We thus use finite-state covering grammars to guide the neural models, either during training and decoding, or just during decoding, away from such \u201cunrecoverable\u201d errors. Such grammars can largely be learned from data."}}
{"id": "Sy-Bisgubr", "cdate": 1514764800000, "mdate": null, "content": {"title": "Multi-representation ensembles and delayed SGD updates improve syntax-based NMT", "abstract": "We explore strategies for incorporating target syntax into Neural Machine Translation. We specifically focus on syntax in ensembles containing multiple sentence representations. We formulate beam search over such ensembles using WFSTs, and describe a delayed SGD update training procedure that is especially effective for long representations like linearized syntax. Our approach gives state-of-the-art performance on a difficult Japanese-English task."}}
{"id": "H1WLJGG_WB", "cdate": 1514764800000, "mdate": null, "content": {"title": "An Operation Sequence Model for Explainable Neural Machine Translation", "abstract": "We propose to achieve explainable neural machine translation (NMT) by changing the output representation to explain itself. We present a novel approach to NMT which generates the target sentence by monotonically walking through the source sentence. Word reordering is modeled by operations which allow setting markers in the target sentence and move a target-side write head between those markers. In contrast to many modern neural models, our system emits explicit word alignment information which is often crucial to practical machine translation as it improves explainability. Our technique can outperform a plain text system in terms of BLEU score under the recent Transformer architecture on Japanese-English and Portuguese-English, and is within 0.5 BLEU difference on Spanish-English."}}
{"id": "HyZ8a-MOWB", "cdate": 1483228800000, "mdate": null, "content": {"title": "Unfolding and Shrinking Neural Machine Translation Ensembles", "abstract": "This work was supported by the U.K. Engineering and Physical Sciences Research Council (EPSRC grant EP/L027623/1)."}}
