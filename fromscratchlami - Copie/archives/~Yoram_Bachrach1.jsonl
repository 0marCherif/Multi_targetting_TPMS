{"id": "CLMuNJSJfhv", "cdate": 1652737864976, "mdate": null, "content": {"title": "Neural Payoff Machines: Predicting Fair and Stable Payoff Allocations Among Team Members", "abstract": "In many multi-agent settings, participants can form teams to achieve collective outcomes that may far surpass their individual capabilities. Measuring the relative contributions of agents and allocating them shares of the reward that promote long-lasting cooperation are difficult tasks. Cooperative game theory offers solution concepts identifying distribution schemes, such as the Shapley value, that fairly reflect the contribution of individuals to the performance of the team or the Core, which reduces the incentive of agents to abandon their team. Applications of such methods include identifying influential features and sharing the costs of joint ventures or team formation. Unfortunately, using these solutions requires tackling a computational barrier as they are hard to compute, even in restricted settings. In this work, we show how cooperative game-theoretic solutions can be distilled into a learned model by training neural networks to propose fair and stable payoff allocations. We show that our approach creates models that can generalize to games far from the training distribution and can predict solutions for more players than observed during training. An important application of our framework is Explainable AI: our approach can be used to speed-up Shapley value computations on many instances."}}
{"id": "Sql8oqJTe9", "cdate": 1646226078330, "mdate": null, "content": {"title": "Learning Truthful, Efficient, and Welfare Maximizing Auction Rules", "abstract": "From social networks to supply chains, more and more aspects of how humans, firms and organizations interact is mediated by artificial learning agents. As the influence of machine learning systems grows, it is paramount that we study how to imbue our modern institutions with our own values and principles.\nHere we consider the problem of allocating goods to buyers who have preferences over them in settings where the seller's aim is not to maximize their monetary gains, but rather to advance some notion of social welfare (e.g. the government trying to award construction licenses for hospitals or schools).\nThis problem has a long history in economics, and solutions take the form of auction rules. Researchers have proposed reliable auction rules that work in extremely general settings, and in the presence of information asymmetry and strategic buyers. However, these protocols require significant payments from participants resulting in low aggregate welfare. Here we address this shortcoming by casting auction rule design as a statistical learning problem, and trade generality for participant welfare effectively and automatically with a novel deep learning network architecture and auction representation. Our analysis shows that our auction rules outperform state-of-the art approaches in terms of participants welfare, applicability, robustness."}}
{"id": "Gm2NT9DPX5u", "cdate": 1621872314726, "mdate": null, "content": {"title": "A Limited-Capacity Minimax Theorem for Non-Convex Games or: How I Learned to Stop Worrying about Mixed-Nash and Love Neural Nets", "abstract": "Adversarial training, a special case of multi-objective optimization, is an increasingly prevalent machine learning technique: some of its most notable applications include GAN-based generative modeling and self-play techniques in reinforcement learning which have been applied to complex games such as Go or Poker. In practice, a \\emph{single} pair of networks is typically trained in order to find an approximate equilibrium of a highly nonconcave-nonconvex adversarial problem. However, while a classic result in game theory states such an equilibrium exists in concave-convex games, there is no analogous guarantee if the payoff is nonconcave-nonconvex. Our main contribution is to provide an approximate minimax theorem for a large class of games where the players pick neural networks including WGAN, StarCraft II, and Blotto Game. Our findings rely on the fact that despite being nonconcave-nonconvex with respect to the neural networks parameters, these games are concave-convex with respect to the actual models (e.g., functions or distributions) represented by these neural networks."}}
{"id": "ThRp8aDiv6H", "cdate": 1609459200000, "mdate": 1653508230062, "content": {"title": "Game-theoretic Vocabulary Selection via the Shapley Value and Banzhaf Index", "abstract": "Roma Patel, Marta Garnelo, Ian Gemp, Chris Dyer, Yoram Bachrach. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021."}}
{"id": "8wa7HrUsElL", "cdate": 1601308233115, "mdate": null, "content": {"title": "D3C: Reducing the Price of Anarchy in Multi-Agent Learning", "abstract": "Even in simple multi-agent systems, fixed incentives can lead to outcomes that are poor for the group and each individual agent. We propose a method, D3C, for online adjustment of agent incentives that reduces the loss incurred at a Nash equilibrium. Agents adjust their incentives by learning to mix their incentive with that of other agents, until a compromise is reached in a distributed fashion. We show that D3C improves outcomes for each agent and the group as a whole in several social dilemmas including a traffic network with Braess\u2019s paradox, a prisoner\u2019s dilemma, and several reinforcement learning domains."}}
{"id": "yJiNE8DMpg", "cdate": 1599674225345, "mdate": null, "content": {"title": "A Neural Architecture for Designing Truthful and Efficient Auctions", "abstract": "Auctions are protocols to allocate goods to buyers who have preferences over them,\nand collect payments in return. Economists have invested significant effort in designing auction rules that result in allocations of the goods that are desirable for the\ngroup as a whole. However, for settings where participants\u2019 valuations of the items\non sale are their private information, the rules of the auction must deter buyers\nfrom misreporting their preferences, so as to maximize their own utility, since misreported preferences hinder the ability for the auctioneer to allocate goods to those\nwho want them most. Manual auction design has yielded excellent mechanisms for\nspecific settings, but requires significant effort when tackling new domains. We\npropose a deep learning based approach to automatically design auctions in a wide\nvariety of domains, shifting the design work from human to machine. We assume\nthat participants\u2019 valuations for the items for sale are independently sampled from\nan unknown but fixed distribution. Our system receives a data-set consisting of\nsuch valuation samples, and outputs an auction rule encoding the desired incentive\nstructure. We focus on producing truthful and efficient auctions that minimize\nthe economic burden on participants. We evaluate the auctions designed by our\nframework on well-studied domains, such as multi-unit and combinatorial auctions,\nshowing that they outperform known auction designs in terms of the economic\nburden placed on participants."}}
{"id": "nilX35xQX12", "cdate": 1577836800000, "mdate": 1653508230147, "content": {"title": "Learning to Play No-Press Diplomacy with Best Response Policy Iteration", "abstract": "Recent advances in deep reinforcement learning (RL) have led to considerable progress in many 2-player zero-sum games, such as Go, Poker and Starcraft. The purely adversarial nature of such games allows for conceptually simple and principled application of RL methods. However real-world settings are many-agent, and agent interactions are complex mixtures of common-interest and competitive aspects. We consider Diplomacy, a 7-player board game designed to accentuate dilemmas resulting from many-agent interactions. It also features a large combinatorial action space and simultaneous moves, which are challenging for RL algorithms. We propose a simple yet effective approximate best response operator, designed to handle large combinatorial action spaces and simultaneous moves. We also introduce a family of policy iteration methods that approximate fictitious play. With these methods, we successfully apply RL to Diplomacy: we show that our agents convincingly outperform the previous state-of-the-art, and game theoretic equilibrium analysis shows that the new process yields consistent improvements."}}
{"id": "XqRyiBxKPMP", "cdate": 1577836800000, "mdate": 1653508230061, "content": {"title": "Learning to Play No-Press Diplomacy with Best Response Policy Iteration", "abstract": "Recent advances in deep reinforcement learning (RL) have led to considerable progress in many 2-player zero-sum games, such as Go, Poker and Starcraft. The purely adversarial nature of such games allows for conceptually simple and principled application of RL methods. However real-world settings are many-agent, and agent interactions are complex mixtures of common-interest and competitive aspects. We consider Diplomacy, a 7-player board game designed to accentuate dilemmas resulting from many-agent interactions. It also features a large combinatorial action space and simultaneous moves, which are challenging for RL algorithms. We propose a simple yet effective approximate best response operator, designed to handle large combinatorial action spaces and simultaneous moves. We also introduce a family of policy iteration methods that approximate fictitious play. With these methods, we successfully apply RL to Diplomacy: we show that our agents convincingly outperform the previous state-of-the-art, and game theoretic equilibrium analysis shows that the new process yields consistent improvements."}}
{"id": "Bklg1grtDr", "cdate": 1569439703683, "mdate": null, "content": {"title": "Neural Design of Contests and All-Pay Auctions using Multi-Agent Simulation", "abstract": "We propose a multi-agent learning approach for designing crowdsourcing contests and all-pay auctions. Prizes in contests incentivise contestants to expend effort on their entries, with different prize allocations resulting in different incentives and bidding behaviors. In contrast to auctions designed manually by economists, our method searches the possible design space using a simulation of the multi-agent learning process, and can thus handle settings where a game-theoretic equilibrium analysis is not tractable. Our method simulates agent learning in contests and evaluates the utility of the resulting outcome for the auctioneer. Given a large contest design space, we assess through simulation many possible contest designs within the space, and fit a neural network to predict outcomes for previously untested contest designs. Finally, we apply mirror descent to optimize the design so as to achieve more desirable outcomes. Our empirical analysis shows our approach closely matches the optimal outcomes in settings where the equilibrium is known, and can produce high quality designs in settings where the equilibrium strategies are not solvable analytically. "}}
{"id": "ryl1r1BYDS", "cdate": 1569439542578, "mdate": null, "content": {"title": "Multiagent Reinforcement Learning in Games with an Iterated Dominance Solution", "abstract": "Multiagent reinforcement learning (MARL) attempts to optimize policies of intelligent agents interacting in the same environment. However, it may fail to converge to a Nash equilibrium in some games.  We study independent MARL under the more demanding solution concept of iterated elimination of strictly dominated strategies.  In dominance solvable games, if players iteratively eliminate strictly dominated strategies until no further strategies can be eliminated, we obtain a single strategy profile. We show that convergence to the iterated dominance solution is guaranteed for several reinforcement learning algorithms (for multiple independent learners). We illustrate an application of our results by studying mechanism design for principal-agent problems, where a principal wishes to incentivize agents to exert costly effort in a joint project when it can only observe whether the project succeeded, but not whether agents actually exerted effort. We show that MARL converges to the desired outcome if the rewards are designed so that exerting effort is the iterated dominance solution, but fails if it is merely a Nash equilibrium."}}
