{"id": "jwhW6oCes5E", "cdate": 1663230833315, "mdate": 1663230833315, "content": {"title": "Hotel2vec: Learning Hotel Embeddings from User Click Sessions with Side Information", "abstract": "We propose a new neural network architecture for learning vector representations of items with attributes, specifically hotels. Unlike\nprevious works, which typically only rely on modeling of user-item interactions for learning item embeddings, we propose a framework\nthat combines several sources of data, including user clicks, hotel attributes (e.g., property type, star rating, average user rating),\namenity information (e.g., if the hotel has free Wi-Fi or free breakfast), and geographic information that leverages an hexagonal\ngeospatial system as well as spatial encoders. During model training, a joint embedding is learned from all of the above information.\nWe show that including structured attributes about hotels enables us to make better predictions in a downstream task than when we\nrely exclusively on click data. We train our embedding model on more than 60 million user click sessions from a leading online travel\nplatform, and learn embeddings for more than one million hotels. Our final learned embeddings integrate distinct sub-embeddings\nfor user clicks, hotel attributes, and geographic information, providing a representation that can be used flexibly depending on the\napplication. An important advantage of the proposed neural model is that it addresses the cold-start problem for hotels with insufficient\nhistorical click information by incorporating additional hotel attributes, which are available for all hotels. We show through the results\nof an online A/B test that our model generates high-quality representations that boost the performance of a hotel recommendation\nsystem on a large online travel platform"}}
{"id": "tFt-aa59nfd", "cdate": 1642493864340, "mdate": 1642493864340, "content": {"title": "Aligning Hotel Embeddings using Domain Adaptation for Next-Item Recommendation", "abstract": "In online platforms it is often the case to have multiple brands under the same group which may target different customer profiles, or have different domains. For example, in the hospitality domain, Expedia Group has multiple brands like Brand Expedia, this http URL and Wotif which have either different traveler profiles or are more relevant in a local context.\nIn this context, learning embeddings for hotels that can be leveraged in recommendation tasks in multiple brands requires to have a common embedding that can be induced using alignment approaches. In the same time, one needs to ensure that this common embedding space does not degrade the performance in any of the brands.\nIn this work we build upon the hotel2vec model and propose a simple regularization approach for aligning hotel embeddings of different brands via domain adaptation. We also explore alignment methods previously used in cross-lingual embeddings to align spaces of different languages. We present results on the task of next-hotel prediction using click sessions from two brands. The results show that the proposed approach can align the two embedding spaces while achieving good performance in both brands. Additionally, with respect to single-brand training we show that the proposed approach can significantly reduce training time and improve the predictive performance. "}}
{"id": "hH74MApIoip", "cdate": 1546300800000, "mdate": null, "content": {"title": "Wasserstein distances for evaluating cross-lingual embeddings", "abstract": "Word embeddings are high dimensional vector representations of words that capture their semantic similarity in the vector space. There exist several algorithms for learning such embeddings both for a single language as well as for several languages jointly. In this work we propose to evaluate collections of embeddings by adapting downstream natural language tasks to the optimal transport framework. We show how the family of Wasserstein distances can be used to solve cross-lingual document retrieval and the cross-lingual document classification problems. We argue on the advantages of this approach compared to more traditional evaluation methods of embeddings like bilingual lexical induction. Our experimental results suggest that using Wasserstein distances on these problems out-performs several strong baselines and performs on par with state-of-the-art models."}}
{"id": "Ag5YK2nbtLh", "cdate": 1546300800000, "mdate": null, "content": {"title": "Hotel2vec: Learning Attribute-Aware Hotel Embeddings with Self-Supervision", "abstract": "We propose a neural network architecture for learning vector representations of hotels. Unlike previous works, which typically only use user click information for learning item embeddings, we propose a framework that combines several sources of data, including user clicks, hotel attributes (e.g., property type, star rating, average user rating), amenity information (e.g., the hotel has free Wi-Fi or free breakfast), and geographic information. During model training, a joint embedding is learned from all of the above information. We show that including structured attributes about hotels enables us to make better predictions in a downstream task than when we rely exclusively on click data. We train our embedding model on more than 40 million user click sessions from a leading online travel platform and learn embeddings for more than one million hotels. Our final learned embeddings integrate distinct sub-embeddings for user clicks, hotel attributes, and geographic information, providing an interpretable representation that can be used flexibly depending on the application. We show empirically that our model generates high-quality representations that boost the performance of a hotel recommendation system in addition to other applications. An important advantage of the proposed neural model is that it addresses the cold-start problem for hotels with insufficient historical click information by incorporating additional hotel attributes which are available for all hotels."}}
{"id": "ot-yu4NZxEs", "cdate": 1514764800000, "mdate": null, "content": {"title": "On the Effectiveness of Feature Set Augmentation Using Clusters of Word Embeddings", "abstract": ""}}
{"id": "BJ-Gvdbdbr", "cdate": 1483228800000, "mdate": null, "content": {"title": "Aggressive Sampling for Multi-class to Binary Reduction with Applications to Text Classification", "abstract": "We address the problem of multi-class classification in the case where the number of classes is very large. We propose a double sampling strategy on top of a multi-class to binary reduction strategy, which transforms the original multi-class problem into a binary classification problem over pairs of examples. The aim of the sampling strategy is to overcome the curse of long-tailed class distributions exhibited in majority of large-scale multi-class classification problems and to reduce the number of pairs of examples in the expanded data. We show that this strategy does not alter the consistency of the empirical risk minimization principle defined over the double sample reduction. Experiments are carried out on DMOZ and Wikipedia collections with 10,000 to 100,000 classes where we show the efficiency of the proposed approach in terms of training and prediction time, memory consumption, and predictive performance with respect to state-of-the-art approaches."}}
{"id": "1s99hUyqGrv", "cdate": 1451606400000, "mdate": null, "content": {"title": "Learning to Search for Recognizing Named Entities in Twitter", "abstract": "We presented in this work our participation in the 2nd Named Entity Recognition for Twitter shared task. The task has been cast as a sequence labeling one and we employed a learning to search approach in order to tackle it. We also leveraged LOD for extracting rich contextual features for the named-entities. Our submission achieved F-scores of 46.16 and 60.24 for the classification and the segmentation tasks and ranked 2nd and 3rd respectively. The post-analysis showed that LOD features improved substantially the performance of our system as they counter-balance the lack of context in tweets. The shared task gave us the opportunity to test the performance of NER systems in short and noisy textual data. The results of the participated systems shows that the task is far to be considered as a solved one and methods with stellar performance in normal texts need to be revised."}}
{"id": "HJNnqHWObr", "cdate": 1388534400000, "mdate": null, "content": {"title": "Re-ranking approach to classification in large-scale power-law distributed category systems", "abstract": "For large-scale category systems, such as Directory Mozilla, which consist of tens of thousand categories, it has been empirically verified in earlier studies that the distribution of documents among categories can be modeled as a power-law distribution. It implies that a significant fraction of categories, referred to as rare categories, have very few documents assigned to them. This characteristic of the data makes it harder for learning algorithms to learn effective decision boundaries which can correctly detect such categories in the test set. In this work, we exploit the distribution of documents among categories to (i) derive an upper bound on the accuracy of any classifier, and (ii) propose a ranking-based algorithm which aims to maximize this upper bound. The empirical evaluation on publicly available large-scale datasets demonstrate that the proposed method not only achieves higher accuracy but also much higher coverage of rare categories as compared to state-of-the-art methods."}}
{"id": "HkWdxKbd-H", "cdate": 1356998400000, "mdate": null, "content": {"title": "On Flat versus Hierarchical Classification in Large-Scale Taxonomies", "abstract": "We study in this paper flat and hierarchical classification strategies in the context of large-scale taxonomies. To this end, we first propose a multiclass, hierarchical data dependent bound on the generalization error of classifiers deployed in large-scale taxonomies. This bound provides an explanation to several empirical results reported in the literature, related to the performance of flat and hierarchical classifiers. We then introduce another type of bounds targeting the approximation error of a family of classifiers, and derive from it features used in a meta-classifier to decide which nodes to prune (or flatten) in a large-scale taxonomy. We finally illustrate the theoretical developments through several experiments conducted on two widely used taxonomies."}}
