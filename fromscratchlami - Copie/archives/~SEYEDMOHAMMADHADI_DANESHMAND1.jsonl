{"id": "AJtuLqCEel9", "cdate": 1693552035506, "mdate": 1693552035506, "content": {"title": "On the impact of activation and normalization in obtaining isometric embeddings at initialization", "abstract": "In this paper, we explore the structure of the penultimate Gram matrix in deep neural networks, which contains the pairwise inner products of outputs corresponding to a batch of inputs. In several architectures it has been observed that this Gram matrix becomes degenerate with depth at initialization, which dramatically slows training. Normalization layers, such as batch or layer normalization, play a pivotal role in preventing the rank collapse issue. Despite promising advances, the existing theoretical results (i) do not extend to layer normalization, which is widely used in transformers, (ii) can not characterize the bias of normalization quantitatively at finite depth.\nTo bridge this gap, we provide a proof that layer normalization, in conjunction with activation layers, biases the Gram matrix of a multilayer perceptron towards isometry at an exponential rate with depth at initialization. We quantify this rate using the Hermite expansion of the activation function, highlighting the importance of higher order (\u22652) Hermite coefficients in the bias towards isometry."}}
{"id": "VQ4m2xovlAw", "cdate": 1672531200000, "mdate": 1681490246414, "content": {"title": "Efficient displacement convex optimization with particle gradient descent", "abstract": ""}}
{"id": "nsno_4xsMCO", "cdate": 1640995200000, "mdate": 1681490246407, "content": {"title": "Entropy Maximization with Depth: A Variational Principle for Random Neural Networks", "abstract": ""}}
{"id": "hlw_foNF_5", "cdate": 1640995200000, "mdate": 1681490246255, "content": {"title": "Polynomial-time sparse measure recovery", "abstract": ""}}
{"id": "_RSgXL8gNnx", "cdate": 1621630185305, "mdate": null, "content": {"title": "Batch Normalization Orthogonalizes Representations in Deep Random Networks", "abstract": "This paper underlines an elegant property of batch-normalization (BN): Successive batch normalizations with random linear updates make samples increasingly orthogonal. We establish a non-asymptotic characterization of the interplay between depth, width, and the orthogonality of deep representations. More precisely, we prove, under a mild assumption, the deviation of the representations from orthogonality rapidly decays with depth up to a term inversely proportional to the network width. This result has two main theoretical and practical implications: 1) Theoretically, as the depth grows, the distribution of the outputs contracts to a Wasserstein-2 ball around an isotropic normal distribution. Furthermore, the radius of this Wasserstein ball shrinks with the width of the network. 2) Practically, the orthogonality of the representations directly influences the performance of stochastic gradient descent (SGD). When representations are initially aligned, we observe SGD wastes many iterations to disentangle representations before the classification. Nevertheless, we experimentally show that starting optimization from orthogonal representations is sufficient to accelerate SGD, with no need for BN."}}
{"id": "Si3SSyPDiJd", "cdate": 1621629704107, "mdate": null, "content": {"title": "Rethinking the Variational Interpretation of Accelerated Optimization Methods", "abstract": "The continuous-time model of Nesterov's momentum provides a thought-provoking perspective for understanding the nature of the acceleration phenomenon in convex optimization. One of the main ideas in this line of research comes from the field of classical mechanics and proposes to link Nesterov's trajectory to the solution of a set of Euler-Lagrange equations relative to the so-called Bregman Lagrangian. In the last years, this approach led to the discovery of many new (stochastic) accelerated algorithms and provided a solid theoretical foundation for the design of structure-preserving accelerated methods. In this work, we revisit this idea and provide an in-depth analysis of the action relative to the Bregman Lagrangian from the point of view of calculus of variations. Our main finding is that, while Nesterov's method is a stationary point for the action, it is often not a minimizer but instead a saddle point for this functional in the space of differentiable curves. This finding challenges the main intuition behind the variational interpretation of Nesterov's method and provides additional insights into the intriguing geometry of accelerated paths.\n"}}
{"id": "l_6wMaNQWor", "cdate": 1609459200000, "mdate": 1681490246532, "content": {"title": "Batch Normalization Orthogonalizes Representations in Deep Random Networks", "abstract": ""}}
{"id": "Wlpi_hEkljN", "cdate": 1609459200000, "mdate": 1681490246765, "content": {"title": "Revisiting the Role of Euler Numerical Integration on Acceleration and Stability in Convex Optimization", "abstract": ""}}
{"id": "MigovPsgeV", "cdate": 1609459200000, "mdate": 1681490246377, "content": {"title": "Rethinking the Variational Interpretation of Accelerated Optimization Methods", "abstract": ""}}
{"id": "Jyk0LXqyFK", "cdate": 1609459200000, "mdate": 1681490246485, "content": {"title": "Revisiting the Role of Euler Numerical Integration on Acceleration and Stability in Convex Optimization", "abstract": ""}}
