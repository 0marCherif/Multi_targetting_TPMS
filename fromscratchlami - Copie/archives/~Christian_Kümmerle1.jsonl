{"id": "2hp6sIBsCDH", "cdate": 1652737565699, "mdate": null, "content": {"title": "Global Linear and Local Superlinear Convergence of IRLS for Non-Smooth Robust Regression", "abstract": "We advance both the theory and practice of robust $\\ell_p$-quasinorm regression for $p \\in (0,1]$ by using novel variants of iteratively reweighted least-squares (IRLS) to solve the underlying non-smooth problem. In the convex case, $p=1$, we prove that this IRLS variant converges globally at a linear rate under a mild, deterministic condition on the feature matrix called the stable range space property. In the non-convex case, $p\\in(0,1)$, we prove that under a similar condition, IRLS converges locally to the global minimizer at a superlinear rate of order $2-p$; the rate becomes quadratic as $p\\to 0$. We showcase the proposed methods in three applications: real phase retrieval, regression without correspondences, and robust face restoration. The results show that (1) IRLS can handle a larger number of outliers than other methods, (2) it is faster than competing methods at the same level of accuracy, (3) it restores a sparsely corrupted face image with satisfactory visual quality."}}
{"id": "jVGaBABbzw", "cdate": 1640995200000, "mdate": 1682570957826, "content": {"title": "Learning Transition Operators From Sparse Space-Time Samples", "abstract": "We consider the nonlinear inverse problem of learning a transition operator $\\mathbf{A}$ from partial observations at different times, in particular from sparse observations of entries of its powers $\\mathbf{A},\\mathbf{A}^2,\\cdots,\\mathbf{A}^{T}$. This Spatio-Temporal Transition Operator Recovery problem is motivated by the recent interest in learning time-varying graph signals that are driven by graph operators depending on the underlying graph topology. We address the nonlinearity of the problem by embedding it into a higher-dimensional space of suitable block-Hankel matrices, where it becomes a low-rank matrix completion problem, even if $\\mathbf{A}$ is of full rank. For both a uniform and an adaptive random space-time sampling model, we quantify the recoverability of the transition operator via suitable measures of incoherence of these block-Hankel embedding matrices. For graph transition operators these measures of incoherence depend on the interplay between the dynamics and the graph topology. We develop a suitable non-convex iterative reweighted least squares (IRLS) algorithm, establish its quadratic local convergence, and show that, in optimal scenarios, no more than $\\mathcal{O}(rn \\log(nT))$ space-time samples are sufficient to ensure accurate recovery of a rank-$r$ operator $\\mathbf{A}$ of size $n \\times n$. This establishes that spatial samples can be substituted by a comparable number of space-time samples. We provide an efficient implementation of the proposed IRLS algorithm with space complexity of order $O(r n T)$ and per-iteration time complexity linear in $n$. Numerical experiments for transition operators based on several graph models confirm that the theoretical findings accurately track empirical phase transitions, and illustrate the applicability and scalability of the proposed algorithm."}}
{"id": "-S1V_oEOE52", "cdate": 1621630054309, "mdate": null, "content": {"title": "Iteratively Reweighted Least Squares for Basis Pursuit with Global Linear Convergence Rate", "abstract": "The recovery of sparse data is at the core of many applications in machine learning and signal processing. While such problems can be tackled using $\\ell_1$-regularization as in the LASSO estimator and in the Basis Pursuit approach, specialized algorithms are typically required to solve the corresponding high-dimensional non-smooth optimization for large instances.\nIteratively Reweighted Least Squares (IRLS) is a widely used algorithm for this purpose due to its excellent numerical performance. However, while existing theory is able to guarantee convergence of this algorithm to the minimizer, it does not provide a global convergence rate. In this paper, we prove that a variant of IRLS converges \\emph{with a global linear rate} to a sparse solution, i.e., with a linear error decrease occurring immediately from any initialization if the measurements fulfill the usual null space property assumption. We support our theory by numerical experiments showing that our linear rate captures the correct dimension dependence. We anticipate that our theoretical findings will lead to new insights for many other use cases of the IRLS algorithm, such as in low-rank matrix recovery."}}
{"id": "f8FF35KsWi", "cdate": 1609459200000, "mdate": 1682570957825, "content": {"title": "Iteratively Reweighted Least Squares for Basis Pursuit with Global Linear Convergence Rate", "abstract": "The recovery of sparse data is at the core of many applications in machine learning and signal processing. While such problems can be tackled using $\\ell_1$-regularization as in the LASSO estimator and in the Basis Pursuit approach, specialized algorithms are typically required to solve the corresponding high-dimensional non-smooth optimization for large instances.Iteratively Reweighted Least Squares (IRLS) is a widely used algorithm for this purpose due to its excellent numerical performance. However, while existing theory is able to guarantee convergence of this algorithm to the minimizer, it does not provide a global convergence rate. In this paper, we prove that a variant of IRLS converges \\emph{with a global linear rate} to a sparse solution, i.e., with a linear error decrease occurring immediately from any initialization if the measurements fulfill the usual null space property assumption. We support our theory by numerical experiments showing that our linear rate captures the correct dimension dependence. We anticipate that our theoretical findings will lead to new insights for many other use cases of the IRLS algorithm, such as in low-rank matrix recovery."}}
{"id": "a1zwRjBePQn", "cdate": 1609459200000, "mdate": 1682570957870, "content": {"title": "A Scalable Second Order Method for Ill-Conditioned Matrix Completion from Few Samples", "abstract": "We propose an iterative algorithm for low-rank matrix completion with that can be interpreted as an iteratively reweighted least squares (IRLS) algorithm, a saddle-escaping smoothing Newton method ..."}}
{"id": "TfbTq_qX8c", "cdate": 1609459200000, "mdate": 1682570957868, "content": {"title": "Dictionary-Sparse Recovery From Heavy-Tailed Measurements", "abstract": "The recovery of signals that are sparse not in a basis, but rather sparse with respect to an over-complete dictionary is one of the most flexible settings in the field of compressed sensing with numerous applications. As in the standard compressed sensing setting, it is possible that the signal can be reconstructed efficiently from few, linear measurements, for example by the so-called $\\ell_1$-synthesis method. However, it has been less well-understood which measurement matrices provably work for this setting. Whereas in the standard setting, it has been shown that even certain heavy-tailed measurement matrices can be used in the same sample complexity regime as Gaussian matrices, comparable results are only available for the restrictive class of sub-Gaussian measurement vectors as far as the recovery of dictionary-sparse signals via $\\ell_1$-synthesis is concerned. In this work, we fill this gap and establish optimal guarantees for the recovery of vectors that are (approximately) sparse with respect to a dictionary via the $\\ell_1$-synthesis method from linear, potentially noisy measurements for a large class of random measurement matrices. In particular, we show that random measurements that fulfill only a small-ball assumption and a weak moment assumption, such as random vectors with i.i.d. Student-$t$ entries with a logarithmic number of degrees of freedom, lead to comparable guarantees as (sub-)Gaussian measurements. As a technical tool, we show a bound on the expectation of the sum of squared order statistics under very general assumptions, which might be of independent interest. As a corollary of our results, we also obtain a slight improvement on the weakest assumption on a measurement matrix with i.i.d. rows sufficient for uniform recovery in standard compressed sensing, improving on results by Lecu\\'e and Mendelson and Dirksen, Lecu\\'e and Rauhut."}}
{"id": "OlMLhcLrl7", "cdate": 1609459200000, "mdate": 1682570957871, "content": {"title": "A Scalable Second Order Method for Ill-Conditioned Matrix Completion from Few Samples", "abstract": "We propose an iterative algorithm for low-rank matrix completion that can be interpreted as an iteratively reweighted least squares (IRLS) algorithm, a saddle-escaping smoothing Newton method or a variable metric proximal gradient method applied to a non-convex rank surrogate. It combines the favorable data-efficiency of previous IRLS approaches with an improved scalability by several orders of magnitude. We establish the first local convergence guarantee from a minimal number of samples for that class of algorithms, showing that the method attains a local quadratic convergence rate. Furthermore, we show that the linear systems to be solved are well-conditioned even for very ill-conditioned ground truth matrices. We provide extensive experiments, indicating that unlike many state-of-the-art approaches, our method is able to complete very ill-conditioned matrices with a condition number of up to $10^{10}$ from few samples, while being competitive in its scalability."}}
{"id": "bl_kSn2W7a", "cdate": 1577836800000, "mdate": 1682570957875, "content": {"title": "Escaping Saddle Points in Ill-Conditioned Matrix Completion with a Scalable Second Order Method", "abstract": "We propose an iterative algorithm for low-rank matrix completion that can be interpreted as both an iteratively reweighted least squares (IRLS) algorithm and a saddle-escaping smoothing Newton method applied to a non-convex rank surrogate objective. It combines the favorable data efficiency of previous IRLS approaches with an improved scalability by several orders of magnitude. Our method attains a local quadratic convergence rate already for a number of samples that is close to the information theoretical limit. We show in numerical experiments that unlike many state-of-the-art approaches, our approach is able to complete very ill-conditioned matrices with a condition number of up to $10^{10}$ from few samples."}}
{"id": "WvDfR0IZH9U", "cdate": 1577836800000, "mdate": 1682570957872, "content": {"title": "On the robustness of noise-blind low-rank recovery from rank-one measurements", "abstract": "We prove new results about the robustness of well-known convex noise-blind optimization formulations for the reconstruction of low-rank matrices from underdetermined linear measurements. Our results are applicable for symmetric rank-one measurements as used in a formulation of the phase retrieval problem. We obtain these results by establishing that with high probability rank-one measurement operators defined by i.i.d. Gaussian vectors exhibit the so-called Schatten-1 quotient property, which corresponds to a lower bound for the inradius of their image of the nuclear norm (Schatten-1) unit ball. We complement our analysis by numerical experiments comparing the solutions of noise-blind and noise-aware formulations. These experiments confirm that noise-blind optimization methods exhibit comparable robustness to noise-aware formulations. Keywords: low-rank matrix recovery, phase retrieval, quotient property, noise-blind, robustness, nuclear norm minimization"}}
{"id": "1QHihoCIxX", "cdate": 1577836800000, "mdate": 1626658530562, "content": {"title": "Iteratively Reweighted Least Squares for \ud835\udcc11-minimization with Global Linear Convergence Rate", "abstract": "The recovery of sparse data is at the core of many applications in machine learning and signal processing. While such problems can be tackled using $\\ell_1$-regularization as in the LASSO estimator and in the Basis Pursuit approach, specialized algorithms are typically required to solve the corresponding high-dimensional non-smooth optimization for large instances. Iteratively Reweighted Least Squares (IRLS) is a widely used algorithm for this purpose due its excellent numerical performance. However, while existing theory is able to guarantee convergence of this algorithm to the minimizer, it does not provide a global convergence rate. In this paper, we prove that a variant of IRLS converges with a global linear rate to a sparse solution, i.e., with a linear error decrease occurring immediately from any initialization, if the measurements fulfill the usual null space property assumption. We support our theory by numerical experiments showing that our linear rate captures the correct dimension dependence. We anticipate that our theoretical findings will lead to new insights for many other use cases of the IRLS algorithm, such as in low-rank matrix recovery."}}
