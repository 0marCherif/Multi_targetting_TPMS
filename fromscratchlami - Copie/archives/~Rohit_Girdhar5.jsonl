{"id": "-vybUmrWpf", "cdate": 1684168474529, "mdate": 1684168474529, "content": {"title": "Physical Reasoning Using Dynamics-Aware Models", "abstract": "A common approach to solving physicalreasoning tasks is to train a value learner on example tasks. A limitation of such an approach is that it requires learning about object dynamics solely from reward values assigned to the final state of a rollout of the environment. This study aims to address this limitation by augmenting the reward value with self-supervised signals about object dynamics. Specifically, we train the model to characterize the similarity of two environment rollouts, jointly with predicting the outcome of the reasoning task. This similarity can be defined as a distance measure between the trajectory of objects in the two rollouts, or learned directly from pixels using a contrastive formulation. Empirically, we find that this approach leads to substantial performance improvements on the PHYRE benchmark for physical reasoning (Bakhtin et al., 2019), establishing a new state-of-the-art."}}
{"id": "c3aDMqg5Lc", "cdate": 1672531200000, "mdate": 1681499702206, "content": {"title": "Cut and Learn for Unsupervised Object Detection and Instance Segmentation", "abstract": ""}}
{"id": "bQLrCPV59h", "cdate": 1672531200000, "mdate": 1679632729181, "content": {"title": "HierVL: Learning Hierarchical Video-Language Embeddings", "abstract": ""}}
{"id": "bCtZM3OVj2G", "cdate": 1672531200000, "mdate": 1679632729186, "content": {"title": "What You Say Is What You Show: Visual Narration Detection in Instructional Videos", "abstract": ""}}
{"id": "6iRdiRV_Tf", "cdate": 1672531200000, "mdate": 1681499702178, "content": {"title": "The effectiveness of MAE pre-pretraining for billion-scale pretraining", "abstract": ""}}
{"id": "3MSPXH1Y0L", "cdate": 1672531200000, "mdate": 1681499702179, "content": {"title": "Learning to Substitute Ingredients in Recipes", "abstract": ""}}
{"id": "vzuOFWPEsk", "cdate": 1668603895623, "mdate": 1668603895623, "content": {"title": "Detecting Twenty-thousand Classes using Image-level Supervision", "abstract": "Abstract. Current object detectors are limited in vocabulary size due to\nthe small scale of detection datasets. Image classi\fers, on the other hand,\nreason about much larger vocabularies, as their datasets are larger and\neasier to collect. We propose Detic, which simply trains the classi\fers of a\ndetector on image classi\fcation data and thus expands the vocabulary of\ndetectors to tens of thousands of concepts. Unlike prior work, Detic does\nnot need complex assignment schemes to assign image labels to boxes\nbased on model predictions, making it much easier to implement and\ncompatible with a range of detection architectures and backbones. Our\nresults show that Detic yields excellent detectors even for classes without\nbox annotations. It outperforms prior work on both open-vocabulary and\nlong-tail detection benchmarks. Detic provides a gain of 2.4 mAP for\nall classes and 8.3 mAP for novel classes on the open-vocabulary LVIS\nbenchmark. On the standard LVIS benchmark, Detic obtains 41.7 mAP\nwhen evaluated on all classes, or only rare classes, hence closing the gap in\nperformance for object categories with few samples. For the \frst time, we\ntrain a detector with all the twenty-one-thousand classes of the ImageNet\ndataset and show that it generalizes to new datasets without \fnetuning.\nCode is available at https://github.com/facebookresearch/Detic."}}
{"id": "BHlep_HNGX5", "cdate": 1648670068860, "mdate": 1648670068860, "content": {"title": "Omnivore: A Single Model for Many Visual Modalities", "abstract": "Prior work has studied different visual modalities in isolation and developed separate architectures for recognition of images, videos, and 3D data. Instead, in this paper, we propose a single model which excels at classifying images, videos, and single-view 3D data using exactly the same model parameters. Our 'Omnivore' model leverages the flexibility of transformer-based architectures and is trained jointly on classification tasks from different modalities. Omnivore is simple to train, uses off-the-shelf standard datasets, and performs at-par or better than modality-specific models of the same size. A single Omnivore model obtains 86.0% on ImageNet, 84.1% on Kinetics, and 67.1% on SUN RGB-D. After finetuning, our models outperform prior work on a variety of vision tasks and generalize across modalities. Omnivore's shared visual representation naturally enables cross-modal recognition without access to correspondences between modalities. We hope our results motivate researchers to model visual modalities together."}}
{"id": "SNhxIgSVG7c", "cdate": 1648669933787, "mdate": null, "content": {"title": "Anticipative Video Transformer", "abstract": "We propose Anticipative Video Transformer (AVT), an end-to-end attention-based video modeling architecture that attends to the previously observed video in order to anticipate future actions. We train the model jointly to predict the next action in a video sequence, while also learning frame feature encoders that are predictive of successive future frames' features. Compared to existing temporal aggregation strategies, AVT has the advantage of both maintaining the sequential progression of observed actions while still capturing long-range dependencies--both critical for the anticipation task. Through extensive experiments, we show that AVT obtains the best reported performance on four popular action anticipation benchmarks: EpicKitchens-55, EpicKitchens-100, EGTEA Gaze+, and 50-Salads; and it wins first place in the EpicKitchens-100 CVPR'21 challenge."}}
{"id": "i4eKjRAXxEhE", "cdate": 1640995200000, "mdate": 1663773493855, "content": {"title": "OmniMAE: Single Model Masked Pretraining on Images and Videos", "abstract": "Transformer-based architectures have become competitive across a variety of visual domains, most notably images and videos. While prior work has studied these modalities in isolation, having a common architecture suggests that one can train a single unified model for multiple visual modalities. Prior attempts at unified modeling typically use architectures tailored for vision tasks, or obtain worse performance compared to single modality models. In this work, we show that masked autoencoding can be used to train a simple Vision Transformer on images and videos, without requiring any labeled data. This single model learns visual representations that are comparable to or better than single-modality representations on both image and video benchmarks, while using a much simpler architecture. In particular, our single pretrained model can be finetuned to achieve 86.5% on ImageNet and 75.3% on the challenging Something Something-v2 video benchmark. Furthermore, this model can be learned by dropping 90% of the image and 95% of the video patches, enabling extremely fast training."}}
