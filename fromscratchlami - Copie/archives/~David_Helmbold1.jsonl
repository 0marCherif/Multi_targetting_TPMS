{"id": "B1EHr3-dWH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Gradient descent with identity initialization efficiently learns positive definite linear transformations", "abstract": "We analyze algorithms for approximating a function $f(x) = \\Phi x$ mapping $\\Re^d$ to $\\Re^d$ using deep linear neural networks, i.e. that learn a function $h$ parameterized by matrices $\\Theta_1,...."}}
{"id": "BJNZbjZd-S", "cdate": 1293840000000, "mdate": null, "content": {"title": "On the Necessity of Irrelevant Variables", "abstract": "This work explores the effects of relevant and irrelevant boolean variables on the accuracy of classifiers. The analysis uses the assumption that the variables are conditionally independent given the class, and focuses on a natural family of learning algorithms for such sources when the relevant variables have a small advantage over random guessing. The main result is that algorithms relying predominately on irrelevant variables have error probabilities that quickly go to 0 in situations where algorithms that limit the use of irrelevant variables have errors bounded below by a positive constant. We also show that accurate learning is possible even when there are so few examples that one cannot determine with high confidence whether or not any individual variable is relevant."}}
{"id": "S1-adP-uWr", "cdate": 915148800000, "mdate": null, "content": {"title": "Potential Boosters?", "abstract": "Recent interpretations of the Adaboost algorithm view it as per(cid:173) forming a gradient descent on a potential function. Simply chang(cid:173) ing the potential function allows one to create new algorithms re(cid:173) lated to AdaBoost. However, these new algorithms are generally not known to have the formal boosting property. This paper ex(cid:173) amines the question of which potential functions lead to new al(cid:173) gorithms that are boosters. The two main results are general sets of conditions on the potential; one set implies that the resulting algorithm is a booster, while the other implies that the algorithm is not. These conditions are applied to previously studied potential functions , such as those used by LogitBoost and Doom II."}}
{"id": "rJ-8hjWubS", "cdate": 820454400000, "mdate": null, "content": {"title": "On-Line Portfolio Selection Using Multiplicative Updates", "abstract": ""}}
{"id": "B1NfQvWubB", "cdate": 788918400000, "mdate": null, "content": {"title": "Worst-case Loss Bounds for Single Neurons", "abstract": "We analyze and compare the well-known Gradient Descent algo(cid:173) rithm and a new algorithm, called the Exponentiated Gradient algorithm, for training a single neuron with an arbitrary transfer function . Both algorithms are easily generalized to larger neural networks, and the generalization of Gradient Descent is the stan(cid:173) dard back-propagation algorithm. In this paper we prove worst(cid:173) case loss bounds for both algorithms in the single neuron case. Since local minima make it difficult to prove worst-case bounds for gradient-based algorithms, we must use a loss function that prevents the formation of spurious local minima. We define such a matching loss function for any strictly increasing differentiable transfer function and prove worst-case loss bound for any such transfer function and its corresponding matching loss. For exam(cid:173) ple, the matching loss for the identity function is the square loss and the matching loss for the logistic sigmoid is the entropic loss. The different structure of the bounds for the two algorithms indi(cid:173) cates that the new algorithm out-performs Gradient Descent when the inputs contain a large number of irrelevant components. 310 D. P. HELMBOLD, J. KIVINEN, M. K. WARMUTH"}}
