{"id": "7ZcyRF7Y3S", "cdate": 1663850398734, "mdate": null, "content": {"title": "Synergies Between Disentanglement and Sparsity: a Multi-Task Learning Perspective", "abstract": "Although disentangled representations are often said to be beneficial for downstream tasks, current empirical and theoretical understanding is limited. In this work, we provide evidence that disentangled representations coupled with sparse base-predictors improve generalization. In the context of multi-task learning, we prove a new identifiability result that provides conditions under which maximally sparse base-predictors yield disentangled representations. Motivated by this theoretical result, we propose a practical approach to learn disentangled representations based on a sparsity-promoting bi-level optimization problem. Finally, we explore a meta-learning version of this algorithm based on group Lasso multiclass SVM base-predictors, for which we derive a tractable dual formulation. It obtains competitive results on standard few-shot classification benchmarks, while each task is using only a fraction of the learned representations. "}}
{"id": "b57KM4ydqpp", "cdate": 1652737542225, "mdate": null, "content": {"title": "The Curse of Unrolling: Rate of Differentiating Through Optimization", "abstract": "Computing the Jacobian of the solution of an optimization problem is a central problem in machine learning, with applications in hyperparameter optimization, meta-learning, optimization as a layer, and dataset distillation, to name a few. Unrolled differentiation is a popular heuristic that approximates the solution using an iterative solver and differentiates it through the computational path. This work provides a non-asymptotic convergence-rate analysis of this approach on quadratic objectives for gradient descent and the Chebyshev method. We show that to ensure convergence of the Jacobian, we can either 1) choose a large learning rate leading to a fast asymptotic convergence but accept that the algorithm may have an arbitrarily long burn-in phase or 2) choose a smaller learning rate leading to an immediate but slower convergence. We refer to this phenomenon as the curse of unrolling.\nFinally, we discuss open problems relative to this approach, such as deriving a practical update rule for the optimal unrolling strategy and making novel connections with the field of Sobolev orthogonal polynomials."}}
{"id": "UvQgwhYi7QM", "cdate": 1652737532805, "mdate": null, "content": {"title": "Beyond L1: Faster and Better Sparse Models with skglm", "abstract": "We propose a new fast algorithm to estimate any sparse generalized linear model with convex or non-convex separable penalties. Our algorithm is able to solve problems with millions of samples and features in seconds, by relying on coordinate descent, working sets and Anderson acceleration.  It handles previously unaddressed models, and  is extensively shown to improve state-of-art algorithms. We provide a flexible, scikit-learn compatible package, which easily handles customized datafits and penalties."}}
{"id": "MJEidN48pbI", "cdate": 1620856739276, "mdate": null, "content": {"title": "Implicit differentiation for fast hyperparameter selection in non-smooth convex learning", "abstract": "Finding the optimal hyperparameters of a model can be cast as a bilevel optimization problem, typically solved using zero-order techniques. In this work we study first-order methods when the inner optimization problem is convex but non-smooth. We show that the forward-mode differentiation of proximal gradient descent and proximal coordinate descent yield sequences of Jacobians converging toward the exact Jacobian. Using implicit differentiation, we show it is possible to leverage the non-smoothness of the inner problem to speed up the computation.  Finally, we provide a bound on the error made on the hypergradient when the inner optimization problem is solved approximately. Results on regression and classification problems reveal computational benefits for hyperparameter optimization, especially when multiple hyperparameters are required."}}
{"id": "UBKPb3UMtP", "cdate": 1620856295540, "mdate": null, "content": {"title": "Model identification and local linear convergence of coordinate descent", "abstract": "For composite nonsmooth optimization problems, Forward-Backward algorithm  achieves  model  identification  (e.g.,support  identification  for the Lasso) after a finite number of iterations, provided the objective function is regular enough.  Results concerning coordinate descent are scarcer and model identification has only been shown for specific estimators, the support-vector machine for instance.  In this work,  we show that cyclic coordinate descent achieves model identification in finite time for a wide class of functions.  In addition, we prove explicit local linear convergence rates for coordinate descent.  Extensive experiments on various estimators and on real datasets demonstrate that these rates match well empirical results."}}
{"id": "wmN0rmMlui", "cdate": 1620856038391, "mdate": null, "content": {"title": "Anderson acceleration of coordinate descent", "abstract": "Acceleration of first order methods is mainly obtained via inertial techniques \u00e0 la Nesterov, or via nonlinear extrapolation. The latter has known a recent surge of interest, with successful applications to gradient and proximal gradient techniques.  On multiple Machine Learning problems, coordinate descent achieves performance significantly superior to full gradient methods. Speeding up coordinate descent in practice is not easy: inertially accelerated versions of coordinate descent are theoretically accelerated, but might not always lead to practical speed-ups. We propose an accelerated version of coordinate descent using extrapolation, showing considerable speed up in practice, compared to inertial accelerated coordinate descent and extrapolated (proximal) gradient descent.  Experiments on least squares, Lasso, elastic net and logistic regression validate the approach."}}
{"id": "Uvh-RcmVucX", "cdate": 1599925499544, "mdate": null, "content": {"title": "Implicit differentiation of Lasso-type models for hyperparameter optimization", "abstract": "Setting regularization parameters for Lasso-type estimators  is  notoriously  difficult,  though  cru-cial  for  obtaining  the  best  accuracy.   The  most popular hyperparameter optimization approach isgrid-search on a held-out dataset. However, grid-search  requires  to  choose  a  predefined  grid  ofparameters and scales exponentially in the num-ber of parameters.  Another class of approaches casts  hyperparameter  optimization  as  a  bi-level optimization  problem,  typically  solved  by  gradient  descent.   The  key  challenge  for  these  approaches is the estimation of the gradient w.r.t.the  hyperparameters.   Computing  that  gradient via  forward  or  backward  automatic  differentia-tion usually suffers from high memory consump-tion,  while  implicit  differentiation  typically  involves solving a linear system which can be prohibitive  and  numerically  unstable.   In  addition, implicit  differentiation  usually  assumes  smoothloss  functions,  which  is  not  the  case  of  Lasso-type problems. This work introduces an efficient implicit  differentiation  algorithm,  without  matrix inversion, tailored for Lasso-type problems. Our proposal scales to high-dimensional data by leveraging the sparsity of the solutions.  Empirically, we demonstrate that the proposed method outperforms a large number of standard methods for hyperparameter optimization."}}
{"id": "ogACAeT9uqX", "cdate": 1599925046401, "mdate": null, "content": {"title": "Support recovery and sup-norm convergence rates for sparse pivotal estimation", "abstract": "In high dimensional sparse regression, pivotalestimators are estimators for which the opti-mal regularization parameter is independentof the noise level. The canonical pivotal estimator is the square-root Lasso, formulated along with its derivatives as a \u201cnon-smooth +non-smooth\u201d optimization problem. Modern techniques to solve these include smoothingthe datafitting term, to benefit from fast efficient proximal algorithms. In this work we show minimax sup-norm convergence ratesfor non smoothed and smoothed, single taskand multitask square-root Lasso-type estimators.  Thanks to our theoretical analysis, we provide some guidelines on how to set the smoothing hyperparameter, and illustrate on synthetic data the interest of suchguidelines."}}
{"id": "S1eEuNreIH", "cdate": 1567802475536, "mdate": null, "content": {"title": "Handling correlated and repeated measurements with the smoothed multivariate square-root Lasso", "abstract": "Sparsity promoting norms are frequently used in high dimensional regression. A limitation of such Lasso-type estimators is that the optimal regularization parameter depends on the unknown noise level. Estimators such as the concomitant Lasso address this dependence by jointly estimating the noise level and the regression coefficients. Additionally, in many applications, the data is obtained by averaging multiple measurements: this reduces the noise variance, but it dramatically reduces sample sizes and prevents refined noise modeling. In this work, we propose a concomitant estimator that can cope with complex noise structure by using non-averaged measurements. The resulting optimization problem is convex and amenable, thanks to smoothing theory, to state-of-the-art optimization techniques that leverage the sparsity of the solutions. Practical benefits are demonstrated on toy datasets, realistic simulated data and real neuroimaging data."}}
