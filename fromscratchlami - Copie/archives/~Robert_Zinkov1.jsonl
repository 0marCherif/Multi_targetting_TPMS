{"id": "HfuFAY9e5oS", "cdate": 1683672077696, "mdate": 1683672077696, "content": {"title": "Mask wearing in community settings reduces SARS-CoV-2 transmission", "abstract": "The effectiveness of mask wearing at controlling severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) transmission has been unclear. While masks are known to substantially reduce disease transmission in healthcare settings [D. K. Chu et al., Lancet 395, 1973\u20131987 (2020); J. Howard et al., Proc. Natl. Acad. Sci. U.S.A. 118, e2014564118 (2021); Y. Cheng et al., Science eabg6296 (2021)], studies in community settings report inconsistent results [H. M. Ollila et al., medRxiv (2020); J. Brainard et al., Eurosurveillance 25, 2000725 (2020); T. Jefferson et al., Cochrane Database Syst. Rev. 11, CD006207 (2020)]. Most such studies focus on how masks impact transmission, by analyzing how effective government mask mandates are. However, we find that widespread voluntary mask wearing, and other data limitations, make mandate effectiveness a poor proxy for mask-wearing effectiveness. We directly analyze the effect of mask wearing on SARS-CoV-2 transmission, drawing on several datasets covering 92 regions on six continents, including the largest survey of wearing behavior (\ud835\udc5b=\n 20 million) [F. Kreuter et al., https://gisumd.github.io/COVID-19-API-Documentation (2020)]. Using a Bayesian hierarchical model, we estimate the effect of mask wearing on transmission, by linking reported wearing levels to reported cases in each region, while adjusting for mobility and nonpharmaceutical interventions (NPIs), such as bans on large gatherings. Our estimates imply that the mean observed level of mask wearing corresponds to a 19% decrease in the reproduction number R. We also assess the robustness of our results in 60 tests spanning 20 sensitivity analyses. In light of these results, policy makers can effectively reduce transmission by intervening to increase mask wearing."}}
{"id": "sK0X9ZmlPkF", "cdate": 1577836800000, "mdate": 1630423574044, "content": {"title": "Simulation-Based Inference for Global Health Decisions", "abstract": "The COVID-19 pandemic has highlighted the importance of in-silico epidemiological modelling in predicting the dynamics of infectious diseases to inform health policy and decision makers about suitable prevention and containment strategies. Work in this setting involves solving challenging inference and control problems in individual-based models of ever increasing complexity. Here we discuss recent breakthroughs in machine learning, specifically in simulation-based inference, and explore its potential as a novel venue for model calibration to support the design and evaluation of public health interventions. To further stimulate research, we are developing software interfaces that turn two cornerstone COVID-19 and malaria epidemiology models COVID-sim, (https://github.com/mrc-ide/covid-sim/) and OpenMalaria (https://github.com/SwissTPH/openmalaria) into probabilistic programs, enabling efficient interpretable Bayesian inference within those simulators."}}
{"id": "rJeMcy2EtH", "cdate": 1571237769545, "mdate": null, "content": {"title": "Efficient Bayesian Inference for Nested Simulators", "abstract": "We introduce two approaches for conducting efficient Bayesian inference in stochastic simulators containing nested stochastic sub-procedures, i.e., internal procedures for which the density cannot be calculated directly such as rejection sampling loops. The resulting class of simulators are used extensively throughout the sciences and can be interpreted as probabilistic generative models. However, drawing inferences from them poses a substantial challenge due to the inability to evaluate even their unnormalised density, preventing the use of many standard inference procedures like Markov Chain Monte Carlo (MCMC). To address this, we introduce inference algorithms based on a two-step approach that first approximates the conditional densities of the individual sub-procedures, before using these approximations to run MCMC methods on the full program. Because the sub-procedures can be dealt with separately and are lower-dimensional than that of the overall problem, this two-step process allows them to be isolated and thus be tractably dealt with, without placing restrictions on the overall dimensionality of the problem. We demonstrate the utility of our approach on a simple, artificially constructed simulator."}}
{"id": "TCUPpX75VXj", "cdate": 1546300800000, "mdate": null, "content": {"title": "Amortized Rejection Sampling in Universal Probabilistic Programming", "abstract": "Naive approaches to amortized inference in probabilistic programs with unbounded loops can produce estimators with infinite variance. This is particularly true of importance sampling inference in programs that explicitly include rejection sampling as part of the user-programmed generative procedure. In this paper we develop a new and efficient amortized importance sampling estimator. We prove finite variance of our estimator and empirically demonstrate our method's correctness and efficiency compared to existing alternatives on generative programs containing rejection sampling loops and discuss how to implement our method in a generic probabilistic programming framework."}}
{"id": "HyWJP_W_-B", "cdate": 1514764800000, "mdate": null, "content": {"title": "Faithful Inversion of Generative Models for Effective Amortized Inference", "abstract": "Inference amortization methods share information across multiple posterior-inference problems, allowing each to be carried out more efficiently. Generally, they require the inversion of the dependency structure in the generative model, as the modeller must learn a mapping from observations to distributions approximating the posterior. Previous approaches have involved inverting the dependency structure in a heuristic way that fails to capture these dependencies correctly, thereby limiting the achievable accuracy of the resulting approximations. We introduce an algorithm for faithfully, and minimally, inverting the graphical model structure of any generative model. Such inverses have two crucial properties: (a) they do not encode any independence assertions that are absent from the model and; (b) they are local maxima for the number of true independencies encoded. We prove the correctness of our approach and empirically show that the resulting minimally faithful inverses lead to better inference amortization than existing heuristic approaches."}}
{"id": "HkWFnGbdZr", "cdate": 1514764800000, "mdate": null, "content": {"title": "Querying Word Embeddings for Similarity and Relatedness", "abstract": ""}}
{"id": "ryh7qqGRZ", "cdate": 1509235236088, "mdate": null, "content": {"title": "End-to-end Training of Differentiable Pipelines Across Machine Learning Frameworks", "abstract": "In this work we present a unified interface and methodology for performing end-to-end gradient-based refinement of pipelines of differentiable machine-learning primitives.  This is distinguished from recent interoperability efforts such as the Open Neural Network Exchange (ONNX) format and other language-centric cross-compilation approaches in that the final pipeline does not need to be implemented nor trained in the same language nor cross-compiled into any single language; in other words, primitives may be written and pre-trained in PyTorch, TensorFlow, Caffe, scikit-learn or any of the other popular machine learning frameworks and fine-tuned end-to-end while being executed directly in their host frameworks.  Provided primitives expose our proposed interface, it is possible to automatically compose all such primitives and refine them based on an end-to-end loss."}}
{"id": "TXbNggO3coN", "cdate": 1483228800000, "mdate": 1630423574049, "content": {"title": "Using synthetic data to train neural networks is model-based reasoning", "abstract": "We draw a formal connection between using synthetic training data to optimize neural network parameters and approximate, Bayesian, model-based reasoning. In particular, training a neural network using synthetic data can be viewed as learning a proposal distribution generator for approximate inference in the synthetic-data generative model. We demonstrate this connection in a recognition task where we develop a novel Captcha-breaking architecture and train it using synthetic data, demonstrating both state-of-the-art performance and a way of computing task-specific posterior uncertainty. Using a neural network trained this way, we also demonstrate successful breaking of real-world Captchas currently used by Facebook and Wikipedia. Reasoning from these empirical results and drawing connections with Bayesian modeling, we discuss the robustness of synthetic data results and suggest important considerations for ensuring good neural network generalization when training with synthetic data."}}
{"id": "FvRl5l0sJV", "cdate": 1483228800000, "mdate": 1630423574048, "content": {"title": "Using Synthetic Data to Train Neural Networks is Model-Based Reasoning", "abstract": "We draw a formal connection between using synthetic training data to optimize neural network parameters and approximate, Bayesian, model-based reasoning. In particular, training a neural network using synthetic data can be viewed as learning a proposal distribution generator for approximate inference in the synthetic-data generative model. We demonstrate this connection in a recognition task where we develop a novel Captcha-breaking architecture and train it using synthetic data, demonstrating both state-of-the-art performance and a way of computing task-specific posterior uncertainty. Using a neural network trained this way, we also demonstrate successful breaking of real-world Captchas currently used by Facebook and Wikipedia. Reasoning from these empirical results and drawing connections with Bayesian modeling, we discuss the robustness of synthetic data results and suggest important considerations for ensuring good neural network generalization when training with synthetic data."}}
{"id": "SkNT2pgdbS", "cdate": 1199145600000, "mdate": null, "content": {"title": "Potential-based Shaping in Model-based Reinforcement Learning", "abstract": "Potential-based shaping was designed as a way of introducing background knowledge into model-free reinforcement-learning algorithms. By identifying states that are likely to have high value, this approach can decrease experience complexity--the number of trials needed to find near-optimal behavior. An orthogonal way of decreasing experience complexity is to use a model-based learning approach, building and exploiting an explicit transition model. In this paper, we show how potential-based shaping can be redefined to work in the model-based setting to produce an algorithm that shares the benefits of both ideas."}}
