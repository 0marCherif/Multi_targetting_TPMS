{"id": "J47HOZxn_z", "cdate": 1685577600000, "mdate": 1683889110088, "content": {"title": "Safe multi-agent reinforcement learning for multi-robot control", "abstract": ""}}
{"id": "lDUjg3LOLl", "cdate": 1672531200000, "mdate": 1681541311966, "content": {"title": "A Human-Centered Safe Robot Reinforcement Learning Framework with Interactive Behaviors", "abstract": ""}}
{"id": "rgNQJhaYun6", "cdate": 1640995200000, "mdate": 1683889110283, "content": {"title": "A Circle Grid-based Approach for Obstacle Avoidance Motion Planning of Unmanned Surface Vehicles", "abstract": "Aiming at an obstacle avoidance problem with dynamic constraints for Unmanned Surface Vehicle (USV), a method based on Circle Grid Trajectory Cell (CGTC) is proposed. Firstly, the ship model and standardization rules are constructed to develop and constrain the trajectory, respectively. Secondly, by analyzing the properties of the circle grid, the circle grid tree is produced to guide the motion of the USV. Then, the kinematics and dynamics of the USV are considered through the on-line trajectory generator by designing a relational function that links the rudder angle, heading angle, and the central angle of the circle grid. Finally, obstacle avoidance is achieved by leveraging the on-line trajectory generator to choose a safe, smooth, and efficient path for the USV. The experimental results indicate that the proposed method can avoid both static and dynamic obstacles, have better performance in terms of distance cost and steering cost comparing with the related methods, and our method only takes 50% steering cost of the grid-based method; the collision avoidance path not only conforms to the USV dynamic characteristic but also provides a reference of steering command."}}
{"id": "a65KFXKcK3O", "cdate": 1640995200000, "mdate": 1667354031689, "content": {"title": "A Review of Safe Reinforcement Learning: Methods, Theory and Applications", "abstract": "Reinforcement learning (RL) has achieved tremendous success in many complex decision making tasks. When it comes to deploying RL in the real world, safety concerns are usually raised, leading to a growing demand for safe RL algorithms, such as in autonomous driving and robotics scenarios. While safety control has a long history, the study of safe RL algorithms is still in the early stages. To establish a good foundation for future research in this thread, in this paper, we provide a review for safe RL from the perspectives of methods, theory and applications. Firstly, we review the progress of safe RL from five dimensions and come up with five problems that are crucial for safe RL being deployed in real-world applications, coined as \"2H3W\". Secondly, we analyze the theory and algorithm progress from the perspectives of answering the \"2H3W\" problems. Then, the sample complexity of safe RL methods is reviewed and discussed, followed by an introduction of the applications and benchmarks of safe RL algorithms. Finally, we open the discussion of the challenging problems in safe RL, hoping to inspire more future research on this thread. To advance the study of safe RL algorithms, we release a benchmark suite, an open-sourced repository containing the implementations of major safe RL algorithms, along with tutorials at the link: https://github.com/chauncygu/Safe-Reinforcement-Learning-Baselines.git."}}
{"id": "6q_Wd2HlMNR", "cdate": 1640995200000, "mdate": 1667354031218, "content": {"title": "Constrained Reinforcement Learning for Vehicle Motion Planning with Topological Reachability Analysis", "abstract": "Rule-based traditional motion planning methods usually perform well with prior knowledge of the macro-scale environments but encounter challenges in unknown and uncertain environments. Deep reinforcement learning (DRL) is a solution that can effectively deal with micro-scale unknown and uncertain environments. Nevertheless, DRL is unstable and lacks interpretability. Therefore, it raises a new challenge: how to combine the effectiveness and overcome the drawbacks of the two methods while guaranteeing stability in uncertain environments. In this study, a multi-constraint and multi-scale motion planning method is proposed for automated driving with the use of constrained reinforcement learning (RL), named RLTT, and comprising RL, a topological reachability analysis used for vehicle path space (TPS), and a trajectory lane model (TLM). First, a dynamic model of vehicles is formulated; then, TLM is developed on the basis of the dynamic model, thus constraining RL action and state space. Second, macro-scale path planning is achieved through TPS, and in the micro-scale range, discrete routing points are achieved via RLTT. Third, the proposed motion planning method is designed by combining sophisticated rules, and a theoretical analysis is provided to guarantee the efficiency of our method. Finally, related experiments are conducted to evaluate the effectiveness of the proposed method; our method can reduce 19.9% of the distance cost in the experiments as compared to the traditional method. Experimental results indicate that the proposed method can help mitigate the gap between data-driven and traditional methods, provide better performance for automated driving, and facilitate the use of RL methods in more fields."}}
{"id": "5hSs2BsA_Myh", "cdate": 1640995200000, "mdate": 1667354032495, "content": {"title": "KAM-Net: Keypoint-Aware and Keypoint-Matching Network for Vehicle Detection From 2-D Point Cloud", "abstract": "Two-dimesional (2-D) LiDAR is an efficient alternative sensor for vehicle detection, which is one of the most critical tasks in autonomous driving. Compared to the fully developed 3-D LiDAR vehicle detection, 2-D LiDAR vehicle detection has much room to improve. Most existing state-of-the-art works represent 2-D point clouds as pseudo-images and then perform detection with traditional object detectors on 2-D images. However, they ignore the sparse representation and geometric information of vehicles in the 2-D cloud points. To address these issues, in this article, we present a novel <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">keypoint-aware and keypoint-matching network</i> termed as KAM-Net, which focuses on better detecting the vehicles by explicitly capturing and extracting the sparse information of L-shape in 2-D LiDAR point clouds. The whole framework consists of two stages\u2014namely, keypoint-aware stage and keypoint-matching stage. The keypoint-aware stage utilizes the heatmap and edge extraction module to simultaneously predict the position of L-shaped keypoints and inflection offset of L-shaped endpoints. The keypoint-matching stage is followed to group the keypoints and produce the oriented bounding boxes with axis by utilizing the endpoint-matching and L-shaped-matching methods. Further, we conduct extensive experiments on a recently released public dataset to evaluate the effectiveness of our approach. The results show that our KAM-Net achieves a new state-of-the-art performance. The source code is available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/ispc-lab/KAM-Net</uri> ."}}
{"id": "BlyXYc4wF2-", "cdate": 1632875480541, "mdate": null, "content": {"title": "Multi-Agent Constrained Policy Optimisation ", "abstract": "Developing reinforcement learning algorithms that satisfy safety constraints is becoming increasingly important in real-world applications. In multi-agent reinforcement learning (MARL) settings, policy optimisation with safety awareness is particularly challenging because each individual agent has to not only meet its own safety constraints, but also consider those of others so that their joint behaviour can be guaranteed safe. Despite its importance, the problem of safe multi-agent learning has not been rigorously studied; very few solutions have been proposed, nor a sharable testing environment or benchmarks. To fill these gaps, in this work, we formulate the safe MARL problem as a constrained Markov game and solve it with policy optimisation methods. Our solutions---Multi-Agent Constrained Policy Optimisation (MACPO) and MAPPO-Lagrangian---leverage the theories from both constrained policy optimisation and multi-agent trust region learning. Crucially, our methods enjoy theoretical guarantees of both monotonic improvement in reward and satisfaction of safety constraints at every iteration. To examine the effectiveness of our methods, we develop the benchmark suite of Safe Multi-Agent MuJoCo that involves a variety of  MARL baselines. Experimental results justify that MACPO/MAPPO-Lagrangian can consistently satisfy safety constraints, meanwhile achieving comparable performance to strong baselines."}}
{"id": "jI97GGA0H_", "cdate": 1621629987108, "mdate": null, "content": {"title": "Settling the Variance of Multi-Agent Policy Gradients", "abstract": "Policy gradient (PG) methods are popular reinforcement learning (RL) methods where a baseline is often applied to reduce the variance of gradient estimates. In multi-agent RL (MARL), although the PG theorem can be naturally extended, the effectiveness of multi-agent PG (MAPG)  methods degrades as the variance of gradient estimates increases rapidly with the number of agents.  In this paper, we offer a rigorous analysis of MAPG methods by, firstly, quantifying the contributions of the number of agents and agents' explorations to the variance of MAPG estimators. Based on this analysis, we derive the optimal baseline (OB) that achieves the minimal variance. In comparison to the OB, we measure the excess variance of existing MARL algorithms such as vanilla MAPG and COMA. Considering using deep neural networks,  we also propose a surrogate version of OB, which can be seamlessly plugged into any existing PG methods in MARL.   On benchmarks of Multi-Agent MuJoCo and StarCraft challenges, our OB technique effectively stabilises training and improves the performance of multi-agent PPO  and COMA algorithms by a significant margin.  Code is released at  \\url{https://github.com/morning9393/Optimal-Baseline-for-Multi-agent-Policy-Gradients}. "}}
{"id": "xLH06zCCnv", "cdate": 1609459200000, "mdate": 1681541312431, "content": {"title": "Settling the Variance of Multi-Agent Policy Gradients", "abstract": ""}}
{"id": "KIF99UVfxk", "cdate": 1609459200000, "mdate": 1681541312412, "content": {"title": "Multi-Agent Constrained Policy Optimisation", "abstract": ""}}
