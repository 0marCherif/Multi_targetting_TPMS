{"id": "0HE8ePmffq", "cdate": 1681833045742, "mdate": null, "content": {"title": "Amortised Inference in Neural Networks for Small-Scale Probabilistic Meta-Learning", "abstract": "N/A"}}
{"id": "zwywBS3GyFs", "cdate": 1677713826810, "mdate": null, "content": {"title": "GeValDi: Generative Validation of Discriminative Models", "abstract": "Evaluation of machine learning (ML) models is critically important for reliable use. Though typically done via unseen data, such validation datasets often need to be large and hard to procure; additionally, mutliple models may perform equally well on such datasets. To address these challenges, we offer GeValdi: a data-efficient method to validate discriminative classifiers by creating samples where such classifiers maximally differ. We demonstrate how such ``maximally different samples'' can be constructed and leveraged to probe the failure modes of classifiers and offer a hierarchically-aware metric to further support fine-grained, comparative model evaluation. "}}
{"id": "2BZDR5JMMS_", "cdate": 1676472365305, "mdate": null, "content": {"title": "GeValDi: Generative Validation of Discriminative Models", "abstract": "The evaluation of machine learning (ML) models is a core tenet of trustworthy use. Evaluation is typically done via a held-out dataset. However, such validation datasets often need to be large and are hard to procure; further, multiple models may perform equally well on such sets. To address these challenges, we offer GeValdi: an efficient method to validate discriminative classifiers by creating samples where such classifiers maximally differ. We demonstrate how such ``maximally different samples'' can be constructed via and leveraged to probe the failure mode of classifiers and offer a hierarchically-aware metric to further support fine-grained, comparative model evaluation. "}}
{"id": "RQQxCLpCVr9", "cdate": 1664815573692, "mdate": null, "content": {"title": "Causal Reasoning in the Presence of Latent Confounders via Neural ADMG Learning", "abstract": "Latent confounding has been a long-standing obstacle for causal reasoning from observational data. One popular approach is to model the data using acyclic directed mixed graphs (ADMGs), which describe ancestral relations between variables using directed and bidirected edges. However, existing methods using ADMGs are based on either linear functional assumptions or a discrete search that is complicated to use and lacks computational tractability for large datasets. In this work, we further extend the existing body of work and develop a novel gradient-based approach to learning an ADMG with nonlinear functional relations from observational data. We first show that the presence of latent confounding is identifiable under the assumptions of bow-free ADMGs with nonlinear additive noise models. With this insight, we propose a novel neural causal model based on autoregressive flows. This not only enables us to model complex causal relationships behind the data, but also estimate their functional relationships (hence treatment effects) simultaneously. We further validate our approach via experiments on both synthetic and real-world datasets, and demonstrate the competitive performance against relevant baselines."}}
{"id": "dcN0CaXQhT", "cdate": 1663850397415, "mdate": null, "content": {"title": "Causal Reasoning in the Presence of Latent Confounders via Neural ADMG Learning", "abstract": "Latent confounding has been a long-standing obstacle for causal reasoning from observational data. One popular approach is to model the data using acyclic directed mixed graphs (ADMGs), which describe ancestral relations between variables using directed and bidirected edges. However, existing methods using ADMGs are based on either linear functional assumptions or a discrete search that is complicated to use and lacks computational tractability for large datasets. In this work, we further extend the existing body of work and develop a novel gradient-based approach to learning an ADMG with nonlinear functional relations from observational data. We first show that the presence of latent confounding is identifiable under the assumptions of bow-free ADMGs with nonlinear additive noise models. With this insight, we propose a novel neural causal model based on autoregressive flows. This not only enables us to model complex causal relationships behind the data, but also estimate their functional relationships (hence treatment effects) simultaneously. We further validate our approach via experiments on both synthetic and real-world datasets, and demonstrate the competitive performance against relevant baselines."}}
{"id": "czv8Ac3Kg7l", "cdate": 1601308355756, "mdate": null, "content": {"title": "Sparse Gaussian Process Variational Autoencoders", "abstract": "Large, multi-dimensional spatio-temporal datasets are omnipresent in modern science and engineering. An effective framework for handling such data are Gaussian process deep generative models (GP-DGMs), which employ GP priors over the latent variables of DGMs. Existing approaches for performing inference in GP-DGMs do not support sparse GP approximations based on inducing points, which are essential for the computational efficiency of GPs, nor do they handle missing data -- a natural occurrence in many spatio-temporal datasets -- in a principled manner. We address these shortcomings with the development of the sparse Gaussian process variational autoencoder (SGP-VAE), characterised by the use of partial inference networks for parameterising sparse GP approximations. Leveraging the benefits of amortised variational inference, the SGP-VAE enables inference in multi-output sparse GPs on previously unobserved data with no additional training. The SGP-VAE is evaluated in a variety of experiments where it outperforms alternative approaches including multi-output GPs and structured VAEs."}}
