{"id": "LxAuq3XJaD", "cdate": 1546300800000, "mdate": null, "content": {"title": "A circular-linear dependence measure under Johnson-Wehrly distributions and its application in Bayesian networks.", "abstract": "Highlights \u2022 The proposed subfamily of Johnson\u2013Wehrly family of distributions has tractable and well-known conditionals. \u2022 A circular mutual information is presented, expressed in a simple and closed form. \u2022 A circular-linear mutual information is also proposed, in a simple and closed form. \u2022 These measures can be used to develop circular-linear Bayesian network models. \u2022 These models outperform traditional linear Bayesian network models. Abstract Circular data jointly observed with linear data are common in various disciplines. Since circular data require different techniques than linear data, it is often misleading to use usual dependence measures for joint data of circular and linear observations. Moreover, although a mutual information measure between circular variables exists, the measure has drawbacks in that it is defined only for a bivariate extension of the wrapped Cauchy distribution and has to be approximated using numerical methods. In this paper, we introduce two measures of dependence, namely, (i) circular-linear mutual information as a measure of dependence between circular and linear variables and (ii) circular-circular mutual information as a measure of dependence between two circular variables. It is shown that the expression for the proposed circular-linear mutual information can be greatly simplified for a subfamily of Johnson\u2013Wehrly distributions. We apply these two dependence measures to learn a circular-linear tree-structured Bayesian network that combines circular and linear variables. To illustrate and evaluate our proposal, we perform experiments with simulated data. We also use a real meteorological data set from different European stations to create a circular-linear tree-structured Bayesian network model. Previous article in issue Next article in issue"}}
{"id": "MBZkQ8C0RqQ", "cdate": 1451606400000, "mdate": null, "content": {"title": "Parametric bootstrap goodness-of-fit testing for Wehrly-Johnson bivariate circular distributions.", "abstract": "The Wehrly\u2013Johnson family of bivariate circular distributions is by far the most general one currently available for modelling data on the torus. It allows complete freedom in the specification of the marginal circular densities as well as the binding circular density which regulates any dependence that might exist between them. We propose a parametric bootstrap approach for testing the goodness-of-fit of Wehrly\u2013Johnson distributions when the forms of their marginal and binding densities are assumed known. The approach admits the use of any test for toroidal uniformity, and we consider versions of it incorporating three such tests. Simulation is used to illustrate the operating characteristics of the approach when the underlying distribution is assumed to be bivariate wrapped Cauchy. An analysis of wind direction data recorded at a Texan weather station illustrates the use of the proposed goodness-of-fit testing procedure."}}
{"id": "LpOdw1Sh2Mp", "cdate": 1293840000000, "mdate": null, "content": {"title": "Projective Power Entropy and Maximum Tsallis Entropy Distributions.", "abstract": "We discuss a one-parameter family of generalized cross entropy between two distributions with the power index, called the projective power entropy. The cross entropy is essentially reduced to the Tsallis entropy if two distributions are taken to be equal. Statistical and probabilistic properties associated with the projective power entropy are extensively investigated including a characterization problem of which conditions uniquely determine the projective power entropy up to the power index. A close relation of the entropy with the Lebesgue space Lp and the dual Lq is explored, in which the escort distribution associates with an interesting property. When we consider maximum Tsallis entropy distributions under the constraints of the mean vector and variance matrix, the model becomes a multivariate q-Gaussian model with elliptical contours, including a Gaussian and t-distribution model. We discuss the statistical estimation by minimization of the empirical loss associated with the projective power entropy. It is shown that the minimum loss estimator for the mean vector and variance matrix under the maximum entropy model are the sample mean vector and the sample variance matrix. The escort distribution of the maximum entropy distribution plays the key role for the derivation."}}
{"id": "Szkju6ZkXxs", "cdate": 1262304000000, "mdate": null, "content": {"title": "Entropy and Divergence Associated with Power Function and the Statistical Application.", "abstract": "In statistical physics, Boltzmann-Shannon entropy provides good understanding for the equilibrium states of a number of phenomena. In statistics, the entropy corresponds to the maximum likelihood method, in which Kullback-Leibler divergence connects Boltzmann-Shannon entropy and the expected log-likelihood function. The maximum likelihood estimation has been supported for the optimal performance, which is known to be easily broken down in the presence of a small degree of model uncertainty. To deal with this problem, a new statistical method, closely related to Tsallis entropy, is proposed and shown to be robust for outliers, and we discuss a local learning property associated with the method."}}
