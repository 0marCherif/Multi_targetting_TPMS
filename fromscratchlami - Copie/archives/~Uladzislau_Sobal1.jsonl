{"id": "92sjaWqLDP", "cdate": 1672531200000, "mdate": 1683629704813, "content": {"title": "A Cookbook of Self-Supervised Learning", "abstract": "Self-supervised learning, dubbed the dark matter of intelligence, is a promising path to advance machine learning. Yet, much like cooking, training SSL methods is a delicate art with a high barrier to entry. While many components are familiar, successfully training a SSL method involves a dizzying set of choices from the pretext tasks to training hyper-parameters. Our goal is to lower the barrier to entry into SSL research by laying the foundations and latest SSL recipes in the style of a cookbook. We hope to empower the curious researcher to navigate the terrain of methods, understand the role of the various knobs, and gain the know-how required to explore how delicious SSL can be."}}
{"id": "dfPuLye6RvY", "cdate": 1663849939844, "mdate": null, "content": {"title": "Light-weight probing of unsupervised representations for Reinforcement Learning", "abstract": "Unsupervised visual representation learning offers the opportunity to leverage large corpora of unlabeled trajectories to form useful visual representations, which can benefit the training of reinforcement learning (RL) algorithms. However, evaluating the fitness of such representations requires training RL algorithms which is both computationally intensive and has high variance outcomes. To alleviate this issue, we design an evaluation protocol for unsupervised RL representations with lower variance and up to 600x lower computational cost. Inspired by the vision community, we propose two linear probing tasks: predicting the reward observed in a given state, and predicting the action of an expert in a given state. These two tasks are generally applicable to many RL domains, and we show through rigorous experimentation that they correlate strongly with the actual downstream control performance on the Atari100k Benchmark. This provides a better method for exploring the space of pretraining algorithms without the need of running RL evaluations for every setting. Leveraging this framework, we further improve existing self-supervised learning (SSL) recipes for RL, highlighting the importance of the forward model, the size of the visual backbone, and the precise formulation of the unsupervised objective."}}
{"id": "SKZlRN6V1Zc", "cdate": 1646378293572, "mdate": null, "content": {"title": "Separating the World and Ego Models for Self-Driving", "abstract": "Training self-driving systems to be robust to the long-tail of driving scenarios is a critical problem.\nModel-based approaches leverage simulation to emulate a wide range of scenarios without putting users at risk in the real world.\nOne promising path to faithful simulation is to train a forward model of the world to predict the future states of both the environment and the ego-vehicle given past states and a sequence of actions.\nIn this paper, we argue that it is beneficial to model the state of the ego-vehicle, which often has simple, predictable and deterministic behavior, separately from the rest of the environment, which is much more complex and highly multimodal.\nWe propose to model the ego-vehicle using a simple and differentiable kinematic model, while training a stochastic convolutional forward model on raster representations of the state to predict the behavior of the rest of the environment.\nWe explore several configurations of such decoupled models, and evaluate their performance both with Model Predictive Control (MPC) and direct policy learning.\nWe test our methods on the task of highway driving and demonstrate lower crash rates and better stability."}}
{"id": "ycVlWMC5G0", "cdate": 1640995200000, "mdate": 1683629704975, "content": {"title": "Light-weight probing of unsupervised representations for Reinforcement Learning", "abstract": "Unsupervised visual representation learning offers the opportunity to leverage large corpora of unlabeled trajectories to form useful visual representations, which can benefit the training of reinforcement learning (RL) algorithms. However, evaluating the fitness of such representations requires training RL algorithms which is computationally intensive and has high variance outcomes. To alleviate this issue, we design an evaluation protocol for unsupervised RL representations with lower variance and up to 600x lower computational cost. Inspired by the vision community, we propose two linear probing tasks: predicting the reward observed in a given state, and predicting the action of an expert in a given state. These two tasks are generally applicable to many RL domains, and we show through rigorous experimentation that they correlate strongly with the actual downstream control performance on the Atari100k Benchmark. This provides a better method for exploring the space of pretraining algorithms without the need of running RL evaluations for every setting. Leveraging this framework, we further improve existing self-supervised learning (SSL) recipes for RL, highlighting the importance of the forward model, the size of the visual backbone, and the precise formulation of the unsupervised objective."}}
{"id": "jrDXY03B2ft", "cdate": 1640995200000, "mdate": 1680275488871, "content": {"title": "Joint Embedding Predictive Architectures Focus on Slow Features", "abstract": ""}}
{"id": "1SHCz8ZSugx", "cdate": 1640995200000, "mdate": 1680275488877, "content": {"title": "Separating the World and Ego Models for Self-Driving", "abstract": ""}}
