{"id": "Ok58hMNXIQ", "cdate": 1664924967929, "mdate": null, "content": {"title": "Towards Discovering Neural Architectures from Scratch", "abstract": "The discovery of neural architectures from scratch is the long-standing goal of Neural Architecture Search (NAS). Searching over a wide spectrum of neural architectures can facilitate the discovery of previously unconsidered but well-performing architectures. In this work, we take a large step towards discovering neural architectures from scratch by expressing architectures algebraically. This algebraic view leads to a more general method for designing search spaces, which allows us to compactly represent search spaces that are 100s of orders of magnitude larger than common spaces from the literature. Further, we propose a Bayesian Optimization strategy to efficiently search over such huge spaces, and demonstrate empirically that both our search space design and our search strategy can be superior to existing baselines. We open source our algebraic NAS approach and provide APIs for PyTorch and TensorFlow at https://github.com/automl/towards_nas_from_scratch."}}
{"id": "UIpwFLrJiDi", "cdate": 1663849879371, "mdate": null, "content": {"title": "Towards Discovering Neural Architectures from Scratch", "abstract": "The discovery of neural architectures from scratch is the long-standing goal of Neural Architecture Search (NAS). Searching over a wide spectrum of neural architectures can facilitate the discovery of previously unconsidered but well-performing architectures. In this work, we take a large step towards discovering neural architectures from scratch by expressing architectures algebraically. This algebraic view leads to a more general method for designing search spaces, which allows us to compactly represent search spaces that are 100s of orders of magnitude larger than common spaces from the literature. Further, we propose a Bayesian Optimization strategy to efficiently search over such huge spaces, and demonstrate empirically that both our search space design and our search strategy can be superior to existing baselines. We open source our algebraic NAS approach and provide APIs for PyTorch and TensorFlow."}}
{"id": "r1HT1AW8PSf", "cdate": 1650547879238, "mdate": null, "content": {"title": "DHA: End-to-End Joint Optimization of Data Augmentation Policy, Hyper-parameter and Architecture", "abstract": "Automated machine learning (AutoML) usually involves several crucial components, such as Data Augmentation (DA) policy, Hyper-Parameter Optimization (HPO), and Neural Architecture Search (NAS). However joint optimization of these components remains challenging due to the largely increased search dimension and the variant input types of each component. In parallel to this, the common practice of searching for the optimal architecture first and then retraining it before deployment in NAS often suffers from the low-performance correlation between the search and retraining stages. An end-to-end solution that integrates the AutoML components and returns a ready-to-use model at the end of the search is desirable. In view of these, we propose DHA, which achieves joint optimization of Data augmentation policy, Hyper-parameter, and Architecture. Specifically, end-to-end NAS is achieved in a differentiable manner by optimizing a compressed lower-dimensional feature space, while DA policy and HPO are updated dynamically at the same time."}}
{"id": "rI54HQv-I-c", "cdate": 1646823196810, "mdate": null, "content": {"title": "Bayesian Generational Population-Based Training", "abstract": "Reinforcement learning (RL) offers the potential for training generally capable agents that can interact autonomously in the real world.\nHowever, one key limitation is the brittleness of RL algorithms to core hyperparameters and network architecture choice.\nFurthermore, non-stationarities such as evolving training data and increased agent complexity mean that different hyperparameters and architectures may be optimal at different points of training.\nThis motivates AutoRL, a class of methods seeking to automate these design choices.\nOne prominent class of AutoRL methods is Population-Based Training (PBT), which have led to impressive performance in several large scale settings.\nIn this paper, we introduce two new innovations in PBT-style methods.\nFirst, we employ trust-region based Bayesian Optimization, enabling full coverage of the high-dimensional mixed hyperparameter search space.\nSecond, we show that using a generational approach, we can also learn both architectures and hyperparameters jointly on-the-fly in a single training run.\nLeveraging the new highly parallelizable Brax physics engine, we show that these innovations lead to dramatic performance gains, significantly outperforming the tuned baseline while learning entire configurations on the fly."}}
{"id": "HW4-ZaHUg5", "cdate": 1645792504936, "mdate": null, "content": {"title": "Bayesian Generational Population-Based Training", "abstract": "Reinforcement learning (RL) offers the potential for training generally capable agents that can interact autonomously in the real world.\nHowever, one key limitation is the brittleness of RL algorithms to core hyperparameters and network architecture choice.\nFurthermore, non-stationarities such as evolving training data and increased agent complexity mean that different hyperparameters and architectures may be optimal at different points of training.\nThis motivates AutoRL, a class of methods seeking to automate these design choices.\nOne prominent class of AutoRL methods is Population-Based Training (PBT), which have led to impressive performance in several large scale settings.\nIn this paper, we introduce two new innovations in PBT-style methods.\nFirst, we employ trust-region based Bayesian Optimization, enabling full coverage of the high-dimensional mixed hyperparameter search space.\nSecond, we show that using a generational approach, we can also learn both architectures and hyperparameters jointly on-the-fly in a single training run.\nLeveraging the new highly parallelizable Brax physics engine, we show that these innovations lead to dramatic performance gains, significantly outperforming the tuned baseline while learning entire configurations on the fly."}}
{"id": "661Wz3zOzlt", "cdate": 1633015336265, "mdate": null, "content": {"title": "DARTS without a Validation Set: Optimizing the Marginal Likelihood", "abstract": "The success of neural architecture search (NAS) has historically been limited by excessive compute requirements. While modern weight-sharing NAS methods such as DARTS are able to finish the search in single-digit GPU days, extracting the final best architecture from the shared weights is notoriously unreliable. Training-Speed-Estimate (TSE), a recently developed generalization estimator with a Bayesian marginal likelihood interpretation, has previously been used in place of the validation loss for gradient-based optimization in DARTS. This prevents the DARTS skip connection collapse, which significantly improves performance on NASBench-201 and the original DARTS search space. We extend those results by applying various DARTS diagnostics and show several unusual behaviors arising from not using a validation set. Furthermore, our experiments yield concrete examples of the depth gap and topology selection in DARTS having a strongly negative impact on the search performance despite generally receiving limited attention in the literature compared to the operations selection. "}}
{"id": "rFJWoYoxrDB", "cdate": 1632875601109, "mdate": null, "content": {"title": "On Redundancy and Diversity in Cell-based Neural Architecture Search", "abstract": "Searching for the architecture cells is a dominant paradigm in NAS. However, little attention has been devoted to the analysis of the cell-based search spaces even though it is highly important for the continual development of NAS. \nIn this work, we conduct an empirical post-hoc analysis of architectures from the popular cell-based search spaces and find that the existing search spaces contain a high degree of redundancy: the architecture performance is less sensitive to changes at large parts of the cells, and universally adopted design rules, like the explicit search for a reduction cell, significantly increase the complexities but have very limited impact on the performance.\nAcross architectures found by a diverse set of search strategies, we consistently find that the parts of the cells that do matter for architecture performance often follow similar and simple patterns. By constraining cells to include these patterns, randomly sampled architectures can match or even outperform the state of the art.\nThese findings cast doubts into our ability to discover truly novel architectures in the existing cell-based search spaces and, inspire our suggestions for improvement to guide future NAS research.\nCode is available at https://github.com/xingchenwan/cell-based-NAS-analysis."}}
{"id": "7oziDfK4Fs", "cdate": 1624022582615, "mdate": null, "content": {"title": "Attacking Graph Classification via Bayesian Optimisation", "abstract": "Graph neural networks have been shown to be vulnerable to adversarial attacks. While the majority of the literature focuses on such vulnerability in node-level classification tasks, little effort has been dedicated to attacks on graph-level classification, an important problem with numerous real-life applications such as biochemistry and social network analysis. The few existing methods often require unrealistic setups, such as access to internal information of the victim models, or an impractically-large number of queries. We present a novel Bayesian optimisation-based attack method  for graph classification models. Our method is black-box, query-efficient and parsimonious with respect to the perturbation applied. We empirically validate the effectiveness and flexibility of the proposed method and analyse patterns behind the adversarial samples produced, which may shed further light on the adversarial robustness of graph classification models. "}}
{"id": "eXxnkL3QfDY", "cdate": 1621630205187, "mdate": null, "content": {"title": "Adversarial Attacks on Graph Classifiers via Bayesian Optimisation", "abstract": "Graph neural networks, a popular class of models effective in a wide range of graph-based learning tasks, have been shown to be vulnerable to adversarial attacks. While the majority of the literature focuses on such vulnerability in node-level classification tasks, little effort has been dedicated to analysing adversarial attacks on graph-level classification, an important problem with numerous real-life applications such as biochemistry and social network analysis. The few existing methods often require unrealistic setups, such as access to internal information of the victim models, or an impractically-large number of queries. We present a novel Bayesian optimisation-based attack method for graph classification models. Our method is black-box, query-efficient and parsimonious with respect to the perturbation applied. We empirically validate the effectiveness and flexibility of the proposed method on a wide range of graph classification tasks involving varying graph properties, constraints and modes of attack. Finally, we analyse common interpretable patterns behind the adversarial samples produced, which may shed further light on the adversarial robustness of graph classification models. "}}
{"id": "5j_lH4OpZBl", "cdate": 1621630205187, "mdate": null, "content": {"title": "Adversarial Attacks on Graph Classifiers via Bayesian Optimisation", "abstract": "Graph neural networks, a popular class of models effective in a wide range of graph-based learning tasks, have been shown to be vulnerable to adversarial attacks. While the majority of the literature focuses on such vulnerability in node-level classification tasks, little effort has been dedicated to analysing adversarial attacks on graph-level classification, an important problem with numerous real-life applications such as biochemistry and social network analysis. The few existing methods often require unrealistic setups, such as access to internal information of the victim models, or an impractically-large number of queries. We present a novel Bayesian optimisation-based attack method for graph classification models. Our method is black-box, query-efficient and parsimonious with respect to the perturbation applied. We empirically validate the effectiveness and flexibility of the proposed method on a wide range of graph classification tasks involving varying graph properties, constraints and modes of attack. Finally, we analyse common interpretable patterns behind the adversarial samples produced, which may shed further light on the adversarial robustness of graph classification models. "}}
