{"id": "E4-uRvmKkeB", "cdate": 1663850033173, "mdate": null, "content": {"title": "Beyond Traditional Transfer Learning: Co-finetuning for Action Localisation", "abstract": "Transfer learning is the predominant paradigm for training deep networks on small target datasets. Models are typically pretrained on large \u201cupstream\u201d datasets for classification, as such labels are easy to collect, and then finetuned on downstream\u201d tasks such as action localisation, which are smaller due to their finer-grained annotations.\n\nIn this paper, we question this approach, and propose co-finetuning -- simultaneously training a single model on multiple \u201cupstream\u201d and \u201cdownstream\u201d tasks. We demonstrate that co-finetuning outperforms traditional transfer learning when using the same total amount of data, and also show how we can easily extend our approach to multiple \u201cupstream\u201d datasets to further improve performance. In particular, co-finetuning significantly improves the performance on rare classes in our downstream task, as it has a regularising effect, and enables the network to learn feature representations that transfer between different datasets. Finally, we observe how co-finetuning with public, video classification datasets, we are able to achieve state-of-the-art results for spatio-temporal action localisation on the challenging AVA and AVA-Kinetics datasets, outperforming recent works which develop intricate models."}}
{"id": "7Rk4Cdg_mRO", "cdate": 1654348672759, "mdate": null, "content": {"title": "SI-Score: An image dataset for fine-grained analysis of robustness to object location, rotation and size", "abstract": "Before deploying machine learning models it is critical to assess their robustness. In the context of deep neural networks for image understanding, changing the object location, rotation and size may affect the predictions in non-trivial ways. In this work we perform a fine-grained analysis of robustness with respect to these factors of variation using SI-Score, a synthetic dataset. In particular, we investigate ResNets, Vision Transformers and CLIP, and identify interesting qualitative differences between these."}}
{"id": "QRBvLayFXI", "cdate": 1621629814500, "mdate": null, "content": {"title": "Revisiting the Calibration of Modern Neural Networks", "abstract": "Accurate estimation of predictive uncertainty (model calibration) is essential for the safe application of neural networks. Many instances of miscalibration in modern neural networks have been reported, suggesting a trend that newer, more accurate models produce poorly calibrated predictions. Here, we revisit this question for recent state-of-the-art image classification models. We systematically relate model calibration and accuracy, and find that the most recent models, notably those not using convolutions, are among the best calibrated. Trends observed in prior model generations, such as decay of calibration with distribution shift or model size, are less pronounced in recent architectures. We also show that model size and amount of pretraining do not fully explain these differences, suggesting that architecture is a major determinant of calibration properties."}}
{"id": "WxqaLcKJuSj", "cdate": 1609459200000, "mdate": null, "content": {"title": "SI-Score: An image dataset for fine-grained analysis of robustness to object location, rotation and size", "abstract": "Before deploying machine learning models it is critical to assess their robustness. In the context of deep neural networks for image understanding, changing the object location, rotation and size may affect the predictions in non-trivial ways. In this work we perform a fine-grained analysis of robustness with respect to these factors of variation using SI-Score, a synthetic dataset. In particular, we investigate ResNets, Vision Transformers and CLIP, and identify interesting qualitative differences between these."}}
{"id": "Rq2zsZqhkv", "cdate": 1601290500080, "mdate": null, "content": {"title": "Fast Differentiable Sorting and Ranking", "abstract": "The sorting operation is one of the most basic and commonly used building blocks in computer programming. In machine learning, it is commonly used for robust statistics. However, seen as a function, it is piecewise linear and as a result includes many kinks at which it is non-differentiable. More problematic is the related ranking operator, commonly used for order statistics and ranking metrics. It is a piecewise constant function, meaning that its derivatives are null or undefined. While numerous works have proposed differentiable proxies to sorting and ranking, they do not achieve the  time complexity one would expect from sorting and ranking operations. In this paper, we propose the first differentiable sorting and ranking operators with  time and  space complexity. Our proposal in addition enjoys exact computation and differentiation. We achieve this feat by constructing differentiable sorting and ranking operators as projections onto the permutahedron, the convex hull of permutations, and using a reduction to isotonic optimization. Empirically, we confirm that our approach is an order of magnitude faster than existing approaches and showcase two novel applications: differentiable Spearman's rank correlation coefficient and soft least trimmed squares."}}
{"id": "WDwOJ7b21Rh", "cdate": 1598677021111, "mdate": null, "content": {"title": "On Robustness and Transferability of Convolutional Neural Networks", "abstract": "Modern deep convolutional networks (CNNs) are often criticized for not generalizing under distributional shifts. However, several recent breakthroughs in transfer learning suggest that these networks can cope with severe distribution shifts and successfully adapt to new tasks from a few training examples. In this work we revisit the out-of-distribution and transfer performance of modern image classification CNNs and investigate the impact of the pre-training data size, the model scale, and the data preprocessing pipeline. We find that increasing both the training set and model sizes significantly improve the distributional shift robustness. Furthermore, we show that, perhaps surprisingly, simple changes in the preprocessing such as modifying the image resolution can significantly mitigate robustness issues in some cases. Finally, we outline the shortcomings of existing robustness evaluation datasets and introduce a synthetic dataset we use for a systematic analysis across common factors of variation."}}
{"id": "Hqf3jq2A1no", "cdate": 1577836800000, "mdate": null, "content": {"title": "Precision-Recall Curves Using Information Divergence Frontiers", "abstract": "Despite the tremendous progress in the estimation of generative models, the development of tools for diagnosing their failures and assessing their performance has advanced at a much slower pace. Re..."}}
{"id": "0XGpiBLYy00", "cdate": 1577836800000, "mdate": null, "content": {"title": "Representation learning from videos in-the-wild: An object-centric approach", "abstract": "We propose a method to learn image representations from uncurated videos. We combine a supervised loss from off-the-shelf object detectors and self-supervised losses which naturally arise from the video-shot-frame-object hierarchy present in each video. We report competitive results on 19 transfer learning tasks of the Visual Task Adaptation Benchmark (VTAB), and on 8 out-of-distribution-generalization tasks, and discuss the benefits and shortcomings of the proposed approach. In particular, it improves over the baseline on all 18/19 few-shot learning tasks and 8/8 out-of-distribution generalization tasks. Finally, we perform several ablation studies and analyze the impact of the pretrained object detector on the performance across this suite of tasks."}}
{"id": "HyxY6JHKwr", "cdate": 1569439680991, "mdate": null, "content": {"title": "You Only Train Once: Loss-Conditional Training of Deep Networks", "abstract": "In many machine learning problems, loss functions are weighted sums of several terms. A typical approach to dealing with these is to train multiple separate models with different selections of weights and then either choose the best one according to some criterion or keep multiple models if it is desirable to maintain a diverse set of solutions. This is inefficient both at training and at inference time. We propose a method that allows replacing multiple models trained on one loss function each by a single model trained on a distribution of losses. At test time a model trained this way can be conditioned to generate outputs corresponding to any loss from the training distribution of losses. We demonstrate this approach on three tasks with parametrized losses: beta-VAE, learned image compression, and fast style transfer."}}
{"id": "BJena3VtwS", "cdate": 1569438915762, "mdate": null, "content": {"title": "The Visual Task Adaptation Benchmark", "abstract": "Representation learning promises to unlock deep learning for the long tail of vision tasks without expansive labelled datasets. Yet, the absence of a unified yardstick to evaluate general visual representations hinders progress. Many sub-fields promise representations, but each has different evaluation protocols that are either too constrained (linear classification), limited in scope (ImageNet, CIFAR, Pascal-VOC), or only loosely related to representation quality (generation). We present the Visual Task Adaptation Benchmark (VTAB): a diverse, realistic, and challenging benchmark to evaluate representations. VTAB embodies one principle: good representations adapt to unseen tasks with few examples. We run a large VTAB study of popular algorithms, answering questions like: How effective are ImageNet representation on non-standard datasets? Are generative models competitive? Is self-supervision useful if one already has labels?"}}
