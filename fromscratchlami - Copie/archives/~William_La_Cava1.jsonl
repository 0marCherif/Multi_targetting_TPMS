{"id": "yzZtDd9wy4", "cdate": 1677628800000, "mdate": 1681917579306, "content": {"title": "Informative missingness: What can we learn from patterns in missing laboratory data in the electronic health record?", "abstract": ""}}
{"id": "FEHdkQ9ekW", "cdate": 1672531200000, "mdate": 1681917579293, "content": {"title": "Interpretable Symbolic Regression for Data Science: Analysis of the 2022 Competition", "abstract": "Symbolic regression searches for analytic expressions that accurately describe studied phenomena. The main attraction of this approach is that it returns an interpretable model that can be insightful to users. Historically, the majority of algorithms for symbolic regression have been based on evolutionary algorithms. However, there has been a recent surge of new proposals that instead utilize approaches such as enumeration algorithms, mixed linear integer programming, neural networks, and Bayesian optimization. In order to assess how well these new approaches behave on a set of common challenges often faced in real-world data, we hosted a competition at the 2022 Genetic and Evolutionary Computation Conference consisting of different synthetic and real-world datasets which were blind to entrants. For the real-world track, we assessed interpretability in a realistic way by using a domain expert to judge the trustworthiness of candidate models.We present an in-depth analysis of the results obtained in this competition, discuss current challenges of symbolic regression algorithms and highlight possible improvements for future competitions."}}
{"id": "lw1WKaIL3LR", "cdate": 1663850457913, "mdate": null, "content": {"title": "Proportional Multicalibration", "abstract": "Multicalibration is a desirable fairness criteria that constrains calibration error among flexibly-defined groups in the data while maintaining overall calibration. However, when outcome probabilities are correlated with group membership, multicalibrated models can exhibit a higher percent calibration error among groups with lower base rates than groups with higher base rates. As a result, it remains possible for a decision-maker to learn to trust or distrust model predictions for specific groups. To alleviate this, we propose proportional multicalibration, a criteria that constrains the percent calibration error among groups and within prediction bins. We prove that satisfying proportional multicalibration bounds a model's multicalibration as well its differential calibration, a stronger fairness criteria inspired by the fairness notion of sufficiency. We provide an efficient algorithm for post-processing risk prediction models for proportional multicalibration and evaluate it empirically. We conduct simulation studies and investigate a real-world application of PMC-postprocessing to prediction of emergency department patient admissions. We observe that proportional multicalibration is a promising criteria for controlling simultenous measures of calibration fairness of a model over intersectional groups with virtually no cost in terms of classification performance.  "}}
{"id": "pPb2h7KeOu", "cdate": 1640995200000, "mdate": 1681917579359, "content": {"title": "Population Diversity Leads to Short Running Times of Lexicase Selection", "abstract": "In this paper we investigate why the running time of lexicase parent selection is empirically much lower than its worst-case bound of $$O(N \\cdot C)$$ . We define a measure of population diversity and prove that high diversity leads to low running times $$O(N + C)$$ of lexicase selection. We then show empirically that genetic programming populations evolved under lexicase selection are diverse for several program synthesis problems, and explore the resulting differences in running time bounds."}}
{"id": "evT4jBO0CgQ", "cdate": 1640995200000, "mdate": 1681917579447, "content": {"title": "Population Diversity Leads to Short Running Times of Lexicase Selection", "abstract": "In this paper we investigate why the running time of lexicase parent selection is empirically much lower than its worst-case bound of O(N*C). We define a measure of population diversity and prove that high diversity leads to low running times O(N + C) of lexicase selection. We then show empirically that genetic programming populations evolved under lexicase selection are diverse for several program synthesis problems, and explore the resulting differences in running time bounds."}}
{"id": "dwbE1J4O0Up", "cdate": 1640995200000, "mdate": 1681917579352, "content": {"title": "A comparative study of GP-based and state-of-the-art classifiers on a synthetic machine learning benchmark", "abstract": "In this paper we compare performance of genetic programming-based symbolic classifiers on a novel synthetic machine learning benchmark called DIGEN. This framework and collection of 40 different classification problems was designed specifically to differentiate performance of leading machine learning methods."}}
{"id": "RtvaenXQh6n", "cdate": 1640995200000, "mdate": 1681917579409, "content": {"title": "Lexicase selection", "abstract": ""}}
{"id": "PT3vzbR_fc", "cdate": 1640995200000, "mdate": 1681917579325, "content": {"title": "PMLB v1.0: an open-source dataset collection for benchmarking machine learning methods", "abstract": ""}}
{"id": "PJUrBnD51yv", "cdate": 1640995200000, "mdate": 1681917579369, "content": {"title": "SLUG: Feature Selection Using Genetic Algorithms and Genetic Programming", "abstract": "We present SLUG, a method that uses genetic algorithms as a wrapper for genetic programming (GP), to perform feature selection while inducing models. This method is first tested on four regular binary classification datasets, and then on 10 synthetic datasets produced by GAMETES, a tool for embedding epistatic gene-gene interactions into noisy datasets. We compare the results of SLUG with the ones obtained by other GP-based methods that had already been used on the GAMETES problems, concluding that the proposed approach is very successful, particularly on the epistatic datasets. We discuss the merits and weaknesses of SLUG and its various parts, i.e. the wrapper and the learner, and we perform additional experiments, aimed at comparing SLUG with other state-of-the-art learners, like decision trees, random forests and extreme gradient boosting. Despite the fact that SLUG is not the most efficient method in terms of training time, it is confirmed as the most effective method in terms of accuracy."}}
{"id": "6LqiHang-C", "cdate": 1640995200000, "mdate": 1681917579350, "content": {"title": "Proportional Multicalibration", "abstract": "Multicalibration is a desirable fairness criteria that constrains calibration error among flexibly-defined groups in the data while maintaining overall calibration. However, when outcome probabilities are correlated with group membership, multicalibrated models can exhibit a higher percent calibration error among groups with lower base rates than groups with higher base rates. As a result, it remains possible for a decision-maker to learn to trust or distrust model predictions for specific groups. To alleviate this, we propose \\emph{proportional multicalibration}, a criteria that constrains the percent calibration error among groups and within prediction bins. We prove that satisfying proportional multicalibration bounds a model's multicalibration as well its \\emph{differential calibration}, a stronger fairness criteria inspired by the fairness notion of sufficiency. We provide an efficient algorithm for post-processing risk prediction models for proportional multicalibration and evaluate it empirically. We conduct simulation studies and investigate a real-world application of PMC-postprocessing to prediction of emergency department patient admissions. We observe that proportional multicalibration is a promising criteria for controlling simultaneous measures of calibration fairness of a model over intersectional groups with virtually no cost in terms of classification performance."}}
