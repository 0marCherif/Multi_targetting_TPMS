{"id": "IsCg7qoy8i9", "cdate": 1663850462896, "mdate": null, "content": {"title": "Benchmarking Algorithms for Domain Generalization in Federated Learning", "abstract": "In this paper, we present a unified platform to study domain generalization in the federated learning (FL) context and conduct extensive empirical evaluations of the current state-of-the-art domain generalization algorithms adapted to FL. In particular, we perform a fair comparison of nine existing algorithms in solving domain generalization {either centralized domain generalization algorithms adapted to the FL context or existing FL domain generalization algorithms } to comprehensively explore the challenges introduced by FL. These challenges include statistical heterogeneity among clients, the number of clients, the number of communication rounds, etc. The evaluations are conducted on three diverse datasets including PACS (image dataset covering photo, sketch, cartoon, and painting domains), iWildCam (image dataset with 323 domains), and Py150 (natural language processing dataset with 8421 domains). The experiments show that the challenges brought by federated learning stay unsolved in the realistic experiment setting. Furthermore, the code base supports fair and reproducible new algorithm evaluation with easy implementation."}}
{"id": "ppxKnb1SIB", "cdate": 1663850148428, "mdate": null, "content": {"title": "Towards Explaining Distribution Shifts", "abstract": "Distribution shift can have fundamental consequences such as signaling a change in the operating environment or significantly reducing the accuracy of downstream models. Thus, understanding such distribution shifts is critical for examining and hopefully mitigating the effect of such a shift. Most prior work has focused on merely detecting if a shift has occurred and assumes any detected shift can be understood and handled appropriately by a human operator. We hope to aid in these manual mitigation tasks by explaining the distribution shift using interpretable transportation maps from the original distribution to the shifted one. We derive our interpretable mappings from a relaxation of the optimal transport problem, where the candidate mappings are restricted to belong to a set of interpretable mappings. We then use quintessential examples of distribution shift in simulated and real-world cases to showcase how our explanatory mappings provide a better balance between detail and interpretability than the de facto standard mean shift explanation by both visual inspection and our PercentExplained metric."}}
{"id": "uhLAcrAZ9cJ", "cdate": 1663850009651, "mdate": null, "content": {"title": "Efficient Federated Domain Translation ", "abstract": "A central theme in federated learning (FL) is the fact that client data distributions are often not independent and identically distributed (IID), which has strong implications on the training process. While most existing FL algorithms focus on the conventional non-IID setting of class imbalance or missing classes across clients, in practice, the distribution differences could be more complex, e.g., changes in class conditional (domain) distributions. In this paper, we consider this complex case in FL wherein each client has access to only one domain distribution. For tasks such as domain generalization, most existing learning algorithms require access to data from multiple clients (i.e., from multiple domains) during training, which is prohibitive in FL. To address this challenge, we propose a federated domain translation method that generates pseudodata for each client which could be useful for multiple downstream learning tasks. We empirically demonstrate that our translation model is more resource-efficient (in terms of both communication and computation) and easier to train in an FL setting than standard domain translation methods. Furthermore, we demonstrate that the learned translation model enables use of state-of-the-art domain generalization methods in a federated setting, which enhances accuracy and robustness to increases in the synchronization period compared to existing methodology."}}
{"id": "k1YfNKf0FZn", "cdate": 1654920315539, "mdate": 1654920315539, "content": {"title": "Feature Shift Detection: Localizing Which Features Have Shifted via Conditional Distribution Tests", "abstract": "While previous distribution shift detection approaches can identify if a shift has occurred, these approaches cannot localize which specific features have caused a distribution shift\u2014a critical step in diagnosing or fixing any underlying issue. For example, in military sensor networks, users will want to detect when one or more of the sensors has been compromised, and critically, they will want to know which specific sensors might be compromised. Thus, we first define a formalization of this problem as multiple conditional distribution hypothesis tests and propose both non-parametric and parametric statistical tests. For both efficiency and flexibility, we then propose to use a test statistic based on the density model score function (i.e., gradient with respect to the input)\u2014which can easily compute test statistics for all dimensions in a single forward and backward pass. Any density model could be used for computing the necessary statistics including deep density models such as normalizing flows or autoregressive models. We additionally develop methods for identifying when and where a shift occurs in multivariate time-series data and show results for multiple scenarios using realistic attack models on both simulated and real world data."}}
{"id": "MpDDnE6_lGC", "cdate": 1654920078502, "mdate": 1654920078502, "content": {"title": "Towards Explaining Image-Based Distribution Shift", "abstract": "Distribution shift can have fundamental consequences such as signaling a change in the operating environment or significantly reducing the accuracy of downstream models. Thus, understanding such distribution shifts is critical for examining and hopefully mitigating the effect of such a shift. Most prior work has focused on either natively handling distribution shift (e.g., Domain Generalization) or merely detecting a shift while assuming any detected shift can be understood and handled appropriately by a human operator. For the latter, we hope to aid in these manual mitigation tasks by explaining the distribution shift to an operator. To this end, we suggest two methods: providing a set of interpretable mappings from the original distribution to the shifted one or providing a set of distributional counterfactual examples. We provide preliminary experiments on these two methods, and discuss important concepts and challenges for moving towards a better understanding of image-based distribution shifts."}}
{"id": "X82LFUs6g5Z", "cdate": 1652737641151, "mdate": null, "content": {"title": "Cooperative Distribution Alignment via JSD Upper Bound", "abstract": "Unsupervised distribution alignment estimates a transformation that maps two or more source distributions to a shared aligned distribution given only samples from each distribution. This task has many applications including generative modeling, unsupervised domain adaptation, and socially aware learning. Most prior works use adversarial learning (i.e., min-max optimization), which can be challenging to optimize and evaluate. A few recent works explore non-adversarial flow-based (i.e., invertible) approaches, but they lack a unified perspective and are limited in efficiently aligning multiple distributions. Therefore, we propose to unify and generalize previous flow-based approaches under a single non-adversarial framework, which we prove is equivalent to minimizing an upper bound on the Jensen-Shannon Divergence (JSD). Importantly, our problem reduces to a min-min, i.e., cooperative, problem and can provide a natural evaluation metric for unsupervised distribution alignment. We show empirical results on both simulated and real-world datasets to demonstrate the benefits of our approach. Code is available at https://github.com/inouye-lab/alignment-upper-bound."}}
{"id": "ArY-zkyHI_l", "cdate": 1632875688599, "mdate": null, "content": {"title": "Resilience to Multiple Attacks via Adversarially Trained MIMO Ensembles", "abstract": "While ensemble methods have been widely used for robustness against random perturbations (\\ie the average case), ensemble approaches for robustness against adversarial perturbations (\\ie the worst case) have remained elusive despite multiple prior attempts. We show that ensemble methods can improve adversarial robustness to multiple attacks if the ensemble is \\emph{adversarially diverse}, which is defined by two properties: 1) the sub-models are adversarially robust themselves and yet 2) adversarial attacks do not transfer easily between sub-models. While at first glance, creating such an ensemble would seem computationally expensive, we demonstrate that an adversarially diverse ensemble can be trained with minimal computational overhead via a Multiple-Input Multiple-Output (MIMO) model. Specifically, we propose to train a MIMO model with adversarial training ({\\emph{MAT}}), where each sub-model can be trained on a different attack type. When computing gradients for generating adversarial examples during training, we use the gradient with respect to the ensemble objective. This has a two-fold benefit: 1) it only requires 1 backward pass and 2) the cross-gradient information between the models promotes robustness against transferable attacks. We empirically demonstrate that {\\emph{MAT}} produces an ensemble of models that is adversarially diverse and significantly improves performance over single models or vanilla ensembles while being comparable to previous state-of-the-art methods. On MNIST, we obtain $99.5\\%$ clean accuracy and ($88.6\\%, 57.1\\%,71.6\\%$) against $(\\ell_\\infty, \\ell_2, \\ell_1)$ attacks, and on CIFAR10, we achieve $79.7\\%$ clean accuracy and ($47.9\\%, 61.8\\%,47.6\\%$) against $(\\ell_\\infty, \\ell_2, \\ell_1)$ attacks, which are comparable to previous state-of-the-art methods."}}
{"id": "kcadk-DShNO", "cdate": 1632875553803, "mdate": null, "content": {"title": "Why be adversarial? Let's cooperate!: Cooperative Dataset Alignment via JSD Upper Bound", "abstract": "Unsupervised dataset alignment estimates a transformation that maps two or more source domains to a shared aligned domain given only the domain datasets.\nThis task has many applications including generative modeling, unsupervised domain adaptation, and socially aware learning.\nMost prior works use adversarial learning (i.e., min-max optimization), which can be challenging to optimize and evaluate.\nA few recent works explore non-adversarial flow-based (i.e., invertible) approaches, but they lack a unified perspective.\nTherefore, we propose to unify and generalize previous flow-based approaches under a single non-adversarial framework, which we prove is equivalent to minimizing an upper bound on the Jensen-Shannon Divergence (JSD).\nImportantly, our problem reduces to a min-min, i.e., cooperative, problem and can provide a natural evaluation metric for unsupervised dataset alignment.\nWe present empirical results of our framework on both simulated and real-world datasets to demonstrate the benefits of our approach."}}
{"id": "_l8XYZe88K4", "cdate": 1622637628871, "mdate": null, "content": {"title": "Why be adversarial? Let's cooperate!: Cooperative Dataset Alignment via JSD Upper Bound", "abstract": "Unsupervised dataset alignment estimates a transformation that maps two or more source domains to a shared aligned domain given only the domain datasets. This task has many applications including generative modeling, unsupervised domain adaptation, and socially aware learning. Most prior works use adversarial learning (i.e., min-max optimization), which can be challenging to optimize and evaluate. A few recent works explore non-adversarial flow-based (i.e., invertible) approaches, but they lack a unified perspective. Therefore, we propose to unify and generalize previous flow-based approaches under a single non-adversarial framework, which we prove is equivalent to minimizing an upper bound on the Jensen-Shannon Divergence (JSD). Importantly, our problem reduces to a min-min, i.e., cooperative, problem and can provide a natural evaluation metric for unsupervised dataset alignment."}}
{"id": "0PyBAWIJh8P", "cdate": 1622637628541, "mdate": null, "content": {"title": "Discrete Tree Flows via Tree-Structured Permutations", "abstract": "While normalizing flows for continuous data have been extensively researched, flows for discrete data have only recently been explored. These prior models, however, suffer from limitations that are distinct from those of continuous flows. Most notably, discrete flow-based models cannot be straightforwardly optimized with conventional deep learning methods because gradients of discrete functions are undefined or zero, and backpropagation can be computationally burdensome compared to alternative discrete algorithms such as decision tree algorithms.  Previous works approximate pseudo-gradients of the discrete functions but do not solve the problem on a fundamental level. Our approach seeks to reduce computational burden and remove the need for pseudo-gradients by developing a discrete flow based on decision trees---building upon the success of efficient tree-based methods for classification and regression for discrete data. We first define a tree-structured permutation (TSP) that compactly encodes a permutation of discrete data where the inverse is easy to compute; thus, we can efficiently compute the density value and sample new data.  We then propose a decision tree algorithm to learn TSPs that estimates the tree structure and simple permutations at each node via a novel criteria. We empirically demonstrate the feasibility of our method on multiple datasets."}}
