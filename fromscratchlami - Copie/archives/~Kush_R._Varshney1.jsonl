{"id": "MYZJ3P1lPS", "cdate": 1675421859897, "mdate": 1675421859897, "content": {"title": "Invariant Risk Minimization Games", "abstract": "The standard risk minimization paradigm of machine learning is brittle when operating in environments whose test distributions are different from the training distribution due to spurious correlations. Training on data from many environments and finding invariant predictors reduces the effect of spurious features by concentrating models on features that have a causal relationship with the outcome. In this work, we pose such invariant risk minimization as finding the Nash equilibrium of an ensemble game among several environments. By doing so, we develop a simple training algorithm that uses best response dynamics and, in our experiments, yields similar or better empirical accuracy with much lower variance than the challenging bi-level optimization problem of Arjovsky et al. (2019). One key theoretical contribution is showing that the set of Nash equilibria for the proposed game are equivalent to the set of invariant predictors for any finite number of environments, even with nonlinear classifiers and transformations. As a result, our method also retains the generalization guarantees to a large set of environments shown in Arjovsky et al. (2019). The proposed algorithm adds to the collection of successful game-theoretic machine learning algorithms such as generative adversarial networks."}}
{"id": "MjsDeTcDEy", "cdate": 1663849954850, "mdate": null, "content": {"title": "What Is Missing in IRM Training and Evaluation? Challenges and Solutions", "abstract": "Invariant risk minimization (IRM) has received increasing attention as a way to acquire environment-agnostic data representations and predictions, and also a principled solution for preventing spurious correlations from being learned and improving models\u2019 out-of-distribution generalization. Yet, recent works have found that the optimality of the originally-proposed IRM optimization (IRMV1) may be compromised in practice or could be impossible to achieve in some scenarios. Therefore, a series of advanced IRM algorithms have been developed that show practical improvement over IRMV1. In this work, we revisit these recent IRM advancements and identify and resolve three practical limitations in IRM training and evaluation. First, we find that the effect of batch size during training has been chronically overlooked in previous studies, leaving room for further improvement. We propose small-batch training and highlight the improvements over a set of large-batch optimization techniques. Second, we find that improper selection of evaluation environments could give a false sense of invariance for IRM. To alleviate this effect, we leverage diversified test-time environments to precisely characterize the invariance of IRM when applied in practice. Third, we revisit Ahuja et al. (2020)\u2019s proposal to convert IRM into an ensemble game and identify a limitation when a single invariant predictor is desired instead of an ensemble of individual predictors. We propose a new IRM variant to address this limitation based on a novel viewpoint of ensemble IRM games as consensus-constrained bi-level optimization. Lastly, we conduct extensive experiments (covering 7 existing IRM variants and 7 datasets) to justify the practical significance of revisiting IRM training and evaluation in a principled manner."}}
{"id": "7cL46kHUu4", "cdate": 1652737604375, "mdate": null, "content": {"title": "Fair Infinitesimal Jackknife: Mitigating the Influence of Biased Training Data Points Without Refitting", "abstract": "In consequential decision-making applications, mitigating unwanted biases in machine learning models that yield systematic disadvantage to members of groups delineated by sensitive attributes such as race and gender is one key intervention to strive for equity. Focusing on demographic parity and equality of opportunity, in this paper we propose an algorithm that improves the fairness of a pre-trained classifier by simply dropping carefully selected training data points. We select instances based on their influence on the fairness metric of interest, computed using an infinitesimal jackknife-based approach. The dropping of training points is done in principle, but in practice does not require the model to be refit. Crucially, we find that such an intervention does not substantially reduce the predictive performance of the model but drastically improves the fairness metric. Through careful experiments, we evaluate the effectiveness of the proposed approach on diverse tasks and find that it consistently improves upon existing alternatives. "}}
{"id": "WPXRVQaP9Oq", "cdate": 1652737593049, "mdate": null, "content": {"title": "On the Safety of Interpretable Machine Learning: A Maximum Deviation Approach", "abstract": "Interpretable and explainable machine learning has seen a recent surge of interest. We focus on safety as a key motivation behind the surge and make the relationship between interpretability and safety more quantitative. Toward assessing safety, we introduce the concept of *maximum deviation* via an optimization problem to find the largest deviation of a supervised learning model from a reference model regarded as safe. We then show how interpretability facilitates this safety assessment. For models including decision trees, generalized linear and additive models, the maximum deviation can be computed exactly and efficiently. For tree ensembles, which are not regarded as interpretable, discrete optimization techniques can still provide informative bounds. For a broader class of piecewise Lipschitz functions, we leverage the multi-armed bandit literature to show that interpretability produces tighter (regret) bounds on the maximum deviation. We present case studies, including one on mortgage approval, to illustrate our methods and the insights about models that may be obtained from deviation maximization."}}
{"id": "SWeAMcbMXc", "cdate": 1648658966071, "mdate": 1648658966071, "content": {"title": "Causal Feature Selection for Algorithmic Fairness", "abstract": "The use of machine learning (ML) in high-stakes societal decisions\nhas encouraged the consideration of fairness throughout the ML\nlifecycle. Although data integration is one of the primary steps to\ngenerate high-quality training data, most of the fairness literature\nignores this stage. In this work, we consider fairness in the integra-\ntion component of data management, aiming to identify features\nthat improve prediction without adding any bias to the dataset.\nWe work under the causal fairness paradigm [45 ]. Without requir-\ning the underlying structural causal model a priori, we propose\nan approach to identify a sub-collection of features that ensure\nfairness of the dataset by performing conditional independence\ntests between different subsets of features. We use group testing to\nimprove the complexity of the approach. We theoretically prove the\ncorrectness of the proposed algorithm and show that sublinear con-\nditional independence tests are sufficient to identify these variables.\nA detailed empirical evaluation is performed on real-world datasets\nto demonstrate the efficacy and efficiency of our technique"}}
{"id": "rtZNQLLocx9", "cdate": 1646077515321, "mdate": null, "content": {"title": "Differentially Private SGDA for Minimax Problems", "abstract": "Stochastic gradient descent ascent (SGDA) and its variants have been the workhorse for solving minimax problems. However,  in contrast to the well-studied stochastic gradient descent (SGD) with differential privacy (DP) constraints,  there is  little work on understanding the generalization (utility)  of SGDA with DP constraints. In this paper, we use the algorithmic stability approach to establish the generalization (utility) of DP-SGDA in different settings. In particular, for the convex-concave setting, we prove that the DP-SGDA can achieve  an optimal utility rate in terms of the weak primal-dual population risk in both smooth and non-smooth cases. To our best knowledge, this is the first-ever-known result for DP-SGDA in the non-smooth case.  We further provide its  utility  analysis in   the nonconvex-strongly-concave setting which is  the  first-ever-known result in terms of the primal population risk.  The convergence and generalization results for this nonconvex setting  are new even in the non-private setting.  Finally,  numerical experiments are conducted to  demonstrate the effectiveness of DP-SGDA  for both convex and nonconvex cases."}}
{"id": "Jt8FYFnyTLR", "cdate": 1632875610788, "mdate": null, "content": {"title": "On the Safety of Interpretable Machine Learning: A Maximum Deviation Approach", "abstract": "Interpretable and explainable machine learning has seen a recent surge of interest. We posit that safety is a key reason behind the demand for explainability. To explore this relationship, we propose a mathematical formulation for assessing the safety of supervised learning models based on their maximum deviation over a certification set. We then show that for interpretable models including decision trees, rule lists, generalized linear and additive models, the maximum deviation can be computed exactly and efficiently. For tree ensembles, which are not regarded as interpretable, discrete optimization techniques can still provide informative bounds. For a broader class of piecewise Lipschitz functions, we repurpose results from the multi-armed bandit literature to show that interpretability produces tighter (regret) bounds on the maximum deviation compared with black box functions. We perform experiments that quantify the dependence of the maximum deviation on model smoothness and certification set size. The experiments also illustrate how the solutions that maximize deviation can suggest safety risks."}}
{"id": "kGXlIEQgvC", "cdate": 1621630229479, "mdate": null, "content": {"title": "CoFrNets: Interpretable Neural Architecture Inspired by Continued Fractions", "abstract": "In recent years there has been a considerable amount of research on local post hoc explanations for neural networks. However, work on building interpretable neural architectures has been relatively sparse. In this paper, we present a novel neural architecture, CoFrNet, inspired by the form of continued fractions which are known to have many attractive properties in number theory, such as fast convergence of approximations to real numbers. We show that CoFrNets can be efficiently trained as well as interpreted leveraging their particular functional form. Moreover, we prove that such architectures are universal approximators based on a proof strategy that is different than the typical strategy used to prove universal approximation results for neural networks based on infinite width (or depth), which is likely to be of independent interest. We experiment on nonlinear synthetic functions and are able to accurately model as well as estimate feature attributions and even higher order terms in some cases, which is a testament to the representational power as well as interpretability of such architectures. To further showcase the power of CoFrNets, we experiment on seven real datasets spanning tabular, text and image modalities, and show that they are either comparable or significantly better than other interpretable models and multilayer perceptrons, sometimes approaching the accuracies of state-of-the-art models."}}
{"id": "DARt2DMRZRn", "cdate": 1621519231935, "mdate": null, "content": {"title": "Biomedical Interpretable Entity Represenations", "abstract": "Pre-trained language models induce dense entity representations that offer strong performance on entity-centric NLP tasks, but such representations are not immediately interpretable. This can be a barrier to model uptake in important domains such as biomedicine. There has been recent work on general interpretable representation learning (Onoe and Durrett, 2020), but these domain-agnostic representations do not readily transfer to the important domain of biomedicine. In this paper, we create a new entity type system and training set from a large corpus of biomedical texts by mapping entities to concepts in a medical ontology, and from these to Wikipedia pages whose categories are our types. From this map- ping we derive Biomedical Interpretable Entity Representations (BIERs), in which dimensions correspond to fine-grained entity types, and values are predicted probabilities that a given entity is of the corresponding type. We propose a novel method that exploits BIER\u2019s final sparse and intermediate dense representations to facilitate model and entity type debugging. We show that BIERs achieve strong performance in biomedical tasks including named entity disambiguation and entity linking, and we provide error analysis to highlight the utility of their interpretability, particularly in low- supervision settings. Finally, we provide our induced 68K biomedical type system, the corresponding 37 million triples of derived data used to train BIER models and our best per- forming model."}}
{"id": "jrA5GAccy_", "cdate": 1601308164861, "mdate": null, "content": {"title": "Empirical or Invariant Risk Minimization? A Sample Complexity Perspective", "abstract": "Recently, invariant risk minimization (IRM) was proposed as a promising solution to address out-of-distribution (OOD) generalization. However, it is unclear when IRM should be preferred over the widely-employed empirical risk minimization (ERM) framework. In this work, we analyze both these frameworks from the perspective of sample complexity, thus taking a firm step towards answering this important question. We find that depending on the type of data generation mechanism, the two approaches might have very different finite sample and asymptotic behavior. For example, in the covariate shift setting we see that the two approaches not only arrive at the same asymptotic solution, but also have similar finite sample behavior with no clear winner. For other distribution shifts such as those involving confounders or anti-causal variables, however, the two approaches arrive at different asymptotic solutions where IRM is guaranteed to be close to the desired OOD solutions in the finite sample regime, while ERM is biased even asymptotically.  We further investigate how different factors --- the number of environments, complexity of the model, and IRM penalty weight ---  impact the sample complexity of IRM in relation to its distance from the OOD solutions. "}}
