{"id": "qzOYx1NeJ5", "cdate": 1679910612380, "mdate": 1679910612380, "content": {"title": "Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science", "abstract": "Through the success of deep learning in various domains, artificial neural networks are currently among the most used artificial intelligence methods. Taking inspiration from the network properties of biological neural networks (e.g. sparsity, scale-freeness), we argue that (contrary to general practice) artificial neural networks, too, should not have fully-connected layers. Here we propose sparse evolutionary training of artificial neural networks, an algorithm which evolves an initial sparse topology (Erd\u0151s\u2013R\u00e9nyi random graph) of two consecutive layers of neurons into a scale-free topology, during learning. Our method replaces artificial neural networks fully-connected layers with sparse ones before training, reducing quadratically the number of parameters, with no decrease in accuracy. We demonstrate our claims on restricted Boltzmann machines, multi-layer perceptrons, and convolutional neural networks for unsupervised and supervised learning on 15 datasets. Our approach has the potential to enable artificial neural networks to scale up beyond what is currently possible."}}
{"id": "ZxOO5jfqSYw", "cdate": 1652737461239, "mdate": null, "content": {"title": "Dynamic Sparse Network for Time Series Classification: Learning What to \u201cSee\u201d", "abstract": "The receptive field (RF), which determines the region of time series to be \u201cseen\u201d and used, is critical to improve the performance for time series classification (TSC). However, the variation of signal scales across and within time series data, makes it challenging to decide on proper RF sizes for TSC. In this paper, we propose a dynamic sparse network (DSN) with sparse connections for TSC, which can learn to cover various RF without cumbersome hyper-parameters tuning. The kernels in each sparse layer are sparse and can be explored under the constraint regions by dynamic sparse training, which makes it possible to reduce the resource cost. The experimental results show that the proposed DSN model can achieve state-of-art performance on both univariate and multivariate TSC datasets with less than 50% computational cost compared with recent baseline methods, opening the path towards more accurate resource-aware methods for time series analyses. Our code is publicly available at: https://github.com/QiaoXiao7282/DSN."}}
{"id": "RLtqs6pzj1-", "cdate": 1632875491978, "mdate": null, "content": {"title": "Deep Ensembling with No Overhead for either Training or Testing: The All-Round Blessings of Dynamic Sparsity", "abstract": "The success of deep ensembles on improving predictive performance, uncertainty estimation, and out-of-distribution robustness has been extensively studied in the machine learning literature. Albeit the promising results, naively training multiple deep neural networks and combining their predictions at inference leads to prohibitive computational costs and memory requirements. Recently proposed efficient ensemble approaches reach the performance of the traditional deep ensembles with significantly lower costs. However, the training resources required by these approaches are still at least the same as training a single dense model. In this work, we draw a unique connection between sparse neural network training and deep ensembles, yielding a novel efficient ensemble learning framework called $FreeTickets$. Instead of training multiple dense networks and averaging them, we directly train sparse subnetworks from scratch and extract diverse yet accurate subnetworks during this efficient, sparse-to-sparse training. Our framework, $FreeTickets$, is defined as the ensemble of these relatively cheap sparse subnetworks. Despite being an ensemble method, $FreeTickets$ has even fewer parameters and training FLOPs than a single dense model. This seemingly counter-intuitive outcome is due to the ultra training/inference efficiency of dynamic sparse training. $FreeTickets$ surpasses the dense baseline in all the following criteria: prediction accuracy, uncertainty estimation, out-of-distribution (OoD) robustness, as well as efficiency for both training and inference. Impressively, $FreeTickets$ outperforms the naive deep ensemble with ResNet50 on ImageNet using around only $1/5$ of the training FLOPs required by the latter. We have released our source code at https://github.com/VITA-Group/FreeTickets."}}
{"id": "I9UvKcd2ko", "cdate": 1609459200000, "mdate": null, "content": {"title": "Sparse Training Theory for Scalable and Efficient Agents", "abstract": "A fundamental task for artificial intelligence is learning. Deep Neural Networks have proven to cope perfectly with all learning paradigms, i.e. supervised, unsupervised, and reinforcement learning. Nevertheless, traditional deep learning approaches make use of cloud computing facilities and do not scale well to autonomous agents with low computational resources. Even in the cloud, they suffer from computational and memory limitations, and they cannot be used to model adequately large physical worlds for agents which assume networks with billions of neurons. These issues are addressed in the last few years by the emerging topic of sparse training, which trains sparse networks from scratch. This paper discusses sparse training state-of-the-art, its challenges and limitations while introducing a couple of new theoretical research directions which has the potential of alleviating sparse training limitations to push deep learning scalability well beyond its current boundaries. Nevertheless, the theoretical advancements impact in complex multi-agents settings is discussed from a real-world perspective, using the smart grid case study."}}
{"id": "IykozwpPXk", "cdate": 1577836800000, "mdate": null, "content": {"title": "Quick and Robust Feature Selection: the Strength of Energy-efficient Sparse Training for Autoencoders", "abstract": "Major complications arise from the recent increase in the amount of high-dimensional data, including high computational costs and memory requirements. Feature selection, which identifies the most relevant and informative attributes of a dataset, has been introduced as a solution to this problem. Most of the existing feature selection methods are computationally inefficient; inefficient algorithms lead to high energy consumption, which is not desirable for devices with limited computational and energy resources. In this paper, a novel and flexible method for unsupervised feature selection is proposed. This method, named QuickSelection, introduces the strength of the neuron in sparse neural networks as a criterion to measure the feature importance. This criterion, blended with sparsely connected denoising autoencoders trained with the sparse evolutionary training procedure, derives the importance of all input features simultaneously. We implement QuickSelection in a purely sparse manner as opposed to the typical approach of using a binary mask over connections to simulate sparsity. It results in a considerable speed increase and memory reduction. When tested on several benchmark datasets, including five low-dimensional and three high-dimensional datasets, the proposed method is able to achieve the best trade-off of classification and clustering accuracy, running time, and maximum memory usage, among widely used approaches for feature selection. Besides, our proposed method requires the least amount of energy among the state-of-the-art autoencoder-based feature selection methods."}}
{"id": "v98MDyLHVEJ", "cdate": 1546300800000, "mdate": null, "content": {"title": "Airport Restroom Cleanliness Prediction Using Real Time User Feedback Data", "abstract": "Large airports aim to offer a maximized experience to its passengers. A main contributor to customer experience is the cleanliness of restrooms, which is measured by feedback devices installed in restrooms at airports. This paper reviews to what extent real-time feedback data and classification techniques can be useful in practice to predict the cleanliness of restrooms. Within this topic, different class definitions of clean and unclean are introduced and a distinction is made between a combined prediction model that includes the entire environment and restroom specific prediction models that focus only on a single restroom. The dataset is imbalanced and visualizations show that there is class overlap. To overcame these limitations various sampling methods with two different encoding mechanisms are investigated. Sampling methods do not improve the performance of the combined prediction model but do improve the performance of some of the restroom-specific prediction models, especially those with a high class imbalance. The major cause of the unsatisfying performance is not class imbalance, but the data ambiguity that leads to class overlap. To obtain prediction models that are useful in practice, we provide recommendations regarding the dataset and how this should be enriched with features that are capable of distinguishing the two classes more clearly."}}
{"id": "phtLWPvTnBd", "cdate": 1546300800000, "mdate": null, "content": {"title": "Collaborative learning for classification and prediction of building energy flexibility", "abstract": "In this paper we propose an simple digital learning platform for flexible energy detection using data with fine granularity. The platform is empowered with artificially intelligent methods aiming to quantify the uncertainty of building energy consumption at building level, as well as at the aggregated level. Two major learning tasks are perform in this context: prediction and classification. Firstly, the building energy prediction with various time steps resolution are perform using methods such as Fully Connected Neural Networks (FCNN), Long short-term memory (LSTM), and Decision Trees (DT). Secondly, a Support Vector Machine (SVM) method is used to unlock the building energy flexibility by performing classification assuming three different levels of flexibility. Further on, a collaborative task is integrate within the platform to improve the multi-class classification accuracy. Through the end, we argue that this approach can be considered a solid integrated and automated basic block able to incorporate future AI models in (near) real-time to explore the benefits at the synergy between built environment and emerging smart grid technologies and applications."}}
{"id": "OHQ8JjXeWfE", "cdate": 1546300800000, "mdate": null, "content": {"title": "On-Line Building Energy Optimization Using Deep Reinforcement Learning", "abstract": "Unprecedented high volumes of data are becoming available with the growth of the advanced metering infrastructure. These are expected to benefit planning and operation of the future power systems and to help customers transition from a passive to an active role. In this paper, we explore for the first time in the smart grid context the benefits of using deep reinforcement learning, a hybrid type of methods that combines reinforcement learning with deep learning, to perform on-line optimization of schedules for building energy management systems. The learning procedure was explored using two methods, Deep Q-learning and deep policy gradient, both of which have been extended to perform multiple actions simultaneously. The proposed approach was validated on the large-scale Pecan Street Inc. database. This highly dimensional database includes information about photovoltaic power generation, electric vehicles and buildings appliances. Moreover, these on-line energy scheduling strategies could be used to provide realtime feedback to consumers to encourage more efficient use of electricity."}}
{"id": "s7-3RPO789b", "cdate": 1514764800000, "mdate": null, "content": {"title": "Enabling Cooperative Behavior for Building Demand Response Based on Extended Joint Action Learning", "abstract": "This paper explores the use of distributed intelligence to assist the integration of the demand as a flexible resource, to mitigate the emerging uncertainty in the power system, while fulfilling the customer's local needs, i.e., comfort management. More exactly, our contribution is twofold. First, we propose a novel cooperative and decentralized reinforcement learning method, dubbed extended joint action learning (eJAL). Second, we perform a comparison between eJAL to noncooperative decentralized decision making strategies, i.e., Q-learning, and a centralized game theoretic approach, i.e., Nash n-player game. This comparison has been conducted on the basis of grid support effectiveness and the loss of comfort for each customer. Various metrics were used to analyze the advantages and disadvantages of each method. We demonstrated that a range of flexibility requests can be met by providing an optimal energy portfolio of buildings without substantially violating comfort constraints. Moreover, we showed that the proposed eJAL method achieves the highest fairness index."}}
{"id": "gVso1VqDHcp", "cdate": 1514764800000, "mdate": null, "content": {"title": "One-Shot Learning using Mixture of Variational Autoencoders: a Generalization Learning approach", "abstract": "Deep learning, even if it is very successful nowadays, traditionally needs very large amounts of labeled data to perform excellent on the classification task. In an attempt to solve this problem, the one-shot learning paradigm, which makes use of just one labeled sample per class and prior knowledge, becomes increasingly important. In this paper, we propose a new one-shot learning method, dubbed MoVAE (Mixture of Variational AutoEncoders), to perform classification. Complementary to prior studies, MoVAE represents a shift of paradigm in comparison with the usual one-shot learning methods, as it does not use any prior knowledge. Instead, it starts from zero knowledge and one labeled sample per class. Afterward, by using unlabeled data and the generalization learning concept (in a way, more as humans do), it is capable to gradually improve by itself its performance. Even more, if there are no unlabeled data available MoVAE can still perform well in one-shot learning classification. We demonstrate empirically the efficiency of our proposed approach on three datasets, i.e. the handwritten digits (MNIST), fashion products (Fashion-MNIST), and handwritten characters (Omniglot), showing that MoVAE outperforms state-of-the-art one-shot learning algorithms."}}
