{"id": "lCK1_0c15jk", "cdate": 1675183479690, "mdate": 1675183479690, "content": {"title": "Beyond calibration: estimating the grouping loss of modern neural networks", "abstract": "The ability to ensure that a classifier gives reliable confidence scores is essential to ensure informed decision-making. To this end, recent work has focused on miscalibration, i.e., the over or under confidence of model scores. Yet calibration is not enough: even a perfectly calibrated classifier with the best possible accuracy can have confidence scores that are far from the true posterior probabilities. This is due to the grouping loss, created by samples with the same confidence scores but different true posterior probabilities. Proper scoring rule theory shows that given the calibration loss, the missing piece to characterize individual errors is the grouping loss. While there are many estimators of the calibration loss, none exists for the grouping loss in standard settings. Here, we propose an estimator to approximate the grouping loss. We show that modern neural network architectures in vision and NLP exhibit grouping loss, notably in distribution shifts settings, which highlights the importance of pre-production validation."}}
{"id": "6w1k-IixnL8", "cdate": 1663850020141, "mdate": null, "content": {"title": "Beyond calibration: estimating the grouping loss of modern neural networks", "abstract": "The ability to ensure that a classifier gives reliable confidence scores is essential to ensure informed decision-making. To this end, recent work has focused on miscalibration, i.e., the over or under confidence of model scores. Yet calibration is not enough: even a perfectly calibrated classifier with the best possible accuracy can have confidence scores that are far from the true posterior probabilities. This is due to the grouping loss, created by samples with the same confidence scores but different true posterior probabilities. Proper scoring rule theory shows that given the calibration loss, the missing piece to characterize individual errors is the grouping loss. While there are many estimators of the calibration loss, none exists for the grouping loss in standard settings. Here, we propose an estimator to approximate the grouping loss. We show that modern neural network architectures in vision and NLP exhibit grouping loss, notably in distribution shifts settings, which highlights the importance of pre-production validation."}}
{"id": "usxt30HpW66", "cdate": 1621630271522, "mdate": null, "content": {"title": "What\u2019s a good imputation to predict with missing values?", "abstract": "How to learn a good predictor on data with missing values? Most efforts focus on first imputing as well as possible and second learning on the completed data to predict the outcome. Yet, this widespread practice has no theoretical grounding. Here we show that for almost all imputation functions, an impute-then-regress procedure with a powerful learner is Bayes optimal. This result holds for all missing-values mechanisms, in contrast with the classic statistical results that require missing-at-random settings to use imputation in probabilistic modeling. Moreover, it implies that perfect conditional imputation is not needed for good prediction asymptotically. In fact, we show that on perfectly imputed data the best regression function will generally be discontinuous, which makes it hard to learn. Crafting instead the imputation so as to leave the regression function unchanged simply shifts the problem to learning discontinuous imputations. Rather, we suggest that it is easier to learn imputation and regression jointly. We propose such a procedure, adapting NeuMiss, a neural network capturing the conditional links across observed and unobserved variables whatever the missing-value pattern. Our experiments confirm that joint imputation and regression through NeuMiss is better than various two step procedures in a finite-sample regime.  "}}
{"id": "wZmVPAClGYH", "cdate": 1577836800000, "mdate": null, "content": {"title": "NeuMiss networks: differentiable programming for supervised learning with missing values", "abstract": "The presence of missing values makes supervised learning much more challenging. Indeed, previous work has shown that even when the response is a linear function of the complete data, the optimal predictor is a complex function of the observed entries and the missingness indicator. As a result, the computational or sample complexities of consistent approaches depend on the number of missing patterns, which can be exponential in the number of dimensions. In this work, we derive the analytical form of the optimal predictor under a linearity assumption and various missing data mechanisms including Missing at Random (MAR) and self-masking (Missing Not At Random). Based on a Neumann-series approximation of the optimal predictor, we propose a new principled architecture, named NeuMiss networks. Their originality and strength come from the use of a new type of non-linearity: the multiplication by the missingness indicator. We provide an upper bound on the Bayes risk of NeuMiss networks, and show that they have good predictive accuracy with both a number of parameters and a computational complexity independent of the number of missing data patterns. As a result they scale well to problems with many features, and remain statistically efficient for medium-sized samples. Moreover, we show that, contrary to procedures using EM or imputation, they are robust to the missing data mechanism, including difficult MNAR settings such as self-masking."}}
{"id": "fgDs5xUGO2h", "cdate": 1577836800000, "mdate": null, "content": {"title": "Linear predictor on linearly-generated data with missing values: non consistency and solutions", "abstract": "We consider building predictors when the data have missing values. We study the seemingly-simple case where the target to predict is a linear function of the fully-observed data and we show that, in the presence of missing values, the optimal predictor may not be linear. In the particular Gaussian case, it can be written as a linear function of multiway interactions between the observed data and the various missing-value indicators. Due to its intrinsic complexity, we study a simple approximation and prove generalization bounds with finite samples, highlighting regimes for which each method performs best. We then show that multilayer perceptrons with ReLU activation functions can be consistent, and can explore good trade-offs between the true model and approximations. Our study highlights the interesting family of models that are beneficial to fit with missing values depending on the amount of data available."}}
{"id": "LUsbvxeP7Ox", "cdate": 1514764800000, "mdate": null, "content": {"title": "Learning from genomic data: efficient representations and algorithms. (D\u00e9veloppement de repr\u00e9sentations et d'algorithmes efficaces pour l'apprentissage statistique sur des donn\u00e9es g\u00e9nomiques)", "abstract": "Since the first sequencing of the human genome in the early 2000s, large endeavours have set out to map the genetic variability among individuals, or DNA alterations in cancer cells. They have laid foundations for the emergence of precision medicine, which aims at integrating the genetic specificities of an individual with its conventional medical record to adapt treatment, or prevention strategies.Translating DNA variations and alterations into phenotypic predictions is however a difficult problem. DNA sequencers and microarrays measure more variables than there are samples, which poses statistical issues. The data is also subject to technical biases and noise inherent in these technologies. Finally, the vast and intricate networks of interactions among proteins obscure the impact of DNA variations on the cell behaviour, prompting the need for predictive models that are able to capture a certain degree of complexity. This thesis presents novel methodological contributions to address these challenges. First, we define a novel representation for tumour mutation profiles that exploits prior knowledge on protein-protein interaction networks. For certain cancers, this representation allows improving survival predictions from mutation data as well as stratifying patients into meaningful subgroups. Second, we present a new learning framework to jointly handle data normalisation with the estimation of a linear model. Our experiments show that it improves prediction performances compared to handling these tasks sequentially. Finally, we propose a new algorithm to scale up sparse linear models estimation with two-way interactions. The obtained speed-up makes this estimation possible and efficient for datasets with hundreds of thousands of main effects, thereby extending the scope of such models to the data from genome-wide association studies."}}
{"id": "ByWyjjb_br", "cdate": 1514764800000, "mdate": null, "content": {"title": "WHInter: A Working set algorithm for High-dimensional sparse second order Interaction models", "abstract": "Learning sparse linear models with two-way interactions is desirable in many application domains such as genomics. $\\ell_1$-regularised linear models are popular to estimate sparse models, yet stan..."}}
{"id": "ylEvG3DGCdN", "cdate": 1483228800000, "mdate": null, "content": {"title": "Supervised Quantile Normalisation", "abstract": "Quantile normalisation is a popular normalisation method for data subject to unwanted variations such as images, speech, or genomic data. It applies a monotonic transformation to the feature values of each sample to ensure that after normalisation, they follow the same target distribution for each sample. Choosing a \"good\" target distribution remains however largely empirical and heuristic, and is usually done independently of the subsequent analysis of normalised data. We propose instead to couple the quantile normalisation step with the subsequent analysis, and to optimise the target distribution jointly with the other parameters in the analysis. We illustrate this principle on the problem of estimating a linear model over normalised data, and show that it leads to a particular low-rank matrix regression problem that can be solved efficiently. We illustrate the potential of our method, which we term SUQUAN, on simulated data, images and genomic data, where it outperforms standard quantile normalisation."}}
{"id": "fyT8vlRdqXU", "cdate": 1483228800000, "mdate": null, "content": {"title": "NetNorM: Capturing cancer-relevant information in somatic exome mutation data with gene networks for cancer stratification and prognosis", "abstract": "Author summary The transition from a normal cell to a cancer cell is driven by genetic alterations, such as mutations, that induce uncontrolled cell proliferation. With the advent of next-generation sequencing technologies (NGS) in the last decade, thousands of tumours have been sequenced and their mutation profiles determined. However, the statistical analysis of these mutation profiles remains challenging. Indeed, two patients usually do not share the same set of mutations and can even have none in common. Moreover, it is difficult to distinguish the few disease-causing mutations from the dozens, often hundreds of mutations observed in a tumour. To alleviate these challenges, it has been proposed to use gene-gene interaction networks as prior knowledge, with the idea that if a gene is mutated and non-functional, then its interacting neighbours might not be able to fulfil their function as well. Here we propose NetNorM, a method that transforms mutation data using gene networks so as to make mutation profiles more amenable to statistical learning. We show that NetNorM significantly improves the prognostic power of mutation data compared to previous approaches, and allows defining meaningful groups of patients based on their mutation profiles."}}
