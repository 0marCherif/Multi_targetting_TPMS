{"id": "vShR0yP_jwV", "cdate": 1706322462921, "mdate": 1706322462921, "content": {"title": "PLAtE: A Large-scale Dataset for List Page Web Extraction", "abstract": "Recently, neural models have been leveraged to significantly improve the performance of information extraction from semi-structured websites. However, a barrier for continued progress is the small number of datasets large enough to train these models. In this work, we introduce the PLAtE (Pages of Lists Attribute Extraction) benchmark dataset as a challenging new web extraction task. PLAtE focuses on shopping data, specifically extractions from product review pages with multiple items encompassing the tasks of: (1) finding product list segmentation boundaries and (2) extracting attributes for each product. PLAtE is composed of 52,898 items collected from 6,694 pages and 156,014 attributes, making it the first large-scale list page web extraction dataset. We use a multi-stage approach to collect and annotate the dataset and adapt three state-of-the-art web extraction models to the two tasks comparing their strengths and weaknesses both quantitatively and qualitatively."}}
{"id": "ypYaQ44ms4", "cdate": 1672531200000, "mdate": 1684444545387, "content": {"title": "Enhancing Multi-Document Summarization with Cross-Document Graph-based Information Extraction", "abstract": ""}}
{"id": "ssUjnSNPRKq", "cdate": 1640995200000, "mdate": 1684444545488, "content": {"title": "Answer Consolidation: Formulation and Benchmarking", "abstract": "Wenxuan Zhou, Qiang Ning, Heba Elfardy, Kevin Small, Muhao Chen. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022."}}
{"id": "d9JnO85YGE", "cdate": 1640995200000, "mdate": 1684444545281, "content": {"title": "PLAtE: A Large-scale Dataset for List Page Web Extraction", "abstract": "Recently, neural models have been leveraged to significantly improve the performance of information extraction from semi-structured websites. However, a barrier for continued progress is the small number of datasets large enough to train these models. In this work, we introduce the PLAtE (Pages of Lists Attribute Extraction) benchmark dataset as a challenging new web extraction task. PLAtE focuses on shopping data, specifically extractions from product review pages with multiple items encompassing the tasks of: (1) finding product-list segmentation boundaries and (2) extracting attributes for each product. PLAtE is composed of 52, 898 items collected from 6, 694 pages and 156, 014 attributes, making it the first largescale list page web extraction dataset. We use a multi-stage approach to collect and annotate the dataset and adapt three state-of-the-art web extraction models to the two tasks comparing their strengths and weaknesses both quantitatively and qualitatively."}}
{"id": "LaiDRusIOs", "cdate": 1640995200000, "mdate": 1684444545712, "content": {"title": "SumREN: Summarizing Reported Speech about Events in News", "abstract": "A primary objective of news articles is to establish the factual record for an event, frequently achieved by conveying both the details of the specified event (i.e., the 5 Ws; Who, What, Where, When and Why regarding the event) and how people reacted to it (i.e., reported statements). However, existing work on news summarization almost exclusively focuses on the event details. In this work, we propose the novel task of summarizing the reactions of different speakers, as expressed by their reported statements, to a given event. To this end, we create a new multi-document summarization benchmark, SUMREN, comprising 745 summaries of reported statements from various public figures obtained from 633 news articles discussing 132 events. We propose an automatic silver training data generation approach for our task, which helps smaller models like BART achieve GPT-3 level performance on this task. Finally, we introduce a pipeline-based framework for summarizing reported speech, which we empirically show to generate summaries that are more abstractive and factual than baseline query-focused summarization approaches."}}
{"id": "Djdhok82SgN", "cdate": 1640995200000, "mdate": 1684444545390, "content": {"title": "A Zero-Shot Claim Detection Framework Using Question Answering", "abstract": ""}}
{"id": "3b7j8VSOYJV", "cdate": 1640995200000, "mdate": 1684444545465, "content": {"title": "Building a Dataset for Automatically Learning to Detect Questions Requiring Clarification", "abstract": ""}}
{"id": "17976QDg2i", "cdate": 1640995200000, "mdate": 1683722881790, "content": {"title": "NewsClaims: A New Benchmark for Claim Detection from News with Attribute Knowledge", "abstract": "Revanth Gangi Reddy, Sai Chetan Chinthakindi, Zhenhailong Wang, Yi Fung, Kathryn Conger, Ahmed ELsayed, Martha Palmer, Preslav Nakov, Eduard Hovy, Kevin Small, Heng Ji. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022."}}
{"id": "p3c9m5g1NRZ", "cdate": 1609459200000, "mdate": 1632326789923, "content": {"title": "Inverse Reinforcement Learning with Natural Language Goals", "abstract": "Humans generally use natural language to communicate task requirements to each other. Ideally, natural language should also be usable for communicating goals to autonomous machines (e.g., robots) to minimize friction in task specification. However, understanding and mapping natural language goals to sequences of states and actions is challenging. Specifically, existing work along these lines has encountered difficulty in generalizing learned policies to new natural language goals and environments. In this paper, we propose a novel adversarial inverse reinforcement learning algorithm to learn a language-conditioned policy and reward function. To improve generalization of the learned policy and reward function, we use a variational goal generator to relabel trajectories and sample diverse goals during training. Our algorithm outperforms multiple baselines by a large margin on a vision-based natural language instruction following dataset (Room-2-Room), demonstrating a promising advance in enabling the use of natural language instructions in specifying agent goals."}}
{"id": "bYUSRYi4Mta", "cdate": 1609459200000, "mdate": 1684444545419, "content": {"title": "Generating Self-Contained and Summary-Centric Question Answer Pairs via Differentiable Reward Imitation Learning", "abstract": ""}}
