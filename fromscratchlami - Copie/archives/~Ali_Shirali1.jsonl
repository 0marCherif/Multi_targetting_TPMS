{"id": "mGgOmvuoSq", "cdate": 1672531200000, "mdate": 1696453510871, "content": {"title": "A Theory of Dynamic Benchmarks", "abstract": ""}}
{"id": "kbAyYTqdoV", "cdate": 1672531200000, "mdate": 1696453492901, "content": {"title": "What Makes ImageNet Look Unlike LAION", "abstract": "ImageNet was famously created from Flickr image search results. What if we recreated ImageNet instead by searching the massive LAION dataset based on image captions alone? In this work, we carry out this counterfactual investigation. We find that the resulting ImageNet recreation, which we call LAIONet, looks distinctly unlike the original. Specifically, the intra-class similarity of images in the original ImageNet is dramatically higher than it is for LAIONet. Consequently, models trained on ImageNet perform significantly worse on LAIONet. We propose a rigorous explanation for the discrepancy in terms of a subtle, yet important, difference in two plausible causal data-generating processes for the respective datasets, that we support with systematic experimentation. In a nutshell, searching based on an image caption alone creates an information bottleneck that mitigates the selection bias otherwise present in image-based filtering. Our explanation formalizes a long-held intuition in the community that ImageNet images are stereotypical, unnatural, and overly simple representations of the class category. At the same time, it provides a simple and actionable takeaway for future dataset creation efforts."}}
{"id": "_2jGGrH2kb", "cdate": 1672531200000, "mdate": 1696453510871, "content": {"title": "Pruning the Way to Reliable Policies: A Multi-Objective Deep Q-Learning Approach to Critical Care", "abstract": "Most medical treatment decisions are sequential in nature. Hence, there is substantial hope that reinforcement learning may make it possible to formulate precise data-driven treatment plans. However, a key challenge for most applications in this field is the sparse nature of primarily mortality-based reward functions, leading to decreased stability of offline estimates. In this work, we introduce a deep Q-learning approach able to obtain more reliable critical care policies. This method integrates relevant but noisy intermediate biomarker signals into the reward specification, without compromising the optimization of the main outcome of interest (e.g. patient survival). We achieve this by first pruning the action set based on all available rewards, and second training a final model based on the sparse main reward but with a restricted action set. By disentangling accurate and approximated rewards through action pruning, potential distortions of the main objective are minimized, all while enabling the extraction of valuable information from intermediate signals that can guide the learning process. We evaluate our method in both off-policy and offline settings using simulated environments and real health records of patients in intensive care units. Our empirical results indicate that pruning significantly reduces the size of the action space while staying mostly consistent with the actions taken by physicians, outperforming the current state-of-the-art offline reinforcement learning method conservative Q-learning. Our work is a step towards developing reliable policies by effectively harnessing the wealth of available information in data-intensive critical care environments."}}
{"id": "i8L9qoeZOS", "cdate": 1663850325301, "mdate": null, "content": {"title": "A Theory of Dynamic Benchmarks", "abstract": "Dynamic benchmarks interweave model fitting and data collection in an attempt to mitigate the limitations of static benchmarks. In contrast to an extensive theoretical and empirical study of the static setting, the dynamic counterpart lags behind due to limited empirical studies and no apparent theoretical foundation to date. Responding to this deficit, we initiate a theoretical study of dynamic benchmarking. We examine two realizations, one capturing current practice and the other modeling more complex settings. In the first model, where data collection and model fitting alternate sequentially, we prove that model performance improves initially but can stall after only three rounds. Label noise arising from, for instance, annotator disagreement leads to even stronger negative results. Our second model generalizes the first to the case where data collection and model fitting have a hierarchical dependency structure. We show that this design guarantees strictly more progress than the first, albeit at a significant increase in complexity. We support our theoretical analysis by simulating dynamic benchmarks on two popular datasets. These results illuminate the benefits and practical limitations of dynamic benchmarking, providing both a theoretical foundation and a causal explanation for observed bottlenecks in empirical work."}}
{"id": "xXcaXUixeK", "cdate": 1640995200000, "mdate": 1682029337875, "content": {"title": "Sequential Nature of Recommender Systems Disrupts the Evaluation Process", "abstract": "Datasets are often generated in a sequential manner, where the previous samples and intermediate decisions or interventions affect subsequent samples. This is especially prominent in cases where there are significant human-AI interactions, such as in recommender systems. To characterize the importance of this relationship across samples, we propose to use adversarial attacks on popular evaluation processes. We present sequence-aware boosting attacks and provide a lower bound on the amount of extra information that can be exploited from a confidential test set solely based on the order of the observed data. We use real and synthetic data to test our methods and show that the evaluation process on the MovieLense-100k dataset can be affected by $$\\sim $$ 1% which is important when considering the close competition. Codes are publicly available ( https://github.com/alishiraliGit/augmented-boosting-attack )."}}
{"id": "WnQW6-4jQzD", "cdate": 1640995200000, "mdate": 1682029337878, "content": {"title": "On the Effect of Triadic Closure on Network Segregation", "abstract": "The tendency for individuals to form social ties with others who are similar to themselves, known as homophily, is one of the most robust sociological principles. Since this phenomenon can lead to patterns of interactions that segregate people along different demographic dimensions, it can also lead to inequalities in access to information, resources, and opportunities. As we consider potential interventions that might alleviate the effects of segregation, we face the challenge that homophily constitutes a pervasive and organic force that is difficult to push back against. Designing effective interventions can therefore benefit from identifying counterbalancing social processes that might be harnessed to work in opposition to segregation. In this work, we show that triadic closure---another common phenomenon that posits that individuals with a mutual connection are more likely to be connected to one another---can be one such process. In doing so, we challenge a long-held belief that triadic closure and homophily work in tandem. By analyzing several fundamental network models using popular integration measures, we demonstrate the desegregating potential of triadic closure. We further empirically investigate this effect on real-world dynamic networks, surfacing observations that mirror our theoretical findings. We leverage these insights to discuss simple interventions that can help reduce segregation in settings that exhibit an interplay between triadic closure and homophily. We conclude with a discussion on qualitative implications for the design of interventions in settings where individuals arrive in an online fashion, and the designer can influence the initial set of connections."}}
{"id": "qKid5MJAnw", "cdate": 1609459200000, "mdate": 1682029337879, "content": {"title": "Using distance on the Riemannian manifold to compare representations in brain and in models", "abstract": ""}}
{"id": "aJLjjpi0Vty", "cdate": 1601308421087, "mdate": null, "content": {"title": "Collaborative Filtering with Smooth Reconstruction of the Preference Function", "abstract": "The problem of predicting the rating of a set of users to a set of items in a recommender system based on partial knowledge of the ratings is widely known as collaborative filtering. In this paper, we consider a mapping of the items into a vector space and study the prediction problem by assuming an underlying smooth preference function for each user, the quantization at each given vector yields the associated rating. To estimate the preference functions, we implicitly cluster the users with similar ratings to form dominant types. Next, we associate each dominant type with a smooth preference function; i.e., the function values for items with nearby vectors shall be close to each other. \nThe latter is accomplished by a rich representation learning in a so-called frequency domain. In this framework, we propose two approaches for learning user and item representations. First, we use an alternating optimization method in the spirit of $k$-means to cluster users and map items. We further make this approach less prone to overfitting by a boosting technique. \nSecond, we present a feedforward neural network architecture consisting of interpretable layers which implicitely clusters the users. The performance of the method is evaluated on two benchmark datasets (ML-100k and ML-1M). Albeit the method benefits from simplicity, it shows a remarkable performance and opens a venue for future research. All codes are publicly available on the GitLab."}}
