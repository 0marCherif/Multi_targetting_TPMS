{"id": "LtoKJjkEnN", "cdate": 1674767450766, "mdate": 1674767450766, "content": {"title": "Consistent Multiclass Algorithms for Complex Metrics and Constraints", "abstract": "We present consistent algorithms for multiclass learning with complex performance metrics and constraints, where the objective and constraints are defined by arbitrary functions of the confusion matrix. This setting includes many common performance metrics such as the multiclass G-mean and micro F1-measure, and constraints such as those on the classifier\u2019s precision and recall and more recent measures of fairness discrepancy. We give a general framework for designing consistent algorithms for such complex design goals by viewing the learning problem as an optimization problem over the set of feasible confusion matrices. We provide multiple instantiations of our framework under different assumptions on the performance metrics and constraints, and in each case show rates of convergence to the optimal (feasible) classifier (and thus asymptotic consistency). Experiments on a variety of multiclass classification tasks and fairness constrained problems show that our algorithms compare favorably to the state-of-the-art baselines."}}
{"id": "SUb2pB_o9vG", "cdate": 1618413354215, "mdate": null, "content": {"title": "Consistent Plug-in Classifiers for Complex Objectives and Constraints", "abstract": "We present a consistent algorithm for constrained classification problems where the\nobjective (e.g. F-measure, G-mean) and the constraints (e.g. demographic parity\nfairness, coverage) are defined by general functions of the confusion matrix. Our\napproach reduces the problem into a sequence of plug-in classifier learning tasks.\nThe reduction is achieved by posing the learning problem as an optimization over\nthe intersection of two sets: the set of confusion matrices that are achievable and\nthose that are feasible. This decoupling of the constraint space then allows us to\nsolve the problem by applying Frank-Wolfe style optimization over the individual\nsets. For objective and constraints that are convex functions of the confusion matrix,\nour algorithm requires O(1/\u270f2) calls to the plug-in subroutine, which improves on\nthe O(1/\u270f3) calls needed by the reduction-based algorithm of Narasimhan (2018)\n[29]. We show empirically that our algorithm is competitive with prior methods,\nwhile being more robust to choices of hyper-parameters."}}
{"id": "vCEhC7nOb6", "cdate": 1601308168633, "mdate": null, "content": {"title": "Inductive Bias of Gradient Descent for Exponentially Weight Normalized Smooth Homogeneous Neural Nets", "abstract": "We analyze the inductive bias of gradient descent for weight normalized smooth homogeneous neural nets, when trained on exponential or cross-entropy loss. Our analysis focuses on exponential weight normalization (EWN), which encourages weight updates along the radial direction. This paper shows that the gradient flow path with EWN is equivalent to gradient flow on standard networks with an adaptive learning rate, and hence causes the weights to be updated in a way that prefers asymptotic relative sparsity. These results can be extended to hold for gradient descent via an appropriate adaptive learning rate. The asymptotic convergence rate of the loss in this setting is given by $\\Theta(\\frac{1}{t(\\log t)^2})$, and is independent of the depth of the network. We contrast these results with the inductive bias of standard weight normalization (SWN) and unnormalized architectures, and demonstrate their implications on synthetic data sets.Experimental results on simple data sets and architectures support our claim on sparse EWN solutions, even with SGD. This demonstrates its potential applications in learning prunable neural networks."}}
{"id": "Hy4I3GZ_bS", "cdate": 1546300800000, "mdate": null, "content": {"title": "On Knowledge distillation from complex networks for response prediction", "abstract": "Siddhartha Arora, Mitesh M. Khapra, Harish G. Ramaswamy. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019."}}
{"id": "Hy-W5_-_WS", "cdate": 1514764800000, "mdate": null, "content": {"title": "On Controllable Sparse Alternatives to Softmax", "abstract": "Converting an n-dimensional vector to a probability distribution over n objects is a commonly used component in many machine learning tasks like multiclass classification, multilabel classification, attention mechanisms etc. For this, several probability mapping functions have been proposed and employed in literature such as softmax, sum-normalization, spherical softmax, and sparsemax, but there is very little understanding in terms how they relate with each other. Further, none of the above formulations offer an explicit control over the degree of sparsity. To address this, we develop a unified framework that encompasses all these formulations as special cases. This framework ensures simple closed-form solutions and existence of sub-gradients suitable for learning via backpropagation. Within this framework, we propose two novel sparse formulations, sparsegen-lin and sparsehourglass, that seek to provide a control over the degree of desired sparsity. We further develop novel convex loss functions that help induce the behavior of aforementioned formulations in the multilabel classification setting, showing improved performance. We also demonstrate empirically that the proposed formulations, when used to compute attention weights, achieve better or comparable performance on standard seq2seq tasks like neural machine translation and abstractive summarization."}}
{"id": "rJNbrh-_WS", "cdate": 1451606400000, "mdate": null, "content": {"title": "Mixture Proportion Estimation via Kernel Embeddings of Distributions", "abstract": "Mixture proportion estimation (MPE) is the problem of estimating the weight of a component distribution in a mixture, given samples from the mixture and component. This problem constitutes a key pa..."}}
{"id": "rJZpZiZO-B", "cdate": 1420070400000, "mdate": null, "content": {"title": "Consistent Multiclass Algorithms for Complex Performance Measures", "abstract": "This paper presents new consistent algorithms for multiclass learning with complex performance measures, defined by arbitrary functions of the confusion matrix. This setting includes as a special c..."}}
{"id": "SJWDMhWd-S", "cdate": 1420070400000, "mdate": null, "content": {"title": "Convex Calibrated Surrogates for Hierarchical Classification", "abstract": "Hierarchical classification problems are multiclass supervised learning problems with a pre-defined hierarchy over the set of class labels. In this work, we study the consistency of hierarchical cl..."}}
{"id": "HkNQUu-ubr", "cdate": 1356998400000, "mdate": null, "content": {"title": "Convex Calibrated Surrogates for Low-Rank Loss Matrices with Applications to Subset Ranking Losses", "abstract": "The design of convex, calibrated surrogate losses, whose minimization entails consistency with respect to a desired target loss, is an important concept to have emerged in the theory of machine learning in recent years. We give an explicit construction of a convex least-squares type surrogate loss that can be designed to be calibrated for any multiclass learning problem for which the target loss matrix has a low-rank structure; the surrogate loss operates on a surrogate target space of dimension at most the rank of the target loss. We use this result to design convex calibrated surrogates for a variety of subset ranking problems, with target losses including the precision@q, expected rank utility, mean average precision, and pairwise disagreement."}}
{"id": "BJNhRvZO-S", "cdate": 1325376000000, "mdate": null, "content": {"title": "Classification Calibration Dimension for General Multiclass Losses", "abstract": "We study consistency properties of surrogate loss functions for general multiclass classification problems, defined by a general loss matrix. We extend the notion of classification calibration, which has been studied for binary and multiclass 0-1 classification problems (and for certain other specific learning problems), to the general multiclass setting, and derive necessary and sufficient conditions for a surrogate loss to be classification calibrated with respect to a loss matrix in this setting. We then introduce the notion of \\emph{classification calibration dimension} of a multiclass loss matrix, which measures the smallest <code>size' of a prediction space for which it is possible to design a convex surrogate that is classification calibrated with respect to the loss matrix. We derive both upper and lower bounds on this quantity, and use these results to analyze various loss matrices. In particular, as one application, we provide a different route from the recent result of Duchi et al.\\ (2010) for analyzing the difficulty of designing</code>low-dimensional' convex surrogates that are consistent with respect to pairwise subset ranking losses. We anticipate the classification calibration dimension may prove to be a useful tool in the study and design of surrogate losses for general multiclass learning problems."}}
