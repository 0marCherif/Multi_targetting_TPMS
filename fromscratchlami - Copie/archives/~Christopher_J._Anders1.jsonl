{"id": "veMxeMHiAIq", "cdate": 1693823790904, "mdate": 1693823790904, "content": {"title": "From Hope to Safety: Unlearning Biases of Deep Models by Enforcing the Right Reasons in Latent Space", "abstract": "Deep Neural Networks are prone to learning spurious correlations embedded in the training data, leading to potentially biased predictions. This poses risks when deploying these models for high-stake decision-making, such as in medical applications. Current methods for post-hoc model correction either require input-level annotations, which are only possible for spatially localized biases, or augment the latent feature space, thereby hoping to enforce the right reasons. We present a novel method ensuring the right reasons on the concept level by reducing the model's sensitivity towards biases through the gradient. When modeling biases via Concept Activation Vectors, we highlight the importance of choosing robust directions, as traditional regression-based approaches such as Support Vector Machines tend to result in diverging directions. We effectively mitigate biases in controlled and real-world settings on the ISIC, Bone Age, ImageNet and CelebA datasets using VGG, ResNet and EfficientNet architectures."}}
{"id": "JgrMWFKRTFF", "cdate": 1672531200000, "mdate": 1681718659108, "content": {"title": "Detecting and Mitigating Mode-Collapse for Flow-based Sampling of Lattice Field Theories", "abstract": "We study the consequences of mode-collapse of normalizing flows in the context of lattice field theory. Normalizing flows allow for independent sampling. For this reason, it is hoped that they can avoid the tunneling problem of local-update MCMC algorithms for multi-modal distributions. In this work, we first point out that the tunneling problem is also present for normalizing flows but is shifted from the sampling to the training phase of the algorithm. Specifically, normalizing flows often suffer from mode-collapse for which the training process assigns vanishingly low probability mass to relevant modes of the physical distribution. This may result in a significant bias when the flow is used as a sampler in a Markov-Chain or with Importance Sampling. We propose a metric to quantify the degree of mode-collapse and derive a bound on the resulting bias. Furthermore, we propose various mitigation strategies in particular in the context of estimating thermodynamic observables, such as the free energy."}}
{"id": "LEslx9-e4pk", "cdate": 1640995200000, "mdate": 1635340116733, "content": {"title": "Towards robust explanations for deep neural networks", "abstract": "Highlights \u2022 We investigate how to enhance the resilience of explanations against manipulation. \u2022 Explanations visualize the relevance of each input feature for the network\u2019s prediction. \u2022 We develop a theoretical framework and derive bounds on the maximal change of an explanation. \u2022 Based on these insights we present three different techniques to increase robustness. \u2022 training with weight decay. \u2022 smoothing activation functions. \u2022 minimizing the Hessian of the network. \u2022 Application of our methods shows significantly improved resilience of explanations. Abstract Explanation methods shed light on the decision process of black-box classifiers such as deep neural networks. But their usefulness can be compromised because they are susceptible to manipulations. With this work, we aim to enhance the resilience of explanations. We develop a unified theoretical framework for deriving bounds on the maximal manipulability of a model. Based on these theoretical insights, we present three different techniques to boost robustness against manipulation: training with weight decay, smoothing activation functions, and minimizing the Hessian of the network. Our experimental results confirm the effectiveness of these approaches."}}
{"id": "J2IUkvkMWI", "cdate": 1640995200000, "mdate": 1682340330965, "content": {"title": "PatClArC: Using Pattern Concept Activation Vectors for Noise-Robust Model Debugging", "abstract": ""}}
{"id": "4r0ryHIwHqR", "cdate": 1640995200000, "mdate": 1635340116735, "content": {"title": "Finding and removing Clever Hans: Using explanation methods to debug and improve deep models", "abstract": "Highlights \u2022 We extend Spectral Relevance Analysis for large scale analyses of Clever Hans traits. \u2022 We introduce Class Artifact Compensation to unlearn or suppress confounders. \u2022 We show that Deep Neural Networks tend to overfit to Clever Hans artifacts in data. \u2022 Using both methods, we discover and unlearn Clever Hans artifacts on modern datasets. \u2022 We demonstrate that Class Artifact Compensation leads to more trustworthy models. Abstract Contemporary learning models for computer vision are typically trained on very large (benchmark) datasets with millions of samples. These may, however, contain biases, artifacts, or errors that have gone unnoticed and are exploitable by the model. In the worst case, the trained model does not learn a valid and generalizable strategy to solve the problem it was trained for, and becomes a \u201cClever Hans\u201d predictor that bases its decisions on spurious correlations in the training data, potentially yielding an unrepresentative or unfair, and possibly even hazardous predictor. In this paper, we contribute by providing a comprehensive analysis framework based on a scalable statistical analysis of attributions from explanation methods for large data corpora. Based on a recent technique \u2014 Spectral Relevance Analysis \u2014 we propose the following technical contributions and resulting findings: (a) a scalable quantification of artifactual and poisoned classes where the machine learning models under study exhibit Clever Hans behavior, (b) several approaches we collectively denote as Class Artifact Compensation, which are able to effectively and significantly reduce a model\u2019s Clever Hans behavior, i.e., we are able to un-Hans models trained on (poisoned) datasets, such as the popular ImageNet data corpus. We demonstrate that Class Artifact Compensation, defined in a simple theoretical framework, may be implemented as part of a neural network\u2019s training or fine-tuning process, or in a post-hoc manner by injecting additional layers, preventing any further propagation of undesired Clever Hans features, into the network architecture. Using our proposed methods, we provide qualitative and quantitative analyses of the biases and artifacts in, e.g., the ImageNet dataset, the Adience benchmark dataset of unfiltered faces, and the ISIC 2019 skin lesion analysis dataset. We demonstrate that these insights can give rise to improved, more representative, and fairer models operating on implicitly cleaned data corpora."}}
{"id": "s8msJGU1Ef4", "cdate": 1632901648872, "mdate": 1632901648872, "content": {"title": "Fairwashing explanations with off-manifold detergent", "abstract": " Explanation methods promise to make black-box classifiers more transparent. As a result, it is hoped that they can act as proof for a sensible, fair and trustworthy decision-making process of the algorithm and thereby increase its acceptance by the end-users. In this paper, we show both theoretically and experimentally that these hopes are presently unfounded. Specifically, we show that, for any classifier g, one can always construct another classifier g~ which has the same behavior on the data (same train, validation, and test error) but has arbitrarily manipulated explanation maps. We derive this statement theoretically using differential geometry and demonstrate it experimentally for various explanation methods, architectures, and datasets. Motivated by our theoretical insights, we then propose a modification of existing explanation methods which makes them significantly more robust."}}
{"id": "FI4p3JtrOG", "cdate": 1632901220868, "mdate": 1632901220868, "content": {"title": "Towards robust explanations for deep neural networks", "abstract": "Explanation methods shed light on the decision process of black-box classifiers such as deep neural networks. But their usefulness can be compromised because they are susceptible to manipulations. With this work, we aim to enhance the resilience of explanations. We develop a unified theoretical framework for deriving bounds on the maximal manipulability of a model. Based on these theoretical insights, we present three different techniques to boost robustness against manipulation: training with weight decay, smoothing activation functions, and minimizing the Hessian of the network. Our experimental results confirm the effectiveness of these approaches."}}
{"id": "DDqQtL50cdZ", "cdate": 1632901115147, "mdate": 1632901115147, "content": {"title": "Explanations can be manipulated and geometry is to blame", "abstract": "Explanation methods aim to make neural networks more trustworthy and interpretable. In this paper, we demonstrate a property of explanation methods which is disconcerting for both of these purposes. Namely, we show that explanations can be manipulated arbitrarily by applying visually hardly perceptible perturbations to the input that keep the network\u2019s output approximately constant. We establish theoretically that this phenomenon can be related to certain geometrical properties of neural networks.  This allows us to derive an upper bound on the susceptibility of explanations to manipulations.  Based on this result, we propose effective mechanisms to enhance the robustness of explanations."}}
{"id": "weru5L3h1ML", "cdate": 1609459200000, "mdate": null, "content": {"title": "Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications", "abstract": "With the broader and highly successful usage of machine learning (ML) in industry and the sciences, there has been a growing demand for explainable artificial intelligence (XAI). Interpretability and explanation methods for gaining a better understanding of the problem-solving abilities and strategies of nonlinear ML, in particular, deep neural networks, are, therefore, receiving increased attention. In this work, we aim to: 1) provide a timely overview of this active emerging field, with a focus on \u201cpost hoc\u201d explanations, and explain its theoretical foundations; 2) put interpretability algorithms to a test both from a theory and comparative evaluation perspective using extensive simulations; 3) outline best practice aspects, i.e., how to best include interpretation methods into the standard usage of ML; and 4) demonstrate successful usage of XAI in a representative selection of application scenarios. Finally, we discuss challenges and possible future directions of this exciting foundational field of ML."}}
{"id": "vn1EJned6FC", "cdate": 1609459200000, "mdate": 1681718659328, "content": {"title": "Machine Learning of Thermodynamic Observables in the Presence of Mode Collapse", "abstract": "Estimating the free energy, as well as other thermodynamic observables, is a key task in lattice field theories. Recently, it has been pointed out that deep generative models can be used in this context [1]. Crucially, these models allow for the direct estimation of the free energy at a given point in parameter space. This is in contrast to existing methods based on Markov chains which generically require integration through parameter space. In this contribution, we will review this novel machine-learning-based estimation method. We will in detail discuss the issue of mode collapse and outline mitigation techniques which are particularly suited for applications at finite temperature."}}
