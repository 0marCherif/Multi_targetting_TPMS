{"id": "F6iq9FU2xnc", "cdate": 1655376345028, "mdate": null, "content": {"title": "Towards Long-Tailed 3D Detection", "abstract": "Contemporary autonomous vehicle (AV) benchmarks have advanced techniques for training 3D detectors, particularly on large-scale lidar data. Surprisingly, although semantic class labels naturally follow a long-tailed distribution, contemporary benchmarks focus on only a few common classes (e.g., pedestrian and car) and neglect many rare classes in-the-tail (e.g., debris and stroller).\nHowever, AVs must still detect rare classes to ensure safe operation. Moreover, semantic classes are often organized within a hierarchy, e.g., tail classes such as child and construction-worker are arguably subclasses of pedestrian. However, such hierarchical relationships are often ignored, which may lead to misleading estimates of performance and missed opportunities for algorithmic innovation. We address these challenges by formally studying the problem of Long-Tailed 3D Detection (LT3D), which evaluates on all classes, including those in-the-tail. We evaluate and innovate upon popular 3D detection codebases, such as CenterPoint and PointPillars, adapting them for LT3D.\nWe develop hierarchical losses that promote feature sharing across common-vs-rare classes, as well as improved detection metrics that award partial credit to \"reasonable\" mistakes respecting the hierarchy (e.g., mistaking a child for an adult). Finally, we point out that fine-grained tail class accuracy is particularly improved via multimodal fusion of RGB images with LiDAR; simply put, small fine-grained classes are challenging to identify from sparse (lidar) geometry alone, suggesting that multimodal cues are crucial to long-tailed 3D detection. Our modifications improve accuracy by 5% AP on average for all classes, and dramatically improve AP for rare classes (e.g., stroller AP improves from 3.6 to 31.6)."}}
{"id": "zW6oLOt3_wI", "cdate": 1640995200000, "mdate": 1668531668504, "content": {"title": "Assessment of a Novel Virtual Environment for Examining Human Cognitive-Motor Performance During Execution of Action Sequences", "abstract": "The examination of neural resource allocation during complex action sequence execution is critical to understanding human behavior. While physical systems are usually used for such assessment, virtual/remote systems offer other approaches with potential benefits such as remote training/evaluation. Here we describe a virtual environment (VLEARN) operated via the internet that has been developed to study the cognitive-motor mechanisms underlying the execution of goal-oriented action sequences in remote and laboratory settings. This study aimed to i) examine the feasibility of evaluating human cognitive-motor behavior when individuals operate VLEARN to complete various tasks; and ii) assess VLEARN by comparing its usability and the resulting performance, mental workload, and mental/physical fatigue during virtual and physical task execution. Results revealed that our approach allowed human cognitive-motor behavior assessment as the tasks completed physically and virtually via VLEARN had similar success rates. Also, there was a relationship between the complexity of the virtual control systems and the dependency on those to complete tasks. Namely, relative to controls with more functionalities, when VLEARN enabled simpler controls, above average usability and similar levels of cognitive-motor performance for both physical and virtual task execution were observed. Thus, a simplification of some aspects of the VLEARN control interface should enhance its usability. Our approach is promising for examining human cognitive-motor behavior and informing multiple applications (e.g., telehealth, remote training)."}}
{"id": "iedsF9FChJX", "cdate": 1640995200000, "mdate": 1668531646550, "content": {"title": "Forecasting from LiDAR via Future Object Detection", "abstract": "Object detection and forecasting are fundamental components of embodied perception. These two problems, how-ever, are largely studied in isolation by the community. In this paper, we propose an end-to-end approachfor detection and motion forecasting based on raw sensor measurement as opposed to ground truth tracks. Instead of predicting the current frame locations and forecasting forward in time, we directly predict future object locations and backcast to determine where each trajectory began. Our approach not only improves overall accuracy compared to other modular or end-to-end baselines, it also prompts us to rethink the role of explicit tracking for embodied perception. Additionally, by linking future and current locations in a many-to-one manner, our approach is able to reason about multiple futures, a capability that was previously considered difficult for end-to-end approaches. We conduct extensive experi-ments on the popular nuScenes dataset and demonstrate the empirical effectiveness of our approach. In addition, we investigate the appropriateness of reusing standard forecasting metrics for an end-to-end setup, and find a number of limitations which allow us to build simple baselines to game these metrics. We address this issue with a novel set of joint forecasting and detection metrics that extend the commonly used AP metrics from the detection community to measuring forecasting accuracy. Our code is available on GitHub."}}
{"id": "amH9JxZN7C", "cdate": 1621629719035, "mdate": null, "content": {"title": "PreferenceNet: Encoding Human Preferences in Auction Design with Deep Learning", "abstract": "The design of optimal auctions is a problem of interest in economics, game theory and computer science. Despite decades of effort, strategyproof, revenue-maximizing auction designs are still not known outside of restricted settings. However, recent methods using deep learning have shown some success in approximating optimal auctions, recovering several known solutions and outperforming strong baselines when optimal auctions are not known. In addition to maximizing revenue, auction mechanisms may also seek to encourage socially desirable constraints such as allocation fairness or diversity. However, these philosophical notions neither have standardization nor do they have widely accepted formal definitions. In this paper, we propose PreferenceNet, an extension of existing neural-network-based auction mechanisms to encode constraints using (potentially human-provided) exemplars of desirable allocations. In addition, we introduce a new metric to evaluate an auction allocations' adherence to such socially desirable constraints and demonstrate that our proposed method is competitive with current state-of-the-art neural-network based auction designs. We validate our approach through human subject research and show that we are able to effectively capture real human preferences."}}
{"id": "OvxroOi4hM", "cdate": 1609459200000, "mdate": 1668531646572, "content": {"title": "A Synthesis-Based Approach for Thermal-to-Visible Face Verification", "abstract": "In recent years, visible-spectrum face verification systems have been shown to match the performance of experienced forensic examiners. However, such systems are ineffective in low-light and nighttime conditions. Thermal face imagery, which captures body heat emissions, effectively augments the visible spectrum, capturing discriminative facial features in scenes with limited illumination. Due to the increased cost and difficulty of obtaining diverse, paired thermal and visible spectrum datasets, not many algorithms and large-scale benchmarks for low-light recognition are available. This paper presents an algorithm that achieves state-of-the-art performance on both the ARL-VTF and TUFTS multi-spectral face datasets. Importantly, we study the impact of face alignment, pixel-level correspondence, and identity classification with label smoothing for multi-spectral face synthesis and verification. We show that our proposed method is widely applicable, robust, and highly effective. In addition, we show that the proposed method significantly outperforms face frontalization methods on profile-to-frontal verification. Finally, we present MILAB-VTF(B), a challenging multi-spectral face dataset that is composed of paired thermal and visible videos. To the best of our knowledge, with face data from 400 subjects, this dataset represents the most extensive collection of publicly available indoor and long-range outdoor thermal-visible face imagery. Lastly, we show that our end-to-end thermal-to-visible face verification system provides strong performance on the MILAB-VTF(B) dataset."}}
{"id": "3cIm3Xr1RR", "cdate": 1609459200000, "mdate": 1668531668506, "content": {"title": "PreferenceNet: Encoding Human Preferences in Auction Design with Deep Learning", "abstract": "The design of optimal auctions is a problem of interest in economics, game theory and computer science. Despite decades of effort, strategyproof, revenue-maximizing auction designs are still not known outside of restricted settings. However, recent methods using deep learning have shown some success in approximating optimal auctions, recovering several known solutions and outperforming strong baselines when optimal auctions are not known. In addition to maximizing revenue, auction mechanisms may also seek to encourage socially desirable constraints such as allocation fairness or diversity. However, these philosophical notions neither have standardization nor do they have widely accepted formal definitions. In this paper, we propose PreferenceNet, an extension of existing neural-network-based auction mechanisms to encode constraints using (potentially human-provided) exemplars of desirable allocations. In addition, we introduce a new metric to evaluate an auction allocations' adherence to such socially desirable constraints and demonstrate that our proposed method is competitive with current state-of-the-art neural-network based auction designs. We validate our approach through human subject research and show that we are able to effectively capture real human preferences."}}
{"id": "cviGW9mqsP", "cdate": 1582060746013, "mdate": null, "content": {"title": "A Dual Path Model With Adaptive Attention For Vehicle Re-Identification", "abstract": "In recent years, attention models have been extensively\nused for person and vehicle re-identification. Most reidentification methods are designed to focus attention on\nkey-point locations. However, depending on the orientation, the contribution of each key-point varies. In this paper,\nwe present a novel dual-path adaptive attention model for\nvehicle re-identification (AAVER). The global appearance\npath captures macroscopic vehicle features while the orientation conditioned part appearance path learns to capture\nlocalized discriminative features by focusing attention\non the most informative key-points. Through extensive\nexperimentation, we show that the proposed AAVER method\nis able to accurately re-identify vehicles in unconstrained\nscenarios, yielding state of the art results on the challenging dataset VeRi-776. As a byproduct, the proposed system\nis also able to accurately predict vehicle key-points and\nshows an improvement of more than 7% over state of the\nart"}}
{"id": "g2vwf7lztcv", "cdate": 1577836800000, "mdate": 1668531668520, "content": {"title": "Deep k-NN Defense Against Clean-Label Data Poisoning Attacks", "abstract": "Targeted clean-label data poisoning is a type of adversarial attack on machine learning systems in which an adversary injects a few correctly-labeled, minimally-perturbed samples into the training data, causing a model to misclassify a particular test sample during inference. Although defenses have been proposed for general poisoning attacks, no reliable defense for clean-label attacks has been demonstrated, despite the attacks\u2019 effectiveness and realistic applications. In this work, we propose a simple, yet highly-effective Deep k-NN defense against both feature collision and convex polytope clean-label attacks on the CIFAR-10 dataset. We demonstrate that our proposed strategy is able to detect over 99% of poisoned examples in both attacks and remove them without compromising model performance. Additionally, through ablation studies, we discover simple guidelines for selecting the value of k as well as for implementing the Deep k-NN defense on real-world datasets with class imbalance. Our proposed defense shows that current clean-label poisoning attack strategies can be annulled, and serves as a strong yet simple-to-implement baseline defense to test future clean-label poisoning attacks. Our code is available on GitHub ."}}
{"id": "KLyoXI6acSZ", "cdate": 1577836800000, "mdate": 1668531646568, "content": {"title": "The Devil Is in the Details: Self-supervised Attention for Vehicle Re-identification", "abstract": "In recent years, the research community has approached the problem of vehicle re-identification (re-id) with attention-based models, specifically focusing on regions of a vehicle containing discriminative information. These re-id methods rely on expensive key-point labels, part annotations, and additional attributes including vehicle make, model, and color. Given the large number of vehicle re-id datasets with various levels of annotations, strongly-supervised methods are unable to scale across different domains. In this paper, we present Self-supervised Attention for Vehicle Re-identification (SAVER), a novel approach to effectively learn vehicle-specific discriminative features. Through extensive experimentation, we show that SAVER improves upon the state-of-the-art on challenging VeRi, VehicleID, Vehicle-1M and VERI-Wild datasets."}}
{"id": "9BwHzuQC9P", "cdate": 1577836800000, "mdate": 1668531668524, "content": {"title": "Towards Real-Time Systems for Vehicle Re-Identification, Multi-Camera Tracking, and Anomaly Detection", "abstract": "Vehicle re-identification, multi-camera vehicle tracking, and anomaly detection are essential for city-scale intelligent transportation systems. Both vehicle re-id and multi-camera tracking are challenging due to variations in aspect-ratio, occlusion, and orientation. Robust re-id and tracking systems must consider small scale variations in a vehicle's appearance to accurately distinguish among vehicles of the same make, model, and color. Scalability is critical for multi-camera systems, as the number of objects in a scene is not known a-priori. Anomaly detection presents a unique challenge due to a dearth of annotations and varied video quality. In this paper, we address the task of vehicle re-id by introducing an unsupervised excitation layer to enhance representation learning. We propose a multi-camera tracking pipeline leveraging this re-id feature extractor to compute a distance matrix and perform clustering to obtain multi-camera vehicle trajectories. Lastly, we leverage background modeling techniques to localize anomalies such as stalled vehicles and collisions. We show the effectiveness of our proposed method on the NVIDIA AI City Challenge, where we obtain 7th place out of 41 teams for the task of vehicle re-id, with an mAP score of 66.68% and achieve state-of-the-art results on the Vehicle-ID dataset. We also obtain an IDF1 score of 12.45% on multi-camera vehicle tracking, and an S4 score of 29.52% for task of anomaly detection, ranking in the top 5 for both tracks."}}
