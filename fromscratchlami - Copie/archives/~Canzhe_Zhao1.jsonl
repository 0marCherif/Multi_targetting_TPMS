{"id": "lu5rBe_lL8m", "cdate": 1672531200000, "mdate": 1696002371471, "content": {"title": "DPMAC: Differentially Private Communication for Cooperative Multi-Agent Reinforcement Learning", "abstract": "Communication lays the foundation for cooperation in human society and in multi-agent reinforcement learning (MARL). Humans also desire to maintain their privacy when communicating with others, yet such privacy concern has not been considered in existing works in MARL. We propose the differentially private multi-agent communication (DPMAC) algorithm, which protects the sensitive information of individual agents by equipping each agent with a local message sender with rigorous (epsilon, delta)-differential privacy (DP) guarantee. In contrast to directly perturbing the messages with predefined DP noise as commonly done in privacy-preserving scenarios, we adopt a stochastic message sender for each agent respectively and incorporate the DP requirement into the sender, which automatically adjusts the learned message distribution to alleviate the instability caused by DP noise. Further, we prove the existence of a Nash equilibrium in cooperative MARL with privacy-preserving communication, which suggests that this problem is game-theoretically learnable. Extensive experiments demonstrate a clear advantage of DPMAC over baseline methods in privacy-preserving scenarios."}}
{"id": "dfBl3kWVGf", "cdate": 1672531200000, "mdate": 1683879281252, "content": {"title": "Differentially Private Temporal Difference Learning with Stochastic Nonconvex-Strongly-Concave Optimization", "abstract": ""}}
{"id": "ajc8Oibf_i", "cdate": 1672531200000, "mdate": 1683879281464, "content": {"title": "Best-of-three-worlds Analysis for Linear Bandits with Follow-the-regularized-leader Algorithm", "abstract": "The linear bandit problem has been studied for many years in both stochastic and adversarial settings. Designing an algorithm that can optimize the environment without knowing the loss type attracts lots of interest. \\citet{LeeLWZ021} propose an algorithm that actively detects the loss type and then switches between different algorithms specially designed for specific settings. However, such an approach requires meticulous designs to perform well in all environments. Follow-the-regularized-leader (FTRL) is another type of popular algorithm that can adapt to different environments. This algorithm is of simple design and the regret bounds are shown to be optimal in traditional multi-armed bandit problems compared with the detect-switch type. Designing an FTRL-type algorithm for linear bandits is an important question that has been open for a long time. In this paper, we prove that the FTRL algorithm with a negative entropy regularizer can achieve the best-of-three-world results for the linear bandit problem. Our regret bounds achieve the same or nearly the same order as the previous detect-switch type algorithm but with a much simpler algorithmic design."}}
{"id": "Njke5qPTM7", "cdate": 1672531200000, "mdate": 1696002371472, "content": {"title": "Learning Adversarial Linear Mixture Markov Decision Processes with Bandit Feedback and Unknown Transition", "abstract": ""}}
{"id": "AX04IRL2_NT", "cdate": 1672531200000, "mdate": 1696002371469, "content": {"title": "Best-of-three-worlds Analysis for Linear Bandits with Follow-the-regularized-leader Algorithm", "abstract": "The linear bandit problem has been studied for many years in both stochastic and adversarial settings. Designing an algorithm that can optimize the environment without knowing the loss type attract..."}}
{"id": "-BT3cqvtw3", "cdate": 1672531200000, "mdate": 1696002371462, "content": {"title": "DPMAC: Differentially Private Communication for Cooperative Multi-Agent Reinforcement Learning", "abstract": "Communication lays the foundation for cooperation in human society and in multi-agent reinforcement learning (MARL). Humans also desire to maintain their privacy when communicating with others, yet such privacy concern has not been considered in existing works in MARL. To this end, we propose the \\textit{differentially private multi-agent communication} (DPMAC) algorithm, which protects the sensitive information of individual agents by equipping each agent with a local message sender with rigorous $(\\epsilon, \\delta)$-differential privacy (DP) guarantee. In contrast to directly perturbing the messages with predefined DP noise as commonly done in privacy-preserving scenarios, we adopt a stochastic message sender for each agent respectively and incorporate the DP requirement into the sender, which automatically adjusts the learned message distribution to alleviate the instability caused by DP noise. Further, we prove the existence of a Nash equilibrium in cooperative MARL with privacy-preserving communication, which suggests that this problem is game-theoretically learnable. Extensive experiments demonstrate a clear advantage of DPMAC over baseline methods in privacy-preserving scenarios."}}
{"id": "sVU54nyaA9K", "cdate": 1663850122863, "mdate": null, "content": {"title": "Learning Adversarial Linear Mixture Markov Decision Processes with Bandit Feedback and Unknown Transition", "abstract": "We study reinforcement learning (RL) with linear function approximation, unknown transition, and adversarial losses in the bandit feedback setting. Specifically, the unknown transition probability function is a linear mixture model \\citep{AyoubJSWY20,ZhouGS21,HeZG22} with a given feature mapping, and the learner only observes the losses of the experienced state-action pairs instead of the whole loss function. We propose an efficient algorithm LSUOB-REPS which achieves $\\widetilde{O}(dS^2\\sqrt{K}+\\sqrt{HSAK})$ regret guarantee with high probability, where $d$ is the ambient dimension of the feature mapping, $S$ is the size of the state space, $A$ is the size of the action space, $H$ is the episode length and $K$ is the number of episodes. Furthermore, we also prove a lower bound of order $\\Omega(dH\\sqrt{K}+\\sqrt{HSAK})$ for this setting. To the best of our knowledge, we make the first step to establish a provably efficient algorithm with a sublinear regret guarantee in this challenging setting and solve the open problem of \\citet{HeZG22}."}}
{"id": "VIwEYmMID9R", "cdate": 1663850117850, "mdate": null, "content": {"title": "DPMAC: Differentially Private Communication for Cooperative Multi-Agent Reinforcement Learning", "abstract": "Communication lays the foundation for cooperation in human society and in multi-agent reinforcement learning (MARL). Humans also desire to maintain their privacy when communicating with others, yet such privacy concern has not been considered in existing works in MARL. We propose the \\textit{differentially private multi-agent communication} (DPMAC) algorithm, which protects the sensitive information of individual agents by equipping each agent with a local message sender with rigorous $(\\epsilon, \\delta)$-differential privacy (DP) guarantee. In contrast to directly perturbing the messages with predefined DP noise as commonly done in privacy-preserving scenarios, we adopt a stochastic message sender for each agent respectively and incorporate the DP requirement into the sender, which automatically adjusts the learned message distribution to alleviate the instability caused by DP noise. Further, we prove the existence of a Nash equilibrium in cooperative MARL with privacy-preserving communication, which suggests that this problem is game-theoretically learnable. Extensive experiments demonstrate a clear advantage of DPMAC over baseline methods in privacy-preserving scenarios."}}
{"id": "nv5NIQ71m2", "cdate": 1640995200000, "mdate": 1683879281261, "content": {"title": "Differentially Private Temporal Difference Learning with Stochastic Nonconvex-Strongly-Concave Optimization", "abstract": "Temporal difference (TD) learning is a widely used method to evaluate policies in reinforcement learning. While many TD learning methods have been developed in recent years, little attention has been paid to preserving privacy and most of the existing approaches might face the concerns of data privacy from users. To enable complex representative abilities of policies, in this paper, we consider preserving privacy in TD learning with nonlinear value function approximation. This is challenging because such a nonlinear problem is usually studied in the formulation of stochastic nonconvex-strongly-concave optimization to gain finite-sample analysis, which would require simultaneously preserving the privacy on primal and dual sides. To this end, we employ a momentum-based stochastic gradient descent ascent to achieve a single-timescale algorithm, and achieve a good trade-off between meaningful privacy and utility guarantees of both the primal and dual sides by perturbing the gradients on both sides using well-calibrated Gaussian noises. As a result, our DPTD algorithm could provide $(\\epsilon,\\delta)$-differential privacy (DP) guarantee for the sensitive information encoded in transitions and retain the original power of TD learning, with the utility upper bounded by $\\widetilde{\\mathcal{O}}(\\frac{(d\\log(1/\\delta))^{1/8}}{(n\\epsilon)^{1/4}})$ (The tilde in this paper hides the log factor.), where $n$ is the trajectory length and $d$ is the dimension. Extensive experiments conducted in OpenAI Gym show the advantages of our proposed algorithm."}}
{"id": "nMZG1DJQDn1", "cdate": 1640995200000, "mdate": 1683879281354, "content": {"title": "Knowledge-aware Conversational Preference Elicitation with Bandit Feedback", "abstract": ""}}
