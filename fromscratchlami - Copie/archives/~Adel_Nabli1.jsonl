{"id": "8Qc0UYFog4", "cdate": 1672531200000, "mdate": 1693079297590, "content": {"title": "A2CiD2: Accelerating Asynchronous Communication in Decentralized Deep Learning", "abstract": "Distributed training of Deep Learning models has been critical to many recent successes in the field. Current standard methods primarily rely on synchronous centralized algorithms which induce major communication bottlenecks and limit their usability to High-Performance Computing (HPC) environments with strong connectivity. Decentralized asynchronous algorithms are emerging as a potential alternative but their practical applicability still lags. In this work, we focus on peerto-peer asynchronous methods due to their flexibility and parallelization potentials. In order to mitigate the increase in bandwidth they require at large scale and in poorly connected contexts, we introduce a principled asynchronous, randomized, gossip-based algorithm which works thanks to a continuous momentum named $\\textbf{A}^2\\textbf{CiD}^2$. In addition to inducing a significant communication acceleration at no cost other than doubling the parameters, minimal adaptation is required to incorporate $\\textbf{A}^2\\textbf{CiD}^2$ to other asynchronous approaches. We demonstrate its efficiency theoretically and numerically. Empirically on the ring graph, adding $\\textbf{A}^2\\textbf{CiD}^2$ has the same effect as doubling the communication rate. In particular, we show consistent improvement on the ImageNet dataset using up to 64 asynchronous workers (A100 GPUs) and various communication network topologies."}}
{"id": "3G-3e5rEGdV", "cdate": 1672531200000, "mdate": 1693079297583, "content": {"title": "DADAO: Decoupled Accelerated Decentralized Asynchronous Optimization", "abstract": "This work introduces DADAO: the first decentralized, accelerated, asynchronous, primal, first-order algorithm to minimize a sum of $L$-smooth and $\\mu$-strongly convex functions distributed over a ..."}}
{"id": "Siln8xpTMrZ", "cdate": 1663849984222, "mdate": null, "content": {"title": "DADAO: Decoupled Accelerated Decentralized Asynchronous Optimization", "abstract": "DADAO is a novel decentralized asynchronous stochastic first order algorithm to minimize a sum of $L$-smooth and $\\mu$-strongly convex functions distributed over a time-varying connectivity network of size $n$.  We model the local gradient updates and gossip communication procedures with separate independent Poisson Point Processes, decoupling the computation and communication steps in addition to making the whole approach completely asynchronous. Our method employs primal gradients and does not use a multi-consensus inner loop nor other ad-hoc mechanisms such as Error Feedback, Gradient Tracking, or a Proximal operator. By relating the inverse of the smallest positive eigenvalue $\\chi^*_1$ and the effective resistance $\\chi_2^*$ of our graph to a necessary minimal communication rate between nodes of the network, we show that our algorithm requires $\\mathcal{O}(n\\sqrt{\\frac{L}{\\mu}}\\log \\epsilon)$ local gradients and only  $\\mathcal{O}(n\\sqrt{\\chi_1^*\\chi_2^*}\\sqrt{\\frac{L}{\\mu}}\\log \\epsilon)$ communications to reach a precision $\\epsilon$. If SGD with uniform noise $\\sigma^2$ is used, we reach a precision $\\epsilon$ with same speed, up to a bias term in $\\mathcal{O}(\\frac{\\sigma^2}{\\sqrt{\\mu L}})$. This improves upon the bounds obtained with current state-of-the-art approaches, our simulations validating the strength of our relatively unconstrained method."}}
{"id": "Urc0el3hd5", "cdate": 1640995200000, "mdate": 1672022986187, "content": {"title": "Speech Sequence Embeddings using Nearest Neighbors Contrastive Learning", "abstract": "We introduce a simple neural encoder architecture that can be trained using an unsupervised contrastive learning objective which gets its positive samples from data-augmented k-Nearest Neighbors search. We show that when built on top of recent self-supervised audio representations, this method can be applied iteratively and yield competitive SSE as evaluated on two tasks: query-by-example of random sequences of speech, and spoken term discovery. On both tasks our method pushes the state-of-the-art by a significant margin across 5 different languages. Finally, we establish a benchmark on a query-by-example task on the LibriSpeech dataset to monitor future improvements in the field."}}
{"id": "LszhhIIuTSv", "cdate": 1640995200000, "mdate": 1652197894591, "content": {"title": "Complexity of the multilevel critical node problem", "abstract": ""}}
{"id": "ibjMebJDGL3", "cdate": 1577836800000, "mdate": 1652197894592, "content": {"title": "Curriculum learning for multilevel budgeted combinatorial problems", "abstract": "Learning heuristics for combinatorial optimization problems through graph neural networks have recently shown promising results on some classic NP-hard problems. These are single-level optimization problems with only one player. Multilevel combinatorial optimization problems are their generalization, encompassing situations with multiple players taking decisions sequentially. By framing them in a multi-agent reinforcement learning setting, we devise a value-based method to learn to solve multilevel budgeted combinatorial problems involving two players in a zero-sum game over a graph. Our framework is based on a simple curriculum: if an agent knows how to estimate the value of instances with budgets up to $B$, then solving instances with budget $B+1$ can be done in polynomial time regardless of the direction of the optimization by checking the value of every possible afterstate. Thus, in a bottom-up approach, we generate datasets of heuristically solved instances with increasingly larger budgets to train our agent. We report results close to optimality on graphs up to $100$ nodes and a $185 \\times$ speedup on average compared to the quickest exact solver known for the Multilevel Critical Node problem, a max-min-max trilevel problem that has been shown to be at least $\\Sigma_2^p$-hard."}}
