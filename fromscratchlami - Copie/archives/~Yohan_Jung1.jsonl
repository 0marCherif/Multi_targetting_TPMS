{"id": "lKO6SvuxNhe", "cdate": 1640995200000, "mdate": 1681713358355, "content": {"title": "Bayesian Convolutional Deep Sets with Task-Dependent Stationary Prior", "abstract": "Convolutional deep sets are the architecture of a deep neural network (DNN) that can model stationary stochastic process. This architecture uses the kernel smoother and the DNN to construct the translation equivariant functional representations, and thus reflects the inductive bias of the stationarity into DNN. However, since this architecture employs the kernel smoother known as the non-parametric model, it may produce ambiguous representations when the number of data points is not given sufficiently. To remedy this issue, we introduce Bayesian convolutional deep sets that construct the random translation equivariant functional representations with stationary prior. Furthermore, we present how to impose the task-dependent prior for each dataset because a wrongly imposed prior forms an even worse representation than that of the kernel smoother. We validate the proposed architecture and its training on various experiments with time-series and image datasets."}}
{"id": "VG5CaY7jto", "cdate": 1640995200000, "mdate": 1667728090505, "content": {"title": "Efficient Approximate Inference for Stationary Kernel on Frequency Domain", "abstract": "Based on the Fourier duality between a stationary kernel and its spectral density, modeling the spectral density using a Gaussian mixture density enables one to construct a flexible kernel, known a..."}}
{"id": "h7pXZZ3rF1g", "cdate": 1609459200000, "mdate": 1667728090749, "content": {"title": "Implicit Kernel Attention", "abstract": "Attention computes the dependency between representations, and it encourages the model to focus on the important selective features. Attention-based models, such as Transformer and graph attention network (GAT), are widely utilized for sequential data and graph-structured data. This paper suggests a new interpretation and generalized structure of the attention in Transformer and GAT. For the attention in Transformer and GAT, we derive that the attention is a product of two parts: 1) the RBF kernel to measure the similarity of two instances and 2) the exponential of L2 norm to compute the importance of individual instances. From this decomposition, we generalize the attention in three ways. First, we propose implicit kernel attention with an implicit kernel function instead of manual kernel selection. Second, we generalize L2 norm as the Lp norm. Third, we extend our attention to structured multi-head attention. Our generalized attention shows better performance on classification, translation, and regression tasks."}}
{"id": "gls08I17Zx", "cdate": 1606146129558, "mdate": null, "content": {"title": "Scalable Hybrid Hidden Markov Model with Gaussian Process Emission for Sequential Time-series Observations", "abstract": "A hidden Markov model (HMM) using Gaussian Process as an emission model has been widely used to model sequential data in complex form. This study particularly introduces the hybrid Bayesian HMM with GP emission using SM kernel, which we call HMM-GPSM, for estimating a hidden state of each time-series observation sequentially observed from a single channel. We then propose a scalable learning method to train the HMM-GPSM model using large-scale data having (1) long sequences of state transitions and (2) a large number of time-series observations for each hidden state. For a long sequence of state transitions, we employ stochastic variational inference (SVI) to efficiently update the parameters of HMM-GPSM. For a large number of data points in each time series observation, we propose the approximate GP emission using the spectral points sampled from the spectral density of SM kernel by Random Fourier feature (RFF) and the efficient inference for the kernel hyperparameters of approximate GP emission and corresponding HMM-GPSM. Specifically, we derive the training loss, i.e., the evidence lower bound of the HMM-GPSM that can be scalably computed for a large number of time-series observations by employing the regularized lower bound of GP emission likelihood with KL divergence. The proposed methods can be used together for the sequential time-series dataset having both (1) and (2). We validate the proposed method on the synthetic using the clustering accuracy and training time as the performance metrics."}}
{"id": "fIFwfgqJB_", "cdate": 1577836800000, "mdate": null, "content": {"title": "Scalable Hybrid HMM with Gaussian Process Emission for Sequential Time-series Data Clustering", "abstract": "Hidden Markov Model (HMM) combined with Gaussian Process (GP) emission can be effectively used to estimate the hidden state with a sequence of complex input-output relational observations. Especially when the spectral mixture (SM) kernel is used for GP emission, we call this model as a hybrid HMM-GPSM. This model can effectively model the sequence of time-series data. However, because of a large number of parameters for the SM kernel, this model can not effectively be trained with a large volume of data having (1) long sequence for state transition and 2) a large number of time-series dataset in each sequence. This paper proposes a scalable learning method for HMM-GPSM. To effectively train the model with a long sequence, the proposed method employs a Stochastic Variational Inference (SVI) approach. Also, to effectively process a large number of data point each time-series data, we approximate the SM kernel using Reparametrized Random Fourier Feature (R-RFF). The combination of these two techniques significantly reduces the training time. We validate the proposed learning method in terms of its hidden-sate estimation accuracy and computation time using large-scale synthetic and real data sets with missing values."}}
{"id": "HJlvKy3VFS", "cdate": 1571237758984, "mdate": null, "content": {"title": "Spectral Mixture Kernel Approximation Using Reparameterized Random Fourier Feature", "abstract": "We propose a method for Spectral Mixture kernel approximation using the Reparameterized Random Fourier Feature (R-RFF) in the sense of both general parameter and natural parameter view. Meanwhile, we provide the effective sampling methods of spectral points which samples the number of spectral points by considering the normalized weight parameters of SM kernel. Also, we develop the regularized sparse spectrum approximation by using Stochastic Gradient Variational Bayes for scalable learning of GP model with SM kernel."}}
