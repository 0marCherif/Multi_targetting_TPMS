{"id": "wZkajy4oBaa", "cdate": 1640995200000, "mdate": 1667617503875, "content": {"title": "Sketching without Worrying: Noise-Tolerant Sketch-Based Image Retrieval", "abstract": "Sketching enables many exciting applications, notably, image retrieval. The fear-to-sketch problem (i.e., \u201cI can't sketch\u201d) has however proven to be fatal for its widespread adoption. This paper tackles this \u201cfear\u201d head on, and for the first time, proposes an auxiliary module for existing retrieval models that predominantly lets the users sketch without having to worry. We first conducted a pilot study that revealed the secret lies in the existence of noisy strokes, but not so much of the \u201cI can't sketch\u201d. We consequently design a stroke subset selector that detects noisy strokes, leaving only those which make a positive contribution towards successful retrieval. Our Reinforcement Learning based formulation quantifies the importance of each stroke present in a given subset, based on the extent to which that stroke contributes to retrieval. When combined with pre-trained retrieval models as a pre-processing module, we achieve a significant gain of 8%-10% over standard baselines and in turn report new state-of-the-art performance. Last but not least, we demonstrate the selector once trained, can also be used in a plug-and-play manner to empower various sketch applications in ways that were not previously possible."}}
{"id": "uDogGPu5wlp", "cdate": 1640995200000, "mdate": 1667617503876, "content": {"title": "Sketch3T: Test-Time Training for Zero-Shot SBIR", "abstract": "Zero-shot sketch-based image retrieval typically asks for a trained model to be applied as is to unseen categories. In this paper, we question to argue that this setup by definition is not compatible with the inherent abstract and subjective nature of sketches \u2013 the model might transfer well to new categories, but will not understand sketches existing in different test-time distribution as a result. We thus extend ZS-SBIR asking it to transfer to both categories and sketch distributions. Our key contribution is a test-time training paradigm that can adapt using just one sketch. Since there is no paired photo, we make use of a sketch raster-vector reconstruction module as a self-supervised auxiliary task. To maintain the fidelity of the trained cross-modal joint embedding during test-time update, we design a novel meta-learning based training paradigm to learn a separation between model updates incurred by this auxiliary task from those off the primary objective of discriminative learning. Extensive experiments show our model to outperform state-of-the-arts, thanks to the proposed test-time adaption that not only transfers to new categories but also accommodates to new sketching styles."}}
{"id": "t7mmLxhJXdU", "cdate": 1640995200000, "mdate": 1667617503883, "content": {"title": "FS-COCO: Towards Understanding of Freehand Sketches of Common Objects in Context", "abstract": "We advance sketch research to scenes with the first dataset of freehand scene sketches, FS-COCO. With practical applications in mind, we collect sketches that convey scene content well but can be sketched within a few minutes by a person with any sketching skills. Our dataset comprises 10,000 freehand scene vector sketches with per point space-time information by 100 non-expert individuals, offering both object- and scene-level abstraction. Each sketch is augmented with its text description. Using our dataset, we study for the first time the problem of fine-grained image retrieval from freehand scene sketches and sketch captions. We draw insights on: (i) Scene salience encoded in sketches using the strokes temporal order; (ii) Performance comparison of image retrieval from a scene sketch and an image caption; (iii) Complementarity of information in sketches and image captions, as well as the potential benefit of combining the two modalities. In addition, we extend a popular vector sketch LSTM-based encoder to handle sketches with larger complexity than was supported by previous work. Namely, we propose a hierarchical sketch decoder, which we leverage at a sketch-specific \"pre-text\" task. Our dataset enables for the first time research on freehand scene sketch understanding and its practical applications."}}
{"id": "nccKuDJWi0", "cdate": 1640995200000, "mdate": 1667617503877, "content": {"title": "Adaptive Fine-Grained Sketch-Based Image Retrieval", "abstract": "The recent focus on Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) has shifted towards generalising a model to new categories without any training data from them. In real-world applications, however, a trained FG-SBIR model is often applied to both new categories and different human sketchers, i.e., different drawing styles. Although this complicates the generalisation problem, fortunately, a handful of examples are typically available, enabling the model to adapt to the new category/style. In this paper, we offer a novel perspective \u2013 instead of asking for a model that generalises, we advocate for one that quickly adapts, with just very few samples during testing (in a few-shot manner). To solve this new problem, we introduce a novel model-agnostic meta-learning (MAML) based framework with several key modifications: (1) As a retrieval task with a margin-based contrastive loss, we simplify the MAML training in the inner loop to make it more stable and tractable. (2) The margin in our contrastive loss is also meta-learned with the rest of the model. (3) Three additional regularisation losses are introduced in the outer loop, to make the meta-learned FG-SBIR model more effective for category/style adaptation. Extensive experiments on public datasets suggest a large gain over generalisation and zero-shot based approaches, and a few strong few-shot baselines."}}
{"id": "jCccYQdouG2", "cdate": 1640995200000, "mdate": 1667617503884, "content": {"title": "Sketching without Worrying: Noise-Tolerant Sketch-Based Image Retrieval", "abstract": "Sketching enables many exciting applications, notably, image retrieval. The fear-to-sketch problem (i.e., \"I can't sketch\") has however proven to be fatal for its widespread adoption. This paper tackles this \"fear\" head on, and for the first time, proposes an auxiliary module for existing retrieval models that predominantly lets the users sketch without having to worry. We first conducted a pilot study that revealed the secret lies in the existence of noisy strokes, but not so much of the \"I can't sketch\". We consequently design a stroke subset selector that {detects noisy strokes, leaving only those} which make a positive contribution towards successful retrieval. Our Reinforcement Learning based formulation quantifies the importance of each stroke present in a given subset, based on the extent to which that stroke contributes to retrieval. When combined with pre-trained retrieval models as a pre-processing module, we achieve a significant gain of 8%-10% over standard baselines and in turn report new state-of-the-art performance. Last but not least, we demonstrate the selector once trained, can also be used in a plug-and-play manner to empower various sketch applications in ways that were not previously possible."}}
{"id": "hbDNkvNaebJ", "cdate": 1640995200000, "mdate": 1667617503884, "content": {"title": "Sketch3T: Test-Time Training for Zero-Shot SBIR", "abstract": "Zero-shot sketch-based image retrieval typically asks for a trained model to be applied as is to unseen categories. In this paper, we question to argue that this setup by definition is not compatible with the inherent abstract and subjective nature of sketches, i.e., the model might transfer well to new categories, but will not understand sketches existing in different test-time distribution as a result. We thus extend ZS-SBIR asking it to transfer to both categories and sketch distributions. Our key contribution is a test-time training paradigm that can adapt using just one sketch. Since there is no paired photo, we make use of a sketch raster-vector reconstruction module as a self-supervised auxiliary task. To maintain the fidelity of the trained cross-modal joint embedding during test-time update, we design a novel meta-learning based training paradigm to learn a separation between model updates incurred by this auxiliary task from those off the primary objective of discriminative learning. Extensive experiments show our model to outperform state of-the-arts, thanks to the proposed test-time adaption that not only transfers to new categories but also accommodates to new sketching styles."}}
{"id": "_s2XG3IISg", "cdate": 1640995200000, "mdate": 1667617503884, "content": {"title": "SceneTrilogy: On Scene Sketches and its Relationship with Text and Photo", "abstract": "We for the first time extend multi-modal scene understanding to include that of free-hand scene sketches. This uniquely results in a trilogy of scene data modalities (sketch, text, and photo), where each offers unique perspectives for scene understanding, and together enable a series of novel scene-specific applications across discriminative (retrieval) and generative (captioning) tasks. Our key objective is to learn a common three-way embedding space that enables many-to-many modality interactions (e.g, sketch+text $\\rightarrow$ photo retrieval). We importantly leverage the information bottleneck theory to achieve this goal, where we (i) decouple intra-modality information by minimising the mutual information between modality-specific and modality-agnostic components via a conditional invertible neural network, and (ii) align \\textit{cross-modalities information} by maximising the mutual information between their modality-agnostic components using InfoNCE, with a specific multihead attention mechanism to allow many-to-many modality interactions. We spell out a few insights on the complementarity of each modality for scene understanding, and study for the first time a series of scene-specific applications like joint sketch- and text-based image retrieval, sketch captioning."}}
{"id": "TjbhFt8xnr", "cdate": 1640995200000, "mdate": 1667617503673, "content": {"title": "A new deep model for family and non-family photo identification", "abstract": "Human trafficking is a global issue of the world and the problems related to human trafficking remain unsolved. This paper presents a new method for the identification of photos of different types of families and non-families such that the method can assist investigation team to find a solution to such issue. We believe that parts of human beings are the main resources for representing family and non-family photos. Based on this intuition, we propose to segment hair, head, cloth, torso, and skin regions from each human in input photos by exploring a self-correlation for human parsing method. This step results in region of interest (ROI). Motivated by ability of deep learning models in solving complex issues and special property of MobileNet, which is light weight model, we further explore MobileNetv2 for the identification of photos of different families and non-families by considering ROI as the input. For the experiment of this work, we consider a dataset of ten classes, which include five family classes, namely, Couple, Nuclear Family, Multi-Cultural Family, Father\u2013Child, Mother\u2013Child and five more non-family classes, namely, Male Friends, Female Friends, Mixed Friends, Male Celebrity, Female Celebrity. The results of the proposed method are demonstrated by testing on our dataset of family and non-family photos classification. Comparative results with the existing methods show that our proposed method outperforms existing methods in terms of classification rate and F-Score."}}
{"id": "Q0GxXTQbJw", "cdate": 1640995200000, "mdate": 1667617503884, "content": {"title": "Adaptive Fine-Grained Sketch-Based Image Retrieval", "abstract": "The recent focus on Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) has shifted towards generalising a model to new categories without any training data from them. In real-world applications, however, a trained FG-SBIR model is often applied to both new categories and different human sketchers, i.e., different drawing styles. Although this complicates the generalisation problem, fortunately, a handful of examples are typically available, enabling the model to adapt to the new category/style. In this paper, we offer a novel perspective -- instead of asking for a model that generalises, we advocate for one that quickly adapts, with just very few samples during testing (in a few-shot manner). To solve this new problem, we introduce a novel model-agnostic meta-learning (MAML) based framework with several key modifications: (1) As a retrieval task with a margin-based contrastive loss, we simplify the MAML training in the inner loop to make it more stable and tractable. (2) The margin in our contrastive loss is also meta-learned with the rest of the model. (3) Three additional regularisation losses are introduced in the outer loop, to make the meta-learned FG-SBIR model more effective for category/style adaptation. Extensive experiments on public datasets suggest a large gain over generalisation and zero-shot based approaches, and a few strong few-shot baselines."}}
{"id": "PHfQ5ZX19DW", "cdate": 1640995200000, "mdate": 1667617503875, "content": {"title": "An Episodic Learning Network for Text Detection on Human Bodies in Sports Images", "abstract": "Due to the proliferation of sports-related multimedia content on the WWW, effective visual search and retrieval present interesting research challenges. These are caused by poor image quality, a wide range of possible camera points of view, pose variations on the part of athletes engaged in playing a sport, deformations of text appearing on sports person\u2019s clothing and uniforms in motion, occlusions caused by other objects, etc. To address these challenges, this paper presents a new method for detecting text on human bodies in sports images. Unlike most existing methods, which attempt to exploit locations of a player\u2019s torso, face, and skin, we propose an end-to-end episodic learning approach that employs inductive learning criteria for detecting clothing regions in an image, which are, in turn, then used for text detection. Our method integrates a Residual Network (ResNet) and Pyramidal Pooling Module (PPM) for generating a spatial attention map. The Progressive Scalable Expansion Algorithm (PSE) is adapted for text detection from these regions. Experimental results on our own dataset as well as several benchmarks (like RBNR and MMM which contain images of runners in marathons, and Re-ID which is a person re-identification dataset) demonstrate that the proposed method outperforms existing methods in terms of precision and F1-score. We also present results for sports images chosen from natural scene text detection datasets such as CTW1500 and MS-COCO to show the proposed method is effective and reliable across a range of inputs."}}
