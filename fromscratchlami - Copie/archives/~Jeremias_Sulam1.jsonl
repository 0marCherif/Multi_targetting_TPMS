{"id": "GScUpOFjoT", "cdate": 1697914903718, "mdate": 1697914903718, "content": {"title": "Fast Hierarchical Games for Image Explanations", "abstract": "As modern complex neural networks keep breaking records and solving harder problems, their predictions also become less and less intelligible. The current lack of interpretability often undermines the deployment of accurate machine learning tools in sensitive settings. In this work, we present a model-agnostic explanation method for image classification based on a hierarchical extension of Shapley coefficients\u2013 Hierarchical Shap (h-Shap) \u2013that resolves some of the limitations of current approaches. Unlike other Shapley-based explanation methods, h-Shap is scalable and can be computed without the need of approximation. Under certain distributional assumptions, such as those common in multiple instance learning, h-Shap retrieves the exact Shapley coefficients with an exponential improvement in computational complexity. We compare our hierarchical approach with popular Shapley-based and non-Shapley-based methods on a synthetic dataset, a medical imaging scenario, and a general computer vision problem, showing that h-Shap outperforms the state-of-the-art in both accuracy and runtime. Code and experiments are made publicly available."}}
{"id": "b_9O9n_D5j", "cdate": 1693969641277, "mdate": 1693969641277, "content": {"title": "Estimating and Controlling for Equalized Odds via Sensitive Attribute Predictors", "abstract": "As the use of machine learning models in real world high-stakes decision settings continues to grow, it is highly important that we are able to audit and control for any potential fairness violations these models may exhibit towards certain groups. To do so, one naturally requires access to sensitive attributes, such as demographics, gender, or other potentially sensitive features that determine group membership. Unfortunately, in many settings, this information is often unavailable. In this work we study the well known equalized odds (EOD) definition of fairness. In a setting without sensitive attributes, we first provide tight and computable upper bounds for the EOD violation of a predictor. These bounds precisely reflect the worst possible EOD violation. Second, we demonstrate how one can provably control the worst-case EOD by a new post-processing correction method. Our results characterize when directly controlling for EOD with respect to the predicted sensitive attributes is -- and when is not -- optimal when it comes to controlling worst-case EOD. Our results hold under assumptions that are milder than previous works, and we illustrate these results with experiments on synthetic and real datasets."}}
{"id": "25Xf-EwPhB", "cdate": 1683883263924, "mdate": 1683883263924, "content": {"title": "Learning to solve TV regularised problems with unrolled algorithms", "abstract": "Total Variation (TV) is a popular regularization strategy that promotes piece-wise constant signals by constraining the L1-norm of the first order derivative of the estimated signal. The resulting optimization problem is usually solved using iterative algorithms such as proximal gradient descent, primal-dual algorithms or ADMM. However, such methods can require a very large number of iterations to converge to a suitable solution. In this paper, we accelerate such iterative algorithms by unfolding proximal gradient descent solvers in order to learn their parameters for 1D TV regularized problems. While this could be done using the synthesis formulation, we demonstrate that this leads to slower performances. The main difficulty in applying such methods in the analysis formulation lies in proposing a way to compute the derivatives through the proximal operator. As our main contribution, we develop and characterize two approaches to do so, describe their benefits and limitations, and discuss the regime where they can actually improve over iterative procedures. We validate those findings with experiments on synthetic and real data."}}
{"id": "KRODJAa6pzE", "cdate": 1621629873880, "mdate": null, "content": {"title": "A Geometric Analysis of Neural Collapse with Unconstrained Features", "abstract": "We provide the first global optimization landscape analysis of Neural Collapse -- an intriguing empirical phenomenon that arises in the last-layer classifiers and features of neural networks during the terminal phase of training. As recently reported by Papyan et al., this phenomenon implies that (i) the class means and the last-layer classifiers all collapse to the vertices of a Simplex Equiangular Tight Frame (ETF) up to scaling, and (ii) cross-example within-class variability of last-layer activations collapses to zero. We study the problem based on a simplified unconstrained feature model, which isolates the topmost layers from the classifier of the neural network. In this context, we show that the classical cross-entropy loss with weight decay has a benign global landscape, in the sense that the only global minimizers are the Simplex ETFs while all other critical points are strict saddles whose Hessian exhibit negative curvature directions. Our analysis of the simplified model not only explains what kind of features are learned in the last layer, but also shows why they can be efficiently optimized, matching the empirical observations in practical deep network architectures. These findings provide important practical implications. As an example, our experiments demonstrate that one may set the feature dimension equal to the number of classes and fix the last-layer classifier to be a Simplex ETF for network training, which reduces memory cost by over 20% on ResNet18 without sacrificing the generalization performance. The source code is available at https://github.com/tding1/Neural-Collapse."}}
{"id": "NuWMKPFPd5x", "cdate": 1621254635269, "mdate": null, "content": {"title": "Learning to solve TV regularised problems with unrolled algorithms", "abstract": "Total Variation (TV) is a popular regularization strategy that promotes piece-wise constant signals by constraining the \u21131-norm of the first order derivative of the estimated signal. The resulting optimization problem is usually solved using iterative algorithms such as proximal gradient descent, primal-dual algorithms or ADMM. However, such methods can require a very large number of iterations to converge to a suitable solution. In this paper, we accelerate such iterative algorithms by unfolding proximal gradient descent solvers in order to learn their parameters for 1D TV regularized problems. While this could be done using the synthesis formulation, we demonstrate that this leads to slower performances. The main difficulty in applying such methods in the analysis formulation lies in proposing a way to compute the derivatives through the proximal operator. As our main contribution, we develop and characterize two approaches to do so, describe their benefits and limitations, and discuss the regime where they can actually improve over iterative procedures. We validate those findings with experiments on synthetic and real data."}}
{"id": "cUF2dHcUou", "cdate": 1621254513943, "mdate": null, "content": {"title": "Adversarial Robustness of Supervised Sparse Coding", "abstract": "Several recent results provide theoretical insights into the phenomena of adversarial examples. Existing results, however, are often limited due to a gap between the simplicity of the models studied and the complexity of those deployed in practice. In this work, we strike a better balance by considering a model that involves learning a representation while at the same time giving a precise generalization bound and a robustness certificate. We focus on the hypothesis class obtained by combining a sparsity-promoting encoder coupled with a linear classifier, and show an interesting interplay between the expressivity and stability of the (supervised) representation map and a notion of margin in the feature space. We bound the robust risk (to \u21132-bounded perturbations) of hypotheses parameterized by dictionaries that achieve a mild encoder gap on training data. Furthermore, we provide a robustness certificate for end-to-end classification. We demonstrate the applicability of our analysis by computing certified accuracy on real data, and compare with other alternatives for certified robustness."}}
{"id": "aSCa60ciI53", "cdate": 1621254413632, "mdate": null, "content": {"title": "On Multi-Layer Basis Pursuit, Efficient Algorithms and Convolutional Neural Networks", "abstract": "Parsimonious representations are ubiquitous in modeling and processing information. Motivated by the recent Multi-Layer\nConvolutional Sparse Coding (ML-CSC) model, we herein generalize the traditional Basis Pursuit problem to a multi-layer setting,\nintroducing similar sparse enforcing penalties at different representation layers in a symbiotic relation between synthesis and analysis\nsparse priors. We explore different iterative methods to solve this new problem in practice, and we propose a new Multi-Layer Iterative\nSoft Thresholding Algorithm (ML-ISTA), as well as a fast version (ML-FISTA). We show that these nested first order algorithms\nconverge, in the sense that the function value of near-fixed points can get arbitrarily close to the solution of the original problem.\nWe further show how these algorithms effectively implement particular recurrent convolutional neural networks (CNNs) that generalize\nfeed-forward ones without introducing any parameters. We present and analyze different architectures resulting unfolding the iterations\nof the proposed pursuit algorithms, including a new Learned ML-ISTA, providing a principled way to construct deep recurrent CNNs.\nUnlike other similar constructions, these architectures unfold a global pursuit holistically for the entire network. We demonstrate the\nemerging constructions in a supervised learning setting, consistently improving the performance of classical CNNs while maintaining\nthe number of parameters constant."}}
{"id": "-d8syCixKEH", "cdate": 1621254350394, "mdate": null, "content": {"title": "Expected patch log likelihood with a sparse prior", "abstract": "Image priors are of great importance in image restoration tasks. These problems can be addressed by decomposing the degraded image into overlapping patches, treating the patches individually and averaging them back together. Recently, the Expected Patch Log Likelihood (EPLL) method has been introduced, arguing that the chosen model should be enforced on the final reconstructed image patches. In the context of a Gaussian Mixture Model (GMM), this idea has been shown to lead to state-of-the-art results in image denoising and debluring. In this paper we combine the EPLL with a sparse-representation prior. Our derivation leads to a close yet extended variant of the popular K-SVD image denoising algorithm, where in order to effectively maximize the EPLL the denoising process should be iterated. This concept lies at the core of the K-SVD formulation, but has not been addressed before due the need to set different denoising thresholds in the successive sparse coding stages. We present a method that intrinsically determines these thresholds in order to improve the image estimate. Our results show a notable improvement over K-SVD in image denoising and inpainting, achieving comparable performance to that of EPLL with GMM in denoising."}}
{"id": "oqmFMXQoeoN", "cdate": 1621254286722, "mdate": null, "content": {"title": "Multi-Layer Convolutional Sparse Modeling: Pursuit and Dictionary Learning", "abstract": "The recently proposed Multi-Layer Convolutional\nSparse Coding (ML-CSC) model, consisting of a cascade of\nconvolutional sparse layers, provides a new interpretation of\nConvolutional Neural Networks (CNNs). Under this framework,\nthe forward pass in a CNN is equivalent to a pursuit algorithm\naiming to estimate the nested sparse representation vectors from a\ngiven input signal. Despite having served as a pivotal connection\nbetween CNNs and sparse modeling, a deeper understanding\nof the ML-CSC is still lacking. In this work, we propose a\nsound pursuit algorithm for the ML-CSC model by adopting\na projection approach. We provide new and improved bounds\non the stability of the solution of such pursuit and we analyze\ndifferent practical alternatives to implement this in practice.\nWe show that the training of the filters is essential to allow\nfor non-trivial signals in the model, and we derive an online\nalgorithm to learn the dictionaries from real data, effectively\nresulting in cascaded sparse convolutional layers. Last, but not\nleast, we demonstrate the applicability of the ML-CSC model\nfor several applications in an unsupervised setting, providing\ncompetitive results. Our work represents a bridge between\nmatrix factorization, sparse dictionary learning and sparse autoencoders, and we analyze these connections in detail."}}
{"id": "ao6LhqGV4fa", "cdate": 1621254217690, "mdate": null, "content": {"title": "Working locally thinking globally: Theoretical guarantees for convolutional sparse coding", "abstract": "The celebrated sparse representation model has led\nto remarkable results in various signal processing tasks in the\nlast decade. However, despite its initial purpose of serving as\na global prior for entire signals, it has been commonly used\nfor modeling low dimensional patches due to the computational\nconstraints it entails when deployed with learned dictionaries. A\nway around this problem has been recently proposed, adopting\na convolutional sparse representation model. This approach\nassumes that the global dictionary is a concatenation of banded\nCirculant matrices. While several works have presented algorithmic solutions to the global pursuit problem under this new model,\nvery few truly-effective guarantees are known for the success of\nsuch methods. In this work, we address the theoretical aspects of\nthe convolutional sparse model providing the first meaningful\nanswers to questions of uniqueness of solutions and success\nof pursuit algorithms, both greedy and convex relaxations, in\nideal and noisy regimes. To this end, we generalize mathematical\nquantities, such as the `0 norm, mutual coherence, Spark and RIP\nto their counterparts in the convolutional setting, intrinsically\ncapturing local measures of the global model. On the algorithmic\nside, we demonstrate how to solve the global pursuit problem by\nusing simple local processing, thus offering a first of its kind\nbridge between global modeling of signals and their patch-based\nlocal treatment."}}
