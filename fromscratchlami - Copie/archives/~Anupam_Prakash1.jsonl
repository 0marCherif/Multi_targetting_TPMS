{"id": "aDxm5J91KA", "cdate": 1640995200000, "mdate": 1682357681988, "content": {"title": "Quantum machine learning with subspace states", "abstract": "We introduce a new approach for quantum linear algebra based on quantum subspace states and present three new quantum machine learning algorithms. The first is a quantum determinant sampling algorithm that samples from the distribution $\\Pr[S]= det(X_{S}X_{S}^{T})$ for $|S|=d$ using $O(nd)$ gates and with circuit depth $O(d\\log n)$. The state of art classical algorithm for the task requires $O(d^{3})$ operations \\cite{derezinski2019minimax}. The second is a quantum singular value estimation algorithm for compound matrices $\\mathcal{A}^{k}$, the speedup for this algorithm is potentially exponential. It decomposes a $\\binom{n}{k}$ dimensional vector of order-$k$ correlations into a linear combination of subspace states corresponding to $k$-tuples of singular vectors of $A$. The third algorithm reduces exponentially the depth of circuits used in quantum topological data analysis from $O(n)$ to $O(\\log n)$. Our basic tool are quantum subspace states, defined as $|Col(X)\\rangle = \\sum_{S\\subset [n], |S|=d} det(X_{S}) |S\\rangle$ for matrices $X \\in \\mathbb{R}^{n \\times d}$ such that $X^{T} X = I_{d}$, that encode $d$-dimensional subspaces of $\\mathbb{R}^{n}$. We develop two efficient state preparation techniques, the first using Givens circuits uses the representation of a subspace as a sequence of Givens rotations, while the second uses efficient implementations of unitaries $\\Gamma(x) = \\sum_{i} x_{i} Z^{\\otimes (i-1)} \\otimes X \\otimes I^{n-i}$ with $O(\\log n)$ depth circuits that we term Clifford loaders."}}
{"id": "JIHdaLXN5RF", "cdate": 1640995200000, "mdate": 1682357682144, "content": {"title": "Low depth algorithms for quantum amplitude estimation", "abstract": "Tudor Giurgica-Tiron, Iordanis Kerenidis, Farrokh Labib, Anupam Prakash, and William Zeng, Quantum 6, 745 (2022). We design and analyze two new low depth algorithms for amplitude estimation (AE) achieving an optimal tradeoff between the quantum speedup and circuit depth. For $\\beta \\in (0,1]$, our algorit\u2026"}}
{"id": "2E1-jvHDBjX", "cdate": 1640995200000, "mdate": 1682357682134, "content": {"title": "Quantum Reinforcement Learning via Policy Iteration", "abstract": "Quantum computing has shown the potential to substantially speed up machine learning applications, in particular for supervised and unsupervised learning. Reinforcement learning, on the other hand, has become essential for solving many decision making problems and policy iteration methods remain the foundation of such approaches. In this paper, we provide a general framework for performing quantum reinforcement learning via policy iteration. We validate our framework by designing and analyzing: \\emph{quantum policy evaluation} methods for infinite horizon discounted problems by building quantum states that approximately encode the value function of a policy $\\pi$; and \\emph{quantum policy improvement} methods by post-processing measurement outcomes on these quantum states. Last, we study the theoretical and experimental performance of our quantum algorithms on two environments from OpenAI's Gym."}}
{"id": "dnVlDfcT-5R", "cdate": 1609459200000, "mdate": 1682357682105, "content": {"title": "Quantum algorithms for Second-Order Cone Programming and Support Vector Machines", "abstract": "We present a quantum interior-point method (IPM) for second-order cone programming (SOCP) that runs in time $\\widetilde{O} \\left( n\\sqrt{r} \\frac{\\zeta \\kappa}{\\delta^2} \\log \\left(1/\\epsilon\\right) \\right)$ where $r$ is the rank and $n$ the dimension of the SOCP, $\\delta$ bounds the distance of intermediate solutions from the cone boundary, $\\zeta$ is a parameter upper bounded by $\\sqrt{n}$, and $\\kappa$ is an upper bound on the condition number of matrices arising in the classical IPM for SOCP. The algorithm takes as its input a suitable quantum description of an arbitrary SOCP and outputs a classical description of a $\\delta$-approximate $\\epsilon$-optimal solution of the given problem. Furthermore, we perform numerical simulations to determine the values of the aforementioned parameters when solving the SOCP up to a fixed precision $\\epsilon$. We present experimental evidence that in this case our quantum algorithm exhibits a polynomial speedup over the best classical algorithms for solving general SOCPs that run in time $O(n^{\\omega+0.5})$ (here, $\\omega$ is the matrix multiplication exponent, with a value of roughly $2.37$ in theory, and up to $3$ in practice). For the case of random SVM (support vector machine) instances of size $O(n)$, the quantum algorithm scales as $O(n^k)$, where the exponent $k$ is estimated to be $2.59$ using a least-squares power law. On the same family random instances, the estimated scaling exponent for an external SOCP solver is $3.31$ while that for a state-of-the-art SVM solver is $3.11$."}}
{"id": "lMMZU8_IwL7", "cdate": 1577836800000, "mdate": 1682357682155, "content": {"title": "Quantum Expectation-Maximization for Gaussian mixture models", "abstract": "We define a quantum version of Expectation-Maximization (QEM), a fundamental tool in unsupervised machine learning, often used to solve Maximum Likelihood (ML) and Maximum A Posteriori (MAP) estima..."}}
{"id": "dTjPC22qA1K", "cdate": 1577836800000, "mdate": 1682357682090, "content": {"title": "Quantum Algorithms for Deep Convolutional Neural Networks", "abstract": "Quantum computing is a powerful computational paradigm with applications in several fields, including machine learning. In the last decade, deep learning, and in particular Convolutional Neural Networks (CNN), have become essential for applications in signal processing and image recognition. Quantum deep learning, however, remains a challenging problem, as it is difficult to implement non linearities with quantum unitaries. In this paper we propose a quantum algorithm for evaluating and training deep convolutional neural networks with potential speedups over classical CNNs for both the forward and backward passes. The quantum CNN (QCNN) reproduces completely the outputs of the classical CNN and allows for non linearities and pooling operations. The QCNN is in particular interesting for deep networks and could allow new frontiers in the image recognition domain, by allowing for many more convolution kernels, larger kernels, high dimensional inputs and high depth input channels. We also present numerical simulations for the classification of the MNIST dataset to provide practical evidence for the efficiency of the QCNN."}}
{"id": "Hygab1rKDS", "cdate": 1569439492980, "mdate": null, "content": {"title": "Quantum Algorithms for Deep Convolutional Neural Networks", "abstract": "Quantum computing is a powerful computational paradigm with applications in several fields, including machine learning. In the last decade, deep learning, and in particular Convolutional Neural Networks (CNN), have become essential for applications in signal processing and image recognition. Quantum deep learning, however, remains a challenging problem, as it is difficult to implement non linearities with quantum unitaries. In this paper we propose a quantum algorithm for evaluating and training deep convolutional neural networks with potential speedups over classical CNNs for both the forward and backward passes. The quantum CNN (QCNN) reproduces completely the outputs of the classical CNN and allows for non linearities and pooling operations. The QCNN is in particular interesting for deep networks and could allow new frontiers in the image recognition domain, by allowing for many more convolution kernels, larger kernels, high dimensional inputs and high depth input channels. We also present numerical simulations for the classification of the MNIST dataset to provide practical evidence for the efficiency of the QCNN."}}
{"id": "Hkgs3aNYDS", "cdate": 1569439154743, "mdate": null, "content": {"title": "Quantum Expectation-Maximization for Gaussian Mixture Models", "abstract": "The Expectation-Maximization (EM) algorithm is a fundamental tool in unsupervised machine learning. It is often used as an efficient way to solve Maximum Likelihood (ML) and Maximum A Posteriori estimation problems, especially for models with latent variables. It is also the algorithm of choice to fit mixture models: generative models that represent unlabelled points originating from $k$ different processes, as samples from $k$ multivariate distributions. In this work we define and use a quantum version of EM to fit a Gaussian Mixture Model. Given quantum access to a dataset of $n$ vectors of dimension $d$, our algorithm has convergence and precision guarantees similar to the classical algorithm, but the runtime is only polylogarithmic in the number of elements in the training set, and is polynomial in other parameters - as the dimension of the feature space, and the number of components in the mixture. We generalize further the algorithm by fitting any mixture model of base distributions in the exponential family. We discuss the performance of the algorithm on datasets that are expected to be classified successfully by those algorithms, arguing that on those cases we can give strong guarantees on the runtime."}}
{"id": "VuSvN2asFk", "cdate": 1546300800000, "mdate": 1682357682131, "content": {"title": "Quantum Algorithms for Portfolio Optimization", "abstract": "We develop the first quantum algorithm for the constrained portfolio optimization problem. The algorithm has running time \u00d5 (n\u221ar \u03b6k/\u03b42 log (1/\u03f5)), where r is the number of positivity and budget constraints, n is the number of assets in the portfolio, \u03f5 the desired precision, and \u03b4, \u03ba, \u03b6 are problem-dependent parameters related to the well-conditioning of the intermediate solutions. If only a moderately accurate solution is required, our quantum algorithm can achieve a polynomial speedup over the best classical algorithms with complexity \u00d5 (\u221arn\u03c9 log(1/\u03f5)), where \u03c9 is the matrix multiplication exponent that has a theoretical value of around 2.373, but is closer to 3 in practice. We also provide some experiments to bound the problem-dependent factors arising in the running time of the quantum algorithm, and these experiments suggest that for most instances the quantum algorithm can potentially achieve an O(n) speedup over its classical counterpart."}}
{"id": "wrpFguYcHau", "cdate": 1514764800000, "mdate": 1682357682129, "content": {"title": "A Quantum Interior Point Method for LPs and SDPs", "abstract": "We present a quantum interior point method with worst case running time $\\widetilde{O}(\\frac{n^{2.5}}{\\xi^{2}} \\mu \\kappa^3 \\log (1/\\epsilon))$ for SDPs and $\\widetilde{O}(\\frac{n^{1.5}}{\\xi^{2}} \\mu \\kappa^3 \\log (1/\\epsilon))$ for LPs, where the output of our algorithm is a pair of matrices $(S,Y)$ that are $\\epsilon$-optimal $\\xi$-approximate SDP solutions. The factor $\\mu$ is at most $\\sqrt{2}n$ for SDPs and $\\sqrt{2n}$ for LP's, and $\\kappa$ is an upper bound on the condition number of the intermediate solution matrices. For the case where the intermediate matrices for the interior point method are well conditioned, our method provides a polynomial speedup over the best known classical SDP solvers and interior point based LP solvers, which have a worst case running time of $O(n^{6})$ and $O(n^{3.5})$ respectively. Our results build upon recently developed techniques for quantum linear algebra and pave the way for the development of quantum algorithms for a variety of applications in optimization and machine learning."}}
