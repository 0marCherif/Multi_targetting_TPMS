{"id": "1i-zpBqRmQC", "cdate": 1686168172761, "mdate": 1686168172761, "content": {"title": "Do Differentiable Simulators Give Better Policy Gradients?", "abstract": "Differentiable simulators promise faster computation time for reinforcement learning by replacing zeroth-order gradient estimates of a stochastic objective with an estimate based on first-order gradients. However, it is yet unclear what factors decide the performance of the two estimators on complex landscapes that involve long-horizon planning and control on physical systems, despite the crucial relevance of this question for the utility of differentiable simulators. We show that characteristics of certain physical systems, such as stiffness or discontinuities, may compromise the efficacy of the first-order estimator, and analyze this phenomenon through the lens of bias and variance. We additionally propose an alpha-order gradient estimator, with alpha between 0 and 1, which correctly utilizes exact gradients to combine the efficiency of first-order estimates with the robustness of zero-order methods. We demonstrate the pitfalls of traditional estimators and the advantages of the -order estimator on some numerical examples."}}
{"id": "qjSK0DZAjLq", "cdate": 1672531200000, "mdate": 1681747108995, "content": {"title": "Statistical Learning under Heterogenous Distribution Shift", "abstract": "This paper studies the prediction of a target $\\mathbf{z}$ from a pair of random variables $(\\mathbf{x},\\mathbf{y})$, where the ground-truth predictor is additive $\\mathbb{E}[\\mathbf{z} \\mid \\mathbf{x},\\mathbf{y}] = f_\\star(\\mathbf{x}) +g_{\\star}(\\mathbf{y})$. We study the performance of empirical risk minimization (ERM) over functions $f+g$, $f \\in \\mathcal{F}$ and $g \\in \\mathcal{G}$, fit on a given training distribution, but evaluated on a test distribution which exhibits covariate shift. We show that, when the class $\\mathcal{F}$ is \"simpler\" than $\\mathcal{G}$ (measured, e.g., in terms of its metric entropy), our predictor is more resilient to \\emph{heterogenous covariate shifts} in which the shift in $\\mathbf{x}$ is much greater than that in $\\mathbf{y}$. These results rely on a novel H\\\"older style inequality for the Dudley integral which may be of independent interest. Moreover, we corroborate our theoretical findings with experiments demonstrating improved resilience to shifts in \"simpler\" features across numerous domains."}}
{"id": "Pn1n4K2MGDp", "cdate": 1672531200000, "mdate": 1681747108933, "content": {"title": "Smoothed Online Learning for Prediction in Piecewise Affine Systems", "abstract": "The problem of piecewise affine (PWA) regression and planning is of foundational importance to the study of online learning, control, and robotics, where it provides a theoretically and empirically tractable setting to study systems undergoing sharp changes in the dynamics. Unfortunately, due to the discontinuities that arise when crossing into different ``pieces,'' learning in general sequential settings is impossible and practical algorithms are forced to resort to heuristic approaches. This paper builds on the recently developed smoothed online learning framework and provides the first algorithms for prediction and simulation in PWA systems whose regret is polynomial in all relevant problem parameters under a weak smoothness assumption; moreover, our algorithms are efficient in the number of calls to an optimization oracle. We further apply our results to the problems of one-step prediction and multi-step simulation regret in piecewise affine dynamical systems, where the learner is tasked with simulating trajectories and regret is measured in terms of the Wasserstein distance between simulated and true data. Along the way, we develop several technical tools of more general interest."}}
{"id": "3tt44Nowk3", "cdate": 1672531200000, "mdate": 1681747109010, "content": {"title": "Oracle-Efficient Smoothed Online Learning for Piecewise Continuous Decision Making", "abstract": "Smoothed online learning has emerged as a popular framework to mitigate the substantial loss in statistical and computational complexity that arises when one moves from classical to adversarial learning. Unfortunately, for some spaces, it has been shown that efficient algorithms suffer an exponentially worse regret than that which is minimax optimal, even when the learner has access to an optimization oracle over the space. To mitigate that exponential dependence, this work introduces a new notion of complexity, the generalized bracketing numbers, which marries constraints on the adversary to the size of the space, and shows that an instantiation of Follow-the-Perturbed-Leader can attain low regret with the number of calls to the optimization oracle scaling optimally with respect to average regret. We then instantiate our bounds in several problems of interest, including online prediction and planning of piecewise continuous functions, which has many applications in fields as diverse as econometrics and robotics."}}
{"id": "uHNs4PHY9u", "cdate": 1664928786777, "mdate": null, "content": {"title": "Learning to Extrapolate: A Transductive Approach", "abstract": "Machine learning systems, especially overparameterized deep neural networks, can generalize to novel testing instances drawn from the same distribution as the training data. However, they fare poorly when evaluated on out-of-support testing points. In this work, we tackle the problem of developing machine learning systems that retain the power of overparametrized function approximators, while enabling extrapolation to out-of-support testing points when possible. This is accomplished by noting that under certain conditions, a \"transductive\" reparameterization can convert an out-of-support extrapolation problem into a problem of within-support combinatorial generalization. We propose a simple strategy based on bilinear embeddings to enable this type of combinatorial generalization, thereby addressing the out-of-support extrapolation problem. We instantiate a simple, practical algorithm applicable to various supervised learning problems and imitation learning tasks.  "}}
{"id": "lid14UkLPd4", "cdate": 1663850227366, "mdate": null, "content": {"title": "Learning to Extrapolate: A Transductive Approach", "abstract": "Machine learning systems, especially with overparameterized deep neural networks, can generalize to novel test instances drawn from the same distribution as the training data. However, they fare poorly when evaluated on out-of-support test points. In this work, we tackle the problem of developing machine learning systems that retain the power of overparameterized function approximators while enabling extrapolation to out-of-support test points when possible. This is accomplished by noting that under certain conditions, a \"transductive\" reparameterization can convert an out-of-support extrapolation problem into a problem of within-support combinatorial generalization. We propose a simple strategy based on bilinear embeddings to enable this type of combinatorial generalization, thereby addressing the out-of-support extrapolation problem under certain conditions. We instantiate a simple, practical algorithm applicable to various supervised learning and imitation learning tasks."}}
{"id": "qYc8VnmUwbv", "cdate": 1652737603426, "mdate": null, "content": {"title": "Efficient and Near-Optimal Smoothed Online Learning for Generalized Linear Functions", "abstract": "Due to the drastic gap in complexity between sequential and batch statistical learning, recent work has studied a smoothed  sequential learning setting, where Nature is constrained to select contexts with density bounded by $1/\\sigma$ with respect to a known measure $\\mu$. Unfortunately, for some function classes, there is an exponential gap between the statistically optimal regret and that which can be achieved efficiently.  In this paper, we give a computationally efficient algorithm that is the first to enjoy the statistically optimal $\\log(T/\\sigma)$ regret for realizable $K$-wise linear classification. We extend our results to settings where the true classifier is linear in an over-parameterized polynomial featurization of the contexts, as well as to a realizable piecewise-regression setting assuming access to an appropriate ERM oracle.  Somewhat surprisingly, standard disagreement-based analyses are insufficient to achieve regret logarithmic in $1/\\sigma$.  Instead,  we develop a novel characterization of the geometry of the disagreement region induced by generalized linear classifiers. Along the way, we develop numerous technical tools of independent interest, including a general anti-concentration bound for the determinant of certain  matrix averages."}}
{"id": "WuJfPCoj7pT", "cdate": 1652737489749, "mdate": null, "content": {"title": "Globally Convergent Policy Search for Output Estimation", "abstract": "We introduce the first direct policy search algorithm which provably converges to the globally optimal dynamic filter for the classical problem of predicting the outputs of a linear dynamical system, given noisy, partial observations. Despite the ubiquity of partial observability in practice, theoretical guarantees for direct policy search algorithms, one of the backbones of modern reinforcement learning, have proven difficult to achieve. This is primarily due to the degeneracies which arise when optimizing over filters that maintain an internal state. In this paper, we provide a new perspective on this challenging problem based on the notion of informativity, which intuitively requires that all components of a filter\u2019s internal state are representative of the true state of the underlying dynamical system. We show that informativity overcomes the aforementioned degeneracy. Specifically, we propose a regularizer which explicitly enforces informativity, and establish that gradient descent on this regularized objective - combined with a \u201creconditioning step\u201d \u2013 converges to the globally optimal cost at a $O(1/T)$ rate."}}
{"id": "kMB2WAfisY", "cdate": 1651378406804, "mdate": null, "content": {"title": "Pathologies and Challenges of Using Differentiable Simulators in Policy Optimization for Contact-Rich Manipulation", "abstract": "Policy search methods in Reinforcement Learning (RL) have shown impressive results in contact-rich tasks such as dexterous manipulation. However, the high variance of zero-order Monte-Carlo gradient estimates results in slow convergence and a requirement for a high number of samples. By replacing these zero-order gradient estimates with first-order ones, differentiable simulators promise faster computation time for policy gradient methods when the model is known. Contrary to this belief, we highlight some of the pathologies of using first-order gradients and show that in many physical scenarios involving rich contact, using zero-order gradients result in better performance. Building on these pathologies and lessons, we propose guidelines for designing differentiable simulators, as well as policy optimization algorithms that use these simulators. By doing so, we hope to reap the benefits of first-order gradients while avoiding the potential pitfalls."}}
{"id": "ghy-HDjskQ", "cdate": 1640995200000, "mdate": 1681747109008, "content": {"title": "Do Differentiable Simulators Give Better Policy Gradients?", "abstract": "Differentiable simulators promise faster computation time for reinforcement learning by replacing zeroth-order gradient estimates of a stochastic objective with an estimate based on first-order gra..."}}
