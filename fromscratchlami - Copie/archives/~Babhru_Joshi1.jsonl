{"id": "zvrnxhplqaf", "cdate": 1683889764933, "mdate": 1683889764933, "content": {"title": "A coherence parameter characterizing generative compressed sensing with Fourier measurements", "abstract": "In Bora et al. (2017), a mathematical framework was developed for compressed sensing guarantees in the setting where the measurement matrix is Gaussian and the signal structure is the range of a generative neural network (GNN). The problem of compressed sensing with GNNs has since been extensively analyzed when the measurement matrix and/or network weights follow a subgaussian distribution. We move beyond the subgaussian assumption, to measurement matrices that are derived by sampling uniformly at random rows of a unitary matrix (including subsampled Fourier measurements as a special case). Specifically, we prove the first known restricted isometry guarantee for generative compressed sensing with subsampled isometries and provide recovery bounds, addressing an open problem of Scarlett et al. (2022, p. 10). Recovery efficacy is characterized by the coherence, a new parameter, which measures the interplay between the range of the network and the measurement matrix. Our approach relies on subspace counting arguments and ideas central to high-dimensional probability. Furthermore, we propose a regularization strategy for training GNNs to have favourable coherence with the measurement operator. We provide compelling numerical simulations that support this regularized training strategy: our strategy yields low coherence networks that require fewer measurements for signal recovery. This, together with our theoretical results, supports coherence as a natural quantity for characterizing generative compressed sensing with subsampled isometries.\n"}}
{"id": "2v7XbFVRH3G", "cdate": 1634622668539, "mdate": null, "content": {"title": "PLUGIn-CS: A simple algorithm for compressive sensing with generative prior", "abstract": "We consider the problem of recovering an unknown latent code vector under a known generative model from compressive measurements. For a $d$-layer deep generative network $\\mathcal{G}:\\mathbb{R}^{n_0}\\rightarrow \\mathbb{R}^{n_d}$ with ReLU activation functions and compressive measurement matrix $\\Phi \\in \\mathbb{R}^{m\\times n_d}$, let the observation be $\\Phi\\mathcal{G}(x)+\\epsilon$ where $\\epsilon$ is noise. We introduce a simple novel algorithm, Partially Linearized Update for Generative Inversion in Compressive Sensing (PLUGIn-CS), to estimate $x$ (and thus $\\mathcal{G}(x)$). We prove that, when sensing matrix and weights are Gaussian, if layer widths $n_i \\gtrsim 5^i n_0$ and number of measurements $m \\gtrsim 2^dn_0$ (both up to log factors), then the algorithm converges geometrically to a (small) neighbourhood of $x$ with high probability. Note the inequality on layer widths allows $n_i>n_{i+1}$ when $i\\geq 1$ and thus allows the network to have some contractive layers. After a sufficient number of iterations, the estimation errors for both $x$ and $\\mathcal{G}(x)$ are at most in the order of $\\sqrt{4^dn_0/m} \\|\\epsilon\\|$. Numerical experiments on synthetic data and real data are provided to validate our theoretical results and to illustrate that the algorithm can effectively recover images from compressive measurements."}}
{"id": "rxAS126OC-A", "cdate": 1621630048607, "mdate": null, "content": {"title": "PLUGIn: A simple algorithm for inverting generative models with recovery guarantees", "abstract": "We consider the problem of recovering an unknown latent code vector under a known generative model. For a $d$-layer deep generative network $\\mathcal{G}:\\mathbb{R}^{n_0}\\rightarrow \\mathbb{R}^{n_d}$ with ReLU activation functions, let the observation be $\\mathcal{G}(x)+\\epsilon$ where $\\epsilon$ is noise. We introduce a simple novel algorithm, Partially Linearized Update for Generative Inversion (PLUGIn), to estimate $x$ (and thus $\\mathcal{G}(x)$). We prove that, when weights are Gaussian and layer widths $n_i \\gtrsim 5^i n_0$ (up to log factors), the algorithm converges geometrically to a neighbourhood of $x$ with high probability. Note the inequality on layer widths allows $n_i>n_{i+1}$ when $i\\geq 1$. To our knowledge, this is the first such result for networks with some contractive layers. After a sufficient number of iterations, the estimation errors for both $x$ and $\\mathcal{G}(x)$ are at most in the order of $\\sqrt{4^dn_0/n_d} \\|\\epsilon\\|$. Thus, the algorithm can denoise when the expansion ratio $n_d/n_0$ is large. Numerical experiments on synthetic data and real data are provided to validate our theoretical results and to illustrate that the algorithm can effectively remove artifacts in an image."}}
{"id": "cW30_-ArooV", "cdate": 1616125026547, "mdate": null, "content": {"title": "Global Guarantees for Blind Demodulation with Generative Priors", "abstract": "We study a deep learning inspired formulation for the blind demodulation prob- lem, which is the task of recovering two unknown vectors from their entrywise multiplication. We consider the case where the unknown vectors are in the range of known deep generative models, G(1) : R^n -> R`^l and  G(2) : R^p to R^l. In the case when the networks corresponding to the generative models are expansive, the weight matrices are random and the dimension of the unknown vectors satisfy `l = Omega(n2 + p2), up to log factors, we show that the empirical risk objective has a favorable landscape for optimization. That is, the objective function has a descent direction at every point outside of a small neighborhood around four hyperbolic curves. We also characterize the local maximizers of the empirical risk objective and, hence, show that there does not exist any other stationary points outside of these neighborhood around four hyperbolic curves and the set of local maximizers. We also implement a gradient descent scheme inspired by the geometry of the landscape of the objective function. In order to converge to a global minimizer, this gradient descent scheme exploits the fact that exactly one of the hyperbolic curve corresponds to the global minimizer, and thus points near this hyperbolic curve have a lower objective value than points close to the other spurious hyperbolic curves. We show that this gradient descent scheme can effectively remove distortions synthetically introduced to the MNIST dataset."}}
{"id": "S1VCYPZubB", "cdate": 1514764800000, "mdate": null, "content": {"title": "A convex program for bilinear inversion of sparse vectors", "abstract": "We consider the bilinear inverse problem of recovering two vectors, x in R^L and w in R^L, from their entrywise product. We consider the case where x and w have known signs and are sparse with respect to known dictionaries of size K and N, respectively. Here, K and N may be larger than, smaller than, or equal to L. We introduce L1-BranchHull, which is a convex program posed in the natural parameter space and does not require an approximate solution or initialization in order to be stated or solved. We study the case where x and w are S1- and S2-sparse with respect to a random dictionary, with the sparse vectors satisfying an effective sparsity condition, and present a recovery guarantee that depends on the number of measurements as L &gt; Omega(S1+S2)(log(K+N))^2. Numerical experiments verify that the scaling constant in the theorem is not too large. One application of this problem is the sweep distortion removal task in dielectric imaging, where one of the signals is a nonnegative reflectivity, and the other signal lives in a known subspace, for example that given by dominant wavelet coefficients. We also introduce a variants of L1-BranchHull for the purposes of tolerating noise and outliers, and for the purpose of recovering piecewise constant signals. We provide an ADMM implementation of these variants and show they can extract piecewise constant behavior from real images."}}
