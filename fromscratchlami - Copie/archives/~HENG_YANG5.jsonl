{"id": "x-LZNmyET-V", "cdate": 1672531200000, "mdate": 1691938978076, "content": {"title": "Boosting Text Augmentation via Hybrid Instance Filtering Framework", "abstract": ""}}
{"id": "dZKQA0ruV3", "cdate": 1672531200000, "mdate": 1691938978319, "content": {"title": "Reactive Perturbation Defocusing for Textual Adversarial Defense", "abstract": "Recent studies have shown that large pre-trained language models are vulnerable to adversarial attacks. Existing methods attempt to reconstruct the adversarial examples. However, these methods usually have limited performance in defense against adversarial examples, while also negatively impacting the performance on natural examples. To overcome this problem, we propose a method called Reactive Perturbation Defocusing (RPD). RPD uses an adversarial detector to identify adversarial examples and reduce false defenses on natural examples. Instead of reconstructing the adversaries, RPD injects safe perturbations into adversarial examples to distract the objective models from the malicious perturbations. Our experiments on three datasets, two objective models, and various adversarial attacks show that our proposed framework successfully repairs up to approximately 97% of correctly identified adversarial examples with only about a 2% performance decrease on natural examples. We also provide a demo of adversarial detection and repair based on our work."}}
{"id": "E2y2TrpJhYN", "cdate": 1663850295145, "mdate": null, "content": {"title": "Perturbation Defocusing for Adversarial Defense", "abstract": "Recent research indicates adversarial attacks are likely to deceive neural systems, including large-scale, pre-trained language models. Given a natural sentence, an attacker replaces a subset of words to fool objective models. To defend against adversarial attacks, existing works aim to reconstruct the adversarial examples. However, these methods show limited defense performance on the adversarial examples whilst also damaging the clean performance on natural examples. To achieve better defense performance, our finding indicates that the reconstruction of adversarial examples is not necessary. More specifically, we inject non-toxic perturbations into adversarial examples, which can disable almost all malicious perturbations. In order to minimize performance sacrifice, we employ an adversarial example detector to distinguish and repair detected adversarial examples, which alleviates the mis-defense on natural examples. Our experimental results on three datasets, two objective models and a variety of adversarial attacks show that the proposed method successfully repairs up to \u223c 97% correctly identified adversarial examples with \u2264\u223c 2% performance sacrifice. We provide an anony-mus demonstration of adversarial detection and repair based on our work."}}
{"id": "pxkYSGZBCJL", "cdate": 1640995200000, "mdate": 1691938978314, "content": {"title": "Evolutionary Multi-Task Injection Testing on Web Application Firewalls", "abstract": "Web application firewall (WAF) plays an integral role nowadays to protect web applications from various malicious injection attacks such as SQL injection, XML injection, and PHP injection, to name a few. However, given the evolving sophistication of injection attacks and the increasing complexity of tuning a WAF, it is challenging to ensure that the WAF is free of injection vulnerabilities such that it will block all malicious injection attacks without wrongly affecting the legitimate message. Automatically testing the WAF is, therefore, a timely and essential task. In this paper, we propose DaNuoYi, an automatic injection testing tool that simultaneously generates test inputs for multiple types of injection attacks on a WAF. Our basic idea derives from the cross-lingual translation in the natural language processing domain. In particular, test inputs for different types of injection attacks are syntactically different but may be semantically similar. Sharing semantic knowledge across multiple programming languages can thus stimulate the generation of more sophisticated test inputs and discovering injection vulnerabilities of the WAF that are otherwise difficult to find. To this end, in DaNuoYi, we train several injection translation models by using multi-task learning that translates the test inputs between any pair of injection attacks. The model is then used by a novel multi-task evolutionary algorithm to co-evolve test inputs for different types of injection attacks facilitated by a shared mating pool and domain-specific mutation operators at each generation. We conduct experiments on three real-world open-source WAFs and six types of injection attacks, the results reveal that DaNuoYi generates up to 3.8x and 5.78x more valid test inputs (i.e., bypassing the underlying WAF) than its state-of-the-art single-task counterparts and the context-free grammar-based injection construction."}}
{"id": "VP6HS3lfHH", "cdate": 1640995200000, "mdate": 1691938978319, "content": {"title": "Combining dynamic local context focus and dependency cluster attention for aspect-level sentiment classification", "abstract": ""}}
{"id": "UealCla1kd", "cdate": 1640995200000, "mdate": 1691938978324, "content": {"title": "Learning for target-dependent sentiment based on local context-aware embedding", "abstract": "Target-dependent sentiment classification is a fine-grained task of natural language processing to analyze the sentiment polarity of the targets. In order to address the difficulty of locating important sentiment information of targeted sentiment classification, recent research mostly applies attention mechanisms to capture the information of important context words, while the attention mechanism is subject to many drawbacks, e.g., dependent on network architecture and expensive. Recent studies show the significant effect of the local context focus (LCF) mechanism in capturing the relatedness between a target\u2019s sentiment and its local context. However, the LCF simply applies the fusion of global and local context features to classify sentiment, neglecting to empower the network to be aware of deep information of local context. In this paper, we propose a novel local context-aware network (LCA-Net) based on the local context embedding (LCE). Moreover, accompanied by the sentiment classification loss, the local context prediction (LCP) loss is proposed to enhance the LCE. The experimental results on three commonly used datasets, i.e., the Laptop and Restaurant datasets from SemEval-2014 and a Twitter social dataset, show that all the LCA-Net variants achieve promising performance improvement compared to existing approaches in extracting local context features. Besides, we implement the LCA-Net with different neural networks, validating the transferability of LCA architecture."}}
{"id": "9lfIL9d-ZR", "cdate": 1640995200000, "mdate": 1691938978338, "content": {"title": "PyABSA: Open Framework for Aspect-based Sentiment Analysis", "abstract": "The advancement of aspect-based sentiment analysis (ABSA) has urged the lack of a user-friendly framework that can largely lower the difficulty of reproducing state-of-the-art ABSA performance, especially for beginners. To meet the demand, we present \\our, a modularized framework built on PyTorch for reproducible ABSA. To facilitate ABSA research, PyABSA supports several ABSA subtasks, including aspect term extraction, aspect sentiment classification, and end-to-end aspect-based sentiment analysis. Concretely, PyABSA integrates 29 models and 26 datasets. With just a few lines of code, the result of a model on a specific dataset can be reproduced. With a modularized design, PyABSA can also be flexibly extended to considered models, datasets, and other related tasks. Besides, PyABSA highlights its data augmentation and annotation features, which significantly address data scarcity. All are welcome to have a try at \\url{https://github.com/yangheng95/PyABSA}."}}
{"id": "2eABed5KgH", "cdate": 1640995200000, "mdate": 1691938978319, "content": {"title": "Augmentor or Filter? Reconsider the Role of Pre-trained Language Model in Text Classification Augmentation", "abstract": "Text augmentation is one of the most effective techniques to solve the critical problem of insufficient data in text classification. Existing text augmentation methods achieve hopeful performance in few-shot text data augmentation. However, these methods usually lead to performance degeneration on public datasets due to poor quality augmentation instances. Our study shows that even employing pre-trained language models, existing text augmentation methods generate numerous low-quality instances and lead to the feature space shift problem in augmentation instances. However, we note that the pre-trained language model is good at finding low-quality instances provided that it has been fine-tuned on the target dataset. To alleviate the feature space shift and performance degeneration in existing text augmentation methods, we propose BOOSTAUG, which reconsiders the role of the language model in text augmentation and emphasizes the augmentation instance filtering rather than generation. We evaluate BOOSTAUG on both sentence-level text classification and aspect-based sentiment classification. The experimental results on seven commonly used text classification datasets show that our augmentation method obtains state-of-the-art performance. Moreover, BOOSTAUG is a flexible framework; we release the code which can help improve existing augmentation methods."}}
{"id": "cyCLL-8KlDr", "cdate": 1609459200000, "mdate": 1691938978321, "content": {"title": "A multi-task learning model for Chinese-oriented aspect polarity classification and aspect term extraction", "abstract": ""}}
{"id": "ZY20lbUfVi", "cdate": 1609459200000, "mdate": 1691938978321, "content": {"title": "Back to Reality: Leveraging Pattern-driven Modeling to Enable Affordable Sentiment Dependency Learning", "abstract": "Aspect-based sentiment classification (ABSC) has revealed the potential dependency of sentiment polarities among different aspects. Our study further explores this phenomenon, positing that adjacent aspects often exhibit similar sentiments, a concept we term \"aspect sentiment coherency.\" We argue that the current research landscape has not fully appreciated the significance of modeling aspect sentiment coherency. To address this gap, we introduce a local sentiment aggregation paradigm (LSA) that facilitates fine-grained sentiment coherency modeling. This approach enables the extraction of implicit sentiments for aspects lacking explicit sentiment descriptions. Leveraging gradient descent, we design a differential-weighted sentiment aggregation window that guides the modeling of aspect sentiment coherency. Experimental results affirm the efficacy of LSA in learning sentiment coherency, as it achieves state-of-the-art performance across three public datasets, thus significantly enhancing existing ABSC models. We have made our code available, providing a ready tool for existing methods to harness the potential of sentiment coherency information."}}
