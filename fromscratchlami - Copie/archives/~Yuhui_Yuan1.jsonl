{"id": "9ND8fMUzOAr", "cdate": 1652737368476, "mdate": null, "content": {"title": "Expediting Large-Scale Vision Transformer for Dense Prediction without Fine-tuning", "abstract": "Vision transformers have recently achieved competitive results across various vision tasks but still suffer from heavy computation costs when processing a large number of tokens. Many advanced approaches have been developed to reduce the total number of tokens in the large-scale vision transformers, especially for image classification tasks. Typically, they select a small group of essential tokens according to their relevance with the [\\texttt{class}] token, then fine-tune the weights of the vision transformer. Such fine-tuning is less practical for dense prediction due to the much heavier computation and GPU memory cost than image classification.\n\nIn this paper, we focus on a more challenging problem, \\ie, accelerating large-scale vision transformers for dense prediction without any additional re-training or fine-tuning. In response to the fact that high-resolution representations are necessary for dense prediction, we present two non-parametric operators, a \\emph{token clustering layer} to decrease the number of tokens and a \\emph{token reconstruction layer} to increase the number of tokens. The following steps are performed to achieve this: (i) we use the token clustering layer to cluster the neighboring tokens together, resulting in low-resolution representations that maintain the spatial structures; (ii) we apply the following transformer layers only to these low-resolution representations or clustered tokens; and (iii) we use the token reconstruction layer to re-create the high-resolution representations from the refined low-resolution representations. The results obtained by our method are promising on five dense prediction tasks including object detection, semantic segmentation, panoptic segmentation, instance segmentation, and depth estimation. Accordingly, our method accelerates $40\\%\\uparrow$ FPS and saves $30\\%\\downarrow$ GFLOPs of ``Segmenter+ViT-L/$16$'' while maintaining $99.5\\%$ of the performance on ADE$20$K without fine-tuning the official weights."}}
{"id": "xEn1xp8iu6", "cdate": 1648731321006, "mdate": 1648731321006, "content": {"title": "Conditional DETR for Fast Training Convergence", "abstract": "The recently-developed DETR approach applies the\ntransformer encoder and decoder architecture to object detection and achieves promising performance. In this paper, we handle the critical issue, slow training convergence,\nand present a conditional cross-attention mechanism for\nfast DETR training. Our approach is motivated by that the\ncross-attention in DETR relies highly on the content embeddings for localizing the four extremities and predicting the\nbox, which increases the need for high-quality content embeddings and thus the training difficulty.\nOur approach, named conditional DETR, learns a conditional spatial query from the decoder embedding for\ndecoder multi-head cross-attention. The benefit is that\nthrough the conditional spatial query, each cross-attention\nhead is able to attend to a band containing a distinct region, e.g., one object extremity or a region inside the object box. This narrows down the spatial range for localizing the distinct regions for object classification and box\nregression, thus relaxing the dependence on the content embeddings and easing the training. Empirical results show\nthat conditional DETR converges 6.7\u00d7 faster for the backbones R50 and R101 and 10\u00d7 faster for stronger backbones\nDC5-R50 and DC5-R101. Code is available at https:\n//github.com/Atten4Vis/ConditionalDETR."}}
{"id": "iuAglOmZXw", "cdate": 1648731175091, "mdate": 1648731175091, "content": {"title": "Semi-Supervised Semantic Segmentation with Cross Pseudo Supervision", "abstract": "In this paper, we study the semi-supervised semantic segmentation problem via exploring both labeled data and extra unlabeled data. We propose a novel consistency regularization approach, called cross pseudo supervision (CPS).\nOur approach imposes the consistency on two segmentation networks perturbed with different initialization for the\nsame input image. The pseudo one-hot label map, output\nfrom one perturbed segmentation network, is used to supervise the other segmentation network with the standard\ncross-entropy loss, and vice versa. The CPS consistency has\ntwo roles: encourage high similarity between the predictions of two perturbed networks for the same input image,\nand expand training data by using the unlabeled data with\npseudo labels. Experiment results show that our approach\nachieves the state-of-the-art semi-supervised segmentation\nperformance on Cityscapes and PASCAL VOC 2012."}}
{"id": "vd59YSVkkMh", "cdate": 1640995200000, "mdate": 1667887431536, "content": {"title": "DETRs with Hybrid Matching", "abstract": "One-to-one set matching is a key design for DETR to establish its end-to-end capability, so that object detection does not require a hand-crafted NMS (non-maximum suppression) method to remove duplicate detections. This end-to-end signature is important for the versatility of DETR, and it has been generalized to a wide range of visual problems, including instance/semantic segmentation, human pose estimation, and point cloud/multi-view-images based detection, etc. However, we note that because there are too few queries assigned as positive samples, the one-to-one set matching significantly reduces the training efficiency of positive samples. This paper proposes a simple yet effective method based on a hybrid matching scheme that combines the original one-to-one matching branch with auxiliary queries that use one-to-many matching loss during training. This hybrid strategy has been shown to significantly improve training efficiency and improve accuracy. In inference, only the original one-to-one match branch is used, thus maintaining the end-to-end merit and the same inference efficiency of DETR. The method is named $\\mathcal{H}$-DETR, and it shows that a wide range of representative DETR methods can be consistently improved across a wide range of visual tasks, including Deformable-DETR, 3DETR/PETRv2, PETR, and TransTrack, among others. Code will be available at: https://github.com/HDETR"}}
{"id": "azeseIeOcKO", "cdate": 1640995200000, "mdate": 1667887431522, "content": {"title": "Region Rebalance for Long-Tailed Semantic Segmentation", "abstract": "In this paper, we study the problem of class imbalance in semantic segmentation. We first investigate and identify the main challenges of addressing this issue through pixel rebalance. Then a simple and yet effective region rebalance scheme is derived based on our analysis. In our solution, pixel features belonging to the same class are grouped into region features, and a rebalanced region classifier is applied via an auxiliary region rebalance branch during training. To verify the flexibility and effectiveness of our method, we apply the region rebalance module into various semantic segmentation methods, such as Deeplabv3+, OCRNet, and Swin. Our strategy achieves consistent improvement on the challenging ADE20K and COCO-Stuff benchmark. In particular, with the proposed region rebalance scheme, state-of-the-art BEiT receives +0.7% gain in terms of mIoU on the ADE20K val set."}}
{"id": "RfBc_YsLfD1", "cdate": 1640995200000, "mdate": 1667887431615, "content": {"title": "RankSeg: Adaptive Pixel Classification with Image Category Ranking for Segmentation", "abstract": "The segmentation task has traditionally been formulated as a complete-label (We use the term \u201ccomplete label\u201d to represent the set of all predefined categories in the dataset.) pixel classification task to predict a class for each pixel from a fixed number of predefined semantic categories shared by all images or videos. Yet, following this formulation, standard architectures will inevitably encounter various challenges under more realistic settings where the scope of categories scales up (e.g., beyond the level of $$1\\textrm{k}$$ ). On the other hand, in a typical image or video, only a few categories, i.e., a small subset of the complete label are present. Motivated by this intuition, in this paper, we propose to decompose segmentation into two sub-problems: (i) image-level or video-level multi-label classification and (ii) pixel-level rank-adaptive selected-label classification. Given an input image or video, our framework first conducts multi-label classification over the complete label, then sorts the complete label and selects a small subset according to their class confidence scores. We then use a rank-adaptive pixel classifier to perform the pixel-wise classification over only the selected labels, which uses a set of rank-oriented learnable temperature parameters to adjust the pixel classifications scores. Our approach is conceptually general and can be used to improve various existing segmentation frameworks by simply using a lightweight multi-label classification head and rank-adaptive pixel classifier. We demonstrate the effectiveness of our framework with competitive experimental results across four tasks, including image semantic segmentation, image panoptic segmentation, video instance segmentation, and video semantic segmentation. Especially, with our RankSeg, Mask2Former gains + $$0.8\\%$$ /+ $$0.7\\%$$ /+ $$0.7\\%$$ on ADE20K panoptic segmentation/YouTubeVIS 2019 video instance segmentation/VSPW video semantic segmentation benchmarks respectively. Code is available at: https://github.com/openseg-group/RankSeg ."}}
{"id": "IFMjtH8OwC", "cdate": 1640995200000, "mdate": 1667887431547, "content": {"title": "Expediting Large-Scale Vision Transformer for Dense Prediction without Fine-tuning", "abstract": "Vision transformers have recently achieved competitive results across various vision tasks but still suffer from heavy computation costs when processing a large number of tokens. Many advanced approaches have been developed to reduce the total number of tokens in large-scale vision transformers, especially for image classification tasks. Typically, they select a small group of essential tokens according to their relevance with the class token, then fine-tune the weights of the vision transformer. Such fine-tuning is less practical for dense prediction due to the much heavier computation and GPU memory cost than image classification. In this paper, we focus on a more challenging problem, i.e., accelerating large-scale vision transformers for dense prediction without any additional re-training or fine-tuning. In response to the fact that high-resolution representations are necessary for dense prediction, we present two non-parametric operators, a token clustering layer to decrease the number of tokens and a token reconstruction layer to increase the number of tokens. The following steps are performed to achieve this: (i) we use the token clustering layer to cluster the neighboring tokens together, resulting in low-resolution representations that maintain the spatial structures; (ii) we apply the following transformer layers only to these low-resolution representations or clustered tokens; and (iii) we use the token reconstruction layer to re-create the high-resolution representations from the refined low-resolution representations. The results obtained by our method are promising on five dense prediction tasks, including object detection, semantic segmentation, panoptic segmentation, instance segmentation, and depth estimation."}}
{"id": "B8YudBzull", "cdate": 1640995200000, "mdate": 1667887431517, "content": {"title": "MLSeg: Image and Video Segmentation as Multi-Label Classification and Selected-Label Pixel Classification", "abstract": "The segmentation task has traditionally been formulated as a complete-label pixel classification task to predict a class for each pixel from a fixed number of predefined semantic categories shared by all images or videos. Yet, following this formulation, standard architectures will inevitably encounter various challenges under more realistic settings where the scope of categories scales up (e.g., beyond the level of 1k). On the other hand, in a typical image or video, only a few categories, i.e., a small subset of the complete label are present. Motivated by this intuition, in this paper, we propose to decompose segmentation into two sub-problems: (i) image-level or video-level multi-label classification and (ii) pixel-level rank-adaptive selected-label classification. Given an input image or video, our framework first conducts multi-label classification over the complete label, then sorts the complete label and selects a small subset according to their class confidence scores. We then use a rank-adaptive pixel classifier to perform the pixel-wise classification over only the selected labels, which uses a set of rank-oriented learnable temperature parameters to adjust the pixel classifications scores. Our approach is conceptually general and can be used to improve various existing segmentation frameworks by simply using a lightweight multi-label classification head and rank-adaptive pixel classifier. We demonstrate the effectiveness of our framework with competitive experimental results across four tasks, including image semantic segmentation, image panoptic segmentation, video instance segmentation, and video semantic segmentation. Especially, with our RankSeg, Mask2Former gains +0.8%/+0.7%/+0.7% on ADE20K panoptic segmentation/YouTubeVIS 2019 video instance segmentation/VSPW video semantic segmentation benchmarks respectively."}}
{"id": "DF8LCjR03tX", "cdate": 1621629745367, "mdate": null, "content": {"title": "HRFormer: High-Resolution Vision Transformer for Dense Predict", "abstract": "We present a High-Resolution Transformer (HRFormer) that learns high-resolution representations for dense prediction tasks, in contrast to the original Vision Transformer that produces low-resolution representations and has high memory and computational cost. We take advantage of the multi-resolution parallel design introduced in high-resolution convolutional networks (HRNet [45]), along with local-window self-attention that performs self-attention over small non-overlapping image windows [21], for improving the memory and computation efficiency. In addition, we introduce a convolution into the FFN to exchange information across the disconnected image windows. We demonstrate the effectiveness of the HighResolution Transformer on both human pose estimation and semantic segmentation tasks, e.g., HRFormer outperforms Swin transformer [27] by 1.3 AP on COCO pose estimation with 50% fewer parameters and 30% fewer FLOPs. Code is available at: https://github.com/HRNet/HRFormer"}}
{"id": "jiYbKTMKWGT", "cdate": 1609459200000, "mdate": 1667887447550, "content": {"title": "Semi-Supervised Semantic Segmentation with Cross Pseudo Supervision", "abstract": "In this paper, we study the semi-supervised semantic segmentation problem via exploring both labeled data and extra unlabeled data. We propose a novel consistency regularization approach, called cross pseudo supervision (CPS). Our approach imposes the consistency on two segmentation networks perturbed with different initialization for the same input image. The pseudo one-hot label map, output from one perturbed segmentation network, is used to supervise the other segmentation network with the standard cross-entropy loss, and vice versa. The CPS consistency has two roles: encourage high similarity between the predictions of two perturbed networks for the same input image, and expand training data by using the unlabeled data with pseudo labels. Experiment results show that our approach achieves the state-of-the-art semi-supervised segmentation performance on Cityscapes and PASCAL VOC 2012. Code is available at https://git.io/CPS."}}
