{"id": "nejmipR21E", "cdate": 1640995200000, "mdate": 1674726281203, "content": {"title": "A Unified Weight Initialization Paradigm for Tensorial Convolutional Neural Networks", "abstract": "Tensorial Convolutional Neural Networks (TCNNs) have attracted much research attention for their power in reducing model parameters or enhancing the generalization ability. However, exploration of TCNNs is hindered even from weight initialization methods. To be specific, general initialization methods, such as Xavier or Kaiming initialization, usually fail to generate appropriate weights for TCNNs. Meanwhile, although there are ad-hoc approaches for specific architectures (e.g., Tensor Ring Nets), they are not applicable to TCNNs with other tensor decomposition methods (e.g., CP or Tucker decomposition). To address this problem, we propose a universal weight initialization paradigm, which generalizes Xavier and Kaiming methods and can be widely applicable to arbitrary TCNNs. Specifically, we first present the Reproducing Transformation to convert the backward process in TCNNs to an equivalent convolution process. Then, based on the convolution operators in the forward and backward processes, we build a unified paradigm to control the variance of features and gradients in TCNNs. Thus, we can derive fan-in and fan-out initialization for various TCNNs. We demonstrate that our paradigm can stabilize the training of TCNNs, leading to faster convergence and better results."}}
{"id": "UJfuBKcpRsI", "cdate": 1640995200000, "mdate": 1667220083141, "content": {"title": "A Unified Weight Initialization Paradigm for Tensorial Convolutional Neural Networks", "abstract": "Tensorial Convolutional Neural Networks (TCNNs) have attracted much research attention for their power in reducing model parameters or enhancing the generalization ability. However, exploration of ..."}}
{"id": "VAEnka3sYe", "cdate": 1577836800000, "mdate": 1674726281204, "content": {"title": "Concatenated Tensor Networks for Deep Multi-Task Learning", "abstract": "Deep Multi-Task Learning has achieved great success in a number of domains. However, the enormous number of parameters results in extremely large storage costs for current deep Multi-Task models. Several methods based on tensor networks were proposed to address this problem. However, the tensor train format based methods only share the information of one mode. The huge central core tensor of the tucker format is hard to be stored and optimized. To tackle these problems, we introduce a novel Concatenated Tensor Network structure, in particular, Projected Entangled Pair States (PEPS) like, into multi-task models. We name the resulted multi-task models as Concatenated Tensor Multi-Task Learning\u00a0(CT-MTL)."}}
