{"id": "8vKKm3Q7QJ", "cdate": 1672092835386, "mdate": 1672092835386, "content": {"title": "Simplified State Space Layers for Sequence Modeling", "abstract": "Models using structured state space sequence (S4) layers have achieved state-of-the-art\nperformance on long-range sequence modeling tasks. An S4 layer combines linear state space\nmodels (SSMs), the HiPPO framework, and deep learning to achieve high performance. We\nbuild on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas\nan S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one\nmulti-input, multi-output SSM. We establish a connection between S5 and S4, and use this to\ndevelop the initialization and parameterization used by the S5 model. The result is a state space\nlayer that can leverage efficient and widely implemented parallel scans, allowing S5 to match\nthe computational efficiency of S4, while also achieving state-of-the-art performance on several\nlong-range sequence modeling tasks. S5 averages 87.4% on the long range arena benchmark, and\n98.5% on the most difficult Path-X task."}}
{"id": "Ai8Hw3AXqks", "cdate": 1663850188621, "mdate": null, "content": {"title": "Simplified State Space Layers for Sequence Modeling", "abstract": "Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer.  Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM.  We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S5 model.  The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks.  S5 averages $87.4\\%$ on the long range arena benchmark, and $98.5\\%$ on the most difficult Path-X task."}}
{"id": "bDyLgfvZ0qJ", "cdate": 1652737607362, "mdate": null, "content": {"title": "SIXO: Smoothing Inference with Twisted Objectives", "abstract": "Sequential Monte Carlo (SMC) is an inference algorithm for state space models that approximates the posterior by sampling from a sequence of target distributions. The target distributions are often chosen to be the filtering distributions, but these ignore information from future observations, leading to practical and theoretical limitations in inference and model learning.  We introduce SIXO, a method that instead learns target distributions that approximate the smoothing distributions, incorporating information from all observations. The key idea is to use density ratio estimation to fit functions that warp the filtering distributions into the smoothing distributions. We then use SMC with these learned targets to define a variational objective for model and proposal learning. SIXO yields provably tighter log marginal lower bounds and offers more accurate posterior inferences and parameter estimates in a variety of domains.\n"}}
{"id": "LjLi240wM0o", "cdate": 1640995200000, "mdate": 1681661326544, "content": {"title": "Simplified State Space Layers for Sequence Modeling", "abstract": "Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages 87.4% on the long range arena benchmark, and 98.5% on the most difficult Path-X task."}}
{"id": "5sLBVclzmr", "cdate": 1640995200000, "mdate": 1682378716323, "content": {"title": "SIXO: Smoothing Inference with Twisted Objectives", "abstract": "Sequential Monte Carlo (SMC) is an inference algorithm for state space models that approximates the posterior by sampling from a sequence of target distributions. The target distributions are often chosen to be the filtering distributions, but these ignore information from future observations, leading to practical and theoretical limitations in inference and model learning. We introduce SIXO, a method that instead learns targets that approximate the smoothing distributions, incorporating information from all observations. The key idea is to use density ratio estimation to fit functions that warp the filtering distributions into the smoothing distributions. We then use SMC with these learned targets to define a variational objective for model and proposal learning. SIXO yields provably tighter log marginal lower bounds and offers significantly more accurate posterior inferences and parameter estimates in a variety of domains."}}
{"id": "BUqZEMycrlc", "cdate": 1609459200000, "mdate": 1645743883845, "content": {"title": "Robust Asymmetric Learning in POMDPs", "abstract": "Policies for partially observed Markov decision processes can be efficiently learned by imitating expert policies generated using asymmetric information. Unfortunately, existing approaches for this..."}}
{"id": "rOWZVzJ5Sec", "cdate": 1577836800000, "mdate": 1645743883896, "content": {"title": "Coping With Simulators That Don't Always Return", "abstract": "Deterministic models are approximations of reality that are easy to interpret and often easier to build than stochastic alternatives. Unfortunately, as nature is capricious, observational data can ..."}}
{"id": "F3PR3xNYuKI", "cdate": 1577836800000, "mdate": null, "content": {"title": "Planning as Inference in Epidemiological Models", "abstract": "In this work we demonstrate how to automate parts of the infectious disease-control policy-making process via performing inference in existing epidemiological models. The kind of inference tasks undertaken include computing the posterior distribution over controllable, via direct policy-making choices, simulation model parameters that give rise to acceptable disease progression outcomes. Among other things, we illustrate the use of a probabilistic programming language that automates inference in existing simulators. Neither the full capabilities of this tool for automating inference nor its utility for planning is widely disseminated at the current time. Timely gains in understanding about how such simulation-based models and inference automation tools applied in support of policymaking could lead to less economically damaging policy prescriptions, particularly during the current COVID-19 pandemic."}}
{"id": "SJecKyhEKr", "cdate": 1571237761930, "mdate": null, "content": {"title": "Coping With Simulators That Don\u2019t Always Return", "abstract": "Deterministic models are approximations of reality that are often easier to build and interpret than stochastic alternatives.  \nUnfortunately, as nature is capricious, observational data can never be fully explained by deterministic models in practice.  \nObservation and process noise need to be added to adapt deterministic models to behave stochastically, such that they are capable of explaining and extrapolating from noisy data.\nAdding process noise to deterministic simulators can induce a failure in the simulator resulting in no return value for certain inputs -- a property we describe as ``brittle.''\nWe investigate and address the wasted computation that arises from these failures, and the effect of such failures on downstream inference tasks.\nWe show that performing inference in this space can be viewed as rejection sampling, and train a conditional normalizing flow as a proposal over noise values such that there is a low probability that the simulator crashes, increasing computational efficiency and inference fidelity for a fixed sample budget when used as the proposal in an approximate inference algorithm."}}
{"id": "SJewmXF88r", "cdate": 1568211742777, "mdate": null, "content": {"title": "The Virtual Patch Clamp: Imputing C. elegans Membrane Potentials from Calcium Imaging", "abstract": "We develop a stochastic whole-brain and body simulator of the nematode roundworm Caenorhabditis elegans (C. elegans) and show that it is sufficiently regularizing to allow imputation of latent membrane potentials from partial calcium fluorescence imaging observations. This is the first attempt we know of to ``complete the circle,'' where an anatomically grounded whole-connectome simulator is used to impute a time-varying ``brain'' state at single-cell fidelity from covariates that are measurable in practice.  Using state of the art Bayesian machine learning methods to condition on readily obtainable data, our method paves the way for neuroscientists to recover interpretable connectome-wide state representations, automatically estimate physiologically relevant parameter values from data, and perform simulations investigating intelligent lifeforms in silico."}}
