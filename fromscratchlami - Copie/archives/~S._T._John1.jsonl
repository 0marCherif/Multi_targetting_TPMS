{"id": "wvZ2WmT1hR", "cdate": 1694186898310, "mdate": 1694186898310, "content": {"title": "Cost-aware learning of relevant contextual variables within Bayesian optimization", "abstract": "Contextual Bayesian Optimization (CBO) is a powerful framework for optimizing\nblack-box, expensive-to-evaluate functions with respect to design variables, while\nsimultaneously efficiently integrating relevant contextual information regarding\nthe environment, such as experimental conditions. However, in many practical\nscenarios, the relevance of contextual variables is not necessarily known beforehand.\nMoreover, the contextual variables can sometimes be optimized themselves, a\nsetting that current CBO algorithms do not take into account. Optimizing contextual\nvariables may be costly, which raises the question of determining a minimal relevant\nsubset. In this paper, we frame this problem as a cost-aware model selection BO\ntask and address it using a novel method, Sensitivity-Analysis-Driven Contextual\nBO (SADCBO). We learn the relevance of context variables by sensitivity analysis\nof the posterior surrogate model at specific input points, whilst minimizing the\ncost of optimization by leveraging recent developments on early stopping for BO.\nWe empirically evaluate our proposed SADCBO against alternatives on synthetic\nexperiments together with extensive ablation studies, and demonstrate a consistent\nimprovement across examples."}}
{"id": "B4LbTDHhUYG", "cdate": 1683883926804, "mdate": 1683883926804, "content": {"title": "Improving Hyperparameter Learning under Approximate Inference in Gaussian Process Models", "abstract": "Approximate inference in Gaussian process (GP) models with non-conjugate likelihoods gets entangled with the learning of the model hyperparameters. We improve hyperparameter learning in GP models and focus on the interplay between variational inference (VI) and the learning target. While VI's lower bound to the marginal likelihood is a suitable objective for inferring the approximate posterior, we show that a direct approximation of the marginal likelihood as in Expectation Propagation (EP) is a better learning objective for hyperparameter optimization. We design a hybrid training procedure to bring the best of both worlds: it leverages conjugate-computation VI for inference and uses an EP-like marginal likelihood approximation for hyperparameter learning. We compare VI, EP, Laplace approximation, and our proposed training procedure and empirically demonstrate the effectiveness of our proposal across a wide range of data sets.\n\n"}}
{"id": "oqRw-a4rf36", "cdate": 1664815581826, "mdate": null, "content": {"title": "Targeted Causal Elicitation", "abstract": "We look at the problem of learning causal structure for a fixed downstream causal effect optimization task. In contrast to previous work which often focuses on running interventional experiments, we consider an often overlooked source of information - the domain expert. In the Bayesian setting, this amounts to augmenting the likelihood with a user model whose parameters account for possible biases of the expert. Such a model can allow for active elicitation in a manner that is most informative to the optimization task at hand. "}}
{"id": "G-vMO8qUts", "cdate": 1664806785879, "mdate": null, "content": {"title": "Joint Point Process Model for Counterfactual Treatment-Outcome Trajectories Under Policy Interventions", "abstract": "Policy makers need to predict the progression of an outcome before adopting a new treatment policy, which defines when and how a sequence of treatments affecting the outcome occurs in continuous time. Commonly, algorithms that predict interventional future outcome trajectories take a fixed sequence of future treatments as input. This excludes scenarios where the policy is unknown or a counterfactual analysis is needed. To handle these limitations, we develop a joint model for treatments and outcomes, which allows for the estimation of treatment policies and effects from sequential treatment--outcome data. It can answer interventional and counterfactual queries about interventions on treatment policies, as we show with a realistic semi-synthetic simulation study. This abstract is based on work that is currently under review (Anonymous)."}}
{"id": "qvAK_mOXh3", "cdate": 1640995200000, "mdate": 1682317862195, "content": {"title": "Towards Improved Learning in Gaussian Processes: The Best of Two Worlds", "abstract": "Gaussian process training decomposes into inference of the (approximate) posterior and learning of the hyperparameters. For non-Gaussian (non-conjugate) likelihoods, two common choices for approximate inference are Expectation Propagation (EP) and Variational Inference (VI), which have complementary strengths and weaknesses. While VI's lower bound to the marginal likelihood is a suitable objective for inferring the approximate posterior, it does not automatically imply it is a good learning objective for hyperparameter optimization. We design a hybrid training procedure where the inference leverages conjugate-computation VI and the learning uses an EP-like marginal likelihood approximation. We empirically demonstrate on binary classification that this provides a good learning objective and generalizes better."}}
{"id": "pqcBEQv5C0", "cdate": 1640995200000, "mdate": 1682317861510, "content": {"title": "Non-separable Spatio-temporal Graph Kernels via SPDEs", "abstract": "Gaussian processes (GPs) provide a principled and direct approach for inference and learning on graphs. However, the lack of justified graph kernels for spatio-temporal modelling has held back their use in graph problems. We leverage an explicit link between stochastic partial differential equations (SPDEs) and GPs on graphs, introduce a framework for deriving graph kernels via SPDEs, and derive non-separable spatio-temporal graph kernels that capture interaction across space and time. We formulate the graph kernels for the stochastic heat equation and wave equation. We show that by providing novel tools for spatio-temporal GP modelling on graphs, we outperform pre-existing graph kernels in real-world applications that feature diffusion, oscillation, and other complicated interactions."}}
{"id": "5_Gi2xJEGQ9", "cdate": 1640995200000, "mdate": 1681713965150, "content": {"title": "Joint Non-parametric Point Process model for Treatments and Outcomes: Counterfactual Time-series Prediction Under Policy Interventions", "abstract": "A treatment policy defines when and what treatments are applied to affect some outcome of interest. Data-driven decision-making requires the ability to predict what happens if a policy is changed. Existing methods that predict how the outcome evolves under different scenarios assume that the tentative sequences of future treatments are fixed in advance, while in practice the treatments are determined stochastically by a policy and may depend, for example, on the efficiency of previous treatments. Therefore, the current methods are not applicable if the treatment policy is unknown or a counterfactual analysis is needed. To handle these limitations, we model the treatments and outcomes jointly in continuous time, by combining Gaussian processes and point processes. Our model enables the estimation of a treatment policy from observational sequences of treatments and outcomes, and it can predict the interventional and counterfactual progression of the outcome after an intervention on the treatment policy (in contrast with the causal effect of a single treatment). We show with real-world and semi-synthetic data on blood glucose progression that our method can answer causal queries more accurately than existing alternatives."}}
{"id": "5PPuxl9fkn", "cdate": 1640995200000, "mdate": 1682317863590, "content": {"title": "Fantasizing with Dual GPs in Bayesian Optimization and Active Learning", "abstract": "Gaussian processes (GPs) are the main surrogate functions used for sequential modelling such as Bayesian Optimization and Active Learning. Their drawbacks are poor scaling with data and the need to run an optimization loop when using a non-Gaussian likelihood. In this paper, we focus on `fantasizing' batch acquisition functions that need the ability to condition on new fantasized data computationally efficiently. By using a sparse Dual GP parameterization, we gain linear scaling with batch size as well as one-step updates for non-Gaussian likelihoods, thus extending sparse models to greedy batch fantasizing acquisition functions."}}
{"id": "rl_JKSR3tG", "cdate": 1609459200000, "mdate": 1682317863261, "content": {"title": "Non-separable Spatio-temporal Graph Kernels via SPDEs", "abstract": "Gaussian processes (GPs) provide a principled and direct approach for inference and learning on graphs. However, the lack of justified graph kernels for spatio-temporal modelling has held back their use in graph problems. We leverage an explicit link between stochastic partial differential equations (SPDEs) and GPs on graphs, introduce a framework for deriving graph kernels via SPDEs, and derive non-separable spatio-temporal graph kernels that capture interaction across space and time. We formulate the graph kernels for the stochastic heat equation and wave equation. We show that by providing novel tools for spatio-temporal GP modelling on graphs, we outperform pre-existing graph kernels in real-world applications that feature diffusion, oscillation, and other complicated interactions."}}
{"id": "r2pX89b24gF", "cdate": 1609459200000, "mdate": 1655125650086, "content": {"title": "Non-parametric modelling of temporal and spatial counts data from RNA-seq experiments", "abstract": "The negative binomial distribution has been shown to be a good model for counts data from both bulk and single-cell RNA-sequencing (RNA-seq). Gaussian process (GP) regression provides a useful non-parametric approach for modelling temporal or spatial changes in gene expression. However, currently available GP regression methods that implement negative binomial likelihood models do not scale to the increasingly large datasets being produced by single-cell and spatial transcriptomics."}}
