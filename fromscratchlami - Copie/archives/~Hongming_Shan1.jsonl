{"id": "glcpbNsuww", "cdate": 1683885879704, "mdate": 1683885879704, "content": {"title": "LICO: Language-Image COnsistent Learning by Manifold and Distribution Alignments", "abstract": "Interpreting the decisions of deep learning models has been discussing for several years since the explosion of deep neural networks. One of the convincing approaches is attention map-based visual interpretation such as CAM, where the generation of attention maps depend merely on categorical labels used for cross-entropy loss. Although current interpretation methods can provide convincible decision clues, they do not consider richer information that human describe an image, yielding partial correspondence between image and attention maps. In this paper, we address this issue by correlating learnable language prompts with corresponding vision features through manifold learning and optimal transport (OT) theory. Specifically, we first minimize the KL-divergence between adjacent matrices of vision and text features to guarantee the consistent global manifold structure. Second, we apply OT to assign local feature maps with corresponding class-specific prompts, then generating fine-grained attention maps. Extensive experiments on eight datasets show that the proposed LICO helps generate more explainable attention maps combined with current interpretation methods such as Grad-CAM. In addition, LICO also facilitate the vanilla convolutional neural networks to achieve higher classification performances without introducing any computational overhead during inference."}}
{"id": "SMvNPEuf04_", "cdate": 1680307200000, "mdate": 1682318550062, "content": {"title": "SAN-Net: Learning generalization to unseen sites for stroke lesion segmentation with self-adaptive normalization", "abstract": ""}}
{"id": "UAG9Q5HLBD", "cdate": 1678895242625, "mdate": 1678895242625, "content": {"title": "Structurally-sensitive multi-scale deep neural network for low-dose CT denoising", "abstract": "Computed tomography (CT) is a popular medical imaging modality and enjoys wide clinical applications. At the same time, the X-ray radiation dose associated with CT scannings raises a public concern due to its potential risks to the patients. Over the past years, major efforts have been dedicated to the development of low-dose CT (LDCT) methods. However, the radiation dose reduction compromises the signal-to-noise ratio, leading to strong noise and artifacts that down-grade the CT image quality. In this paper, we propose a novel 3-D noise reduction method, called structurally sensitive multi-scale generative adversarial net, to improve the LDCT image quality. Specifically, we incorporate 3-D volumetric information to improve the image quality. Also, different loss functions for training denoising models are investigated. Experiments show that the proposed method can effectively preserve the structural and textural information in reference to the normal-dose CT images and significantly suppress noise and artifacts. Qualitative visual assessments by three experienced radiologists demonstrate that the proposed method retrieves more information and outperforms competing methods."}}
{"id": "EOY36-2ckk", "cdate": 1678893726818, "mdate": 1678893726818, "content": {"title": "CT Super-Resolution GAN Constrained by the Identical, Residual, and Cycle Learning Ensemble (GAN-CIRCLE)", "abstract": "In this paper, we present a semi-supervised deep learning approach to accurately recover high-resolution (HR) CT images from low-resolution (LR) counterparts. Specifically, with the generative adversarial network (GAN) as the building block, we enforce the cycle-consistency in terms of the Wasserstein distance to establish a nonlinear end-to-end mapping from noisy LR input images to denoised and deblurred HR outputs. We also include the joint constraints in the loss function to facilitate structural preservation. In this process, we incorporate deep convolutional neural network (CNN), residual learning, and network in network techniques for feature extraction and restoration. In contrast to the current trend of increasing network depth and complexity to boost the imaging performance, we apply a parallel 1 \u00d7 1 CNN to compress the output of the hidden layer and optimize the number of layers and the number of filters for each convolutional layer. The quantitative and qualitative evaluative results demonstrate that our proposed model is accurate, efficient and robust for super-resolution (SR) image restoration from noisy LR input images. In particular, we validate our composite SR networks on three large-scale CT datasets, and obtain promising results as compared to the other state-of-the-art methods."}}
{"id": "Q2kg3N8T1L_", "cdate": 1677628800000, "mdate": 1682318550157, "content": {"title": "M3NAS: Multi-Scale and Multi-Level Memory-Efficient Neural Architecture Search for Low-Dose CT Denoising", "abstract": "Lowering the radiation dose in computed tomography (CT) can greatly reduce the potential risk to public health. However, the reconstructed images from dose-reduced CT or low-dose CT (LDCT) suffer from severe noise which compromises the subsequent diagnosis and analysis. Recently, convolutional neural networks have achieved promising results in removing noise from LDCT images. The network architectures that are used are either handcrafted or built on top of conventional networks such as ResNet and U-Net. Recent advances in neural network architecture search (NAS) have shown that the network architecture has a dramatic effect on the model performance. This indicates that current network architectures for LDCT may be suboptimal. Therefore, in this paper, we make the first attempt to apply NAS to LDCT and propose a multi-scale and multi-level memory-efficient NAS for LDCT denoising, termed M3NAS. On the one hand, the proposed M3NAS fuses features extracted by different scale cells to capture multi-scale image structural details. On the other hand, the proposed M3NAS can search a hybrid cell- and network-level structure for better performance. In addition, M3NAS can effectively reduce the number of model parameters and increase the speed of inference. Extensive experimental results on two different datasets demonstrate that the proposed M3NAS can achieve better performance and fewer parameters than several state-of-the-art methods. In addition, we also validate the effectiveness of the multi-scale and multi-level architecture for LDCT denoising, and present further analysis for different configurations of super-net."}}
{"id": "H7wp0KHdLyLS", "cdate": 1677628800000, "mdate": 1682318549924, "content": {"title": "Mutual Information Boosted Precipitation Nowcasting from Radar Images", "abstract": "Precipitation nowcasting has long been a challenging problem in meteorology. While recent studies have introduced deep neural networks into this area and achieved promising results, these models still struggle with the rapid evolution of rainfall and extremely imbalanced data distribution, resulting in poor forecasting performance for convective scenarios. In this article, we evaluate the amount of information in different precipitation nowcasting tasks of varying lengths using mutual information. We propose two strategies: the mutual information-based reweighting strategy (MIR) and a mutual information-based training strategy (time superimposing strategy (TSS)). MIR reinforces neural network models to improve the forecasting accuracy for convective scenarios while maintaining prediction performance for rainless scenarios and overall nowcasting image quality. The TSS strategy enhances the model\u2019s forecasting performance by adopting a curriculum learning-like method. Although the proposed strategies are simple, the experimental results show that they are effective and can be applied to various state-of-the-art models."}}
{"id": "A4HouYxKzv", "cdate": 1677628800000, "mdate": 1682318550169, "content": {"title": "Physics-/Model-Based and Data-Driven Methods for Low-Dose Computed Tomography: A survey", "abstract": "Since 2016, deep learning (DL) has advanced tomographic imaging with remarkable successes, especially in low-dose computed tomography (LDCT) imaging. Despite being driven by big data, the LDCT denoising and pure end-to-end reconstruction networks often suffer from the black-box nature and major issues, such as instabilities, which are major barriers to applying DL methods in LDCT applications. An emerging trend is to integrate imaging physics and models into deep networks, enabling a hybridization of physics-/model-based and data-driven elements. In this article, we systematically review the physics-/model-based data-driven methods for LDCT, summarize the loss functions and training strategies, evaluate the performance of different methods, and discuss relevant issues and future directions."}}
{"id": "GBJvKjNdb1VV", "cdate": 1675209600000, "mdate": 1682318549854, "content": {"title": "Forget less, count better: a domain-incremental self-distillation learning benchmark for lifelong crowd counting", "abstract": "Crowd counting has important applications in public safety and pandemic control. A robust and practical crowd counting system has to be capable of continuously learning with the newly incoming domain data in real-world scenarios instead of fitting one domain only. Off-the-shelf methods have some drawbacks when handling multiple domains: (1) the models will achieve limited performance (even drop dramatically) among old domains after training images from new domains due to the discrepancies in intrinsic data distributions from various domains, which is called catastrophic forgetting; (2) the well-trained model in a specific domain achieves imperfect performance among other unseen domains because of domain shift; (3) it leads to linearly increasing storage overhead, either mixing all the data for training or simply training dozens of separate models for different domains when new ones are available. To overcome these issues, we investigate a new crowd counting task in incremental domain training setting called lifelong crowd counting. Its goal is to alleviate catastrophic forgetting and improve the generalization ability using a single model updated by the incremental domains. Specifically, we propose a self-distillation learning framework as a benchmark (forget less, count better, or FLCB) for lifelong crowd counting, which helps the model leverage previous meaningful knowledge in a sustainable manner for better crowd counting to mitigate the forgetting when new data arrive. A new quantitative metric, normalized Backward Transfer (nBwT), is developed to evaluate the forgetting degree of the model in the lifelong learning process. Extensive experimental results demonstrate the superiority of our proposed benchmark in achieving a low catastrophic forgetting degree and strong generalization ability."}}
{"id": "p6-rdFQi3C", "cdate": 1672531200000, "mdate": 1682318550170, "content": {"title": "LIT-Former: Linking In-plane and Through-plane Transformers for Simultaneous CT Image Denoising and Deblurring", "abstract": "This paper studies 3D low-dose computed tomography (CT) imaging. Although various deep learning methods were developed in this context, typically they perform denoising due to low-dose and deblurring for super-resolution separately. Up to date, little work was done for simultaneous in-plane denoising and through-plane deblurring, which is important to improve clinical CT images. For this task, a straightforward method is to directly train an end-to-end 3D network. However, it demands much more training data and expensive computational costs. Here, we propose to link in-plane and through-plane transformers for simultaneous in-plane denoising and through-plane deblurring, termed as LIT-Former, which can efficiently synergize in-plane and through-plane sub-tasks for 3D CT imaging and enjoy the advantages of both convolution and transformer networks. LIT-Former has two novel designs: efficient multi-head self-attention modules (eMSM) and efficient convolutional feed-forward networks (eCFN). First, eMSM integrates in-plane 2D self-attention and through-plane 1D self-attention to efficiently capture global interactions of 3D self-attention, the core unit of transformer networks. Second, eCFN integrates 2D convolution and 1D convolution to extract local information of 3D convolution in the same fashion. As a result, the proposed LIT-Former synergizes these two sub-tasks, significantly reducing the computational complexity as compared to 3D counterparts and enabling rapid convergence. Extensive experimental results on simulated and clinical datasets demonstrate superior performance over state-of-the-art models."}}
{"id": "_Rz_Hnz020n", "cdate": 1672531200000, "mdate": 1682318550608, "content": {"title": "Twin Contrastive Learning with Noisy Labels", "abstract": "Learning from noisy data is a challenging task that significantly degenerates the model performance. In this paper, we present TCL, a novel twin contrastive learning model to learn robust representations and handle noisy labels for classification. Specifically, we construct a Gaussian mixture model (GMM) over the representations by injecting the supervised model predictions into GMM to link label-free latent variables in GMM with label-noisy annotations. Then, TCL detects the examples with wrong labels as the out-of-distribution examples by another two-component GMM, taking into account the data distribution. We further propose a cross-supervision with an entropy regularization loss that bootstraps the true targets from model predictions to handle the noisy labels. As a result, TCL can learn discriminative representations aligned with estimated labels through mixup and contrastive learning. Extensive experimental results on several standard benchmarks and real-world datasets demonstrate the superior performance of TCL. In particular, TCL achieves 7.5\\% improvements on CIFAR-10 with 90\\% noisy label -- an extremely noisy scenario. The source code is available at \\url{https://github.com/Hzzone/TCL}."}}
