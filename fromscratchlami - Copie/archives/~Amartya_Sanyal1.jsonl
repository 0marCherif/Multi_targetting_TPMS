{"id": "2ib_yegOoJ", "cdate": 1683021371126, "mdate": 1683021371126, "content": {"title": "Certifying Ensembles: A General Certification Theory with S-Lipschitzness ", "abstract": "Improving and guaranteeing the robustness of deep learning models has been a topic of intense research. Ensembling, which combines several classifiers to provide a better model, has shown to be beneficial for generalisation, uncertainty estimation, calibration, and mitigating the effects of concept drift. However, the impact of ensembling on certified robustness is less well understood. In this work, we generalise Lipschitz continuity by introducing S-Lipschitz classifiers, which we use to analyse the theoretical robustness of ensembles. Our results are precise conditions when ensembles of robust classifiers are more robust than any constituent classifier, as well as conditions when they are less robust. "}}
{"id": "vVXRNcltT6", "cdate": 1676472364436, "mdate": null, "content": {"title": "How to Make Semi-Private Learning Effective", "abstract": "In Semi-Private (SP) learning, the learner has access to both public and private data, and the differential privacy requirement is imposed solely on the private data. We propose a computationally  efficient  algorithm that, under mild assumptions on the data, provably achieves significantly lower sample complexity and can  be efficiently run on realistic datasets. To achieve this, we leverage the features extracted by pre-trained networks.  To validate its empirical effectiveness, we propose a particularly challenging set of experiments under tight privacy constraints ($\\epsilon=0.1$) and with a focus on low-data regimes. In all the settings, our algorithm exhibits significantly improved performance over the available baseline. "}}
{"id": "sbabGqakIc", "cdate": 1668734781415, "mdate": null, "content": {"title": "Certified defences hurt generalisation", "abstract": "In recent years, much work has been devoted to designing certified\ndefences for neural networks, i.e., methods for learning neural\nnetworks that are provably robust to certain adversarial\nperturbations. Due to the non-convexity of the problem, dominant\napproaches in this area rely on convex approximations, which are\ninherently loose. In this paper, we question the effectiveness of such\napproaches for realistic computer vision tasks. First, we provide\nextensive empirical evidence to show that certified defences suffer\nnot only worse accuracy but also worse robustness and fairness than\nempirical defences. We hypothesise that the reason for why certified\ndefences suffer in generalisation is (i) the large number of\nrelaxed non-convex constraints and (ii) strong alignment between the\nadversarial perturbations and the \"signal\" direction. We provide a\ncombination of theoretical and experimental evidence to support these\nhypotheses."}}
{"id": "h1j5I0WVxoI", "cdate": 1664725483013, "mdate": null, "content": {"title": "Certified defences hurt generalisation", "abstract": "In recent years, much work has been devoted to designing certified defences for neural networks, i.e., methods for learning neural networks that are provably robust to certain adversarial perturbations. Due to the non-convexity of the problem, dominant approaches in this area rely on convex approximations, which are inherently loose. In this paper, we question the effectiveness of such approaches for realistic computer vision tasks. First, we provide extensive empirical evidence to show that certified defences suffer not only worse accuracy but also worse robustness and fairness than empirical defences. We hypothesise that the reason for why certified defences suffer in generalisation is (i) the large number of relaxed non-convex constraints and (ii) strong alignment between the adversarial perturbations and the \"signal\" direction. We provide a combination of theoretical and experimental evidence to support these hypotheses."}}
{"id": "n0okuXMlI7V", "cdate": 1663850388514, "mdate": null, "content": {"title": "Catastrophic overfitting is a bug but it is caused by features", "abstract": "Adversarial training (AT) is the de facto method to build robust neural networks, but it is computationally expensive. To overcome this, fast single-step attacks can be used, but doing so is prone to catastrophic overfitting (CO). This is when networks gain non-trivial robustness during the first stages of AT, but then reach a breaking point where they become vulnerable in just a few iterations. Although some works have succeeded at preventing CO, the different mechanisms that lead to this failure mode are still poorly understood. In this work, we study the onset of CO in single-step AT methods through controlled modifications of typical datasets of natural images. In particular, we show that CO can be induced when injecting the images with seemingly innocuous features that are very useful for non-robust classification but need to be combined with other features to obtain a robust classifier. This new perspective provides important insights into the mechanisms that lead to CO and improves our understanding of the general dynamics of adversarial training."}}
{"id": "LiXDW7CF94J", "cdate": 1663850320510, "mdate": null, "content": {"title": "How robust is unsupervised representation learning to distribution shift?", "abstract": "The robustness of machine learning algorithms to distributions shift is primarily discussed in the context of supervised learning (SL). As such, there is a lack of insight on the robustness of the representations learned from unsupervised methods, such as self-supervised learning (SSL) and auto-encoder based algorithms (AE), to distribution shift. We posit that the input-driven objectives of unsupervised algorithms lead to representations that are more robust to distribution shift than the target-driven objective of SL. We verify this by extensively evaluating the performance of SSL and AE on both synthetic and realistic distribution shift datasets. Following observations that the linear layer used for classification itself can be susceptible to spurious correlations, we evaluate the representations using a linear\nhead trained on a small amount of out-of-distribution (OOD) data, to isolate the robustness of the learned representations from that of the linear head. We also develop \u201ccontrollable\u201d versions of existing realistic domain generalisation datasets with adjustable degrees of distribution shifts. This allows us to study the robustness of different learning algorithms under versatile yet realistic distribution shift\nconditions. Our experiments show that representations learned from unsupervised learning algorithms generalise better than SL under a wide variety of extreme as well as realistic distribution shifts."}}
{"id": "0_TxFpAsEI", "cdate": 1663850275396, "mdate": null, "content": {"title": "A law of adversarial risk, interpolation, and label noise", "abstract": "In supervised learning, it has been shown that label noise in the data can be interpolated without penalties on test accuracy.  We show that interpolating label noise induces adversarial vulnerability, and prove the first theorem showing the relationship between label noise and adversarial risk for any data distribution.  Our results are almost tight if we do not make any assumptions on the inductive bias of the learning algorithm. We then investigate how different components of this problem affect this result including properties of the distribution. We also discuss non-uniform label noise distributions; and prove a new theorem showing uniform label noise induces nearly as large an adversarial risk as the worst poisoning with the same noise rate.  Then, we provide theoretical and empirical evidence that uniform label noise is more harmful than typical real-world label noise.  Finally, we show how inductive biases amplify the effect of label noise and argue the need for future work in this direction."}}
{"id": "zKDcZBVVEWm", "cdate": 1653750179221, "mdate": null, "content": {"title": "How robust are pre-trained models to distribution shift? ", "abstract": "The vulnerability of machine learning models to spurious correlations has mostly been discussed in the context of supervised learning (SL). However, there is a lack of insight on how spurious correlations affect the performance of popular self-supervised learning (SSL) and auto-encoder based models (AE). In this work, we shed light on this by evaluating the performance of these models on both real world and synthetic distribution shift datasets. Following observations that the linear head itself can be susceptible to spurious correlations, we develop a new evaluation scheme with the linear head trained on out-of-distribution (OOD) data, to isolate the performance of the pre-trained models from a potential bias of the linear head used for evaluation. With this new methodology, we show that SSL models are consistently more robust to distribution shifts and thus better at OOD generalisation than AE and SL models."}}
{"id": "VrICAK9UV8", "cdate": 1653595785916, "mdate": null, "content": {"title": "How robust are pre-trained models to distribution shift?", "abstract": "The vulnerability of machine learning models to spurious correlations has mostly been discussed in the context of supervised learning (SL). However, there is a lack of insight on how spurious correlations affect the performance of popular self-supervised learning (SSL) and auto-encoder based models (AE). In this work, we shed light on this by evaluating the performance of these models on both real world and synthetic distribution shift datasets. Following observations that the linear head itself can be susceptible to spurious correlations, we develop a new evaluation scheme with the linear head trained on out-of-distribution (OOD) data, to isolate the performance of the pre-trained models from a potential bias of the linear head used for evaluation. With this new methodology, we show that SSL models are consistently more robust to distribution shifts and thus better at OOD generalisation than AE and SL models."}}
{"id": "NENo__bExYu", "cdate": 1652737805789, "mdate": null, "content": {"title": "Make Some Noise: Reliable and Efficient Single-Step Adversarial Training", "abstract": "Recently, Wong et al. (2020) showed that adversarial training with single-step FGSM leads to a characteristic failure mode named catastrophic overfitting (CO), in which a model becomes suddenly vulnerable to multi-step attacks. Experimentally they showed that simply adding a random perturbation prior to FGSM (RS-FGSM) could prevent CO. However,  Andriushchenko & Flammarion (2020) observed that RS-FGSM still leads to CO for larger perturbations, and proposed a computationally expensive regularizer (GradAlign) to avoid it. In this work, we methodically revisit the role of noise and clipping in single-step adversarial training. Contrary to previous intuitions, we find that using a stronger noise around the clean sample combined with \\textit{not clipping} is highly effective in avoiding CO for large perturbation radii. We then propose Noise-FGSM (N-FGSM) that, while providing the benefits of single-step adversarial training, does not suffer from CO. Empirical analyses on a large suite of experiments show that N-FGSM is able to match or surpass the performance of previous state of-the-art GradAlign while achieving 3$\\times$ speed-up."}}
