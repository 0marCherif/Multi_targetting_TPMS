{"id": "-CoNloheTs", "cdate": 1663850004604, "mdate": null, "content": {"title": "An Exact Poly-Time Membership-Queries Algorithm for Extracting a Three-Layer ReLU Network", "abstract": "We consider the natural problem of learning a ReLU network from queries, which was recently remotivated by model extraction attacks. In this work, we present a polynomial-time algorithm that can learn a depth-two ReLU network from queries under mild general position assumptions. We also present a polynomial-time algorithm that, under mild general position assumptions, can learn a rich class of depth-three ReLU networks from queries. For instance, it can learn most networks where the number of first layer neurons is smaller than the dimension and the number of second layer neurons.\n\nThese two results substantially improve state-of-the-art: Until our work, polynomial-time algorithms were only shown to learn from queries depth-two networks under the assumption that either the underlying distribution is Gaussian (Chen et al. (2021)) or that the weights matrix rows are linearly independent (Milli et al. (2019)). For depth three or more, there were no known poly-time results."}}
{"id": "je-vtE9clO", "cdate": 1640995200000, "mdate": 1681655626769, "content": {"title": "On the Sample Complexity of Two-Layer Networks: Lipschitz vs. Element-Wise Lipschitz Activation", "abstract": ""}}
{"id": "SQJDX46Z0H", "cdate": 1609459200000, "mdate": 1681500508355, "content": {"title": "An Exact Poly-Time Membership-Queries Algorithm for Extraction a three-Layer ReLU Network", "abstract": ""}}
{"id": "nqkBtlF360e", "cdate": 1546300800000, "mdate": null, "content": {"title": "Generalization Bounds for Neural Networks via Approximate Description Length", "abstract": "We investigate the sample complexity of networks with bounds on the magnitude of its weights. In particular, we consider the class \\[ H=\\left\\{W_t\\circ\\rho\\circ \\ldots\\circ\\rho\\circ W_{1} :W_1,\\ldots,W_{t-1}\\in M_{d, d}, W_t\\in M_{1,d}\\right\\} \\] where the spectral norm of each $W_i$ is bounded by $O(1)$, the Frobenius norm is bounded by $R$, and $\\rho$ is the sigmoid function $\\frac{e^x}{1+e^x}$ or the smoothened ReLU function $ \\ln (1+e^x)$. We show that for any depth $t$, if the inputs are in $[-1,1]^d$, the sample complexity of $H$ is $\\tilde O\\left(\\frac{dR^2}{\\epsilon^2}\\right)$. This bound is optimal up to log-factors, and substantially improves over the previous state of the art of $\\tilde O\\left(\\frac{d^2R^2}{\\epsilon^2}\\right)$. We furthermore show that this bound remains valid if instead of considering the magnitude of the $W_i$'s, we consider the magnitude of $W_i - W_i^0$, where $W_i^0$ are some reference matrices, with spectral norm of $O(1)$. By taking the $W_i^0$ to be the matrices at the onset of the training process, we get sample complexity bounds that are sub-linear in the number of parameters, in many typical regimes of parameters. To establish our results we develop a new technique to analyze the sample complexity of families $H$ of predictors. We start by defining a new notion of a randomized approximate description of functions $f:X\\to\\mathbb{R}^d$. We then show that if there is a way to approximately describe functions in a class $H$ using $d$ bits, then $d/\\epsilon^2$ examples suffices to guarantee uniform convergence. Namely, that the empirical loss of all the functions in the class is $\\epsilon$-close to the true loss. Finally, we develop a set of tools for calculating the approximate description length of classes of functions that can be presented as a composition of linear function classes and non-linear functions."}}
{"id": "GYPiP3Y1z7I", "cdate": 1546300800000, "mdate": null, "content": {"title": "Generalization Bounds for Neural Networks via Approximate Description Length", "abstract": "We investigate the sample complexity of networks with bounds on the magnitude of its weights. In particular, we consider the class \\[ \\cn = \\left\\{W_t\\circ\\rho\\circ W_{t-1}\\circ\\rho\\ldots\\circ \\rho\\circ W_{1} : W_1,\\ldots,W_{t-1}\\in M_{d\\times d}, W_t\\in M_{1,d} \\right\\} \\] where the spectral norm of each $W_i$ is bounded by $O(1)$, the Frobenius norm is bounded by $R$, and $\\rho$ is the sigmoid function $\\frac{e^x}{1 + e^x}$ or the smoothened ReLU function $ \\ln\\left(1 + e^x\\right)$. We show that for any depth $t$, if the inputs are in $[-1,1]^d$, the sample complexity of $\\cn$ is $\\tilde O\\left(\\frac{dR^2}{\\epsilon^2}\\right)$. This bound is optimal up to log-factors, and substantially improves over the previous state of the art of $\\tilde O\\left(\\frac{d^2R^2}{\\epsilon^2}\\right)$, that was established in a recent line of work. We furthermore show that this bound remains valid if instead of considering the magnitude of the $W_i$'s, we consider the magnitude of $W_i - W_i^0$, where $W_i^0$ are some reference matrices, with spectral norm of $O(1)$. By taking the $W_i^0$ to be the matrices in the onset of the training process, we get sample complexity bounds that are sub-linear in the number of parameters, in many {\\em typical} regimes of parameters. To establish our results we develop a new technique to analyze the sample complexity of families $\\ch$ of predictors. We start by defining a new notion of a randomized approximate description of functions $f:\\cx\\to\\reals^d$. We then show that if there is a way to approximately describe functions in a class $\\ch$ using $d$ bits, then $\\frac{d}{\\epsilon^2}$ examples suffices to guarantee uniform convergence. Namely, that the empirical loss of all the functions in the class is $\\epsilon$-close to the true loss. Finally, we develop a set of tools for calculating the approximate description length of classes of functions that can be presented as a composition of linear function classes and non-linear functions."}}
