{"id": "eFQooTXqE_p", "cdate": 1546300800000, "mdate": null, "content": {"title": "WALL-E: An Efficient Reinforcement Learning Research Framework", "abstract": "There are two halves to RL systems: experience collection time and policy learning time. For a large number of samples in rollouts, experience collection time is the major bottleneck. Thus, it is necessary to speed up the rollout generation time with multi-process architecture support. Our work, dubbed WALL-E, utilizes multiple rollout samplers running in parallel to rapidly generate experience. Due to our parallel samplers, we experience not only faster convergence times, but also higher average reward thresholds. For example, on the MuJoCo HalfCheetah-v2 task, with $N = 10$ parallel sampler processes, we are able to achieve much higher average return than those from using only a single process architecture."}}
{"id": "SJ4LjjWuZH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Learning to Explore via Meta-Policy Gradient", "abstract": "The performance of off-policy learning, including deep Q-learning and deep deterministic policy gradient (DDPG), critically depends on the choice of the exploration policy. Existing exploration met..."}}
{"id": "8IlYFwkvrp", "cdate": 1514764800000, "mdate": null, "content": {"title": "Variational Inference for Policy Gradient", "abstract": "Inspired by the seminal work on Stein Variational Inference and Stein Variational Policy Gradient, we derived a method to generate samples from the posterior variational parameter distribution by \\textit{explicitly} minimizing the KL divergence to match the target distribution in an amortize fashion. Consequently, we applied this varational inference technique into vanilla policy gradient, TRPO and PPO with Bayesian Neural Network parameterizations for reinforcement learning problems."}}
{"id": "iC2lV-B98C", "cdate": 1483228800000, "mdate": null, "content": {"title": "Stochastic Variance Reduction for Policy Gradient Estimation", "abstract": "Recent advances in policy gradient methods and deep learning have demonstrated their applicability for complex reinforcement learning problems. However, the variance of the performance gradient estimates obtained from the simulation is often excessive, leading to poor sample efficiency. In this paper, we apply the stochastic variance reduced gradient descent (SVRG) to model-free policy gradient to significantly improve the sample-efficiency. The SVRG estimation is incorporated into a trust-region Newton conjugate gradient framework for the policy optimization. On several Mujoco tasks, our method achieves significantly better performance compared to the state-of-the-art model-free policy gradient methods in robotic continuous control such as trust region policy optimization (TRPO)"}}
{"id": "ryWGTVWOZS", "cdate": 1388534400000, "mdate": null, "content": {"title": "Practical Lessons from Predicting Clicks on Ads at Facebook", "abstract": "Online advertising allows advertisers to only bid and pay for measurable user responses, such as clicks on ads. As a consequence, click prediction systems are central to most online advertising systems. With over 750 million daily active users and over 1 million active advertisers, predicting clicks on Facebook ads is a challenging machine learning task. In this paper we introduce a model which combines decision trees with logistic regression, outperforming either of these methods on its own by over 3%, an improvement with significant impact to the overall system performance. We then explore how a number of fundamental parameters impact the final prediction performance of our system. Not surprisingly, the most important thing is to have the right features: those capturing historical information about the user or ad dominate other types of features. Once we have the right features and the right model (decisions trees plus logistic regression), other factors play small roles (though even small improvements are important at scale). Picking the optimal handling for data freshness, learning rate schema and data sampling improve the model slightly, though much less than adding a high-value feature, or picking the right model to begin with."}}
{"id": "BJ4qtReuWr", "cdate": 1388534400000, "mdate": null, "content": {"title": "Online Classification Using a Voted RDA Method", "abstract": "We propose a voted dual averaging method for online classification problems with explicit regularization. This method employs the update rule of the regularized dual averaging (RDA) method proposed by Xiao, but only on the subsequence of training examples where a classification error is made. We derive a bound on the number of mistakes made by this method on the training set, as well as its generalization error rate. We also introduce the concept of relative strength of regularization, and show how it affects the mistake bound and generalization performance. We examine the method using l1-regularization on a large-scale natural language processing task, and obtained state-of-the-art classification performance with fairly sparse models."}}
{"id": "WTO0GkAQMInT", "cdate": 1293840000000, "mdate": null, "content": {"title": "Multicore Gibbs Sampling in Dense, Unstructured Graphs.", "abstract": ""}}
