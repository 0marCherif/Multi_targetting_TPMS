{"id": "hlsu-HrU7ON", "cdate": 1663850062679, "mdate": null, "content": {"title": "Intrinsic Motivation via Surprise Memory", "abstract": "We present a new computing model for intrinsic rewards in reinforcement learning that addresses the limitations of existing surprise-driven explorations. The reward is the novelty of the surprise rather than the surprise norm. We estimate the surprise novelty as retrieval errors of a memory network wherein the memory stores and reconstructs surprises. Our surprise memory (SM) augments the capability of surprise-based intrinsic motivators, maintaining the agent's interest in exciting exploration while reducing unwanted attraction to unpredictable or noisy observations. Our experiments demonstrate that the SM combined with various surprise predictors exhibits efficient exploring behaviors and significantly boosts the final performance in sparse reward environments, including Noisy-TV, navigation and challenging Atari games. "}}
{"id": "pF5aR69c9c", "cdate": 1652737514228, "mdate": null, "content": {"title": "Learning to Constrain Policy Optimization with Virtual Trust Region", "abstract": "We introduce a constrained optimization method for policy gradient reinforcement learning, which uses two trust regions to regulate each policy update. In addition to using the proximity of one single old policy as the first trust region as done by prior works, we propose forming a second trust region by constructing another virtual policy that represents a wide range of past policies. We then enforce the new policy to stay closer to the virtual policy, which is beneficial if the old policy performs poorly. We propose a mechanism to automatically build the virtual policy from a memory buffer of past policies, providing a new capability for dynamically selecting appropriate trust regions during the optimization process. Our proposed method, dubbed Memory-Constrained Policy Optimization (MCPO), is examined in diverse environments, including robotic locomotion control, navigation with sparse rewards and Atari games, consistently demonstrating competitive performance against recent on-policy constrained policy gradient methods."}}
{"id": "C7jm6YgJaT", "cdate": 1652737483619, "mdate": null, "content": {"title": "Momentum Adversarial Distillation: Handling Large Distribution Shifts in Data-Free Knowledge Distillation", "abstract": "Data-free Knowledge Distillation (DFKD) has attracted attention recently thanks to its appealing capability of transferring knowledge from a teacher network to a student network without using training data. The main idea is to use a generator to synthesize data for training the student. As the generator gets updated, the distribution of synthetic data will change. Such distribution shift could be large if the generator and the student are trained adversarially, causing the student to forget the knowledge it acquired at the previous steps. To alleviate this problem, we propose a simple yet effective method called Momentum Adversarial Distillation (MAD) which maintains an exponential moving average (EMA) copy of the generator and uses synthetic samples from both the generator and the EMA generator to train the student. Since the EMA generator can be considered as an ensemble of the generator's old versions and often undergoes a smaller change in updates compared to the generator, training on its synthetic samples can help the student recall the past knowledge and prevent the student from adapting too quickly to the new updates of the generator. Our experiments on six benchmark datasets including big datasets like ImageNet and Places365 demonstrate the superior performance of MAD over competing methods for handling the large distribution shift problem. Our method also compares favorably to existing DFKD methods and even achieves state-of-the-art results in some cases."}}
{"id": "q9MiHdIa7Ae", "cdate": 1640995200000, "mdate": 1668025908803, "content": {"title": "Momentum Adversarial Distillation: Handling Large Distribution Shifts in Data-Free Knowledge Distillation", "abstract": "Data-free Knowledge Distillation (DFKD) has attracted attention recently thanks to its appealing capability of transferring knowledge from a teacher network to a student network without using training data. The main idea is to use a generator to synthesize data for training the student. As the generator gets updated, the distribution of synthetic data will change. Such distribution shift could be large if the generator and the student are trained adversarially, causing the student to forget the knowledge it acquired at previous steps. To alleviate this problem, we propose a simple yet effective method called Momentum Adversarial Distillation (MAD) which maintains an exponential moving average (EMA) copy of the generator and uses synthetic samples from both the generator and the EMA generator to train the student. Since the EMA generator can be considered as an ensemble of the generator's old versions and often undergoes a smaller change in updates compared to the generator, training on its synthetic samples can help the student recall the past knowledge and prevent the student from adapting too quickly to new updates of the generator. Our experiments on six benchmark datasets including big datasets like ImageNet and Places365 demonstrate the superior performance of MAD over competing methods for handling the large distribution shift problem. Our method also compares favorably to existing DFKD methods and even achieves state-of-the-art results in some cases."}}
{"id": "lBGeXe8JL6", "cdate": 1640995200000, "mdate": 1682338515913, "content": {"title": "Black-Box Few-Shot Knowledge Distillation", "abstract": ""}}
{"id": "HkyWb91n86U", "cdate": 1640995200000, "mdate": 1668137664380, "content": {"title": "Towards Effective and Robust Neural Trojan Defenses via Input Filtering", "abstract": ""}}
{"id": "5nX2rS2I_Ga", "cdate": 1640995200000, "mdate": 1682338515899, "content": {"title": "Face Swapping as A Simple Arithmetic Operation", "abstract": ""}}
{"id": "2J8dZaw-eb", "cdate": 1640995200000, "mdate": 1682338516320, "content": {"title": "Black-box Few-shot Knowledge Distillation", "abstract": ""}}
{"id": "2FHHLFd4sS", "cdate": 1640995200000, "mdate": 1682338516239, "content": {"title": "Front-door Adjustment via Style Transfer for Out-of-distribution Generalisation", "abstract": ""}}
{"id": "7yuU9VeIpde", "cdate": 1632875518442, "mdate": null, "content": {"title": "Memory-Constrained Policy Optimization", "abstract": "We introduce a new constrained optimization method for policy gradient reinforcement learning, which uses two trust regions to regulate each policy update. In addition to using the proximity of one single old policy as the first trust region as done by prior works, we propose to form a second trust region through the construction of another virtual policy that represents a wide range of past policies. We then enforce the new policy to stay closer to the virtual policy, which is beneficial in case the old policy performs badly. More importantly, we propose a mechanism to automatically build the virtual policy from a memory buffer of past policies, providing a new capability for dynamically selecting appropriate trust regions during the optimization process. Our proposed method, dubbed as Memory-Constrained Policy Optimization (MCPO), is examined on a diverse suite of environments including robotic locomotion control, navigation with sparse rewards and Atari games, consistently demonstrating competitive performance against recent on-policy constrained policy gradient methods. "}}
