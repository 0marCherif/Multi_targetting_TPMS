{"id": "46a9qS-4KL", "cdate": 1684684359012, "mdate": 1684684359012, "content": {"title": "FInding Hidden Cliques of size \\sqrt{N/e} in nearly linear time", "abstract": "Consider an Erd\u00f6s-Renyi random graph in which each edge is present independently with probability 1/2, except for a subset $\\sC_N$ of the vertices that form a clique (a completely connected subgraph). We consider the problem of identifying the clique, given a realization of such a random graph.\nThe best known algorithm provably finds the clique in linear time with high probability, provided $|\\sC_N|\\ge 1.261\\sqrt{N}$ \\cite{dekel2011finding}. Spectral methods can be shown to fail on cliques smaller than N\u203e\u203e\u221a. In this paper we describe a nearly linear time algorithm that succeeds with high probability for $|\\sC_N|\\ge (1+\\eps)\\sqrt{N/e}$ for any $\\eps>0$. This is the first algorithm that provably improves over spectral methods.\nWe further generalize the hidden clique problem to other background graphs (the standard case corresponding to the complete graph on N vertices). For large girth regular graphs of degree (\u0394+1) we prove that `local' algorithms succeed if $|\\sC_N|\\ge (1+\\eps)N/\\sqrt{e\\Delta}$ and fail if $|\\sC_N|\\le(1-\\eps)N/\\sqrt{e\\Delta}$."}}
{"id": "cAG_24XZItc", "cdate": 1609459200000, "mdate": 1633011290198, "content": {"title": "Near-optimal inference in adaptive linear regression", "abstract": "When data is collected in an adaptive manner, even simple methods like ordinary least squares can exhibit non-normal asymptotic behavior. As an undesirable consequence, hypothesis tests and confidence intervals based on asymptotic normality can lead to erroneous results. We propose an online debiasing estimator to correct these distributional anomalies in least squares estimation. Our proposed method takes advantage of the covariance structure present in the dataset and provides sharper estimates in directions for which more information has accrued. We establish an asymptotic normality property for our proposed online debiasing estimator under mild conditions on the data collection process, and provide asymptotically exact confidence intervals. We additionally prove a minimax lower bound for the adaptive linear regression problem, thereby providing a baseline by which to compare estimators. There are various conditions under which our proposed estimator achieves the minimax lower bound up to logarithmic factors. We demonstrate the usefulness of our theory via applications to multi-armed bandit, autoregressive time series estimation, and active learning with exploration."}}
{"id": "wkakJlah7HJ", "cdate": 1546300800000, "mdate": 1684684745711, "content": {"title": "Online Debiasing for Adaptively Collected High-dimensional Data", "abstract": "Adaptive collection of data is commonplace in applications throughout science and engineering. From the point of view of statistical inference however, adaptive data collection induces memory and correlation in the samples, and poses significant challenge. We consider the high-dimensional linear regression, where the samples are collected adaptively, and the sample size $n$ can be smaller than $p$, the number of covariates. In this setting, there are two distinct sources of bias: the first due to regularization imposed for consistent estimation, e.g. using the LASSO, and the second due to adaptivity in collecting the samples. We propose \"online debiasing\", a general procedure for estimators such as the LASSO, which addresses both sources of bias. In two concrete contexts $(i)$ time series analysis and $(ii)$ batched data collection, we demonstrate that online debiasing optimally debiases the LASSO estimate when the underlying parameter $\\theta_0$ has sparsity of order $o(\\sqrt{n}/\\log p)$. In this regime, the debiased estimator can be used to compute $p$-values and confidence intervals of optimal size."}}
{"id": "el9zTlBQkoy", "cdate": 1546300800000, "mdate": 1684684745705, "content": {"title": "The threshold for SDP-refutation of random regular NAE-3SAT", "abstract": ""}}
{"id": "leTgt3ykMZ", "cdate": 1514764800000, "mdate": 1684684745692, "content": {"title": "The threshold for SDP-refutation of random regular NAE-3SAT", "abstract": "Unlike its cousin 3SAT, the NAE-3SAT (not-all-equal-3SAT) problem has the property that spectral/SDP algorithms can efficiently refute random instances when the constraint density is a large constant (with high probability). But do these methods work immediately above the \"satisfiability threshold\", or is there still a range of constraint densities for which random NAE-3SAT instances are unsatisfiable but hard to refute? We show that the latter situation prevails, at least in the context of random regular instances and SDP-based refutation. More precisely, whereas a random $d$-regular instance of NAE-3SAT is easily shown to be unsatisfiable (whp) once $d \\geq 8$, we establish the following sharp threshold result regarding efficient refutation: If $d < 13.5$ then the basic SDP, even augmented with triangle inequalities, fails to refute satisfiability (whp), if $d > 13.5$ then even the most basic spectral algorithm refutes satisfiability~(whp)."}}
{"id": "fal4IBlBlBX", "cdate": 1514764800000, "mdate": 1684684745849, "content": {"title": "Contextual Stochastic Block Models", "abstract": "We provide the first information theoretic tight analysis for inference of latent community structure given a sparse graph along with high dimensional node covariates, correlated with the same latent communities. Our work bridges recent theoretical breakthroughs in the detection of latent community structure without nodes covariates and a large body of empirical work using diverse heuristics for combining node covariates with graphs for inference. The tightness of our analysis implies in particular, the information theoretical necessity of combining the different sources of information. Our analysis holds for networks of large degrees as well as for a Gaussian version of the model."}}
{"id": "SyWNXKZdWB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Contextual Stochastic Block Models", "abstract": "We provide the first information theoretical tight analysis for inference of latent community structure given a sparse graph along with high dimensional node covariates, correlated with the same latent communities. Our work bridges recent theoretical breakthroughs in detection of latent community structure without nodes covariates and a large body of empirical work using diverse heuristics for combining node covariates with graphs for inference. The tightness of our analysis implies in particular, the information theoretic necessity of combining the different sources of information. Our analysis holds for networks of large degrees as well as for a Gaussian version of the model."}}
{"id": "BJWlu2-uZH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Accurate Inference for Adaptive Linear Models", "abstract": "Estimators computed from adaptively collected data do not behave like their non-adaptive brethren.Rather, the sequential dependence of the collection policy can lead to severe distributional biases..."}}
{"id": "fsPZbt5zLoU", "cdate": 1483228800000, "mdate": 1633011290205, "content": {"title": "Accurate Inference for Adaptive Linear Models", "abstract": "Estimators computed from adaptively collected data do not behave like their non-adaptive brethren. Rather, the sequential dependence of the collection policy can lead to severe distributional biases that persist even in the infinite data limit. We develop a general method -- $\\mathbf{W}$-decorrelation -- for transforming the bias of adaptive linear regression estimators into variance. The method uses only coarse-grained information about the data collection policy and does not need access to propensity scores or exact knowledge of the policy. We bound the finite-sample bias and variance of the $\\mathbf{W}$-estimator and develop asymptotically correct confidence intervals based on a novel martingale central limit theorem. We then demonstrate the empirical benefits of the generic $\\mathbf{W}$-decorrelation procedure in two different adaptive data settings: the multi-armed bandit and the autoregressive time series."}}
{"id": "Hk-TJF-uWH", "cdate": 1483228800000, "mdate": null, "content": {"title": "Inference in Graphical Models via Semidefinite Programming Hierarchies", "abstract": "Maximum A posteriori Probability (MAP) inference in graphical models amounts to solving a graph-structured combinatorial optimization problem. Popular inference algorithms such as belief propagation (BP) and generalized belief propagation (GBP) are intimately related to linear programming (LP) relaxation within the Sherali-Adams hierarchy. Despite the popularity of these algorithms, it is well understood that the Sum-of-Squares (SOS) hierarchy based on semidefinite programming (SDP) can provide superior guarantees. Unfortunately, SOS relaxations for a graph with $n$ vertices require solving an SDP with $n^{\\Theta(d)}$ variables where $d$ is the degree in the hierarchy. In practice, for $d\\ge 4$, this approach does not scale beyond a few tens of variables. In this paper, we propose binary SDP relaxations for MAP inference using the SOS hierarchy with two innovations focused on computational efficiency. Firstly, in analogy to BP and its variants, we only introduce decision variables corresponding to contiguous regions in the graphical model. Secondly, we solve the resulting SDP using a non-convex Burer-Monteiro style method, and develop a sequential rounding procedure. We demonstrate that the resulting algorithm can solve problems with tens of thousands of variables within minutes, and outperforms BP and GBP on practical problems such as image denoising and Ising spin glasses. Finally, for specific graph types, we establish a sufficient condition for the tightness of the proposed partial SOS relaxation."}}
