{"id": "r_uZZ-G6Qbc", "cdate": 1609459200000, "mdate": 1646674424573, "content": {"title": "WikiTableT: A Large-Scale Data-to-Text Dataset for Generating Wikipedia Article Sections", "abstract": "Mingda Chen, Sam Wiseman, Kevin Gimpel. Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021."}}
{"id": "r6M-Z-Mamb9", "cdate": 1609459200000, "mdate": 1646674424571, "content": {"title": "TVRecap: A Dataset for Generating Stories with Character Descriptions", "abstract": "We introduce TVStoryGen, a story generation dataset that requires generating detailed TV show episode recaps from a brief summary and a set of documents describing the characters involved. Unlike other story generation datasets, TVStoryGen contains stories that are authored by professional screen-writers and that feature complex interactions among multiple characters. Generating stories in TVStoryGen requires drawing relevant information from the lengthy provided documents about characters based on the brief summary. In addition, we propose to train reverse models on our dataset for evaluating the faithfulness of generated stories. We create TVStoryGen from fan-contributed websites, which allows us to collect 26k episode recaps with 1868.7 tokens on average. Empirically, we take a hierarchical story generation approach and find that the neural model that uses oracle content selectors for character descriptions demonstrates the best performance on automatic metrics, showing the potential of our dataset to inspire future research on story generation with constraints. Qualitative analysis shows that the best-performing model sometimes generates content that is unfaithful to the short summaries, suggesting promising directions for future work."}}
{"id": "SbVWb-Mambq", "cdate": 1609459200000, "mdate": 1646674424580, "content": {"title": "SummScreen: A Dataset for Abstractive Screenplay Summarization", "abstract": "We introduce SummScreen, a summarization dataset comprised of pairs of TV series transcripts and human written recaps. The dataset provides a challenging testbed for abstractive summarization for several reasons. Plot details are often expressed indirectly in character dialogues and may be scattered across the entirety of the transcript. These details must be found and integrated to form the succinct plot descriptions in the recaps. Also, TV scripts contain content that does not directly pertain to the central plot but rather serves to develop characters or provide comic relief. This information is rarely contained in recaps. Since characters are fundamental to TV series, we also propose two entity-centric evaluation metrics. Empirically, we characterize the dataset by evaluating several methods, including neural models and those based on nearest neighbors. An oracle extractive approach outperforms all benchmarked models according to automatic metrics, showing that the neural models are unable to fully exploit the input transcripts. Human evaluation and qualitative analysis reveal that our non-oracle models are competitive with their oracle counterparts in terms of generating faithful plot events and can benefit from better content selectors. Both oracle and non-oracle models generate unfaithful facts, suggesting future research directions."}}
{"id": "rVgZbWMpXb9", "cdate": 1577836800000, "mdate": 1646674424574, "content": {"title": "How to Ask Better Questions? A Large-Scale Multi-Domain Dataset for Rewriting Ill-Formed Questions", "abstract": "We present a large-scale dataset for the task of rewriting an ill-formed natural language question to a well-formed one. Our multi-domain question rewriting (MQR) dataset is constructed from human contributed Stack Exchange question edit histories. The dataset contains 427,719 question pairs which come from 303 domains. We provide human annotations for a subset of the dataset as a quality estimate. When moving from ill-formed to well-formed questions, the question quality improves by an average of 45 points across three aspects. We train sequence-to-sequence neural models on the constructed dataset and obtain an improvement of 13.2% in BLEU-4 over baseline methods built from other data resources. We release the MQR dataset to encourage research on the problem of question rewriting.1"}}
{"id": "r5bbbfaXb5", "cdate": 1577836800000, "mdate": 1646674424596, "content": {"title": "Learning Probabilistic Sentence Representations from Paraphrases", "abstract": "Probabilistic word embeddings have shown effectiveness in capturing notions of generality and entailment, but there is very little work on doing the analogous type of investigation for sentences. In this paper we define probabilistic models that produce distributions for sentences. Our best-performing model treats each word as a linear transformation operator applied to a multivariate Gaussian distribution. We train our models on paraphrases and demonstrate that they naturally capture sentence specificity. While our proposed model achieves the best performance overall, we also show that specificity is represented by simpler architectures via the norm of the sentence vectors. Qualitative analysis shows that our probabilistic model captures sentential entailment and provides ways to analyze the specificity and preciseness of individual words."}}
{"id": "SqLW--M6QZ9", "cdate": 1577836800000, "mdate": 1646674424595, "content": {"title": "Mining Knowledge for Natural Language Inference from Wikipedia Categories", "abstract": "Accurate lexical entailment (LE) and natural language inference (NLI) often require large quantities of costly annotations. To alleviate the need for labeled data, we introduce WikiNLI: a resource for improving model performance on NLI and LE tasks. It contains 428,899 pairs of phrases constructed from naturally annotated category hierarchies in Wikipedia. We show that we can improve strong baselines such as BERT and RoBERTa by pretraining them on WikiNLI and transferring the models on downstream tasks. We conduct systematic comparisons with phrases extracted from other knowledge bases such as WordNet and Wikidata to find that pretraining on WikiNLI gives the best performance. In addition, we construct WikiNLI in other languages, and show that pretraining on them improves performance on NLI tasks of corresponding languages."}}
{"id": "ScU4Z-MpmWc", "cdate": 1577836800000, "mdate": 1646674424610, "content": {"title": "Controllable Paraphrasing and Translation with a Syntactic Exemplar", "abstract": "Most prior work on exemplar-based syntactically controlled paraphrase generation relies on automatically-constructed large-scale paraphrase datasets, which are costly to create. We sidestep this prerequisite by adapting models from prior work to be able to learn solely from bilingual text (bitext). Despite only using bitext for training, and in near zero-shot conditions, our single proposed model can perform four tasks: controlled paraphrase generation in both languages and controlled machine translation in both language directions. To evaluate these tasks quantitatively, we create three novel evaluation datasets. Our experimental results show that our models achieve competitive results on controlled paraphrase generation and strong performance on controlled machine translation. Analysis shows that our models learn to disentangle semantics and syntax in their latent representations, but still suffer from semantic drift."}}
{"id": "SBrWbWz6QZ9", "cdate": 1577836800000, "mdate": 1646674424607, "content": {"title": "Learning Probabilistic Sentence Representations from Paraphrases", "abstract": "Mingda Chen, Kevin Gimpel. Proceedings of the 5th Workshop on Representation Learning for NLP. 2020."}}
{"id": "BxZbbz6mW5", "cdate": 1577836800000, "mdate": 1646674424577, "content": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT."}}
{"id": "B_-b-bGTXbc", "cdate": 1577836800000, "mdate": 1646674424600, "content": {"title": "Mining Knowledge for Natural Language Inference from Wikipedia Categories", "abstract": "Mingda Chen, Zewei Chu, Karl Stratos, Kevin Gimpel. Findings of the Association for Computational Linguistics: EMNLP 2020. 2020."}}
