{"id": "S1xcHEHxIr", "cdate": 1567802434179, "mdate": null, "content": {"title": "The Label Complexity of Active Learning from Observational Data", "abstract": "Counterfactual learning from observational data involves learning a classifier on an entire population based on data that is observed conditioned on a selection policy. This work considers this problem in an active setting, where the learner additionally has access to unlabeled examples and can choose to get a subset of these labeled by an oracle.   Prior work on this problem uses disagreement-based active learning, along with an importance weighted loss estimator to account for counterfactuals, which leads to a high label complexity. We show how to instead incorporate a more efficient counterfactual risk minimizer into the active learning algorithm. This requires us to modify both the counterfactual risk to make it amenable to active learning, as well as the active learning process to make it amenable to the risk. We provably demonstrate that the result of this is an algorithm which is statistically consistent as well as more label-efficient than prior work."}}
{"id": "HJ-1KnW_ZH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Active Learning with Logged Data", "abstract": "We consider active learning with logged data, where labeled examples are drawn conditioned on a predetermined logging policy, and the goal is to learn a classifier on the entire population, not jus..."}}
{"id": "S14xxuWObH", "cdate": 1483228800000, "mdate": null, "content": {"title": "Revisiting Perceptron: Efficient and Label-Optimal Learning of Halfspaces", "abstract": "It has been a long-standing problem to efficiently learn a halfspace using as few labels as possible in the presence of noise. In this work, we propose an efficient Perceptron-based algorithm for actively learning homogeneous halfspaces under the uniform distribution over the unit sphere. Under the bounded noise condition~\\cite{MN06}, where each label is flipped with probability at most $\\eta &lt; \\frac 1 2$, our algorithm achieves a near-optimal label complexity of $\\tilde{O}\\left(\\frac{d}{(1-2\\eta)^2}\\ln\\frac{1}{\\epsilon}\\right)$ in time $\\tilde{O}\\left(\\frac{d^2}{\\epsilon(1-2\\eta)^3}\\right)$. Under the adversarial noise condition~\\cite{ABL14, KLS09, KKMS08}, where at most a $\\tilde \\Omega(\\epsilon)$ fraction of labels can be flipped, our algorithm achieves a near-optimal label complexity of $\\tilde{O}\\left(d\\ln\\frac{1}{\\epsilon}\\right)$ in time $\\tilde{O}\\left(\\frac{d^2}{\\epsilon}\\right)$. Furthermore, we show that our active learning algorithm can be converted to an efficient passive learning algorithm that has near-optimal sample complexities with respect to $\\epsilon$ and $d$."}}
{"id": "ByZhCdZdWB", "cdate": 1451606400000, "mdate": null, "content": {"title": "Active Learning from Imperfect Labelers", "abstract": "We study active learning where the labeler can not only return incorrect labels but also abstain from labeling. We consider different noise and abstention conditions of the labeler. We propose an algorithm which utilizes abstention responses, and analyze its statistical consistency and query complexity under fairly natural assumptions on the noise and abstention rate of the labeler. This algorithm is adaptive in a sense that it can automatically request less queries with a more informed or less noisy labeler. We couple our algorithm with lower bounds to show that under some technical conditions, it achieves nearly optimal query complexity."}}
