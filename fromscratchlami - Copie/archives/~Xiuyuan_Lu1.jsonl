{"id": "xampQmrqD8U", "cdate": 1676827072215, "mdate": null, "content": {"title": "Approximate Thompson Sampling via Epistemic Neural Networks", "abstract": "Thompson sampling (TS) is a popular heuristic for action selection, but it requires sampling from a posterior distribution.\nUnfortunately, this can become computationally intractable in complex environments, such as those modeled using neural networks.\nApproximate posterior samples can produce effective actions, but only if they reasonably approximate joint predictive distributions of outputs across inputs.\nNotably, accuracy of marginal predictive distributions does not suffice.\nEpistemic neural networks (ENNs) are designed to produce accurate joint predictive distributions.\nWe compare a range of ENNs through computational experiments that assess their performance in approximating TS across bandit and reinforcement learning environments.  \nThe results indicate that ENNs serve this purpose well and illustrate how the quality of joint predictive distributions drives performance.\nFurther, we demonstrate that the \\textit{epinet} --- a small additive network that estimates uncertainty --- matches the performance of large ensembles at orders of magnitude lower computational cost.\nThis enables effective application of TS with computation that scales gracefully to complex environments."}}
{"id": "KUBjrkUTp_", "cdate": 1672531200000, "mdate": 1681712943103, "content": {"title": "Approximate Thompson Sampling via Epistemic Neural Networks", "abstract": "Thompson sampling (TS) is a popular heuristic for action selection, but it requires sampling from a posterior distribution. Unfortunately, this can become computationally intractable in complex environments, such as those modeled using neural networks. Approximate posterior samples can produce effective actions, but only if they reasonably approximate joint predictive distributions of outputs across inputs. Notably, accuracy of marginal predictive distributions does not suffice. Epistemic neural networks (ENNs) are designed to produce accurate joint predictive distributions. We compare a range of ENNs through computational experiments that assess their performance in approximating TS across bandit and reinforcement learning environments. The results indicate that ENNs serve this purpose well and illustrate how the quality of joint predictive distributions drives performance. Further, we demonstrate that the \\textit{epinet} -- a small additive network that estimates uncertainty -- matches the performance of large ensembles at orders of magnitude lower computational cost. This enables effective application of TS with computation that scales gracefully to complex environments."}}
{"id": "2i1vkghTOl0", "cdate": 1664580906391, "mdate": 1664580906391, "content": {"title": "From Predictions to Decisions: The Importance of Joint Predictive Distributions", "abstract": "A fundamental challenge for any intelligent system is prediction: given some inputs, can you predict corresponding outcomes? Most work on supervised learning has focused on producing accurate marginal predictions for each input. However, we show that for a broad class of decision problems, accurate joint predictions are required to deliver good performance. In particular, we establish several results pertaining to combinatorial decision problems, sequential predictions, and multi-armed bandits to elucidate the essential role of joint predictive distributions. Our treatment of multi-armed bandits introduces an approximate Thompson sampling algorithm and analytic techniques that lead to a new kind of regret bound."}}
{"id": "c6ibx0yl-aG", "cdate": 1652737604141, "mdate": null, "content": {"title": "An Analysis of Ensemble Sampling", "abstract": "Ensemble sampling serves as a practical approximation to Thompson sampling when maintaining an exact posterior distribution over model parameters is computationally intractable. In this paper, we establish a regret bound that ensures desirable behavior when ensemble sampling is applied to the linear bandit problem. This represents the first rigorous regret analysis of ensemble sampling and is made possible by leveraging information-theoretic concepts and novel analytic techniques that may prove useful beyond the scope of this paper."}}
{"id": "JyTT03dqCFD", "cdate": 1652737585404, "mdate": null, "content": {"title": "The Neural Testbed: Evaluating Joint Predictions", "abstract": "\nPredictive distributions quantify uncertainties ignored by point estimates. This paper introduces The Neural Testbed: an open source benchmark for controlled and principled evaluation of agents that generate such predictions. Crucially, the testbed assesses agents not only on the quality of their marginal predictions per input, but also on their joint predictions across many inputs. We evaluate a range of agents using a simple neural network data generating process.\n\nOur results indicate that some popular Bayesian deep learning agents do not fare well with joint predictions, even when they can produce accurate marginal predictions. We also show that the quality of joint predictions drives performance in downstream decision tasks. We find these results are robust across choice a wide range of generative models, and highlight the practical importance of joint predictions to the community."}}
{"id": "rFb8y8Io5e9", "cdate": 1646077511404, "mdate": null, "content": {"title": "Evaluating High-Order Predictive Distributions in Deep Learning", "abstract": "Most work on supervised learning research has focused on marginal predictions.\nIn decision problems, joint predictive distributions are essential for good performance.\nPrevious work has developed methods for assessing low-order predictive distributions with inputs sampled i.i.d. from the testing distribution.\nWith low-dimensional inputs, these methods distinguish agents that effectively estimate uncertainty from those that do not.\nWe establish that the predictive distribution order required for such differentiation increases greatly with input dimension, rendering these methods impractical.\nTo accommodate high-dimensional inputs, we introduce \\textit{dyadic sampling}, which focuses on predictive distributions associated with random \\textit{pairs} of inputs.\nWe demonstrate that this approach efficiently distinguishes agents in high-dimensional examples involving simple logistic regression as well as complex synthetic and empirical data."}}
{"id": "omz-x0gsPK", "cdate": 1640995200000, "mdate": 1664580035721, "content": {"title": "An Analysis of Ensemble Sampling", "abstract": "Ensemble sampling serves as a practical approximation to Thompson sampling when maintaining an exact posterior distribution over model parameters is computationally intractable. In this paper, we establish a Bayesian regret bound that ensures desirable behavior when ensemble sampling is applied to the linear bandit problem. This represents the first rigorous regret analysis of ensemble sampling and is made possible by leveraging information-theoretic concepts and novel analytic techniques that may prove useful beyond the scope of this paper."}}
{"id": "e18MSgDAAK4", "cdate": 1640995200000, "mdate": 1681712943057, "content": {"title": "Robustness of Epinets against Distributional Shifts", "abstract": "Recent work introduced the epinet as a new approach to uncertainty modeling in deep learning. An epinet is a small neural network added to traditional neural networks, which, together, can produce predictive distributions. In particular, using an epinet can greatly improve the quality of joint predictions across multiple inputs, a measure of how well a neural network knows what it does not know. In this paper, we examine whether epinets can offer similar advantages under distributional shifts. We find that, across ImageNet-A/O/C, epinets generally improve robustness metrics. Moreover, these improvements are more significant than those afforded by even very large ensembles at orders of magnitude lower computational costs. However, these improvements are relatively small compared to the outstanding issues in distributionally-robust deep learning. Epinets may be a useful tool in the toolbox, but they are far from the complete solution."}}
{"id": "KGINR95oKa", "cdate": 1640995200000, "mdate": 1681712942975, "content": {"title": "Evaluating high-order predictive distributions in deep learning", "abstract": "Most work on supervised learning research has focused on marginal predictions. In decision problems, joint predictive distributions are essential for good performance. Previous work has developed m..."}}
{"id": "A87CVY-WSZ", "cdate": 1640995200000, "mdate": 1681712942915, "content": {"title": "Evaluating High-Order Predictive Distributions in Deep Learning", "abstract": "Most work on supervised learning research has focused on marginal predictions. In decision problems, joint predictive distributions are essential for good performance. Previous work has developed methods for assessing low-order predictive distributions with inputs sampled i.i.d. from the testing distribution. With low-dimensional inputs, these methods distinguish agents that effectively estimate uncertainty from those that do not. We establish that the predictive distribution order required for such differentiation increases greatly with input dimension, rendering these methods impractical. To accommodate high-dimensional inputs, we introduce \\textit{dyadic sampling}, which focuses on predictive distributions associated with random \\textit{pairs} of inputs. We demonstrate that this approach efficiently distinguishes agents in high-dimensional examples involving simple logistic regression as well as complex synthetic and empirical data."}}
