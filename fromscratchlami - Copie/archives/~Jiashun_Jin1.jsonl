{"id": "iN3Lh-Vy2TH", "cdate": 1663849889281, "mdate": null, "content": {"title": "Phase transition for detecting a small community in a large network", "abstract": "How to detect a small community in a large network is an interesting problem, including clique detection as a special case, where a naive degree-based $\\chi^2$-test was shown to be powerful in the presence of an Erd\u00f6s-Renyi (ER) background. Using Sinkhorn's theorem, we show that the signal captured by the $\\chi^2$-test may be a modeling artifact, and it may disappear once we replace the Erd\u00f6s-Renyi model by a broader network model.  We show that the recent SgnQ test is more appropriate for such a setting. The test is optimal in detecting communities with sizes comparable to the whole network, but has never been studied for our setting, which is substantially different and more challenging.  Using a degree-corrected block model (DCBM), we establish phase transitions of this testing problem concerning the size of the small community and the edge densities in small and large communities.  When the size of the small community is larger than $\\sqrt{n}$, the SgnQ test is optimal  for it attains the computational lower bound (CLB), the information lower bound for methods allowing polynomial computation time. When the size of the small community is smaller than $\\sqrt{n}$, we establish the parameter regime where the SgnQ test has full power and make some conjectures of the CLB. We also study the classical information lower bound (LB) and show that there is always a gap between the CLB and LB in our range of interest. \t"}}
{"id": "fcMd-tuWwiO", "cdate": 1652737423421, "mdate": null, "content": {"title": "A sharp NMF result with applications in network modeling  ", "abstract": "Given an $n \\times n$  non-negative rank-$K$ matrix $\\Omega$ where $m$ eigenvalues are negative, when can we write $\\Omega = Z P Z'$ for non-negative matrices $Z \\in \\mathbb{R}^{n, K}$ and $P \\in \\mathbb{R}^{K, K}$?  While most existing works focused on the case of $m = 0$, our primary interest is on the case of  general $m$. With new proof ideas we develop, we present sharp results on when the NMF problem is solvable, which significantly extend existing results on this topic. The NMF problem is partially motivated by applications in network modeling.   For a network with $K$ communities,  rank-$K$ models are popular, with many proposals. The DCMM model is \na recent rank-$K$ model which is especially useful and interpretable in practice. To enjoy such properties,  it is of interest to study \nwhen a rank-$K$ model can be rewritten as a DCMM model. Using our NMF results, we show that for a rank-$K$ model with parameters in the most interesting range, we can always rewrite it as a DCMM model.  "}}
{"id": "bYIddUC7AYO", "cdate": 1621629905921, "mdate": null, "content": {"title": "Sharp Impossibility Results for Hyper-graph Testing", "abstract": "In a broad Degree-Corrected Mixed-Membership (DCMM) setting, we test whether a non-uniform hypergraph has only one community or has multiple communities. Since both the null and alternative hypotheses have many unknown parameters, \nthe challenge is, given an alternative, how to identify the null that is hardest to separate from the alternative. We approach this by proposing a degree matching strategy where the main idea is leveraging the theory for tensor scaling to create a least favorable pair of hypotheses. We present a  result on standard  minimax lower bound theory and a result on Region of Impossibility (which is more informative than the minimax lower bound). We show that our lower bounds are tight by introducing a new test that attains the lower bound up to a logarithmic factor. We also discuss the case where the hypergraphs may have mixed-memberships."}}
{"id": "asOAwZwrwe", "cdate": 1514764800000, "mdate": null, "content": {"title": "Network Global Testing by Counting Graphlets", "abstract": "Consider a large social network with possibly severe degree heterogeneity and mixed-memberships. We are interested in testing whether the network has only one community or there are more than one c..."}}
{"id": "US2nE4Nqoe", "cdate": 1514764800000, "mdate": null, "content": {"title": "SCORE+ for Network Community Detection", "abstract": "A network may have weak signals and severe degree heterogeneity, and may be very sparse in one occurrence but very dense in another. SCORE (Jin, 2015) is a recent approach to network community detection. It accommodates severe degree heterogeneity and is adaptive to different levels of sparsity, but its performance for networks with weak signals is unclear. In this paper, we show that in a broad class of network settings where we allow for weak signals, severe degree heterogeneity, and a wide range of network sparsity, SCORE achieves prefect clustering and has the so-called \"exponential rate\" in Hamming clustering errors. The proof uses the most recent advancement on entry-wise bounds for the leading eigenvectors of the network adjacency matrix.   The theoretical analysis assures us that SCORE continues to work well in the weak signal settings, but it does not rule out the possibility that SCORE may be further improved to have better performance in real applications, especially for networks with weak signals. As a second contribution of the paper, we propose SCORE+ as an improved version of SCORE. We investigate SCORE+ with 8 network data sets and found that it outperforms several representative approaches. In particular, for the 6 data sets with relatively strong signals, SCORE+ has similar performance as that of SCORE, but for the 2 data sets (Simmons, Caltech) with possibly weak signals, SCORE+ has much lower error rates. SCORE+ proposes several changes to SCORE. We carefully explain the rationale underlying each of these changes, using a mixture of theoretical and numerical study."}}
{"id": "SfbfUSQqxFZ", "cdate": 1514764800000, "mdate": null, "content": {"title": "Matrix Masking", "abstract": "Adding noise; Data perturbation; Recodings; Sampling; Synthetic data Matrix Maskingrefers to a class of statistical disclosure limitation (SDL) methods used to protect confidentiality of statistical..."}}
{"id": "CfdKpndjrwC", "cdate": 1514764800000, "mdate": null, "content": {"title": "Statistical Disclosure Limitation for~Data~Access", "abstract": "Confidentiality protection; Multiplicity; Privacy protection; Restricted data; Risk-utility tradeoff\nStatistical Disclosure Limitation refers to the broad array of methods used to protect confidentiality of statistical data, i.e., fulfilling an obligation to data providers or respondents not to transmit their information to an unauthorized party. Data Access refers to complementary obligations of statistical agencies and others to provide information for statistical purposes without violating promises of confidentiality.\nStarting in the early twentieth century, U.S. government statistical agencies worked to develop approaches for the protection of the confidentiality of data gathered on individuals and organizations. As such agencies also have a public obligation to use the data for the public good, they have developed both a culture of confidentiality protection and a set of statistical techniques to assure that data are released in a form..."}}
{"id": "pHjQbWwkwoj", "cdate": 1451606400000, "mdate": null, "content": {"title": "Component-wise gradient boosting and false discovery control in survival analysis with high-dimensional covariates", "abstract": "Technological advances that allow routine identification of high-dimensional risk factors have led to high demand for statistical techniques that enable full utilization of these rich sources of information for genetics studies. Variable selection for censored outcome data as well as control of false discoveries (i.e. inclusion of irrelevant variables) in the presence of high-dimensional predictors present serious challenges. This article develops a computationally feasible method based on boosting and stability selection. Specifically, we modified the component-wise gradient boosting to improve the computational feasibility and introduced random permutation in stability selection for controlling false discoveries."}}
{"id": "pBT5lvQFnja", "cdate": 1388534400000, "mdate": null, "content": {"title": "Optimality of graphlet screening in high dimensional variable selection", "abstract": "Consider a linear model Y = X\u03b2+\u03c9z, where X has n rows and p columns and z - N(0, In). We assume both p and n are large, including the case of p \u226b n. The unknown signal vector \u03b2 is assumed to be sparse in the sense that only a small fraction of its components is nonzero. The goal is to identify such nonzero coordinates (i.e., variable selection). We are primarily interested in the regime where signals are both rare and weak so that successful variable selection is challenging but is still possible. We assume the Gram matrix G = X\u2032X is sparse in the sense that each row has relatively few large entries (diagonals of G are normalized to 1). The sparsity of G naturally induces the sparsity of the so-called Graph of Strong Dependence (GOSD). The key insight is that there is an interesting interplay between the signal sparsity and graph sparsity: in a broad context, the signals decompose into many small-size components of GOSD that are disconnected to each other. We propose Graphlet Screening for variable selection. This is a two-step Screen and Clean procedure, where in the first step, we screen subgraphs of GOSD with sequential \u03c72-tests, and in the second step, we clean with penalized MLE. The main methodological innovation is to use GOSD to guide both the screening and cleaning processes. For any variable selection procedure \u03b2, we measure its performance by the Hamming distance between the sign vectors of \u03b2 and \u03b2, and assess the optimality by the minimax Hamming distance. Compared with more stringent criteria such as exact support recovery or oracle property, which demand strong signals, the Hamming distance criterion is more appropriate for weak signals since it naturally allows a small fraction of errors. We show that in a broad class of situations, Graphlet Screening achieves the optimal rate of convergence in terms of the Hamming distance. Unlike Graphlet Screening, well-known procedures such as the L0/L1-penalization methods do not utilize local graphic structure for variable selection, so they generally do not achieve the optimal rate of convergence, even in very simple settings and even if the tuning parameters are ideally set. The the presented algorithm is implemented as R-CRAN package ScreenClean and in matlab (available at http://www.stat.cmu.edu/~jiashun/Research/software/GS-matlab/)."}}
{"id": "n0ytJqI8B_y", "cdate": 1388534400000, "mdate": null, "content": {"title": "Coauthorship and Citation Networks for Statisticians", "abstract": "We have collected and cleaned two network data sets: Coauthorship and Citation networks for statisticians. The data sets are based on all research papers published in four of the top journals in statistics from $2003$ to the first half of $2012$. We analyze the data sets from many different perspectives, focusing on (a) centrality, (b) community structures, and (c) productivity, patterns and trends. For (a), we have identified the most prolific/collaborative/highly cited authors. We have also identified a handful of \"hot\" papers, suggesting \"Variable Selection\" as one of the \"hot\" areas. For (b), we have identified about $15$ meaningful communities or research groups, including large-size ones such as \"Spatial Statistics\", \"Large-Scale Multiple Testing\", \"Variable Selection\" as well as small-size ones such as \"Dimensional Reduction\", \"Objective Bayes\", \"Quantile Regression\", and \"Theoretical Machine Learning\". For (c), we find that over the 10-year period, both the average number of papers per author and the fraction of self citations have been decreasing, but the proportion of distant citations has been increasing. These suggest that the statistics community has become increasingly more collaborative, competitive, and globalized. Our findings shed light on research habits, trends, and topological patterns of statisticians. The data sets provide a fertile ground for future researches on or related to social networks of statisticians."}}
