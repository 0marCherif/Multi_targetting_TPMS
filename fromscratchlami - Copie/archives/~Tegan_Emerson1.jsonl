{"id": "VRJAsmqwemg", "cdate": 1664194166641, "mdate": null, "content": {"title": "Do Neural Networks Trained with Topological Features Learn Different Internal Representations?", "abstract": "There is a growing body of work that leverages features extracted via topological data analysis to train machine learning models. While this field, sometimes known as topological machine learning (TML), has seen some notable successes, an understanding of how the process of learning from topological features differs from the process of learning from raw data is still limited. In this work, we begin to address one component of this larger issue by asking whether a model trained with topological features learns internal representations of data that are fundamentally different than those learned by a model trained with the original raw data. To quantify \"different\", we exploit two popular metrics that can be used to measure the similarity of the hidden representations of data within neural networks, neural stitching and centered kernel alignment. From these we draw a range of conclusions about how training with topological features does and does not change the representations that a model learns. Perhaps unsurprisingly, we find that structurally, the hidden representations of models trained and evaluated on topological features differ substantially compared to those trained and evaluated on the corresponding raw data. On the other hand, our experiments show that in some cases, these representations can be reconciled (at least to the degree required to solve the corresponding task) using a simple affine transformation. We conjecture that this means that neural networks trained on raw data may extract some limited topological features in the process of making predictions."}}
{"id": "SCD0hn3kMHw", "cdate": 1652737614904, "mdate": null, "content": {"title": "In What Ways Are Deep Neural Networks Invariant and How Should We Measure This?", "abstract": "It is often said that a deep learning model is ``invariant'' to some specific type of transformation. However, what is meant by this statement strongly depends on the context in which it is made. In this paper we explore the nature of invariance and equivariance of deep learning models with the goal of better understanding the ways that they actually capture these concepts on a formal level. We introduce a family of invariance and equivariance metrics that allow us to quantify these properties in a way that disentangles them from other metrics such as loss or accuracy. We use our metrics to better understand the two most popular methods used to build invariance into networks, data augmentation and equivariant layers. We draw a range of conclusions about invariance and equivariance in deep learning models, ranging from whether initializing a model with pretrained weights has an effect on a trained model's invariance, to the extent to which invariance learned via training can generalize to out-of-distribution data."}}
{"id": "8qugS9JqAxD", "cdate": 1652737613565, "mdate": null, "content": {"title": "On the Symmetries of Deep Learning Models and their Internal Representations", "abstract": "Symmetry has been a fundamental tool in the exploration of a broad range of complex systems. In machine learning, symmetry has been explored in both models and data. In this paper we seek to connect the symmetries arising from the architecture of a family of models with the symmetries of that family\u2019s internal representation of data. We do this by calculating a set of fundamental symmetry groups, which we call the intertwiner groups of the model. Each of these arises from a particular nonlinear layer of the model and different nonlinearities result in different symmetry groups. These groups change the weights of a model in such a way that the underlying function that the model represents remains constant but the internal representations of data inside the model may change. We connect intertwiner groups to a model\u2019s internal representations of data through a range of experiments that probe similarities between hidden states across models with the same architecture. Our work suggests that the symmetries of a network are propagated into the symmetries in that network\u2019s representation of data, providing us with a better understanding of how architecture affects the learning and prediction process. Finally, we speculate that for ReLU networks, the intertwiner groups may provide a justification for the common practice of concentrating model interpretability exploration on the activation basis in hidden layers rather than arbitrary linear combinations thereof."}}
{"id": "Bq8z34ZkTlc", "cdate": 1646223667876, "mdate": null, "content": {"title": "TopTemp: Parsing Precipitate Structure from Temper Topology", "abstract": "Technological advances are in part enabled by the development of novel manufacturing processes that give rise to new materials or material property improvements. Development and evaluation of new manufacturing methodologies is labor-, time-, and resource-intensive expensive due to complex, poorly defined relationships between advanced manufacturing process parameters and the resulting microstructures. In this work, we present a topological representation of temper (heat-treatment) dependent material micro-structure, as captured by scanning electron microscopy, called TopTemp. We show that this topological representation is able to support temper classification of microstructures in a data limited setting, generalizes well to previously unseen samples, is robust to image perturbations, and captures domain interpretable features. The presented work outperforms conventional deep learning baselines and is a first step towards improving understanding of process parameters and resulting material properties."}}
{"id": "BcLe3E-kpxq", "cdate": 1646223667755, "mdate": null, "content": {"title": "Random Filters for Enriching the Discriminatory Power of Topological Representations", "abstract": "Topological representations of data are inherently coarse summaries which endows them with certain desirable properties like stability but also potentially inhibits their discriminatory power relative to fine-scale learned features. In this work we present a novel framework for enriching the discriminatory power of topological representations based on random filters and capturing \u201cinterference topology\u201d rather than direct topology. We show that our random filters outperform previously explored structured image filters while requiring orders of magnitude\nless computational time. The approach is demonstrated on the MNIST dataset but is broadly applicable across data sets and modalities. This work is concluded with a discussion of the mathematical intuition underlying the approach and identification of future directions to enable deeper understanding and theoretical results."}}
{"id": "HcIgi4Zkpe9", "cdate": 1646223666627, "mdate": null, "content": {"title": "Fiber Bundle Morphisms as a Framework for Modeling Many-to-Many Maps", "abstract": "While it is not generally reflected in the `nice' datasets used for benchmarking machine learning algorithms, the real-world is full of processes that would be best described as many-to-many. That is, a single input can potentially yield many different outputs (whether due to noise, imperfect measurement, or intrinsic stochasticity in the process) and many different inputs can yield the same output (that is, the map is not injective). For example, imagine a sentiment analysis task where, due to linguistic ambiguity, a single statement can have a range of different sentiment interpretations while at the same time many distinct statements can represent the same sentiment. When modeling such a multivalued function $f: X \\rightarrow Y$, it is frequently useful to be able to model the distribution on $f(x)$ for specific input $x$ as well as the distribution on fiber $f^{-1}(y)$ for specific output $y$. Such an analysis helps the user (i) better understand the variance intrinsic to the process they are studying and (ii) understand the range of specific input $x$ that can be used to achieve output $y$. Following existing work which used a fiber bundle framework to better model many-to-one processes, we describe how morphisms of fiber bundles provide a template for building models which naturally capture the structure of many-to-many processes."}}
{"id": "SbCndr5Yu6T", "cdate": 1636366601807, "mdate": null, "content": {"title": "Differential Property Prediction: A Machine Learning Approach to Experimental Design in Advanced Manufacturing", "abstract": "Advanced manufacturing techniques have enabled the production of materials with state-of-the-art properties. In many cases however, the development of physics-based models of these techniques lags behind their use in the lab. This means that designing and running experiments proceeds largely via trial and error. This is sub-optimal since experiments are cost-, time-, and labor-intensive. In this work we propose a machine learning framework, differential property classification (DPC), which enables an experimenter to leverage machine learning's unparalleled pattern matching capability to pursue data-driven experimental design. DPC takes two possible experiment parameter sets and outputs a prediction of which will produce a material with a more desirable property specified by the operator. We demonstrate the success of DPC on AA7075 tube manufacturing process and mechanical property data using shear assisted processing and extrusion (ShAPE), a solid phase processing technology. We show that by focusing on the experimenter's need to choose between multiple candidate experimental parameters, we can reframe the challenging regression task of predicting material properties from processing parameters, into a classification task on which machine learning models can achieve good performance."}}
{"id": "SoL4OeSluTr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Motion Segmentation via Generalized Curvatures.", "abstract": "New depth sensors, like the Microsoft Kinect, produce streams of human pose data. These discrete pose streams can be viewed as noisy samples of an underlying continuous ideal curve that describes a trajectory through high-dimensional pose space. This paper introduces a technique for generalized curvature analysis (GCA) that determines features along the trajectory which can be used to characterize change and segment motion. Tools are developed for approximating generalized curvatures at mean points along a curve in terms of the singular values of local mean-centered data balls. The features of the GCA algorithm are illustrated on both synthetic and real examples, including data collected from a Kinect II sensor. We also applied GCA to the Carnegie Mellon University Motion Capture (MoCaP) database. Given that GCA scales linearly with the length of the time series we are able to analyze large data sets without down sampling. It is demonstrated that the generalized curvature approximations can be used to segment pose streams into motions and transitions between motions. The GCA algorithm can identify 94.2 percent of the transitions between motions without knowing the set of possible motions in advance, even though the subjects do not stop or pause between motions."}}
{"id": "B7N45-Xe_6S", "cdate": 1546300800000, "mdate": null, "content": {"title": "Image Recovery in the Infrared Domain via Path-Augmented Compressive Sampling Matching Pursuit.", "abstract": "We consider compressive sensing as a means of acquiring high-resolution images from low-cost, low-resolution sensors in the infrared domain. In particular, we reduce errors arising from basis mismatch between the observed image and the signal model by modifying a baseline matching pursuit recovery algorithm. Specifically, we introduce a modification to the analysis step which seeks to find more representative image atoms by searching over a 2-Wasserstein geodesic formed between the two most-correlated atoms at that step. We test our extension by quantifying recovery performance on an ensemble of representative infrared maritime scenes and find improvement over baseline when measured using PSNR, SSIM, and a metric that quantifies global edge recovery performance. We find that the most notable gains occur for very low sparsity levels which favors reduced computational load for the recovery."}}
{"id": "B1Whw6Z_Wr", "cdate": 1514764800000, "mdate": null, "content": {"title": "Path Orthogonal Matching Pursuit for Sparse Reconstruction and Denoising of SWIR Maritime Imagery", "abstract": "We introduce an extension that may be used to augment algorithms used for the sparse decomposition of signals into a linear combination of atoms drawn from a dictionary such as those used in support of, for example, compressive sensing, k-sparse representation, and denoising. Our augmentation may be applied to any reconstruction algorithm that relies on the selection and sorting of high-correlation atoms during an analysis or identification phase by generating a \"path\" between the two highest-correlation atoms. Here we investigate two types of path: a linear combination (Euclidean geodesic) and a construction relying on an optimal transport map (2-Wasserstein geodesic). We test our extension by performing image denoising and k-sparse representation using atoms from a learned overcomplete kSVD dictionary. We study the application of our techniques on SWIR imagery of maritime vessels and show that our methods outperform orthogonal matching pursuit. We conclude that these methods, having shown success in our two tested problem domains, will also be useful for reducing \"basis mismatch\" error that arises in the recovery of compressively sampled images."}}
