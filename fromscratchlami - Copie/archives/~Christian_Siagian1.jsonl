{"id": "BJ4ZtAZuWB", "cdate": 1420070400000, "mdate": null, "content": {"title": "Fixation bank: Learning to reweight fixation candidates", "abstract": "Predicting where humans will fixate in a scene has many practical applications. Biologically-inspired saliency models decompose visual stimuli into feature maps across multiple scales, and then integrate different feature channels, e.g., in a linear, MAX, or MAP. However, to date there is no universally accepted feature integration mechanism. Here, we propose a new a data-driven solution: We first build a \u201cfixation bank\u201d by mining training samples, which maintains the association between local patterns of activation, in 4 feature channels (color, intensity, orientation, motion) around a given location, and corresponding human fixation density at that location. During testing, we decompose feature maps into blobs, extract local activation patterns around each blob, match those patterns against the fixation bank by group lasso, and determine weights of blobs based on reconstruction errors. Our final saliency map is the weighted sum of all blobs. Our system thus incorporates some amount of spatial and featural context information into the location-dependent weighting mechanism. Tested on two standard data sets (DIEM for training and test, and CRCNS for test only; total of 23,670 training and 15,793 + 4,505 test frames), our model slightly but significantly outperforms 7 state-of-the-art saliency models."}}
{"id": "BkVCiYW_WS", "cdate": 1388534400000, "mdate": null, "content": {"title": "A System for Assisting the Visually Impaired in Localization and Grasp of Desired Objects", "abstract": "A prototype wearable visual aid for helping visually impaired people find desired objects in their environment is described. The system is comprised of a head-worn camera to capture the scene, an Android phone interface to specify a desired object, and an attention-biasing-enhanced object recognition algorithm to identify three most likely object candidate regions, select the best-matching one, and pass its location to an object tracking algorithm. The object is tracked as the user\u2019s head moves, and auditory feedback is provided to help the user maintain the object in the field of view, enabling easy reach and grasp. The implementation and integration of the system leading to testing of the working prototype with visually-impaired subjects at the Braille Institute in Los Angeles (demonstration in the accompanying video) is described. Results indicate that this system has clear potential to help visually-impaired users in achieving near-real-time object localization and grasp."}}
{"id": "B7VPWCre_pH", "cdate": 1167609600000, "mdate": null, "content": {"title": "Rapid Biologically-Inspired Scene Classification Using Features Shared with Visual Attention.", "abstract": "We describe and validate a simple context-based scene recognition algorithm for mobile robotics applications. The system can differentiate outdoor scenes from various sites on a college campus using a multiscale set of early-visual features, which capture the \"gist\" of the scene into a low-dimensional signature vector. Distinct from previous approaches, the algorithm presents the advantage of being biologically plausible and of having low-computational complexity, sharing its low-level features with a model for visual attention that may operate concurrently on a robot. We compare classification accuracy using scenes filmed at three outdoor sites on campus (13,965 to 34,711 frames per site). Dividing each site into nine segments, we obtain segment classification rates between 84.21 percent and 88.62 percent. Combining scenes from all sites (75,073 frames in total) yields 86.45 percent correct classification, demonstrating the generalization and scalability of the approach"}}
{"id": "B14d1CZ_ZH", "cdate": 1104537600000, "mdate": null, "content": {"title": "Gist: A Mobile Robotics Application of Context-Based Vision in Outdoor Environment", "abstract": "We present context-based scene recognition for mobile robotics applications. Our classifier is able to differentiate outdoor scenes without temporal filtering relatively well from a variety of locations at a college campus using a set of features that together capture the \"gist\" of the scene. We compare the classification accuracy of a set of scenes from 1551 frames filmed outdoors along a path and dividing them to four and twelve different legs while obtaining a classifi- cation rate of 67.96 percent and 48.61 percent, respectively. We also tested the scalability of the features by comparing the classification results from the previous scenes with four legs with a longer path with eleven legs while obtaining a classification rate of 55.08 percent. In the end, some ideas are put forth to improve the theoretical strength of the gist features."}}
{"id": "H1XsgJG_ZH", "cdate": 1072915200000, "mdate": null, "content": {"title": "Biologically-Inspired Face Detection: Non-Brute-Force-Search Approach", "abstract": "We present a biologically-inspired face detection system. The system applies notions such as saliency, gist, and gaze to localize a face without performing blind spatial search. The saliency model consists of highly parallel low-level computations that operate in domains such as intensity, orientation, and color. It is used to direct attention to a set of conspicuous locations in an image as starting points. The gist model, computed in parallel with the saliency model, estimates holistic image characteristics such as dominant contours and magnitude in high and low spatial frequency bands. We are limiting its use to predicting the likely head size based on the entire scene. Also, instead of identifying face as a single entity, this system performs detection by parts and uses spatial configuration constraints to be robust against occlusion and perspective."}}
