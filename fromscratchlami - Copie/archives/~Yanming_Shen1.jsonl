{"id": "9KxmgfE7Cx", "cdate": 1686188698463, "mdate": 1686188698463, "content": {"title": "To Copy Rather Than Memorize: A Vertical Learning Paradigm for Knowledge Graph Completion", "abstract": "Embedding models have shown great power in knowledge graph completion (KGC) task. By learning structural constraints for each training triple, these methods implicitly memorize intrinsic relation rules to infer missing links. However, this paper points out that the multi-hop relation rules are hard to be reliably memorized due to the inherent deficiencies of such implicit memorization strategy, making embedding models underperform in predicting links between distant entity pairs. To alleviate this problem, we present Vertical Learning Paradigm (VLP), which extends embedding models by allowing to explicitly copy target information from related factual triples for more accurate prediction. Rather than solely relying on the implicit memory, VLP directly provides additional cues to improve the generalization ability of embedding models, especially making the distant link prediction significantly easier. Moreover, we also propose a novel relative distance based negative sampling technique (ReD) for more effective optimization. Experiments demonstrate the validity and generality of our proposals on two standard benchmarks. Our code is available at https://github.com/rui9812/VLP."}}
{"id": "qCHGS-4GOIq", "cdate": 1677628800000, "mdate": 1681689848120, "content": {"title": "STTG-TTE: spatial-temporal gated multi-modality approach for travel time estimation based on temporal convolutional networks", "abstract": "Travel time forecasting has become a core component of smart transportation systems, which assists both travelers and traffic organizers with route planning, travel schedule adjustments, ride-sharing, navigation applications, and efficient traffic management. However, timely and accurate travel time forecasting still remains a critical challenge owing to the complex nonlinear and dynamic fluctuations of spatial\u2013temporal dependencies. Also, spatial sparseness is a big issue in traffic forecasting, since adopting the implicit interactions between the close traffic regions leads to superficial characterization of spatio-temporal dependences. In this paper, we propose a new deep learning-based framework (STTG-TTE) that addresses these drawbacks and improves the travel time estimation. First, we build a geo-hashing algorithm for the data sparsity issue that incorporates fluctuations of nearby and distant traffic situations in terms of spatio-temporal dependencies. Second, a new spatio-temporal correlation modeling method is proposed to fully leverage large-scale spatial and temporal traffic patterns using temporal convolutional networks integrated with a gated multi-modality mechanism. Then, for external factors\u2019 representation, a new dual-gated Res-Net multi-modality-based module is proposed. Finally, we fuse these representations of multi-components dynamically and utilize the transformer model, which is conducive to learning intersections among these multiple factors for obtaining accurate prediction results. Experiments on two large-scale real-world traffic datasets from two different urban regions (Chengdu taxi-datsets and NYC-Bike datasets) demonstrate that the proposed model is superior to state-of-the-art baseline models."}}
{"id": "1MQkGJd0A2_", "cdate": 1675209600000, "mdate": 1681689848325, "content": {"title": "Travel time prediction based on route links' similarity", "abstract": "Accurate travel time prediction allows passengers to schedule their journeys efficiently. However, cyclical factors (time intervals of the day, weather conditions, and holidays), unpredictable factors (incidents, abnormal weather), and other complicated factors (dynamic traffic conditions, dwell times, and variation in travel demand) make accurate bus travel time prediction complicated. This paper aims to achieve accurate travel time prediction. To do so, we propose a clustering method that identifies travel time paradigms of different route links and clusters them based on their similarity using the nonnegative matrix factorization algorithm. Additionally, we propose a deep learning model based on CNN with spatial\u2013temporal attention and gating mechanisms to select the most relevant features and capture their dependencies and correlations. For each defined cluster, we train a separate model to predict the travel time at various time intervals over the day. As a result, the travel times of all journey links from related prediction models are aggregated to predict the total journey time. Extensive experiments using data collected from four different bus lines in Beijing show that our method outperforms the compared baselines."}}
{"id": "YP3gE31ne-F", "cdate": 1671866796124, "mdate": null, "content": {"title": "A New Perspective on the Effects of Spectrum in Graph Neural Networks", "abstract": "Many improvements on GNNs can be deemed as operations on the spectrum of the underlying graph matrix, which motivates us to directly study the characteristics of the spectrum and their effects on GNN performance. By generalizing most existing GNN architectures, we show that the correlation issue caused by the unsmooth spectrum becomes the obstacle to leveraging more powerful graph filters as well as developing deep architectures, which therefore restricts GNNs\u2019 performance. Inspired by this, we propose the correlation-free architecture which naturally removes the correlation issue among different channels, making it possible to utilize more sophisticated filters within each channel. The final correlation-free architecture with more powerful filters consistently boosts the performance of learning graph representations. Code is available at https://github.com/qslim/gnn-spectrum."}}
{"id": "v91CeKpam4x", "cdate": 1640995200000, "mdate": 1681689848117, "content": {"title": "NodeTrans: A Graph Transfer Learning Approach for Traffic Prediction", "abstract": "Recently, deep learning methods have made great progress in traffic prediction, but their performance depends on a large amount of historical data. In reality, we may face the data scarcity issue. In this case, deep learning models fail to obtain satisfactory performance. Transfer learning is a promising approach to solve the data scarcity issue. However, existing transfer learning approaches in traffic prediction are mainly based on regular grid data, which is not suitable for the inherent graph data in the traffic network. Moreover, existing graph-based models can only capture shared traffic patterns in the road network, and how to learn node-specific patterns is also a challenge. In this paper, we propose a novel transfer learning approach to solve the traffic prediction with few data, which can transfer the knowledge learned from a data-rich source domain to a data-scarce target domain. First, a spatial-temporal graph neural network is proposed, which can capture the node-specific spatial-temporal traffic patterns of different road networks. Then, to improve the robustness of transfer, we design a pattern-based transfer strategy, where we leverage a clustering-based mechanism to distill common spatial-temporal patterns in the source domain, and use these knowledge to further improve the prediction performance of the target domain. Experiments on real-world datasets verify the effectiveness of our approach."}}
{"id": "olZAWuGOUz", "cdate": 1640995200000, "mdate": 1648683971575, "content": {"title": "HousE: Knowledge Graph Embedding with Householder Parameterization", "abstract": "The effectiveness of knowledge graph embedding (KGE) largely depends on the ability to model intrinsic relation patterns and mapping properties. However, existing approaches can only capture some of them with insufficient modeling capacity. In this work, we propose a more powerful KGE framework named HousE, which involves a novel parameterization based on two kinds of Householder transformations: (1) Householder rotations to achieve superior capacity of modeling relation patterns; (2) Householder projections to handle sophisticated relation mapping properties. Theoretically, HousE is capable of modeling crucial relation patterns and mapping properties simultaneously. Besides, HousE is a generalization of existing rotation-based models while extending the rotations to high-dimensional spaces. Empirically, HousE achieves new state-of-the-art performance on five benchmark datasets. Our code is available at https://github.com/anrep/HousE."}}
{"id": "YAP2lX2keC", "cdate": 1640995200000, "mdate": 1681689848121, "content": {"title": "GSTA: gated spatial-temporal attention approach for travel time prediction", "abstract": "Accurate travel time prediction between two locations is one of the most substantial services in transport. In travel time prediction, origin\u2013destination (OD) method is more challenging since it has no intermediate trajectory points. This paper puts forward a deep learning-based model, called Gated Spatial\u2013Temporal Attention (GSTA), to optimize the OD travel time prediction. While many trip features are available, their relations and particular contributions to the output are usually unknown. To give our model the flexibility to select the most relevant features, we develop a feature selection module with an integration unit and a gating mechanism to pass or suppress the trip feature based on its contribution. To capture spatial\u2013temporal dependencies and correlations in the short and long term, we propose a new pair-wise attention mechanism with spatial inference and temporal reasoning. In addition, we adapt and integrate multi-head attention to improve model performance in case of sophisticated dependencies in long term. Extensive experiments on two large taxi datasets in New York City, USA, and Chengdu, China demonstrate the superiority of our model in comparison with other models."}}
{"id": "VyiaGR5VkX", "cdate": 1640995200000, "mdate": 1681689823916, "content": {"title": "A New Perspective on the Effects of Spectrum in Graph Neural Networks", "abstract": "Many improvements on GNNs can be deemed as operations on the spectrum of the underlying graph matrix, which motivates us to directly study the characteristics of the spectrum and their effects on G..."}}
{"id": "VcDHN35_7pp", "cdate": 1640995200000, "mdate": 1681689823880, "content": {"title": "TCSA-Net: A Temporal-Context-Based Self-Attention Network for Next Location Prediction", "abstract": "Next location prediction aims to find the location that the user will visit next. It plays a fundamental role for location-based applications. However, the heterogeneity and sparsity of the trajectory data pose great challenges to the task. Recently, RNN-based methods have shown promising performance in learining the spatio-temporal characteristics of the trajectory. While the effectiveness of location prediction has been improved, the computational efficiency and the long-term preferences still leave space for further research. The self-attention mechanism is viewed as a promising solution for parallel computation and exploiting sequential regularities from sparse data. But the huge memory cost and the neglect of temporal information make it infeasible to directly modeling human mobility regularities. In this paper, we propose a temporal-context-based self-attention network named TCSA-Net, which can simultaneously exploit long- and short-term mvoement preferences from sparse and long trajectories. In particular, we design a novel two-stage self-attention architecture that can learn long-term dependency under constrained memory budget. Further, we propose a multi-modal embedding layer to model two complementary temporal contexts and provide more abundant temporal and sequential information. Extensive experiments on two real-life datasets show that the TCSA-Net significantly outperforms the state-of-the-art methods in terms of standard evaluation metrics."}}
{"id": "KP9ZP0p0FHe", "cdate": 1640995200000, "mdate": 1681689823880, "content": {"title": "Soft-mask: Adaptive Substructure Extractions for Graph Neural Networks", "abstract": "For learning graph representations, not all detailed structures within a graph are relevant to the given graph tasks. Task-relevant structures can be $localized$ or $sparse$ which are only involved in subgraphs or characterized by the interactions of subgraphs (a hierarchical perspective). A graph neural network should be able to efficiently extract task-relevant structures and be invariant to irrelevant parts, which is challenging for general message passing GNNs. In this work, we propose to learn graph representations from a sequence of subgraphs of the original graph to better capture task-relevant substructures or hierarchical structures and skip $noisy$ parts. To this end, we design soft-mask GNN layer to extract desired subgraphs through the mask mechanism. The soft-mask is defined in a continuous space to maintain the differentiability and characterize the weights of different parts. Compared with existing subgraph or hierarchical representation learning methods and graph pooling operations, the soft-mask GNN layer is not limited by the fixed sample or drop ratio, and therefore is more flexible to extract subgraphs with arbitrary sizes. Extensive experiments on public graph benchmarks show that soft-mask mechanism brings performance improvements. And it also provides interpretability where visualizing the values of masks in each layer allows us to have an insight into the structures learned by the model."}}
