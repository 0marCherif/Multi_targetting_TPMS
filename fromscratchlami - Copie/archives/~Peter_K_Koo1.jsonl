{"id": "ITOQhccyRsk", "cdate": 1632499908482, "mdate": null, "content": {"title": "Uncovering motif interactions from convolutional-attention networks for genomics", "abstract": "A major goal of computational genomics is to understand how sequence patterns, called motifs, interact to regulate gene expression.  In principle, convolution-attention networks (CANs) should provide an inductive bias to infer motif interactions; convolutions can capture motifs while self-attention learns their interactions. However, it is unclear the extent to which this is true in practice. Here we perform an empirical study on synthetic data to test the efficacy of uncovering motif interactions in CANs. We find that irrespective of design choice, interpreting local attention (i.e. on an individual sequence basis) is noisy, leading to many false positive motif interactions. To address this issue, we propose Global Interactions via Filter Activity Correlations (GLIFAC). GLIFAC robustly uncovers motif interactions across a wide spectrum of model choices. This work provides guidance on design choices for CANs that lead to better interpretability for regulatory genomics without sacrificing generalization performance."}}
{"id": "LGgo0wPM2MF", "cdate": 1632499908210, "mdate": null, "content": {"title": "Towards trustworthy explanations with gradient-based attribution methods", "abstract": "The low interpretability of deep neural networks (DNNs) remains a key barrier to their wide-spread adoption in the sciences. Attribution methods offer a promising solution, providing feature importance scores that serve as first-order model explanations for a given input. In practice, gradient-based attribution methods, such as saliency maps, can yield noisy importance scores depending on model architecture and training procedure. Here we explore how various regularization techniques affect model explanations with saliency maps using synthetic regulatory genomic data, which allows us to quantitatively assess the efficacy of attribution maps. Strikingly, we find that generalization performance does not imply better saliency explanations; though unlike before, we do not observe a clear tradeoff. Interestingly, we find that conventional regularization strategies, when tuned appropriately, can yield high generalization and interpretability performance, similar to what can be achieved with more sophisticated techniques, such as manifold mixup. Our work challenges the conventional knowledge that model selection should be based on test performance; another criterion is needed to sub-select models ideally suited for downstream post hoc interpretability for scientific discovery. "}}
{"id": "BBlA-ANL78V", "cdate": 1617674668660, "mdate": null, "content": {"title": "Global Importance Analysis: An Interpretability Method to Quantify Importance of Genomic Features in Deep Neural Networks", "abstract": "Deep neural networks have demonstrated improved performance at predicting the sequence specificities of DNA- and RNA-binding proteins compared to previous methods that rely on k-mers and position weight matrices. To gain insights into why a DNN makes a given prediction, model interpretability methods, such as attribution methods, can be employed to identify motif-like representations along a given sequence. Because explanations are given on an individual sequence basis and can vary substantially across sequences, deducing generalizable trends across the dataset and quantifying their effect size remains a challenge. Here we introduce global importance analysis (GIA), a model interpretability method that quantifies the population-level effect size that putative patterns have on model predictions. GIA provides an avenue to quantitatively test hypotheses of putative patterns and their interactions with other patterns, as well as map out specific functions the network has learned. As a case study, we demonstrate the utility of GIA on the computational task of predicting RNA-protein interactions from sequence. We first introduce a convolutional network, we call ResidualBind, and benchmark its performance against previous methods on RNAcompete data. Using GIA, we then demonstrate that in addition to sequence motifs, ResidualBind learns a model that considers the number of motifs, their spacing, and sequence context, such as RNA secondary structure and GC-bias."}}
{"id": "byj2AP4KRtf", "cdate": 1614361133000, "mdate": null, "content": {"title": "Single Layers of Attention Suffice to Predict Protein Contacts ", "abstract": "The established approach to unsupervised protein contact prediction estimates coevolving positions using undirected graphical models. This approach trains a Potts model on a Multiple Sequence Alignment. Increasingly large Transformers are being pretrained on unlabeled, unaligned protein sequence databases but have demonstrated mixed results for downstream tasks, including contact prediction. We argue that attention is a principled model of protein interactions, grounded in real properties of protein family data. We introduce an energy-based attention layer, factored attention, and show that it achieves comparable performance to Potts models while sharing parameters both within and across families. We contrast factored attention with the Transformer to indicate that the Transformer leverages hierarchical signal in protein family databases not captured by our single-layer models. This raises the exciting possibility for the development of powerful structured models of protein family databases."}}
{"id": "oVz-YWdiMjt", "cdate": 1601308377054, "mdate": null, "content": {"title": "Single Layers of Attention Suffice to Predict Protein Contacts", "abstract": "The established approach to unsupervised protein contact prediction estimates coevolving positions using undirected graphical models. This approach trains a Potts model on a Multiple Sequence Alignment, then predicts that the edges with highest weight correspond to contacts in the 3D structure. On the other hand, increasingly large Transformers are being pretrained on protein sequence databases but have demonstrated mixed results for downstream tasks, including contact prediction. This has sparked discussion about the role of scale and attention-based models in unsupervised protein representation learning. We argue that attention is a principled model of protein interactions, grounded in real properties of protein family data. We introduce a simplified attention layer, factored attention, and show that it achieves comparable performance to Potts models, while sharing parameters both within and across families. Further, we extract contacts from the attention maps of a pretrained Transformer and show they perform competitively with the other two approaches. This provides evidence that large-scale pretraining can learn meaningful protein features when presented with unlabeled and unaligned data. We contrast factored attention with the Transformer to indicate that the Transformer leverages hierarchical signal in protein family databases not captured by our single-layer models. This raises the exciting possibility for the development of powerful structured models of protein family databases."}}
{"id": "Ez6sRNSEty1", "cdate": 1599178813797, "mdate": null, "content": {"title": "Inferring sequence-structure preferences of rna-binding proteins with convolutional residual networks", "abstract": "To infer the sequence and RNA structure specificities of RNA-binding proteins (RBPs) from experiments that enrich for bound sequences, we introduce a convolutional residual network which we call ResidualBind. ResidualBind significantly outperforms previous methods on experimental data from many RBP families. We interrogate ResidualBind to identify what features it has learned from high-affinity sequences with saliency analysis along with 1st-order and 2nd-order in silico mutagenesis. We show that in addition to sequence motifs, ResidualBind learns a model that includes the number of motifs, their spacing, and both positive and negative effects of RNA structure context. Strikingly, ResidualBind learns RNA structure context, including detailed base-pairing relationships, directly from sequence data, which we confirm on synthetic data. ResidualBind is a powerful, flexible, and interpretable model that can uncover cis-recognition preferences across a broad spectrum of RBPs."}}
{"id": "c83pRnB43KGW", "cdate": 1599178714463, "mdate": null, "content": {"title": "Robust Neural Networks are More Interpretable for Genomics", "abstract": "Deep neural networks (DNNs) have been applied to a variety of regulatory genomics tasks. For interpretability, attribution methods are employed to provide importance scores for each nucleotide in a given sequence. However, even with state-of-the-art DNNs, there is no guarantee that these methods can recover interpretable, biological representations. Here we perform systematic experiments on synthetic genomic data to raise awareness of this issue. We find that deeper networks have better generalization performance, but attribution methods recover less interpretable representations. Then, we show training methods promoting robustness \u2013 including regularization, injecting random noise into the data, and adversarial training \u2013 significantly improve interpretability of DNNs, especially for smaller datasets."}}
{"id": "J1DKag_TUw", "cdate": 1599178668196, "mdate": null, "content": {"title": "Improving representations of genomic sequence motifs in convolutional networks with exponential activations", "abstract": "Deep convolutional neural networks (CNNs) trained on regulatory genomic sequences tend to learn distributed feature representations across many filters, making it challenging to decipher biologically meaningful representations, such as sequence motifs. Here we perform a comprehensive analysis on synthetic sequences to investigate the role that CNN activations have on model interpretability. We introduce a novel application of the exponential activation that when applied to first layer filters, consistently leads to interpretable and robust representations of motifs compared to other commonly used activations, both qualitatively and quantitatively. Strikingly, we demonstrate that CNNs with better test performance do not necessarily imply more interpretable representations with attribution methods. We find that CNNs with exponential activations significantly improve the efficacy of a CNN9s ability to recover biologically meaningful representations with attribution methods. We demonstrate these results generalize to real DNA sequences across several in vivo datasets. Together, this work demonstrates how a small modification to existing CNNs, i.e. setting exponential activations in the first layer, can significantly improve the robustness and interpretabilty of learned representations directly in convolutional filters and indirectly with attribution methods."}}
{"id": "5Sa7CaLEHTh", "cdate": 1599178595195, "mdate": null, "content": {"title": "Unified framework for modeling multivariate distributions in biological sequences", "abstract": "Revealing the functional sites of biological sequences, such as evolutionary conserved, structurally interacting or co-evolving protein sites, is a fundamental, and yet challenging task. Different frameworks and models were developed to approach this challenge, including Position-Specific Scoring Matrices, Markov Random Fields, Multivariate Gaussian models and most recently Autoencoders. Each of these methods has certain advantages, and while they have generated a set of insights for better biological predictions, these have been restricted to the corresponding methods and were difficult to translate to the complementary domains. Here we propose a unified framework for the above-mentioned models, that allows for interpretable transformations between the different methods and naturally incorporates the advantages and insight gained individually in the different communities. We show how, by using the unified framework, we are able to achieve state-of-the-art performance for protein structure prediction, while enhancing interpretability of the prediction process."}}
{"id": "_d65gtEdD8L", "cdate": 1599178531138, "mdate": null, "content": {"title": "Representation learning of genomic sequence motifs with convolutional neural networks", "abstract": "Although convolutional neural networks (CNNs) have been applied to a variety of computational genomics problems, there remains a large gap in our understanding of how they build representations of regulatory genomic sequences. Here we perform systematic experiments on synthetic sequences to reveal how CNN architecture, specifically convolutional filter size and max-pooling, influences the extent that sequence motif representations are learned by first layer filters. We find that CNNs designed to foster hierarchical representation learning of sequence motifs\u2014assembling partial features into whole features in deeper layers\u2014tend to learn distributed representations, i.e. partial motifs. On the other hand, CNNs that are designed to limit the ability to hierarchically build sequence motif representations in deeper layers tend to learn more interpretable localist representations, i.e. whole motifs. We then validate that this representation learning principle established from synthetic sequences generalizes to in vivo sequences."}}
