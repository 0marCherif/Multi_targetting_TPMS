{"id": "xhxMQ1siYKR", "cdate": 1577836800000, "mdate": null, "content": {"title": "Robust Self-organization in Games: Symmetries, Conservation Laws and Dimensionality Reduction", "abstract": "Games are an increasingly useful tool for training and testing learning algorithms. Recent examples include GANs, AlphaZero and the AlphaStar league. However, multi-agent learning can be extremely difficult to predict and control. Learning dynamics can yield chaotic behavior even in simple games. In this paper, we present basic mechanism design tools for constructing games with predictable and controllable dynamics. We present a robust framework for dimensionality reduction arguments in large network games."}}
{"id": "gMBYWg7A42K", "cdate": 1577836800000, "mdate": null, "content": {"title": "Better Depth-Width Trade-offs for Neural Networks through the lens of Dynamical Systems", "abstract": "The expressivity of neural networks as a function of their depth, width and type of activation units has been an important question in deep learning theory. Recently, depth separation results for ReLU networks were obtained via a new connection with dynamical systems, using a generalized notion of fixed points of a continuous map $f$, called periodic points. In this work, we strengthen the connection with dynamical systems and we improve the existing width lower bounds along several aspects. Our first main result is period-specific width lower bounds that hold under the stronger notion of $L^1$-approximation error, instead of the weaker classification error. Our second contribution is that we provide sharper width lower bounds, still yielding meaningful exponential depth-width separations, in regimes where previous results wouldn't apply. A byproduct of our results is that there exists a universal constant characterizing the depth-width trade-offs, as long as $f$ has odd periods. Technically, our results follow by unveiling a tighter connection between the following three quantities of a given function: its period, its Lipschitz constant and the growth rate of the number of oscillations arising under compositions of the function $f$ with itself."}}
{"id": "F5RyGGQk1wI", "cdate": 1577836800000, "mdate": null, "content": {"title": "From Chaos to Order: Symmetry and Conservation Laws in Game Dynamics", "abstract": "Games are an increasingly useful tool for training and testing learning algorithms. Recent examples include GANs, AlphaZero and the AlphaStar league. However, multi-agent learning can be extremely ..."}}
{"id": "E6g8m5pygi", "cdate": 1577836800000, "mdate": null, "content": {"title": "Better depth-width trade-offs for neural networks through the lens of dynamical systems", "abstract": "The expressivity of neural networks as a function of their depth, width and type of activation units has been an important question in deep learning theory. Recently, depth separation results for R..."}}
{"id": "6cmuQj5YhgJ", "cdate": 1577836800000, "mdate": null, "content": {"title": "Efficient Statistics for Sparse Graphical Models from Truncated Samples", "abstract": "In this paper, we study high-dimensional estimation from truncated samples. We focus on two fundamental and classical problems: (i) inference of sparse Gaussian graphical models and (ii) support recovery of sparse linear models. (i) For Gaussian graphical models, suppose $d$-dimensional samples ${\\bf x}$ are generated from a Gaussian $N(\\mu,\\Sigma)$ and observed only if they belong to a subset $S \\subseteq \\mathbb{R}^d$. We show that ${\\mu}$ and ${\\Sigma}$ can be estimated with error $\\epsilon$ in the Frobenius norm, using $\\tilde{O}\\left(\\frac{\\textrm{nz}({\\Sigma}^{-1})}{\\epsilon^2}\\right)$ samples from a truncated $\\mathcal{N}({\\mu},{\\Sigma})$ and having access to a membership oracle for $S$. The set $S$ is assumed to have non-trivial measure under the unknown distribution but is otherwise arbitrary. (ii) For sparse linear regression, suppose samples $({\\bf x},y)$ are generated where $y = {\\bf x}^\\top{{\\Omega}^*} + \\mathcal{N}(0,1)$ and $({\\bf x}, y)$ is seen only if $y$ belongs to a truncation set $S \\subseteq \\mathbb{R}$. We consider the case that ${\\Omega}^*$ is sparse with a support set of size $k$. Our main result is to establish precise conditions on the problem dimension $d$, the support size $k$, the number of observations $n$, and properties of the samples and the truncation that are sufficient to recover the support of ${\\Omega}^*$. Specifically, we show that under some mild assumptions, only $O(k^2 \\log d)$ samples are needed to estimate ${\\Omega}^*$ in the $\\ell_\\infty$-norm up to a bounded error. For both problems, our estimator minimizes the sum of the finite population negative log-likelihood function and an $\\ell_1$-regularization term."}}
{"id": "6HVDuKkWmT8", "cdate": 1577836800000, "mdate": null, "content": {"title": "On the Analysis of EM for truncated mixtures of two Gaussians", "abstract": "Motivated by a recent result of Daskalakis et al. (2018), we analyze the population version of Expectation-Maximization (EM) algorithm for the case of \\textit{truncated} mixtures of two Gaussians. ..."}}
{"id": "5PF5CfnvVvH", "cdate": 1577836800000, "mdate": null, "content": {"title": "Last iterate convergence in no-regret learning: constrained min-max optimization for convex-concave landscapes", "abstract": "In a recent series of papers it has been established that variants of Gradient Descent/Ascent and Mirror Descent exhibit last iterate convergence in convex-concave zero-sum games. Specifically, \\cite{DISZ17, LiangS18} show last iterate convergence of the so called \"Optimistic Gradient Descent/Ascent\" for the case of \\textit{unconstrained} min-max optimization. Moreover, in \\cite{Metal} the authors show that Mirror Descent with an extra gradient step displays last iterate convergence for convex-concave problems (both constrained and unconstrained), though their algorithm does not follow the online learning framework; it uses extra information rather than \\textit{only} the history to compute the next iteration. In this work, we show that \"Optimistic Multiplicative-Weights Update (OMWU)\" which follows the no-regret online learning framework, exhibits last iterate convergence locally for convex-concave games, generalizing the results of \\cite{DP19} where last iterate convergence of OMWU was shown only for the \\textit{bilinear case}. We complement our results with experiments that indicate fast convergence of the method."}}
{"id": "BJe55gBtvH", "cdate": 1569439890190, "mdate": null, "content": {"title": "Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem", "abstract": "Understanding the representational power of Deep Neural Networks (DNNs) and how their structural properties (e.g., depth, width, type of activation unit) affect the functions they can compute, has been an important yet challenging question in deep learning and approximation theory. In a seminal paper, Telgarsky high- lighted the benefits of depth by presenting a family of functions (based on sim- ple triangular waves) for which DNNs achieve zero classification error, whereas shallow networks with fewer than exponentially many nodes incur constant error. Even though Telgarsky\u2019s work reveals the limitations of shallow neural networks, it doesn\u2019t inform us on why these functions are difficult to represent and in fact he states it as a tantalizing open question to characterize those functions that cannot be well-approximated by smaller depths.\nIn this work, we point to a new connection between DNNs expressivity and Sharkovsky\u2019s Theorem from dynamical systems, that enables us to characterize the depth-width trade-offs of ReLU networks for representing functions based on the presence of a generalized notion of fixed points, called periodic points (a fixed point is a point of period 1). Motivated by our observation that the triangle waves used in Telgarsky\u2019s work contain points of period 3 \u2013 a period that is special in that it implies chaotic behaviour based on the celebrated result by Li-Yorke \u2013 we proceed to give general lower bounds for the width needed to represent periodic functions as a function of the depth. Technically, the crux of our approach is based on an eigenvalue analysis of the dynamical systems associated with such functions."}}
{"id": "-QzylF4bv9", "cdate": 1514764800000, "mdate": null, "content": {"title": "Three Body Problems in Evolutionary Game Dynamics: Convergence, Periodicity and Limit Cycles", "abstract": "We study the asymptotic behavior of replicator dynamics in settings of network interaction. We focus on three agent graphical games where each edge/game is either a 2x2 zero-sum or a 2x2 coordination game. Using tools from dynamical systems such as Lyapunov functions and invariant functions we establish that this simple family of games can exhibit an interesting range of behaviors such as global convergence, periodicity for all initial conditions as well as limit cycles. In contrast, we do not observe more complex behavior such as toroids or chaos while it is possible to reproduce them in slightly more complicated settings."}}
