{"id": "OOQK1CmdN3", "cdate": 1672531200000, "mdate": 1681636855487, "content": {"title": "Language Is Not All You Need: Aligning Perception with Language Models", "abstract": ""}}
{"id": "n9RgQNR4Jsr", "cdate": 1640995200000, "mdate": 1683880330515, "content": {"title": "XFUND: A Benchmark Dataset for Multilingual Visually Rich Form Understanding", "abstract": ""}}
{"id": "mNDu1gjILy", "cdate": 1640995200000, "mdate": 1683983845384, "content": {"title": "LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking", "abstract": "Self-supervised pre-training techniques have achieved remarkable progress in Document AI. Most multimodal pre-trained models use a masked language modeling objective to learn bidirectional representations on the text modality, but they differ in pre-training objectives for the image modality. This discrepancy adds difficulty to multimodal representation learning. In this paper, we propose LayoutLMv3 to pre-train multimodal Transformers for Document AI with unified text and image masking. Additionally, LayoutLMv3 is pre-trained with a word-patch alignment objective to learn cross-modal alignment by predicting whether the corresponding image patch of a text word is masked. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model for both text-centric and image-centric Document AI tasks. Experimental results show that LayoutLMv3 achieves state-of-the-art performance not only in text-centric tasks, including form understanding, receipt understanding, and document visual question answering, but also in image-centric tasks such as document image classification and document layout analysis. The code and models are publicly available at https://aka.ms/layoutlmv3."}}
{"id": "kSkhAAmVLq", "cdate": 1640995200000, "mdate": 1684160983257, "content": {"title": "LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking", "abstract": "Self-supervised pre-training techniques have achieved remarkable progress in Document AI. Most multimodal pre-trained models use a masked language modeling objective to learn bidirectional representations on the text modality, but they differ in pre-training objectives for the image modality. This discrepancy adds difficulty to multimodal representation learning. In this paper, we propose \\textbf{LayoutLMv3} to pre-train multimodal Transformers for Document AI with unified text and image masking. Additionally, LayoutLMv3 is pre-trained with a word-patch alignment objective to learn cross-modal alignment by predicting whether the corresponding image patch of a text word is masked. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model for both text-centric and image-centric Document AI tasks. Experimental results show that LayoutLMv3 achieves state-of-the-art performance not only in text-centric tasks, including form understanding, receipt understanding, and document visual question answering, but also in image-centric tasks such as document image classification and document layout analysis. The code and models are publicly available at \\url{https://aka.ms/layoutlmv3}."}}
{"id": "W10zSjrWt8o", "cdate": 1640995200000, "mdate": 1684160982571, "content": {"title": "XDoc: Unified Pre-training for Cross-Format Document Understanding", "abstract": ""}}
{"id": "Nu5mWdlPWG5", "cdate": 1640995200000, "mdate": 1682434139792, "content": {"title": "MarkupLM: Pre-training of Text and Markup Language for Visually Rich Document Understanding", "abstract": ""}}
{"id": "IKbBEcFoqMX", "cdate": 1640995200000, "mdate": 1683880330456, "content": {"title": "DiT: Self-supervised Pre-training for Document Image Transformer", "abstract": "Image Transformer has recently achieved significant progress for natural image understanding, either using supervised (ViT, DeiT, etc.) or self-supervised (BEiT, MAE, etc.) pre-training techniques. In this paper, we propose \\textbf{DiT}, a self-supervised pre-trained \\textbf{D}ocument \\textbf{I}mage \\textbf{T}ransformer model using large-scale unlabeled text images for Document AI tasks, which is essential since no supervised counterparts ever exist due to the lack of human-labeled document images. We leverage DiT as the backbone network in a variety of vision-based Document AI tasks, including document image classification, document layout analysis, table detection as well as text detection for OCR. Experiment results have illustrated that the self-supervised pre-trained DiT model achieves new state-of-the-art results on these downstream tasks, e.g. document image classification (91.11 $\\rightarrow$ 92.69), document layout analysis (91.0 $\\rightarrow$ 94.9), table detection (94.23 $\\rightarrow$ 96.55) and text detection for OCR (93.07 $\\rightarrow$ 94.29). The code and pre-trained models are publicly available at \\url{https://aka.ms/msdit}."}}
{"id": "9aBEEAglewc", "cdate": 1640995200000, "mdate": 1682434139773, "content": {"title": "DiT: Self-supervised Pre-training for Document Image Transformer", "abstract": "Image Transformer has recently achieved significant progress for natural image understanding, either using supervised (ViT, DeiT, etc.) or self-supervised (BEiT, MAE, etc.) pre-training techniques. In this paper, we propose DiT, a self-supervised pre-trained Document Image Transformer model using large-scale unlabeled text images for Document AI tasks, which is essential since no supervised counterparts ever exist due to the lack of human-labeled document images. We leverage DiT as the backbone network in a variety of vision-based Document AI tasks, including document image classification, document layout analysis, table detection as well as text detection for OCR. Experiment results have illustrated that the self-supervised pre-trained DiT model achieves new state-of-the-art results on these downstream tasks, e.g. document image classification (91.11 - 92.69), document layout analysis (91.0 - 94.9), table detection (94.23 - 96.55) and text detection for OCR (93.07 - 94.29). The code and pre-trained models are publicly available at https://aka.ms/msdit."}}
{"id": "8C_HKXOY_2", "cdate": 1640995200000, "mdate": 1684160983122, "content": {"title": "XDoc: Unified Pre-training for Cross-Format Document Understanding", "abstract": "The surge of pre-training has witnessed the rapid development of document understanding recently. Pre-training and fine-tuning framework has been effectively used to tackle texts in various formats, including plain texts, document texts, and web texts. Despite achieving promising performance, existing pre-trained models usually target one specific document format at one time, making it difficult to combine knowledge from multiple document formats. To address this, we propose XDoc, a unified pre-trained model which deals with different document formats in a single model. For parameter efficiency, we share backbone parameters for different formats such as the word embedding layer and the Transformer layers. Meanwhile, we introduce adaptive layers with lightweight parameters to enhance the distinction across different formats. Experimental results have demonstrated that with only 36.7% parameters, XDoc achieves comparable or even better performance on a variety of downstream tasks compared with the individual pre-trained models, which is cost effective for real-world deployment. The code and pre-trained models will be publicly available at \\url{https://aka.ms/xdoc}."}}
{"id": "7ZZZexPE21G", "cdate": 1640995200000, "mdate": 1684160982761, "content": {"title": "A Simple yet Effective Learnable Positional Encoding Method for Improving Document Transformer Model", "abstract": ""}}
