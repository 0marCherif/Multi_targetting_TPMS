{"id": "qCi50QTfX3", "cdate": 1672143584777, "mdate": 1672143584777, "content": {"title": "Language models show human-like content effects on reasoning", "abstract": "Abstract reasoning is a key ability for an intelligent system. Large language models achieve abovechance performance on abstract reasoning tasks, but exhibit many imperfections. However, human\nabstract reasoning is also imperfect, and depends on our knowledge and beliefs about the content of\nthe reasoning problem. For example, humans reason much more reliably about logical rules that are\ngrounded in everyday situations than arbitrary rules about abstract attributes. The training experiences\nof language models similarly endow them with prior expectations that reflect human knowledge and\nbeliefs. We therefore hypothesized that language models would show human-like content effects on\nabstract reasoning problems. We explored this hypothesis across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task (Wason,\n1968). We find that state of the art large language models (with 7 or 70 billion parameters; Hoffmann\net al., 2022) reflect many of the same patterns observed in humans across these tasks \u2014 like humans,\nmodels reason more effectively about believable situations than unrealistic or abstract ones. Our findings have implications for understanding both these cognitive effects, and the factors that contribute\nto language model performance."}}
{"id": "et9T2bwf_v", "cdate": 1672143474007, "mdate": 1672143474007, "content": {"title": "Can language models learn from explanations in context?", "abstract": "Language Models (LMs) can perform new\ntasks by adapting to a few in-context examples.\nFor humans, explanations that connect examples to task principles can improve learning.\nWe therefore investigate whether explanations\nof few-shot examples can help LMs. We annotate questions from 40 challenging tasks with\nanswer explanations, and various matched control explanations. We evaluate how different\ntypes of explanations, instructions, and controls affect zero- and few-shot performance.\nWe analyze these results using statistical multilevel modeling techniques that account for the\nnested dependencies among conditions, tasks,\nprompts, and models. We find that explanations can improve performance\u2014even without tuning. Furthermore, explanations handtuned for performance on a small validation set\noffer substantially larger benefits, and building a prompt by selecting examples and explanations together substantially improves performance over selecting examples alone. Finally, even untuned explanations outperform\ncarefully matched controls, suggesting that the\nbenefits are due to the link between an example and its explanation, rather than lower-level\nfeatures. However, only large models benefit.\nIn summary, explanations can support the incontext lea"}}
{"id": "MND1kmmNy0O", "cdate": 1663850325895, "mdate": null, "content": {"title": "Solving Math Word Problems with Process-based and Outcome-based Feedback", "abstract": "Recent work has shown that prompting language models to generate reasoning steps improves performance on many reasoning tasks. When moving beyond prompting, this raises the question of how we should supervise the finetuning of such models: outcome-based approaches which supervise the final result, or process-based approaches which supervise the reasoning process itself? Differences between these approaches might naturally be expected not just in final-answer errors but also in reasoning errors, which can be difficult to detect and are problematic in many real-world domains such as education.  We run the first comprehensive comparison between process- and outcome-based approaches trained on a natural language task, GSM8K. We find that pure outcome-based supervision produces similar final-answer error rates with less label supervision. However, for correct reasoning steps we find it necessary to use process-based supervision or supervision from learned reward models that emulate process-based feedback. In total, we improve the previous best results from 16.8% $\\rightarrow$ 12.7% final-answer error and 14.0% $\\rightarrow$ 3.4% reasoning error among final-answer-correct solutions."}}
{"id": "3Pf3Wg6o-A4", "cdate": 1663850310732, "mdate": null, "content": {"title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning", "abstract": "Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 46 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system."}}
{"id": "YSzTMntO1KY", "cdate": 1621630042280, "mdate": null, "content": {"title": "SIMONe: View-Invariant, Temporally-Abstracted Object Representations via Unsupervised Video Decomposition", "abstract": "To help agents reason about scenes in terms of their building blocks, we wish to extract the compositional structure of any given scene (in particular, the configuration and characteristics of objects comprising the scene). This problem is especially difficult when scene structure needs to be inferred while also estimating the agent\u2019s location/viewpoint, as the two variables jointly give rise to the agent\u2019s observations. We present an unsupervised variational approach to this problem. Leveraging the shared structure that exists across different scenes, our model learns to infer two sets of latent representations from RGB video input alone: a set of \"object\" latents, corresponding to the time-invariant, object-level contents of the scene, as well as a set of \"frame\" latents, corresponding to global time-varying elements such as viewpoint. This factorization of latents allows our model, SIMONe, to represent object attributes in an allocentric manner which does not depend on viewpoint. Moreover, it allows us to disentangle object dynamics and summarize their trajectories as time-abstracted, view-invariant, per-object properties. We demonstrate these capabilities, as well as the model's performance in terms of view synthesis and instance segmentation, across three procedurally generated video datasets.     "}}
{"id": "X17EOUP2Cgt", "cdate": 1621629980678, "mdate": null, "content": {"title": "Unsupervised Object-Based Transition Models For 3D Partially Observable Environments", "abstract": "We present a slot-wise, object-based transition model that decomposes a scene into objects, aligns them (with respect to a slot-wise object memory) to maintain a consistent order across time, and predicts how those objects evolve over successive frames. The model is trained end-to-end without supervision using transition losses at the level of the object-structured representation rather than pixels. Thanks to the introduction of our novel alignment module, the model deals properly with two issues that are not handled satisfactorily by other transition models, namely object persistence and object identity. We show that the combination of an object-level loss and correct object alignment over time enables the model to outperform a state-of-the-art baseline, and allows it to deal well with object occlusion and re-appearance in partially observable environments."}}
{"id": "1oVNAVhJ4GP", "cdate": 1621629980678, "mdate": null, "content": {"title": "Unsupervised Object-Based Transition Models For 3D Partially Observable Environments", "abstract": "We present a slot-wise, object-based transition model that decomposes a scene into objects, aligns them (with respect to a slot-wise object memory) to maintain a consistent order across time, and predicts how those objects evolve over successive frames. The model is trained end-to-end without supervision using transition losses at the level of the object-structured representation rather than pixels. Thanks to the introduction of our novel alignment module, the model deals properly with two issues that are not handled satisfactorily by other transition models, namely object persistence and object identity. We show that the combination of an object-level loss and correct object alignment over time enables the model to outperform a state-of-the-art baseline, and allows it to deal well with object occlusion and re-appearance in partially observable environments."}}
{"id": "H1gcw1HYPr", "cdate": 1569439585638, "mdate": null, "content": {"title": "AlignNet: Self-supervised Alignment Module", "abstract": "The natural world consists of objects that we perceive as persistent in space and time, even though these objects appear, disappear and reappear in our field of view as we move. This can be attributed to our notion of object persistence -- our knowledge that objects typically continue to exist, even if we can no longer see them -- and our ability to track objects. Drawing inspiration from the psychology literature on `sticky indices', we propose the AlignNet, a model that learns to assign unique indices to new objects when they first appear and reassign the index to subsequent instances of that object. By introducing a persistent object-based memory, the AlignNet may be used to keep track of objects across time, even if they disappear and reappear later. We implement the AlignNet as a graph network applied to a bipartite graph, in which the input nodes are objects from two sets that we wish to align. The network is trained to predict the edges which connect two instances of the same object across sets. The model is also capable of identifying when there are no matches and dealing with these cases. We perform experiments to show the model's ability to deal with the appearance, disappearance and reappearance of objects. Additionally, we demonstrate how a persistent object-based memory can help solve question-answering problems in a partially observable environment."}}
{"id": "S1l6ITVKPS", "cdate": 1569439060720, "mdate": null, "content": {"title": "An Explicitly Relational Neural Network Architecture", "abstract": "With a view to bridging the gap between deep learning and symbolic AI, we present a novel end-to-end neural network architecture that learns to form propositional representations with an explicitly relational structure from raw pixel data. In order to evaluate and analyse the architecture, we introduce a family of simple visual relational reasoning tasks of varying complexity. We show that the proposed architecture, when pre-trained on a curriculum of such tasks, learns to generate reusable representations that better facilitate subsequent learning on previously unseen tasks when compared to a number of baseline architectures. The workings of a successfully trained model are visualised to shed some light on how the architecture functions."}}
{"id": "d6w6cma3Erv", "cdate": 1546300800000, "mdate": null, "content": {"title": "Inverting the Generator of a Generative Adversarial Network.", "abstract": "Generative adversarial networks (GANs) learn a deep generative model that is able to synthesize novel, high-dimensional data samples. New data samples are synthesized by passing latent samples, drawn from a chosen prior distribution, through the generative model. Once trained, the latent space exhibits interesting properties that may be useful for downstream tasks such as classification or retrieval. Unfortunately, GANs do not offer an \u201cinverse model,\u201d a mapping from data space back to latent space, making it difficult to infer a latent representation for a given data sample. In this paper, we introduce a technique, inversion, to project data samples, specifically images, to the latent space using a pretrained GAN. Using our proposed inversion technique, we are able to identify which attributes of a data set a trained GAN is able to model and quantify GAN performance, based on a reconstruction loss. We demonstrate how our proposed inversion technique may be used to quantitatively compare the performance of various GAN models trained on three image data sets. We provide codes for all of our experiments in the website (https://github.com/ToniCreswell/InvertingGAN)."}}
