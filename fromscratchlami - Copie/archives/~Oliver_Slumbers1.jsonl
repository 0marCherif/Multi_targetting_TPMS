{"id": "tPsudHtyYZO", "cdate": 1672531200000, "mdate": 1695988314026, "content": {"title": "Timing is Everything: Learning to Act Selectively with Costly Actions and Budgetary Constraints", "abstract": ""}}
{"id": "k4rQUvP0McB", "cdate": 1672531200000, "mdate": 1695988314035, "content": {"title": "A Game-Theoretic Framework for Managing Risk in Multi-Agent Systems", "abstract": "In order for agents in multi-agent systems (MAS) to be safe, they need to take into account the risks posed by the actions of other agents. However, the dominant paradigm in game theory (GT) assume..."}}
{"id": "AUEwrXjHhN", "cdate": 1672531200000, "mdate": 1681737988474, "content": {"title": "Ensemble Value Functions for Efficient Exploration in Multi-Agent Reinforcement Learning", "abstract": "Cooperative multi-agent reinforcement learning (MARL) requires agents to explore to learn to cooperate. Existing value-based MARL algorithms commonly rely on random exploration, such as $\\epsilon$-greedy, which is inefficient in discovering multi-agent cooperation. Additionally, the environment in MARL appears non-stationary to any individual agent due to the simultaneous training of other agents, leading to highly variant and thus unstable optimisation signals. In this work, we propose ensemble value functions for multi-agent exploration (EMAX), a general framework to extend any value-based MARL algorithm. EMAX trains ensembles of value functions for each agent to address the key challenges of exploration and non-stationarity: (1) The uncertainty of value estimates across the ensemble is used in a UCB policy to guide the exploration of agents to parts of the environment which require cooperation. (2) Average value estimates across the ensemble serve as target values. These targets exhibit lower variance compared to commonly applied target networks and we show that they lead to more stable gradients during the optimisation. We instantiate three value-based MARL algorithms with EMAX, independent DQN, VDN and QMIX, and evaluate them in 21 tasks across four environments. Using ensembles of five value functions, EMAX improves sample efficiency and final evaluation returns of these algorithms by 53%, 36%, and 498%, respectively, averaged all 21 tasks."}}
{"id": "_BoPed4tYww", "cdate": 1663850294187, "mdate": null, "content": {"title": "Timing is Everything: Learning to Act Selectively with Costly Actions and Budgetary Constraints", "abstract": "Many real-world settings involve costs for performing actions; transaction costs\nin financial systems and fuel costs being common examples. In these settings,\nperforming actions at each time step quickly accumulates costs leading to vastly\nsuboptimal outcomes. Additionally, repeatedly acting produces wear and tear and\nultimately, damage. Determining when to act is crucial for achieving successful\noutcomes and yet, the challenge of efficiently learning to behave optimally when\nactions incur minimally bounded costs remains unresolved. In this paper, we intro-\nduce a reinforcement learning (RL) framework named Learnable Impulse Control\nReinforcement Algorithm (LICRA), for learning to optimally select both when\nto act and which actions to take when actions incur costs. At the core of LICRA\nis a nested structure that combines RL and a form of policy known as impulse\ncontrol which learns to maximise objectives when actions incur costs. We prove\nthat LICRA, which seamlessly adopts any RL method, converges to policies that\noptimally select when to perform actions and their optimal magnitudes. We then\naugment LICRA to handle problems in which the agent can perform at most k < \u221e\nactions and more generally, faces a budget constraint. We show LICRA learns the\noptimal value function and ensures budget constraints are satisfied almost surely.\nWe demonstrate empirically LICRA\u2019s superior performance against benchmark\nRL methods in OpenAI gym\u2019s Lunar Lander and in Highway environments and a\nvariant of the Merton portfolio problem within finance."}}
{"id": "5Z1rblK1Be5", "cdate": 1663850132243, "mdate": null, "content": {"title": "A Risk-Averse Equilibrium for Multi-Agent Systems", "abstract": "In multi-agent systems, intelligent agents are tasked with making decisions that lead to optimal outcomes when actions of the other agents are as expected, whilst also being prepared for their unexpected behaviour. In this work, we introduce a novel risk-averse solution concept that allows the learner to accommodate low probability actions by finding the strategy with minimum variance, given any level of expected utility. We first prove the existence of such a risk-averse equilibrium, and propose one fictitious-play type learning algorithm for smaller games that enjoys provable convergence guarantees in games classes including zero-sum and potential. Furthermore, we propose an approximation method for larger games based on iterative population-based training that generates a population of risk- averse agents. Empirically, our equilibrium is shown to be able to reduce the utility variance, specifically in the sense that other agents\u2019 low probability behaviour is better accounted for by our equilibrium in comparison to playing other solutions. Importantly, we show that our population of agents that approximate a risk-averse equilibrium is particularly effective against unseen opposing populations, especially in the case of guaranteeing a minimum level of performance, which is critical to safety-aware multi-agent systems."}}
{"id": "zkk_7sV6gm8", "cdate": 1652737591088, "mdate": null, "content": {"title": "Timing is Everything: Learning to Act Selectively with Costly Actions and Budgetary Constraints", "abstract": "Many real-world settings involve costs for performing actions; transaction costs in financial systems and fuel costs being common examples. In these settings, performing actions at each time step quickly accumulates costs leading to vastly suboptimal outcomes. Additionally, repeatedly acting produces wear and tear and ultimately, damage.  Determining when to act is crucial for achieving successful outcomes and yet, the challenge of efficiently \\textit{learning} to behave optimally when actions incur minimally bounded costs remains unresolved. In this paper, we introduce a  reinforcement learning (RL) framework named Learnable Impulse Control Reinforcement Algorithm (LICRA), for learning to optimally select both when to act and which actions to take when actions incur costs. At the core of LICRA is a nested structure that combines RL and a form of policy known as \\textit{impulse control} which learns to maximise objectives when actions incur costs. We prove that LICRA, which seamlessly adopts any RL method, converges to policies that optimally select when to perform actions and their optimal magnitudes. We then augment LICRA to handle problems in which the agent can perform at most $k<\\infty$ actions and more generally, faces a budget constraint. We show LICRA learns the optimal value function and ensures budget constraints are satisfied almost surely. We demonstrate empirically LICRA's superior performance against benchmark RL methods in OpenAI gym's Lunar Lander and in Highway environments."}}
{"id": "rfeLi91pec", "cdate": 1646226078080, "mdate": null, "content": {"title": "A Game-Theoretic Approach for Improving Generalization Ability of TSP Solvers", "abstract": "In this paper, we introduce a two-player zero-sum framework between a trainable \\emph{Solver} and a \\emph{Data Generator} to improve the generalization ability of deep learning-based solvers for Traveling Salesman Problems (TSP).\nGrounded in  \\textsl{Policy Space Response Oracle} (PSRO) methods, our two-player framework outputs a  population of best-responding Solvers, over which we can mix and output a combined model that achieves the least exploitability against the Generator, and thereby the most  generalizable performance on different TSP tasks. \nWe conduct experiments on a variety of TSP instances with different types and sizes. Results suggest  that our Solvers achieve the state-of-the-art performance even on tasks the Solver never meets, whilst the performance of other deep learning-based Solvers drops sharply due to over-fitting. \nTo demonstrate the principle of our framework, we study  the learning outcome of the proposed two-player game and demonstrate that the exploitability of the Solver population decreases during training, and it eventually approximates  the Nash equilibrium along with the Generator."}}
{"id": "t9H6AEwbli6", "cdate": 1640995200000, "mdate": 1695988314036, "content": {"title": "Online Double Oracle", "abstract": "Solving strategic games with huge action spaces is a critical yet under-explored topic in economics, operations research and artificial intelligence. This paper proposes new learning algorithms for solving two-player zero-sum normal-form games where the number of pure strategies is prohibitively large. Specifically, we combine no-regret analysis from online learning with Double Oracle (DO) from game theory. Our method---\\emph{Online Double Oracle (ODO)}---is provably convergent to a Nash equilibrium (NE). Most importantly, unlike normal DO, ODO is \\emph{rational} in the sense that each agent in ODO can exploit a strategic adversary with a regret bound of $\\mathcal{O}(\\sqrt{ k \\log(k)/T})$, where $k$ is not the total number of pure strategies, but rather the size of \\emph{effective strategy set}. In many applications, we empirically show that $k$ is linearly dependent on the support size of the NE. On tens of different real-world matrix games, ODO outperforms DO, PSRO, and no-regret algorithms such as Multiplicative Weights Update by a significant margin, both in terms of convergence rate to a NE, and average payoff against strategic adversaries."}}
{"id": "dcotfmKeA4", "cdate": 1640995200000, "mdate": 1681541312091, "content": {"title": "LIGS: Learnable Intrinsic-Reward Generation Selection for Multi-Agent Learning", "abstract": ""}}
{"id": "Omrt4F6J2YQ", "cdate": 1640995200000, "mdate": 1681541312195, "content": {"title": "Learning Risk-Averse Equilibria in Multi-Agent Systems", "abstract": ""}}
