{"id": "q573PLtX3t", "cdate": 1672531200000, "mdate": 1681514553082, "content": {"title": "Baldur: Whole-Proof Generation and Repair with Large Language Models", "abstract": ""}}
{"id": "IUikebJ1Bf0", "cdate": 1652737834008, "mdate": null, "content": {"title": "Autoformalization with Large Language Models", "abstract": "Autoformalization is the process of automatically translating from natural language mathematics to formal specifications and proofs. A successful autoformalization system could advance the fields of formal verification, program synthesis, and artificial intelligence.\nWhile the long-term goal of autoformalization seemed elusive for a long time, we show large language models provide new prospects towards this goal. We make the surprising observation that LLMs can correctly translate a significant portion ($25.3\\%$) of mathematical competition problems perfectly to formal specifications in Isabelle/HOL. We demonstrate the usefulness of this process by improving a previously introduced neural theorem prover via training on these autoformalized theorems. Our methodology results in a new state-of-the-art result on the MiniF2F theorem proving benchmark, improving the proof rate from~$29.6\\%$ to~$35.2\\%$."}}
{"id": "Z9ds67I8qpd", "cdate": 1640995200000, "mdate": 1681514553080, "content": {"title": "Memorizing Transformers", "abstract": ""}}
{"id": "SEqgnUeLRV", "cdate": 1640995200000, "mdate": 1681514553083, "content": {"title": "Memorizing Transformers", "abstract": ""}}
{"id": "KdtNI6IZpH", "cdate": 1640995200000, "mdate": 1681514553083, "content": {"title": "Autoformalization with Large Language Models", "abstract": ""}}
{"id": "TrjbxzRcnf-", "cdate": 1632875732637, "mdate": null, "content": {"title": "Memorizing Transformers", "abstract": "Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights.  \nWe instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of past inputs. We demonstrate that an approximate $k$NN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 262K tokens. \nOn benchmarks including code and mathematics, we find that the model is capable of making use of newly defined functions and theorems during test time."}}
{"id": "O4TE57kehc1", "cdate": 1621630343085, "mdate": null, "content": {"title": "Neural Circuit Synthesis from Specification Patterns", "abstract": "We train hierarchical Transformers on the task of synthesizing hardware circuits directly out of high-level logical speci\ufb01cations in linear-time temporal logic (LTL). The LTL synthesis problem is a well-known algorithmic challenge with a long history and an annual competition is organized to track the improvement of algorithms and tooling over time. New approaches using machine learning might open a lot of possibilities in this area, but suffer from the lack of suf\ufb01cient amounts of training data. In this paper, we consider a method to generate large amounts of additional training data, i.e., pairs of speci\ufb01cations and circuits implementing them. We ensure that this synthetic data is suf\ufb01ciently close to human-written speci\ufb01cations by mining common patterns from the speci\ufb01cations used in the synthesis competitions. We show that hierarchical Transformers trained on this synthetic data solve a signi\ufb01cant portion of problems from the synthesis competitions, and even out-of-distribution examples from a recent case study."}}
{"id": "nVTvhWZ0F3", "cdate": 1609459200000, "mdate": 1681514553137, "content": {"title": "LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning", "abstract": ""}}
{"id": "l8qzevGN6J", "cdate": 1609459200000, "mdate": 1681514553099, "content": {"title": "Neural Circuit Synthesis from Specification Patterns", "abstract": ""}}
{"id": "jMsQlyShlBD", "cdate": 1609459200000, "mdate": 1650473377448, "content": {"title": "Mathematical Reasoning via Self-supervised Skip-tree Training", "abstract": "We demonstrate that self-supervised language modeling applied to mathematical formulas enables logical reasoning. To measure the logical reasoning abilities of language models, we formulate several..."}}
