{"id": "_JjIFD4dBc", "cdate": 1672531200000, "mdate": 1700925602123, "content": {"title": "Stochastic Gradient Descent with Preconditioned Polyak Step-size", "abstract": "Stochastic Gradient Descent (SGD) is one of the many iterative optimization methods that are widely used in solving machine learning problems. These methods display valuable properties and attract researchers and industrial machine learning engineers with their simplicity. However, one of the weaknesses of this type of methods is the necessity to tune learning rate (step-size) for every loss function and dataset combination to solve an optimization problem and get an efficient performance in a given time budget. Stochastic Gradient Descent with Polyak Step-size (SPS) is a method that offers an update rule that alleviates the need of fine-tuning the learning rate of an optimizer. In this paper, we propose an extension of SPS that employs preconditioning techniques, such as Hutchinson's method, Adam, and AdaGrad, to improve its performance on badly scaled and/or ill-conditioned datasets."}}
{"id": "ZbzcLy5I4rz", "cdate": 1663849958361, "mdate": null, "content": {"title": "Stochastic Gradient Methods with Preconditioned Updates", "abstract": "This work considers non-convex finite sum minimization. There are a number of algorithms for such problems, but existing methods often work poorly when the problem is badly scaled and/or ill-conditioned, and a primary goal of this work is to introduce methods that alleviate this issue. Thus, here we include a preconditioner that is based upon Hutchinson's approach to approximating the diagonal of the Hessian, and couple it with several gradient based methods to give new `scaled' algorithms: Scaled  SARAH and Scaled L-SVRG. Theoretical complexity guarantees under smoothness assumptions are presented, and we prove linear convergence when both smoothness and the PL-condition is assumed. Because our adaptively scaled methods use approximate partial second order curvature information, they are better able to mitigate the impact of badly scaled problems, and this improved practical performance is demonstrated in the numerical experiments that are also presented in this work."}}
{"id": "rjDziEPQLQs", "cdate": 1652737531823, "mdate": null, "content": {"title": "A Damped Newton Method Achieves Global $\\mathcal O \\left(\\frac{1}{k^2}\\right)$  and Local Quadratic  Convergence Rate", "abstract": "In this paper, we present the first stepsize schedule for Newton method resulting in fast global and local convergence guarantees. In particular, we a) prove an $\\mathcal O \\left( 1/{k^2} \\right)$ global rate, which matches the state-of-the-art global rate of cubically regularized Newton method of Polyak and Nesterov (2006) and of regularized Newton method of Mishchenko (2021), and the later variant of Doikov and Nesterov (2021), b) prove a local quadratic rate, which matches the best-known local rate of second-order methods, and c) our stepsize formula is simple, explicit, and does not require solving any subproblem. Our convergence proofs hold under affine-invariant assumptions closely related to the notion of self-concordance. Finally, our method has competitive performance when compared to existing baselines which share the same fast global convergence guarantees."}}
{"id": "qi8EJ1YfxN", "cdate": 1640995200000, "mdate": 1679921170694, "content": {"title": "Suppressing Poisoning Attacks on Federated Learning for Medical Imaging", "abstract": ""}}
{"id": "hyVdCMrSYO", "cdate": 1640995200000, "mdate": 1679921169469, "content": {"title": "Stochastic Gradient Methods with Preconditioned Updates", "abstract": ""}}
{"id": "RiOpkhKXsy", "cdate": 1640995200000, "mdate": 1679921169641, "content": {"title": "The power of first-order smooth optimization for black-box non-smooth problems", "abstract": ""}}
{"id": "LEbTHvnf3j7", "cdate": 1640995200000, "mdate": 1684009194129, "content": {"title": "A Damped Newton Method Achieves Global $\\mathcal O \\left(\\frac{1}{k^2}\\right)$ and Local Quadratic Convergence Rate", "abstract": "In this paper, we present the first stepsize schedule for Newton method resulting in fast global and local convergence guarantees. In particular, we a) prove an $\\mathcal O \\left( 1/{k^2} \\right)$ global rate, which matches the state-of-the-art global rate of cubically regularized Newton method of Polyak and Nesterov (2006) and of regularized Newton method of Mishchenko (2021), and the later variant of Doikov and Nesterov (2021), b) prove a local quadratic rate, which matches the best-known local rate of second-order methods, and c) our stepsize formula is simple, explicit, and does not require solving any subproblem. Our convergence proofs hold under affine-invariant assumptions closely related to the notion of self-concordance. Finally, our method has competitive performance when compared to existing baselines which share the same fast global convergence guarantees."}}
{"id": "EY9jWlu5Wh", "cdate": 1640995200000, "mdate": 1684009194114, "content": {"title": "Hyperfast second-order local solvers for efficient statistically preconditioned distributed optimization", "abstract": ""}}
{"id": "ARNg67KrkR9", "cdate": 1640995200000, "mdate": 1684009194132, "content": {"title": "Efficient numerical methods to solve sparse linear equations with application to PageRank", "abstract": "Over the last two decades, the PageRank problem has received increased interest from the academic community as an efficient tool to estimate web-page importance in information retrieval. Despite nu..."}}
{"id": "x0ctYKE6zn", "cdate": 1609459200000, "mdate": 1684009194125, "content": {"title": "Composite optimization for the resource allocation problem", "abstract": "In this paper, we consider resource allocation problem stated as a convex minimization problem with linear constraints. To solve this problem, we use gradient and accelerated gradient descent appli..."}}
