{"id": "0qnryNf6XwR", "cdate": 1663850113549, "mdate": null, "content": {"title": "When are smooth-ReLUs ReLU-like?", "abstract": "ReLU is one of the most popular activations in deep learning, especially thanks to its stabilizing effect on training. However, because it is non-differentiable at the origin, it complicates the use of analysis methods that examine derivatives, such as the Neural Tangent Kernel (NTK). Many smooth relaxations try to retain the practical benefits of ReLU while increasing network regularity. Although their success has ranged widely, some notable architectures (e.g., the BERT family) do utilize them. We present a theoretical characterization of smooth-ReLUs within fully-connected feed-forward neural networks. In addition to the well-known SWISH and GeLU, we introduce  GumbelLU, AlgebraicLU, and GudermanLU, as new relaxations. All these activations can be characterized by a positive temperature parameter which we can lower to  continuously improve the approximation. By studying the interplay of initialization schemes with temperature, we confirm that when these relaxations converge uniformly to ReLU, the statistical properties of the corresponding neural networks at initialization also converge to those of ReLU networks. Moreover, we derive temperature-dependent critical initialization schemes with which networks based on these activations exhibit stable ReLU-like behavior at any temperature. Finally, we empirically study both classes of networks on MNIST and CIFAR-10 in the full-batch training regime. We show that, while all networks exhibit very similar train loss trajectories at criticality, smooth-ReLU networks feature differentiable NTKs throughout training, whereas ReLU networks exhibit stochastic NTK fluctuations. Our results clarify how smooth-ReLU relaxations reproduce the practical benefits of ReLU in everywhere-smooth neural networks."}}
{"id": "ybIy0LZZipR", "cdate": 1640995200000, "mdate": 1677845804839, "content": {"title": "Reference-Limited Compositional Zero-Shot Learning", "abstract": ""}}
{"id": "TytZk4tWO5", "cdate": 1632875630202, "mdate": null, "content": {"title": "Reference-Limited Compositional Learning: A Realistic Assessment for Human-level Compositional Generalization", "abstract": "To narrow the considerable gap between artificial and human intelligence, we propose a new task, namely reference-limited compositional learning (RLCL), which reproduces three core challenges to mimic human perception: compositional learning, few-shot, and few referential compositions. Building upon the setting, we propose two benchmarks that consist of multiple datasets with diverse compositional labels, providing a suitable and realistic platform for systematically assessing progress on the task. Moreover, we extend popular few-shot and compositional learning approaches to serve as baselines, and also introduce a simple method that achieves better performance in recognizing unseen compositions. Extensive experiments demonstrate that existing solutions struggle with the challenges imposed by the RLCL task, revealing substantial research space for pursuing human-level compositional generalization ability."}}
