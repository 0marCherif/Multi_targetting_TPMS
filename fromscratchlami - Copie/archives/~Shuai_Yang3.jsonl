{"id": "labc3tIzyX_", "cdate": 1640995200000, "mdate": 1666353544120, "content": {"title": "Unsupervised Image-to-Image Translation with Generative Prior", "abstract": "Unsupervised image-to-image translation aims to learn the translation between two visual domains without paired data. Despite the recent progress in image translation models, it remains challenging to build mappings between complex domains with drastic visual discrepancies. In this work, we present a novel framework, Generative Priorguided UNsupervised Image-to-image Translation (GP-UNIT), to improve the overall quality and applicability of the translation algorithm. Our key insight is to leverage the generative prior from pre-trained class-conditional GANs (e.g., BigGAN) to learn rich content correspondences across various domains. We propose a novel coarse-to-fine scheme: we first distill the generative prior to capture a robust coarse-level content representation that can link objects at an abstract semantic level, based on which finelevel content features are adaptively learned for more accurate multi-level content correspondences. Extensive experiments demonstrate the superiority of our versatile framework over state-of-the-art methods in robust, high-quality and diversified translations, even for challenging and distant domains. Code is available at https://github.com/williamyang1991/GP-UNIT."}}
{"id": "S7l9pt49NY", "cdate": 1640995200000, "mdate": 1681542551684, "content": {"title": "VToonify: Controllable High-Resolution Portrait Video Style Transfer", "abstract": ""}}
{"id": "3l2O4vaxJP", "cdate": 1640995200000, "mdate": 1681653722738, "content": {"title": "CLAST: Contrastive Learning for Arbitrary Style Transfer", "abstract": ""}}
{"id": "-kqKIPjuk3t", "cdate": 1640995200000, "mdate": 1666353544111, "content": {"title": "Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer", "abstract": "Recent studies on StyleGAN show high performance on artistic portrait generation by transfer learning with limited data. In this paper, we explore more challenging exemplar-based high-resolution portrait style transfer by introducing a novel DualStyleGAN with flexible control of dual styles of the original face domain and the extended artistic portrait domain. Different from StyleGAN, DualStyleGAN provides a natural way of style transfer by characterizing the content and style of a portrait with an intrinsic style path and a new extrinsic style path, respectively. The del-icately designed extrinsic style path enables our model to modulate both the color and complex structural styles hierarchically to precisely pastiche the style example. Furthermore, a novel progressive fine-tuning scheme is introduced to smoothly transform the generative space of the model to the target domain, even with the above modifications on the network architecture. Experiments demonstrate the superiority of DualStyleGAN over state-of-the-art methods in high-quality portrait style transfer and flexible stylecontrol. Code is available at https://github.com/williamyang1991/DualStyleGAN."}}
{"id": "bjhhx5ny2Gv", "cdate": 1609459200000, "mdate": 1681653722666, "content": {"title": "Instance-Aware Coherent Video Style Transfer for Chinese Ink Wash Painting", "abstract": ""}}
{"id": "TK7YHVlrrlt", "cdate": 1609459200000, "mdate": 1681653722669, "content": {"title": "Controllable Sketch-to-Image Translation for Robust Face Synthesis", "abstract": ""}}
{"id": "ZbdxzlQAXB9", "cdate": 1577836800000, "mdate": 1667353441362, "content": {"title": "Consistent Video Style Transfer via Relaxation and Regularization", "abstract": "In recent years, neural style transfer has attracted more and more attention, especially for image style transfer. However, temporally consistent style transfer for videos is still a challenging problem. Existing methods, either relying on a significant amount of video data with optical flows or using single-frame regularizers, fail to handle strong motions or complex variations, therefore have limited performance on real videos. In this article, we address the problem by jointly considering the intrinsic properties of stylization and temporal consistency. We first identify the cause of the conflict between style transfer and temporal consistency, and propose to reconcile this contradiction by relaxing the objective function, so as to make the stylization loss term more robust to motions. Through relaxation, style transfer is more robust to inter-frame variation without degrading the subjective effect. Then, we provide a novel formulation and understanding of temporal consistency. Based on the formulation, we analyze the drawbacks of existing training strategies and derive a new regularization. We show by experiments that the proposed regularization can better balance the spatial and temporal performance. Based on relaxation and regularization, we design a zero-shot video style transfer framework. Moreover, for better feature migration, we introduce a new module to dynamically adjust inter-channel distributions. Quantitative and qualitative results demonstrate the superiority of our method over state-of-the-art style transfer methods. Our project is publicly available at: <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://daooshee.github.io/ReReVST/</uri> ."}}
{"id": "CINbjDXhwVM", "cdate": 1577836800000, "mdate": 1681653559869, "content": {"title": "Deep Plastic Surgery: Robust and Controllable Image Editing with Human-Drawn Sketches", "abstract": ""}}
{"id": "ZOaTGipQOlO", "cdate": 1546300800000, "mdate": 1681653560046, "content": {"title": "TET-GAN: Text Effects Transfer via Stylization and Destylization", "abstract": ""}}
{"id": "8s1vmXrpzoH", "cdate": 1546300800000, "mdate": 1681653559874, "content": {"title": "Controllable Artistic Text Style Transfer via Shape-Matching GAN", "abstract": ""}}
