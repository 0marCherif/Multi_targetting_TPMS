{"id": "byJEJyNudfb", "cdate": 1677628800000, "mdate": 1682408905789, "content": {"title": "Semi-supervised portrait matting using transformer", "abstract": ""}}
{"id": "WYWKZWlnJo4", "cdate": 1677628800000, "mdate": 1682408905725, "content": {"title": "Pose-Guided Hierarchical Semantic Decomposition and Composition for Human Parsing", "abstract": "Human parsing is a fine-grained semantic segmentation task, which needs to understand human semantic parts. Most existing methods model human parsing as a general semantic segmentation, which ignores the inherent relationship among hierarchical human parts. In this work, we propose a pose-guided hierarchical semantic decomposition and composition framework for human parsing. Specifically, our method includes a semantic maintained decomposition and composition (SMDC) module and a pose distillation (PC) module. SMDC progressively disassembles the human body to focus on the more concise regions of interest in the decomposition stage and then gradually assembles human parts under the guidance of pose information in the composition stage. Notably, SMDC maintains the atomic semantic labels during both stages to avoid the error propagation issue of the hierarchical structure. To further take advantage of the relationship of human parts, we introduce pose information as explicit guidance for the composition. However, the discrete structure prediction in pose estimation is against the requirement of the continuous region in human parsing. To this end, we design a PC module to broadcast the maximum responses of pose estimation to form the continuous structure in the way of knowledge distillation. The experimental results on the look-into-person (LIP) and PASCAL-Person-Part datasets demonstrate the superiority of our method compared with the state-of-the-art methods, that is, 55.21% mean Intersection of Union (mIoU) on LIP and 69.88% mIoU on PASCAL-Person-Part."}}
{"id": "FdWw7i4wFCp", "cdate": 1675209600000, "mdate": 1684189272119, "content": {"title": "Dual Attention Mechanism Based Outline Loss for Image Stylization", "abstract": "Image stylization has attracted considerable attention from various fields. Although impressive results have been achieved, existing methods pay less attention to the preservation of outline and putting constraint on it when training, which makes generated images suffering from different degrees of distortion. To address this issue, we propose a dual attention mechanism based outline loss to enhance the restriction of outline consistency by incorporating an outline detection module and a dual attention module. Specifically, an outline detection module is used to detect outlines of the source image and the stylized image, which are further compared and enforced to be consistent with each other by a carefully-elaborated outline loss. Additionally, the dual attention module first guides the model to focus on regions of the source image whose style has the biggest difference from the target image during stylization based on the style attention feature map obtained by the auxiliary classifier. Then, an outline attention map is predicted to highlight regions where the outlines are prone to distort during stylization, which further facilitates the outline loss to execute stronger constraint on these regions. Experimental results show the superiority of our method compared to the existing state-of-the-art methods"}}
{"id": "t8rNCrEXMs", "cdate": 1672531200000, "mdate": 1682408905702, "content": {"title": "MoLo: Motion-augmented Long-short Contrastive Learning for Few-shot Action Recognition", "abstract": "Current state-of-the-art approaches for few-shot action recognition achieve promising performance by conducting frame-level matching on learned visual features. However, they generally suffer from two limitations: i) the matching procedure between local frames tends to be inaccurate due to the lack of guidance to force long-range temporal perception; ii) explicit motion learning is usually ignored, leading to partial information loss. To address these issues, we develop a Motion-augmented Long-short Contrastive Learning (MoLo) method that contains two crucial components, including a long-short contrastive objective and a motion autodecoder. Specifically, the long-short contrastive objective is to endow local frame features with long-form temporal awareness by maximizing their agreement with the global token of videos belonging to the same class. The motion autodecoder is a lightweight architecture to reconstruct pixel motions from the differential features, which explicitly embeds the network with motion dynamics. By this means, MoLo can simultaneously learn long-range temporal context and motion cues for comprehensive few-shot matching. To demonstrate the effectiveness, we evaluate MoLo on five standard benchmarks, and the results show that MoLo favorably outperforms recent advanced methods. The source code is available at https://github.com/alibaba-mmai-research/MoLo."}}
{"id": "UZCVO-dima", "cdate": 1672531200000, "mdate": 1678936264409, "content": {"title": "CLIP-guided Prototype Modulating for Few-shot Action Recognition", "abstract": ""}}
{"id": "QnB89Jk4go", "cdate": 1672531200000, "mdate": 1682408905585, "content": {"title": "Parallel Reasoning Network for Human-Object Interaction Detection", "abstract": "Human-Object Interaction (HOI) detection aims to learn how human interacts with surrounding objects. Previous HOI detection frameworks simultaneously detect human, objects and their corresponding interactions by using a predictor. Using only one shared predictor cannot differentiate the attentive field of instance-level prediction and relation-level prediction. To solve this problem, we propose a new transformer-based method named Parallel Reasoning Network(PR-Net), which constructs two independent predictors for instance-level localization and relation-level understanding. The former predictor concentrates on instance-level localization by perceiving instances' extremity regions. The latter broadens the scope of relation region to reach a better relation-level semantic understanding. Extensive experiments and analysis on HICO-DET benchmark exhibit that our PR-Net effectively alleviated this problem. Our PR-Net has achieved competitive results on HICO-DET and V-COCO benchmarks."}}
{"id": "AQ57ShbpOVF", "cdate": 1672531200000, "mdate": 1682408905786, "content": {"title": "Semantic Segmentation via Pixel-to-Center Similarity Calculation", "abstract": "Since the fully convolutional network has achieved great success in semantic segmentation, lots of works have been proposed focusing on extracting discriminative pixel feature representations. However, we observe that existing methods still suffer from two typical challenges, i.e. (i) large intra-class feature variation in different scenes, (ii) small inter-class feature distinction in the same scene. In this paper, we first rethink semantic segmentation from a perspective of similarity between pixels and class centers. Each weight vector of the segmentation head represents its corresponding semantic class in the whole dataset, which can be regarded as the embedding of the class center. Thus, the pixel-wise classification amounts to computing similarity in the final feature space between pixels and the class centers. Under this novel view, we propose a Class Center Similarity layer (CCS layer) to address the above-mentioned challenges by generating adaptive class centers conditioned on different scenes and supervising the similarities between class centers. It utilizes a Adaptive Class Center Module (ACCM) to generate class centers conditioned on each scene, which adapt the large intra-class variation between different scenes. Specially designed loss functions are introduced to control both inter-class and intra-class distances based on predicted center-to-center and pixel-to-center similarity, respectively. Finally, the CCS layer outputs the processed pixel-to-center similarity as the segmentation prediction. Extensive experiments demonstrate that our model performs favourably against the state-of-the-art CNN-based methods."}}
{"id": "7MT2aU4o9KL", "cdate": 1672531200000, "mdate": 1678936264183, "content": {"title": "HyRSM++: Hybrid Relation Guided Temporal Set Matching for Few-shot Action Recognition", "abstract": ""}}
{"id": "z_kprXPZDI", "cdate": 1668401668966, "mdate": 1668401668966, "content": {"title": "Learning from Untrimmed Videos: Self-Supervised Video Representation Learning with Hierarchical Consistency", "abstract": "Natural videos provide rich visual contents for self-supervised learning. Yet most existing approaches for learning spatio-temporal representations rely on manually trimmed videos, leading to limited diversity in visual patterns and limited performance gain. In this work, we aim to learn representations by leveraging more abundant information in untrimmed videos. To this end, we propose to learn a hierarchy of consistencies in videos, ie, visual consistency and topical consistency, corresponding respectively to clip pairs that tend to be visually similar when separated by a short time span and share similar topics when separated by a long time span. Specifically, a hierarchical consistency learning framework HiCo is presented, where the visually consistent pairs are encouraged to have the same representation through contrastive learning, while the topically consistent pairs are coupled through a topical classifier that distinguishes whether they are topicrelated. Further, we impose a gradual sampling algorithm for proposed hierarchical consistency learning, and demonstrate its theoretical superiority. Empirically, we show that not only HiCo can generate stronger representations on untrimmed videos, it also improves the representation quality when applied to trimmed videos. This is in contrast to standard contrastive learning that fails to learn appropriate representations from untrimmed videos."}}
{"id": "TnkPwoR8Kvx", "cdate": 1667351435776, "mdate": 1667351435776, "content": {"title": "Domain adaptation for image dehazing", "abstract": "Image dehazing"}}
