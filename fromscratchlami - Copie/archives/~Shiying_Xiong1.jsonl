{"id": "D4A-v0kltaX", "cdate": 1601308321291, "mdate": null, "content": {"title": "Neural Partial Differential Equations with Functional Convolution", "abstract": "We present a lightweighted neural PDE representation to discover the hidden structure and predict the solution of different nonlinear PDEs. Our key idea is to leverage the prior of ``\"translational similarity\" of numerical PDE differential operators to drastically reduce the scale of learning model and training data. We implemented three central network components, including a neural functional convolution operator, a Picard forward iterative procedure, and an adjoint backward gradient calculator. Our novel paradigm fully leverages the multifaceted priors that stem from the sparse and smooth nature of the physical PDE solution manifold and the various mature numerical techniques such as adjoint solver, linearization, and iterative procedure to accelerate the computation. We demonstrate the efficacy of our method by robustly discovering the model and accurately predicting the solutions of various types of PDEs with small-scale networks and training sets. We highlight that all the PDE examples we showed were trained with up to 8 data samples and within 325 network parameters. "}}
{"id": "B5VvQrI49Pa", "cdate": 1601308311144, "mdate": null, "content": {"title": "Nonseparable Symplectic Neural Networks", "abstract": "Predicting the behaviors of Hamiltonian systems has been drawing increasing attention in scientific machine learning. However, the vast majority of the literature was focused on predicting separable Hamiltonian systems with their kinematic and potential energy terms being explicitly decoupled, while building data-driven paradigms to predict nonseparable Hamiltonian systems that are ubiquitous in fluid dynamics and quantum mechanics were rarely explored. The main computational challenge lies in the effective embedding of symplectic priors to describe the inherently coupled evolution of position and momentum, which typically exhibits intricate dynamics. To solve the problem, we propose a novel neural network architecture, Nonseparable Symplectic Neural Networks (NSSNNs), to uncover and embed the symplectic structure of a nonseparable Hamiltonian system from limited observation data. The enabling mechanics of our approach is an augmented symplectic time integrator to decouple the position and momentum energy terms and facilitate their evolution. We demonstrated the efficacy and versatility of our method by predicting a wide range of Hamiltonian systems, both separable and nonseparable, including chaotic vortical flows. We showed the unique computational merits of our approach to yield long-term, accurate, and robust predictions for large-scale Hamiltonian systems by rigorously enforcing symplectomorphism."}}
{"id": "k16LHiZVGmF", "cdate": 1601308308003, "mdate": null, "content": {"title": "RoeNets: Predicting Discontinuity of Hyperbolic Systems from Continuous Data", "abstract": "Predicting future discontinuous phenomena that are unobservable from the training data sets has been a challenging problem for scientific machine learning. In this paper, we introduce a novel learning paradigm to predict the emergence and evolution of various kinds of discontinuities for hyperbolic dynamic systems based on smooth observation data. At the heart of our approach is a templaterizable and data-driven Riemann solver that functions as a strong inductive prior to tackle the potential discontinuities.\nThe key design of our templaterized Riemann approximator is inspired by the classical Roe solver (P. L. Roe, J. Comput. Phys., vol. 43, 1981), which served as a fundamental mathematical tool for simulating various hyperbolic systems in computational physics. \nBy carefully designing the computing primitives, data flow, and incorporating a novel pseudoinverse processing module, we enable our data-driven predictor to inherently satisfy all the essential mathematical criteria of a Roe hyperbolic solver derived from the first principles and hence deliver accurate predictions of hyperbolic dynamics."}}
{"id": "_8EQ_gMAHFy", "cdate": 1601308305853, "mdate": null, "content": {"title": "VortexNet: Learning Complex Dynamic Systems with Physics-Embedded Networks", "abstract": "In this paper, we present a novel physics-rooted network structure that dramatically facilitates the learning of complex dynamic systems. Our method is inspired by the Vortex Method in fluid dynamics, whose key idea lies in that, given the observed flow field, instead of describing it with a function of space and time, one can equivalently understand the observation as being caused by a number of Lagrangian particles ----- vortices, flowing with the field.  Since the number of such vortices are much smaller than that of the Eulerian, grid discretization, this Lagrangian discretization in essence encodes the system dynamics on a compact physics-based latent space. Our method enforces such Lagrangian discretization with a Encoder---Dynamics---Decode network structure, and trains it with a novel three-stage curriculum learning algorithm. With data generated from the high precision Eulerian DNS method, our alorithm takes advantage of the simplifying power of the Lagrangian method while persisting the physical integrity.\nThis method fundamentally differs from the current approaches in the field of physics-informed learning, and provides superior results for being more versatile, yielding more physical-correctness with less data sample, and faster to compute at high precision. Beyond providing a viable way of simulating complex fluid at high-precision, our method opens up a brand new horizon for embedding knowledge prior via constructing physically-valid latent spaces, which can be applied to further research areas beyond physical simulation.\n"}}
