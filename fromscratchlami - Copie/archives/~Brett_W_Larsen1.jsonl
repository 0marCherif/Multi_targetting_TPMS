{"id": "nBRNjUPhWr", "cdate": 1664872119734, "mdate": null, "content": {"title": "Unmasking the Lottery Ticket Hypothesis: Efficient Adaptive Pruning for Finding Winning Tickets", "abstract": "Modern deep learning involves training costly, highly overparameterized networks, thus motivating the search for sparser networks that require less compute and memory but can still be trained to the same accuracy as the full network (i.e. matching). Iterative magnitude pruning (IMP) is a state of the art algorithm that can find such highly sparse matching subnetworks, known as winning tickets, that can be retrained from initialization or an early training stage. IMP operates by iterative cycles of training, masking a fraction of smallest magnitude weights, rewinding unmasked weights back to an early training point, and repeating. Despite its simplicity, the underlying principles for when and how IMP finds winning tickets remain elusive. In particular, what useful information does an IMP mask found at the end of training convey to a rewound network near the beginning of training? We find that\u2014at higher sparsities\u2014pairs of pruned networks at successive pruning iterations are connected by a linear path with zero error barrier if and only if they are matching. This indicates that masks found at the end of training encodes information about the identity of an axial subspace that intersects a desired linearly connected mode of a matching sublevel set. We leverage this observation to design a simple adaptive pruning heuristic for speeding up the discovery of winning tickets and achieve a 30% reduction in computation time on CIFAR-100. These results make progress toward demystifying the existence of winning tickets with an eye towards enabling the development of more efficient pruning algorithms."}}
{"id": "xSsW2Am-ukZ", "cdate": 1663850165322, "mdate": null, "content": {"title": "Unmasking the Lottery Ticket Hypothesis: What's Encoded in a Winning Ticket's Mask?", "abstract": "As neural networks get larger and costlier, it is important to find sparse networks that require less compute and memory but can be trained to the same accuracy as the full network (i.e. matching). Iterative magnitude pruning (IMP) is a state of the art algorithm that can find such highly sparse matching subnetworks, known as winning tickets. IMP iterates through cycles of training, pruning a fraction of smallest magnitude weights, rewinding unpruned weights back to an early training point, and repeating. Despite its simplicity, the principles underlying when and how IMP finds winning tickets remain elusive. In particular, what useful information does an IMP mask found at the end of training convey to a rewound network near the beginning of training? How does SGD allow the network to extract this information? And why is iterative pruning needed, i.e. why can't we prune to very high sparsities in one shot? We investigate these questions through the lens of the geometry of the error landscape. First, we find that\u2014at higher sparsities\u2014pairs of pruned networks at successive pruning iterations are connected by a linear path with zero error barrier if and only if they are matching. This indicates that masks found at the end of training convey to the rewind point the identity of an axial subspace that intersects a desired linearly connected mode of a matching sublevel set. Second, we show SGD can exploit this information due to a strong form of robustness: it can return to this mode despite strong perturbations early in training. Third, we show how the flatness of the error landscape at the end of training limits the fraction of weights that can be pruned at each iteration of IMP. This analysis yields a new quantitative link between IMP performance and the Hessian eigenspectrum. Finally, we show that the role of retraining in IMP is to find a network with new small weights to prune. Overall, these results make progress toward demystifying the existence of winning tickets by revealing the fundamental role of error landscape geometry in the algorithms used to find them."}}
{"id": "U5QRuy_LjUY", "cdate": 1653595785430, "mdate": null, "content": {"title": "Pre-Training on a Data Diet: Identifying Sufficient Examples for Early Training", "abstract": "A striking observation about iterative magnitude pruning (IMP; Frankle et al. 2020) is that\u2014after just a few hundred steps of dense training\u2014the method can find a sparse sub-network that can be trained to the same accuracy as the dense network. However, the same does not hold at step 0, i.e., random initialization. In this work, we seek to understand how this early phase of pre-training leads to a good initialization for IMP through the lens of the data distribution. Empirically we observe that, holding the number of pre-training iterations constant, training on a small fraction of (randomly chosen)  data suffices to obtain an equally good initialization for IMP. We additionally observe that by pre-training only on \"easy\" training data we can decrease the number of steps necessary to find a good initialization for IMP compared to training on the full dataset or a randomly chosen subset. Combined, these results provide new insight into the role played by data in the early phase of training."}}
{"id": "QLPzCpu756J", "cdate": 1652737806961, "mdate": null, "content": {"title": "Lottery Tickets on a Data Diet: Finding Initializations with Sparse Trainable Networks", "abstract": "A striking observation about iterative magnitude pruning (IMP; Frankle et al. 2020) is that\u2014after just a few hundred steps of dense training\u2014the method can find a sparse sub-network that can be trained to the same accuracy as the dense network. However, the same does not hold at step 0, i.e. random initialization. In this work, we seek to understand how this early phase of pre-training leads to a good initialization for IMP both through the lens of the data distribution and the loss landscape geometry. Empirically we observe that, holding the number of pre-training iterations constant, training on a small fraction of (randomly chosen) data suffices to obtain an equally good initialization for IMP. We additionally observe that by pre-training only on \"easy\" training data, we can decrease the number of steps necessary to find a good initialization for IMP compared to training on the full dataset or a randomly chosen subset. Finally, we identify novel properties of the loss landscape of dense networks that are predictive of IMP performance, showing in particular that more examples being linearly mode connected in the dense network correlates well with good initializations for IMP. Combined, these results provide new insight into the role played by the early phase training in IMP."}}
{"id": "jQIBqeRO3T", "cdate": 1640995200000, "mdate": 1675220245000, "content": {"title": "Lottery Tickets on a Data Diet: Finding Initializations with Sparse Trainable Networks", "abstract": "A striking observation about iterative magnitude pruning (IMP; Frankle et al. 2020) is that $\\unicode{x2014}$ after just a few hundred steps of dense training $\\unicode{x2014}$ the method can find a sparse sub-network that can be trained to the same accuracy as the dense network. However, the same does not hold at step 0, i.e. random initialization. In this work, we seek to understand how this early phase of pre-training leads to a good initialization for IMP both through the lens of the data distribution and the loss landscape geometry. Empirically we observe that, holding the number of pre-training iterations constant, training on a small fraction of (randomly chosen) data suffices to obtain an equally good initialization for IMP. We additionally observe that by pre-training only on \"easy\" training data, we can decrease the number of steps necessary to find a good initialization for IMP compared to training on the full dataset or a randomly chosen subset. Finally, we identify novel properties of the loss landscape of dense networks that are predictive of IMP performance, showing in particular that more examples being linearly mode connected in the dense network correlates well with good initializations for IMP. Combined, these results provide new insight into the role played by the early phase training in IMP."}}
{"id": "Z261N15u7iH", "cdate": 1640995200000, "mdate": 1654533859393, "content": {"title": "Sketching Matrix Least Squares via Leverage Scores Estimates", "abstract": "We consider the matrix least squares problem of the form $\\| \\mathbf{A} \\mathbf{X}-\\mathbf{B} \\|_F^2$ where the design matrix $\\mathbf{A} \\in \\mathbb{R}^{N \\times r}$ is tall and skinny with $N \\gg r$. We propose to create a sketched version $\\| \\tilde{\\mathbf{A}}\\mathbf{X}-\\tilde{\\mathbf{B}} \\|_F^2$ where the sketched matrices $\\tilde{\\mathbf{A}}$ and $\\tilde{\\mathbf{B}}$ contain weighted subsets of the rows of $\\mathbf{A}$ and $\\mathbf{B}$, respectively. The subset of rows is determined via random sampling based on leverage score estimates for each row. We say that the sketched problem is $\\epsilon$-accurate if its solution $\\tilde{\\mathbf{X}}_{\\rm \\text{opt}} = \\text{argmin } \\| \\tilde{\\mathbf{A}}\\mathbf{X}-\\tilde{\\mathbf{B}} \\|_F^2$ satisfies $\\|\\mathbf{A}\\tilde{\\mathbf{X}}_{\\rm \\text{opt}}-\\mathbf{B} \\|_F^2 \\leq (1+\\epsilon) \\min \\| \\mathbf{A}\\mathbf{X}-\\mathbf{B} \\|_F^2$ with high probability. We prove that the number of samples required for an $\\epsilon$-accurate solution is $O(r/(\\beta \\epsilon))$ where $\\beta \\in (0,1]$ is a measure of the quality of the leverage score estimates."}}
{"id": "WnUTv72UzKY", "cdate": 1640995200000, "mdate": 1675220245047, "content": {"title": "Unmasking the Lottery Ticket Hypothesis: What's Encoded in a Winning Ticket's Mask?", "abstract": "Modern deep learning involves training costly, highly overparameterized networks, thus motivating the search for sparser networks that can still be trained to the same accuracy as the full network (i.e. matching). Iterative magnitude pruning (IMP) is a state of the art algorithm that can find such highly sparse matching subnetworks, known as winning tickets. IMP operates by iterative cycles of training, masking smallest magnitude weights, rewinding back to an early training point, and repeating. Despite its simplicity, the underlying principles for when and how IMP finds winning tickets remain elusive. In particular, what useful information does an IMP mask found at the end of training convey to a rewound network near the beginning of training? How does SGD allow the network to extract this information? And why is iterative pruning needed? We develop answers in terms of the geometry of the error landscape. First, we find that$\\unicode{x2014}$at higher sparsities$\\unicode{x2014}$pairs of pruned networks at successive pruning iterations are connected by a linear path with zero error barrier if and only if they are matching. This indicates that masks found at the end of training convey the identity of an axial subspace that intersects a desired linearly connected mode of a matching sublevel set. Second, we show SGD can exploit this information due to a strong form of robustness: it can return to this mode despite strong perturbations early in training. Third, we show how the flatness of the error landscape at the end of training determines a limit on the fraction of weights that can be pruned at each iteration of IMP. Finally, we show that the role of retraining in IMP is to find a network with new small weights to prune. Overall, these results make progress toward demystifying the existence of winning tickets by revealing the fundamental role of error landscape geometry."}}
{"id": "L5R-6UsZJV", "cdate": 1640995200000, "mdate": 1675220244996, "content": {"title": "Towards a more general understanding of the algorithmic utility of recurrent connections", "abstract": "Author summary In striking contrast to the majority of current-day artificial neural network research which primarily focuses on feedforward architectures, biological brains make extensive use of lateral and recurrent connections. This raises the possibility that this difference makes a fundamental contribution to the gap in computational power between real neural circuits and artificial neural networks. Thus, despite the difficulty of making effective comparisons between different network architectures, developing a more detailed understanding of the computational role played by such connections is a pressing challenge. Here, we leverage the computational capabilities of large-scale machine learning to robustly explore how differences in architectures affect a network\u2019s ability to learn tasks that require propagation of global information. We first focus on the task of determining whether two pixels are connected in an image which has an elegant and efficient recurrent solution: propagate a connected label or tag along paths. Inspired by this solution, we show that it can be generalized in many ways, including propagating multiple tags at once and changing the computation performed on the result of the propagation. Strikingly, this simple expansion of the tag propagation network is sufficient to solve a crucial abstraction to temporal connectedness at the core of many decision-making problems, which we illustrate for an abstracted competitive foraging task. Our results shed light on the set of computational tasks that can be solved efficiently by recurrent computation and how these solutions may relate to the structure of neural activity."}}
{"id": "5z2mLnRTQH", "cdate": 1640995200000, "mdate": 1675220244998, "content": {"title": "How many degrees of freedom do we need to train deep networks: a loss landscape perspective", "abstract": "A variety of recent works, spanning pruning, lottery tickets, and training within random subspaces, have shown that deep neural networks can be trained using far fewer degrees of freedom than the total number of parameters. We analyze this phenomenon for random subspaces by first examining the success probability of hitting a training loss sublevel set when training within a random subspace of a given training dimensionality. We find a sharp phase transition in the success probability from $0$ to $1$ as the training dimension surpasses a threshold. This threshold training dimension increases as the desired final loss decreases, but decreases as the initial loss decreases. We then theoretically explain the origin of this phase transition, and its dependence on initialization and final desired loss, in terms of properties of the high-dimensional geometry of the loss landscape. In particular, we show via Gordon's escape theorem, that the training dimension plus the Gaussian width of the desired loss sublevel set, projected onto a unit sphere surrounding the initialization, must exceed the total number of parameters for the success probability to be large. In several architectures and datasets, we measure the threshold training dimension as a function of initialization and demonstrate that it is a small fraction of the total parameters, implying by our theory that successful training with so few dimensions is possible precisely because the Gaussian width of low loss sublevel sets is very large. Moreover, we compare this threshold training dimension to more sophisticated ways of reducing training degrees of freedom, including lottery tickets as well as a new, analogous method: lottery subspaces."}}
{"id": "ChMLTGRjFcU", "cdate": 1632875690008, "mdate": null, "content": {"title": "How many degrees of freedom do we need to train deep networks: a loss landscape perspective", "abstract": "A variety of recent works, spanning pruning, lottery tickets, and training within random subspaces, have shown that deep neural networks can be trained using far fewer degrees of freedom than the total number of parameters. We analyze this phenomenon for random subspaces by first examining the success probability of hitting a training loss sublevel set when training within a random subspace of a given training dimensionality.  We find a sharp phase transition in the success probability from $0$ to $1$ as the training dimension surpasses a threshold. This threshold training dimension increases as the desired final loss decreases, but decreases as the initial loss decreases. We then theoretically explain the origin of this phase transition, and its dependence on initialization and final desired loss, in terms of properties of the high-dimensional geometry of the loss landscape.  In particular, we show via Gordon's escape theorem, that the training dimension plus the Gaussian width of the desired loss sublevel set, projected onto a unit sphere surrounding the initialization, must exceed the total number of parameters for the success probability to be large.  In several architectures and datasets, we measure the threshold training dimension as a function of initialization and demonstrate that it is a small fraction of the total parameters, implying by our theory that successful training with so few dimensions is possible precisely because the Gaussian width of low loss sublevel sets is very large. Moreover, we compare this threshold training dimension to more sophisticated ways of reducing training degrees of freedom, including lottery tickets as well as a new, analogous method: lottery subspaces. "}}
