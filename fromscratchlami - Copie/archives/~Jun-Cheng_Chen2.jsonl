{"id": "uu2aoHG5MHu", "cdate": 1683908690928, "mdate": null, "content": {"title": "Lightweight Deep Learning: An Overview", "abstract": "With the recent success of the deep neural networks (DNNs) in the field of artificial intelligence, the urge of deploying DNNs has drawn tremendous attention because it can benefit a wide range of applications on edge or embedded devices. Lightweight deep learning (DL) indicates the procedures of compressing DNN models into more compact ones, which are suitable to be executed on edge devices due to their limited resources and computational capabilities while maintaining comparable performance as the original. Currently, the approaches of model compression include but not limited to network pruning, quantization, knowledge distillation, neural architecture search. In this work, we present a fresh overview to summarize recent development and challenges for model compression."}}
{"id": "S92kUJw6vm", "cdate": 1668591990800, "mdate": 1668591990800, "content": {"title": "Continual Learning for Visual Search with Backward Consistent Feature Embedding", "abstract": "In visual search, the gallery set could be incrementally growing and added to the database in practice. However,\nexisting methods rely on the model trained on the entire dataset, ignoring the continual updating of the model. Besides, as the model updates, the new model must re-extract features for the entire gallery set to maintain compatible feature space, imposing a high computational cost for a large gallery set. To address the issues of long-term visual search, we introduce a continual learning (CL) approach that can handle the incrementally growing gallery set with backward embedding consistency. We enforce the losses of inter-session data coherence, neighbor-session model coherence, and intra-session discrimination to conduct a continual learner. In addition to the disjoint setup, our CL solution also tackles the situation of increasingly adding new classes for the blurry boundary without assuming all categories known in the beginning and during model update. To our knowledge, this is the first CL method both tackling the issue of backward-consistent feature embedding and allowing novel classes to occur in the new sessions. Extensive experiments on various benchmarks show the efficacy of our approach under a wide range of setups."}}
{"id": "Tm_qlEjc2B", "cdate": 1668591510223, "mdate": 1668591510223, "content": {"title": "Class-Aware Robust Adversarial Training for Object Detection", "abstract": "Object detection is an important computer vision task with plenty of real-world applications; therefore, how to\nenhance its robustness against adversarial attacks has emerged as a crucial issue. However, most of the previous defense methods focused on the classification task and had few analysis in the context of the object detection task. In this work, to address the issue, we present a novel class-aware robust adversarial training paradigm for the object detection task. For a given image, the proposed approach generates an universal adversarial perturbation to simultaneously attack all the occurred objects in the image through jointly maximizing the respective loss for each object. Meanwhile, instead of normalizing the total loss with the number of objects, the proposed approach decomposes the total loss into class-wise losses and normalizes each class loss using the number of objects for the class. The\nadversarial training based on the class weighted loss can not only balances the influence of each class but also effectively and evenly improves the adversarial robustness of trained models for all the object classes as compared with the previous defense methods. Furthermore, with the recent development of fast adversarial training, we provide a fast version of the proposed algorithm which can be trained faster than the traditional adversarial training while keeping comparable performance. With extensive experiments on the challenging PASCAL-VOC and MS-COCO datasets, the evaluation results demonstrate that the proposed defense methods can effectively enhance the robustness of the object detection models.\n"}}
{"id": "o_Qrw9f512w", "cdate": 1663849948609, "mdate": null, "content": {"title": "Learning to Count Everything: Transformer-based Trackers are Strong Baselines for Class Agnostic Counting", "abstract": "Class agnostic counting (CAC) is a vision task which can be used to count the total occurrence number of any given reference objects in the query image. The task is usually formulated as density map estimation problem through similarity computation among few image samples of the reference object and the query image. In this paper, we show the the popular and effective similarity computation operation, bilinear similarity, actually share high resemblance with self-attention and cross-attention operations which are widely used in the transformer architecture. Inspired by this observation, since the formulation of visual object tracking task is similar to CAC, we show the advanced attention modules of transformer-based trackers are actually powerful matching tools for the CAC task. These modules allow to learn more distinct features to capture the shared patterns among the query and reference images. In addition, we propose a transformer-based class agnostic counting framework by adapting transformer-based trackers for CAC. We demonstrate the effectiveness of the proposed framework with two state-of-the-art transformer-based trackers, MixFormer and TransT, with extensive experiments and ablation studies. The proposed methods outperform other state-of-the-art methods on the challenging FSC-147 and CARPK datasets and achieve new state-of-the-art performances. The code will be publicly available upon acceptance. "}}
{"id": "cviGW9mqsP", "cdate": 1582060746013, "mdate": null, "content": {"title": "A Dual Path Model With Adaptive Attention For Vehicle Re-Identification", "abstract": "In recent years, attention models have been extensively\nused for person and vehicle re-identification. Most reidentification methods are designed to focus attention on\nkey-point locations. However, depending on the orientation, the contribution of each key-point varies. In this paper,\nwe present a novel dual-path adaptive attention model for\nvehicle re-identification (AAVER). The global appearance\npath captures macroscopic vehicle features while the orientation conditioned part appearance path learns to capture\nlocalized discriminative features by focusing attention\non the most informative key-points. Through extensive\nexperimentation, we show that the proposed AAVER method\nis able to accurately re-identify vehicles in unconstrained\nscenarios, yielding state of the art results on the challenging dataset VeRi-776. As a byproduct, the proposed system\nis also able to accurately predict vehicle key-points and\nshows an improvement of more than 7% over state of the\nart"}}
{"id": "ZK9iNCCPh6", "cdate": 1581954525139, "mdate": null, "content": {"title": "Unconstrained face verification using deep cnn features", "abstract": "In this paper, we present an algorithm for unconstrained face verification based on deep convolutional features and\nevaluate it on the newly released IARPA Janus Benchmark A (IJB-A) dataset as well as on the traditional Labeled Face\nin the Wild (LFW) dataset. The IJB-A dataset includes realworld unconstrained faces from 500 subjects with full pose\nand illumination variations which are much harder than the LFW and Youtube Face (YTF) datasets. The deep convolutional neural network (DCNN) is trained using the CASIA-WebFace dataset. Results of experimental evaluations on the IJB-A and the LFW datasets are provided."}}
{"id": "BQ4DC1XxOaH", "cdate": 1546300800000, "mdate": null, "content": {"title": "Unsupervised Domain-Specific Deblurring via Disentangled Representations.", "abstract": "Image deblurring aims to restore the latent sharp images from the corresponding blurred ones. In this paper, we present an unsupervised method for domain-specific, single-image deblurring based on disentangled representations. The disentanglement is achieved by splitting the content and blur features in a blurred image using content encoders and blur encoders. We enforce a KL divergence loss to regularize the distribution range of extracted blur attributes such that little content information is contained. Meanwhile, to handle the unpaired training data, a blurring branch and the cycle-consistency loss are added to guarantee that the content structures of the deblurred results match the original images. We also add an adversarial loss on deblurred results to generate visually realistic images and a perceptual loss to further mitigate the artifacts. We perform extensive experiments on the tasks of face and text deblurring using both synthetic datasets and real images, and achieve improved results compared to recent state-of-the-art deblurring methods."}}
{"id": "BixgNep8e_6H", "cdate": 1514764800000, "mdate": null, "content": {"title": "Unconstrained Still/Video-Based Face Verification with Deep Convolutional Neural Networks.", "abstract": "Over the last 5 years, methods based on Deep Convolutional Neural Networks (DCNNs) have shown impressive performance improvements for object detection and recognition problems. This has been made possible due to the availability of large annotated datasets, a better understanding of the non-linear mapping between input images and class labels as well as the affordability of GPUs. In this paper, we present the design details of a deep learning system for unconstrained face recognition, including modules for face detection, association, alignment and face verification. The quantitative performance evaluation is conducted using the IARPA Janus Benchmark A (IJB-A), the JANUS Challenge Set 2 (JANUS CS2), and the Labeled Faces in the Wild (LFW) dataset. The IJB-A dataset includes real-world unconstrained faces of 500 subjects with significant pose and illumination variations which are much harder than the LFW and Youtube Face datasets. JANUS CS2 is the extended version of IJB-A which contains not only all the images/frames of IJB-A but also includes the original videos. Some open issues regarding DCNNs for face verification problems are then discussed."}}
{"id": "B1V72hZO-H", "cdate": 1514764800000, "mdate": null, "content": {"title": "A Semi-Automatic 2D Solution for Vehicle Speed Estimation From Monocular Videos", "abstract": "In this work, we present a novel approach for vehicle speed estimation from monocular videos. The pipeline consists of modules for multi-object detection, robust tracking, and speed estimation. The tracking algorithm has the capability for jointly tracking individual vehicles and estimating velocities in the image domain. However, since camera parameters are often unavailable and extensive variations are present in the scenes, transforming measurements in the image domain to real world is challenging. We propose a simple two-stage algorithm to approximate the transformation. Images are first rectified to restore affine properties, then the scaling factor is compensated for each scene. We show the effectiveness of the proposed method with extensive experiments on the traffic speed analysis dataset in the NVIDIA AI City challenge. We achieve a detection rate of 1.0 in vehicle detection and tracking, and Root Mean Square Error of 9.54 (mph) for the task of vehicle speed estimation in unconstrained traffic videos."}}
{"id": "B14S6JM_-B", "cdate": 1514764800000, "mdate": null, "content": {"title": "Deep Density Clustering of Unconstrained Faces", "abstract": "In this paper, we consider the problem of grouping a collection of unconstrained face images in which the number of subjects is not known. We propose an unsupervised clustering algorithm called Deep Density Clustering (DDC) which is based on measuring density affinities between local neighborhoods in the feature space. By learning the minimal covering sphere for each neighborhood, information about the underlying structure is encapsulated. The encapsulation is also capable of locating high-density region of the neighborhood, which aids in measuring the neighborhood similarity. We theoretically show that the encapsulation asymptotically converges to a Parzen window density estimator. Our experiments show that DDC is a superior candidate for clustering unconstrained faces when the number of subjects is unknown. Unlike conventional linkage and density-based methods that are sensitive to the selection operating points, DDC attains more consistent and improved performance. Furthermore, the density-aware property reduces the difficulty in finding appropriate operating points."}}
