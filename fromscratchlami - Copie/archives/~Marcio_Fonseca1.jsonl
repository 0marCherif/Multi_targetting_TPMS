{"id": "dpTAw1Ojo8n", "cdate": 1640995200000, "mdate": 1664209721213, "content": {"title": "Factorizing Content and Budget Decisions in Abstractive Summarization of Long Documents by Sampling Summary Views", "abstract": "We argue that disentangling content selection from the budget used to cover salient content improves the performance and applicability of abstractive summarizers. Our method, FactorSum, does this disentanglement by factorizing summarization into two steps through an energy function: (1) generation of abstractive summary views; (2) combination of these views into a final summary, following a budget and content guidance. This guidance may come from different sources, including from an advisor model such as BART or BigBird, or in oracle mode -- from the reference. This factorization achieves significantly higher ROUGE scores on multiple benchmarks for long document summarization, namely PubMed, arXiv, and GovReport. Most notably, our model is effective for domain adaptation. When trained only on PubMed samples, it achieves a 46.29 ROUGE-1 score on arXiv, which indicates a strong performance due to more flexible budget adaptation and content selection less dependent on domain-specific textual structure."}}
{"id": "A6dxTMHoiWs", "cdate": 1609459200000, "mdate": 1664209768694, "content": {"title": "Evaluating Topic Models in Portuguese Political Comments About Bills from Brazil's Chamber of Deputies", "abstract": "The popular participation in Law-making is an important resource in the evolution of Democracy and Direct Legislation. The amount of legislative documents produced within the past decade has risen dramatically, making it difficult for law practitioners to attend to legislation and still listen to the opinion of the citizens. This work focuses on the use of topic models for summarizing and visualizing Brazilian comments about legislation (bills). In this paper, we provide a qualitative evaluation from a legal expert and compare it with the topics predicted by our model. For such, we designed a specific sentence embedding technique able to induce models for Portuguese texts, and we used these models as topic model, obtaining very good results. We experimentally compared our proposal with other techniques for multilingual sentence embeddings, evaluating them in three topical corpora prepared by us, two of them annotated by a specialist and the other automatically annotated by hashtags."}}
{"id": "HJgiVXY88r", "cdate": 1568211762954, "mdate": null, "content": {"title": "Learning to predict visual brain activity by predicting future sensory states", "abstract": "Deep predictive coding networks are neuroscience-inspired unsupervised learning models that learn to predict future sensory states. We build upon the PredNet implementation by Lotter, Kreiman, and Cox (2016) to investigate if predictive coding representations are useful to predict brain activity in the visual cortex. We use representational similarity analysis (RSA) to compare PredNet representations to functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG) data from the Algonauts Project (Cichy et al., 2019). In contrast to previous findings in the literature (Khaligh-Razavi & Kriegeskorte, 2014), we report empirical data suggesting that unsupervised models trained to predict frames of videos without further fine-tuning may outperform supervised image classification baselines in terms of correlation to spatial (fMRI) and temporal (MEG) data."}}
{"id": "vsrQjYyOSX", "cdate": 1546300800000, "mdate": 1664209721213, "content": {"title": "Unsupervised predictive coding models may explain visual brain representation", "abstract": "Deep predictive coding networks are neuroscience-inspired unsupervised learning models that learn to predict future sensory states. We build upon the PredNet implementation by Lotter, Kreiman, and Cox (2016) to investigate if predictive coding representations are useful to predict brain activity in the visual cortex. We use representational similarity analysis (RSA) to compare PredNet representations to functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG) data from the Algonauts Project. In contrast to previous findings in the literature (Khaligh-Razavi &Kriegeskorte, 2014), we report empirical data suggesting that unsupervised models trained to predict frames of videos may outperform supervised image classification baselines. Our best submission achieves an average noise normalized score of 16.67% and 27.67% on the fMRI and MEG tracks of the Algonauts Challenge."}}
