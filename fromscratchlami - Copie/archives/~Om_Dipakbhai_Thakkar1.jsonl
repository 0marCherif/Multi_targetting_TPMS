{"id": "q2Y4V9wJXPd", "cdate": 1674503423999, "mdate": 1674503423999, "content": {"title": "Recycling Scraps: Improving Private Learning by Leveraging Intermediate Checkpoints", "abstract": "All state-of-the-art (SOTA) differentially private machine learning (DP ML) methods are iterative in nature, and their privacy analyses allow publicly releasing the intermediate training checkpoints. However, DP ML benchmarks, and even practical deployments, typically use only the final training checkpoint to make predictions. In this work, for the first time, we comprehensively explore various methods that aggregate intermediate checkpoints to improve the utility of DP training. Empirically, we demonstrate that checkpoint aggregations provide significant gains in the prediction accuracy over the existing SOTA for CIFAR10 and StackOverflow datasets, and that these gains get magnified in settings with periodically varying training data distributions. For instance, we improve SOTA StackOverflow accuracies to 22.7% (+0.43% absolute) for eps=8.2, and 23.84% (+0.43%) for eps=18.9. Theoretically, we show that uniform tail averaging of checkpoints improves the empirical risk minimization bound compared to the last checkpoint of DP-SGD. Lastly, we initiate an exploration into estimating the uncertainty that DP noise adds in the predictions of DP ML models. We prove that, under standard assumptions on the loss function, the sample variance from last few checkpoints provides a good approximation of the variance of the final model of a DP run. Empirically, we show that the last few checkpoints can provide a reasonable lower bound for the variance of a converged DP model."}}
{"id": "IqyE47_SVKQ", "cdate": 1672531200000, "mdate": 1681753835153, "content": {"title": "Why Is Public Pretraining Necessary for Private Model Training?", "abstract": "In the privacy-utility tradeoff of a model trained on benchmark language and vision tasks, remarkable improvements have been widely reported with the use of pretraining on publicly available data. This is in part due to the benefits of transfer learning, which is the standard motivation for pretraining in non-private settings. However, the stark contrast in the improvement achieved through pretraining under privacy compared to non-private settings suggests that there may be a deeper, distinct cause driving these gains. To explain this phenomenon, we hypothesize that the non-convex loss landscape of a model training necessitates an optimization algorithm to go through two phases. In the first, the algorithm needs to select a good \"basin\" in the loss landscape. In the second, the algorithm solves an easy optimization within that basin. The former is a harder problem to solve with private data, while the latter is harder to solve with public data due to a distribution shift or data scarcity. Guided by this intuition, we provide theoretical constructions that provably demonstrate the separation between private training with and without public pretraining. Further, systematic experiments on CIFAR10 and LibriSpeech provide supporting evidence for our hypothesis."}}
{"id": "IskSBCo0-0", "cdate": 1663850443087, "mdate": null, "content": {"title": "Recycling Scraps: Improving Private Learning by Leveraging Intermediate Checkpoints", "abstract": "All state-of-the-art (SOTA) differentially private machine learning (DP ML) methods are iterative in nature, and their privacy analyses allow publicly releasing the intermediate training checkpoints. However, DP ML benchmarks, and even practical deployments, typically use only the final training checkpoint to make predictions. In this work, for the first time, we comprehensively explore various methods that aggregate intermediate checkpoints to improve the utility of DP training. Empirically, we demonstrate that checkpoint aggregations provide significant gains in the prediction accuracy over the existing SOTA for CIFAR10 and StackOverflow datasets, and that these gains get magnified in settings with periodically varying training data distributions.  For instance, we improve  SOTA StackOverflow accuracies to 22.7\\% (+0.43\\% absolute) for $\\epsilon=8.2$, and 23.84\\%  (+0.43\\%) for $\\epsilon=18.9$. Theoretically, we show that uniform tail averaging of checkpoints improves the empirical risk minimization bound compared to the last checkpoint of DP-SGD. Lastly, we initiate an exploration into estimating the uncertainty that DP noise adds in the predictions of DP ML models. We prove that, under standard assumptions on the loss function, the sample variance from last few checkpoints provides a good approximation of the variance of the final model of a DP run. Empirically, we show that the last few checkpoints can provide a reasonable lower bound for the variance of a converged DP model. "}}
{"id": "7bJizxLKrR", "cdate": 1663850163656, "mdate": null, "content": {"title": "Measuring Forgetting of Memorized Training Examples", "abstract": "Machine learning models exhibit two seemingly contradictory phenomena: training data memorization and various forms of forgetting. In memorization, models overfit specific training examples and become susceptible to privacy attacks. In forgetting, examples which appeared early in training are forgotten by the end. In this work, we connect these phenomena.\nWe propose a technique to measure to what extent models ``forget'' the specifics of training examples, becoming less susceptible to privacy attacks on examples they have not seen recently.\nWe show that, while non-convexity can prevent forgetting from happening in the worst-case, standard image,speech, and language models empirically do forget examples over time.\nWe identify nondeterminism as a potential explanation, showing that deterministically trained models do not forget.\nOur results suggest that examples seen early when training with extremely large datasets---for instance those examples used to pre-train a model---may observe privacy benefits at the expense of examples seen later."}}
{"id": "zC1inM9_ZG2", "cdate": 1640995200000, "mdate": 1681491092210, "content": {"title": "Extracting Targeted Training Data from ASR Models, and How to Mitigate It", "abstract": ""}}
{"id": "mfUWYtZIdAi", "cdate": 1640995200000, "mdate": 1677868325834, "content": {"title": "Detecting Unintended Memorization in Language-Model-Fused ASR", "abstract": ""}}
{"id": "ebgSI8voYG_", "cdate": 1640995200000, "mdate": 1682354696745, "content": {"title": "A Method to Reveal Speaker Identity in Distributed ASR Training, and How to Counter IT", "abstract": "End-to-end Automatic Speech Recognition (ASR) models are commonly trained over spoken utterances using optimization methods like Stochastic Gradient Descent (SGD). In distributed settings like Federated Learning, model training requires transmission of gradients over a network. In this work, we design the first method for revealing the identity of the speaker of a training utterance with access only to a gradient. We propose Hessian-Free Gradients Matching, an input reconstruction technique that operates without second derivatives of the loss function (required in prior works), which can be expensive to compute. We show the effectiveness of our method using the DeepSpeech model architecture, demonstrating that it is possible to reveal the speaker\u2019s identity with 34% top-1 accuracy (51% top-5 accuracy) on the LibriSpeech dataset. Further, we study the effect of Dropout on the success of our method. We show that a dropout rate of 0.2 can reduce the speaker identity accuracy to 0% top-1 (0.5% top-5)."}}
{"id": "YLvpb5htEUw", "cdate": 1640995200000, "mdate": 1681491092188, "content": {"title": "Extracting Targeted Training Data from ASR Models, and How to Mitigate It", "abstract": ""}}
{"id": "YIvch6WUyH", "cdate": 1640995200000, "mdate": 1681753835197, "content": {"title": "Recycling Scraps: Improving Private Learning by Leveraging Intermediate Checkpoints", "abstract": "All state-of-the-art (SOTA) differentially private machine learning (DP ML) methods are iterative in nature, and their privacy analyses allow publicly releasing the intermediate training checkpoints. However, DP ML benchmarks, and even practical deployments, typically use only the final training checkpoint to make predictions. In this work, for the first time, we comprehensively explore various methods that aggregate intermediate checkpoints to improve the utility of DP training. Empirically, we demonstrate that checkpoint aggregations provide significant gains in the prediction accuracy over the existing SOTA for CIFAR10 and StackOverflow datasets, and that these gains get magnified in settings with periodically varying training data distributions. For instance, we improve SOTA StackOverflow accuracies to 22.7% (+0.43% absolute) for $\\epsilon=8.2$, and 23.84% (+0.43%) for $\\epsilon=18.9$. Theoretically, we show that uniform tail averaging of checkpoints improves the empirical risk minimization bound compared to the last checkpoint of DP-SGD. Lastly, we initiate an exploration into estimating the uncertainty that DP noise adds in the predictions of DP ML models. We prove that, under standard assumptions on the loss function, the sample variance from last few checkpoints provides a good approximation of the variance of the final model of a DP run. Empirically, we show that the last few checkpoints can provide a reasonable lower bound for the variance of a converged DP model."}}
{"id": "W9s2krLz_6L", "cdate": 1640995200000, "mdate": 1677868325753, "content": {"title": "Detecting Unintended Memorization in Language-Model-Fused ASR", "abstract": ""}}
