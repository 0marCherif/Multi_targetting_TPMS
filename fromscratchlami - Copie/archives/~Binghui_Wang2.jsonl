{"id": "sQsIyObLQUZ", "cdate": 1675209600000, "mdate": 1682339621735, "content": {"title": "DisP+V: A Unified Framework for Disentangling Prototype and Variation From Single Sample per Person", "abstract": ""}}
{"id": "nDDWMfJP0Ka", "cdate": 1672531200000, "mdate": 1682339621832, "content": {"title": "IDGI: A Framework to Eliminate Explanation Noise from Integrated Gradients", "abstract": ""}}
{"id": "J9knEULnZ5", "cdate": 1672531200000, "mdate": 1682339621833, "content": {"title": "Turning Strengths into Weaknesses: A Certified Robustness Inspired Attack Framework against Graph Neural Networks", "abstract": ""}}
{"id": "1UDtqybGTI", "cdate": 1672531200000, "mdate": 1682339621675, "content": {"title": "A Certified Radius-Guided Attack Framework to Image Segmentation Models", "abstract": ""}}
{"id": "1Q3hNfLgoo", "cdate": 1672531200000, "mdate": 1682339621644, "content": {"title": "Interpreting Disparate Privacy-Utility Tradeoff in Adversarial Learning via Attribute Correlation", "abstract": ""}}
{"id": "m4LuXs62vp", "cdate": 1668058897732, "mdate": 1668058897732, "content": {"title": "UniCR: Universally Approximated Certified Robustness via Randomized Smoothing", "abstract": "We study certified robustness of machine learning classifiers against adversarial perturbations. In particular, we propose the first universally approximated certified robustness (UniCR) framework, which can approximate the robustness certification of any input on any classifier against any perturbations with noise generated by any continuous probability distribution. Compared with the state-of-the-art certified defenses, UniCR provides many significant benefits: (1) the first universal robustness certification framework for the above 4 \u201cany\u201ds; (2) automatic robustness certification that avoids case-by-case analysis, (3) tightness validation of certified robustness, and (4) optimality validation of noise distributions used by randomized smoothing. We conduct extensive experiments to validate the above benefits of UniCR and the advantages of UniCR over state-of-the-art certified defenses against  perturbations."}}
{"id": "V58aHM1Jdd", "cdate": 1667361839670, "mdate": null, "content": {"title": "DisP+V: A Unified Framework for Disentangling Prototype and Variation From Single Sample per Person", "abstract": "Single sample per person face recognition (SSPP FR) is one of the most challenging problems in FR due to the extreme lack of enrolment data. To date, the most popular SSPP FR methods are the generic learning methods, which recognize query face images based on the so-called prototype plus variation (i.e., P+V) model. However, the classic P+V model suffers from two major limitations: 1) it linearly combines the prototype and variation images in the observational pixel-spatial space and cannot generalize to multiple nonlinear variations, e.g., poses, which are common in face images and 2) it would be severely impaired once the enrolment face images are contaminated by nuisance variations. To address the two limitations, it is desirable to disentangle the prototype and variation in a latent feature space and to manipulate the images in a semantic manner. To this end, we propose a novel disentangled prototype plus variation model, dubbed DisP+V, which consists of an encoder-decoder generator and two discriminators. The generator and discriminators play two adversarial games such that the generator nonlinearly encodes the images into a latent semantic space, where the more discriminative prototype feature and the less discriminative variation feature are disentangled. Meanwhile, the prototype and variation features can guide the generator to generate an identity-preserved prototype and the corresponding variation, respectively. Experiments on various real-world face datasets demonstrate the superiority of our DisP+V model over the classic P+V model for SSPP FR. Furthermore, DisP+V demonstrates its unique characteristics in both prototype recovery and face editing/interpolation."}}
{"id": "2skHw9HVf3", "cdate": 1663850116675, "mdate": null, "content": {"title": "TAPPFL: TASK-AGNOSTIC PRIVACY-PRESERVING REPRESENTATION LEARNING FOR FEDERATED LEARNING AGAINST ATTRIBUTE INFERENCE ATTACKS", "abstract": "Federated learning (FL), a new collaborative learning paradigm, has been widely studied recently due to its property to collaboratively train data from different sources without needing to share the raw training data. Nevertheless, recent studies show that an adversary (e.g., an honest-but-curious server) can still be possible to infer private information about the training data, e.g., sensitive information such as income, race, and sexual orientation. To mitigate the attribute inference attacks, various existing privacy-preserving FL methods can be adopted/adapted. However, all these existing methods have key limitations: they need to know the FL task in advance, or have intolerable computational overheads or utility losses, or do not have provable privacy guarantees. We aim to address all these issues and design a task-agnostic privacy-preserving FL (short for TAPPFL) method against attribute inference attacks from the information-theoretic perspective. Specifically, we formally formulate TAPPFL via two mutual information goals, where one goal learns task-agnostic data representations that contain the least information about the private attribute in each device\u2019s data, and the other goal includes as much information as possible about the training data to maintain utility. However, it is intractable to compute exact mutual information in general. Then, we derive tractable variational mutual information bounds, and each bound can be parameterized via a neural network. Next, we alternatively train these parameterized neural networks to approximate the true mutual information and learn privacy-preserving representations for device data. We also derive theoretical privacy guarantees of our TAPPFL against worst-case attribute inference attacks. Extensive results on multiple datesets and applications validates the effectiveness of our TAPPFL to protect data privacy, maintain the FL utility, and be efficient as well."}}
{"id": "3FdmckXo3cN", "cdate": 1663850001990, "mdate": null, "content": {"title": "BPFL: Towards Efficient Byzantine-Robust and Provably Privacy-Preserving Federated Learning", "abstract": "Federated learning (FL) is an emerging distributed learning paradigm without sharing participating clients' private data. However, existing works show that FL is vulnerable to both Byzantine (security) attacks and data reconstruction (privacy) attacks. Existing FL defenses only address one of the two attacks, and also face the efficiency issue. We propose BPFL, an efficient Byzantine-robust and provably privacy-preserving FL method that addresses all the issues. Specifically, we draw on the state-of-the-art Byzantine-robust FL method and use similarity metrics to measure the robustness of each participating client  in FL. The validity of clients are formulated as circuit constraints on similarity metrics  and verified via a zero-knowledge proof. Moreover, the client models are masked by a shared random vector, which is generated based on homomorphic encryption. In doing so, the server receives the masked client models rather than the true ones, which are proven to be private. BPFL is also efficient due to the usage of non-interactive zero-knowledge proof. Experimental results on various datasets show that our BPFL is efficient, Byzantine-robust, and privacy-preserving."}}
{"id": "yPfpSTrfg9", "cdate": 1640995200000, "mdate": 1668522568507, "content": {"title": "Bandits for Structure Perturbation-based Black-box Attacks to Graph Neural Networks with Theoretical Guarantees", "abstract": "Graph neural networks (GNNs) have achieved state-of-the-art performance in many graph-based tasks such as node classification and graph classification. However, many recent works have demonstrated that an attacker can mislead GNN models by slightly perturbing the graph structure. Existing attacks to GNNs are either under the less practical threat model where the attacker is assumed to access the GNN model parameters, or under the practical black-box threat model but consider perturbing node features that are shown to be not enough effective. In this paper, we aim to bridge this gap and consider black-box attacks to GNNs with structure perturbation as well as with theoretical guarantees. We propose to address this challenge through bandit techniques. Specifically, we formulate our attack as an online optimization with bandit feedback. This original problem is essentially NP-hard due to the fact that perturbing the graph structure is a binary optimization problem. We then propose an online attack based on bandit optimization which is proven to be {sublinear} to the query number $T$, i.e., $\\mathcal{O}(\\sqrt{N}T^{3/4})$ where $N$ is the number of nodes in the graph. Finally, we evaluate our proposed attack by conducting experiments over multiple datasets and GNN models. The experimental results on various citation graphs and image graphs show that our attack is both effective and efficient. Source code is available at~\\url{https://github.com/Metaoblivion/Bandit_GNN_Attack}"}}
