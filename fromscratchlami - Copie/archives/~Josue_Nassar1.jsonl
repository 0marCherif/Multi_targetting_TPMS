{"id": "4zvqhhB_5o", "cdate": 1672531200000, "mdate": 1681650764428, "content": {"title": "Streaming Variational Monte Carlo", "abstract": ""}}
{"id": "xjb563TH-GH", "cdate": 1663850306029, "mdate": null, "content": {"title": "Representational Dissimilarity Metric Spaces for Stochastic Neural Networks", "abstract": "Quantifying similarity between neural representations---e.g. hidden layer activation vectors---is a perennial problem in deep learning and neuroscience research. Existing methods compare deterministic responses (e.g. artificial networks that lack stochastic layers) or averaged responses (e.g., trial-averaged firing rates in biological data). However, these measures of _deterministic_ representational similarity ignore the scale and geometric structure of noise, both of which play important roles in neural computation. To rectify this, we generalize previously proposed shape metrics (Williams et al. 2021) to quantify differences in _stochastic_ representations. These new distances satisfy the triangle inequality, and thus can be used as a rigorous basis for many supervised and unsupervised analyses. Leveraging this novel framework, we find that the stochastic geometries of neurobiological representations of oriented visual gratings and naturalistic scenes respectively resemble untrained and trained deep network representations. Further, we are able to more accurately predict certain network attributes (e.g. training hyperparameters) from its position in stochastic (versus deterministic) shape space."}}
{"id": "uRahS3xqG0p", "cdate": 1640995200000, "mdate": 1681756559720, "content": {"title": "BAM: Bayes with Adaptive Memory", "abstract": "Online learning via Bayes' theorem allows new data to be continuously integrated into an agent's current beliefs. However, a naive application of Bayesian methods in non-stationary environments leads to slow adaptation and results in state estimates that may converge confidently to the wrong parameter value. A common solution when learning in changing environments is to discard/downweight past data; however, this simple mechanism of \"forgetting\" fails to account for the fact that many real-world environments involve revisiting similar states. We propose a new framework, Bayes with Adaptive Memory (BAM), that takes advantage of past experience by allowing the agent to choose which past observations to remember and which to forget. We demonstrate that BAM generalizes many popular Bayesian update rules for non-stationary environments. Through a variety of experiments, we demonstrate the ability of BAM to continuously adapt in an ever-changing world."}}
{"id": "LFebEbA9Mt", "cdate": 1640995200000, "mdate": 1681946418517, "content": {"title": "Representational dissimilarity metric spaces for stochastic neural networks", "abstract": "Quantifying similarity between neural representations -- e.g. hidden layer activation vectors -- is a perennial problem in deep learning and neuroscience research. Existing methods compare deterministic responses (e.g. artificial networks that lack stochastic layers) or averaged responses (e.g., trial-averaged firing rates in biological data). However, these measures of _deterministic_ representational similarity ignore the scale and geometric structure of noise, both of which play important roles in neural computation. To rectify this, we generalize previously proposed shape metrics (Williams et al. 2021) to quantify differences in _stochastic_ representations. These new distances satisfy the triangle inequality, and thus can be used as a rigorous basis for many supervised and unsupervised analyses. Leveraging this novel framework, we find that the stochastic geometries of neurobiological representations of oriented visual gratings and naturalistic scenes respectively resemble untrained and trained deep network representations. Further, we are able to more accurately predict certain network attributes (e.g. training hyperparameters) from its position in stochastic (versus deterministic) shape space."}}
{"id": "NdOoQnYPj_", "cdate": 1632875561567, "mdate": null, "content": {"title": "BAM: Bayes with Adaptive Memory", "abstract": "Online learning via Bayes' theorem allows new data to be continuously integrated into an agent's current beliefs. However, a naive application of Bayesian methods in non-stationary environments leads to slow adaptation and results in state estimates that may converge confidently to the wrong parameter value. A common solution when learning in changing environments is to discard/downweight past data; however, this simple mechanism of \"forgetting\" fails to account for the fact that many real-world environments involve revisiting similar states. We propose a new framework, Bayes with Adaptive Memory (BAM), that takes advantage of past experience by allowing the agent to choose which past observations to remember and which to forget. We demonstrate that BAM generalizes many popular Bayesian update rules for non-stationary environments. Through a variety of experiments, we demonstrate the ability of BAM to continuously adapt in an ever-changing world."}}
{"id": "MFEmWr2Tk6", "cdate": 1577836800000, "mdate": 1681946418477, "content": {"title": "On 1/n neural representation and robustness", "abstract": "Understanding the nature of representation in neural networks is a goal shared by neuroscience and machine learning. It is therefore exciting that both fields converge not only on shared questions but also on similar approaches. A pressing question in these areas is understanding how the structure of the representation used by neural networks affects both their generalization, and robustness to perturbations. In this work, we investigate the latter by juxtaposing experimental results regarding the covariance spectrum of neural representations in the mouse V1 (Stringer et al) with artificial neural networks. We use adversarial robustness to probe Stringer et al\u2019s theory regarding the causal role of a 1/n covariance spectrum. We empirically investigate the benefits such a neural code confers in neural networks, and illuminate its role in multi-layer architectures. Our results show that imposing the experimentally observed structure on artificial neural networks makes them more robust to adversarial attacks. Moreover, our findings complement the existing theory relating wide neural networks to kernel methods, by showing the role of intermediate representations."}}
{"id": "UIeyVC_lP4P", "cdate": 1546300800000, "mdate": 1681946418478, "content": {"title": "Tree-Structured Recurrent Switching Linear Dynamical Systems for Multi-Scale Modeling", "abstract": "Many real-world systems studied are governed by complex, nonlinear dynamics. By modeling these dynamics, we can gain insight into how these systems work, make predictions about how they will behave, and develop strategies for controlling them. While there are many methods for modeling nonlinear dynamical systems, existing techniques face a trade off between offering interpretable descriptions and making accurate predictions. Here, we develop a class of models that aims to achieve both simultaneously, smoothly interpolating between simple descriptions and more complex, yet also more accurate models. Our probabilistic model achieves this multi-scale property through of a hierarchy of locally linear dynamics that jointly approximate global nonlinear dynamics. We call it the tree-structured recurrent switching linear dynamical system. To fit this model, we present a fully-Bayesian sampling procedure using Polya-Gamma data augmentation to allow for fast and conjugate Gibbs sampling. Through a variety of synthetic and real examples, we show how these models outperform existing methods in both interpretability and predictive capability."}}
{"id": "HkzRQhR9YX", "cdate": 1538087974177, "mdate": null, "content": {"title": "Tree-Structured Recurrent Switching Linear Dynamical Systems for Multi-Scale Modeling", "abstract": "Many real-world systems studied are governed by complex, nonlinear dynamics.  By modeling these dynamics, we can gain insight into how these systems work, make predictions about how they will behave, and develop strategies for controlling them. While there are many methods for modeling nonlinear dynamical systems, existing techniques face a trade off between offering interpretable descriptions and making accurate predictions.  Here, we develop a class of models that aims to achieve both simultaneously, smoothly interpolating between simple descriptions and more complex, yet also more accurate models.  Our probabilistic model achieves this multi-scale property through of a hierarchy of locally linear dynamics that jointly approximate global nonlinear dynamics. We call it the tree-structured recurrent switching linear dynamical system. To fit this model, we present a fully-Bayesian sampling procedure using Polya-Gamma data augmentation to allow for fast and conjugate Gibbs sampling.  Through a variety of synthetic and real examples, we show how these models outperform existing methods in both interpretability and predictive capability."}}
{"id": "mNoafKMN3O5", "cdate": 1514764800000, "mdate": null, "content": {"title": "Learning Structured Neural Dynamics From Single Trial Population Recording", "abstract": "To understand the complex nonlinear dynamics of neural circuits, we fit a structured state-space model called tree-structured recurrent switching linear dynamical system (TrSLDS) to noisy high-dimensional neural time series. TrSLDS is a multi-scale hierarchical generative model for the state-space dynamics where each node of the latent tree captures locally linear dynamics. TrSLDS can be learned efficiently and in a fully Bayesian manner using Gibbs sampling. We showcase TrSLDS' potential of inferring low-dimensional interpretable dynamical systems on a variety of examples."}}
{"id": "ZdNnTpD8AEq", "cdate": 1514764800000, "mdate": null, "content": {"title": "Improved Adaptive Importance Sampling Based on Variational Inference", "abstract": "In Monte Carlo-based Bayesian inference, it is important to generate samples from a target distribution, which are then used, e.g., to compute expectations with respect to the target distribution. Quite often, the target distribution is the posterior of parameters of interest, and drawing samples from it can be exceedingly difficult. Monte Carlo-based methods, like adaptive importance sampling (AIS), is built on the importance sampling principle to approximate a target distribution using a set of samples and their corresponding weights. Variational inference (VI) attempts to approximate the posterior by minimizing the Kullback-Leibler divergence (KLD) between the posterior and a set of simpler parametric distributions. While AIS often performs well, it struggles to approximate multimodal distributions and suffers when applied to high dimensional problems. By contrast, VI is fast and scales well with the dimension, but typically underestimates the variance of the target distribution. In this paper, we combine both methods to overcome their individual drawbacks and create an efficient and robust novel technique for drawing better samples from a target distribution. Our contribution is two-fold. First, we show how to do a smart initialization of AIS using VI. Second, we propose a method for adapting the parameters of the proposal distributions of the AIS, where the adaptation depends on the performance of the VI step. Computer simulations reveal that the new method improves the performance of the individual methods and shows promise to be applied to challenging scenarios."}}
