{"id": "EBC60mxBwyw", "cdate": 1663850385949, "mdate": null, "content": {"title": "How gradient estimator variance and bias impact learning in neural networks", "abstract": "There is growing interest in understanding how real brains may approximate gradients and how gradients can be used to train neuromorphic chips. However, neither real brains nor neuromorphic chips can perfectly follow the loss gradient, so parameter updates would necessarily use gradient estimators that have some variance and/or bias. Therefore, there is a need to understand better how variance and bias in gradient estimators impact learning dependent on network and task properties. Here, we show that variance and bias can impair learning on the training data, but some degree of variance and bias in a gradient estimator can be beneficial for generalization. We find that the ideal amount of variance and bias in a gradient estimator are dependent on several properties of the network and task: the size and activity sparsity of the network, the norm of the gradient, and the curvature of the loss landscape. As such, whether considering biologically-plausible learning algorithms or algorithms for training neuromorphic chips, researchers can analyze these properties to determine whether their approximation to gradient descent will be effective for learning given their network and task properties."}}
{"id": "ii9X4vtZGTZ", "cdate": 1652737597482, "mdate": null, "content": {"title": "$\\alpha$-ReQ : Assessing Representation Quality in Self-Supervised Learning by measuring eigenspectrum decay", "abstract": "Self-Supervised Learning (SSL) with large-scale unlabelled datasets enables learning useful representations for multiple downstream tasks. However, assessing the quality of such representations efficiently poses nontrivial challenges. Existing approaches train linear probes (with frozen features) to evaluate performance on a given task. This is expensive both computationally, since it requires retraining a new prediction head for each downstream task, and statistically, requires task-specific labels for multiple tasks. This poses a natural question, how do we efficiently determine the \"goodness\" of representations learned with SSL across a wide range of potential downstream tasks? In particular, a task-agnostic statistical measure of representation quality, that predicts generalization without explicit downstream task evaluation, would be highly desirable. \n   \nIn this work, we analyze characteristics of learned representations $\\mathbf{f_\\theta}$, in well-trained neural networks with canonical architectures \\& across SSL objectives. We observe that the eigenspectrum of the empirical feature covariance $\\mathrm{Cov}(\\mathbf{f_\\theta}$) can be well approximated with the family of power-law distribution. We analytically and empirically (using multiple datasets, e.g. CIFAR, STL10, MIT67, ImageNet) demonstrate that the decay coefficient $\\alpha$ serves as a measure of representation quality for tasks that are solvable with a linear readout, i.e. there exist well-defined intervals for $\\alpha$ where models exhibit excellent downstream generalization. Furthermore, our experiments suggest that key design parameters in SSL algorithms, such as BarlowTwins, implicitly modulate the decay coefficient of the eigenspectrum ($\\alpha$). As $\\alpha$ depends only on the features themselves, this measure for model selection with hyperparameter tuning for BarlowTwins enables search with less compute."}}
{"id": "_4xg5moXVg", "cdate": 1652737398078, "mdate": null, "content": {"title": "Beyond accuracy: generalization properties of bio-plausible temporal credit assignment rules", "abstract": "To unveil how the brain learns, ongoing work seeks  biologically-plausible approximations of gradient descent algorithms for training recurrent neural networks (RNNs). Yet, beyond task accuracy, it is unclear if such learning rules converge to solutions that exhibit different levels of generalization than their non-biologically-plausible counterparts. Leveraging results from deep learning theory based on loss landscape curvature, we ask: how do biologically-plausible gradient approximations affect generalization? We first demonstrate that state-of-the-art biologically-plausible learning rules for training RNNs exhibit worse and more variable generalization performance compared to their machine learning counterparts that follow the true gradient more closely. Next, we verify that such generalization performance is correlated significantly with loss landscape curvature, and we show that biologically-plausible learning rules tend to approach high-curvature regions in synaptic weight space. Using tools from dynamical systems, we derive theoretical arguments and present a theorem explaining this phenomenon. This predicts our numerical results, and explains why biologically-plausible rules lead to worse and more variable generalization properties. Finally, we suggest potential remedies that could be used by the brain to mitigate this effect. To our knowledge, our analysis is the first to identify the reason for this generalization gap between artificial and biologically-plausible learning rules, which can help guide future investigations into how the brain learns solutions that generalize."}}
{"id": "lQH7xXRTc98", "cdate": 1640995200000, "mdate": 1652657384557, "content": {"title": "Investigating Power laws in Deep Representation Learning", "abstract": "Representation learning that leverages large-scale labelled datasets, is central to recent progress in machine learning. Access to task relevant labels at scale is often scarce or expensive, motivating the need to learn from unlabelled datasets with self-supervised learning (SSL). Such large unlabelled datasets (with data augmentations) often provide a good coverage of the underlying input distribution. However evaluating the representations learned by SSL algorithms still requires task-specific labelled samples in the training pipeline. Additionally, the generalization of task-specific encoding is often sensitive to potential distribution shift. Inspired by recent advances in theoretical machine learning and vision neuroscience, we observe that the eigenspectrum of the empirical feature covariance matrix often follows a power law. For visual representations, we estimate the coefficient of the power law, $\\alpha$, across three key attributes which influence representation learning: learning objective (supervised, SimCLR, Barlow Twins and BYOL), network architecture (VGG, ResNet and Vision Transformer), and tasks (object and scene recognition). We observe that under mild conditions, proximity of $\\alpha$ to 1, is strongly correlated to the downstream generalization performance. Furthermore, $\\alpha \\approx 1$ is a strong indicator of robustness to label noise during fine-tuning. Notably, $\\alpha$ is computable from the representations without knowledge of any labels, thereby offering a framework to evaluate the quality of representations in unlabelled datasets."}}
{"id": "XluIir28a2Q", "cdate": 1640995200000, "mdate": 1681494076282, "content": {"title": "Beyond accuracy: generalization properties of bio-plausible temporal credit assignment rules", "abstract": ""}}
{"id": "QH_2kU8yx6", "cdate": 1609459200000, "mdate": 1667377086017, "content": {"title": "CCN GAC Workshop: Issues with learning in biological recurrent neural networks", "abstract": "We provide a brief review of the common assumptions about biological learning with findings from experimental neuroscience and contrast them with the efficiency of gradient-based learning in recurrent neural networks. The key issues discussed in this review include: synaptic plasticity, neural circuits, theory-experiment divide, and objective functions. We conclude with recommendations for both theoretical and experimental neuroscientists when designing new studies that could help bring clarity to these issues."}}
{"id": "BGQLbUej49H", "cdate": 1609459200000, "mdate": 1682363786684, "content": {"title": "Estimating brain age from structural MRI and MEG data: Insights from dimensionality reduction techniques", "abstract": ""}}
{"id": "lc_LE6gX2NQ", "cdate": 1596479211129, "mdate": null, "content": {"title": "Dimensionality and flexibility of learning in biological recurrent neural networks", "abstract": "Title: Dimensionality and flexibility of learning in biological recurrent neural networks\nScientific Question: Does full-rank gradient descent accurately describe the dynamics of\nsynaptic plasticity in biological recurrent neural networks?"}}
{"id": "BkNeAkZOWS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Training Autoencoders in Sparse Domain", "abstract": "Autoencoders (AE) are essential in learning representation of large data (like images) for dimensionality reduction. Images are converted to sparse domain using transforms like Fast Fourier Transform (FFT) or Discrete Cosine Transform (DCT) where information that requires encoding is minimal. By optimally selecting the feature-rich frequencies, we are able to learn the latent vectors more robustly. We successfully show enhanced performance of autoencoders in sparse domain for images."}}
{"id": "B1bgW0gOZB", "cdate": 1514764800000, "mdate": null, "content": {"title": "AdGAP: Advanced Global Average Pooling", "abstract": "Global average pooling (GAP) has been used previously to generate class activation maps. The motivation behind AdGAP comes from the fact that the convolutional filters possess position information of the essential features and hence, combination of the feature maps could help us locate the class instances in an image. Our novel architecture generates promising results and unlike previous methods, the architecture is not sensitive to the size of the input image, thus promising wider application."}}
