{"id": "UVu1j3kDgV", "cdate": 1675364333226, "mdate": 1675364333226, "content": {"title": "On the evaluation metrics of paraphrase generation", "abstract": "In  this  paper  we  revisit  automatic  metrics for   paraphrase   evaluation   and   obtain   two findings  that  disobey  conventional  wisdom: (1) Reference-free metrics achieve better per- formance  than  their  reference-based  counter- parts.   (2)  Most  commonly  used  metrics  do not  align  well  with  human  annotation.    Un- derlying  reasons  behind  the  above  findings are  explored  through  additional  experiments and  in-depth  analyses.   Based  on  the  exper- iments  and  analyses,  we  propose  ParaScore, a  new  evaluation  metric  for  paraphrase  gen- eration.   It  possesses  the  merits  of  reference- based and reference-free metrics and explicitly models  lexical  divergence.   Experimental  re- sults demonstrate that ParaScore significantly outperforms existing metrics. "}}
{"id": "zDTNMLvgv9h", "cdate": 1672531200000, "mdate": 1681650527560, "content": {"title": "Federated Nearest Neighbor Machine Translation", "abstract": ""}}
{"id": "wYDCcYgbr2B", "cdate": 1672531200000, "mdate": 1696516993627, "content": {"title": "Rethinking Translation Memory Augmented Neural Machine Translation", "abstract": ""}}
{"id": "JqaohEe3e22", "cdate": 1672531200000, "mdate": 1696516993630, "content": {"title": "Rethinking Translation Memory Augmented Neural Machine Translation", "abstract": "This paper rethinks translation memory augmented neural machine translation (TM-augmented NMT) from two perspectives, i.e., a probabilistic view of retrieval and the variance-bias decomposition principle. The finding demonstrates that TM-augmented NMT is good at the ability of fitting data (i.e., lower bias) but is more sensitive to the fluctuations in the training data (i.e., higher variance), which provides an explanation to a recently reported contradictory phenomenon on the same translation task: TM-augmented NMT substantially advances vanilla NMT under the high-resource scenario whereas it fails under the low-resource scenario. Then we propose a simple yet effective TM-augmented NMT model to promote the variance and address the contradictory phenomenon. Extensive experiments show that the proposed TM-augmented NMT achieves consistent gains over both conventional NMT and existing TM-augmented NMT under two variance-preferable (low-resource and plug-and-play) scenarios as well as the high-resource scenario."}}
{"id": "DSBC4P5x6W", "cdate": 1672531200000, "mdate": 1696516993634, "content": {"title": "Nearest Neighbor Machine Translation is Meta-Optimizer on Output Projection Layer", "abstract": "Nearest Neighbor Machine Translation ($k$NN-MT) has achieved great success on domain adaptation tasks by integrating pre-trained Neural Machine Translation (NMT) models with domain-specific token-level retrieval. However, the reasons underlying its success have not been thoroughly investigated. In this paper, we provide a comprehensive analysis of $k$NN-MT through theoretical and empirical studies. Initially, we offer a theoretical interpretation of the working mechanism of $k$NN-MT as an efficient technique to implicitly execute gradient descent on the output projection layer of NMT, indicating that it is a specific case of model fine-tuning. Subsequently, we conduct multi-domain experiments and word-level analysis to examine the differences in performance between $k$NN-MT and entire-model fine-tuning. Our findings suggest that: (1) Incorporating $k$NN-MT with adapters yields comparable translation performance to fine-tuning on in-domain test sets, while achieving better performance on out-of-domain test sets; (2) Fine-tuning significantly outperforms $k$NN-MT on the recall of low-frequency domain-specific words, but this gap could be bridged by optimizing the context representations with additional adapter layers."}}
{"id": "9M24mzhufU", "cdate": 1672531200000, "mdate": 1696730334983, "content": {"title": "Federated Nearest Neighbor Machine Translation", "abstract": ""}}
{"id": "R1U5G2spbLd", "cdate": 1663850203072, "mdate": null, "content": {"title": "Federated Nearest Neighbor Machine Translation", "abstract": "To protect user privacy and meet legal regulations, federated learning (FL) is attracting significant attention. Training neural machine translation (NMT) models with traditional FL algorithm (e.g., FedAvg) typically relies on multi-round model-based interactions. However, it is impractical and inefficient for machine translation tasks due to the vast communication overheads and heavy synchronization. In this paper, we propose a novel federated nearest neighbor (FedNN) machine translation framework that, instead of multi-round model-based interactions, leverages one-round memorization-based interaction to share knowledge across different clients to build low-overhead privacy-preserving systems. The whole approach equips the public NMT model trained on large-scale accessible data with a $k$-nearest-neighbor ($k$NN) classifier and integrates the external datastore constructed by private text data in all clients to form the final FL model.  A two-phase datastore encryption strategy is introduced to achieve privacy-preserving during this process.  Extensive experiments show that FedNN significantly reduces computational and communication costs compared with FedAvg, while maintaining promising performance in different FL settings."}}
{"id": "qc511KgHOWT", "cdate": 1640995200000, "mdate": 1674697766383, "content": {"title": "Investigating Data Variance in Evaluations of Automatic Machine Translation Metrics", "abstract": ""}}
{"id": "cXGkcr5Mii", "cdate": 1640995200000, "mdate": 1696730334983, "content": {"title": "On the Evaluation Metrics for Paraphrase Generation", "abstract": ""}}
{"id": "ajavGkZQZP", "cdate": 1640995200000, "mdate": 1682383239316, "content": {"title": "Neural Machine Translation with Contrastive Translation Memories", "abstract": ""}}
