{"id": "mhYpv7YMz7n", "cdate": 1679903671994, "mdate": null, "content": {"title": "eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers", "abstract": "Large-scale diffusion-based generative models have led to breakthroughs in text-conditioned high-resolution image synthesis. Starting from random noise, such text-to-image diffusion models gradually synthesize images in an iterative fashion while conditioning on text prompts. We find that their synthesis behavior qualitatively changes throughout this process: Early in sampling, generation strongly relies on the text prompt to generate text-aligned content, while later, the text conditioning is almost entirely ignored. This suggests that sharing model parameters throughout the entire generation process may not be ideal. Therefore, in contrast to existing works, we propose to train an ensemble of text-to-image diffusion models specialized for different synthesis stages. To maintain training efficiency, we initially train a single model, which is then split into specialized models that are trained for the specific stages of the iterative generation process. Our ensemble of diffusion models, called eDiff-I, results in improved text alignment while maintaining the same inference computation cost and preserving high visual quality, outperforming previous large-scale text-to-image diffusion models on the standard benchmark. In addition, we train our model to exploit a variety of embeddings for conditioning, including the T5 text, CLIP text, and CLIP image embeddings. We show that these different embeddings lead to different behaviors. Notably, the CLIP image embedding allows an intuitive way of transferring the style of a reference image to the target text-to-image output. Lastly, we show a technique that enables eDiff-I's \"paint-with-words\" capability. A user can select the word in the input text and paint it in a canvas to control the output, which is very handy for crafting the desired image in mind. The project page is available at https://deepimagination.cc/eDiff-I/"}}
{"id": "4oXTQ6m_ws8", "cdate": 1663849914270, "mdate": null, "content": {"title": "The Role of ImageNet Classes in Fr\u00e9chet Inception Distance", "abstract": "Fr\u00e9chet Inception Distance (FID) is the primary metric for ranking models in data-driven generative modeling. While remarkably successful, the metric is known to sometimes disagree with human judgement. We investigate a root cause of these discrepancies, and visualize what FID \"looks at\" in generated images. We show that the feature space that FID is (typically) computed in is so close to the ImageNet classifications that aligning the histograms of Top-$N$ classifications between sets of generated and real images can reduce FID substantially \u2014 without actually improving the quality of results. Thus, we conclude that FID is prone to intentional or accidental distortions. As a practical example of an accidental distortion, we discuss a case where an ImageNet pre-trained FastGAN achieves a FID comparable to StyleGAN2, while being worse in terms of human evaluation."}}
{"id": "k7FuTOWMOc7", "cdate": 1652737376327, "mdate": null, "content": {"title": "Elucidating the Design Space of Diffusion-Based Generative Models", "abstract": "We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices. This lets us identify several changes to both the sampling and training processes, as well as preconditioning of the score networks. Together, our improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a class-conditional setting and 1.97 in an unconditional setting, with much faster sampling (35 network evaluations per image) than prior designs. To further demonstrate their modular nature, we show that our design changes dramatically improve both the efficiency and quality obtainable with pre-trained score networks from previous work, including improving the FID of a previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after re-training with our proposed improvements to a new SOTA of 1.36."}}
{"id": "VnAwNNJiwDb", "cdate": 1652737355084, "mdate": null, "content": {"title": "Generating Long Videos of Dynamic Scenes", "abstract": "We present a video generation model that accurately reproduces object motion, changes in camera viewpoint, and new content that arises over time. Existing video generation methods often fail to produce new content as a function of time while maintaining consistencies expected in real environments, such as plausible dynamics and object persistence. A common failure case is for content to never change due to over-reliance on inductive bias to provide temporal consistency, such as a single latent code that dictates content for the entire video. On the other extreme, without long-term consistency, generated videos may morph unrealistically between different scenes. To address these limitations, we prioritize the time axis by redesigning the temporal latent representation and learning long-term consistency from data by training on longer videos. We leverage a two-phase training strategy, where we separately train using longer videos at a low resolution and shorter videos at a high resolution. To evaluate the capabilities of our model, we introduce two new benchmark datasets with explicit focus on long-term temporal dynamics."}}
{"id": "Owggnutk6lE", "cdate": 1621630154456, "mdate": null, "content": {"title": "Alias-Free Generative Adversarial Networks", "abstract": "We observe that despite their hierarchical convolutional nature, the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner. This manifests itself as, e.g., detail appearing to be glued to image coordinates instead of the surfaces of depicted objects. We trace the root cause to careless signal processing that causes aliasing in the generator network. Interpreting all signals in the network as continuous, we derive generally applicable, small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process. The resulting networks match the FID of StyleGAN2 but differ dramatically in their internal representations, and they are fully equivariant to translation and rotation even at subpixel scales. Our results pave the way for generative models better suited for video and animation."}}
{"id": "24_bs9h9c", "cdate": 1581700340290, "mdate": null, "content": {"title": "Generating Training Data for Denoising Real RGB Images via Camera Pipeline Simulation", "abstract": "Image reconstruction techniques such as denoising often\nneed to be applied to the RGB output of cameras and cellphones. Unfortunately, the commonly used additive white\nnoise (AWGN) models do not accurately reproduce the noise\nand the degradation encountered on these inputs. This is\nparticularly important for learning-based techniques, because the mismatch between training and real world data\nwill hurt their generalization. This paper aims to accurately simulate the degradation and noise transformation\nperformed by camera pipelines. This allows us to generate realistic degradation in RGB images that can be used\nto train machine learning models. We use our simulation to\nstudy the importance of noise modeling for learning-based\ndenoising. Our study shows that a realistic noise model\nis required for learning to denoise real JPEG images. A\nneural network trained on realistic noise outperforms the\none trained with AWGN by 3 dB. An ablation study of our\npipeline shows that simulating denoising and demosaicking\nis important to this improvement and that realistic demosaicking algorithms, which have been rarely considered, is\nneeded. We believe this simulation will also be useful for\nother image reconstruction tasks, and we will distribute our\ncode publicly."}}
{"id": "ryldhrrxUH", "cdate": 1567802800348, "mdate": null, "content": {"title": "Computational Mirrors: Blind Inverse Light Transport by Deep Matrix Factorization", "abstract": "We recover a video of the motion taking place in a hidden scene by observing changes in indirect illumination in a nearby uncalibrated visible region. We solve this problem by factoring the observed video into a matrix product between the unknown hidden scene video and an unknown light transport matrix. This task is extremely ill-posed, as any non-negative factorization will satisfy the data. Inspired by recent work on the Deep Image Prior, we parameterize the factor matrices using randomly initialized convolutional neural networks trained in a one-off manner, and show that this results in decompositions that reflect the true motion in the hidden scene."}}
