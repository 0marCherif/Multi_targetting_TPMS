{"id": "DkBVDOYXoyC", "cdate": 1683991960312, "mdate": 1683991960312, "content": {"title": "Recovering AES Keys with a Deep Cold Boot Attack", "abstract": "Cold boot attacks inspect the corrupted random access memory soon after the power has been shut down. While most of the bits have been corrupted, many bits, at random locations, have not. Since the keys in many encryption schemes are being expanded in memory into longer keys with fixed redundancies, the keys can often be restored. In this work, we combine a novel cryptographic variant of a deep error correcting code technique with a modified SAT solver scheme to apply the attack on AES keys. Even though AES consists of Rijndael S-box elements, that are specifically designed to be resistant to linear and differential cryptanalysis, our method provides a novel formalization of the AES key scheduling as a computational graph, which is implemented by a neural message passing network. Our results show that our methods outperform the state of the art attack methods by a very large margin."}}
{"id": "wwyRg_T8a7", "cdate": 1672531200000, "mdate": 1681664478640, "content": {"title": "Separate And Diffuse: Using a Pretrained Diffusion Model for Improving Source Separation", "abstract": "The problem of speech separation, also known as the cocktail party problem, refers to the task of isolating a single speech signal from a mixture of speech signals. Previous work on source separation derived an upper bound for the source separation task in the domain of human speech. This bound is derived for deterministic models. Recent advancements in generative models challenge this bound. We show how the upper bound can be generalized to the case of random generative models. Applying a diffusion model Vocoder that was pretrained to model single-speaker voices on the output of a deterministic separation model leads to state-of-the-art separation results. It is shown that this requires one to combine the output of the separation model with that of the diffusion model. In our method, a linear combination is performed, in the frequency domain, using weights that are inferred by a learned model. We show state-of-the-art results on 2, 3, 5, 10, and 20 speakers on multiple benchmarks. In particular, for two speakers, our method is able to surpass what was previously considered the upper performance bound."}}
{"id": "kqHkCVS7wbj", "cdate": 1663850557002, "mdate": null, "content": {"title": "Decision S4: Efficient Sequence-Based RL via State Spaces Layers", "abstract": "Recently, sequence learning methods have been applied to the problem of off-policy\nReinforcement Learning, including the seminal work on Decision Transformers,\nwhich employs transformers for this task. Since transformers are parameter-heavy,\ncannot benefit from history longer than a fixed window size, and are not computed\nusing recurrence, we set out to investigate the suitability of the S4 family of\nmodels, which are based on state-space layers and have been shown to outperform\ntransformers, especially in modeling long-range dependencies. In this work, we\npresent two main algorithms: (i) an off-policy training procedure that works with\ntrajectories, while still maintaining the training efficiency of the S4 model. (ii) An\non-policy training procedure that is trained in a recurrent manner, benefits from\nlong-range dependencies, and is based on a novel stable actor-critic mechanism.\nOur results indicate that our method outperforms multiple variants of decision\ntransformers, as well as the other baseline methods on most tasks, while reducing\nthe latency, number of parameters, and training time by several orders of magnitude,\nmaking our approach more suitable for real-world RL"}}
{"id": "x5mtJD2ovc", "cdate": 1663849825962, "mdate": null, "content": {"title": "kNN-Diffusion: Image Generation via Large-Scale Retrieval", "abstract": "Recent text-to-image models have achieved impressive results. However, since they require large-scale datasets of text-image pairs, it is impractical to train them on new domains where data is scarce or not labeled.\nIn this work, we propose using large-scale retrieval methods, in particular, efficient k-Nearest-Neighbors (kNN), which offers novel capabilities: (1) training a substantially small and efficient text-to-image diffusion model using only pre-trained multi-modal embeddings, but without an explicit text-image dataset, (2) generating out-of-distribution images by simply swapping the retrieval database at inference time, and (3) performing text-driven local semantic manipulations while preserving object identity. To demonstrate the robustness of our method, we apply our kNN approach on two state-of-the-art diffusion backbones, and show results on several different datasets. As evaluated by human studies and automatic metrics, our method achieves state-of-the-art results compared to existing approaches that train text-to-image generation models using images-only dataset."}}
{"id": "yz-BFLEDgb", "cdate": 1640995200000, "mdate": 1681664478327, "content": {"title": "Zero-Shot Voice Conditioning for Denoising Diffusion TTS Models", "abstract": "We present a novel way of conditioning a pretrained denoising diffusion speech model to produce speech in the voice of a novel person unseen during training. The method requires a short (~3 seconds) sample from the target person, and generation is steered at inference time, without any training steps. At the heart of the method lies a sampling process that combines the estimation of the denoising model with a low-pass version of the new speaker's sample. The objective and subjective evaluations show that our sampling method can generate a voice similar to that of the target speaker in terms of frequency, with an accuracy comparable to state-of-the-art methods, and without training."}}
{"id": "oJHoqmH12c", "cdate": 1640995200000, "mdate": 1681664478093, "content": {"title": "SepIt: Approaching a Single Channel Speech Separation Bound", "abstract": "We present an upper bound for the Single Channel Speech Separation task, which is based on an assumption regarding the nature of short segments of speech. Using the bound, we are able to show that while the recent methods have made great progress for a few speakers, there is room for improvement for five and ten speakers. We then introduce a Deep neural network, SepIt, that iteratively improves the different speakers' estimation. At test time, SpeIt has a varying number of iterations per test sample, based on a mutual information criterion that arises from our analysis. In an extensive set of experiments, SepIt outperforms the state of the art neural networks for 2, 3, 5, and 10 speakers."}}
{"id": "iJhW0VMQkj", "cdate": 1640995200000, "mdate": 1681664478566, "content": {"title": "Neural Decoding with Optimization of Node Activations", "abstract": "The problem of maximum likelihood decoding with a neural decoder for error-correcting code is considered. It is shown that the neural decoder can be improved with two novel loss terms on the node's activations. The first loss term imposes a sparse constraint on the node's activations. Whereas, the second loss term tried to mimic the node's activations from a teacher decoder which has better performance. The proposed method has the same run time complexity and model size as the neural Belief Propagation decoder, while improving the decoding performance by up to $1.1dB$ on BCH codes."}}
{"id": "hGBgyRKDyX", "cdate": 1640995200000, "mdate": 1663769513147, "content": {"title": "SepIt: Approaching a Single Channel Speech Separation Bound", "abstract": "We present an upper bound for the Single Channel Speech Separation task, which is based on an assumption regarding the nature of short segments of speech. Using the bound, we are able to show that while the recent methods have made significant progress for a few speakers, there is room for improvement for five and ten speakers. We then introduce a Deep neural network, SepIt, that iteratively improves the different speakers' estimation. At test time, SpeIt has a varying number of iterations per test sample, based on a mutual information criterion that arises from our analysis. In an extensive set of experiments, SepIt outperforms the state-of-the-art neural networks for 2, 3, 5, and 10 speakers."}}
{"id": "cV4jxh7UNXC", "cdate": 1640995200000, "mdate": 1663769513222, "content": {"title": "Zero-Shot Voice Conditioning for Denoising Diffusion TTS Models", "abstract": "We present a novel way of conditioning a pretrained denoising diffusion speech model to produce speech in the voice of a novel person unseen during training. The method requires a short (~3 seconds) sample from the target person, and generation is steered at inference time, without any training steps. At the heart of the method lies a sampling process that combines the estimation of the denoising model with a low-pass version of the new speaker's sample. The objective and subjective evaluations show that our sampling method can generate a voice similar to that of the target speaker in terms of frequency, with an accuracy comparable to state-of-the-art methods, and without training."}}
{"id": "IA5PP7eWXtF", "cdate": 1640995200000, "mdate": 1663769513020, "content": {"title": "A-Muze-Net: Music Generation by Composing the Harmony Based on the Generated Melody", "abstract": "We present a method for the generation of Midi files of piano music. The method models the right and left hands using two networks, where the left hand is conditioned on the right hand. This way, the melody is generated before the harmony. The Midi is represented in a way that is invariant to the musical scale, and the melody is represented, for the purpose of conditioning the harmony, by the content of each bar, viewed as a chord. Finally, notes are added randomly, based on this chord representation, in order to enrich the generated audio. Our experiments show a significant improvement over the state of the art for training on such datasets, and demonstrate the contribution of each of the novel components."}}
