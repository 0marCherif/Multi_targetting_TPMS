{"id": "lBH8t__oWTM", "cdate": 1667373460920, "mdate": 1667373460920, "content": {"title": " Robust Tracking via Uncertainty-aware Semantic Consistency", "abstract": "Robust tracking has a variety of practical applications. Despite many years of progress, it is still a difficult problem due to enormous uncertainties in real-world scenes. To address this issue, we propose a robust anchor-free based tracking model with uncertainty estimation. Within the model, a new datadriven uncertainty estimation strategy is proposed to generate uncertainty-aware features with promising discriminative and descriptive power. Then, a simple yet effective pyramid-wise cross correlation operation is constructed to extract multi-scale semantic features that provide rich correlation information for uncertainty-aware estimation and thus enhances the tracking robustness. Finally, a semantic consistency checking branch is designed to further estimate uncertainty of output results from the classification and regression branches by adaptively generating semantically consistent labels. Experiments on six benchmarks (i.e., OTB100, VOT2018, VOT2020, TrackingNet, GOT-10K and LaSOT) show the competing performance of our tracker with 130 FPS."}}
{"id": "Vh4lzfhKzI", "cdate": 1609459200000, "mdate": null, "content": {"title": "SecureFace: Face Template Protection", "abstract": "It has been shown that face images can be reconstructed from their representations (templates). We propose a randomized CNN to generate protected face biometric templates given the input face image and a user-specific key. The use of user-specific keys introduces randomness to the secure template and hence strengthens the template security. To further enhance the security of the templates, instead of storing the key, we store a secure sketch that can be decoded to generate the key with genuine queries submitted to the system. We have evaluated the proposed protected template generation method using three benchmarking datasets for the face (FRGC v2.0, CFP, and IJB-A). The experimental results justify that the protected template generated by the proposed method are non-invertible and cancellable, while preserving the verification performance."}}
{"id": "y4p8LCKYWI", "cdate": 1580733714178, "mdate": null, "content": {"title": "Robust Multi-modality Anchor Graph-based Label Prediction for RGB-Infrared Tracking", "abstract": "Given massive video data generated from different applications such as security monitoring and traffic management, to save cost and human labour, developing an industrial intelligent video analytic system, which can automatically extract and analyze the meaningful content of videos, is essential. For achieving the objective of motion perception in video analytic system, a key problem is how to perform effective tracking of object of interest so that the location and the status of the tracked object can be inferred accurately. To solve this problem, with the popularity of RGB-infrared dual camera systems, this paper proposes a new RGB-infrared tracking framework which aims to exploit information from both RGB and infrared modalities to enhance the tracking robustness. In particular, within the tracking framework, a robust multi-modality anchor graph-based label prediction model is developed, which is able to 1) construct a scalable graph representation of the relationship of the samples based on local anchor approximation; 2) defuse a limited amount of known labels to large amount of unlabeled sample efficiently based on transductive learning strategy; and 3) adaptive incorporate importance weights for measuring modality discriminability. Efficient optimization algorithms are derived to solve the prediction model. Experimental results on various multi- modality videos demonstrate the effectiveness of the proposed method."}}
{"id": "xnidMuZpoEv", "cdate": 1577836800000, "mdate": null, "content": {"title": "Deep Refinement: capsule network with attention mechanism-based system for text classification", "abstract": "Most of the text in the questions of community question\u2013answering systems does not consist of a definite mechanism for the restriction of inappropriate and insincere content. A given piece of text can be insincere if it asserts false claims or assumes something which is debatable or has a non-neutral or exaggerated tone about an individual or a group. In this paper, we propose a pipeline called Deep Refinement which utilizes some of the state-of-the-art methods for information retrieval from highly sparse data such as capsule network and attention mechanism. We have applied the Deep Refinement pipeline to classify the text primarily into two categories, namely sincere and insincere. Our novel approach \u2018Deep Refinement\u2019 provides a system for the classification of such questions in order to ensure enhanced monitoring and information quality. The database used to understand the real concept of what actually makes up sincere and insincere includes quora insincere question dataset. Our proposed question classification method outperformed previously used text classification methods, as evident from the F1 score of 0.978."}}
{"id": "thPNo3jYCIR", "cdate": 1577836800000, "mdate": null, "content": {"title": "Cross-Modality Person Re-Identification via Modality-Aware Collaborative Ensemble Learning", "abstract": "Visible thermal person re-identification (VT-ReID) is a challenging cross-modality pedestrian retrieval problem due to the large intra-class variations and modality discrepancy across different cameras. Existing VT-ReID methods mainly focus on learning cross-modality sharable feature representations by handling the modality-discrepancy in feature level. However, the modality difference in classifier level has received much less attention, resulting in limited discriminability. In this paper, we propose a novel modality-aware collaborative ensemble (MACE) learning method with middle-level sharable two-stream network (MSTN) for VT-ReID, which handles the modality-discrepancy in both feature level and classifier level. In feature level, MSTN achieves much better performance than existing methods by capturing sharable discriminative middle-level features in convolutional layers. In classifier level, we introduce both modality-specific and modality-sharable identity classifiers for two modalities to handle the modality discrepancy. To utilize the complementary information among different classifiers, we propose an ensemble learning scheme to incorporate the modality sharable classifier and the modality specific classifiers. In addition, we introduce a collaborative learning strategy, which regularizes modality-specific identity predictions and the ensemble outputs. Extensive experiments on two cross-modality datasets demonstrate that the proposed method outperforms current state-of-the-art by a large margin, achieving rank-1/mAP accuracy 51.64%/50.11% on the SYSU-MM01 dataset, and 72.37%/69.09% on the RegDB dataset."}}
{"id": "lfKXPzOetek", "cdate": 1577836800000, "mdate": null, "content": {"title": "Bi-Directional Center-Constrained Top-Ranking for Visible Thermal Person Re-Identification", "abstract": "Visible thermal person re-identification (VT-REID) is a task of matching person images captured by thermal and visible cameras, which is an extremely important issue in night-time surveillance applications. Existing cross-modality recognition works mainly focus on learning sharable feature representations to handle the cross-modality discrepancies. However, apart from the cross-modality discrepancy caused by different camera spectrums, VT-REID also suffers from large cross-modality and intra-modality variations caused by different camera environments and human poses, and so on. In this paper, we propose a dual-path network with a novel bi-directional dual-constrained top-ranking (BDTR) loss to learn discriminative feature representations. It is featured in two aspects: 1) end-to-end learning without extra metric learning step and 2) the dual-constraint simultaneously handles the cross-modality and intra-modality variations to ensure the feature discriminability. Meanwhile, a bi-directional center-constrained top-ranking (eBDTR) is proposed to incorporate the previous two constraints into a single formula, which preserves the properties to handle both cross-modality and intra-modality variations. The extensive experiments on two cross-modality re-ID datasets demonstrate the superiority of the proposed method compared to the state-of-the-arts."}}
{"id": "koGnlKYPz7D", "cdate": 1577836800000, "mdate": null, "content": {"title": "Improving Night-Time Pedestrian Retrieval With Distribution Alignment and Contextual Distance", "abstract": "Night-time pedestrian retrieval is a cross-modality retrieval task of retrieving person images between day-time visible images and night-time thermal images. It is a very challenging problem due to modality difference, camera variations, and person variations, but it plays an important role in night-time video surveillance. The existing cross-modality retrieval usually focuses on learning modality sharable feature representations to bridge the modality gap. In this article, we propose to utilize auxiliary information to improve the retrieval performance, which consistently improves the performance with different baseline loss functions. Our auxiliary information contains two major parts: cross-modality feature distribution and contextual information. The former aligns the cross-modality feature distributions between two modalities to improve the performance, and the latter optimizes the cross-modality distance measurement with the contextual information. We also demonstrate that abundant annotated visible pedestrian images, which are easily accessible, help to improve the cross-modality pedestrian retrieval as well. The proposed method is featured in two aspects: the auxiliary information does not need additional human intervention or annotation; it learns discriminative feature representations in an end-to-end deep learning manner. Extensive experiments on two cross-modality pedestrian retrieval datasets demonstrate the superiority of the proposed method, achieving much better performance than the state-of-the-arts."}}
{"id": "hhjoOL6s3O", "cdate": 1577836800000, "mdate": null, "content": {"title": "Facial Expression Recognition Using Spatial-Temporal Semantic Graph Network", "abstract": "Motions of facial components convey significant information of facial expressions. Although remarkable advancement has been made, the dynamic of facial topology has not been fully exploited. In this paper, a novel facial expression recognition (FER) algorithm called Spatial Temporal Semantic Graph Network (STSGN) is proposed to automatically learn spatial and temporal patterns through end-to-end feature learning from facial topology structure. The proposed algorithm not only has greater discriminative power to capture the dynamic patterns of facial expression and stronger generalization capability to handle different variations but also higher interpretability. Experimental evaluation on two popular datasets, CK+ and Oulu-CASIA, shows that our algorithm achieves more competitive results than other state-of-the-art methods."}}
{"id": "fO3hnwIuci", "cdate": 1577836800000, "mdate": null, "content": {"title": "Fine-Grained Spatial Alignment Model for Person Re-Identification With Focal Triplet Loss", "abstract": "Recent advances of person re-identification have well advocated the usage of human body cues to boost performance. However, most existing methods still retain on exploiting a relatively coarse-grained local information. Such information may include redundant backgrounds that are sensitive to the apparently similar persons when facing challenging scenarios like complex poses, inaccurate detection, occlusion and misalignment. In this paper we propose a novel Fine-Grained Spatial Alignment Model (FGSAM) to mine fine-grained local information to handle the aforementioned challenge effectively. In particular, we first design a pose resolve net with channel parse blocks (CPB) to extract pose information in pixel-level. This network allows the proposed model to be robust to complex pose variations while suppressing the redundant backgrounds caused by inaccurate detection and occlusion. Given the extracted pose information, a locally reinforced alignment mode is further proposed to address the misalignment problem between different local parts by considering different local parts along with attribute information in a fine-grained way. Finally, a focal triplet loss is designed to effectively train the entire model, which imposes a constraint on the intra-class and an adaptively weight adjustment mechanism to handle the hard sample problem. Extensive evaluations and analysis on Market1501, DukeMTMC-reid and PETA datasets demonstrate the effectiveness of FGSAM in coping with the problems of misalignment, occlusion and complex poses."}}
{"id": "P6o2bPb7etn", "cdate": 1577836800000, "mdate": null, "content": {"title": "Fusion of iris and sclera using phase intensive rubbersheet mutual exclusion for periocular recognition", "abstract": "In biometrics, periocular recognition analysis is an essential constituent for identifying the human being. Among prevailing the modalities, ocular biometric traits such as iris, sclera and periocular eye movement have experienced noteworthy consciousness in the recent past. In this paper, we are presenting new multi-biometric fusion method called Phase Intensive Mutual Exclusive Distribution (PI-MED) method by combining periocular features (i.e. iris and sclera) for identity verification. The main objective of the proposed PI-MED method is to reduce the matching fusion time and overhead during human recognition in biometrics. Initially, iris modality and sclera modality is pre-processed using Phase Intensive Rubber Sheeting Local Pattern Extraction to generate the vector of score. After that, the extracted iris and sclera features are given to the Mutual Exclusive Bayesian fusion model. The fusion model is applied at the score level for reducing fusion overhead. In this model, feature fusion is generated based on the log likelihood ratio by using covariance matrix measurement. Finally with fusion features, Distributed Hamming Distance Template Matching (DHDTM) algorithm is designed to evaluate the recognition rate of test data with available training data. The results show that the DHDTM significantly improves the recognition rate of human biometric samples when compared to the conventional person identification methods. Several tests were conducted to evaluate the performance of the proposed methods of standard biometric databases using three metrics, namely, matching fusion time, overhead and true positive rate. From the experimental results, the proposed PI-MED method reduces the matching fusion time and overhead by 47% and 45% when compared to existing methods. Similarly, the proposed PI-MED method increases the true positive rate by 33% when compared to existing methods."}}
