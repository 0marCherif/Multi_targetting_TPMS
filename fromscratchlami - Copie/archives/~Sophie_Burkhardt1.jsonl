{"id": "zM-1oVvfJnQ", "cdate": 1672531200000, "mdate": 1682323316667, "content": {"title": "Ordinal Regression for Difficulty Estimation of StepMania Levels", "abstract": "StepMania is a popular open-source clone of a rhythm-based video game. As is common in popular games, there is a large number of community-designed levels. It is often difficult for players and level authors to determine the difficulty level of such community contributions. In this work, we formalize and analyze the difficulty prediction task on StepMania levels as an ordinal regression (OR) task. We standardize a more extensive and diverse selection of this data resulting in five data sets, two of which are extensions of previous work. We evaluate many competitive OR and non-OR models, demonstrating that neural network-based models significantly outperform the state of the art and that StepMania-level data makes for an excellent test bed for deep OR models. We conclude with a user experiment showing our trained models' superiority over human labeling."}}
{"id": "iFWzNIAhDHe", "cdate": 1672531200000, "mdate": 1682323316565, "content": {"title": "Potential-based reward shaping for learning to play text-based adventure games", "abstract": "Text-based games are a popular testbed for language-based reinforcement learning (RL). In previous work, deep Q-learning is commonly used as the learning agent. Q-learning algorithms are challenging to apply to complex real-world domains due to, for example, their instability in training. Therefore, in this paper, we adapt the soft-actor-critic (SAC) algorithm to the text-based environment. To deal with sparse extrinsic rewards from the environment, we combine it with a potential-based reward shaping technique to provide more informative (dense) reward signals to the RL agent. We apply our method to play difficult text-based games. The SAC method achieves higher scores than the Q-learning methods on many games with only half the number of training steps. This shows that it is well-suited for text-based games. Moreover, we show that the reward shaping technique helps the agent to learn the policy faster and achieve higher scores. In particular, we consider a dynamically learned value function as a potential function for shaping the learner's original sparse reward signals."}}
{"id": "HLXqhGiLuix", "cdate": 1672531200000, "mdate": 1682323316806, "content": {"title": "Deep Anomaly Detection on Tennessee Eastman Process Data", "abstract": "This paper provides the first comprehensive evaluation and analysis of modern (deep-learning) unsupervised anomaly detection methods for chemical process data. We focus on the Tennessee Eastman process dataset, which has been a standard litmus test to benchmark anomaly detection methods for nearly three decades. Our extensive study will facilitate choosing appropriate anomaly detection methods in industrial applications."}}
{"id": "mmgN3yNSBo", "cdate": 1609459200000, "mdate": 1667809973025, "content": {"title": "Rule Extraction From Binary Neural Networks With Convolutional Rules for Model Validation", "abstract": "Classification approaches that allow to extract logical rules such as decision trees are often considered to be more interpretable than neural networks. Also, logical rules are comparatively easy to verify with any possible input. This is an important part in systems that aim to ensure correct operation of a given model. However, for high-dimensional input data such as images, the individual symbols, i.e. pixels, are not easily interpretable. Therefore, rule-based approaches are not typically used for this kind of high-dimensional data. We introduce the concept of first-order convolutional rules, which are logical rules that can be extracted using a convolutional neural network (CNN), and whose complexity depends on the size of the convolutional filter and not on the dimensionality of the input. Our approach is based on rule extraction from binary neural networks with stochastic local search. We show how to extract rules that are not necessarily short, but characteristic of the input, and easy to visualize. Our experiments show that the proposed approach is able to model the functionality of the neural network while at the same time producing interpretable logical rules. Thus, we demonstrate the potential of rule-based approaches for images which allows to combine advantages of neural networks and rule learning."}}
{"id": "MgziTeVJVhh", "cdate": 1609459200000, "mdate": 1682323316653, "content": {"title": "Topic-Guided Knowledge Graph Construction for Argument Mining", "abstract": "Decision-making tasks usually follow five steps: identifying the problem, collecting data, extracting evidence, iden-tifying arguments, and making the decision. This paper focuses on two steps of decision-making: extracting evidence by building knowledge graphs (KGs) of specialized topics and identifying sentences' arguments through sentence-level argument mining. We present a hybrid model that combines topic modeling using latent Dirichlet allocation (LDA) and word embeddings to obtain external knowledge from structured and unstructured data. We use a topic model to extract topic- and sentence-specific evidence from the structured knowledge base Wikidata. A knowledge graph is constructed based on the cosine similarity between the entity word vectors of Wikidata and the vector of the given sentence. A second graph based on topic-specific articles found via Google supplements the general incompleteness of the structured knowledge base. Combining these graphs, we obtain a graph-based model that, as our evaluation shows, successfully capitalizes on both structured and unstructured data."}}
{"id": "M57PH55Vvm3", "cdate": 1609459200000, "mdate": 1675157048699, "content": {"title": "Focusing Knowledge-based Graph Argument Mining via Topic Modeling", "abstract": "Decision-making usually takes five steps: identifying the problem, collecting data, extracting evidence, identifying pro and con arguments, and making decisions. Focusing on extracting evidence, this paper presents a hybrid model that combines latent Dirichlet allocation and word embeddings to obtain external knowledge from structured and unstructured data. We study the task of sentence-level argument mining, as arguments mostly require some degree of world knowledge to be identified and understood. Given a topic and a sentence, the goal is to classify whether a sentence represents an argument in regard to the topic. We use a topic model to extract topic- and sentence-specific evidence from the structured knowledge base Wikidata, building a graph based on the cosine similarity between the entity word vectors of Wikidata and the vector of the given sentence. Also, we build a second graph based on topic-specific articles found via Google to tackle the general incompleteness of structured knowledge bases. Combining these graphs, we obtain a graph-based model which, as our evaluation shows, successfully capitalizes on both structured and unstructured data."}}
{"id": "rnmnt8w6z6M", "cdate": 1600159416326, "mdate": null, "content": {"title": "Decoupling Sparsity and Smoothness in the DirichletVariational Autoencoder Topic Model", "abstract": "Recent work on variational autoencoders (VAEs) has enabled the development of generative topic models using neural networks.  Topic models based on latent Dirichlet allocation (LDA) successfully use the Dirichlet distribution as a prior for the topic and word distributions to enforce sparseness.  However, there is a trade-off between sparsity and smoothness in Dirichlet distributions.  Sparsity is important for a low reconstruction error during training of the autoencoder, whereas smoothness enables generalization and leads to a better log-likelihood of the test data.  Both of these properties are encoded in the Dirichlet parameter vector.  By rewriting this parameter vector into a product of a sparse binary vector anda smoothness vector,  we decouple the two properties,  leading to a model that features both a competitive topic coherence and a high log-likelihood.  Efficient training is enabledusing rejection sampling variational inference for the reparameterization of the Dirichlet distribution.  Our experiments show that our method is competitive with other recent VAE topic models."}}
{"id": "CiuCJiKhsq", "cdate": 1577836800000, "mdate": 1682323316588, "content": {"title": "Rule Extraction from Binary Neural Networks with Convolutional Rules for Model Validation", "abstract": "Most deep neural networks are considered to be black boxes, meaning their output is hard to interpret. In contrast, logical expressions are considered to be more comprehensible since they use symbols that are semantically close to natural language instead of distributed representations. However, for high-dimensional input data such as images, the individual symbols, i.e. pixels, are not easily interpretable. We introduce the concept of first-order convolutional rules, which are logical rules that can be extracted using a convolutional neural network (CNN), and whose complexity depends on the size of the convolutional filter and not on the dimensionality of the input. Our approach is based on rule extraction from binary neural networks with stochastic local search. We show how to extract rules that are not necessarily short, but characteristic of the input, and easy to visualize. Our experiments show that the proposed approach is able to model the functionality of the neural network while at the same time producing interpretable logical rules."}}
{"id": "2DF1HQsv8Lr", "cdate": 1577836800000, "mdate": 1682323316679, "content": {"title": "Towards Identifying Drug Side Effects from Social Media Using Active Learning andCrowd Sourcing", "abstract": ""}}
{"id": "ajRg_kB2HFe", "cdate": 1546300800000, "mdate": 1682323316695, "content": {"title": "A Survey of Multi-Label Topic Models", "abstract": "Every day, an enormous amount of text data is produced. Sources of text data include news, social media, emails, text messages, medical reports, scientific publications and fiction. To keep track of this data, there are categories, key words, tags or labels that are assigned to each text. Automatically predicting such labels is the task of multi-label text classification. Often however, we are interested in more than just the pure classification: rather, we would like to understand which parts of a text belong to the label, which words are important for the label or which labels occur together. Because of this, topic models may be used for multi-label classification as an interpretable model that is flexible and easily extensible. This survey demonstrates the manifold possibilities and flexibility of the topic model framework for the complex setting of multi-label text classification by categorizing different variants of models."}}
