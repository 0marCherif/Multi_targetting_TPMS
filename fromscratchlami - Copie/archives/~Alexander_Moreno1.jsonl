{"id": "uphN8X3uf4", "cdate": 1676827098612, "mdate": null, "content": {"title": "KrADagrad: Kronecker Approximation-Domination Gradient Preconditioned Stochastic Optimization", "abstract": "Second order stochastic optimizers allow parameter update step size and direction to adapt to loss curvature, but have traditionally required too much memory and compute for deep learning. Recently, Shampoo [Gupta et al., 2018] introduced a Kronecker factored preconditioner to reduce these requirements: it is used for large deep models [Anil et al., 2020] and in production [Anil et al., 2022]. However, it takes inverse matrix roots of ill-conditioned matrices. This requires 64-bit precision, imposing strong hardware constraints. In this paper, we propose a novel factorization, Kronecker Approximation-Domination (KrAD). Using KrAD, we update a matrix that directly approximates the inverse empirical Fisher matrix (like full matrix AdaGrad), avoiding inversion and hence 64-bit precision. We then propose KrADagrad$^\\star$, with similar computational costs to Shampoo and the same regret. Synthetic ill-conditioned experiments show improved performance over Shampoo for 32-bit precision, while for several real datasets we have comparable or better generalization."}}
{"id": "x_kBZYiUrxR", "cdate": 1654519633669, "mdate": null, "content": {"title": "PulseImpute: A Novel Benchmark Task for Pulsative Physiological Signal Imputation", "abstract": "The promise of Mobile Health (mHealth) is the ability to use wearable sensors to monitor participant physiology at high frequencies during daily life to enable temporally-precise health interventions. However, a major challenge is frequent missing data. Despite a rich imputation literature, existing techniques are ineffective for the pulsative signals which comprise many mHealth applications, and a lack of available datasets has stymied progress. We address this gap with PulseImpute, the first large-scale pulsative signal imputation challenge which includes realistic mHealth missingness models, an extensive set of baselines, and clinically-relevant downstream tasks. Our baseline models include a novel transformer-based architecture designed to exploit the structure of pulsative signals. We hope that PulseImpute will enable the ML community to tackle this important and challenging task."}}
{"id": "qmy23tNBvbh", "cdate": 1652737778792, "mdate": null, "content": {"title": "Kernel Multimodal Continuous Attention", "abstract": "Attention mechanisms take an expectation of a data representation with respect to probability weights. Recently, (Martins et al. 2020, 2021) proposed continuous attention mechanisms, focusing on unimodal attention densities from the exponential and deformed exponential families: the latter has sparse support. (Farinhas et al 2021) extended this to to multimodality via Gaussian mixture attention densities. In this paper, we extend this to kernel exponential families (Canu and Smola 2006) and our new sparse counterpart, kernel deformed exponential families. Theoretically, we show new existence results for both kernel exponential and deformed exponential families, and that the deformed case has similar approximation capabilities to kernel exponential families. Lacking closed form expressions for the context vector, we use numerical integration: we show exponential convergence for both kernel exponential and deformed exponential families. Experiments show that kernel continuous attention often outperforms unimodal continuous attention, and the sparse variant tends to highlight peaks of time series."}}
{"id": "eHi3Ru_P42I", "cdate": 1640995200000, "mdate": 1679339490522, "content": {"title": "PulseImpute: A Novel Benchmark Task for Pulsative Physiological Signal Imputation", "abstract": ""}}
{"id": "SK1nec-Ehd", "cdate": 1632875762450, "mdate": null, "content": {"title": "PulseImpute: A Novel Benchmark Task and Architecture for Imputation of Physiological Signals", "abstract": " Providing care for patients with chronic diseases is one of the biggest drivers of the nation\u2019s rising healthcare costs, but many of these diseases are linked to mutable health behaviors. Mobile health (mHealth) biophysical sensors that continuously measure our current conditions provide the framework for a personalized guidance system for the maintenance of healthy behaviors. However, this physiological sensor data is plagued with missingness due to insecure attachments, wireless dropout, battery, and adherence issues. These issues cripple their rich diagnostic utility as well as their ability to enable temporally-precise interventions. While there is a sizable amount of research focusing on imputation methods, surprisingly, no works have addressed the patterns of missingness, quasi-periodic signal structure, and the between subject heterogeneity that characterizes physiological signals in mHealth applications. We present the PulseImpute Challenge, the first challenge dataset for physiological signal imputation which includes a large set of baselines' performances on realistic missingness models and data. Next, we demonstrate the potential to address this quasi-periodic structure and heterogeneity with our Dilated Convolution Bottleneck (DCB) Transformer, a transformer architecture with a self-attention mechanism that is able to attend to corresponding waveform features in quasi-periodic signals. By utilizing stacked dilated convolutions with bottleneck layers for query and key transformations, we visually demonstrate that the kernel similarity in the attention model gives high similarity to similar temporal features across quasi-periodic periods. We hope the release of our challenge task definitions and baseline implementations will spur the community to address this challenging and important problem. \n "}}
{"id": "hqkN6lE1fFQ", "cdate": 1632875433169, "mdate": null, "content": {"title": "Kernel Deformed Exponential Families for Sparse Continuous Attention", "abstract": "Attention mechanisms take an expectation of a data representation with respect to probability weights. This creates summary statistics that focus on important features. Recently, Martins et al. (2020, 2021) proposed continuous attention mechanisms, focusing on unimodal attention densities from the exponential and deformed exponential families: the latter has sparse support. Farinhas et al. (2021) extended this to use Gaussian mixture attention densities, which are a flexible class with dense support. In this paper, we extend this to two general flexible classes: kernel exponential families and our new sparse counterpart kernel deformed exponential families. Theoretically, we show new existence results for both kernel exponential and deformed exponential families, and that the deformed case has similar approximation capabilities to kernel exponential families. Experiments show that kernel deformed exponential families can attend to non-overlapping intervals of time."}}
{"id": "tSpk4MDgaIa", "cdate": 1609459200000, "mdate": 1679339490519, "content": {"title": "Transformers for prompt-level EMA non-response prediction", "abstract": ""}}
{"id": "STf-7DnCve5", "cdate": 1609459200000, "mdate": 1645894746675, "content": {"title": "Efficient Learning and Decoding of the Continuous-Time Hidden Markov Model for Disease Progression Modeling", "abstract": "The Continuous-Time Hidden Markov Model (CT-HMM) is an attractive approach to modeling disease progression due to its ability to describe noisy observations arriving irregularly in time. However, the lack of an efficient parameter learning algorithm for CT-HMM restricts its use to very small models or requires unrealistic constraints on the state transitions. In this paper, we present the first complete characterization of efficient EM-based learning methods for CT-HMM models, as well as the first solution to decoding the optimal state transition sequence and the corresponding state dwelling time. We show that EM-based learning consists of two challenges: the estimation of posterior state probabilities and the computation of end-state conditioned statistics. We solve the first challenge by reformulating the estimation problem as an equivalent discrete time-inhomogeneous hidden Markov model. The second challenge is addressed by adapting three distinct approaches from the continuous time Markov chain (CTMC) literature to the CT-HMM domain. Additionally, we further improve the efficiency of the most efficient method by a factor of the number of states. Then, for decoding, we incorporate a state-of-the-art method from the (CTMC) literature, and extend the end-state conditioned optimal state sequence decoding to the CT-HMM case with the computation of the expected state dwelling time. We demonstrate the use of CT-HMMs with more than 100 states to visualize and predict disease progression using a glaucoma dataset and an Alzheimer's disease dataset, and to decode and visualize the most probable state transition trajectory for individuals on the glaucoma dataset, which helps to identify progressing phenotypes in a comprehensive way. Finally, we apply the CT-HMM modeling and decoding strategy to investigate the progression of language acquisition and development."}}
{"id": "BIXbtEJNMQ5", "cdate": 1609459200000, "mdate": 1648668464944, "content": {"title": "Kernel Deformed Exponential Families for Sparse Continuous Attention", "abstract": "Attention mechanisms take an expectation of a data representation with respect to probability weights. This creates summary statistics that focus on important features. Recently, (Martins et al. 2020, 2021) proposed continuous attention mechanisms, focusing on unimodal attention densities from the exponential and deformed exponential families: the latter has sparse support. (Farinhas et al. 2021) extended this to use Gaussian mixture attention densities, which are a flexible class with dense support. In this paper, we extend this to two general flexible classes: kernel exponential families and our new sparse counterpart kernel deformed exponential families. Theoretically, we show new existence results for both kernel exponential and deformed exponential families, and that the deformed case has similar approximation capabilities to kernel exponential families. Experiments show that kernel deformed exponential families can attend to multiple compact regions of the data domain."}}
{"id": "H9UbFEJ4MX5", "cdate": 1577836800000, "mdate": 1648668464955, "content": {"title": "A Functional EM Algorithm for Panel Count Data with Missing Counts", "abstract": "Panel count data describes aggregated counts of recurrent events observed at discrete time points. To understand dynamics of health behaviors, the field of quantitative behavioral research has evolved to increasingly rely upon panel count data collected via multiple self reports, for example, about frequencies of smoking using in-the-moment surveys on mobile devices. However, missing reports are common and present a major barrier to downstream statistical learning. As a first step, under a missing completely at random assumption (MCAR), we propose a simple yet widely applicable functional EM algorithm to estimate the counting process mean function, which is of central interest to behavioral scientists. The proposed approach wraps several popular panel count inference methods, seamlessly deals with incomplete counts and is robust to misspecification of the Poisson process assumption. Theoretical analysis of the proposed algorithm provides finite-sample guarantees by expanding parametric EM theory to our general non-parametric setting. We illustrate the utility of the proposed algorithm through numerical experiments and an analysis of smoking cessation data. We also discuss useful extensions to address deviations from the MCAR assumption and covariate effects."}}
