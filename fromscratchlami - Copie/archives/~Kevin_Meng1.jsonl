{"id": "MkbcAHIYgyS", "cdate": 1663849981048, "mdate": null, "content": {"title": "Mass-Editing Memory in a Transformer", "abstract": "Recent work has shown exciting promise in updating large language models with new memories, so as to replace obsolete information or add specialized knowledge. However, this line of work is predominantly limited to updating single associations. We develop MEMIT, a method for directly updating a language model with many memories, demonstrating experimentally that it can scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B), exceeding prior work by an order of magnitude. Our code and data will be open-sourced upon publication."}}
{"id": "-h6WAS6eE4", "cdate": 1652737603575, "mdate": null, "content": {"title": "Locating and Editing Factual Associations in GPT", "abstract": "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME).  We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available in the supplemental materials."}}
{"id": "sH_EyjJP0jX", "cdate": 1640995200000, "mdate": 1682318049899, "content": {"title": "Mass-Editing Memory in a Transformer", "abstract": "Recent work has shown exciting promise in updating large language models with new memories, so as to replace obsolete information or add specialized knowledge. However, this line of work is predominantly limited to updating single associations. We develop MEMIT, a method for directly updating a language model with many memories, demonstrating experimentally that it can scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B), exceeding prior work by orders of magnitude. Our code and data are at https://memit.baulab.info."}}
{"id": "pperdYwN4mx", "cdate": 1640995200000, "mdate": 1682318049901, "content": {"title": "Locating and Editing Factual Knowledge in GPT", "abstract": "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/"}}
{"id": "MAtYWPEwo5", "cdate": 1640995200000, "mdate": 1682318049865, "content": {"title": "Exploiting and Defending Against the Approximate Linearity of Apple's NeuralHash", "abstract": "Perceptual hashes map images with identical semantic content to the same $n$-bit hash value, while mapping semantically-different images to different hashes. These algorithms carry important applications in cybersecurity such as copyright infringement detection, content fingerprinting, and surveillance. Apple's NeuralHash is one such system that aims to detect the presence of illegal content on users' devices without compromising consumer privacy. We make the surprising discovery that NeuralHash is approximately linear, which inspires the development of novel black-box attacks that can (i) evade detection of \"illegal\" images, (ii) generate near-collisions, and (iii) leak information about hashed images, all without access to model parameters. These vulnerabilities pose serious threats to NeuralHash's security goals; to address them, we propose a simple fix using classical cryptographic standards."}}
{"id": "I23cYS7j90s", "cdate": 1609459200000, "mdate": 1652416160135, "content": {"title": "A Dashboard for Mitigating the COVID-19 Misinfodemic", "abstract": "Zhengyuan Zhu, Kevin Meng, Josue Caraballo, Israa Jaradat, Xiao Shi, Zeyu Zhang, Farahnaz Akrami, Haojin Liao, Fatma Arslan, Damian Jimenez, Mohanmmed Samiul Saeef, Paras Pathak, Chengkai Li. Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations. 2021."}}
{"id": "ro2McR_DLZs", "cdate": 1577836800000, "mdate": 1652416160139, "content": {"title": "Gradient-Based Adversarial Training on Transformer Networks for Detecting Check-Worthy Factual Claims", "abstract": "We present a study on the efficacy of adversarial training on transformer neural network models, with respect to the task of detecting check-worthy claims. In this work, we introduce the first adversarially-regularized, transformer-based claim spotter model that achieves state-of-the-art results on multiple challenging benchmarks. We obtain a 4.70 point F1-score improvement over current state-of-the-art models on the ClaimBuster Dataset and CLEF2019 Dataset, respectively. In the process, we propose a method to apply adversarial training to transformer models, which has the potential to be generalized to many similar text classification tasks. Along with our results, we are releasing our codebase and manually labeled datasets. We also showcase our models' real world usage via a live public API."}}
{"id": "pDq8PJgfou", "cdate": 1546300800000, "mdate": 1682318049857, "content": {"title": "Through-Wall Pose Imaging in Real-Time with a Many-to-Many Encoder/Decoder Paradigm", "abstract": "Overcoming the visual barrier and developing \"see-through vision\" has been one of mankind's long-standing dreams. Unlike visible light, Radio Frequency (RF) signals penetrate opaque obstructions and reflect highly off humans. This paper establishes a deep-learning model that can be trained to reconstruct continuous video of a 15-point human skeleton even through visual occlusion. The training process adopts a student/teacher learning procedure inspired by the Feynman learning technique, in which video frames and RF data are first collected simultaneously using a co-located setup containing an optical camera and an RF antenna array transceiver. Next, the video frames are processed with a computer-vision-based gait analysis \"teacher\" module to generate ground-truth human skeletons for each frame. Then, the same type of skeleton is predicted from corresponding RF data using a \"student\" deep-learning model consisting of a Residual Convolutional Neural Network (CNN), Region Proposal Network (RPN), and Recurrent Neural Network with Long-Short Term Memory (LSTM) that 1) extracts spatial features from RF images, 2) detects all people present in a scene, and 3) aggregates information over many time-steps, respectively. The model is shown to both accurately and completely predict the pose of humans behind visual obstruction solely using RF signals. Primary academic contributions include the novel many-to-many imaging methodology, unique integration of RPN and LSTM networks, and original training pipeline."}}
{"id": "856fnQ2WRV", "cdate": 1514764800000, "mdate": 1682318049910, "content": {"title": "Vehicle Action Prediction Using Artificial Intelligence", "abstract": "Each year, car accidents on United States roadways claim tens of thousands of lives and injure millions of others, of which almost half involve a combination of two critical pre-crash events: turning and changing lanes. Advanced Driver Assistance Systems (ADAS) that are currently installed in vehicles provide reactive protections that warn drivers of dangers up to 0.5 seconds ahead of collisions. However, rule of thumb suggests 2 seconds for safety in emergency reactions; many lives could be saved even with a slight improvement to the warning time. This paper develops an innovative two-stage neural network model that predicts drivers' actions before fatal collisions can occur. In a novel procedural flow, data is collected from sensors and devices installed inside and outside the vehicle including two cameras, a Global Positioning System (GPS) module, an Onboard Diagnostics-II (OBD-II) interface, and a gyroscope, preprocessed with a Convolutional Neural Network-based (CNN) computer vision model to extract facial movements and rotation, filtered and selected with the Classification and Regression Tree (CART), and modeled with a Recurrent Neural Network w/ Long Short-Term Memory (RNN-LSTM). Results show that the methodology presented in this paper is superior compared to existing ones."}}
