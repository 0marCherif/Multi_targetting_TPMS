{"id": "WiH-y3kxNDn", "cdate": 1654041600000, "mdate": 1683554758502, "content": {"title": "Report on the 1st Workshop on Audio Collection Human Interaction (AudioCHI 2022) at CHIIR 2022", "abstract": "This is a report from the AudioCHI 2022 workshop which was organised in conjunction with the CHIIR conference in March 2022. The workshop was organised to bring together researchers in spoken content retrieval with expertise on human computer interaction in information access to examine opportunities and challenges for advancing technologies for search and interaction with spoken content. The workshop was originally planned to be a full day event, but due to pandemic-imposed travelling constraints, it was instead held as a virtual half day event to accommodate time zone differences for participants. Date: 14 March, 2022. Website: https://speechretrievalworkshop.github.io/."}}
{"id": "u-SfqnLLBg", "cdate": 1640995200000, "mdate": 1683554758538, "content": {"title": "The Contribution of Lyrics and Acoustics to Collaborative Understanding of Mood", "abstract": "In this work, we study the association between song lyrics and mood through a data-driven analysis. Our data set consists of nearly one million songs, with song-mood associations derived from user playlists on the Spotify streaming platform. We take advantage of state-of-the-art natural language processing models based on transformers to learn the association between the lyrics and moods. We find that a pretrained transformer-based language model in a zero-shot setting -- i.e., out of the box with no further training on our data -- is powerful for capturing song-mood associations. Moreover, we illustrate that training on song-mood associations results in a highly accurate model that predicts these associations for unseen songs. Furthermore, by comparing the prediction of a model using lyrics with one using acoustic features, we observe that the relative importance of lyrics for mood prediction in comparison with acoustics depends on the specific mood. Finally, we verify if the models are capturing the same information about lyrics and acoustics as humans through an annotation task where we obtain human judgments of mood-song relevance based on lyrics and acoustics."}}
{"id": "VkZNzalCeb", "cdate": 1640995200000, "mdate": 1683554758513, "content": {"title": "The Contribution of Lyrics and Acoustics to Collaborative Understanding of Mood", "abstract": "In this work, we study the association between song lyrics and mood through a data-driven analysis. Our data set consists of nearly one million songs, with song-mood associations derived from user playlists on the Spotify streaming platform. We take advantage of state-of-the-art natural language processing models based on transformers to learn the association between the lyrics and moods. We find that a pretrained transformer-based language model in a zero-shot setting -- i.e., out of the box with no further training on our data -- is powerful for capturing song-mood associations. Moreover, we illustrate that training on song-mood associations results in a highly accurate model that predicts these associations for unseen songs. Furthermore, by comparing the prediction of a model using lyrics with one using acoustic features, we observe that the relative importance of lyrics for mood prediction in comparison with acoustics depends on the specific mood. Finally, we verify if the models are capturing the same information about lyrics and acoustics as humans through an annotation task where we obtain human judgments of mood-song relevance based on lyrics and acoustics."}}
{"id": "VP_u9kmfsJr", "cdate": 1640995200000, "mdate": 1683554758486, "content": {"title": "CHIIR Workshop on Audio Collection Human Interaction (AudioCHI 2022): http: //speechretrievalworkshop.github.io", "abstract": "The AudioCHI 2022 workshop focusses on human engagement with spoken material in search settings, including live stream audio and collections. Spoken material comes in many forms, including for example: factual or entertaining (or both!), timely or of historical interest, local or global, single speaker or conversations. Users engage with spoken material for a variety of reasons, including entertainment, current affairs, education, and research. While there has been considerable previous work studying spoken document retrieval or more generally spoken content retrieval, AudioCHI 2022 is the first meeting to explore user engagement with audio content, including discussing: (i) how content analysis might establish verbal and non-verbal features for rich content representations, and (ii) and use cases and human factors in interaction with spoken audio content, and their interaction with more established topics relating to spoken content retrieval. The workshop brings together researchers in spoken content retrieval with expertise on human computer interaction in information access to examine opportunities and challenges for advancing technologies for search and interaction with spoken content."}}
{"id": "BERY5koQnju", "cdate": 1640995200000, "mdate": 1683554758495, "content": {"title": "Cem Mil Podcasts: A Spoken Portuguese Document Corpus", "abstract": "This document describes the Portuguese language podcast dataset released by Spotify for academic research purposes. We give an overview of how the data was sampled, some basic statistics over the collection, as well as brief information of distribution over Brazilian and Portuguese dialects."}}
{"id": "6DjCKG4hEN", "cdate": 1609459200000, "mdate": 1683554734465, "content": {"title": "The in-the-Wild Speech Medical Corpus", "abstract": "Automatic detection of speech affecting (SA) diseases has received significant attention, particularly in clinical scenarios. However, the same task in in-the-wild conditions is often neglected, in part, due to the lack of appropriate datasets.In this work, we present the in-the-Wild Speech Medical (WSM) Corpus, a collection of in-the-wild videos, featuring subjects potentially affected by a SA disease - specifically, depression or Parkinson\u2019s disease. The WSM Corpus contains a total 928 videos, and over 131 hours of speech. Each video is accompanied by a crowdsourced annotation for perceived age/gender, and self-reported health status of the speaker. The WSM Corpus is balanced over all the labels.In this work we present a detailed description of the collection, and annotation processes of the WSM corpus. Furthermore, we present present several baseline systems for the detection of SA diseases using speech alone, thus motivating the use of this type of in-the-wild data in paralinguistic audiovisual tasks."}}
{"id": "-eSkQM-J9z3", "cdate": 1577836800000, "mdate": 1683554734692, "content": {"title": "Automatic In-the-wild Dataset Annotation with Deep Generalized Multiple Instance Learning", "abstract": ""}}
{"id": "bofxBXhkkyu", "cdate": 1546300800000, "mdate": 1683554734708, "content": {"title": "In-the-Wild End-to-End Detection of Speech Affecting Diseases", "abstract": "Speech is a complex bio-signal that has the potential to provide a rich bio-marker for health. It enables the development of non-invasive routes to early diagnosis and monitoring of speech affecting diseases, such as the ones studied in this work: Depression, and Parkinson's Disease. However, the major limitation of current speech based diagnosis and monitoring tools is the lack of large and diverse datasets. Existing datasets are small, and collected under very controlled conditions. As such, there is an upper bound in the complexity of the models that can be trained using these datasets. There is also limited applicability in real life scenarios where the channel and noise conditions, among others, are impossible to control. In this work, we show that datasets collected from in-the-wild sources, such as collections of vlogs, can contribute to improve the performance of diagnosis tools both in controlled and in-the-wild conditions, even though the data are noisier. Moreover, we show that it is possible to successfully move away from hand-crafted features (i.e. features that are computed based on predefined algorithms, that based on human expertise) and adopt end-to-end modeling paradigms, such as CNN-LSTMs, that extract data driven features from the raw spectrograms of the speech signal, and capture temporal information from the speech signals."}}
{"id": "dZPQ4zToVe", "cdate": 1514764800000, "mdate": 1683554734811, "content": {"title": "Speech Analytics for Medical Applications", "abstract": "Speech has the potential to provide a rich bio-marker for health, allowing a non-invasive route to early diagnosis and monitoring of a range of conditions related to human physiology and cognition. With the rise of speech related machine learning applications over the last decade, there has been a growing interest in developing speech based tools that perform non-invasive diagnosis. This talk covers two aspects related to this growing trend. One is the collection of large in-the-wild multimodal datasets in which the speech of the subject is affected by certain medical conditions. Our mining effort has been focused on video blogs (vlogs), and explores audio, video, text and metadata cues, in order to retrieve vlogs that include a single speaker which, at some point, admits that he/she is currently affected by a given disease. The second aspect is patient privacy. In this context, we explore recent developments in cryptography and, in particular in Fully Homomorphic Encryption, to develop an encrypted version of a neural network trained with unencrypted data, in order to produce encrypted predictions of health-related labels. As a proof-of-concept, we have selected two target diseases: Cold and Depression, to show our results and discuss these two aspects."}}
{"id": "bjvJ_azkv1", "cdate": 1514764800000, "mdate": 1683554734706, "content": {"title": "Querying Depression Vlogs", "abstract": "Speech based diagnosis-aid tools for depression typically depend on few and small datasets, that are expensive to collect. The limited availability of training data poses a limitation to the quality that these systems can achieve. An unexplored alternative for large scale source of data are vlogs collected from online multimedia repositories. Along with the automation of the mining process, it is necessary to automate the labeling process too.In this work, we propose a framework to automatically label a corpus of in-the-wild vlogs of possibly depressed subjects, and we estimate the quality of the predicted labels, without ever having access to a ground truth for the majority of the corpus. The framework uses a small subset to train a model and estimate the labels for the remainder of the corpus. Then, using the predicted labels, we train a noisy model and attempt to reconstruct the labels of the original labeled subset. We hypothesize that the quality of the estimated labels for the unlabelled subset of the corpus is correlated to the quality of the label reconstruction of the labeled subset.The results of the bi-modal experiment using in-the-wild data are compared to the ones obtained using controlled data."}}
