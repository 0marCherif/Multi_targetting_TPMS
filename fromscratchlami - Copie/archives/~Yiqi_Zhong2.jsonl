{"id": "ofuPqO_TCS", "cdate": 1640995200000, "mdate": 1668592863171, "content": {"title": "Shadows can be Dangerous: Stealthy and Effective Physical-world Adversarial Attack by Natural Phenomenon", "abstract": "Estimating the risk level of adversarial examples is essential for safely deploying machine learning models in the real world. One popular approach for physical-world attacks is to adopt the \u201csticker-pasting\u201d strategy, which however suffers from some limitations, including difficulties in access to the target or printing by valid colors. A new type of non-invasive attacks emerged recently, which attempt to cast perturbation onto the target by optics based tools, such as laser beam and projector. However, the added optical patterns are artificial but not natural. Thus, they are still conspicuous and attention-grabbed, and can be easily noticed by humans. In this paper, we study a new type of optical adversarial examples, in which the perturbations are generated by a very common natural phenomenon, shadow, to achieve naturalistic and stealthy physical-world adversarial attack under the black-box setting. We extensively evaluate the effectiveness of this new attack on both simulated and real-world environments. Experimental results on traffic sign recognition demonstrate that our algorithm can generate adversarial examples effectively, reaching 98.23% and 90.47% success rates on LISA and GTSRB test sets respectively, while continuously misleading a moving camera over 95% of the time in real-world scenarios. We also offer discussions about the limitations and the defense mechanism of this attack <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> Our code is available at https://github.com/hncszyq/ShadowAttack."}}
{"id": "Fy01WSiGwIC", "cdate": 1640995200000, "mdate": 1668592863190, "content": {"title": "Exploiting the Potential of Datasets: A Data-Centric Approach for Model Robustness", "abstract": "Robustness of deep neural networks (DNNs) to malicious perturbations is a hot topic in trustworthy AI. Existing techniques obtain robust models given fixed datasets, either by modifying model structures, or by optimizing the process of inference or training. While significant improvements have been made, the possibility of constructing a high-quality dataset for model robustness remain unexplored. Follow the campaign of data-centric AI launched by Andrew Ng, we propose a novel algorithm for dataset enhancement that works well for many existing DNN models to improve robustness. Transferable adversarial examples and 14 kinds of common corruptions are included in our optimized dataset. In the data-centric robust learning competition hosted by Alibaba Group and Tsinghua University, our algorithm came third out of more than 3000 competitors in the first stage while we ranked fourth in the second stage. Our code is available at \\url{https://github.com/hncszyq/tianchi_challenge}."}}
{"id": "I3V7nknDYVs", "cdate": 1609459200000, "mdate": 1668592863196, "content": {"title": "Different Dimension Issues in Deep Feature Space for Finger-Vein Recognition", "abstract": "Hand-crafted approaches were the dominating solutions and recently, more convolutional neural network (CNN)-based methods have been proposed for finger-vein recognition. However, the previous deep learning methods usually designed the network architecture with increasing layers and parameters, which incurs device memory issues and processing speed issues. Although many researchers have devoted to design image enhancement algorithms to improve the recognition performance of hand-crafted methods, it is interesting to investigate whether deep learning method can achieve satisfactory performance without image enhancement. This paper focuses on two different dimension issues: lightweight CNN design and the impact of image enhancement on deep learning methods. The experimental results demonstrate that the proposed method LFVRN is comparable or superior to the prior competition winners. In addition, image enhancement is validated not inevitable for the proposed lightweight CNN model LFVRN."}}
