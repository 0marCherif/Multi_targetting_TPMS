{"id": "05g7mnKJyJ7", "cdate": 1683910859733, "mdate": 1683910859733, "content": {"title": "Is Local SGD Better than Minibatch SGD?", "abstract": "We study local SGD (also known as parallel SGD and federated averaging), a natural and frequently\nused stochastic distributed optimization method. Its theoretical foundations are currently lacking and\nwe highlight how all existing error guarantees in the convex setting are dominated by a simple baseline,\nminibatch SGD. (1) For quadratic objectives we prove that local SGD strictly dominates minibatch SGD\nand that accelerated local SGD is minimax optimal for quadratics; (2) For general convex objectives we\nprovide the first guarantee that at least sometimes improves over minibatch SGD; (3) We show that\nindeed local SGD does not dominate minibatch SGD by presenting a lower bound on the performance\nof local SGD that is worse than the minibatch SGD guarantee."}}
{"id": "KKfjOEvDwQ", "cdate": 1664731452750, "mdate": null, "content": {"title": "Distributed Online and Bandit Convex Optimization", "abstract": "We study the problems of distributed online and bandit convex optimization against an adaptive adversary. Our goal is to minimize the average regret on M machines working in parallel over T rounds that can communicate R times intermittently. Assuming the underlying cost functions are convex, our results show collaboration is not beneficial if the machines have access to the first-order gradient information at the queried points. We show that in this setting, simple non-collaborative algorithms are min-max optimal, as opposed to the case for stochastic functions, where each machine samples the cost functions from a fixed distribution. Next, we consider the more challenging setting of federated optimization with bandit (zeroth-order) feedback, where the machines can only access values of the cost functions at the queried points. The key finding here is to identify the high-dimensional regime where collaboration is beneficial and may even lead to a linear speedup in the number of machines. Our results are the first attempts towards bridging the gap between distributed online optimization against stochastic and adaptive adversaries."}}
{"id": "TZQ3PKL3fPr", "cdate": 1664731452636, "mdate": null, "content": {"title": "On Convexity and Linear Mode Connectivity in Neural Networks", "abstract": "In many cases, neural networks trained with stochastic gradient descent (SGD) that share an early and often small portion of the training trajectory have solutions connected by a linear path of low loss. This phenomenon, called linear mode connectivity (LMC), has been leveraged for pruning and model averaging in large neural network models, but it is not well understood how broadly or why it occurs. LMC suggests that SGD trajectories somehow end up in a \\textit{``convex\"} region of the loss landscape and stay there. In this work, we confirm that this eventually does happen by finding a high-dimensional convex hull of low loss between the endpoints of several SGD trajectories. But to our surprise, simple measures of convexity do not show any obvious transition at the point when SGD will converge into this region. To understand this convex hull better, we investigate the functional behaviors of its endpoints. We find that only a small number of correct predictions are shared between all endpoints of a hull, and an even smaller number of correct predictions are shared between the hulls, even when the final accuracy is high for every endpoint. Thus, we tie LMC more tightly to convexity, and raise several new questions about the source of this convexity in neural network optimization."}}
{"id": "SNElc7QmMDe", "cdate": 1652737602689, "mdate": null, "content": {"title": "Towards Optimal Communication Complexity in Distributed Non-Convex Optimization", "abstract": "We study the problem of distributed stochastic non-convex optimization with intermittent communication. We consider the full participation setting where $M$ machines work in parallel over $R$ communication rounds and the partial participation setting where $M$ machines are sampled independently every round from some meta-distribution over machines. We propose and analyze a new algorithm that improves existing methods by requiring fewer and lighter variance reduction operations. We also present lower bounds, showing our algorithm is either $\\textit{optimal}$ or $\\textit{almost optimal}$ in most settings. Numerical experiments demonstrate the superior performance of our algorithm."}}
{"id": "ui0sz9Y2x9X", "cdate": 1621630318303, "mdate": null, "content": {"title": "A Stochastic Newton Algorithm for Distributed Convex Optimization", "abstract": "We propose and analyze a stochastic Newton algorithm for homogeneous distributed stochastic convex optimization, where each machine can calculate stochastic gradients of the same population objective, as well as stochastic Hessian-vector products (products of an independent unbiased estimator of the Hessian of the population objective with arbitrary vectors), with many such stochastic computations performed between rounds of communication.  We show that our method can reduce the number, and frequency, of required communication rounds, compared to existing methods without hurting performance, by proving convergence guarantees for quasi-self-concordant objectives (e.g., logistic regression), alongside empirical evidence."}}
{"id": "5BD4_awH4Fd", "cdate": 1621630318303, "mdate": null, "content": {"title": "A Stochastic Newton Algorithm for Distributed Convex Optimization", "abstract": "We propose and analyze a stochastic Newton algorithm for homogeneous distributed stochastic convex optimization, where each machine can calculate stochastic gradients of the same population objective, as well as stochastic Hessian-vector products (products of an independent unbiased estimator of the Hessian of the population objective with arbitrary vectors), with many such stochastic computations performed between rounds of communication.  We show that our method can reduce the number, and frequency, of required communication rounds, compared to existing methods without hurting performance, by proving convergence guarantees for quasi-self-concordant objectives (e.g., logistic regression), alongside empirical evidence."}}
{"id": "NgqDLWWMBc", "cdate": 1609459200000, "mdate": 1682335675659, "content": {"title": "A Stochastic Newton Algorithm for Distributed Convex Optimization", "abstract": "We propose and analyze a stochastic Newton algorithm for homogeneous distributed stochastic convex optimization, where each machine can calculate stochastic gradients of the same population objective, as well as stochastic Hessian-vector products (products of an independent unbiased estimator of the Hessian of the population objective with arbitrary vectors), with many such stochastic computations performed between rounds of communication. We show that our method can reduce the number, and frequency, of required communication rounds, compared to existing methods without hurting performance, by proving convergence guarantees for quasi-self-concordant objectives (e.g., logistic regression), alongside empirical evidence."}}
{"id": "725ieSgeIpr", "cdate": 1609459200000, "mdate": 1682353149267, "content": {"title": "A Stochastic Newton Algorithm for Distributed Convex Optimization", "abstract": "We propose and analyze a stochastic Newton algorithm for homogeneous distributed stochastic convex optimization, where each machine can calculate stochastic gradients of the same population objective, as well as stochastic Hessian-vector products (products of an independent unbiased estimator of the Hessian of the population objective with arbitrary vectors), with many such stochastic computations performed between rounds of communication. We show that our method can reduce the number, and frequency, of required communication rounds compared to existing methods without hurting performance, by proving convergence guarantees for quasi-self-concordant objectives (e.g., logistic regression), alongside empirical evidence."}}
{"id": "pBxDp66wJn", "cdate": 1577836800000, "mdate": 1681925567836, "content": {"title": "Don't Use Large Mini-batches, Use Local SGD", "abstract": "Mini-batch stochastic gradient methods (SGD) are state of the art for distributed training of deep neural networks. Drastic increases in the mini-batch sizes have lead to key efficiency and scalability gains in recent years. However, progress faces a major roadblock, as models trained with large batches often do not generalize well, i.e. they do not show good accuracy on new data. As a remedy, we propose a \\emph{post-local} SGD and show that it significantly improves the generalization performance compared to large-batch training on standard benchmarks while enjoying the same efficiency (time-to-accuracy) and scalability. We further provide an extensive study of the communication efficiency vs. performance trade-offs associated with a host of \\emph{local SGD} variants."}}
{"id": "SHrxdGy_idv", "cdate": 1577836800000, "mdate": null, "content": {"title": "Minibatch vs Local SGD for Heterogeneous Distributed Learning", "abstract": "We analyze Local SGD (aka parallel or federated SGD) and Minibatch SGD in the heterogeneous distributed setting, where each machine has access to stochastic gradient estimates for a different, machine-specific, convex objective; the goal is to optimize w.r.t. the average objective; and machines can only communicate intermittently. We argue that, (i) Minibatch SGD (even without acceleration) dominates all existing analysis of Local SGD in this setting, (ii) accelerated Minibatch SGD is optimal when the heterogeneity is high, and (iii) present the first upper bound for Local SGD that improves over Minibatch SGD in a non-homogeneous regime."}}
