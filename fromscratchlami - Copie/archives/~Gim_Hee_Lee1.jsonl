{"id": "Lre0kF1I_D", "cdate": 1698715549674, "mdate": 1698715549674, "content": {"title": "Generalized Few-Shot Point Cloud Segmentation Via Geometric Words", "abstract": "Existing fully-supervised point cloud segmentation methods suffer in the dynamic testing environment with emerging new classes. Few-shot point cloud segmentation algorithms address this problem by learning to adapt to new classes at the sacrifice of segmentation accuracy for the base classes, which severely impedes its practicality. This largely motivates us to present the first attempt at a more practical paradigm of generalized few-shot point cloud segmentation, which requires the model to generalize to new categories with only a few support point clouds and simultaneously retain the capability to segment base classes. We propose the geometric words to represent geometric components shared between the base and novel classes, and incorporate them into a novel geometric-aware semantic representation to facilitate better generalization to the new classes without forgetting the old ones. Moreover, we introduce geometric prototypes to guide the segmentation with geometric prior knowledge. Extensive experiments on S3DIS and ScanNet consistently illustrate the superior performance of our method over baseline methods."}}
{"id": "9HDWm1TCdem", "cdate": 1683882904314, "mdate": 1683882904314, "content": {"title": "ContraNeRF: Generalizable Neural Radiance Fields for Synthetic-to-real Novel View Synthesis via Contrastive Learning", "abstract": "Although many recent works have investigated generalizable NeRF-based novel view synthesis for unseen scenes, they seldom consider the synthetic-to-real generalization, which is desired in many practical applications. In this work, we first investigate the effects of synthetic data in synthetic-to-real novel view synthesis and surprisingly observe that models trained with synthetic data tend to produce sharper but less accurate volume densities. For pixels where the volume densities are correct, fine-grained details will be obtained. Otherwise, severe artifacts will be produced. To maintain the advantages of using synthetic data while avoiding its negative effects, we propose to introduce geometry-aware contrastive learning to learn multi-view consistent features with geometric constraints. Meanwhile, we adopt cross-view attention to further enhance the geometry perception of features by querying features across input views. Experiments demonstrate that under the synthetic-to-real setting, our method can render images with higher quality and better fine-grained details, outperforming existing generalizable novel view synthesis methods in terms of PSNR, SSIM, and LPIPS. When trained on real data, our method also achieves state-of-the-art results. https://haoy945.github.io/contranerf/"}}
{"id": "HHjx_GRWkVx", "cdate": 1681593856175, "mdate": 1681593856175, "content": {"title": "SESS: Self-Ensembling Semi-Supervised 3D Object Detection", "abstract": "The performance of existing point cloud-based 3D object detection methods heavily relies on large-scale high-quality 3D annotations. However, such annotations are often tedious and expensive to collect. Semi-supervised learning is a good alternative to mitigate the data annotation issue, but has remained largely unexplored in 3D object detection. Inspired by the recent success of self-ensembling technique in semi-supervised image classification task, we propose SESS, a self-ensembling semi-supervised 3D object detection framework. Specifically, we design a thorough perturbation scheme to enhance generalization of the network on unlabeled and new unseen data. Furthermore, we propose three consistency losses to enforce the consistency between two sets of predicted 3D object proposals, to facilitate the learning of structure and semantic invariances of objects. Extensive experiments conducted on SUN RGB-D and ScanNet datasets demonstrate the effectiveness of SESS in both inductive and transductive semi-supervised 3D object detection. Our SESS achieves competitive performance compared to the state-of-the-art fully-supervised method by using only 50% labeled data. "}}
{"id": "_VQl4_lArq", "cdate": 1668765099948, "mdate": 1668765099948, "content": {"title": "CVM-Net: Cross-View Matching Network for Image-Based Ground-to-Aerial Geo-Localization", "abstract": "\nThe problem of localization on a geo-referenced aerial/satellite map given a query ground view image re- mains challenging due to the drastic change in viewpoint that causes traditional image descriptors based matching to fail. We leverage on the recent success of deep learn- ing to propose the CVM-Net for the cross-view image-based ground-to-aerial geo-localization task. Specifically, our network is based on the Siamese architecture to do metric learning for the matching task. We first use the fully con- volutional layers to extract local image features, which are then encoded into global image descriptors using the pow- erful NetVLAD. As part of the training procedure, we also introduce a simple yet effective weighted soft margin rank- ing loss function that not only speeds up the training con- vergence but also improves the final matching accuracy. Ex- perimental results show that our proposed network signifi- cantly outperforms the state-of-the-art approaches on two existing benchmarking datasets. Our code and models are publicly available on the project website1."}}
{"id": "66lwEpqBZA", "cdate": 1668595437114, "mdate": 1668595437114, "content": {"title": "Rethinking IoU-based Optimization for Single-stage 3D Object Detection", "abstract": "Since Intersection-over-Union (IoU) based optimization maintains the consistency of the final IoU prediction metric and losses, it has been widely used in both regression and classification branches of single-stage 2D object detectors. Recently, several 3D object detection methods adopt IoU-based optimization and directly replace the 2D IoU with 3D IoU. However, such a direct computation in 3D is very costly due to the complex implementation and inefficient backward operations. Moreover, 3D IoU-based optimization is sub-optimal as it is sensitive to rotation and thus can cause training instability and detection performance deterioration. In this paper, we propose a novel Rotation-Decoupled IoU (RDIoU) method that can mitigate the rotation-sensitivity issue, and produce more efficient optimization objectives compared with 3D IoU during the training stage. Specifically, our RDIoU simplifies the complex interactions of regression parameters by decoupling the rotation variable as an independent term, yet preserving the geometry of 3D IoU. By incorporating RDIoU into both the regression and classification branches, the network is encouraged to learn more precise bounding boxes and concurrently overcome the misalignment issue between classification and regression. Extensive experiments on the benchmark KITTI and Waymo Open Dataset validate that our RDIoU method can bring substantial improvement for the single-stage 3D object detection."}}
{"id": "djwUhRnYSaE", "cdate": 1667356281814, "mdate": 1667356281814, "content": {"title": "Convolutional Sequence to Sequence Model for Human Dynamics", "abstract": "Human motion modeling is a classic problem in com-\nputer vision and graphics. Challenges in modeling human\nmotion include high dimensional prediction as well as ex-\ntremely complicated dynamics.We present a novel approach\nto human motion modeling based on convolutional neural\nnetworks (CNN). The hierarchical structure of CNN makes\nit capable of capturing both spatial and temporal correla-\ntions effectively. In our proposed approach, a convolutional\nlong-term encoder is used to encode the whole given motion\nsequence into a long-term hidden variable, which is used\nwith a decoder to predict the remainder of the sequence.\nThe decoder itself also has an encoder-decoder structure, in\nwhich the short-term encoder encodes a shorter sequence to\na short-term hidden variable, and the spatial decoder maps\nthe long and short-term hidden variable to motion predic-\ntions. By using such a model, we are able to capture both\ninvariant and dynamic information of human motion, which\nresults in more accurate predictions. Experiments show that\nour algorithm outperforms the state-of-the-art methods on\nthe Human3.6M and CMU Motion Capture datasets. Our\ncode is available at the project website"}}
{"id": "-c4rqPHc8l", "cdate": 1667355980509, "mdate": 1667355980509, "content": {"title": "Weakly Supervised Generative Network for Multiple 3D Human Pose Hypotheses", "abstract": "3D human pose estimation from a monocular image or\n2D joints is an ill-posed problem because of depth ambi-\nguity and occluded joints. We argue that 3D human pose\nestimation from a monocular input is an inverse problem\nwhere multiple feasible solutions can exist. In this paper,\nwe propose a novel approach to generate multiple feasible\nhypotheses of the 3D pose from 2D joints. In contrast to\nexisting deep learning approaches which minimize a mean\nsquare error based on an unimodal Gaussian distribution,\nour method is able to generate multiple feasible hypothe-\nses of 3D pose based on a multimodal mixture density net-\nworks. Our experiments show that the 3D poses estimated\nby our approach from an input of 2D joints are consistent\nin 2D reprojections, which supports our argument that mul-\ntiple solutions exist for the 2D-to-3D inverse problem. Fur-\nthermore, we show state-of-the-art performance on the Hu-\nman3.6M dataset in both best hypothesis and multi-view\nsettings, and we demonstrate the generalization capacity\nof our model by testing on the MPII and MPI-INF-3DHP\ndatasets. Our code is available at the project website."}}
{"id": "C_eJSunNhoZ", "cdate": 1667355854554, "mdate": 1667355854554, "content": {"title": "Generating Multiple Hypotheses for 3D Human Pose Estimation with Mixture Density Network", "abstract": "3D human pose estimation from a monocular image or\n2D joints is an ill-posed problem because of depth ambi-\nguity and occluded joints. We argue that 3D human pose\nestimation from a monocular input is an inverse problem\nwhere multiple feasible solutions can exist. In this paper,\nwe propose a novel approach to generate multiple feasible\nhypotheses of the 3D pose from 2D joints. In contrast to\nexisting deep learning approaches which minimize a mean\nsquare error based on an unimodal Gaussian distribution,\nour method is able to generate multiple feasible hypothe-\nses of 3D pose based on a multimodal mixture density net-\nworks. Our experiments show that the 3D poses estimated\nby our approach from an input of 2D joints are consistent\nin 2D reprojections, which supports our argument that mul-\ntiple solutions exist for the 2D-to-3D inverse problem. Fur-\nthermore, we show state-of-the-art performance on the Hu-\nman3.6M dataset in both best hypothesis and multi-view\nsettings, and we demonstrate the generalization capacity\nof our model by testing on the MPII and MPI-INF-3DHP\ndatasets. Our code is available at the project website."}}
{"id": "t4_l1ToqFi", "cdate": 1667355733115, "mdate": 1667355733115, "content": {"title": "From Synthetic to Real: Unsupervised Domain Adaptation for Animal Pose Estimation", "abstract": "Animal pose estimation is an important field that has re-\nceived increasing attention in the recent years. The main\nchallenge for this task is the lack of labeled data. Exist-\ning works circumvent this problem with pseudo labels gen-\nerated from data of other easily accessible domains such\nas synthetic data. However, these pseudo labels are noisy\neven with consistency check or confidence-based filtering\ndue to the domain shift in the data. To solve this prob-\nlem, we design a multi-scale domain adaptation module\n(MDAM) to reduce the domain gap between the synthetic\nand real data. We further introduce an online coarse-to-\nfine pseudo label updating strategy. Specifically, we pro-\npose a self-distillation module in an inner coarse-update\nloop and a mean-teacher in an outer fine-update loop to\ngenerate new pseudo labels that gradually replace the old\nones. Consequently, our model is able to learn from the\nold pseudo labels at the early stage, and gradually switch\nto the new pseudo labels to prevent overfitting in the later\nstage. We evaluate our approach on the TigDog and VisDA\n2019 datasets, where we outperform existing approaches by\na large margin. We also demonstrate the generalization\nability of our model by testing extensively on both unseen\ndomains and unseen animal categories. Our code is avail-\nable at the project website."}}
{"id": "jpWa2RnZpIK", "cdate": 1663850220629, "mdate": null, "content": {"title": "MaskNeRF: Masked Neural Radiance Fields for Sparse View Synthesis", "abstract": "Although Neural Radiance Fields (NeRF) has achieved impressive 3D reconstruction with dense view images, its performance degrades significantly when the training views are sparse. We observe that under the sparse view setting, it is important to learn the correspondence of pixels among different views, i.e., the 3D consistency, to improve the reconstruction quality. To achieve this, we first propose the Hard-Mask that utilizes the depth information to locate pixels with correspondence relationship and then assigns higher loss weights on these pixels. The key idea is to achieve pixel-wise differentiated optimization of NeRF based on the 3D consistency among target views and source views instead of treating each pixel equally. This optimization strategy helps NeRF-based algorithms to learn fine-grained object details with limited data. To deal with the absence of accurate depth information, the Soft-Mask is proposed to estimate the correspondence relationship based on the trend of training losses. Our proposed method can serve as a plug-in component for existing NeRF-based view-synthesis models. Extensive experiments on recent representative works, including NeRF, IBRNet and MVSNeRF, show that our method can significantly improve the model performance under sparse view conditions (e.g., up to 70\\% improvement in PSNR on DTU dataset). "}}
