{"id": "yt83n50dMSY", "cdate": 1696388095229, "mdate": 1696388095229, "content": {"title": "AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?", "abstract": "Can we better anticipate an actor\u2019s future actions (e.g. mix eggs) by knowing what commonly happens after his/her current action (e.g. crack eggs)? What if we also know the longer-term goal of the actor (e.g. making egg fried rice)? The long-term action anticipation (LTA) task aims to predict an actor\u2019s future behavior from video observations in the form of verb and noun sequences, and it is crucial for human-machine interaction. We propose to formulate the LTA task from two perspectives: a bottom-up approach that predicts the next actions autoregressively by modeling temporal dynamics; and a top-down approach that infers the goal of the actor and plans the needed procedure to accomplish the goal. We hypothesize that large language models (LLMs), which have been pretrained on procedure text data (e.g. recipes, how-tos), have the potential to help LTA from both perspectives. It can help provide the prior knowledge on the possible next actions, and infer the goal given the observed part of a procedure, respectively. To leverage the LLMs, we propose a two-stage framework, AntGPT. It first recognizes the actions already performed in the observed videos and then asks an LLM to predict the future actions via conditioned generation, or to infer the goal and plan the whole procedure by chain-of-thought prompting. Empirical results on the Ego4D LTA v1 and v2 benchmarks, EPIC-Kitchens-55, as well as EGTEA GAZE+ demonstrate the effectiveness of our proposed approach. AntGPT achieves state-of-the-art performance on all above benchmarks, and can successfully infer the goal and thus perform goal-conditioned \u201ccounterfactual\u201d prediction via qualitative analysis."}}
{"id": "ZLV97qcAokA", "cdate": 1668794406305, "mdate": 1668794406305, "content": {"title": "HDMapGen: A Hierarchical Graph Generative Model of High Definition Maps", "abstract": "High Definition (HD) maps are maps with precise definitions of road lanes with rich semantics of the traffic rules. They are critical for several key stages in an autonomous driving system, including motion forecasting and planning. However, there are only a small amount of real-world road topologies and geometries, which significantly limits our ability to test out the self-driving stack to generalize onto new unseen scenarios. To address this issue, we introduce a new challenging task to generate HD maps.\nIn this work, we explore several autoregressive models using different data representations, including sequence, plain graph, and hierarchical graph. We propose HDMapGen, a hierarchical graph generation model capable of producing high-quality and diverse HD maps through a coarse-to-fine approach. Experiments on the Argoverse dataset and an in-house dataset show that HDMapGen significantly outperforms baseline methods. Additionally, we demonstrate that HDMapGen achieves high scalability and efficiency."}}
{"id": "V8isglQkt74", "cdate": 1663850523996, "mdate": null, "content": {"title": "Towards Learning Implicit Symbolic Representation for Visual Reasoning", "abstract": "Visual reasoning tasks are designed to test a learning algorithm's capability to infer causal relationships, discover object interactions, and understand temporal dynamics, all from visual cues. It is commonly believed that to achieve compositional generalization on visual reasoning, an explicit abstraction of the visual scene must be constructed; for example, object detection can be applied to the visual input to produce representations that are then processed by a neural network or a neuro-symbolic framework. We demonstrate that a simple and general self-supervised approach is able to learn implicit symbolic representations with general-purpose neural networks, enabling the end-to-end learning of visual reasoning directly from raw visual inputs. Our proposed approach ``compresses'' each frame of a video into a small set of tokens with a transformer network. The self-supervised learning objective is to reconstruct each image based on the compressed temporal context. To minimize the reconstruction loss, the network must learn a compact representation for each image, as well as capture temporal dynamics and object permanence from temporal context. We evaluate the proposed approach on two visual reasoning benchmarks, CATER and ACRE. We observe that self-supervised pretraining is essential to achieve compositional generalization for our end-to-end trained neural network, and our proposed method achieves on par or better performance compared to recent neuro-symbolic approaches that often require additional object-level supervision."}}
{"id": "uARGOm09Vnr", "cdate": 1663850513244, "mdate": null, "content": {"title": "Towards A Unified Neural Architecture for Visual Recognition and Reasoning", "abstract": "Recognition and reasoning are two pillars of visual understanding.  However, these tasks have an imbalance in focus; whereas recent advances in neural networks have shown strong empirical performance in visual recognition, there has been comparably much less success in solving visual reasoning.  Intuitively, unifying these two tasks under a singular framework is desirable, as they are mutually dependent and beneficial.  Motivated by the recent success of multi-task transformers for visual recognition and language understanding, we propose a unified neural architecture for visual recognition and reasoning tasks with a generic interface (e.g., tokens) for all tasks.  Our framework enables the principled investigation of how different visual recognition tasks, datasets, and inductive biases can help enable spatiotemporal reasoning capabilities.  Noticeably, we find that object detection, which requires spatial localization of individual objects, is the most beneficial recognition task for reasoning.  We further demonstrate via probing that implicit object-centric representations emerge automatically inside our framework.  We also discover that visual reasoning and object detection respond to drastically different model components; certain architectural choices such as the backbone model of the visual encoder have a significant impact on visual reasoning, but little on object detection.  Given the results of our experiments, we believe that a fruitful direction forward is to consider visual reasoning a first-class citizen alongside visual recognition, as they are strongly correlated but benefit from potentially different design choices."}}
{"id": "NFzHAognkpQ", "cdate": 1663850476575, "mdate": null, "content": {"title": "Steerable Equivariant Representation Learning", "abstract": "Pre-trained deep image representations are useful for post-training tasks such as classification through transfer learning, image retrieval, and object detection. Data augmentations are a crucial aspect of pre-training robust representations in both supervised and self-supervised settings. Data augmentations explicitly or implicitly promote \\emph{invariance} in the embedding space to the input image transformations. This invariance reduces generalization to those downstream tasks which rely on sensitivity to these particular data augmentations. In this paper, we propose a method of learning representations that are instead \\emph{equivariant} to data augmentations. We achieve this equivariance through the use of \\emph{steerable} representations. Our representations can be manipulated directly in embedding space via learned linear maps. We demonstrate that our resulting steerable and equivariant representations lead to better performance on transfer learning and robustness: e.g. we improve linear probe top-1 accuracy by between 1\\% to 3\\% for transfer; and ImageNet-C accuracy by upto 3.4\\%. We further show that the steerability of our representations provides significant speedup (nearly $50\\times$) for test-time augmentations; by applying a large number of augmentations for out-of-distribution detection, we significantly improve OOD AUC on the ImageNet-C dataset over an invariant representation."}}
{"id": "E4-uRvmKkeB", "cdate": 1663850033173, "mdate": null, "content": {"title": "Beyond Traditional Transfer Learning: Co-finetuning for Action Localisation", "abstract": "Transfer learning is the predominant paradigm for training deep networks on small target datasets. Models are typically pretrained on large \u201cupstream\u201d datasets for classification, as such labels are easy to collect, and then finetuned on downstream\u201d tasks such as action localisation, which are smaller due to their finer-grained annotations.\n\nIn this paper, we question this approach, and propose co-finetuning -- simultaneously training a single model on multiple \u201cupstream\u201d and \u201cdownstream\u201d tasks. We demonstrate that co-finetuning outperforms traditional transfer learning when using the same total amount of data, and also show how we can easily extend our approach to multiple \u201cupstream\u201d datasets to further improve performance. In particular, co-finetuning significantly improves the performance on rare classes in our downstream task, as it has a regularising effect, and enables the network to learn feature representations that transfer between different datasets. Finally, we observe how co-finetuning with public, video classification datasets, we are able to achieve state-of-the-art results for spatio-temporal action localisation on the challenging AVA and AVA-Kinetics datasets, outperforming recent works which develop intricate models."}}
{"id": "0n4owcW1eBL", "cdate": 1663769612631, "mdate": 1663769612631, "content": {"title": "Learning Audio-Video Modalities from Image Captions", "abstract": "A major challenge in text-video and text-audio retrieval is the lack of large-scale training data. This is unlike image-captioning, where datasets are in the order of millions of samples. To close this gap we propose a new video mining pipeline which involves transferring captions from image captioning datasets to video clips with no additional manual effort. Using this pipeline, we create a new large-scale, weakly labelled audio-video captioning dataset consisting of millions of paired clips and captions. We show that training a multimodal transformed based model on this data achieves competitive performance on video retrieval and video captioning, matching or even outperforming HowTo100M pretraining with 20x fewer clips. We also show that our mined clips are suitable for text-audio pretraining, and achieve state of the art results for the task of audio retrieval."}}
{"id": "Bhzlu_XsHgq", "cdate": 1645749103540, "mdate": null, "content": {"title": "Multiview Transformers for Video Recognition", "abstract": "Video understanding requires reasoning at multiple spatiotemporal resolutions -- from short fine-grained motions to events taking place over longer durations. Although transformer architectures have recently advanced the state-of-the-art, they have not explicitly modelled different spatiotemporal resolutions. To this end, we present Multiview Transformers for Video Recognition (MTV). Our model consists of separate encoders to represent different views of the input video with lateral connections to fuse information across views. We present thorough ablation studies of our model and show that MTV consistently performs better than single-view counterparts in terms of accuracy and computational cost across a range of model sizes. Furthermore, we achieve state-of-the-art results on five standard datasets, and improve even further with large-scale pretraining. We will release code and pretrained checkpoints."}}
{"id": "fKsSAZ5Kj5", "cdate": 1640995200000, "mdate": 1668001334730, "content": {"title": "Masking Modalities for Cross-modal Video Retrieval", "abstract": "Pre-training on large scale unlabelled datasets has shown impressive performance improvements in the fields of computer vision and natural language processing. Given the advent of large-scale instructional video datasets, a common strategy for pre-training video encoders is to use the accompanying speech as weak supervision. However, as speech is used to supervise the pre-training, it is never seen by the video encoder, which does not learn to process that modality. We address this drawback of current pre-training methods, which fail to exploit the rich cues in spoken language. Our proposal is to pre-train a video encoder using all the available video modalities as supervision, namely, appearance, sound, and transcribed speech. We mask an entire modality in the input and predict it using the other two modalities. This encourages each modality to collaborate with the others, and our video encoder learns to process appearance and audio as well as speech. We show the superior performance of our \u2018modality masking\u2019 pre-training approach for video retrieval on the How2R, YouCook2 and Condensed Movies datasets."}}
{"id": "clSXjLZzQdd", "cdate": 1640995200000, "mdate": 1668001335191, "content": {"title": "TL;DW? Summarizing Instructional Videos with Task Relevance and Cross-Modal Saliency", "abstract": ""}}
