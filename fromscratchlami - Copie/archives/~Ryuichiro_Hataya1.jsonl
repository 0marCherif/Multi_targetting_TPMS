{"id": "xiDwfjLZgJ", "cdate": 1672531200000, "mdate": 1696156637488, "content": {"title": "Noncommutative C*-algebra Net: Learning Neural Networks with Powerful Product Structure in C*-algebra", "abstract": "We propose a new generalization of neural networks with noncommutative $C^*$-algebra. An important feature of $C^*$-algebras is their noncommutative structure of products, but the existing $C^*$-algebra net frameworks have only considered commutative $C^*$-algebras. We show that this noncommutative structure of $C^*$-algebras induces powerful effects in learning neural networks. Our framework has a wide range of applications, such as learning multiple related neural networks simultaneously with interactions and learning invariant features with respect to group actions. We also show the validity of our framework numerically, which illustrates its potential power."}}
{"id": "TSmpAeMWc0", "cdate": 1672531200000, "mdate": 1696156637458, "content": {"title": "Sketch-based Medical Image Retrieval", "abstract": "The amount of medical images stored in hospitals is increasing faster than ever; however, utilizing the accumulated medical images has been limited. This is because existing content-based medical image retrieval (CBMIR) systems usually require example images to construct query vectors; nevertheless, example images cannot always be prepared. Besides, there can be images with rare characteristics that make it difficult to find similar example images, which we call isolated samples. Here, we introduce a novel sketch-based medical image retrieval (SBMIR) system that enables users to find images of interest without example images. The key idea lies in feature decomposition of medical images, whereby the entire feature of a medical image can be decomposed into and reconstructed from normal and abnormal features. By extending this idea, our SBMIR system provides an easy-to-use two-step graphical user interface: users first select a template image to specify a normal feature and then draw a semantic sketch of the disease on the template image to represent an abnormal feature. Subsequently, it integrates the two kinds of input to construct a query vector and retrieves reference images with the closest reference vectors. Using two datasets, ten healthcare professionals with various clinical backgrounds participated in the user test for evaluation. As a result, our SBMIR system enabled users to overcome previous challenges, including image retrieval based on fine-grained image characteristics, image retrieval without example images, and image retrieval for isolated samples. Our SBMIR system achieves flexible medical image retrieval on demand, thereby expanding the utility of medical image databases."}}
{"id": "QqSW8JHnsja", "cdate": 1672531200000, "mdate": 1696156637459, "content": {"title": "MNISQ: A Large-Scale Quantum Circuit Dataset for Machine Learning on/for Quantum Computers in the NISQ era", "abstract": "We introduce the first large-scale dataset, MNISQ, for both the Quantum and the Classical Machine Learning community during the Noisy Intermediate-Scale Quantum era. MNISQ consists of 4,950,000 data points organized in 9 subdatasets. Building our dataset from the quantum encoding of classical information (e.g., MNIST dataset), we deliver a dataset in a dual form: in quantum form, as circuits, and in classical form, as quantum circuit descriptions (quantum programming language, QASM). In fact, also the Machine Learning research related to quantum computers undertakes a dual challenge: enhancing machine learning exploiting the power of quantum computers, while also leveraging state-of-the-art classical machine learning methodologies to help the advancement of quantum computing. Therefore, we perform circuit classification on our dataset, tackling the task with both quantum and classical models. In the quantum endeavor, we test our circuit dataset with Quantum Kernel methods, and we show excellent results up to $97\\%$ accuracy. In the classical world, the underlying quantum mechanical structures within the quantum circuit data are not trivial. Nevertheless, we test our dataset on three classical models: Structured State Space sequence model (S4), Transformer and LSTM. In particular, the S4 model applied on the tokenized QASM sequences reaches an impressive $77\\%$ accuracy. These findings illustrate that quantum circuit-related datasets are likely to be quantum advantageous, but also that state-of-the-art machine learning methodologies can competently classify and recognize quantum circuits. We finally entrust the quantum and classical machine learning community the fundamental challenge to build more quantum-classical datasets like ours and to build future benchmarks from our experiments. The dataset is accessible on GitHub and its circuits are easily run in qulacs or qiskit."}}
{"id": "MR8V4o4X1e-", "cdate": 1672531200000, "mdate": 1696156637458, "content": {"title": "An Empirical Investigation of Pre-trained Model Selection for Out-of-Distribution Generalization and Calibration", "abstract": "In the realm of out-of-distribution generalization tasks, finetuning has risen as a key strategy. While the most focus has been on optimizing learning algorithms, our research highlights the influence of pre-trained model selection in finetuning on out-of-distribution performance and inference uncertainty. Balancing model size constraints of a single GPU, we examined the impact of varying pre-trained datasets and model parameters on performance metrics like accuracy and expected calibration error. Our findings underscore the significant influence of pre-trained model selection, showing marked performance improvements over algorithm choice. Larger models outperformed others, though the balance between memorization and true generalization merits further investigation. Ultimately, our research emphasizes the importance of pre-trained model selection for enhancing out-of-distribution generalization."}}
{"id": "2rEhJh2LEOv", "cdate": 1672531200000, "mdate": 1696156637457, "content": {"title": "Nystr\u00f6m Method for Accurate and Scalable Implicit Differentiation", "abstract": "The essential difficulty of gradient-based bilevel optimization using implicit differentiation is to estimate the inverse Hessian vector product with respect to neural network parameters. This pape..."}}
{"id": "W8iK1oESv8", "cdate": 1640995200000, "mdate": 1668507875643, "content": {"title": "Meta Approach to Data Augmentation Optimization", "abstract": "Data augmentation policies drastically improve the performance of image recognition tasks, especially when the policies are optimized for the target data and tasks. In this paper, we propose to optimize image recognition models and data augmentation policies simultaneously to improve the performance using gradient descent. Unlike prior methods, our approach avoids using proxy tasks or reducing search space, and can directly improve the validation performance. Our method achieves efficient and scalable training by approximating the gradient of policies by implicit gradient with Neumann series approximation. We demonstrate that our approach can improve the performance of various image classification tasks, including fine-grained image recognition, without using dataset-specific hyperparameter tuning."}}
{"id": "Ucei4dMQhO", "cdate": 1640995200000, "mdate": 1674544627510, "content": {"title": "Will Large-scale Generative Models Corrupt Future Datasets?", "abstract": "Recently proposed large-scale text-to-image generative models such as DALL$\\cdot$E 2, Midjourney, and StableDiffusion can generate high-quality and realistic images from users' prompts. Not limited to the research community, ordinary Internet users enjoy these generative models, and consequently a tremendous amount of generated images have been shared on the Internet. Meanwhile, today's success of deep learning in the computer vision field owes a lot to images collected from the Internet. These trends lead us to a research question: \"will such generated images impact the quality of future datasets and the performance of computer vision models positively or negatively?\" This paper empirically answers this question by simulating contamination. Namely, we generate ImageNet-scale and COCO-scale datasets using a state-of-the-art generative model and evaluate models trained on ``contaminated'' datasets on various tasks including image classification and image generation. Throughout experiments, we conclude that generated images negatively affect downstream performance, while the significance depends on tasks and the amount of generated images. The generated datasets are available via https://github.com/moskomule/dataset-contamination."}}
{"id": "9e4hd-2Ujx", "cdate": 1640995200000, "mdate": 1668507875645, "content": {"title": "DJMix: Unsupervised Task-agnostic Image Augmentation for Improving Robustness of Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (CNNs) are vulnerable to unseen test-time noise on input images, such as defocus blur or JPEG-compression artifacts. Improving the robustness to such noise is important for real-world applications. In this paper, we propose DJMix, a training method for CNNs to obtain identical representations to a given image and its discretized one. As a result, CNNs trained with DJMix can ignore unnecessary details of inputs and become robust to input noise. We verify the effectiveness of our method on several datasets of various tasks, namely, classification, semantic segmentation, and object detection using clean and noisy test images."}}
{"id": "80GQMJCj5oD", "cdate": 1632875619783, "mdate": null, "content": {"title": "Gradient-based Hyperparameter Optimization without Validation Data for Learning fom Limited Labels", "abstract": "Optimizing hyperparameters of machine learning algorithms especially for limited labeled data is important but difficult, because then obtaining enough validation data is practically impossible. Bayesian model selection enables hyperparameter optimization \\emph{without validation data}, but it requires Hessian log determinants, which is computationally demanding for deep neural networks. We study methods to efficiently approximate Hessian log determinants and empirically demonstrate that approximated Bayesian model selection can effectively tune hyperparameters of algorithms of deep semi-supervised learning and learning from noisy labels."}}
{"id": "I2AD-xWJ2-J", "cdate": 1614361132031, "mdate": null, "content": {"title": "Graph Energy-based Model for Molecular Graph Generation", "abstract": "We present Graph Energy-based Model (GEM), an energy-based model for molecular graph generation. GEM uses dequantization and gradient symmetrization to incorporate generation by stochastic gradient Langevin dynamics for graph representation that is discrete and includes symmetric constraint. Experimental results show that \\gem can comparably design compounds as other deep generative approaches."}}
