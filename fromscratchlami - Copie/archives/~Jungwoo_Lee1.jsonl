{"id": "XgJWwOcODXC", "cdate": 1677628800000, "mdate": 1682350311223, "content": {"title": "Straggler-Exploiting Fully Private Distributed Matrix Multiplication With Chebyshev Polynomials", "abstract": "In this paper, we consider coded computation for matrix multiplication tasks in distributed computing to mitigate straggler effects. We assume that the stragglers\u2019 computation results can be leveraged at the master by assigning multiple sub-tasks to the workers. We propose a new coded computation scheme, namely Chebyshev coded fully private matrix multiplication (CFP), to preserve the privacy of a master in a scenario where a master wants to obtain a matrix multiplication result from the libraries which are shared by the workers, while concealing both of the two indices of the desired matrices from each worker. The key idea of CFP is to introduce Chebyshev polynomials, which have commutative property, in queries sent to workers to allocate sub-tasks. We also extend CFP to keep the privacy of a master from colluding workers. In conclusion, we show that CFP can preserve the privacy of a master from each worker and efficiently mitigate straggler effects compared to existing schemes."}}
{"id": "HaFRdl9A5Y", "cdate": 1676827099429, "mdate": null, "content": {"title": "On the Convergence of Continual Learning with Adaptive Methods", "abstract": "One of the objectives of continual learning is to prevent catastrophic forgetting in learning multiple tasks sequentially, and the existing solutions have been driven by the conceptualization of the plasticity-stability dilemma. However, the convergence of continual learning for each sequential task is less studied so far. In this paper, we provide a convergence analysis of memory-based continual learning with stochastic gradient descent and empirical evidence that training current tasks causes the cumulative degradation of previous tasks.\nWe propose an adaptive method for nonconvex continual learning (NCCL), which adjusts step sizes of both previous and current tasks with the gradients. The proposed method can achieve the same convergence rate as the SGD method when the catastrophic forgetting term which we define in the paper is suppressed at each iteration. Further, we demonstrate that the proposed algorithm improves the performance of continual learning over existing methods for several image classification tasks."}}
{"id": "oar2wBIn9", "cdate": 1672531200000, "mdate": 1682350309230, "content": {"title": "Bridging the Gap between Model Explanations in Partially Annotated Multi-label Classification", "abstract": "Due to the expensive costs of collecting labels in multi-label classification datasets, partially annotated multi-label classification has become an emerging field in computer vision. One baseline approach to this task is to assume unobserved labels as negative labels, but this assumption induces label noise as a form of false negative. To understand the negative impact caused by false negative labels, we study how these labels affect the model's explanation. We observe that the explanation of two models, trained with full and partial labels each, highlights similar regions but with different scaling, where the latter tends to have lower attribution scores. Based on these findings, we propose to boost the attribution scores of the model trained with partial labels to make its explanation resemble that of the model trained with full labels. Even with the conceptually simple approach, the multi-label classification performance improves by a large margin in three different datasets on a single positive label setting and one on a large-scale partial label setting. Code is available at https://github.com/youngwk/BridgeGapExplanationPAMC."}}
{"id": "Dlh4MnWlpb", "cdate": 1672531200000, "mdate": 1682350310051, "content": {"title": "Distributional Prototypical Methods for Reliable Explanation Space Construction", "abstract": "As deep learning has been successfully deployed in diverse applications, there is an ever increasing need to explain its decision. To explain decisions, case-based reasoning has proved to be effective in many areas. The prototype-based explanation is a method that provides an explanation of the model\u2019s prediction using the distance between an input and learned prototypes to effectively perform case-based reasoning. However, existing methods are less reliable because distance is not always consistent with human perception. In this study, we construct a latent space which we call an <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$explanation space$ </tex-math></inline-formula> with distributional embedding and latent space regularization. This explanation space ensures that similar (in terms of human-interpretable features) images share similar latent representations, and therefore provides a reliable explanation for the consistency between distance-based explanation and human perception. The explanation space also provides additional explanation by transition, allowing the user to understand the factors that affect the distance. Throughout extensive experiments including human evaluation, we have shown that the explanation space provides a more human-understandable explanation."}}
{"id": "-WXCYvc5E-P", "cdate": 1665251222885, "mdate": null, "content": {"title": "Perturbed Quantile Regression for Distributional Reinforcement Learning", "abstract": "Distributional reinforcement learning aims to learn distribution of return under stochastic environments. Since the learned distribution of return contains rich information about the stochasticity of the environment, previous studies have relied on descriptive statistics, such as standard deviation, for optimism in the face of uncertainty. However, using the uncertainty from an empirical distribution can hinder convergence and performance when exploring with the certain criterion that has an one-sided tendency on risk in these methods. In this paper, we propose a novel distributional reinforcement learning that explores by randomizing risk criterion to reach a risk-neutral optimal policy. First, we provide a perturbed distributional Bellman optimality operator by distorting the risk measure in action selection. Second, we prove the convergence and optimality of the proposed method by using the weaker contraction property. Our theoretical results support that the proposed method does not fall into biased exploration and is guaranteed to converge to an optimal return distribution. Finally, we empirically show that our method outperforms other existing distribution-based algorithms in various environments including 55 Atari games."}}
{"id": "j9Na6QMTCE4", "cdate": 1664731443804, "mdate": null, "content": {"title": "Adaptive Methods for Nonconvex Continual Learning", "abstract": "One of the objectives of continual learning is to prevent catastrophic forgetting in learning multiple tasks sequentially,\nand the existing solutions have been driven by the conceptualization of the plasticity-stability dilemma.\nHowever, the convergence of continual learning for each sequential task is less studied so far.\nIn this paper, we provide a convergence analysis of memory-based continual learning with stochastic gradient descent\nand empirical evidence that training current tasks causes the cumulative degradation of previous tasks.\nWe propose an adaptive method for nonconvex continual learning (NCCL), which adjusts step sizes of both previous and current tasks with the gradients.\nThe proposed method can achieve the same convergence rate as the SGD method when the catastrophic forgetting term which we define in the paper is suppressed at each iteration.\nFurther, we demonstrate that the proposed algorithm improves the performance of continual learning over existing methods for several image classification tasks. "}}
{"id": "N6NO4o_b5r", "cdate": 1663850266251, "mdate": null, "content": {"title": "Group-wise Verifiable Distributed Computing for Machine Learning under Adversarial Attacks", "abstract": "Distributed computing has been a promising solution in machine learning to accelerate the training procedure on large-scale dataset by utilizing multiple workers in parallel. However, there remain two major issues that still need to be addressed: i) adversarial attacks from malicious workers, and ii) the effect of slow workers known as stragglers. In this paper, we tackle both problems simultaneously by proposing Group-wise Verifiable Coded Computing (GVCC), which leverages coding techniques and group-wise verification to provide robustness to adversarial attacks and resiliency to straggler effects in distributed computing. The key idea of GVCC is to verify a group of computation results from workers at a time, while providing resilience to stragglers through encoding tasks assigned to workers with Group-wise Verifiable Codes. Experimental results show that GVCC outperforms the existing methods in terms of overall processing time and verification time for executing matrix multiplication, which is a key computational component in machine learning and deep learning. "}}
{"id": "qmfulB2xIF", "cdate": 1640995200000, "mdate": 1667337093855, "content": {"title": "Large Loss Matters in Weakly Supervised Multi-Label Classification", "abstract": "Weakly supervised multi-label classification (WSML) task, which is to learn a multi-label classification using partially observed labels per image, is becoming increasingly important due to its huge annotation cost. In this work, we first regard unobserved labels as negative labels, casting the WSML task into noisy multi-label classification. From this point of view, we empirically observe that memorization effect, which was first discovered in a noisy multi-class setting, also occurs in a multi-label setting. That is, the model first learns the representation of clean labels, and then starts memorizing noisy labels. Based on this finding, we propose novel methods for WSML which reject or correct the large loss samples to prevent model from memorizing the noisy label. Without heavy and complex components, our proposed methods outperform previous state-of-the-art WSML methods on several partial label settings including Pascal VOC 2012, MS COCO, NUSWIDE, CUB, and OpenImages V3 datasets. Various analysis also show that our methodology actually works well, validating that treating large loss properly matters in a weakly supervised multi-label classification. Our code is available at https://github.com/snucml/LargeLossMatters."}}
{"id": "bu1_S1Gzpd", "cdate": 1640995200000, "mdate": 1667526393506, "content": {"title": "Large Loss Matters in Weakly Supervised Multi-Label Classification", "abstract": "Weakly supervised multi-label classification (WSML) task, which is to learn a multi-label classification using partially observed labels per image, is becoming increasingly important due to its huge annotation cost. In this work, we first regard unobserved labels as negative labels, casting the WSML task into noisy multi-label classification. From this point of view, we empirically observe that memorization effect, which was first discovered in a noisy multi-class setting, also occurs in a multi-label setting. That is, the model first learns the representation of clean labels, and then starts memorizing noisy labels. Based on this finding, we propose novel methods for WSML which reject or correct the large loss samples to prevent model from memorizing the noisy label. Without heavy and complex components, our proposed methods outperform previous state-of-the-art WSML methods on several partial label settings including Pascal VOC 2012, MS COCO, NUSWIDE, CUB, and OpenImages V3 datasets. Various analysis also show that our methodology actually works well, validating that treating large loss properly matters in a weakly supervised multi-label classification. Our code is available at https://github.com/snucml/LargeLossMatters."}}
{"id": "WTftm-KmiY", "cdate": 1640995200000, "mdate": 1682350310685, "content": {"title": "Repair Rates for Multiple Descriptions on Distributed Storage", "abstract": "In a traditional distributed storage system, a source can be restored perfectly when a certain subset of servers is contacted. The coding is independent of the contents of the source. This paper considers instead a lossy source coding version of this problem where the more servers that are contacted, the higher the quality of the restored source. An example could be video stored on distributed storage. In information theory, this is called the multiple description problem, where the distortion depends on the number of descriptions received. The problem considered in this paper is how to restore the system operation when one of the servers fail and a new server replaces it, that is, repair. The requirement is that the distortions in the restored system should be no more than in the original system. The question is how many extra bits are needed for repair. We find an achievable rate and show that this is optimal in certain cases. One conclusion is that it is necessary to design the multiple description codes with repair in mind; just using an existing multiple description code results in unnecessary high repair rates."}}
