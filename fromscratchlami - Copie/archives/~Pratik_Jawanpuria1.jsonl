{"id": "qXO-HEBb0b", "cdate": 1675355737503, "mdate": 1675355737503, "content": {"title": "Riemannian accelerated gradient methods via extrapolation", "abstract": "In this paper, we propose a simple acceleration scheme for Riemannian gradient methods by extrapolating iterates on manifolds. We show when the iterates are generated from Rieman- nian gradient descent method, the accelerated scheme achieves the optimal convergence rate asymptotically and is computationally more favorable than the recently proposed Riemannian Nesterov accelerated gradient methods. Our experiments verify the practical benefit of the novel acceleration strategy."}}
{"id": "xz9sWj3HHt9", "cdate": 1675355653123, "mdate": 1675355653123, "content": {"title": "Statistical Optimal Transport posed as Learning Kernel Embedding", "abstract": "The objective in statistical Optimal Transport (OT) is to consistently estimate the optimal transport plan/map solely using samples from the given source and target marginal distributions. This work takes the novel approach of posing statistical OT as that of learning the transport plan\u2019s kernel mean embedding from sample based estimates of marginal embeddings. The proposed estimator controls over- fitting by employing maximum mean discrepancy based regularization, which is complementary to \u03c6-divergence (entropy) based regularization popularly employed in existing estimators. A key result is that, under very mild conditions, \u03b5-optimal recovery of the transport plan as well as the Barycentric-projection based transport map is possible with a sample complexity that is completely dimension-free. Moreover, the implicit smoothing in the kernel mean embeddings enables out-of-sample estimation. An appropriate representer theorem is proved leading to a kernelized convex formulation for the estimator, which can then be potentially used to perform OT even in non-standard domains. Empirical results illustrate the efficacy of the proposed approach."}}
{"id": "WYrrkeK4xHr", "cdate": 1675355546807, "mdate": 1675355546807, "content": {"title": "Efficient robust optimal transport: formulations and algorithms.", "abstract": "Optimal transport (OT) is a powerful geometric tool for comparing two distributions and has been employed in various machine learning applications. In this work, we propose a novel OT formulation that takes feature correlations into account while learning the transport plan between two distributions. We model the feature-feature relationship via a symmetric positive semi-definite Mahalanobis metric in the OT cost function. For a certain class of regularizers on the metric, we show that the optimization strategy can be considerably simplified by exploiting the problem structure. For high-dimensional data, we additionally propose suitable low-dimensional modeling of the Mahalanobis metric. Overall, we view the resulting optimization problem as a non-linear OT problem, which we solve using the Frank-Wolfe algorithm. Empirical results on the discriminative learning setting, such as tag prediction and multi-class classification, illustrate the good performance of our approach.\n"}}
{"id": "M3OoX46DQL", "cdate": 1675355358750, "mdate": 1675355358750, "content": {"title": "Riemannian block SPD coupling manifold and its application to optimal transport", "abstract": "In this work, we study the optimal transport (OT) problem between symmetric positive definite (SPD) matrix-valued measures. We formulate the above as a generalized optimal transport problem where the cost, the marginals, and the coupling are represented as block matrices and each component block is a SPD matrix. The summation of row blocks and column blocks in the coupling matrix are constrained by the given block-SPD marginals. We endow the set of such block-coupling matrices with a novel Riemannian manifold structure. This allows to exploit the versatile Riemannian optimization framework to solve generic SPD matrix-valued OT problems. We illustrate the usefulness of the proposed approach in several applications.\n"}}
{"id": "IQW1hhJWx4", "cdate": 1675355257560, "mdate": 1675355257560, "content": {"title": "Riemannian block SPD coupling manifold and its application to optimal transport", "abstract": "In this work, we study the optimal transport (OT) problem between symmetric positive definite (SPD) matrix-valued measures. We formulate the above as a generalized optimal transport problem where the cost, the marginals, and the coupling are represented as block matrices and each component block is a SPD matrix. The summation of row blocks and column blocks in the coupling matrix are constrained by the given block-SPD marginals. We endow the set of such block-coupling matrices with a novel Riemannian manifold structure. This allows to exploit the versatile Riemannian optimization framework to solve generic SPD matrix-valued OT problems. We illustrate the usefulness of the proposed approach in several applications.\n"}}
{"id": "Q2JtPbGA8sH", "cdate": 1675355174528, "mdate": 1675355174528, "content": {"title": "Learning multilingual word embeddings in a latent metric space: a geometric approach", "abstract": "We propose a novel geometric approach for learning bilingual mappings given monolingual embeddings and a bilingual dictionary. Our approach decouples the source-to-target language transformation into (a) language-specific rotations on the original embeddings to align them in a common, latent space, and (b) a language-independent similarity metric in this common space to better model the similarity between the embeddings. Overall, we pose the bilingual mapping problem as a classification problem on smooth Riemannian manifolds. Empirically, our approach outperforms previous approaches on the bilingual lexicon induction and cross-lingual word similarity tasks.\nWe next generalize our framework to represent multiple languages in a common latent space. Language-specific rotations for all the languages and a common similarity metric in the latent space are learned jointly from bilingual dictionaries for multiple language pairs. We illustrate the effectiveness of joint learning for multiple languages in an indirect word translation setting."}}
{"id": "KrEdbvPubm", "cdate": 1675354718054, "mdate": 1675354718054, "content": {"title": "SPOT: A framework for selection of prototypes using optimal transport", "abstract": "In this work, we develop an optimal transport (OT) based framework to select informative prototypical examples that best represent a given target dataset. Summarizing a given target dataset via representative examples is an important problem in several machine learning applications where human understanding of the learning models and underlying data distribution is essential for decision making. We model the prototype selection problem as learning a sparse (empirical) probability distribution having the minimum OT distance from the target distribution. The learned probability measure supported on the chosen prototypes directly corresponds to their importance in representing the target data. We show that our objective function enjoys a key property of submodularity and propose an efficient greedy method that is both computationally fast and possess deterministic approximation guarantees. Empirical results on several real world benchmarks illustrate the efficacy of our approach.\n"}}
{"id": "d5MN87JhdiN", "cdate": 1664731454437, "mdate": null, "content": {"title": "Rieoptax: Riemannian Optimization in JAX", "abstract": "We present Rieoptax, an open source Python library for Riemannian optimization in JAX. We show that many differential geometric primitives, such as Riemannian exponential and logarithm maps, are usually faster in Rieoptax than existing frameworks in Python, both on CPU and GPU. We support a range of basic and advanced stochastic optimization solvers like Riemannian stochastic gradient, stochastic variance reduction, and adaptive gradient methods. A distinguishing feature of the proposed toolbox is that we also support differentially private optimization on Riemannian manifolds. "}}
{"id": "ZCHxGFmc62a", "cdate": 1621629895311, "mdate": null, "content": {"title": "On Riemannian Optimization over Positive Definite Matrices with the Bures-Wasserstein Geometry", "abstract": "In this paper, we comparatively analyze the Bures-Wasserstein (BW) geometry with the popular Affine-Invariant (AI) geometry for Riemannian optimization on the symmetric positive definite (SPD) matrix manifold. Our study begins with an observation that the BW metric has a linear dependence on SPD matrices in contrast to the quadratic dependence of the AI metric. We build on this to show that the BW metric is a more suitable and robust choice for several Riemannian optimization problems over ill-conditioned SPD matrices. We show that the BW geometry has a non-negative curvature, which further improves convergence rates of algorithms over the non-positively curved AI geometry. Finally, we verify that several popular cost functions, which are known to be geodesic convex under the AI geometry, are also geodesic convex under the BW geometry. Extensive experiments on various applications support our findings. \n"}}
{"id": "B1EjZs-uZr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Riemannian adaptive stochastic gradient algorithms on matrix manifolds", "abstract": "Adaptive stochastic gradient algorithms in the Euclidean space have attracted much attention lately. Such explorations on Riemannian manifolds, on the other hand, are relatively new, limited, and c..."}}
