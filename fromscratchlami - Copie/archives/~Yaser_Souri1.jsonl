{"id": "Rtszx74snBL", "cdate": 1680020498184, "mdate": 1680020498184, "content": {"title": "Robust Action Segmentation from Timestamp Supervision", "abstract": "Action segmentation is the task of predicting an action label for each frame of an untrimmed video. As obtaining annotations to train an approach for action segmentation in a fully supervised way is expensive, various approaches have been proposed to train action segmentation models using different forms of weak supervision, e.g., action transcripts, action sets, or more recently timestamps. Timestamp supervision is a promising type of weak supervision as obtaining one timestamp per action is less expensive than annotating all frames, but it provides more information than other forms of weak supervision. However, previous works assume that every action instance is annotated with a timestamp, which is a restrictive assumption since it assumes that annotators do not miss any action. In this work, we relax this restrictive assumption and take missing annotations for some action instances into account. We show that our approach is more robust to missing annotations compared to other approaches and various baselines."}}
{"id": "TJz9AiO_D5y", "cdate": 1618745815914, "mdate": null, "content": {"title": "Fast Bird Part Localization for Fine-Grained Categorization", "abstract": "Incorporating precise part information has proved to be crucial in building accurate fine-grained categorization systems in recent studies. The state-of-the-art approach for part localization uses a convolutional neural network and needs thousands of forward passes of the network, which is very time consuming. In this paper, an efficient method is proposed for part localization, with only one forward pass of the network. The proposed method provides improved generalization capability, compared to the state-of-the-art, and the ability to detect multiple part instances simultaneously, without much computational overhead. Experiments on the Caltech-UCSD Birds dataset show that the proposed method, while being much faster, achieves comparable accuracy to the state-of-the-art."}}
{"id": "Bc-3OJILe5", "cdate": 1609459200000, "mdate": 1645793139720, "content": {"title": "FIFA: Fast Inference Approximation for Action Segmentation", "abstract": "We introduce FIFA, a fast approximate inference method for action segmentation and alignment. Unlike previous approaches, FIFA does not rely on expensive dynamic programming for inference. Instead, it uses an approximate differentiable energy function that can be minimized using gradient-descent. FIFA is a general approach that can replace exact inference, improving its speed by more than 5 times while maintaining its performance. FIFA is an anytime inference algorithm that provides a better speed vs. accuracy trade-off compared to exact inference. We apply FIFA on top of state-of-the-art approaches for weakly supervised action segmentation and alignment as well as fully supervised action segmentation. FIFA achieves state-of-the-art results for most metrics on two action segmentation datasets."}}
{"id": "rVxb3dkLUeq", "cdate": 1577836800000, "mdate": 1645793139685, "content": {"title": "On Evaluating Weakly Supervised Action Segmentation Methods", "abstract": "Action segmentation is the task of temporally segmenting every frame of an untrimmed video. Weakly supervised approaches to action segmentation, especially from transcripts have been of considerable interest to the computer vision community. In this work, we focus on two aspects of the use and evaluation of weakly supervised action segmentation approaches that are often overlooked: the performance variance over multiple training runs and the impact of selecting feature extractors for this task. To tackle the first problem, we train each method on the Breakfast dataset 5 times and provide average and standard deviation of the results. Our experiments show that the standard deviation over these repetitions is between 1 and 2.5% and significantly affects the comparison between different approaches. Furthermore, our investigation on feature extraction shows that, for the studied weakly-supervised action segmentation methods, higher-level I3D features perform worse than classical IDT features."}}
{"id": "HcL-h_kLLl5", "cdate": 1546300800000, "mdate": 1645793139702, "content": {"title": "Hierarchical Graph-Rnns for Action Detection of Multiple Activities", "abstract": "In this paper, we propose an approach that spatially lo-calizes the activities in a video frame where each person can perform multiple activities at the same time. Our approach takes the temporal scene context as well as the relations of the actions of detected persons into account. While the temporal context is modeled by a temporal recurrent neural network (RNN), the relations of the actions are modeled by a graph RNN. Both networks are trained together and the proposed approach achieves state of the art results on the AVA dataset."}}
{"id": "Bpv-ndkIUxc", "cdate": 1546300800000, "mdate": 1645793139689, "content": {"title": "What Object Should I Use? - Task Driven Object Detection", "abstract": "When humans have to solve everyday tasks, they simply pick the objects that are most suitable. While the question which object should one use for a specific task sounds trivial for humans, it is very difficult to answer for robots or other autonomous systems. This issue, however, is not addressed by current benchmarks for object detection that focus on detecting object categories. We therefore introduce the COCO-Tasks dataset which comprises about 40,000 images where the most suitable objects for 14 tasks have been annotated. We furthermore propose an approach that detects the most suitable objects for a given task. The approach builds on a Gated Graph Neural Network to exploit the appearance of each object as well as the global context of all present objects in the scene. In our experiments, we show that the proposed approach outperforms other approaches that are evaluated on the dataset like classification or ranking approaches."}}
{"id": "BMZnd1ILe9", "cdate": 1451606400000, "mdate": 1645793139667, "content": {"title": "Deep Relative Attributes", "abstract": "Visual attributes are great means of describing images or scenes, in a way both humans and computers understand. In order to establish a correspondence between images and to be able to compare the strength of each property between images, relative attributes were introduced. However, since their introduction, hand-crafted and engineered features were used to learn increasingly complex models for the problem of relative attributes. This limits the applicability of those methods for more realistic cases. We introduce a deep neural network architecture for the task of relative attribute prediction. A convolutional neural network (ConvNet) is adopted to learn the features by including an additional layer (ranking layer) that learns to rank the images based on these features. We adopt an appropriate ranking loss to train the whole network in an end-to-end fashion. Our proposed method outperforms the baseline and state-of-the-art methods in relative attribute prediction on various coarse and fine-grained datasets. Our qualitative results along with the visualization of the saliency maps show that the network is able to learn effective features for each specific attribute. Source code of the proposed method is available at https://github.com/yassersouri/ghiaseddin."}}
