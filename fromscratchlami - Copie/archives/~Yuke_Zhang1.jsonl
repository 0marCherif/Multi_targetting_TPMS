{"id": "3ieUGDWGrt4", "cdate": 1681147861289, "mdate": null, "content": {"title": "C2PI: An Efficient Crypto-Clear Two-Party Neural Network Private Inference", "abstract": "Recently, private inference (PI) has addressed the rising concern over data and model privacy in machine learning inference as a service. However, existing PI frameworks suffer from high computational and communication costs due to the expensive multi-party computation (MPC) protocols. Existing literature has developed lighter MPC protocols to yield more efficient PI schemes. We, in contrast, %rethink existing PI frameworks and propose to lighten them by introducing an empirically-defined privacy evaluation. To that end, we reformulate the threat model of PI and use inference data privacy attacks (IDPAs) to evaluate data privacy. We then present an enhanced IDPA, named distillation-based inverse-network attack (DINA), for improved privacy evaluation. Finally, we leverage the findings from DINA and propose C2PI, a two-party PI framework presenting an efficient partitioning of the neural network model and requiring only the initial few layers to be performed with MPC protocols. Based on our experimental evaluations, relaxing the formal data privacy guarantees C2PI can speed up existing PI frameworks, including Delphi and Cheetah, up to 2.89 times and 3.88 times under LAN and WAN settings, respectively, and save up to 2.75 times communication costs."}}
{"id": "OwwLpYVvC2", "cdate": 1674770724597, "mdate": 1674770724597, "content": {"title": "LEARNING TO LINEARIZE DEEP NEURAL NETWORKS FOR SECURE AND EFFICIENT PRIVATE INFERENCE", "abstract": "The large number of ReLU non-linearity operations in existing deep neural networks makes them ill-suited for latency-efficient private inference (PI). Existing techniques to reduce ReLU operations often involve manual effort and sacrifice significant accuracy. In this paper, we first present a novel measure of\nnon-linearity layers\u2019 ReLU sensitivity, enabling mitigation of the time-consuming\nmanual efforts in identifying the same. Based on this sensitivity, we then present\nSENet, a three-stage training method that for a given ReLU budget, automatically\nassigns per-layer ReLU counts, decides the ReLU locations for each layer\u2019s activation map, and trains a model with significantly fewer ReLUs to potentially yield\nlatency and communication efficient PI. Experimental evaluations with multiple\nmodels on various datasets show SENet\u2019s superior performance both in terms of\nreduced ReLUs and improved classification accuracy compared to existing alternatives. In particular, SENet can yield models that require up to \u223c2\u00d7 fewer ReLUs while yielding similar accuracy. For a similar ReLU budget SENet can yield\nmodels with \u223c2.32% improved classification accuracy, evaluated on CIFAR-100."}}
{"id": "7g21FDychp", "cdate": 1672531200000, "mdate": 1681674645167, "content": {"title": "Learning to Linearize Deep Neural Networks for Secure and Efficient Private Inference", "abstract": "The large number of ReLU non-linearity operations in existing deep neural networks makes them ill-suited for latency-efficient private inference (PI). Existing techniques to reduce ReLU operations often involve manual effort and sacrifice significant accuracy. In this paper, we first present a novel measure of non-linearity layers' ReLU sensitivity, enabling mitigation of the time-consuming manual efforts in identifying the same. Based on this sensitivity, we then present SENet, a three-stage training method that for a given ReLU budget, automatically assigns per-layer ReLU counts, decides the ReLU locations for each layer's activation map, and trains a model with significantly fewer ReLUs to potentially yield latency and communication efficient PI. Experimental evaluations with multiple models on various datasets show SENet's superior performance both in terms of reduced ReLUs and improved classification accuracy compared to existing alternatives. In particular, SENet can yield models that require up to ~2x fewer ReLUs while yielding similar accuracy. For a similar ReLU budget SENet can yield models with ~2.32% improved classification accuracy, evaluated on CIFAR-100."}}
{"id": "BGF9IeDfmlH", "cdate": 1663849892582, "mdate": null, "content": {"title": "Learning to Linearize Deep Neural Networks  for Secure and Efficient Private Inference", "abstract": "The large number of ReLU non-linearity operations in existing deep neural networks makes them ill-suited for latency-efficient private inference (PI). Existing techniques to reduce ReLU operations often involve manual effort and sacrifice significant accuracy. In this paper, we first present a novel measure of non-linearity layers\u2019 ReLU sensitivity, enabling mitigation of the time-consuming manual efforts in identifying the same. Based on this sensitivity, we then present SENet, a three-stage training method that for a given ReLU budget, automatically assigns per-layer ReLU counts, decides the ReLU locations for each layer\u2019s activation map, and trains a model with significantly fewer ReLUs to potentially yield latency and communication efficient PI. Experimental evaluations with multiple models on various datasets show SENet\u2019s superior performance both in terms of reduced ReLUs and improved classification accuracy compared to existing alternatives. In particular, SENet can yield models that require up to \u223c2\u00d7 fewer ReLUs while yielding similar accuracy. For a similar ReLU budget SENet can yield models with \u223c2.32% improved classification accuracy, evaluated on CIFAR-100."}}
{"id": "hweRHy_Arvl", "cdate": 1640995200000, "mdate": 1681674644667, "content": {"title": "TriLock: IC Protection with Tunable Corruptibility and Resilience to SAT and Removal Attacks", "abstract": "Sequential logic locking has been studied over the last decade as a method to protect sequential circuits from reverse engineering. However, most of the existing sequential logic locking techniques are threatened by increasingly more sophisticated SAT-based attacks, efficiently using input queries to a SAT solver to rule out incorrect keys, as well as removal attacks based on structural analysis. In this paper, we propose TriLock, a sequential logic locking method that simultaneously addresses these vulnerabilities. TriLock can achieve high, tunable functional corruptibility while still guaranteeing exponential queries to the SAT solver in a SAT-based attack. Further, it adopts a state re-encoding method to obscure the boundary between the original state registers and those inserted by the locking method, thus making it more difficult to detect and remove the locking-related components."}}
{"id": "-VCgT_TOze", "cdate": 1640995200000, "mdate": 1681674644037, "content": {"title": "TriLock: IC Protection with Tunable Corruptibility and Resilience to SAT and Removal Attacks", "abstract": "Sequential logic locking has been studied over the last decade as a method to protect sequential circuits from reverse engineering. However, most of the existing sequential logic locking techniques are threatened by increasingly more sophisticated SAT-based attacks, efficiently using input queries to a SAT solver to rule out incorrect keys, as well as removal attacks based on structural analysis. In this paper, we propose TriLock, a sequential logic locking method that simultaneously addresses these vulnerabilities. TriLock can achieve high, tunable functional corruptibility while still guaranteeing exponential queries to the SAT solver in a SAT-based attack. Further, it adopts a state re-encoding method to obscure the boundary between the original state registers and those inserted by the locking method, thus making it more difficult to detect and remove the locking-related components."}}
{"id": "luWFXLelA2", "cdate": 1609459200000, "mdate": 1681674644819, "content": {"title": "Fun-SAT: Functional Corruptibility-Guided SAT-Based Attack on Sequential Logic Encryption", "abstract": "The SAT attack has shown to be efficient against most combinational logic encryption methods. It can be extended to attack sequential logic encryption techniques by leveraging circuit unrolling and model checking methods. However, with no guidance on the number of times that a circuit needs to be unrolled to find the correct key, the attack tends to solve many time-consuming Boolean satisfiability (SAT) and model checking problems, which can significantly hamper its efficiency. In this paper, we introduce Fun-SAT, a functional corruptibility-guided SAT-based attack that can significantly decrease the SAT solving and model checking time of a SAT-based attack on sequential encryption by efficiently estimating the minimum required number of circuit unrollings. Fun-SAT relies on a notion of functional corruptibility for encrypted sequential circuits and its relationship with the required number of circuit unrollings in a SAT-based attack. Numerical results show that Fun-SAT can be, on average, 90\u00d7 faster than previous attacks against state-of-the-art encryption methods, when both attacks successfully complete before a one-day time-out. Moreover, Fun-SAT completes before the time-out on many more circuits."}}
{"id": "bQ2csIS7Ldl", "cdate": 1577836800000, "mdate": null, "content": {"title": "Deep Model Compression and Inference Speedup of Sum-Product Networks on Tensor Trains", "abstract": "Sum-product networks (SPNs) constitute an emerging class of neural networks with clear probabilistic semantics and superior inference speed over other graphical models. This brief reveals an important connection between SPNs and tensor trains (TTs), leading to a new canonical form which we call tensor SPNs (tSPNs). Specifically, we demonstrate the intimate relationship between a valid SPN and a TT. For the first time, through mapping an SPN onto a tSPN and employing specially customized optimization techniques, we demonstrate improvements up to a factor of 100 on both model compression and inference speedup for various data sets with negligible loss in accuracy."}}
{"id": "K1hw8qcvGn3", "cdate": 1577836800000, "mdate": 1682359600330, "content": {"title": "A Reconfigurable Passive Switched-Capacitor Multiply-and-Accumulate Unit for Approximate Computing", "abstract": "A reconfigurable switched-capacitor based multiply and accumulate unit (MACU) is proposed in this work. The multiplication is achieved by a digital-to-capacitance converter (DCC) with a flexible weight resolution of 2-6 bits. The accumulation output is digitized using a reconfigurable 6-9 bit successive approximation register analog to digital converter (SAR ADC). The design is implemented in 65nm CMOS process. The power consumption tends to reduce with the reduction of ADC resolution. The proposed MACU achieved an efficiency of 20.83 TOPS/W at an output resolution of 6. Simulation results for edge detection, MNIST image classification, and speech denoising are presented to prove the effectiveness of the proposed unit."}}
