{"id": "sOgyNWyN6Gu", "cdate": 1676591078610, "mdate": null, "content": {"title": "Accelerating Policy Gradient by Estimating Value Function from Prior Computation in Deep Reinforcement Learning", "abstract": "This paper investigates the use of prior computation to estimate the value function to improve sample efficiency in on-policy policy gradient methods in reinforcement learning. Our approach is to estimate the value function from prior computations, such as from the Q-network learned in DQN or the value function trained for different but related environments. In particular, we learn a new value function for the target task while combining it with a value estimate from the prior computation. Finally, the resulting value function is used as a baseline in the policy gradient method. This use of a baseline has the theoretical property of reducing variance in gradient computation and thus improving sample efficiency. The experiments show the successful use of prior value estimates in various settings and improved sample efficiency in several tasks."}}
{"id": "0Q9OGm0YhF", "cdate": 1672531200000, "mdate": 1680015953580, "content": {"title": "Accelerating Policy Gradient by Estimating Value Function from Prior Computation in Deep Reinforcement Learning", "abstract": ""}}
{"id": "HnLFY8F9uS", "cdate": 1663850400584, "mdate": null, "content": {"title": "Robust Policy Optimization in Deep Reinforcement Learning", "abstract": "Entropy can play an essential role in policy optimization by selecting the stochastic policy, which eventually helps better explore the environment in reinforcement learning (RL). A proper balance between exploration and exploitation is challenging and might depend on the particular RL task. However, the stochasticity often reduces as the training progresses; thus, the policy becomes less exploratory. Therefore, in many cases, the policy can converge to sub-optimal due to a lack of representative data during training. Moreover, this issue can even be severe in high-dimensional environments. This paper investigates whether keeping a certain entropy threshold throughout training can help better policy learning. In particular, we propose an algorithm Robust Policy Optimization (RPO), which leverages a perturbed Gaussian distribution to encourage high-entropy actions. We evaluated our methods on various continuous control tasks from DeepMind Control, OpenAI Gym, Pybullet, and IsaacGym. We observed that in many settings, RPO increases the policy entropy early in training and then maintains a certain level of entropy throughout the training period. Eventually, our agent RPO shows consistently improved performance compared to PPO and other techniques such as data augmentation and entropy regularization. Furthermore, in several settings, our method stays robust in performance, while other baseline mechanisms fail to improve and even worsen the performance."}}
{"id": "mLfeEze0-6", "cdate": 1640995200000, "mdate": 1680015953538, "content": {"title": "Bootstrap Advantage Estimation for Policy Optimization in Reinforcement Learning", "abstract": ""}}
{"id": "aJtCqtYnDk", "cdate": 1640995200000, "mdate": 1680015953519, "content": {"title": "Bootstrap State Representation Using Style Transfer for Better Generalization in Deep Reinforcement Learning", "abstract": ""}}
{"id": "BOSnnqknt6D", "cdate": 1640995200000, "mdate": 1680015953550, "content": {"title": "Robust Policy Optimization in Deep Reinforcement Learning", "abstract": ""}}
{"id": "S0NsaRIxvQ", "cdate": 1632875720630, "mdate": null, "content": {"title": "Adversarial Style Transfer for Robust Policy Optimization in Reinforcement Learning", "abstract": "This paper proposes an algorithm that aims to improve generalization for reinforcement learning agents by removing overfitting to confounding features. Our approach consists of a max-min game theoretic objective. A generator transfers the style of observation during reinforcement learning. An additional goal of the generator is to perturb the observation, which maximizes the agent's probability of taking a different action. In contrast, a policy network updates its parameters to minimize the effect of such perturbations, thus staying robust while maximizing the expected future reward. Based on this setup, we propose a practical deep reinforcement learning algorithm, Adversarial Robust Policy Optimization (ARPO), to find an optimal policy that generalizes to unseen environments. We evaluate our approach on visually enriched and diverse Procgen benchmarks. Empirically, we observed that our agent ARPO performs better in generalization and sample efficiency than a few state-of-the-art algorithms."}}
{"id": "reZZHjAXMX5", "cdate": 1609459200000, "mdate": 1648668316936, "content": {"title": "SARTRES: a semi-autonomous robot teleoperation environment for surgery", "abstract": "Teleoperated surgical robots can provide immediate medical assistance in austere and hostile environments. However, such scenarios are time-sensitive and require high-bandwidth and low-latency comm..."}}
{"id": "SNe-SsCQMmc", "cdate": 1609459200000, "mdate": 1648668316966, "content": {"title": "DESERTS: DElay-tolerant SEmi-autonomous Robot Teleoperation for Surgery", "abstract": "Telesurgery can be hindered by high-latency and low-bandwidth communication networks, often found in austere settings. Even delays of less than one second are known to negatively impact surgeries. To tackle the effects of connectivity associated with telerobotic surgeries, we propose the DESERTS framework. DESERTS provides a novel simulator interface where the surgeon can operate directly on a virtualized reality simulation and the activities are mirrored in a remote robot, almost simultaneously. Thus, the surgeon can perform the surgery uninterrupted, while high-level commands are extracted from his motions and are sent to a remote robotic agent. The simulated setup mirrors the remote environment, including an alpha-blended view of the remote scene. The framework abstracts the actions into atomic surgical maneuvers (surgemes) which eliminate the need to transmit compressed video information. This system uses a deep learning based architecture to perform live recognition of the surgemes executed by the operator. The robot then executes the received surgemes, thereby achieving semi-autonomy. The framework\u2019s performance was tested on a peg transfer task. We evaluated the accuracy of the recognition and execution module independently as well as during live execution. Furthermore, we assessed the framework\u2019s performance in the presence of increasing delays. Notably, the system maintained a task success rate of 87% from no-delays to 5 seconds of delay."}}
{"id": "HU9bBiRmfQ9", "cdate": 1609459200000, "mdate": 1648668316965, "content": {"title": "Sequential Prediction with Logic Constraints for Surgical Robotic Activity Recognition", "abstract": "Many real-world time-sensitive and high-stake applications (e.g., surgical, rescue, and recovery robotics) exhibit sequential nature; thus, applying Recurrent Neural Network (RNN)-based sequential models is an attractive approach to detect robotic activity. One limitation of such approaches is data scarcity. As a result, limited training samples may lead to over-fitting, producing incorrect predictions during deployment. Nevertheless, abundant domain knowledge may still be available, which may help formulate logic constraints. In this paper, we propose a novel way to integrate domain knowledge into RNN-based sequential prediction. We build a Markov Logic Network (MLN)-based classifier that automatically learns constraint weights from data. We propose two methods to incorporate this MLN-based prediction: (i) PriorLayer, in which the values of the hidden layer of the RNN are combined with weights learned from logic constraints in an additional neural network layer, and (ii) Conflation, in which class probabilities from RNN predictions and constraint weights are combined based on the conflation of class probabilities. We evaluate robotic activity classification methods on a simulated OpenAI Gym environment and a real-world DESK dataset for surgical robotics. We observe that our proposed MLN-based approaches boost the performance of LSTM-based networks. In particular, MLN boosts the accuracy of LSTM from 71% to 84% on the Gym dataset and from 68% to 72% on the Taurus robot dataset. Furthermore, MLN (i.e., PriorLayer) shows regularization capability where it improves accuracy in initial LSTM training while avoiding over-fitting early, thus improves the final classification accuracy on unseen data. The code is available at https://github.com/masud99r/prediction-with-logic-constraints."}}
