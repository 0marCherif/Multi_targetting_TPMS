{"id": "PHZWHNrZ56", "cdate": 1677628800000, "mdate": 1681652377811, "content": {"title": "Special Issue on Theoretical Foundations of Evolutionary Computation", "abstract": ""}}
{"id": "akeVfjnPfTS", "cdate": 1676827073743, "mdate": null, "content": {"title": "Fast Teammate Adaptation in the Presence of Sudden Policy Change", "abstract": "Cooperative multi-agent reinforcement learning (MARL), where agents coordinates with teammate(s) for a shared goal, may sustain non-stationary caused by the policy change of teammates. Prior works mainly concentrate on the policy change cross episodes, ignoring the fact that teammates may suffer from sudden policy change within an episode, which might lead to miscoordination and poor performance. We formulate the problem as an open Dec-POMDP, where we control some agents to coordinate with uncontrolled teammates, whose policies could be changed within one episode. Then we develop a new framework Fast teammates adaptation (Fastap) to address the problem. Concretely, we first train versatile teammates' policies and assign them to different clusters via the Chinese Restaurant Process (CRP). Then, we train the controlled agent(s) to coordinate with the sampled uncontrolled teammates by capturing their identifications as context for fast adaptation. Finally, each agent applies its local information to anticipate the teammates' context for decision-making accordingly. This process proceeds alternately, leading to a robust policy that can adapt to any teammates during the decentralized execution phase. We show in multiple multi-agent benchmarks that Fastap can achieve superior performance than multiple baselines in stationary and non-stationary scenarios. "}}
{"id": "loclo758Yas", "cdate": 1672531200000, "mdate": 1681652378113, "content": {"title": "Benchmarking Algorithms for Submodular Optimization Problems Using IOHProfiler", "abstract": ""}}
{"id": "XYhElR55DV", "cdate": 1672531200000, "mdate": 1681652377757, "content": {"title": "Multi-objective evolutionary algorithms are generally good: Maximizing monotone submodular functions over sequences", "abstract": ""}}
{"id": "bLmSMXbqXr", "cdate": 1663850262171, "mdate": null, "content": {"title": "Quality-Similar Diversity via Population Based Reinforcement Learning", "abstract": "Diversity is a growing research topic in Reinforcement Learning (RL). Previous research on diversity has mainly focused on promoting diversity to encourage exploration and thereby improve quality (the cumulative reward), maximizing diversity subject to quality constraints, or jointly maximizing quality and diversity, known as the quality-diversity problem. In this work, we present the quality-similar diversity problem that features diversity among policies of similar qualities. In contrast to task-agnostic diversity, we focus on task-specific diversity defined by a set of user-specified Behavior Descriptors (BDs). A BD is a scalar function of a trajectory (e.g., the fire action rate for an Atari game), which delivers the type of diversity the user prefers. To derive the gradient of the user-specified diversity with respect to a policy, which is not trivially available, we introduce a set of BD estimators and connect it with the classical policy gradient theorem. Based on the diversity gradient, we develop a population-based RL algorithm to adaptively and efficiently optimize the population diversity at multiple quality levels throughout training. Extensive results on MuJoCo and Atari demonstrate that our algorithm significantly outperforms previous methods in terms of generating user-specified diverse policies across different quality levels."}}
{"id": "h3jZCLjhtmV", "cdate": 1652737761747, "mdate": null, "content": {"title": "Multi-agent Dynamic Algorithm Configuration", "abstract": "Automated algorithm configuration relieves users from tedious, trial-and-error tuning tasks. A popular algorithm configuration tuning paradigm is dynamic algorithm configuration (DAC), in which an agent learns dynamic configuration policies across instances by reinforcement learning (RL). However, in many complex algorithms, there may exist different types of configuration hyperparameters, and such heterogeneity may bring difficulties for classic DAC which uses a single-agent RL policy. In this paper, we aim to address this issue and propose multi-agent DAC (MA-DAC), with one agent working for one type of configuration hyperparameter. MA-DAC formulates the dynamic configuration of a complex algorithm with multiple types of hyperparameters as a contextual multi-agent Markov decision process and solves it by a cooperative multi-agent RL (MARL) algorithm. To instantiate, we apply MA-DAC to a well-known optimization algorithm for multi-objective optimization problems. Experimental results show the effectiveness of MA-DAC in not only achieving superior performance compared with other configuration tuning approaches based on heuristic rules, multi-armed bandits, and single-agent RL, but also being capable of generalizing to different problem classes. Furthermore, we release the environments in this paper as a benchmark for testing MARL algorithms, with the hope of facilitating the application of MARL."}}
{"id": "SUzPos_pUC", "cdate": 1652737760344, "mdate": null, "content": {"title": "Monte Carlo Tree Search based Variable Selection for High Dimensional Bayesian Optimization", "abstract": "Bayesian optimization (BO) is a class of popular methods for expensive black-box optimization, and has been widely applied to many scenarios. However, BO suffers from the curse of dimensionality, and scaling it to high-dimensional problems is still a challenge. In this paper, we propose a variable selection method MCTS-VS based on Monte Carlo tree search (MCTS), to iteratively select and optimize a subset of variables. That is, MCTS-VS constructs a low-dimensional subspace via MCTS and optimizes in the subspace with any BO algorithm. We give a theoretical analysis of the general variable selection method to reveal how it can work. Experiments on high-dimensional synthetic functions and real-world problems (e.g., MuJoCo locomotion tasks) show that MCTS-VS equipped with a proper BO optimizer can achieve state-of-the-art performance."}}
{"id": "z2i8axpSYK", "cdate": 1640995200000, "mdate": 1679906069593, "content": {"title": "Multi-objective Evolutionary Instance Selection for Multi-label Classification", "abstract": ""}}
{"id": "rzVLgteZac", "cdate": 1640995200000, "mdate": 1679906069568, "content": {"title": "Robust Neural Network Pruning by Cooperative Coevolution", "abstract": ""}}
{"id": "pEiOgtpLLv", "cdate": 1640995200000, "mdate": 1681652377818, "content": {"title": "ZOOpt: a toolbox for derivative-free optimization", "abstract": ""}}
