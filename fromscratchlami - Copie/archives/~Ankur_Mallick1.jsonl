{"id": "nKLgsONGLO", "cdate": 1640995200000, "mdate": 1682320894698, "content": {"title": "Matchmaker: Data Drift Mitigation in Machine Learning for Large-Scale Systems", "abstract": ""}}
{"id": "QC5kl9LyqIR", "cdate": 1640995200000, "mdate": 1682631600431, "content": {"title": "Rateless Sum-Recovery Codes For Distributed Non-Linear Computations", "abstract": "We address the problem of slowdown caused by straggling nodes in distributed non-linear computations. Many common non-linear computations can be written as a sum of inexpensive non-linear functions (e.g. Taylor series). Based on this observation, we propose a new class of rateless codes called rateless sum-recovery codes whose aim is to recover the sum of source symbols, without necessarily recovering individual symbols. Source symbols correspond to individual inexpensive functions and each encoded symbol is the sum of a subset of source symbols. Encoded symbols are computed in a distributed fashion and for a computation that can be written as a sum of m inexpensive functions, successful sum-recovery is possible with high probability as long as slightly more than m encoded symbols are received. Our code is rateless, systematic and has sparse parities. Moreover, encoded symbols are constructed by sampling without replacement at individual nodes, thereby making decoding superfluous if the encoded symbols from any node cover all source symbols. We validate our claims through a range of simulations and also discuss open questions for future works.A full version of this paper is accessible at [1]."}}
{"id": "IFHfuyD62zP", "cdate": 1640995200000, "mdate": 1682631600431, "content": {"title": "Rateless codes for near-perfect load balancing in distributed matrix-vector multiplication", "abstract": ""}}
{"id": "BKeJmkspvc", "cdate": 1621630295046, "mdate": null, "content": {"title": "Leveraging Spatial and Temporal Correlations in Sparsified Mean Estimation", "abstract": "We study the problem of estimating at a central server the mean of a set of vectors distributed across several nodes (one vector per node). When the vectors are high-dimensional, the communication cost of sending entire vectors may be prohibitive, and it may be imperative for them to use sparsification techniques. While most existing work on sparsified mean estimation is agnostic to the characteristics of the data vectors, in many practical applications such as federated learning, there may be spatial correlations (similarities in the vectors sent by different nodes) or temporal correlations (similarities in the data sent by a single node over different iterations of the algorithm) in the data vectors. We leverage these correlations by simply modifying the decoding method used by the server to estimate the mean. We provide an analysis of the resulting estimation error as well as experiments for PCA, K-Means and Logistic Regression, which show that our estimators consistently outperform more sophisticated and expensive sparsification methods."}}
{"id": "pqBcqfWjfC", "cdate": 1609459200000, "mdate": 1682631600430, "content": {"title": "Leveraging Spatial and Temporal Correlations in Sparsified Mean Estimation", "abstract": "We study the problem of estimating at a central server the mean of a set of vectors distributed across several nodes (one vector per node). When the vectors are high-dimensional, the communication cost of sending entire vectors may be prohibitive, and it may be imperative for them to use sparsification techniques. While most existing work on sparsified mean estimation is agnostic to the characteristics of the data vectors, in many practical applications such as federated learning, there may be spatial correlations (similarities in the vectors sent by different nodes) or temporal correlations (similarities in the data sent by a single node over different iterations of the algorithm) in the data vectors. We leverage these correlations by simply modifying the decoding method used by the server to estimate the mean. We provide an analysis of the resulting estimation error as well as experiments for PCA, K-Means and Logistic Regression, which show that our estimators consistently outperform more sophisticated and expensive sparsification methods."}}
{"id": "HreZgP1FLg9", "cdate": 1609459200000, "mdate": 1645805399831, "content": {"title": "Rateless Codes for Distributed Non-linear Computations", "abstract": "Machine learning today involves massive distributed computations running on cloud servers, which are highly susceptible to slowdown or straggling. Recent work has demonstrated the effectiveness of erasure codes in mitigating such slowdown for linear computations, by adding redundant computations such that the entire computation can be recovered as long as a subset of nodes finish their assigned tasks. However, most machine learning algorithms typically involve non-linear computations that cannot be directly handled by these coded computing approaches. In this work, we propose a coded computing strategy for mitigating the effect of stragglers on non-linear distributed computations. Our strategy relies on the observation that many expensive non-linear functions can be decomposed into sums of cheap non-linear functions. We show that erasure codes, specifically rateless codes can be used to generate and compute random linear combinations of these functions at the nodes such that the original function can be computed as long as a subset of nodes return their computations. Simulations and experiments on AWS Lambda demonstrate the superiority of our approach over various uncoded baselines.A full version of this paper is accessible at [1]"}}
{"id": "Hb4Zgv1tLlc", "cdate": 1609459200000, "mdate": 1645805399840, "content": {"title": "Deep kernels with probabilistic embeddings for small-data learning", "abstract": "Gaussian Processes (GPs) are known to provide accurate predictions and uncertainty estimates even with small amounts of labeled data by capturing similarity between data points through their kernel..."}}
{"id": "B24-xwkFIl5", "cdate": 1609459200000, "mdate": 1645805399838, "content": {"title": "Leveraging Spatial and Temporal Correlations in Sparsified Mean Estimation", "abstract": "We study the problem of estimating at a central server the mean of a set of vectors distributed across several nodes (one vector per node). When the vectors are high-dimensional, the communication cost of sending entire vectors may be prohibitive, and it may be imperative for them to use sparsification techniques. While most existing work on sparsified mean estimation is agnostic to the characteristics of the data vectors, in many practical applications such as federated learning, there may be spatial correlations (similarities in the vectors sent by different nodes) or temporal correlations (similarities in the data sent by a single node over different iterations of the algorithm) in the data vectors. We leverage these correlations by simply modifying the decoding method used by the server to estimate the mean. We provide an analysis of the resulting estimation error as well as experiments for PCA, K-Means and Logistic Regression, which show that our estimators consistently outperform more sophisticated and expensive sparsification methods."}}
{"id": "nTSyWOpFA7h", "cdate": 1577836800000, "mdate": null, "content": {"title": "Probabilistic Neighbourhood Component Analysis: Sample Efficient Uncertainty Estimation in Deep Learning", "abstract": "While Deep Neural Networks (DNNs) achieve state-of-the-art accuracy in various applications, they often fall short in accurately estimating their predictive uncertainty and, in turn, fail to recognize when these predictions may be wrong. Several uncertainty-aware models, such as Bayesian Neural Network (BNNs) and Deep Ensembles have been proposed in the literature for quantifying predictive uncertainty. However, research in this area has been largely confined to the big data regime. In this work, we show that the uncertainty estimation capability of state-of-the-art BNNs and Deep Ensemble models degrades significantly when the amount of training data is small. To address the issue of accurate uncertainty estimation in the small-data regime, we propose a probabilistic generalization of the popular sample-efficient non-parametric kNN approach. Our approach enables deep kNN classifier to accurately quantify underlying uncertainties in its prediction. We demonstrate the usefulness of the proposed approach by achieving superior uncertainty quantification as compared to state-of-the-art on a real-world application of COVID-19 diagnosis from chest X-Rays. Our code is available at https://github.com/ankurmallick/sample-efficient-uq"}}
{"id": "bq6qWSnqzjL", "cdate": 1577836800000, "mdate": null, "content": {"title": "Rateless Codes for Near-Perfect Load Balancing in Distributed Matrix-Vector Multiplication", "abstract": "Large-scale machine learning and data mining applications require computer systems to perform massive matrix-vector and matrix-matrix multiplication operations that need to be parallelized across multiple nodes. The presence of stragglers -- nodes that unpredictably slowdown or fail -- is a major bottleneck in such distributed computations. We propose a rateless fountain coding strategy to address this issue. Our idea is to create linear combinations of the m rows of the matrix and assign these encoded rows to different worker nodes. The original matrix-vector product can be decoded as soon as slightly more than m row-vector products are collectively finished by the nodes. We show that our approach achieves optimal latency and performs zero redundant computations asymptotically. Experiments on Amazon EC2 show that rateless coding gives as much as 3x speed-up over uncoded schemes."}}
