{"id": "EOjCF9cA7q", "cdate": 1667492818155, "mdate": 1667492818155, "content": {"title": "Zero-Shot Quantization Brought Closer to the Teacher", "abstract": "Model quantization is considered as a promising method\nto greatly reduce the resource requirements of deep neural\nnetworks. To deal with the performance drop induced by\nquantization errors, a popular method is to use training data\nto fine-tune quantized networks. In real-world environments,\nhowever, such a method is frequently infeasible because\ntraining data is unavailable due to security, privacy, or confidentiality concerns. Zero-shot quantization addresses such\nproblems, usually by taking information from the weights of\na full-precision teacher network to compensate the performance drop of the quantized networks. In this paper, we first\nanalyze the loss surface of state-of-the-art zero-shot quantization techniques and provide several findings. In contrast\nto usual knowledge distillation problems, zero-shot quantization often suffers from 1) the difficulty of optimizing multiple\nloss terms together, and 2) the poor generalization capability\ndue to the use of synthetic samples. Furthermore, we observe\nthat many weights fail to cross the rounding threshold during\ntraining the quantized networks even when it is necessary\nto do so for better performance. Based on the observations,\nwe propose AIT, a simple yet powerful technique for zeroshot quantization, which addresses the aforementioned two\nproblems in the following way: AIT i) uses a KL distance\nloss only without a cross-entropy loss, and ii) manipulates\ngradients to guarantee that a certain portion of weights are\nproperly updated after crossing the rounding thresholds. Experiments show that AIT outperforms the performance of\nmany existing methods by a great margin, taking over the\noverall state-of-the-art position in the field"}}
{"id": "y9z89lQhV3", "cdate": 1640995200000, "mdate": 1667493017811, "content": {"title": "GCoM: a detailed GPU core model for accurate analytical modeling of modern GPUs", "abstract": "Analytical models can greatly help computer architects perform orders of magnitude faster early-stage design space exploration than using cycle-level simulators. To facilitate rapid design space exploration for graphics processing units (GPUs), prior studies have proposed GPU analytical models which capture first-order stall events causing performance degradation; however, the existing analytical models cannot accurately model modern GPUs due to their outdated and highly abstract GPU core microarchitecture assumptions. Therefore, to accurately evaluate the performance of modern GPUs, we need a new GPU analytical model which accurately captures the stall events incurred by the significant changes in the core microarchitectures of modern GPUs. We propose GCoM, an accurate GPU analytical model which faithfully captures the key core-side stall events of modern GPUs. Through detailed microarchitecture-driven GPU core modeling, GCoM accurately models modern GPUs by revealing the following key core-side stalls overlooked by the existing GPU analytical models. First, GCoM identifies the compute structural stall events caused by the limited per-sub-core functional units. Second, GCoM exposes the memory structural stalls due to the limited banks and shared nature of per-core L1 data caches. Third, GCoM correctly predicts the memory data stalls induced by the sectored L1 data caches which split a cache line into a set of sectors sharing the same tag. Fourth, GCoM captures the idle stalls incurred by the inter- and intra-core load imbalances. Our experiments using an NVIDIA RTX 2060 configuration show that GCoM greatly improves the modeling accuracy by achieving a mean absolute error of 10.0% against Accel-Sim cycle-level simulator, whereas the state-of-the-art GPU analytical model achieves a mean absolute error of 44.9%."}}
{"id": "sAjqOkkzdsj", "cdate": 1640995200000, "mdate": 1667493017562, "content": {"title": "SALoBa: Maximizing Data Locality and Workload Balance for Fast Sequence Alignment on GPUs", "abstract": "Sequence alignment forms an important backbone in many sequencing applications. A commonly used strategy for sequence alignment is an approximate string matching with a two-dimensional dynamic programming approach. Although some prior work has been conducted on GPU acceleration of a sequence alignment, we identify several shortcomings that limit exploiting the full computational capability of modern GPUs. This paper presents SALoBa, a GPU-accelerated sequence alignment library focused on seed extension. Based on the analysis of previous work with real-world sequencing data, we propose techniques to exploit the data locality and improve work-load balancing. The experimental results reveal that SALoBa significantly improves the seed extension kernel compared to state-of-the-art GPU-based methods."}}
{"id": "hgFL_PES2ZC", "cdate": 1640995200000, "mdate": 1667493017607, "content": {"title": "It's All In the Teacher: Zero-Shot Quantization Brought Closer to the Teacher", "abstract": "Model quantization is considered as a promising method to greatly reduce the resource requirements of deep neural networks. To deal with the performance drop induced by quantization errors, a popular method is to use training data to fine-tune quantized networks. In real-world environments, however, such a method is frequently infeasible because training data is unavailable due to security, privacy, or confidentiality concerns. Zero-shot quantization addresses such problems, usually by taking information from the weights of a full-precision teacher network to compensate the performance drop of the quantized networks. In this paper, we first analyze the loss surface of state-of-the-art zero-shot quantization techniques and provide several findings. In contrast to usual knowledge distillation problems, zero-shot quantization often suffers from 1) the difficulty of optimizing multiple loss terms together, and 2) the poor generalization capability due to the use of synthetic samples. Furthermore, we observe that many weights fail to cross the rounding threshold during training the quantized networks even when it is necessary to do so for better performance. Based on the observations, we propose AIT, a simple yet powerful technique for zero-shot quantization, which addresses the aforementioned two problems in the following way: AIT i) uses a KL distance loss only without a cross-entropy loss, and ii) manipulates gradients to guarantee that a certain portion of weights are properly updated after crossing the rounding thresholds. Experiments show that AIT outperforms the performance of many existing methods by a great margin, taking over the overall state-of-the-art position in the field."}}
{"id": "P_zhFo8kio", "cdate": 1640995200000, "mdate": 1667493017704, "content": {"title": "ComPreEND: Computation Pruning through Predictive Early Negative Detection for ReLU in a Deep Neural Network Accelerator", "abstract": "A vast amount of activation values of DNNs are zeros due to ReLU (Rectified Linear Unit), which is one of the most common activation functions used in modern neural networks. Since ReLU outputs zero for all negative inputs, the inputs to ReLU do not need to be determined exactly as long as they are negative. However, many accelerators usually do not consider such aspects of DNNs, losing a huge amount of opportunities for speedups and energy savings. To exploit such opportunities, we propose early negative detection (END), a computation pruning technique that detects the negative results at an early stage. The key to the early negative detection is the adoption of inverted two's complement representation for filter parameters. This ensures that as soon as the intermediate results become negative, the final results are guaranteed to be negative. Upon detection, the remaining computation can be skipped and the following ReLU output can be simply set to zero. We also propose a DNN accelerator architecture (ComPreEND) that takes advantage of such skipping. ComPreEND with END significantly improves both the energy efficiency and the performance according to the evaluation. Compared to the baseline, we obtain 20.5 and 29.3 percent speedup with accurate mode and predictive mode, and energy savings by 28.4 and 41.4 percent, respectively."}}
{"id": "FwPoIfpH5y", "cdate": 1640995200000, "mdate": 1667493017748, "content": {"title": "Enabling hard constraints in differentiable neural network and accelerator co-exploration", "abstract": "Co-exploration of an optimal neural architecture and its hardware accelerator is an approach of rising interest which addresses the computational cost problem, especially in low-profile systems. The large co-exploration space is often handled by adopting the idea of differentiable neural architecture search. However, despite the superior search efficiency of the differentiable co-exploration, it faces a critical challenge of not being able to systematically satisfy hard constraints such as frame rate. To handle the hard constraint problem of differentiable co-exploration, we propose HDX, which searches for hard-constrained solutions without compromising the global design objectives. By manipulating the gradients in the interest of the given hard constraint, high-quality solutions satisfying the constraint can be obtained."}}
{"id": "7Bb-lvNJRE", "cdate": 1640995200000, "mdate": 1667493017503, "content": {"title": "GuardiaNN: Fast and Secure On-Device Inference in TrustZone Using Embedded SRAM and Cryptographic Hardware", "abstract": "As more and more mobile/embedded applications employ Deep Neural Networks (DNNs) involving sensitive user data, mobile/embedded devices must provide a highly secure DNN execution environment to prevent privacy leaks. Aimed at securing DNN data, recent studies execute part of a DNN in a trusted execution environment (e.g., TrustZone) to isolate DNN execution from the other processes; however, as the trusted execution environments for mobile/embedded devices provide limited memory protection, DNN data remain unencrypted in DRAM and become vulnerable to physical attacks. The devices can prevent the physical attacks by keeping DNN data encrypted in DRAM; when DNN data get referenced during DNN execution, they get loaded to the SRAM and get decrypted by a CPU core. Unfortunately, using the SRAM with demand paging greatly increases DNN execution time due to the inefficient use of the SRAM and the high CPU consumption of data encryption/decryption. In this paper, we present GuardiaNN, a fast and secure DNN framework which greatly accelerates DNN execution without sacrificing security guarantees. To accelerate secure DNN execution, GuardiaNN first reduces slow DRAM accesses with direct convolutions and maximizes the reuse of SRAM-stored data with DNN-friendly SRAM management. Then, aimed at dedicating the limited CPU resources to DNN execution, GuardiaNN offloads DNN data encryption/decryption onto secure cryptographic hardware and employs pipelining to overlap DNN execution with the encryption/decryption. For eight DNNs chosen from five representative mobile/embedded application domains, our implementation of GuardiaNN on STM32MP157C-DK2 development board achieves a geomean speedup of 15.3x and a geomean energy efficiency improvement of 15.2x over a baseline secure DNN framework which employs demand-paged SRAM to secure sensitive data."}}
{"id": "0d5o8j1DQJH", "cdate": 1640995200000, "mdate": 1667493017688, "content": {"title": "It's All In the Teacher: Zero-Shot Quantization Brought Closer to the Teacher", "abstract": "Model quantization is considered as a promising method to greatly reduce the resource requirements of deep neural networks. To deal with the performance drop induced by quantization errors, a popular method is to use training data to fine-tune quantized networks. In real-world environments, however, such a method is frequently infeasible because training data is unavailable due to security, privacy, or confidentiality concerns. Zero-shot quantization addresses such problems, usually by taking information from the weights of a full-precision teacher network to compensate the performance drop of the quantized networks. In this paper, we first analyze the loss surface of state-of-the-art zero-shot quantization techniques and provide several findings. In contrast to usual knowledge distillation problems, zero-shot quantization often suffers from 1) the difficulty of optimizing multiple loss terms together, and 2) the poor generalization capability due to the use of synthetic samples. Furthermore, we observe that many weights fail to cross the rounding threshold during training the quantized networks even when it is necessary to do so for better performance. Based on the observations, we propose AIT, a simple yet powerful technique for zero-shot quantization, which addresses the aforementioned two problems in the following way: AIT i) uses a KL distance loss only without a cross-entropy loss, and ii) manipulates gradients to guarantee that a certain portion of weights are properly updated after crossing the rounding thresholds. Experiments show that AIT outperforms the performance of many existing methods by a great margin, taking over the overall state-of-the-art position in the field."}}
{"id": "e1GzwU4W2Kh", "cdate": 1632875505740, "mdate": null, "content": {"title": "ConCoDE: Hard-constrained Differentiable Co-Exploration Method for Neural Architectures and Hardware Accelerators", "abstract": "While DNNs achieve over-human performances in a number of areas, it is often accompanied by the skyrocketing computational costs. \nCo-exploration of an optimal neural architecture and its hardware accelerator is an approach of rising interest which addresses the computational cost problem, especially in low-profile systems (e.g., embedded, mobile). \nThe difficulty of having to search the large co-exploration space is often addressed by adopting the idea of differentiable neural architecture search. \nDespite the superior search efficiency of the differentiable co-exploration, it faces a critical challenge of not being able to systematically satisfy hard constraints, such as frame rate or power budget.\nTo handle the hard constraint problem of differentiable co-exploration, we propose ConCoDE,  \nwhich searches for hard-constrained solutions without compromising the global design objectives.\nBy manipulating the gradients in the interest of the given hard constraint, high-quality solutions satisfying the constraint can be obtained.\nExperimental results show that ConCoDE is able to meet the constraints even in tight conditions. \nWe also show that the solutions searched by ConCoDE exhibit high quality compared to those searched without any constraint. "}}
{"id": "ejo1_Weiart", "cdate": 1621629802022, "mdate": null, "content": {"title": "Qimera: Data-free Quantization with Synthetic Boundary Supporting Samples", "abstract": "Model quantization is known as a promising method to compress deep neural networks, especially for inferences on lightweight mobile or edge devices. \nHowever, model quantization usually requires access to the original training data to maintain the accuracy of the full-precision models, which is often infeasible in real-world scenarios for security and privacy issues.\nA popular approach to perform quantization without access to the original data is to use synthetically generated samples, based on batch-normalization statistics or adversarial learning.\nHowever, the drawback of such approaches is that they primarily rely on random noise input to the generator to attain diversity of the synthetic samples. \nWe find that this is often insufficient to capture the distribution of the original data, especially around the decision boundaries.\nTo this end, we propose Qimera, a method that uses superposed latent embeddings to generate synthetic boundary supporting samples.\nFor the superposed embeddings to better reflect the original distribution, we also propose using an additional disentanglement mapping layer and extracting information from the full-precision model.\nThe experimental results show that Qimera achieves state-of-the-art performances for various settings on data-free quantization. \nCode is available at https://github.com/iamkanghyunchoi/qimera."}}
