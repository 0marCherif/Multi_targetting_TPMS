{"id": "WNKaEKnHV2", "cdate": 1696226600936, "mdate": 1696226600936, "content": {"title": "On Continuity of Robust and Accurate Classifiers", "abstract": "The reliability of a learning model is key to the\nsuccessful deployment of machine learning in various applications. Creating a robust model, particularly one unaffected by\nadversarial attacks, requires a comprehensive understanding of\nthe adversarial examples phenomenon. However, it is difficult to\ndescribe the phenomenon due to the complicated nature of the\nproblems in machine learning. It has been shown that adversarial\ntraining can improve the robustness of the hypothesis. However,\nthis improvement comes at the cost of decreased performance on\nnatural samples. Hence, it has been suggested that robustness\nand accuracy of a hypothesis are at odds with each other.\nIn this paper, we put forth the alternative proposal that it\nis the continuity of a hypothesis that is incompatible with its\nrobustness and accuracy. In other words, a continuous function\ncannot effectively learn the optimal robust hypothesis. To this\nend, we will introduce a framework for a rigorous study of\nharmonic and holomorphic hypothesis in learning theory terms\nand provide empirical evidence that continuous hypotheses does\nnot perform as well as discontinuous hypotheses in some common\nmachine learning tasks. From a practical point of view, our\nresults suggests that a robust and accurate learning rule would\ntrain different continuous hypotheses for different regions of the\ndomain. From a theoretical perspective, our analysis explains\nthe adversarial examples phenomenon as a conflict between the\ncontinuity of a sequence of functions and its uniform convergence\nto a discontinuous function."}}
{"id": "ttnf-Wibn2R", "cdate": 1663850473428, "mdate": null, "content": {"title": "An Analytic Framework for Robust Training of Differentiable Hypothesis", "abstract": "The reliability of a learning model is key to the successful deployment of machine learning in various industries. Creating a robust model, particularly one unaffected by adversarial attacks, requires a comprehensive understanding of the adversarial examples phenomenon. However, it is difficult to describe the phenomenon due to the complicated nature of the problems in machine learning. Consequently, many studies investigate the phenomenon by proposing a simplified model of how adversarial examples occur and validate it by predicting some aspect of the phenomenon. While these studies cover many different characteristics of the adversarial examples, they have not reached a holistic approach to the geometric and analytic modeling of the phenomenon. We observe the phenomenon in many applications of machine learning, and its effects seems to be independent of the choice of the hypothesis class. In this paper, we propose a formalization of robustness in learning theoretic terms and give a geometrical description of the phenomenon in analytic classifiers. We then utilize the proposal to devise a robust classification learning rule for differentiable hypothesis classes and showcase our framework on synthetic and real-world data."}}
{"id": "XZri0r-fJ6S", "cdate": 1640995200000, "mdate": 1681671650755, "content": {"title": "An Analytic Framework for Robust Training of Artificial Neural Networks", "abstract": "The reliability of a learning model is key to the successful deployment of machine learning in various industries. Creating a robust model, particularly one unaffected by adversarial attacks, requires a comprehensive understanding of the adversarial examples phenomenon. However, it is difficult to describe the phenomenon due to the complicated nature of the problems in machine learning. Consequently, many studies investigate the phenomenon by proposing a simplified model of how adversarial examples occur and validate it by predicting some aspect of the phenomenon. While these studies cover many different characteristics of the adversarial examples, they have not reached a holistic approach to the geometric and analytic modeling of the phenomenon. This paper propose a formal framework to study the phenomenon in learning theory and make use of complex analysis and holomorphicity to offer a robust learning rule for artificial neural networks. With the help of complex analysis, we can effortlessly move between geometric and analytic perspectives of the phenomenon and offer further insights on the phenomenon by revealing its connection with harmonic functions. Using our model, we can explain some of the most intriguing characteristics of adversarial examples, including transferability of adversarial examples, and pave the way for novel approaches to mitigate the effects of the phenomenon."}}
{"id": "Hfw5Q2Zn1w", "cdate": 1632875455490, "mdate": null, "content": {"title": "Modeling and Eliminating Adversarial Examples using Function Theory of Several Complex Variables", "abstract": "The reliability of a learning model is key to the successful deployment of machine learning in various industries. Training a robust model, unaffected by adversarial attacks, requires a comprehensive understanding of the adversarial examples phenomenon. This paper presents a model and a solution for the existence and transfer of adversarial examples in analytic hypotheses. Grounded in the function theory of several complex variables, we propose the class of complex-valued holomorphic hypotheses as a natural way to represent the submanifold of the samples and the decision boundary simultaneously. To describe the mechanism in which the adversarial examples occur and transfer, we specialize the definitions of the optimal Bayes and the maximum margin classifiers to this class of hypotheses. The approach is validated initially on both synthetic and real-world classification problems using polynomials. Backed by theoretical and experimental results, we believe the analysis to apply to other classes of analytic hypotheses such as neural networks."}}
{"id": "wB_jgC44xx9", "cdate": 1577836800000, "mdate": 1675281451856, "content": {"title": "Towards Explaining Adversarial Examples Phenomenon in Artificial Neural Networks", "abstract": "In this paper, we study the adversarial examples existence and adversarial training from the standpoint of convergence and provide evidence that pointwise convergence in ANNs can explain these observations. The main contribution of our proposal is that it relates the objective of the evasion attacks and adversarial training with concepts already defined in learning theory. Also, we extend and unify some of the other proposals in the literature and provide alternative explanations on the observations made in those proposals. Through different experiments, we demonstrate that the framework is valuable in the study of the phenomenon and is applicable to real-world problems."}}
