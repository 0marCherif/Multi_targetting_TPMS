{"id": "HMDJEn-vmb1", "cdate": 1677628800000, "mdate": 1681312749486, "content": {"title": "MetaDrive: Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning", "abstract": ""}}
{"id": "5ZKyH_ijp2f", "cdate": 1672531200000, "mdate": 1680937992935, "content": {"title": "Two-Stage Constrained Actor-Critic for Short Video Recommendation", "abstract": ""}}
{"id": "O5rKg7IRQIO", "cdate": 1663850540265, "mdate": null, "content": {"title": "Guarded Policy Optimization with Imperfect Online Demonstrations", "abstract": "The Teacher-Student Framework (TSF) is a reinforcement learning setting where a teacher agent guards the training of a student agent by intervening and providing online demonstrations. Assuming optimal, the teacher policy has the perfect timing and capability to intervene in the learning process of the student agent, providing safety guarantee and exploration guidance. Nevertheless, in many real-world settings it is expensive or even impossible to obtain a well-performing teacher policy. In this work, we relax the assumption of a well-performing teacher and develop a new method that can incorporate arbitrary teacher policies with modest or inferior performance. We instantiate an Off-Policy Reinforcement Learning algorithm, termed Teacher-Student Shared Control (TS2C), which incorporates teacher intervention based on trajectory-based value estimation. Theoretical analysis validates that the proposed TS2C algorithm attains efficient exploration and substantial safety guarantee without being affected by the teacher's own performance. Experiments on various continuous control tasks show that our method can exploit teacher policies at different performance levels while maintaining a low training cost. Moreover, the student policy surpasses the imperfect teacher policy in terms of higher accumulated reward in held-out testing environments. Code is available at https://metadriverse.github.io/TS2C."}}
{"id": "ueLzUF_B4F", "cdate": 1640995200000, "mdate": 1680937993145, "content": {"title": "PrefRec: Preference-based Recommender Systems for Reinforcing Long-term User Engagement", "abstract": ""}}
{"id": "Ba3odanehCw", "cdate": 1621629781374, "mdate": null, "content": {"title": "Regret Minimization Experience Replay in Off-Policy Reinforcement Learning", "abstract": "In reinforcement learning, experience replay stores past samples for further reuse. Prioritized sampling is a promising technique to better utilize these samples. Previous criteria of prioritization include TD error, recentness and corrective feedback, which are mostly heuristically designed. In this work, we start from the regret minimization objective, and obtain an optimal prioritization strategy for Bellman update that can directly maximize the return of the policy. The theory suggests that data with higher hindsight TD error, better on-policiness and more accurate Q value should be assigned with higher weights during sampling. Thus most previous criteria only consider this strategy partially. We not only provide theoretical justifications for previous criteria, but also propose two new methods to compute the prioritization weight, namely ReMERN and ReMERT. ReMERN learns an error network, while ReMERT exploits the temporal ordering of states. Both methods outperform previous prioritized sampling algorithms in challenging RL benchmarks, including MuJoCo, Atari and Meta-World."}}
{"id": "5AixAJweEyC", "cdate": 1621629781374, "mdate": null, "content": {"title": "Regret Minimization Experience Replay in Off-Policy Reinforcement Learning", "abstract": "In reinforcement learning, experience replay stores past samples for further reuse. Prioritized sampling is a promising technique to better utilize these samples. Previous criteria of prioritization include TD error, recentness and corrective feedback, which are mostly heuristically designed. In this work, we start from the regret minimization objective, and obtain an optimal prioritization strategy for Bellman update that can directly maximize the return of the policy. The theory suggests that data with higher hindsight TD error, better on-policiness and more accurate Q value should be assigned with higher weights during sampling. Thus most previous criteria only consider this strategy partially. We not only provide theoretical justifications for previous criteria, but also propose two new methods to compute the prioritization weight, namely ReMERN and ReMERT. ReMERN learns an error network, while ReMERT exploits the temporal ordering of states. Both methods outperform previous prioritized sampling algorithms in challenging RL benchmarks, including MuJoCo, Atari and Meta-World."}}
