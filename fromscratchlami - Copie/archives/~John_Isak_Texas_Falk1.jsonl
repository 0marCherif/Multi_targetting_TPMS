{"id": "emEuzFbOYxA", "cdate": 1674045815641, "mdate": 1674045815641, "content": {"title": "Robust Meta-Representation Learning via Global Label Inference and Classification", "abstract": "Few-shot learning (FSL) is a central problem in meta-learning, where learners must efficiently learn\nfrom few labeled examples. Within FSL, feature pre-training has recently become an increasingly\npopular strategy to significantly improve generalization performance. However, the contribution\nof pre-training is often overlooked and understudied, with limited theoretical understanding of\nits impact on meta-learning performance. Further, pre-training requires a consistent set of global\nlabels shared across training tasks, which may be unavailable in practice. In this work, we address\nthe above issues by first showing the connection between pre-training and meta-learning. We\ndiscuss why pre-training yields more robust meta-representation and connect the theoretical\nanalysis to existing works and empirical results. Secondly, we introduce Meta Label Learning\n(MeLa), a novel meta-learning algorithm that learns task relations by inferring global labels across\ntasks. This allows us to exploit pre-training for FSL even when global labels are unavailable or\nill-defined. Lastly, we introduce an augmented pre-training procedure that further improves the\nlearned meta-representation. Empirically, MeLa outperforms existing methods across a diverse\nrange of benchmarks, in particular under a more challenging setting where the number of training\ntasks is limited and labels are task-specific. We also provide extensive ablation study to highlight\nits key properties."}}
{"id": "lfe1CdzuXBJ", "cdate": 1652737753139, "mdate": null, "content": {"title": "Group Meritocratic Fairness in Linear Contextual Bandits", "abstract": "We study the linear contextual bandit problem where an agent has to select one candidate from a pool and each candidate belongs to a sensitive group. In this setting, candidates' rewards may not be directly comparable between groups, for example when the agent is an employer hiring candidates from different ethnic groups and some groups have a lower reward due to discriminatory bias and/or social injustice. We propose a notion of fairness that states that the agent's policy is fair when it selects a candidate with highest relative rank, \nwhich measures how good the reward is when compared to candidates from the same group. This is a very strong notion of fairness, since the relative rank is not directly observed by the agent and depends on the underlying reward model and on the distribution of rewards. Thus we study the problem of learning a policy which approximates a fair policy under the condition that the contexts are independent between groups and the distribution of rewards of each group is absolutely continuous. In particular, we design a greedy policy which at each round constructs a ridge regression estimate from the observed context-reward pairs, and then computes an estimate of the relative rank of each candidate using the empirical cumulative distribution function. We prove that, despite its simplicity and the lack of an initial exploration phase, the greedy policy achieves, up to log factors and with high probability, a fair pseudo-regret of order $\\sqrt{dT}$ after $T$ rounds, where $d$ is the dimension of the context vectors. The policy also satisfies demographic parity at each round when averaged over all possible information available before the selection. Finally, we use simulated settings and experiments on the US census data to show that our policy achieves sub-linear fair pseudo-regret also in practice."}}
{"id": "rNgqwPUsqgq", "cdate": 1646077535324, "mdate": null, "content": {"title": "Implicit kernel meta-learning using kernel integral forms", "abstract": "Meta-learning algorithms have made significant progress in the context of meta-learning for image classification but less attention has been given to the regression setting. In this paper we propose to learn the probability distribution representing a random feature kernel that we wish to use within kernel ridge regression (KRR). We introduce two instances of this meta-learning framework, learning a neural network pushforward for a translation-invariant kernel and an affine pushforward for a neural network random feature kernel, both mapping from a Gaussian latent distribution. We learn the parameters of the pushforward by minimizing a meta-loss associated to the KRR objective. Since the resulting kernel does not admit an analytical form, we adopt a random feature sampling approach to approximate it. We call the resulting method Implicit Kernel Meta-Learning (IKML). We derive a meta-learning bound for IKML, which shows the role played by the number of tasks $T$, the task sample size $n$, and the number of random features $M$. In particular the bound implies that $M$ can be the chosen independently of $T$ and only mildly dependent on $n$. We introduce one synthetic and two real-world meta-learning regression benchmark datasets. Experiments on these datasets show that IKML performs best or close to best when compared against competitive meta-learning methods."}}
{"id": "u6GE3BJV7gs", "cdate": 1640995200000, "mdate": 1683708450000, "content": {"title": "Group Meritocratic Fairness in Linear Contextual Bandits", "abstract": "We study the linear contextual bandit problem where an agent has to select one candidate from a pool and each candidate belongs to a sensitive group. In this setting, candidates' rewards may not be directly comparable between groups, for example when the agent is an employer hiring candidates from different ethnic groups and some groups have a lower reward due to discriminatory bias and/or social injustice. We propose a notion of fairness that states that the agent's policy is fair when it selects a candidate with highest relative rank, which measures how good the reward is when compared to candidates from the same group. This is a very strong notion of fairness, since the relative rank is not directly observed by the agent and depends on the underlying reward model and on the distribution of rewards. Thus we study the problem of learning a policy which approximates a fair policy under the condition that the contexts are independent between groups and the distribution of rewards of each group is absolutely continuous. In particular, we design a greedy policy which at each round constructs a ridge regression estimate from the observed context-reward pairs, and then computes an estimate of the relative rank of each candidate using the empirical cumulative distribution function. We prove that, despite its simplicity and the lack of an initial exploration phase, the greedy policy achieves, up to log factors and with high probability, a fair pseudo-regret of order $\\sqrt{dT}$ after $T$ rounds, where $d$ is the dimension of the context vectors. The policy also satisfies demographic parity at each round when averaged over all possible information available before the selection. Finally, we use simulated settings and experiments on the US census data to show that our policy achieves sub-linear fair pseudo-regret also in practice."}}
{"id": "XJdoGmcrXp", "cdate": 1640995200000, "mdate": 1683708449997, "content": {"title": "Group Meritocratic Fairness in Linear Contextual Bandits", "abstract": "We study the linear contextual bandit problem where an agent has to select one candidate from a pool and each candidate belongs to a sensitive group. In this setting, candidates' rewards may not be directly comparable between groups, for example when the agent is an employer hiring candidates from different ethnic groups and some groups have a lower reward due to discriminatory bias and/or social injustice. We propose a notion of fairness that states that the agent's policy is fair when it selects a candidate with highest relative rank, which measures how good the reward is when compared to candidates from the same group. This is a very strong notion of fairness, since the relative rank is not directly observed by the agent and depends on the underlying reward model and on the distribution of rewards. Thus we study the problem of learning a policy which approximates a fair policy under the condition that the contexts are independent between groups and the distribution of rewards of each group is absolutely continuous. In particular, we design a greedy policy which at each round constructs a ridge regression estimate from the observed context-reward pairs, and then computes an estimate of the relative rank of each candidate using the empirical cumulative distribution function. We prove that, despite its simplicity and the lack of an initial exploration phase, the greedy policy achieves, up to log factors and with high probability, a fair pseudo-regret of order $\\sqrt{dT}$ after $T$ rounds, where $d$ is the dimension of the context vectors. The policy also satisfies demographic parity at each round when averaged over all possible information available before the selection. Finally, we use simulated settings and experiments on the US census data to show that our policy achieves sub-linear fair pseudo-regret also in practice."}}
{"id": "6u5j56lgKa", "cdate": 1640995200000, "mdate": 1681516643259, "content": {"title": "Robust Meta-Representation Learning via Global Label Inference and Classification", "abstract": ""}}
{"id": "5JuOHhSC75l", "cdate": 1640995200000, "mdate": 1683708450013, "content": {"title": "Implicit kernel meta-learning using kernel integral forms", "abstract": "Meta-learning algorithms have made significant progress in the context of meta-learning for image classification but less attention has been given to the regression setting. In this paper we propos..."}}
