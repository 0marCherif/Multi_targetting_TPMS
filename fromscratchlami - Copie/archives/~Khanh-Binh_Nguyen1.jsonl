{"id": "yUd724oBq_", "cdate": 1672531200000, "mdate": 1695826296538, "content": {"title": "Boosting Semi-Supervised Learning by bridging high and low-confidence predictions", "abstract": "Pseudo-labeling is a crucial technique in semi-supervised learning (SSL), where artificial labels are generated for unlabeled data by a trained model, allowing for the simultaneous training of labeled and unlabeled data in a supervised setting. However, several studies have identified three main issues with pseudo-labeling-based approaches. Firstly, these methods heavily rely on predictions from the trained model, which may not always be accurate, leading to a confirmation bias problem. Secondly, the trained model may be overfitted to easy-to-learn examples, ignoring hard-to-learn ones, resulting in the \\textit{\"Matthew effect\"} where the already strong become stronger and the weak weaker. Thirdly, most of the low-confidence predictions of unlabeled data are discarded due to the use of a high threshold, leading to an underutilization of unlabeled data during training. To address these issues, we propose a new method called ReFixMatch, which aims to utilize all of the unlabeled data during training, thus improving the generalizability of the model and performance on SSL benchmarks. Notably, ReFixMatch achieves 41.05\\% top-1 accuracy with 100k labeled examples on ImageNet, outperforming the baseline FixMatch and current state-of-the-art methods."}}
{"id": "-5PVFDBrMzQ", "cdate": 1672531200000, "mdate": 1695826296538, "content": {"title": "EUNNet: Efficient UN-Normalized Convolution Layer for Stable Training of Deep Residual Networks Without Batch Normalization Layer", "abstract": "Batch Normalization (BN) is an essential component of the Deep Neural Networks (DNNs) architectures. It helps improve stability, convergence, and generalization. However, studies are showing that BN might introduce several concerns. Although there are methods for training DNNs without BN using proper weight initialization, they require several learnable scalars or accurate fine-tuning to the training hyperparameters. As a result, in this study, we aim to stabilize the training process of un-normalized networks without using proper weight initialization and to minimize the hyperparameters fine-tuning step. We propose EUNConv, an Efficient UN-normalized Convolutional layer, which helps train un-normalized Deep Residual Networks (ResNets) by using hyperparameters of the normalized networks. Furthermore, we introduce Efficient UN-normalized Neural Network (EUNNet), which replaces all of the conventional convolutional layers of ResNets with our proposed EUNConv. Experimental results show that the proposed EUNNet achieves the same or even better performance than previous methods in various tasks: image recognition, object detection, and segmentation. In particular, EUNNet requires less fine-tuning and less sensitivity to hyperparameters than previous methods."}}
{"id": "uk1GJC-BF30", "cdate": 1640995200000, "mdate": 1684124360606, "content": {"title": "Checkerboard Dropout: A Structured Dropout With Checkerboard Pattern for Convolutional Neural Networks", "abstract": "Dropout is adopted in many state-of-the-art Deep Neural Networks (DNNs) to ease the overfitting problem by randomly removing features from feature maps. However, previous studies show the limitations of dropout application to Convolutional Neural Networks (CNNs) due to an increase in spatial correlation of the zeroed-out values in the output feature maps, which limits the generalization and performance of the network. Recently, DropBlock has been successfully applied as an efficient structured dropout to mitigate the spatial correlation problem by dropping a continuous region and to allay the randomness of the conventional dropout. However, DropBlock does not completely remove the randomness. The reason is because DropBlock still randomly chooses the center points to generate regions to drops. This paper proposes a novel method, Checkerboard Dropout, to handle the randomness of the conventional dropout as well as DropBlock and to further reduce the spatial correlation between the zeroed-out values in output feature maps. The proposed method is evaluated with large-scale image classification and object detection tasks. Experimental results show that the proposed Checkerboard Dropout improves the top-1 accuracy by 2.17% for a baseline of ResNet-50 on ImageNet dataset, 1.44% AP for a baseline of RetinaNet with ResNet-50 + FPN backbone and outperformed the previous approach - DropBlock - 0.3% top-1 accuracy for ImageNet dataset and 0.64% AP for MS COCO dataset. Furthermore, the proposed Checkerboard Dropout also improves the model generalization, localization and segmentation capability toward complex objects through Grad-CAM and instance segmentation visualizations."}}
