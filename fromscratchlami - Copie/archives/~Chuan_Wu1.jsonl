{"id": "3MZnNARib5", "cdate": 1652737361307, "mdate": null, "content": {"title": "SAPipe: Staleness-Aware Pipeline for Data Parallel DNN Training", "abstract": "Data parallelism across multiple machines is widely adopted for accelerating distributed deep learning, but it is hard to achieve linear speedup due to the heavy communication. In this paper, we propose SAPipe, a performant system that pushes the training speed of data parallelism to its fullest extent. By introducing partial staleness, the communication overlaps the computation with minimal staleness in SAPipe. To mitigate additional problems incurred by staleness, SAPipe adopts staleness compensation techniques including weight prediction and delay compensation with provably lower error bounds. Additionally, SAPipe presents an algorithm-system co-design with runtime optimization to minimize system overhead for the staleness training pipeline and staleness compensation. We have implemented SAPipe in the BytePS framework, compatible to both TensorFlow and PyTorch. Our experiments show that SAPipe achieves up to 157% speedups over BytePS (non-stale), and outperforms PipeSGD in accuracy by up to 13.7%."}}
{"id": "8qQ48aMXR_g", "cdate": 1632875519253, "mdate": null, "content": {"title": "On Locality in Graph Learning via Graph Neural Network", "abstract": "Theoretical understanding on the learning process of graph neural network (GNN) has been lacking. The common practice in GNN training is to adapt strategies from other machine learning families, despite the striking differences between learning non-graph and graph-structured data. This results in unstable learning performance (e.g., accuracy) for GNN. In this paper, we study how the training set in the input graph effects the performance of GNN.  Combining the topology awareness of GNN and the dependence (topology) of data samples, we formally derive a structural relation between the performance of GNN and the coverage of the training set in the graph. More specifically, the distance of the training set to the rest of the vertexes in the graph is negatively correlated to the learning outcome of GNN. We further validate our theory on different graph data sets with extensive experiments. Using the derived result as a guidance, we also investigate the initial data labelling problem in active learning of GNN, and show that locality-aware data labelling substantially outperforms the prevailing random sampling approach. "}}
{"id": "Bm4-m1fx_TS", "cdate": 1546300800000, "mdate": null, "content": {"title": "Learning Resource Allocation and Pricing for Cloud Profit Maximization.", "abstract": "Cloud computing has been widely adopted to support various computation services. A fundamental problem faced by cloud providers is how to efficiently allocate resources upon user requests and price the resource usage, in order to maximize resource efficiency and hence provider profit. Existing studies establish detailed performance models of cloud resource usage, and propose offline or online algorithms to decide allocation and pricing. Differently, we adopt a blackbox approach, and leverage model-free Deep Reinforcement Learning (DRL) to capture dynamics of cloud users and better characterize inherent connections between an optimal allocation/pricing policy and the states of the dynamic cloud system. The goal is to learn a policy that maximizes net profit of the cloud provider through trial and error, which is better than decisions made on explicit performance models. We combine long short-term memory (LSTM) units with fully-connected neural networks in our DRL to deal with online user arrivals, and adjust the output and update methods of basic DRL algorithms to address both resource allocation and pricing. Evaluation based on real-world datasets shows that our DRL approach outperforms basic DRL algorithms and state-of-theart white-box online cloud resource allocation/pricing algorithms significantly, in terms of both profit and the number of accepted users."}}
{"id": "HJSA_e1AW", "cdate": 1518730188512, "mdate": null, "content": {"title": "Normalized Direction-preserving Adam", "abstract": "Optimization algorithms for training deep models not only affects the convergence rate and stability of the training process, but are also highly related to the generalization performance of trained models. While adaptive algorithms, such as Adam and RMSprop, have shown better optimization performance than stochastic gradient descent (SGD) in many scenarios, they often lead to worse generalization performance than SGD, when used for training deep neural networks (DNNs). In this work, we identify two problems regarding the direction and step size for updating the weight vectors of hidden units, which may degrade the generalization performance of Adam. As a solution, we propose the normalized direction-preserving Adam (ND-Adam) algorithm, which controls the update direction and step size more precisely, and thus bridges the generalization gap between Adam and SGD. Following a similar rationale, we further improve the generalization performance in classification tasks by regularizing the softmax logits. By bridging the gap between SGD and Adam, we also shed some light on why certain optimization algorithms generalize better than others."}}
{"id": "HkZgGUW_ZS", "cdate": 1388534400000, "mdate": null, "content": {"title": "An optimization framework for entity recognition and disambiguation", "abstract": "We present a system for entity recognition and disambiguation (ERD) in short text, aiming at identifying all text fragments referring to an entity contained in Freebase. The task is organized in two steps. Given a short text the first step is discovering text fragments which possibly refer to an entity.Since multiple entities may share common mention, identifying which entity the mention is referring to in the given short text is necessary. Our system integrates three kinds of features: mention-entity similarity, entity-entity similarity and context-mention entity similarity. By considering every possible combination of mention-entity pair, we select the one with highest confidence score. An implementation of our system is described, along with our evaluation results. Experiments show that the proposed features improve the performance to a certain extent."}}
