{"id": "rKdnKUMipww", "cdate": 1692964494414, "mdate": 1692964494414, "content": {"title": "Learning Transformations To Reduce the Geometric Shift in Object Detection", "abstract": "The performance of modern object detectors drops when\nthe test distribution differs from the training one. Most of\nthe methods that address this focus on object appearance\nchanges caused by, e.g., different illumination conditions,\nor gaps between synthetic and real images. Here, by contrast, we tackle geometric shifts emerging from variations in\nthe image capture process, or due to the constraints of the\nenvironment causing differences in the apparent geometry\nof the content itself. We introduce a self-training approach\nthat learns a set of geometric transformations to minimize\nthese shifts without leveraging any labeled data in the new\ndomain, nor any information about the cameras. We evaluate our method on two different shifts, i.e., a camera\u2019s field\nof view (FoV) change and a viewpoint change. Our results\nevidence that learning geometric transformations helps detectors to perform better in the target domains."}}
{"id": "1CZm6VKzNOb", "cdate": 1692964417607, "mdate": 1692964417607, "content": {"title": "CLIP the Gap: A Single Domain Generalization Approach for Object Detection", "abstract": "Single Domain Generalization (SDG) tackles the problem of training a model on a single source domain so that\nit generalizes to any unseen target domain. While this has\nbeen well studied for image classification, the literature on\nSDG object detection remains almost non-existent. To address the challenges of simultaneously learning robust object localization and representation, we propose to leverage\na pre-trained vision-language model to introduce semantic\ndomain concepts via textual prompts. We achieve this via\na semantic augmentation strategy acting on the features extracted by the detector backbone, as well as a text-based\nclassification loss. Our experiments evidence the benefits of\nour approach, outperforming by 10% the only existing SDG\nobject detection method, Single-DGOD [52], on their own\ndiverse weather-driving benchmark."}}
{"id": "ojShWAXZDo", "cdate": 1672531200000, "mdate": 1679929420497, "content": {"title": "Two-level Data Augmentation for Calibrated Multi-view Detection", "abstract": ""}}
{"id": "oI2mJMN8i1", "cdate": 1672531200000, "mdate": 1679929420478, "content": {"title": "CLIP the Gap: A Single Domain Generalization Approach for Object Detection", "abstract": ""}}
{"id": "mg_90AMy4Kv", "cdate": 1672531200000, "mdate": 1679929420359, "content": {"title": "Learning Transformations To Reduce the Geometric Shift in Object Detection", "abstract": ""}}
{"id": "el_QOB_CFj", "cdate": 1672531200000, "mdate": 1679929420380, "content": {"title": "Multi-view Tracking Using Weakly Supervised Human Motion Prediction", "abstract": ""}}
{"id": "S7YE6Czxupr", "cdate": 1546300800000, "mdate": null, "content": {"title": "SoDeep: A Sorting Deep Net to Learn Ranking Loss Surrogates.", "abstract": "Several tasks in machine learning are evaluated using non-differentiable metrics such as mean average precision or Spearman correlation. However, their non-differentiability prevents from using them as objective functions in a learning framework. Surrogate and relaxation methods exist but tend to be specific to a given metric. In the present work, we introduce a new method to learn approximations of such non-differentiable objective functions. Our approach is based on a deep architecture that approximates the sorting of arbitrary sets of scores. It is trained virtually for free using synthetic data. This sorting deep (SoDeep) net can then be combined in a plug-and-play manner with existing deep architectures. We demonstrate the interest of our approach in three different tasks that require ranking: Cross-modal text-image retrieval, multi-label image classification and visual memorability ranking. Our approach yields very competitive results on these three tasks, which validates the merit and the flexibility of SoDeep as a proxy for sorting operation in ranking-based losses."}}
{"id": "By48qR-_WB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Finding Beans in Burgers: Deep Semantic-Visual Embedding With Localization", "abstract": "Several works have proposed to learn a two-path neural network that maps images and texts, respectively, to a same shared Euclidean space where geometry captures useful semantic relationships. Such a multi-modal embedding can be trained and used for various tasks, notably image captioning. In the present work, we introduce a new architecture of this type, with a visual path that leverages recent space-aware pooling mechanisms. Combined with a textual path which is jointly trained from scratch, our semantic-visual embedding offers a versatile model. Once trained under the supervision of captioned images, it yields new state-of-the-art performance on cross-modal retrieval. It also allows the localization of new concepts from the embedding space into any input image, delivering state-of-the-art result on the visual grounding of phrases."}}
