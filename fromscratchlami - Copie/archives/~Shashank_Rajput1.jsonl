{"id": "oJb6kPIjOu", "cdate": 1695597703720, "mdate": 1695597703720, "content": {"title": "The expressive power of tuning only the normalization layers", "abstract": "Feature normalization transforms such as Batch and Layer-Normalization have become indispensable ingredients of state-of-the-art deep neural networks. Recent studies on fine-tuning large pretrained models indicate that just tuning the parameters of these affine transforms can achieve high accuracy for downstream tasks. These findings open the questions about the expressive power of tuning the normalization layers of frozen networks. In this work, we take the first step towards this question and show that for random ReLU networks, fine-tuning only its normalization layers can reconstruct any target network that is $O(\\sqrt{width})$ times smaller. We show that this holds even for randomly sparsified networks, under sufficient overparameterization, in agreement with prior empirical work."}}
{"id": "f2lX4uFQCLl", "cdate": 1675827743531, "mdate": null, "content": {"title": "LOOPED TRANSFORMERS AS PROGRAMMABLE COMPUTERS", "abstract": "We present a framework for using transformer networks as universal computers by programming them with specific weights and placing them in a loop. Our input sequence acts as a punchcard, consisting of instructions and memory for data read/writes. We demonstrate that a constant number of encoder layers can emulate basic computing blocks, including lexicographic operations, non-linear functions, function calls, program counters, and conditional branches.\nUsing this framework, we emulate a computer using a simple instruction-set architecture, which allows us to map iterative algorithms  to programs that   can be executed by a constant depth looped transformer network. We show how a single frozen transformer, instructed by its input, can  emulate a basic calculator, a basic linear algebra library, and even a full backpropagation, in-context learning algorithm. Our findings reveal the potential of transformer networks as programmable compute units and offer insight into the mechanics of attention."}}
{"id": "3V9tRSJLw8", "cdate": 1675191751925, "mdate": 1675191751925, "content": {"title": "Looped Transformers as Programmable Computers", "abstract": "We present a framework for using transformer networks as universal computers by programming them with specific weights and placing them in a loop. Our input sequence acts as a punchcard, consisting of instructions and memory for data read/writes. We demonstrate that a constant number of encoder layers can emulate basic computing blocks, including embedding edit operations, non-linear functions, function calls, program counters, and conditional branches.\nUsing these building blocks, we emulate a small instruction-set computer. This allows us to map iterative algorithms  to programs that   can be executed by a looped, 13-layer transformer. We show how this transformer, instructed by its input, can  emulate a basic calculator, a basic linear algebra library, and in-context learning algorithms that employ backpropagation. Our work highlights the versatility of the attention mechanism, and demonstrates that even shallow transformers can execute full-fledged, general-purpose programs."}}
{"id": "s_PJMEGIUfa", "cdate": 1652737465651, "mdate": null, "content": {"title": "LIFT: Language-Interfaced Fine-Tuning for Non-language Machine Learning Tasks", "abstract": "Fine-tuning pretrained language models (LMs) without making any architectural changes has become a norm for learning various language downstream tasks. However, for non-language downstream tasks, a common practice is to employ task-specific designs for input, output layers, and loss functions. For instance, it is possible to fine-tune an LM into an MNIST classifier by replacing the word embedding layer with an image patch embedding layer, the word token output layer with a 10-way output layer, and the word prediction loss with a 10-way classification loss, respectively. A natural question arises: Can LM fine-tuning solve non-language downstream tasks without changing the model architecture or loss function? To answer this, we propose Language-Interfaced Fine-Tuning (LIFT) and study its efficacy and limitations by conducting an extensive empirical study on a suite of non-language classification and regression tasks. LIFT does not make any changes to the model architecture or loss function, and it solely relies on the natural language interface, enabling \"no-code machine learning with LMs.\"  We find that LIFT performs comparably well across a wide range of low-dimensional classification and regression tasks, matching the performances of the best baselines in many cases, especially for the classification tasks. We also report experimental results on the fundamental properties of LIFT, including inductive bias, robustness, and sample complexity. We also analyze the effect of pretraining on LIFT and a few properties/techniques specific to LIFT, e.g., context-aware learning via appropriate prompting, calibrated predictions, data generation, and two-stage fine-tuning. Our code is available at https://github.com/UW-Madison-Lee-Lab/LanguageInterfacedFineTuning."}}
{"id": "LdlwbBP2mlq", "cdate": 1632875740859, "mdate": null, "content": {"title": "Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond", "abstract": "In distributed learning, local SGD (also known as federated averaging) and its simple baseline minibatch SGD are widely studied optimization methods. Most existing analyses of these methods assume independent and unbiased gradient estimates obtained via with-replacement sampling. In contrast, we study shuffling-based variants: minibatch and local Random Reshuffling, which draw stochastic gradients without replacement and are thus closer to practice. For smooth functions satisfying the Polyak-\u0141ojasiewicz condition, we obtain convergence bounds (in the large epoch regime) which show that these shuffling-based variants converge faster than their with-replacement counterparts. Moreover, we prove matching lower bounds showing that our convergence analysis is tight. Finally, we propose an algorithmic modification called synchronized shuffling that leads to convergence rates faster than our lower bounds in near-homogeneous settings."}}
{"id": "YiBa9HKTyXE", "cdate": 1632875692926, "mdate": null, "content": {"title": "Permutation-Based SGD: Is Random Optimal?", "abstract": "A recent line of ground-breaking results for permutation-based SGD  has corroborated a widely observed phenomenon: random permutations offer faster convergence than with-replacement sampling. However, is random optimal? We show that this depends heavily on what functions we are optimizing, and the convergence gap between optimal and random permutations can vary from exponential to nonexistent. We first show that for 1-dimensional strongly convex functions, with smooth second derivatives, there exist optimal permutations that offer exponentially faster convergence compared to random. However, for general strongly convex functions, random permutations are optimal. Finally, we show that for quadratic, strongly-convex functions, there are easy-to-construct permutations that lead to accelerated convergence compared to random. Our results suggest that a general convergence characterization of optimal permutations cannot capture the nuances of individual function classes, and can  mistakenly indicate that one cannot do much better than random."}}
{"id": "dFRbxGpNWw5", "cdate": 1621630028102, "mdate": null, "content": {"title": "An Exponential Improvement on the Memorization Capacity of Deep Threshold Networks", "abstract": "It is well known that modern deep neural networks are powerful enough to memorize datasets even when the labels have been randomized. Recently, Vershynin(2020) settled a long standing question by Baum(1988), proving that deep threshold networks can memorize $n$ points in $d$ dimensions using $\\widetilde{\\mathcal{O}}(e^{1/\\delta^2}+\\sqrt{n})$ neurons and $\\widetilde{\\mathcal{O}}(e^{1/\\delta^2}(d+\\sqrt{n})+n)$ weights, where $\\delta$ is the minimum distance between the points. In this work, we improve the dependence on $\\delta$ from exponential to almost linear, proving that $\\widetilde{\\mathcal{O}}(\\frac{1}{\\delta}+\\sqrt{n})$ neurons and $\\widetilde{\\mathcal{O}}(\\frac{d}{\\delta}+n)$ weights are sufficient. Our construction uses Gaussian random weights only in the first layer, while all the subsequent layers use binary or integer weights. We also prove new lower bounds by connecting memorization in neural networks to the purely geometric problem of separating $n$ points on a sphere using hyperplanes."}}
{"id": "Q7aQWvOSe-u", "cdate": 1609459200000, "mdate": null, "content": {"title": "Permutation-Based SGD: Is Random Optimal?", "abstract": "A recent line of ground-breaking results for permutation-based SGD has corroborated a widely observed phenomenon: random permutations offer faster convergence than with-replacement sampling. However, is random optimal? We show that this depends heavily on what functions we are optimizing, and the convergence gap between optimal and random permutations can vary from exponential to nonexistent. We first show that for 1-dimensional strongly convex functions, with smooth second derivatives, there exist permutations that offer exponentially faster convergence compared to random. However, for general strongly convex functions, random permutations are optimal. Finally, we show that for quadratic, strongly-convex functions, there are easy-to-construct permutations that lead to accelerated convergence compared to random. Our results suggest that a general convergence characterization of optimal permutations cannot capture the nuances of individual function classes, and can mistakenly indicate that one cannot do much better than random."}}
{"id": "qnZnfkfvoMq", "cdate": 1598835876157, "mdate": null, "content": {"title": "Attack of the Tails: Yes, You Really Can Backdoor Federated Learning", "abstract": "Due to its decentralized nature, Federated Learning (FL) lends itself to adversarial attacks in the form of backdoors during training. The goal of a backdoor is to corrupt the performance of the trained model on specific sub-tasks (e.g., by classifying green cars as frogs). A range of FL backdoor attacks have been introduced in the literature, but also methods to defend against them, and it is currently an open question whether FL systems can be tailored to be robust against backdoors. In this work, we provide evidence to the contrary. We first establish that, in the general case, robustness to backdoors implies model robustness to adversarial examples, a major open problem in itself. Furthermore, detecting the presence of a backdoor in a FL model is unlikely assuming first order oracles or polynomial time. We couple our theoretical results with a new family of backdoor attacks, which we refer to as edge-case backdoors. An edge-case backdoor forces a model to misclassify on seemingly easy inputs that are however unlikely to be part of the training, or test data, i.e., they live on the tail of the input distribution. We explain how these edge-case backdoors can lead to unsavory failures and may have serious repercussions on fairness, and exhibit that with careful tuning at the side of the adversary, one can insert them across a range of machine learning tasks (e.g., image classification, OCR, text prediction, sentiment analysis)."}}
{"id": "wz6wi2dfMn", "cdate": 1577836800000, "mdate": null, "content": {"title": "Closing the convergence gap of SGD without replacement", "abstract": "Stochastic gradient descent without replacement sampling is widely used in practice for model training. However, the vast majority of SGD analyses assumes data is sampled with replacement, and when the function minimized is strongly convex, an $\\mathcal{O}\\left(\\frac{1}{T}\\right)$ rate can be established when SGD is run for $T$ iterations. A recent line of breakthrough works on SGD without replacement (SGDo) established an $\\mathcal{O}\\left(\\frac{n}{T^2}\\right)$ convergence rate when the function minimized is strongly convex and is a sum of $n$ smooth functions, and an $\\mathcal{O}\\left(\\frac{1}{T^2}+\\frac{n^3}{T^3}\\right)$ rate for sums of quadratics. On the other hand, the tightest known lower bound postulates an $\\Omega\\left(\\frac{1}{T^2}+\\frac{n^2}{T^3}\\right)$ rate, leaving open the possibility of better SGDo convergence rates in the general case. In this paper, we close this gap and show that SGD without replacement achieves a rate of $\\mathcal{O}\\left(\\frac{1}{T^2}+\\frac{n^2}{T^3}\\right)$ when the sum of the functions is a quadratic, and offer a new lower bound of $\\Omega\\left(\\frac{n}{T^2}\\right)$ for strongly convex functions that are sums of smooth functions."}}
