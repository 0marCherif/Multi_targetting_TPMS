{"id": "L9UMeoeU2i", "cdate": 1675827739742, "mdate": null, "content": {"title": "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters", "abstract": "Chain-of-Thought (CoT) prompting, which encourages language models (LMs) to generate intermediate rationales for the final answer through in-context demonstrations, dramatically improves large LMs' ability to solve reasoning tasks. Despite its success, there is little understanding on what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance. In this paper, we show that prompting with invalid demonstrations affects little in CoT reasoning, achieving over 80-90% of the performance obtained using the original CoT under various metrics, while still generating coherent lines of reasoning during inference. Further experiments show that other aspects of the rationales, such as being relevant to the query and correctly ordering the reasoning steps, are the actual key to the effectiveness of CoT. Overall, these findings deepen our understanding of CoT prompting, while leading to new questions regarding large LMs\u2019 capability to learn to reason in context and reflections on benchmarking few-shot reasoning."}}
{"id": "oJ4NfoO9mow", "cdate": 1640995200000, "mdate": 1673980816937, "content": {"title": "DOM-LM: Learning Generalizable Representations for HTML Documents", "abstract": ""}}
{"id": "mM9xh53kAP", "cdate": 1640995200000, "mdate": 1673980816941, "content": {"title": "TURL: Table Understanding through Representation Learning", "abstract": ""}}
{"id": "fCjNxSrpMf", "cdate": 1640995200000, "mdate": 1673980816937, "content": {"title": "Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach", "abstract": ""}}
{"id": "Zu3HIPzyQC", "cdate": 1640995200000, "mdate": 1673980625841, "content": {"title": "What do LLMs Know about Financial Markets? A Case Study on Reddit Market Sentiment Analysis", "abstract": ""}}
{"id": "OIKop7UC4u", "cdate": 1640995200000, "mdate": 1664914407341, "content": {"title": "Bootstrapping a User-Centered Task-Oriented Dialogue System", "abstract": "We present TacoBot, a task-oriented dialogue system built for the inaugural Alexa Prize TaskBot Challenge, which assists users in completing multi-step cooking and home improvement tasks. TacoBot is designed with a user-centered principle and aspires to deliver a collaborative and accessible dialogue experience. Towards that end, it is equipped with accurate language understanding, flexible dialogue management, and engaging response generation. Furthermore, TacoBot is backed by a strong search engine and an automated end-to-end test suite. In bootstrapping the development of TacoBot, we explore a series of data augmentation strategies to train advanced neural language processing models and continuously improve the dialogue experience with collected real conversations. At the end of the semifinals, TacoBot achieved an average rating of 3.55/5.0."}}
{"id": "IfOoDVLtxPT", "cdate": 1640995200000, "mdate": 1673980816941, "content": {"title": "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters", "abstract": ""}}
{"id": "0LUcrTebx4D", "cdate": 1640995200000, "mdate": 1673980816939, "content": {"title": "Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments", "abstract": ""}}
{"id": "zCM2fiQKBfZ", "cdate": 1609459200000, "mdate": 1639599267413, "content": {"title": "Structure-Grounded Pretraining for Text-to-SQL", "abstract": "Xiang Deng, Ahmed Hassan Awadallah, Christopher Meek, Oleksandr Polozov, Huan Sun, Matthew Richardson. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021."}}
{"id": "tjGvyk5JYO2", "cdate": 1609459200000, "mdate": 1639599267391, "content": {"title": "ReasonBERT: Pre-trained to Reason with Distant Supervision", "abstract": "Xiang Deng, Yu Su, Alyssa Lees, You Wu, Cong Yu, Huan Sun. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021."}}
