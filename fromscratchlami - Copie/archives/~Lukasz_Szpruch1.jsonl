{"id": "9hXskf1K7zQ", "cdate": 1664816293326, "mdate": null, "content": {"title": "TAPAS: a Toolbox for Adversarial Privacy Auditing of Synthetic Data", "abstract": "Personal data collected at scale promises to improve decision-making and accelerate innovation. However, sharing and using such data raises serious privacy concerns. A promising solution is to produce synthetic data, artificial records to share instead of real data. Since synthetic records are not linked to real persons, this intuitively prevents classical re-identification attacks. However, this is insufficient to protect privacy. We here present PrivE, a toolbox of attacks to evaluate synthetic data privacy under a wide range of scenarios. These attacks include generalizations of prior works and novel attacks. We also introduce a general framework for reasoning about privacy threats to synthetic data and showcase PrivE on several examples."}}
{"id": "VtlGqVzja48", "cdate": 1621630207480, "mdate": null, "content": {"title": "Identifiability in inverse reinforcement learning", "abstract": "Inverse reinforcement learning attempts to reconstruct the reward function in a Markov decision problem, using observations of agent actions. As already observed in Russell [1998] the problem is ill-posed, and the reward function is not identifiable, even under the presence of perfect information about optimal behavior. We provide a resolution to this non-identifiability for problems with entropy regularization. For a given environment, we fully characterize the reward functions leading to a given policy and demonstrate that, given demonstrations of actions for the same reward under two distinct discount factors, or under sufficiently different environments, the unobserved reward can be recovered up to a constant. We also give general necessary and sufficient conditions for reconstruction of time-homogeneous rewards on finite horizons, and for action-independent rewards, generalizing recent results of Kim et al. [2021] and Fu et al. [2018]."}}
{"id": "7x9Dy07fm0n", "cdate": 1577836800000, "mdate": null, "content": {"title": "Multi-level Monte Carlo methods for the approximation of invariant measures of stochastic differential equations", "abstract": "We develop a framework that allows the use of the multi-level Monte Carlo (MLMC) methodology (Giles in Acta Numer. 24:259\u2013328, 2015. https://doi.org/10.1017/S096249291500001X) to calculate expectations with respect to the invariant measure of an ergodic SDE. In that context, we study the (over-damped) Langevin equations with a strongly concave potential. We show that when appropriate contracting couplings for the numerical integrators are available, one can obtain a uniform-in-time estimate of the MLMC variance in contrast to the majority of the results in the MLMC literature. As a consequence, a root mean square error of $$\\mathcal {O}(\\varepsilon )$$ O(\u03b5) is achieved with $$\\mathcal {O}(\\varepsilon ^{-2})$$ O(\u03b5-2) complexity on par with Markov Chain Monte Carlo (MCMC) methods, which, however, can be computationally intensive when applied to large datasets. Finally, we present a multi-level version of the recently introduced stochastic gradient Langevin dynamics method (Welling and Teh, in: Proceedings of the 28th ICML, 2011) built for large datasets applications. We show that this is the first stochastic gradient MCMC method with complexity $$\\mathcal {O}(\\varepsilon ^{-2}|\\log {\\varepsilon }|^{3})$$ O(\u03b5-2|log\u03b5|3), in contrast to the complexity $$\\mathcal {O}(\\varepsilon ^{-3})$$ O(\u03b5-3) of currently available methods. Numerical experiments confirm our theoretical findings."}}
{"id": "iqJ1EOcW2y_", "cdate": 1546300800000, "mdate": null, "content": {"title": "Iterative Multilevel density estimation for McKean-Vlasov SDEs via projections", "abstract": "In this paper, we present a generic methodology for the efficient numerical approximation of the density function of the McKean-Vlasov SDEs. The weak error analysis for the projected process motivates us to combine the iterative Multilevel Monte Carlo method for McKean-Vlasov SDEs \\cite{szpruch2019} with non-interacting kernels and projection estimation of particle densities \\cite{belomestny2018projected}. By exploiting smoothness of the coefficients for McKean-Vlasov SDEs, in the best case scenario (i.e $C^{\\infty}$ for the coefficients), we obtain the complexity of order $O(\\epsilon^{-2}|\\log\\epsilon|^4)$ for the approximation of expectations and $O(\\epsilon^{-2}|\\log\\epsilon|^5)$ for density estimation."}}
{"id": "afECs98-N4U", "cdate": 1546300800000, "mdate": null, "content": {"title": "An Adaptive Euler-Maruyama Scheme for Stochastic Differential Equations with Discontinuous Drift and its Convergence Analysis", "abstract": "We study the strong approximation of stochastic differential equations with discontinuous drift coefficients and (possibly) degenerate diffusion coefficients. To account for the discontinuity of the drift coefficient we construct an adaptive step sizing strategy for the explicit Euler--Maruyama scheme. As a result, we obtain a numerical method which has---up to logarithmic terms---strong convergence order $1/2$ with respect to the average computational cost. We support our theoretical findings with several numerical examples."}}
{"id": "8PH5G_clAt", "cdate": 1546300800000, "mdate": null, "content": {"title": "On the geometry of Stein variational gradient descent", "abstract": "Bayesian inference problems require sampling or approximating high-dimensional probability distributions. The focus of this paper is on the recently introduced Stein variational gradient descent methodology, a class of algorithms that rely on iterated steepest descent steps with respect to a reproducing kernel Hilbert space norm. This construction leads to interacting particle systems, the mean-field limit of which is a gradient flow on the space of probability distributions equipped with a certain geometrical structure. We leverage this viewpoint to shed some light on the convergence properties of the algorithm, in particular addressing the problem of choosing a suitable positive definite kernel function. Our analysis leads us to considering certain nondifferentiable kernels with adjusted tails. We demonstrate significant performs gains of these in various numerical experiments."}}
{"id": "dWYnYfdE_g", "cdate": 1514764800000, "mdate": null, "content": {"title": "V-integrability, asymptotic stability and comparison property of explicit numerical schemes for non-linear SDEs", "abstract": "Advancing research. Creating connections."}}
{"id": "Sfv71tWwcjM", "cdate": 1514764800000, "mdate": null, "content": {"title": "Martingale Functional Control variates via Deep Learning", "abstract": "We develop several deep learning algorithms for approximating families of parametric PDE solutions. The proposed algorithms approximate solutions together with their gradients, which in the context of mathematical finance means that the derivative prices and hedging strategies are computed simulatenously. Having approximated the gradient of the solution one can combine it with a Monte-Carlo simulation to remove the bias in the deep network approximation of the PDE solution (derivative price). This is achieved by leveraging the Martingale Representation Theorem and combining the Monte Carlo simulation with the neural network. The resulting algorithm is robust with respect to quality of the neural network approximation and consequently can be used as a black-box in case only limited a priori information about the underlying problem is available. We believe this is important as neural network based algorithms often require fair amount of tuning to produce satisfactory results. The methods are empirically shown to work for high-dimensional problems (e.g. 100 dimensions). We provide diagnostics that shed light on appropriate network architectures."}}
{"id": "2Pt0ohrKx3I", "cdate": 1388534400000, "mdate": null, "content": {"title": "First order strong approximations of scalar SDEs defined in a domain", "abstract": "We are interested in strong approximations of one-dimensional SDEs which have non-Lipschitz coefficients and which take values in a domain. Under a set of general assumptions we derive an implicit scheme that preserves the domain of the SDEs and is strongly convergent with rate one. Moreover, we show that this general result can be applied to many SDEs we encounter in mathematical finance and bio-mathematics. We will demonstrate flexibility of our approach by analyzing classical examples of SDEs with sublinear coefficients (CIR, CEV models and Wright\u2013Fisher diffusion) and also with superlinear coefficients (3/2-volatility, A\u00eft-Sahalia model). Our goal is to justify an efficient Multilevel Monte Carlo method for a rich family of SDEs, which relies on good strong convergence properties."}}
{"id": "1H-OhA3oiPv", "cdate": 1356998400000, "mdate": null, "content": {"title": "Strong convergence and stability of implicit numerical methods for stochastic differential equations with non-globally Lipschitz continuous coefficients", "abstract": "We are interested in the strong convergence and almost sure stability of Euler\u2013Maruyama (EM) type approximations to the solutions of stochastic differential equations (SDEs) with non-linear and non-Lipschitzian coefficients. Motivation comes from finance and biology where many widely applied models do not satisfy the standard assumptions required for the strong convergence. In addition we examine the globally almost surely asymptotic stability in this non-linear setting for EM type schemes. In particular, we present a stochastic counterpart of the discrete LaSalle principle from which we deduce stability properties for numerical methods."}}
