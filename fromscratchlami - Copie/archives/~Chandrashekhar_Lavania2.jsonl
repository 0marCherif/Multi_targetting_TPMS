{"id": "uR727MXpd0T", "cdate": 1640995200000, "mdate": 1664560187867, "content": {"title": "Enhancing Contrastive Learning with Temporal Cognizance for Audio-Visual Representation Generation", "abstract": "Audio-visual data allows us to leverage different modalities for downstream tasks. The idea being individual streams can complement each other in the given task, thereby resulting in a model with improved performance. In this work, we present our experimental results on action recognition and video summarization tasks. The proposed modeling approach builds upon the recent advances in contrastive loss based audio-visual representation learning. Temporally cognizant audio-visual discrimination is achieved in a Transformer model by learning with a masked feature reconstruction loss over a fixed time window in addition to learning via contrastive loss. Overall, our results indicate that the addition of temporal information significantly improved the performance of the contrastive loss based framework. We achieve an action classification accuracy of 66.2% versus the next best baseline at 64.7% on the HMDB dataset. For video summarization, we attain an F1 score of 43.5 verses 42.2 on the SumMe dataset."}}
{"id": "_1HETTYd7Wr", "cdate": 1621630067688, "mdate": null, "content": {"title": "Constrained Robust Submodular Partitioning", "abstract": "In the robust submodular partitioning problem, we aim to allocate a set of items into $m$ blocks, so that the evaluation of the minimum block according to a submodular function is maximized. Robust submodular partitioning promotes the diversity of every block in the partition. It has many applications in machine learning, e.g., partitioning data for distributed training so that the gradients computed on every block are consistent. We study an extension of the robust submodular partition problem with additional constraints (e.g., cardinality, multiple matroids, and/or knapsack) on every block. For example, when partitioning data for distributed training, we can add a constraint that the number of samples of each class is the same in each partition block, ensuring data balance. We present two classes of algorithms, i.e., Min-Block Greedy based algorithms (with an $\\Omega(1/m)$ bound), and Round-Robin Greedy based algorithms (with a constant bound) and show that under various constraints, they still have good approximation guarantees. Interestingly, while normally the latter runs in only weakly polynomial time, we show that using the two together yields strongly polynomial running time while preserving the approximation guarantee. Lastly, we apply the algorithms on a real-world machine learning data partitioning problem showing good results."}}
{"id": "paECmBKDgTJ", "cdate": 1609459200000, "mdate": 1672905719586, "content": {"title": "Constrained Robust Submodular Partitioning", "abstract": ""}}
{"id": "TMZA6q3Azkt", "cdate": 1609459200000, "mdate": 1672905719618, "content": {"title": "A Practical Online Framework for Extracting Running Video Summaries under a Fixed Memory Budget", "abstract": ""}}
{"id": "plZI6ErDTj8", "cdate": 1546300800000, "mdate": 1672905719585, "content": {"title": "Fixing Mini-batch Sequences with Hierarchical Robust Partitioning", "abstract": ""}}
{"id": "l9Zo5ROO71W", "cdate": 1546300800000, "mdate": 1672905719494, "content": {"title": "Auto-Summarization: A Step Towards Unsupervised Learning of a Submodular Mixture", "abstract": ""}}
{"id": "QvMapLzrQn", "cdate": 1546300800000, "mdate": null, "content": {"title": "Fixing Mini-batch Sequences with Hierarchical Robust Partitioning.", "abstract": "We propose a general and efficient hierarchical robust partitioning framework to generate a deterministic sequence of mini-batches, one that offers assurances of being high quality, unlike a random..."}}
{"id": "BiNJcDWk_8M", "cdate": 1546300800000, "mdate": null, "content": {"title": "Auto-Summarization: A Step Towards Unsupervised Learning of a Submodular Mixture.", "abstract": "We introduce an approach that requires the specification of only a handful of hyperparameters to determine a mixture of submodular functions for use in data science applications. Two techniques, applied in succession, are used to achieve this. The first involves training an autoencoder neural network constrainedly so that the bottleneck features have the following characteristic: the larger a feature's value, the more an input sample should have an automatically learnt property. This is analogous to bag of-words features, but where the \u201cwords\u201d are learnt automatically. The second technique instantiates a mixture of submodular functions, each of which consists of a concave composed with a modular function comprised of the learnt neural network features. We introduce a mixture weight learning approach that does not (as is common) directly utilize supervised summary information. Instead, it optimizes a set of meta-objectives each of which corresponds to a likely necessary condition on what constitutes a good summarization objective. While hyperparameter optimization is often the bane of unsupervised methods, our approach reduces the learning of a summarization function (which most generally involves learning 2n parameters) down to the problem of selecting only a handful of hyperparameters. Empirical results on three very different modalities of data (i.e., image, text, and machine learning training data) show that our method produces functions that perform significantly better than a variety of unsupervised baseline methods."}}
{"id": "hRztRvDKB2", "cdate": 1483228800000, "mdate": 1672905719505, "content": {"title": "Reducing total latency in online real-time inference and decoding via combined context window and model smoothing latencies", "abstract": ""}}
{"id": "QiSa80Bv4cs", "cdate": 1483228800000, "mdate": null, "content": {"title": "Reducing total latency in online real-time inference and decoding via combined context window and model smoothing latencies.", "abstract": "Real-time low-latency online inference and decoding in sequential probabilistic models are important in many interactive systems, including automatic speech recognition (ASR) and streaming environments. We study total inference latency (TL) in such systems, the additively combined latency of the inherent look-ahead of a deep neural network's (DNN) contextual window (CWL) in a DNN-HMM hybrid system and the latency incurred during Kalman-style smoothing in a dynamic probabilistic model (MSL) (hence, TL = CWL + MSL). For a fixed TL, the best accuracy can occur with a strictly positive MSL, often by quite a bit, a surprising result given the DNN's power. Furthermore, we find that accuracy is often improved with smaller TL and larger MSL. These results suggest that for optimal low-latency real-time decoding, the size of a DNN context window along with model smoothing should be jointly considered."}}
