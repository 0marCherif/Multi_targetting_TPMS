{"id": "XomEU3eNeSQ", "cdate": 1663850018600, "mdate": null, "content": {"title": "Code Translation with Compiler Representations", "abstract": "In this paper, we leverage low-level compiler intermediate representations (IR) code translation. Traditional transpilers rely on syntactic information and handcrafted rules, which limits their applicability and produces unnatural-looking code. Applying neural machine translation (NMT) approaches to code has successfully broadened the set of programs on which one can get a natural-looking translation. However, they treat the code as sequences of text tokens, and still do not differentiate well enough between similar pieces of code which have different semantics in different languages. The consequence is low quality translation, reducing the practicality of NMT, and stressing the need for an approach significantly increasing its accuracy. Here we propose to augment code translation with IRs, specifically LLVM IR, with results on the C++, Java, Rust, and Go languages. Our method improves upon the state of the art for unsupervised code translation, increasing the number of correct translations by 11% on average, and up to 79% for the Java \u2192 Rust pair with greedy decoding. With beam search, it increases the number of correct translations by 5.5% in average. We extend previous test sets for code translation, by adding hundreds of Go and Rust functions. Additionally, we train models with high performance on the problem of IR decompilation, generating programming source code from IR, and study using IRs as intermediary pivot for translation."}}
{"id": "cmt-6KtR4c4", "cdate": 1632875541742, "mdate": null, "content": {"title": "Leveraging Automated Unit Tests for Unsupervised Code Translation", "abstract": "With little to no parallel data available for programming languages, unsupervised methods are well-suited to source code translation. However, the majority of unsupervised machine translation approaches rely on back-translation, a method developed in the context of natural language translation and one that inherently involves training on noisy inputs. Unfortunately, source code is highly sensitive to small changes; a single token can result in compilation failures or erroneous programs, unlike natural languages where small inaccuracies may not change the meaning of a sentence. To address this issue, we propose to leverage an automated unit-testing system to filter out invalid translations, thereby creating a fully tested parallel corpus. We found that fine-tuning an unsupervised model with this filtered data set significantly reduces the noise in the translations so-generated, comfortably outperforming the state-of-the-art for all language pairs studied. In particular, for Java\u2192Python and Python\u2192C++ we outperform the best previous methods by more than 16% and 24% respectively, reducing the error rate by more than 35%."}}
{"id": "3ez9BSHTNT", "cdate": 1621629922105, "mdate": null, "content": {"title": "DOBF: A Deobfuscation Pre-Training Objective for Programming Languages", "abstract": "Recent advances in self-supervised learning have dramatically improved the state of the art on a wide variety of tasks. However, research in language model pre-training has mostly focused on natural languages, and it is unclear whether models like BERT and its variants provide the best pre-training when applied to other modalities, such as source code. \nIn this paper, we introduce a new pre-training objective, DOBF, that leverages the structural aspect of programming languages and pre-trains a model to recover the original version of obfuscated source code. We show that models pre-trained with DOBF significantly outperform existing approaches on multiple downstream tasks, providing relative improvements of up to 12.2% in unsupervised code translation, and 5.3% in natural language code search. Incidentally, we found that our pre-trained model is able to deobfuscate fully obfuscated source files, and to suggest descriptive variable names."}}
{"id": "saxNrgOKjax", "cdate": 1609459200000, "mdate": 1649867875402, "content": {"title": "Leveraging Automated Unit Tests for Unsupervised Code Translation", "abstract": "With little to no parallel data available for programming languages, unsupervised methods are well-suited to source code translation. However, the majority of unsupervised machine translation approaches rely on back-translation, a method developed in the context of natural language translation and one that inherently involves training on noisy inputs. Unfortunately, source code is highly sensitive to small changes; a single token can result in compilation failures or erroneous programs, unlike natural languages where small inaccuracies may not change the meaning of a sentence. To address this issue, we propose to leverage an automated unit-testing system to filter out invalid translations, thereby creating a fully tested parallel corpus. We found that fine-tuning an unsupervised model with this filtered data set significantly reduces the noise in the translations so-generated, comfortably outperforming the state-of-the-art for all language pairs studied. In particular, for Java $\\to$ Python and Python $\\to$ C++ we outperform the best previous methods by more than 16% and 24% respectively, reducing the error rate by more than 35%."}}
{"id": "bGzTYtMwiDr", "cdate": 1609459200000, "mdate": 1649867875602, "content": {"title": "Inspirational Adversarial Image Generation", "abstract": "The task of image generation started receiving some attention from artists and designers, providing inspiration for new creations. However, exploiting the results of deep generative models such as Generative Adversarial Networks can be long and tedious given the lack of existing tools. In this work, we propose a simple strategy to inspire creators with new generations learned from a dataset of their choice, while providing some control over the output. We design a simple optimization method to find the optimal latent parameters corresponding to the closest generation to any input inspirational image. Specifically, we allow the generation given an inspirational image of the user\u2019s choosing by performing several optimization steps to recover optimal parameters from the model\u2019s latent space. We tested several exploration methods from classical gradient descents to gradient-free optimizers. Many gradient-free optimizers just need comparisons (better/worse than another image), so they can even be used without numerical criterion nor inspirational image, only with human preferences. Thus, by iterating on one\u2019s preferences we can make robust facial composite or fashion generation algorithms. Our results on four datasets of faces, fashion images, and textures show that satisfactory images are effectively retrieved in most cases."}}
{"id": "9a21gXmBlI6", "cdate": 1609459200000, "mdate": 1649867875499, "content": {"title": "DOBF: A Deobfuscation Pre-Training Objective for Programming Languages", "abstract": "Recent advances in self-supervised learning have dramatically improved the state of the art on a wide variety of tasks. However, research in language model pre-training has mostly focused on natural languages, and it is unclear whether models like BERT and its variants provide the best pre-training when applied to other modalities, such as source code. In this paper, we introduce a new pre-training objective, DOBF, that leverages the structural aspect of programming languages and pre-trains a model to recover the original version of obfuscated source code. We show that models pre-trained with DOBF significantly outperform existing approaches on multiple downstream tasks, providing relative improvements of up to 13% in unsupervised code translation, and 24% in natural language code search. Incidentally, we found that our pre-trained model is able to de-obfuscate fully obfuscated source files, and to suggest descriptive variable names."}}
{"id": "4D4Rjrwaw3q", "cdate": 1601308364985, "mdate": null, "content": {"title": "Black-Box Optimization Revisited: Improving Algorithm Selection Wizards through Massive Benchmarking", "abstract": "Existing studies in black-box optimization for machine learning suffer from low\ngeneralizability, caused by a typically selective choice of problem instances used\nfor training and testing different optimization algorithms. Among other issues,\nthis practice promotes overfitting and poor-performing user guidelines. To address\nthis shortcoming, we propose in this work a benchmark suite, OptimSuite,\nwhich covers a broad range of black-box optimization problems, ranging from\nacademic benchmarks to real-world applications, from discrete over numerical\nto mixed-integer problems, from small to very large-scale problems, from noisy\nover dynamic to static problems, etc. We demonstrate the advantages of such a\nbroad collection by deriving from it Automated Black Box Optimizer (ABBO), a\ngeneral-purpose algorithm selection wizard. Using three different types of algorithm\nselection techniques, ABBO achieves competitive performance on all\nbenchmark suites. It significantly outperforms previous state of the art on some of\nthem, including YABBOB and LSGO. ABBO relies on many high-quality base\ncomponents. Its excellent performance is obtained without any task-specific\nparametrization. The benchmark collection, the ABBO wizard, its base solvers,\nas well as all experimental data are reproducible and open source in OptimSuite."}}
{"id": "p8vbd8nTkZN", "cdate": 1577836800000, "mdate": 1649867875605, "content": {"title": "Tarsier: Evolving Noise Injection in Super-Resolution GANs", "abstract": "Super-resolution aims at increasing the resolution and level of detail within an image. The current state of the art in general single-image super-resolution is held by NESRGAN+, which injects a Gaussian noise after each residual layer at training time. In this paper, we harness evolutionary methods to improve NESRGAN+ by optimizing the noise injection at inference time. More precisely, we use Diagonal CMA to optimize the injected noise according to a novel criterion combining quality assessment and realism. Our results are validated by the PIRM perceptual score and a human study. Our method outperforms NESRGAN+ on several standard super-resolution datasets. More generally, our approach can be used to optimize any method based on noise injection."}}
{"id": "Wv4ITh0tuZY", "cdate": 1577836800000, "mdate": 1649867875398, "content": {"title": "Evolutionary super-resolution", "abstract": "Super-resolution increases the resolution of an image. Using evolutionary optimization, we optimize the noise injection of a super-resolution method for improving the results. More generally, our approach can be used to optimize any method based on noise injection."}}
{"id": "UTp_oDOLfGT", "cdate": 1577836800000, "mdate": 1649867875501, "content": {"title": "Adversarial Attacks on Linear Contextual Bandits", "abstract": "Contextual bandit algorithms are applied in a wide range of domains, from advertising to recommender systems, from clinical trials to education. In many of these domains, malicious agents may have incentives to force a bandit algorithm into a desired behavior For instance, an unscrupulous ad publisher may try to increase their own revenue at the expense of the advertisers; a seller may want to increase the exposure of their products, or thwart a competitor\u2019s advertising campaign. In this paper, we study several attack scenarios and show that a malicious agent can force a linear contextual bandit algorithm to pull any desired arm T \u2212 o(T) times over a horizon of T steps, while applying adversarial modifications to either rewards or contexts with a cumulative cost that only grow logarithmically as O(log T). We also investigate the case when a malicious agent is interested in affecting the behavior of the bandit algorithm in a single context (e.g., a specific user). We first provide sufficient conditions for the feasibility of the attack and an efficient algorithm to perform an attack. We empirically validate the proposed approaches on synthetic and real-world datasets."}}
