{"id": "Kcmh8ym11V", "cdate": 1684063983971, "mdate": 1684063983971, "content": {"title": "Joint Transfer of Model Knowledge and Fairness Over Domains Using Wasserstein Distance", "abstract": "Owing to the increasing use of machine learning in our daily lives, the problem of fairness has recently become an important topic in machine learning societies. Recent studies regarding fairness in machine learning have been conducted to attempt to ensure statistical independence between individual model predictions and designated sensitive attributes. However, in reality, cases exist in which the sensitive variables of data used for learning models differ from the data upon which the model is applied. In this paper, we investigate a methodology for developing a fair classification model for data with limited or no labels, by transferring knowledge from another data domain where information is fully available. This is done by controlling the Wasserstein distances between relevant distributions. Subsequently, we obtain a fair model that could be successfully applied to two datasets with different sensitive attributes. We present theoretical results validating that our approach provably transfers both classification performance and fairness over domains. Experimental results show that our method does indeed promote fairness for the target domain, while retaining reasonable classification accuracy, and that it often outperforms comparative models in terms of joint fairness."}}
{"id": "mvwWCngsgZ", "cdate": 1640995200000, "mdate": 1683879674632, "content": {"title": "Robust Probabilistic Time Series Forecasting", "abstract": "Probabilistic time series forecasting has played critical role in decision-making processes due to its capability to quantify uncertainties. Deep forecasting models, however, could be prone to input perturbations, and the notion of such perturbations, together with that of robustness, has not even been completely established in the regime of probabilistic forecasting. In this work, we propose a framework for robust probabilistic time series forecasting. First, we generalize the concept of adversarial input perturbations, based on which we formulate the concept of robustness in terms of bounded Wasserstein deviation. Then we extend the randomized smoothing technique to attain robust probabilistic forecasters with theoretical robustness certificates against certain classes of adversarial perturbations. Lastly, extensive experiments demonstrate that our methods are empirically effective in enhancing the forecast quality under additive adversarial attacks and forecast consistency under supplement of noisy observations. The code for our experiments is available at https://github.com/tetrzim/robust-probabilistic-forecasting."}}
{"id": "Iym9s-Kjd9", "cdate": 1640995200000, "mdate": 1683879674630, "content": {"title": "Robust Probabilistic Time Series Forecasting", "abstract": "Probabilistic time series forecasting has played critical role in decision-making processes due to its capability to quantify uncertainties. Deep forecasting models, however, could be prone to input perturbations, and the notion of such perturbations, together with that of robustness, has not even been completely established in the regime of probabilistic forecasting. In this work, we propose a framework for robust probabilistic time series forecasting. First, we generalize the concept of adversarial input perturbations, based on which we formulate the concept of robustness in terms of bounded Wasserstein deviation. Then we extend the randomized smoothing technique to attain robust probabilistic forecasters with theoretical robustness certificates against certain classes of adversarial perturbations. Lastly, extensive experiments demonstrate that our methods are empirically effective in enhancing the forecast quality under additive adversarial attacks and forecast consistency under supplement of noisy observations."}}
{"id": "dILbKeJjtK", "cdate": 1609459200000, "mdate": 1683879674613, "content": {"title": "Accelerated Algorithms for Smooth Convex-Concave Minimax Problems with O(1/k^2) Rate on Squared Gradient Norm", "abstract": "In this work, we study the computational complexity of reducing the squared gradient magnitude for smooth minimax optimization problems. First, we present algorithms with accelerated $\\mathcal{O}(1..."}}
{"id": "UdYD9Qwj37", "cdate": 1609459200000, "mdate": 1683879674608, "content": {"title": "Fair Clustering with Fair Correspondence Distribution", "abstract": ""}}
{"id": "LsRYiMZedyM", "cdate": 1609459200000, "mdate": 1683879674616, "content": {"title": "WGAN with an Infinitely Wide Generator Has No Spurious Stationary Points", "abstract": "Generative adversarial networks (GAN) are a widely used class of deep generative models, but their minimax training dynamics are not understood very well. In this work, we show that GANs with a 2-layer infinite-width generator and a 2-layer finite-width discriminator trained with stochastic gradient ascent-descent have no spurious stationary points. We then show that when the width of the generator is finite but wide, there are no spurious stationary points within a ball whose radius becomes arbitrarily large (to cover the entire parameter space) as the width goes to infinity."}}
{"id": "HFqQ5Bg7fXW", "cdate": 1609459200000, "mdate": 1683879674633, "content": {"title": "WGAN with an Infinitely Wide Generator Has No Spurious Stationary Points", "abstract": "Generative adversarial networks (GAN) are a widely used class of deep generative models, but their minimax training dynamics are not understood very well. In this work, we show that GANs with a 2-l..."}}
{"id": "9ZUz4qxWXPU", "cdate": 1609459200000, "mdate": 1683879674640, "content": {"title": "Atomic cross-chain settlement model for central banks digital currency", "abstract": ""}}
