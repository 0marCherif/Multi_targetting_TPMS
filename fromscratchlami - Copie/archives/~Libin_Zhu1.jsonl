{"id": "VJaoe7Rp9tZ", "cdate": 1676827098860, "mdate": null, "content": {"title": "Neural Tangent Kernel at Initialization: Linear Width Suffices", "abstract": "In this paper we study the problem of lower bounding the minimum eigenvalue of the neural tangent kernel (NTK) at initialization, an important quantity for the theoretical analysis of training in neural networks. We consider feedforward neural networks with smooth activation functions. Without any distributional assumptions on the input, we present a novel result: we show that for suitable initialization variance, $\\widetilde{\\Omega}(n)$ width, where $n$ is the number of training samples, suffices to ensure that the NTK at initialization is positive definite, improving prior results for smooth activations under our setting. Prior to our work, the sufficiency of linear width has only been shown either for networks with ReLU activation functions, and sublinear width has been shown for smooth networks but with additional conditions on the distribution of the data. The technical challenge in the analysis stems from the layerwise inhomogeneity of smooth activation functions and we handle the challenge using generalized Hermite series expansion of such activations."}}
{"id": "Du06rWqW8za", "cdate": 1671946894479, "mdate": 1671946894479, "content": {"title": "Loss landscapes and optimization in over-parameterized non-linear systems and neural networks", "abstract": "The success of deep learning is due, to a large extent, to the remarkable effectiveness of gradient-based optimization methods applied to large neural networks. The purpose of this work is to propose a modern view and a general mathematical framework for loss landscapes and efficient optimization in over-parameterized machine learning models and systems of non-linear equations, a setting that includes over-parameterized deep neural networks. Our starting observation is that optimization landscapes corresponding to such systems are generally not convex, even locally around a global minimum, a condition we call essential non-convexity. We argue that instead they satisfy PL\u204e, a variant of the Polyak-\u0141ojasiewicz condition [32], [25] on most (but not all) of the parameter space, which guarantees both the existence of solutions and efficient optimization by (stochastic) gradient descent (SGD/GD). The PL\u204e condition of these systems is closely related to the condition number of the tangent kernel associated to a non-linear system showing how a PL\u204e-based non-linear theory parallels classical analyses of over-parameterized linear equations. We show that wide neural networks satisfy the PL\u204e condition, which explains the (S)GD convergence to a global minimum. Finally we propose a relaxation of the PL\u204e condition applicable to \u201calmost\u201d over-parameterized systems."}}
{"id": "PINRbk7h01", "cdate": 1663850178020, "mdate": null, "content": {"title": "Restricted Strong Convexity of Deep Learning Models with Smooth Activations", "abstract": "We consider the problem of optimization of deep learning models with smooth activation functions. While there exist influential results on the problem from the ``near initialization'' perspective, we shed considerable new light on the problem. In particular, we make two key technical contributions for such models with $L$ layers, $m$ width, and $\\sigma_0^2$ initialization variance. First, for suitable $\\sigma_0^2$, we establish a $O(\\frac{\\text{poly}(L)}{\\sqrt{m}})$ upper bound on the spectral norm of the Hessian of such models, considerably sharpening prior results. Second, we introduce a new analysis of optimization based on Restricted Strong Convexity (RSC) which holds as long as the squared norm of the average gradient of predictors is $\\Omega(\\frac{\\text{poly}(L)}{\\sqrt{m}})$ for the square loss. We also present results for more general losses. The RSC based analysis does not need the ``near initialization\" perspective and guarantees geometric convergence for gradient descent (GD). To the best of our knowledge, ours is the first result on establishing geometric convergence of GD based on RSC for deep learning models, thus becoming an alternative sufficient condition for convergence that does not depend on the widely-used Neural Tangent Kernel (NTK). We share preliminary experimental results supporting our theoretical advances."}}
{"id": "GNFimGDfEiV", "cdate": 1663849977492, "mdate": null, "content": {"title": "Quadratic models for understanding neural network dynamics", "abstract": "In this work, we show that recently proposed quadratic models capture optimization and generalization properties of wide neural networks that cannot be captured by linear models.  In particular, we prove that quadratic models for shallow ReLU networks exhibit the \"catapult phase\" from Lewkowycz et al. (2020) that arises when training such models with large learning rates. We then empirically show that the behaviour of quadratic models parallels that of neural networks in generalization, especially in the catapult phase regime. Our analysis further demonstrates that quadratic models are an effective tool for analysis of neural networks. "}}
{"id": "Tean8bBjlbB", "cdate": 1652737641350, "mdate": null, "content": {"title": "Transition to Linearity of General Neural Networks with Directed Acyclic Graph Architecture", "abstract": "In this paper we show that feedforward neural networks corresponding to arbitrary directed acyclic graphs undergo transition to linearity as their ``width'' approaches infinity. The width of these general networks is characterized by the minimum in-degree of their neurons, except for the input and first layers. Our results identify the mathematical structure underlying transition to linearity and generalize a number of recent works aimed at characterizing transition to linearity or constancy of the Neural Tangent Kernel for standard architectures. "}}
{"id": "rbRB2GRSrH", "cdate": 1640995200000, "mdate": 1684136928711, "content": {"title": "Transition to Linearity of Wide Neural Networks is an Emerging Property of Assembling Weak Models", "abstract": "Wide neural networks with linear output layer have been shown to be near-linear, and to have near-constant neural tangent kernel (NTK), in a region containing the optimization path of gradient descent. These findings seem counter-intuitive since in general neural networks are highly complex models. Why does a linear structure emerge when the neural networks become wide? In this work, we provide a new perspective on this \"transition to linearity\" by considering a neural network as an assembly model recursively built from a set of sub-models corresponding to individual neurons. In this view, we show that the linearity of wide neural networks is, in fact, an emerging property of assembling a large number of diverse ``weak'' sub-models, none of which dominate the assembly."}}
{"id": "oSK7pksrz", "cdate": 1640995200000, "mdate": 1682626672049, "content": {"title": "Restricted Strong Convexity of Deep Learning Models with Smooth Activations", "abstract": "We consider the problem of optimization of deep learning models with smooth activation functions. While there exist influential results on the problem from the ``near initialization'' perspective, we shed considerable new light on the problem. In particular, we make two key technical contributions for such models with $L$ layers, $m$ width, and $\\sigma_0^2$ initialization variance. First, for suitable $\\sigma_0^2$, we establish a $O(\\frac{\\text{poly}(L)}{\\sqrt{m}})$ upper bound on the spectral norm of the Hessian of such models, considerably sharpening prior results. Second, we introduce a new analysis of optimization based on Restricted Strong Convexity (RSC) which holds as long as the squared norm of the average gradient of predictors is $\\Omega(\\frac{\\text{poly}(L)}{\\sqrt{m}})$ for the square loss. We also present results for more general losses. The RSC based analysis does not need the ``near initialization\" perspective and guarantees geometric convergence for gradient descent (GD). To the best of our knowledge, ours is the first result on establishing geometric convergence of GD based on RSC for deep learning models, thus becoming an alternative sufficient condition for convergence that does not depend on the widely-used Neural Tangent Kernel (NTK). We share preliminary experimental results supporting our theoretical advances."}}
{"id": "nWj_wJ8R0G", "cdate": 1640995200000, "mdate": 1684136928614, "content": {"title": "Transition to Linearity of General Neural Networks with Directed Acyclic Graph Architecture", "abstract": "In this paper we show that feedforward neural networks corresponding to arbitrary directed acyclic graphs undergo transition to linearity as their ``width'' approaches infinity. The width of these general networks is characterized by the minimum in-degree of their neurons, except for the input and first layers. Our results identify the mathematical structure underlying transition to linearity and generalize a number of recent works aimed at characterizing transition to linearity or constancy of the Neural Tangent Kernel for standard architectures."}}
{"id": "hgkC_8ptF9", "cdate": 1640995200000, "mdate": 1684136928621, "content": {"title": "Quadratic models for understanding neural network dynamics", "abstract": "While neural networks can be approximated by linear models as their width increases, certain properties of wide neural networks cannot be captured by linear models. In this work we show that recently proposed Neural Quadratic Models can exhibit the \"catapult phase\" [Lewkowycz et al. 2020] that arises when training such models with large learning rates. We then empirically show that the behaviour of neural quadratic models parallels that of neural networks in generalization, especially in the catapult phase regime. Our analysis further demonstrates that quadratic models can be an effective tool for analysis of neural networks."}}
{"id": "7Xd4-R-Zt3e", "cdate": 1640995200000, "mdate": 1680416709308, "content": {"title": "A note on Linear Bottleneck networks and their Transition to Multilinearity", "abstract": ""}}
