{"id": "xnAb9_a13w", "cdate": 1669149155463, "mdate": 1669149155463, "content": {"title": "Deep learning models for histologic grading of breast cancer and association with disease prognosis", "abstract": "Histologic grading of breast cancer involves review and scoring of three well-established morphologic features: mitotic count, nuclear pleomorphism, and tubule formation. Taken together, these features form the basis of the Nottingham Grading System which is used to inform breast cancer characterization and prognosis. In this study, we develop deep learning models to perform histologic scoring of all three components using digitized hematoxylin and eosin-stained slides containing invasive breast carcinoma. We first evaluate model performance using pathologist-based reference standards for each component. To complement this typical approach to evaluation, we further evaluate the deep learning models via prognostic analyses. The individual component models perform at or above published benchmarks for algorithm-based grading approaches, achieving high concordance rates with pathologist grading. Further, prognostic performance using deep learning-based grading is on par with that of pathologists performing review of matched slides. By providing scores for each component feature, the deep-learning based approach also provides the potential to identify the grading components contributing most to prognostic value. This may enable optimized prognostic models, opportunities to improve access to consistent grading, and approaches to better understand the links between histologic features and clinical outcomes in breast cancer."}}
{"id": "2swewfyAQKx", "cdate": 1633629018226, "mdate": 1633629018226, "content": {"title": "Determining breast cancer biomarker status and associated morphological features using deep learning", "abstract": "Background\nBreast cancer management depends on biomarkers including estrogen receptor, progesterone receptor, and human epidermal growth factor receptor 2 (ER/PR/HER2). Though existing scoring systems are widely used and well-validated, they can involve costly preparation and variable interpretation. Additionally, discordances between histology and expected biomarker findings can prompt repeat testing to address biological, interpretative, or technical reasons for unexpected results.\n\nMethods\nWe developed three independent deep learning systems (DLS) to directly predict ER/PR/HER2 status for both focal tissue regions (patches) and slides using hematoxylin-and-eosin-stained (H&E) images as input. Models were trained and evaluated using pathologist annotated slides from three data sources. Areas under the receiver operator characteristic curve (AUCs) were calculated for test sets at both a patch-level (>135 million patches, 181 slides) and slide-level (n\u2009=\u20093274 slides, 1249 cases, 37 sites). Interpretability analyses were performed using Testing with Concept Activation Vectors (TCAV), saliency analysis, and pathologist review of clustered patches.\n\nResults\nThe patch-level AUCs are 0.939 (95%CI 0.936\u20130.941), 0.938 (0.936\u20130.940), and 0.808 (0.802\u20130.813) for ER/PR/HER2, respectively. At the slide level, AUCs are 0.86 (95%CI 0.84\u20130.87), 0.75 (0.73\u20130.77), and 0.60 (0.56\u20130.64) for ER/PR/HER2, respectively. Interpretability analyses show known biomarker-histomorphology associations including associations of low-grade and lobular histology with ER/PR positivity, and increased inflammatory infiltrates with triple-negative staining.\n\nConclusions\nThis study presents rapid breast cancer biomarker estimation from routine H&E slides and builds on prior advances by prioritizing interpretability of computationally learned features in the context of existing pathological knowledge."}}
{"id": "oN0Krn1joQ", "cdate": 1581700631764, "mdate": null, "content": {"title": "A Video-Based Method for Automatically Rating Ataxia", "abstract": "For many movement disorders, such as Parkinson\u2019s disease and ataxia, disease progression is\nvisually assessed by a clinician using a numerical disease rating scale. These tests are subjective,\ntime-consuming, and must be administered by a professional. This can be problematic where specialists are not available, or when a patient is not consistently evaluated by the same clinician.\nWe present an automated method for quantifying the severity of motion impairment in patients\nwith ataxia, using only video recordings. We consider videos of the finger-to-nose test, a common\nmovement task used as part of the assessment of ataxia progression during the course of routine\nclinical checkups.\n\nOur method uses neural network-based pose estimation and optical flow techniques to track the\nmotion of the patient\u2019s hand in a video recording. We extract features that describe qualities of the\nmotion such as speed and variation in performance. Using labels provided by an expert clinician,\nwe train a supervised learning model that predicts severity according to the Brief Ataxia Rating\nScale (BARS). The performance of our system is comparable to that of a group of ataxia specialists\nin terms of mean error and correlation, and our system\u2019s predictions were consistently within the\nrange of inter-rater variability. This work demonstrates the feasibility of using computer vision and\nmachine learning to produce consistent and clinically useful measures of motor impairment."}}
{"id": "7E9FoujmTZ", "cdate": 1581700486617, "mdate": null, "content": {"title": "Predicting Range of Acceptable Photographic Tonal Adjustments", "abstract": "There is often more than one way to select tonal adjust- ment for a photograph, and different individuals may prefer different adjustments. However, selecting good adjustments is challenging. This paper describes a method to predict whether a given tonal rendition is acceptable for a photograph, which we use to characterize its range of acceptable adjustments. We gathered a dataset of image \"acceptability\" over brightness and contrast adjustments. We find that unacceptable renditions can be explained in terms of over-exposure, under-exposure, and low contrast. Based on this observation, we propose a machine-learning algorithm to assess whether an adjusted photograph looks acceptable. We show that our algorithm can differentiate unsightly renditions from reasonable ones. Finally, we describe proof-of-concept applications that use our algorithm to guide the exploration of the possible tonal renditions of a photograph."}}
{"id": "24_bs9h9c", "cdate": 1581700340290, "mdate": null, "content": {"title": "Generating Training Data for Denoising Real RGB Images via Camera Pipeline Simulation", "abstract": "Image reconstruction techniques such as denoising often\nneed to be applied to the RGB output of cameras and cellphones. Unfortunately, the commonly used additive white\nnoise (AWGN) models do not accurately reproduce the noise\nand the degradation encountered on these inputs. This is\nparticularly important for learning-based techniques, because the mismatch between training and real world data\nwill hurt their generalization. This paper aims to accurately simulate the degradation and noise transformation\nperformed by camera pipelines. This allows us to generate realistic degradation in RGB images that can be used\nto train machine learning models. We use our simulation to\nstudy the importance of noise modeling for learning-based\ndenoising. Our study shows that a realistic noise model\nis required for learning to denoise real JPEG images. A\nneural network trained on realistic noise outperforms the\none trained with AWGN by 3 dB. An ablation study of our\npipeline shows that simulating denoising and demosaicking\nis important to this improvement and that realistic demosaicking algorithms, which have been rarely considered, is\nneeded. We believe this simulation will also be useful for\nother image reconstruction tasks, and we will distribute our\ncode publicly."}}
{"id": "SJWtnFbOWH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Learning-Based Video Motion Magnification", "abstract": "Video motion magnification techniques allow us to see small motions previously invisible to the naked eyes, such as those of vibrating airplane wings, or swaying buildings under the influence of the wind. Because the motion is small, the magnification results are prone to noise or excessive blurring. The state of the art relies on hand-designed filters to extract representations that may not be optimal. In this paper, we seek to learn the filters directly from examples using deep convolutional neural networks. To make training tractable, we carefully design a synthetic dataset that captures small motion well, and use two-frame input for training. We show that the learned filters achieve high-quality results on real videos, with less ringing artifacts and better noise characteristics than previous methods. While our model is not trained with temporal filters, we found that the temporal filters can be used with our extracted representations up\u00a0to a moderate magnification, enabling a frequency-based motion selection. Finally, we analyze the learned filters and show that they behave similarly to the derivative filters used in previous works. Our code, trained model, and datasets will be available online."}}
{"id": "BkWOGxzO-S", "cdate": 1514764800000, "mdate": null, "content": {"title": "On the Importance of Label Quality for Semantic Segmentation", "abstract": "Convolutional networks (ConvNets) have become the dominant approach to semantic image segmentation. Producing accurate, pixel--level labels required for this task is a tedious and time consuming process; however, producing approximate, coarse labels could take only a fraction of the time and effort. We investigate the relationship between the quality of labels and the performance of ConvNets for semantic segmentation. We create a very large synthetic dataset with perfectly labeled street view scenes. From these perfect labels, we synthetically coarsen labels with different qualities and estimate human--hours required for producing them. We perform a series of experiments by training ConvNets with a varying number of training images and label quality. We found that the performance of ConvNets mostly depends on the time spent creating the training labels. That is, a larger coarsely--annotated dataset can yield the same performance as a smaller finely--annotated one. Furthermore, fine--tuning coarsely pre--trained ConvNets with few finely-annotated labels can yield comparable or superior performance to training it with a large amount of finely-annotated labels alone, at a fraction of the labeling cost. We demonstrate that our result is also valid for different network architectures, and various object classes in an urban scene."}}
