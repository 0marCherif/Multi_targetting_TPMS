{"id": "oplrEMzj2mH", "cdate": 1674557340486, "mdate": 1674557340486, "content": {"title": "Gamma-convergence of a nonlocal perimeter arising in adversarial machine learning", "abstract": "In this paper we prove Gamma-convergence of a nonlocal perimeter of Minkowski type to a local anisotropic perimeter. The nonlocal model describes the regularizing effect of adversarial training in binary classifications. The energy essentially depends on the interaction between two distributions modelling likelihoods for the associated classes. We overcome typical strict regularity assumptions for the distributions by only assuming that they have bounded BV densities. In the natural topology coming from compactness, we prove Gamma-convergence to a weighted perimeter with weight determined by an anisotropic function of the two densities. Despite being local, this sharp interface limit reflects classification stability with respect to adversarial perturbations. We further apply our results to deduce Gamma-convergence of the associated total variations, to study the asymptotics of adversarial training, and to prove Gamma-convergence of graph discretizations for the nonlocal perimeter."}}
{"id": "EYQfFHVq_m", "cdate": 1674557263361, "mdate": 1674557263361, "content": {"title": "Polarized consensus-based dynamics for optimization and sampling", "abstract": "In this paper we propose polarized consensus-based dynamics in order to make consensus-based optimization (CBO) and sampling (CBS) applicable for objective functions with several global minima or distributions with many modes, respectively. For this, we \"polarize\" the dynamics with a localizing kernel and the resulting model can be viewed as a bounded confidence model for opinion formation in the presence of common objective. Instead of being attracted to a common weighted mean as in the original consensus-based methods, which prevents the detection of more than one minimum or mode, in our method every particle is attracted to a weighted mean which gives more weight to nearby particles. The resulting dynamics possess mean-field interpretations with Fokker--Planck equations that are structurally similar to the ones of original CBO and CBS, and we prove that the polarized CBS dynamics is unbiased in case of a Gaussian target. We also propose a computationally more efficient generalization which works with a predefined number of clusters and improves upon our polarized baseline method for high-dimensional optimization."}}
{"id": "i1tbneV03t", "cdate": 1674557138647, "mdate": 1674557138647, "content": {"title": "Ratio convergence rates for Euclidean first-passage percolation: Applications to the graph infinity Laplacian", "abstract": "In this paper we prove the first quantitative convergence rates for the graph infinity Laplace equation for length scales at the connectivity threshold. In the graph-based semi-supervised learning community this equation is also known as Lipschitz learning. The graph infinity Laplace equation is characterized by the metric on the underlying space, and convergence rates follow from convergence rates for graph distances. At the connectivity threshold, this problem is related to Euclidean first passage percolation, which is concerned with the Euclidean distance function dh(x,y) on a homogeneous Poisson point process on Rd, where admissible paths have step size at most h>0. Using a suitable regularization of the distance function and subadditivity we prove that dhs(0,se1)/s\u2192\u03c3 as s\u2192\u221e almost surely where \u03c3\u22651 is a dimensional constant and hs\u2273log(s)1d. A convergence rate is not available due to a lack of approximate superadditivity when hs\u2192\u221e. Instead, we prove convergence rates for the ratio dh(0,se1)dh(0,2se1)\u219212 when h is frozen and does not depend on s. Combining this with the techniques that we developed in (Bungert, Calder, Roith, IMA Journal of Numerical Analysis, 2022), we show that this notion of ratio convergence is sufficient to establish uniform convergence rates for solutions of the graph infinity Laplace equation at percolation length scales."}}
{"id": "ZBZXBoDnckH", "cdate": 1674557060008, "mdate": 1674557060008, "content": {"title": "Improving Robustness against Real-World and Worst-Case Distribution Shifts through Decision Region Quantification", "abstract": "The reliability of neural networks is essential for their use in safety-critical applications. Existing approaches generally aim at improving the robustness of neural networks to either real-world distribution shifts (e.g., common corruptions and perturbations, spatial transformations, and natural adversarial examples) or worst-case distribution shifts (e.g., optimized adversarial examples). In this work, we propose the Decision Region Quantification (DRQ) algorithm to improve the robustness of any differentiable pre-trained model against both real-world and worst-case distribution shifts in the data. DRQ analyzes the robustness of local decision regions in the vicinity of a given data point to make more reliable predictions. We theoretically motivate the DRQ algorithm by showing that it effectively smooths spurious local extrema in the decision surface. Furthermore, we propose an implementation using targeted and untargeted adversarial attacks. An extensive empirical evaluation shows that DRQ increases the robustness of adversarially and non-adversarially trained models against real-world and worst-case distribution shifts on several computer vision benchmark datasets."}}
{"id": "N-bLvDHWpGK", "cdate": 1649238225411, "mdate": 1649238225411, "content": {"title": "Continuum Limit of Lipschitz Learning on Graphs", "abstract": "Tackling semi-supervised learning problems with graph-based methods has become a trend in recent years since graphs can represent all kinds of data and provide a suitable framework for studying continuum limits, e.g., of differential operators. A popular strategy here is p-Laplacian learning, which poses a smoothness condition on the sought inference function on the set of unlabeled data. For p<\u221e continuum limits of this approach were studied using tools from \u0393-convergence. For the case p=\u221e, which is referred to as Lipschitz learning, continuum limits of the related infinity-Laplacian equation were studied using the concept of viscosity solutions.\nIn this work, we prove continuum limits of Lipschitz learning using \u0393-convergence. In particular, we define a sequence of functionals which approximate the largest local Lipschitz constant of a graph function and prove \u0393-convergence in the L\u221e-topology to the supremum norm of the gradient as the graph becomes denser. Furthermore, we show compactness of the functionals which implies convergence of minimizers. In our analysis we allow a varying set of labeled data which converges to a general closed set in the Hausdorff distance. We apply our results to nonlinear ground states, i.e., minimizers with constrained Lp-norm, and, as a by-product, prove convergence of graph distance functions to geodesic distance functions."}}
{"id": "0-RYD7GgIou", "cdate": 1649238162639, "mdate": 1649238162639, "content": {"title": "Identifying Untrustworthy Predictions in Neural Networks by Geometric Gradient Analysis", "abstract": "The susceptibility of deep neural networks to untrustworthy predictions, including out-of-distribution (OOD) data and adversarial examples, still prevent their widespread use in safety-critical applications. Most existing methods either require a re-training of a given model to achieve robust identification of adversarial attacks or are limited to out-of-distribution sample detection only. In this work, we propose a geometric gradient analysis (GGA) to improve the identification of untrustworthy predictions without retraining of a given model. GGA analyzes the geometry of the loss landscape of neural networks based on the saliency maps of their respective input. To motivate the proposed approach, we provide theoretical connections between gradients' geometrical properties and local minima of the loss function. Furthermore, we demonstrate that the proposed method outperforms prior approaches in detecting OOD data and adversarial attacks, including state-of-the-art and adaptive attacks."}}
{"id": "RqU6UabTgs3", "cdate": 1649238036892, "mdate": 1649238036892, "content": {"title": "CLIP: Cheap Lipschitz Training of Neural Networks", "abstract": "Despite the large success of deep neural networks (DNN) in recent years, most neural networks still lack mathematical guarantees in terms of stability. For instance, DNNs are vulnerable to small or even imperceptible input perturbations, so called adversarial examples, that can cause false predictions. This instability can have severe consequences in applications which influence the health and safety of humans, e.g., biomedical imaging or autonomous driving. While bounding the Lipschitz constant of a neural network improves stability, most methods rely on restricting the Lipschitz constants of each layer which gives a poor bound for the actual Lipschitz constant.\nIn this paper we investigate a variational regularization method named CLIP for controlling the Lipschitz constant of a neural network, which can easily be integrated into the training procedure. We mathematically analyze the proposed model, in particular discussing the impact of the chosen regularization parameter on the output of the network. Finally, we numerically evaluate our method on both a nonlinear regression problem and the MNIST and Fashion-MNIST classification databases, and compare our results with a weight regularization approach."}}
{"id": "37ffQL4A5oo", "cdate": 1649237961747, "mdate": 1649237961747, "content": {"title": "Neural Architecture Search via Bregman Iterations", "abstract": "We propose a novel strategy for Neural Architecture Search (NAS) based on Bregman iterations. Starting from a sparse neural network our gradient-based one-shot algorithm gradually adds relevant parameters in an inverse scale space manner. This allows the network to choose the best architecture in the search space which makes it well-designed for a given task, e.g., by adding neurons or skip connections. We demonstrate that using our approach one can unveil, for instance, residual autoencoders for denoising, deblurring, and classification tasks. Code is available at https://github.com/TimRoith/BregmanLearning."}}
{"id": "uq4xrgNgJhY", "cdate": 1649237874613, "mdate": 1649237874613, "content": {"title": "Uniform Convergence Rates for Lipschitz Learning on Graphs", "abstract": "Lipschitz learning is a graph-based semi-supervised learning method where one extends labels from a labeled to an unlabeled data set by solving the infinity Laplace equation on a weighted graph. In this work we prove uniform convergence rates for solutions of the graph infinity Laplace equation as the number of vertices grows to infinity. Their continuum limits are absolutely minimizing Lipschitz extensions with respect to the geodesic metric of the domain where the graph vertices are sampled from. We work under very general assumptions on the graph weights, the set of labeled vertices, and the continuum domain. Our main contribution is that we obtain quantitative convergence rates even for very sparsely connected graphs, as they typically appear in applications like semi-supervised learning. In particular, our framework allows for graph bandwidths down to the connectivity radius. For proving this we first show a quantitative convergence statement for graph distance functions to geodesic distance functions in the continuum. Using the \"comparison with distance functions\" principle, we can pass these convergence statements to infinity harmonic functions and absolutely minimizing Lipschitz extensions."}}
{"id": "OB6KwYy-UxK", "cdate": 1649237813945, "mdate": 1649237813945, "content": {"title": "The Geometry of Adversarial Training in Binary Classification", "abstract": "We establish an equivalence between a family of adversarial training problems for non-parametric binary classification and a family of regularized risk minimization problems where the regularizer is a nonlocal perimeter functional. The resulting regularized risk minimization problems admit exact convex relaxations of the type L1+ (nonlocal) TV, a form frequently studied in image analysis and graph-based learning. A rich geometric structure is revealed by this reformulation which in turn allows us to establish a series of properties of optimal solutions of the original problem, including the existence of minimal and maximal solutions (interpreted in a suitable sense), and the existence of regular solutions (also interpreted in a suitable sense). In addition, we highlight how the connection between adversarial training and perimeter minimization problems provides a novel, directly interpretable, statistical motivation for a family of regularized risk minimization problems involving perimeter/total variation. The majority of our theoretical results are independent of the distance used to define adversarial attacks."}}
