{"id": "jjKrKivdTt4", "cdate": 1672852351062, "mdate": 1672852351062, "content": {"title": "Local feature selection for multiple instance learning", "abstract": "We propose a local feature selection method for the Multiple Instance Learning (MIL) framework. Unlike conventional feature selection algorithms that assign a global set of features to the whole data set, our algorithm, called Multiple Instance Local Salient Feature Selection (MI-LSFS), searches the feature space to find the relevant features within each bag. We also propose a new multiple instance classification algorithm, called Multiple Instance Learning via Embedded Structures with Local Feature Selection (MILES-LFS), by integrating the information learned by MI-LSFS during the feature selection process. In MILES-LFS, we use information learned by MI-LSFS to identify a reduced subset of representative bags. For each representative bag, we identify its most representative instances. Using the instance prototypes of all representative bags and their relevant features, we project and map the MIL data to a standard feature vector data. Finally, we train a 1-Norm support vector machine (1-Norm SVM) to learn the classifier. We investigate the performance of MI-LSFS in selecting the local relevant features using synthetic and benchmark data sets. The results confirm that MI-LSFS can identify the relevant features for each bag. We also investigate the performance of the proposed MILES-LFS algorithm on several synthetic and real benchmark data sets. The results confirm that MILES-LFS has a robust classification performance comparable to the well-known MILES algorithm. More importantly, our results confirm that using the reduced set of prototypes to project the MIL data reduces the computational time significantly without affecting the classification accuracy."}}
{"id": "9spvoEbpY9", "cdate": 1640995200000, "mdate": 1681786987275, "content": {"title": "Local feature selection for multiple instance learning", "abstract": "We propose a local feature selection method for the Multiple Instance Learning (MIL) framework. Unlike conventional feature selection algorithms that assign a global set of features to the whole data set, our algorithm, called Multiple Instance Local Salient Feature Selection (MI-LSFS), searches the feature space to find the relevant features within each bag. We also propose a new multiple instance classification algorithm, called Multiple Instance Learning via Embedded Structures with Local Feature Selection (MILES-LFS), by integrating the information learned by MI-LSFS during the feature selection process. In MILES-LFS, we use information learned by MI-LSFS to identify a reduced subset of representative bags. For each representative bag, we identify its most representative instances. Using the instance prototypes of all representative bags and their relevant features, we project and map the MIL data to a standard feature vector data. Finally, we train a 1-Norm support vector machine (1-Norm SVM) to learn the classifier. We investigate the performance of MI-LSFS in selecting the local relevant features using synthetic and benchmark data sets. The results confirm that MI-LSFS can identify the relevant features for each bag. We also investigate the performance of the proposed MILES-LFS algorithm on several synthetic and real benchmark data sets. The results confirm that MILES-LFS has a robust classification performance comparable to the well-known MILES algorithm. More importantly, our results confirm that using the reduced set of prototypes to project the MIL data reduces the computational time significantly without affecting the classification accuracy."}}
{"id": "aadVI8K7y0O", "cdate": 1600228530457, "mdate": null, "content": {"title": "Applying Mining Schemes to Software Fault Prediction: A Proposed Approach Aimed at Test Cost Reduction", "abstract": "Software fault prediction based on mining of code and design metrics has been considered by many researchers. Fault detection systems predict faults by using software metrics and data mining techniques. Various classifiers have already been used in this case; however Na\u00efve Bayes classifier is the most commonly used. According to the results of a study performed by Lessman, no significant performance difference could be detected among the top 17 classifiers. In this paper, we will extend that study by  examining the performance of 37 different classifiers in fault detection systems. We will review the results and aim to choose an appropriate classifier (Bagging) which depicts a higher performance and accuracy compared to the others. Finally, we propose a fault detection system with higher performance which manages to decrease the cost of software fault detection simultaneously. We investigate our classifier selection by evaluating the methods on a number of other datasets. Our results indicate that Bagging classifier has the highest performance in fault detection."}}
{"id": "otmNPxWcRRF", "cdate": 1546300800000, "mdate": null, "content": {"title": "Molecule specific normalization for protein and metabolite biomarker discovery", "abstract": "The paper proposes a molecule specific normalization algorithm, called MSN, which adopts a robust surface fitting strategy to minimize the molecular profile difference of a group of house-keeping molecules across samples. The house-keeping molecules are those molecules whose abundance levels were not affected by the biological treatment. The applications of the MSN method on two different datasets showed that MSN is a highly efficient normalization algorithm that yields the highest sensitivity and accuracy compared to five existing normalization algorithms"}}
{"id": "ib8ZBhtzyCP", "cdate": 1546300800000, "mdate": null, "content": {"title": "Ensemble feature selection for biomarker discovery in mass spectrometry-based metabolomics", "abstract": "Biomarker discovery, i.e., identifying the discriminative features that are responsible for alteration of a biological system, is often solved by feature selection implemented by machine learning approaches. While many individual feature selection methods are used in biomarker discovery, the nature of omics data (small number of samples, large number of features, and noisy data) makes each of those individual feature selection algorithms unstable. In this paper, we investigate various ensemble feature selection methods to improve the reliability of the molecular biomarker selection by combining the complementary information of multiple feature selection methods. We compare the performance of different ensemble approaches and evaluate their performances using a metabolomics dataset containing three sample groups. Our results indicate that our ensemble approach outperforms the individual feature selection algorithms and provides more stable results."}}
{"id": "N1NTatyUqv", "cdate": 1483228800000, "mdate": null, "content": {"title": "An ensemble feature selection method for biomarker discovery", "abstract": "Feature selection in Liquid Chromatography-Mass Spectrometry (LC-MS)-based metabolomics data (biomarker discovery) have become an important topic for machine learning researchers. High dimensionality and small sample size of LC-MS data make feature selection a challenging task. The goal of biomarker discovery is to select the few most discriminative features among a large number of irreverent ones. To improve the reliability of the discovered biomarkers, we use an ensemble-based approach. Ensemble learning can improve the accuracy of feature selection by combining multiple algorithms that have complementary information. In this paper, we propose an ensemble approach to combine the results of filter-based feature selection methods. To evaluate the proposed approach, we compared it to two commonly used methods, t-test and PLS-DA, using a real data set."}}
