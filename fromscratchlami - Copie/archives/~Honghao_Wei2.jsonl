{"id": "zATbhf0_KG_", "cdate": 1672531200000, "mdate": 1682382338209, "content": {"title": "Provably Efficient Model-Free Algorithms for Non-stationary CMDPs", "abstract": "We study model-free reinforcement learning (RL) algorithms in episodic non-stationary constrained Markov Decision Processes (CMDPs), in which an agent aims to maximize the expected cumulative reward subject to a cumulative constraint on the expected utility (cost). In the non-stationary environment, reward, utility functions, and transition kernels can vary arbitrarily over time as long as the cumulative variations do not exceed certain variation budgets. We propose the first model-free, simulator-free RL algorithms with sublinear regret and zero constraint violation for non-stationary CMDPs in both tabular and linear function approximation settings with provable performance guarantees. Our results on regret bound and constraint violation for the tabular case match the corresponding best results for stationary CMDPs when the total budget is known. Additionally, we present a general framework for addressing the well-known challenges associated with analyzing non-stationary CMDPs, without requiring prior knowledge of the variation budget. We apply the approach for both tabular and linear approximation settings."}}
{"id": "rwdpFgfVpvN", "cdate": 1652737403536, "mdate": null, "content": {"title": "Online Convex Optimization with Hard Constraints: Towards the Best of Two Worlds and Beyond", "abstract": "This paper considers online convex optimization with hard constraints and analyzes achievable regret and cumulative hard constraint violation (violation for short). The problem distinguishes itself from online convex optimization with soft constraints, where a violation at one round can be compensated/cancelled by a conservative decision at a different round. We propose a RECtified Online Optimization algorithm (RECOO) and consider two settings: fixed constraints and adversarial constraints. Both settings have been considered in the literature. Compared with existing results, {\\em RECOO achieves the best of two worlds and beyond.}  For the fixed-constraints setting, RECOO achieves $O\\left(\\sqrt{T}\\right)$ regret and $O(1)$  violation, where $T$ is the learning horizon. The best known results in this case are $O(\\sqrt{T})$ regret and $O\\left(T^{1/4}\\right)$ violation. For the adversarial-constraints setting, it guarantees $O(\\sqrt{T})$ regret and $O(T^{3/4})$ violation, which match the best existing results.  When the loss functions are strongly convex,  RECOO can guarantee $O(\\log T)$ regret and $O(1)$ violation for fixed constraints, and $O(\\log T)$ regret and $O(\\sqrt{T\\log T})$ violation for adversarial constraints. Both these results are order-wise better than the existing bounds. The regret and violation bounds mentioned above use the best fixed decision in hindsight as the baseline. This paper further considers a dynamic baseline where the comparator sequence is time-varying. This paper shows that RECOO not only improves the existing results in the fixed-constraints setting  but also {\\em for the first time,} guarantees dynamic regret and violation bounds in the adversarial-constraints setting. Our experiment results confirm that RECOO outperforms several existing algorithms for both fixed and adversarial constraints.  "}}
{"id": "hbc8IPZO8-M", "cdate": 1640995200000, "mdate": 1683898439694, "content": {"title": "On low-complexity quickest intervention of mutated diffusion processes through local approximation", "abstract": ""}}
{"id": "h439kwGMvKf", "cdate": 1640995200000, "mdate": 1684096314733, "content": {"title": "A Provably-Efficient Model-Free Algorithm for Infinite-Horizon Average-Reward Constrained Markov Decision Processes", "abstract": "This paper presents a model-free reinforcement learning (RL) algorithm for infinite-horizon average-reward Constrained Markov Decision Processes (CMDPs). Considering a learning horizon K, which is sufficiently large, the proposed algorithm achieves sublinear regret and zero constraint violation. The bounds depend on the number of states S, the number of actions A, and two constants which are independent of the learning horizon K."}}
{"id": "Ws8J0WmGK-", "cdate": 1640995200000, "mdate": 1684096314734, "content": {"title": "Online Convex Optimization with Hard Constraints: Towards the Best of Two Worlds and Beyond", "abstract": "This paper considers online convex optimization with hard constraints and analyzes achievable regret and cumulative hard constraint violation (violation for short). The problem distinguishes itself from online convex optimization with soft constraints, where a violation at one round can be compensated/cancelled by a conservative decision at a different round. We propose a RECtified Online Optimization algorithm (RECOO) and consider two settings: fixed constraints and adversarial constraints. Both settings have been considered in the literature. Compared with existing results, {\\em RECOO achieves the best of two worlds and beyond.} For the fixed-constraints setting, RECOO achieves $O\\left(\\sqrt{T}\\right)$ regret and $O(1)$ violation, where $T$ is the learning horizon. The best known results in this case are $O(\\sqrt{T})$ regret and $O\\left(T^{1/4}\\right)$ violation. For the adversarial-constraints setting, it guarantees $O(\\sqrt{T})$ regret and $O(T^{3/4})$ violation, which match the best existing results. When the loss functions are strongly convex, RECOO can guarantee $O(\\log T)$ regret and $O(1)$ violation for fixed constraints, and $O(\\log T)$ regret and $O(\\sqrt{T\\log T})$ violation for adversarial constraints. Both these results are order-wise better than the existing bounds. The regret and violation bounds mentioned above use the best fixed decision in hindsight as the baseline. This paper further considers a dynamic baseline where the comparator sequence is time-varying. This paper shows that RECOO not only improves the existing results in the fixed-constraints setting but also {\\em for the first time,} guarantees dynamic regret and violation bounds in the adversarial-constraints setting. Our experiment results confirm that RECOO outperforms several existing algorithms for both fixed and adversarial constraints."}}
{"id": "Qs3Z-TIpgCu", "cdate": 1640995200000, "mdate": 1684096314769, "content": {"title": "Scalable and Sample Efficient Distributed Policy Gradient Algorithms in Multi-Agent Networked Systems", "abstract": "This paper studies a class of multi-agent reinforcement learning (MARL) problems where the reward that an agent receives depends on the states of other agents, but the next state only depends on the agent's own current state and action. We name it REC-MARL standing for REward-Coupled Multi-Agent Reinforcement Learning. REC-MARL has a range of important applications such as real-time access control and distributed power control in wireless networks. This paper presents a distributed policy gradient algorithm for REC-MARL. The proposed algorithm is distributed in two aspects: (i) the learned policy is a distributed policy that maps a local state of an agent to its local action and (ii) the learning/training is distributed, during which each agent updates its policy based on its own and neighbors' information. The learned algorithm achieves a stationary policy and its iterative complexity bounds depend on the dimension of local states and actions. The experimental results of our algorithm for the real-time access control and power control in wireless networks show that our policy significantly outperforms the state-of-the-art algorithms and well-known benchmarks."}}
{"id": "IXWDYqAqba7", "cdate": 1640995200000, "mdate": 1683899204962, "content": {"title": "Triple-Q: A Model-Free Algorithm for Constrained Reinforcement Learning with Sublinear Regret and Zero Constraint Violation", "abstract": "This paper presents the first model-free, simulator-free reinforcement learning algorithm for Constrained Markov Decision Processes (CMDPs) with sublinear regret and zero constraint violation. The algorithm is named Triple-Q because it includes three key components: a Q-function (also called action-value function) for the cumulative reward, a Q-function for the cumulative utility for the constraint, and a virtual-Queue that (over)-estimates the cumulative constraint violation. Under Triple-Q, at each step, an action is chosen based on the pseudo-Q-value that is a combination of the three \u201cQ\u201d values. The algorithm updates the reward and utility Q-values with learning rates that depend on the visit counts to the corresponding (state, action) pairs and are periodically reset. In the episodic CMDP setting, Triple-Q achieves $\\tilde{\\cal O}\\left(\\frac{1 }{\\delta}H^4 S^{\\frac{1}{2}}A^{\\frac{1}{2}}K^{\\frac{4}{5}} \\right)$ regret, where $K$ is the total number of episodes, $H$ is the number of steps in each episode, $S$ is the number of states, $A$ is the number of actions, and $\\delta$ is Slater\u2019s constant. Furthermore, {Triple-Q} guarantees zero constraint violation, both on expectation and with a high probability, when $K$ is sufficiently large. Finally, the computational complexity of {Triple-Q} is similar to SARSA for unconstrained MDPs, and is computationally efficient."}}
{"id": "I35jI__dZ7z", "cdate": 1640995200000, "mdate": 1683898439680, "content": {"title": "On Low-Complexity Quickest Intervention of Mutated Diffusion Processes Through Local Approximation", "abstract": "We consider the problem of controlling a mutated diffusion process with an unknown mutation time. The problem is formulated as the quickest intervention problem with the mutation modeled by a change-point, which is a generalization of the quickest change-point detection (QCD). Our goal is to intervene in the mutated process as soon as possible while maintaining a low intervention cost with optimally chosen intervention actions. This model and the proposed algorithms can be applied to pandemic prevention (such as Covid-19) or misinformation containment. We formulate the problem as a partially observed Markov decision process (POMDP) and convert it to an MDP through the belief state of the change-point. We first propose a grid approximation approach to calculate the optimal intervention policy, whose computational complexity could be very high when the number of grids is large. In order to reduce the computational complexity, we further propose a low-complexity threshold-based policy through the analysis of the first-order approximation of the value functions in the ``local intervention'' regime. Simulation results show the low-complexity algorithm has a similar performance as the grid approximation and both perform much better than the QCD-based algorithms."}}
{"id": "80ag8V37vA", "cdate": 1609459200000, "mdate": 1684096314735, "content": {"title": "A Provably-Efficient Model-Free Algorithm for Constrained Markov Decision Processes", "abstract": "This paper presents the first model-free, simulator-free reinforcement learning algorithm for Constrained Markov Decision Processes (CMDPs) with sublinear regret and zero constraint violation. The algorithm is named Triple-Q because it includes three key components: a Q-function (also called action-value function) for the cumulative reward, a Q-function for the cumulative utility for the constraint, and a virtual-Queue that (over)-estimates the cumulative constraint violation. Under Triple-Q, at each step, an action is chosen based on the pseudo-Q-value that is a combination of the three \"Q\" values. The algorithm updates the reward and utility Q-values with learning rates that depend on the visit counts to the corresponding (state, action) pairs and are periodically reset. In the episodic CMDP setting, Triple-Q achieves $\\tilde{\\cal O}\\left(\\frac{1 }{\\delta}H^4 S^{\\frac{1}{2}}A^{\\frac{1}{2}}K^{\\frac{4}{5}} \\right)$ regret, where $K$ is the total number of episodes, $H$ is the number of steps in each episode, $S$ is the number of states, $A$ is the number of actions, and $\\delta$ is Slater's constant. Furthermore, Triple-Q guarantees zero constraint violation, both on expectation and with a high probability, when $K$ is sufficiently large. Finally, the computational complexity of Triple-Q is similar to SARSA for unconstrained MDPs and is computationally efficient."}}
{"id": "7xPFp6Jowk", "cdate": 1609459200000, "mdate": 1684096314734, "content": {"title": "FORK: A FORward-looKing Actor for Model-Free Reinforcement Learning", "abstract": "In this paper, we propose a new type of Actor, named forward-looking Actor or FORK for short, for Actor-Critic algorithms. FORK can be easily integrated into a model-free Actor-Critic algorithm. Our experiments on six Box2D and MuJoCo environments with continuous state and action spaces demonstrate significant performance improvement FORK can bring to the state-of-the-art algorithms."}}
