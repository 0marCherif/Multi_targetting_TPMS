{"id": "_V3kUd_BhGD", "cdate": 1661683228761, "mdate": null, "content": {"title": "Self-pretrained V-net based on PCRL for Abdominal Organ Segmentation", "abstract": "Abdomen organ segmentation has many important clinical applications. However, the manual annotating process is time-consuming and labor-intensive. In the \"Fast and Low-resource semi-supervised Abdominal oRgan sEgmentation in CT\" challenge, the organizer provide massive unlabeled CT images. To effectively utilize unlabeled cases, we propose a self-pretrained V-net. Inspired by the preservational contrastive representation learning (PCRL), the proposed method consists of two steps: 1) using a large amount of unlabeled data to obtain a pre-trained model, 2) using a small amount of labeled data to perform fully supervised fine-tuning on the basis of the former. The feature extraction part used in both stages uses the same backbone network. The difference is that the pre-training stage introduces the additional image reconstruction branch and the corresponding momentum branch to construct image reconstruction and contrastive learning, and the fully-supervised model downstream uses a fully convolutional network for segmentation prediction. In the pre-training stage, by incorporating diverse image reconstruction tasks into the contrastive learning, the representation ability of the backbone network for specific image data during the upstream feature extraction process is enhanced. Besides, the half-precision (Float16) is used in the prediction stage, which reduces the GPU load by about 36% without losing the prediction accuracy and the maximum used GPU memory is 1719 MB. Quantitative evaluation on the FLARE2022 validation cases, this method achieves the average dice similarity coefficient (DSC) of 0.4811 and average normalized surface distance (NSD) of 0.4513."}}
{"id": "9-YjU4I3zwT", "cdate": 1658474446475, "mdate": null, "content": {"title": "Self-pretrained V-net based on PCRL for Abdominal Organ Segmentation", "abstract": "Abdomen organ segmentation has many important clinical applications. However, the manual annotating process is time-consuming and labor-intensive. In the \"Fast and Low-resource semi-supervised Abdominal oRgan sEgmentation in CT\" challenge, the organizer provide massive unlabeled CT images. To effectively utilize unlabeled cases, we propose a self-pretrained V-net. Inspired by the preservational contrastive representation learning (PCRL),  the proposed method consists of two steps: 1) using a large amount of unlabeled data to obtain a pre-trained model, 2) using a small amount of labeled data to perform fully supervised fine-tuning on the basis of the former. The feature extraction part used in both stages uses the same backbone network. The difference is that the pre-training stage introduces the additional image reconstruction branch and the corresponding momentum branch to construct image reconstruction and contrastive learning, and the fully-supervised model downstream uses a fully convolutional network for segmentation prediction. In the pre-training stage, by incorporating diverse image reconstruction tasks into the contrastive learning, the representation ability of the backbone network for specific image data during the upstream feature extraction process is enhanced. Besides, the half-precision (Float32) is used in the prediction stage, which reduces the GPU load by about 36$\\%$ without losing the prediction accuracy and the maximum used GPU memory is 1719 MB. Quantitative evauation oon the FLARE2022 validation cases, this method achieves the average dice similarity coefficient (DSC) of 0.4811 and average normalized surface distance (NSD) of 0.4513."}}
