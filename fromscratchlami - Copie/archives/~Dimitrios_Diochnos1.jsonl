{"id": "iLN8MQzfl-D", "cdate": 1634227967728, "mdate": 1634227967728, "content": {"title": " Understanding the Semantic Content of Sparse Word Embeddings Using a Commonsense Knowledge Base ", "abstract": "Word embeddings have developed into a major NLP tool with broad applicability. Understanding the semantic content of word embeddings remains an important challenge for additional applications. One aspect of this issue is to explore the interpretability of word embeddings. Sparse word embeddings have been proposed as models with improved interpretability. Continuing this line of research, we investigate the extent to which human interpretable semantic concepts emerge along the bases of sparse word representations. In order to have a broad framework for evaluation, we consider three general approaches for constructing sparse word representations, which are then evaluated in multiple ways. We propose a novel methodology to evaluate the semantic content of word embeddings using a commonsense knowledge base, applied here to the sparse case. This methodology is illustrated by two techniques using the ConceptNet knowledge base. The first approach assigns a commonsense concept label to the individual dimensions of the embedding space. The second approach uses a metric, derived by spreading activation, to quantify the coherence of coordinates along the individual axes. We also provide results on the relationship between the two approaches. The results show, for example, that in the individual dimensions of sparse word embeddings, words having high coefficients are more semantically related in terms of path lengths in the knowledge base than the ones having zero coefficients."}}
{"id": "HduWz9okIgc", "cdate": 1609459200000, "mdate": 1645767561574, "content": {"title": "On the Evolvability of Monotone Conjunctions with an Evolutionary Mutation Mechanism", "abstract": "A Bernoulli(p)n distribution Bn,p over {0, 1}n is a product distribution where each variable is satisfied with the same constant probability p. Diochnos (2016) showed that Valiant's swapping algorithm for monotone conjunctions converges efficiently under Bn,p distributions over {0, 1}n for any 0 &lt; p &lt; 1. We continue the study of monotone conjunctions in Valiant's framework of evolvability. In particular, we prove that given a Bn,p distribution characterized by some p \u2208 (0, 1/3] \u222a {1/2}, then an evolutionary mechanism that relies on the basic mutation mechanism of a (1+1) evolutionary algorithm converges efficiently, with high probability, to an \u03b5-optimal hypothesis. Furthermore, for 0 &lt; \u03b1 \u2264 3/13, a slight modification of the algorithm, with a uniform setup this time, evolves with high probability an \u03b5-optimal hypothesis, for every Bn,p distribution such that p \u2208 [\u03b1, 1/3 - 4\u03b1/9] \u222a {1/3} \u222a {1/2}."}}
{"id": "BWZGcskLgq", "cdate": 1609459200000, "mdate": 1645767561579, "content": {"title": "Learning Reliable Rules under Class Imbalance", "abstract": "We are interested in learning rules under class imbalance. In this direction we extend the traditional model of probably approximately correct (PAC) learning to also include explicitly among its goals high recall and high precision at the end of the learning process. We establish relationships for the recall and the precision of a learned hypothesis as a function of its risk and the rate of the minority class. We then show that we can PAC learn a concept class with high recall and high precision by allowing a polynomial increase in the time and space complexity of traditional PAC learning algorithms that generate hypotheses with low risk. In sequence, by introducing a pre-processing phase on such algorithms, with a constant-size overhead on the overall sample complexity, we are able, with high probability, to compute a lower bound of the true unknown rate p of the minority class, in the interval [p/8, p). Thus, we extend our positive results on PAC learning with high recall and high precision by also waiving the requirement that such a lower bound on the rate of the minority class is given to the learners by some oracle ahead of time. We conclude our work by exploring two popular PAC learning algorithms for monotone conjunctions."}}
{"id": "Rugyf0mb-Wn", "cdate": 1577836800000, "mdate": null, "content": {"title": "Understanding the Semantic Content of Sparse Word Embeddings Using a Commonsense Knowledge Base", "abstract": "Word embeddings have developed into a major NLP tool with broad applicability. Understanding the semantic content of word embeddings remains an important challenge for additional applications. One aspect of this issue is to explore the interpretability of word embeddings. Sparse word embeddings have been proposed as models with improved interpretability. Continuing this line of research, we investigate the extent to which human interpretable semantic concepts emerge along the bases of sparse word representations. In order to have a broad framework for evaluation, we consider three general approaches for constructing sparse word representations, which are then evaluated in multiple ways. We propose a novel methodology to evaluate the semantic content of word embeddings using a commonsense knowledge base, applied here to the sparse case. This methodology is illustrated by two techniques using the ConceptNet knowledge base. The first approach assigns a commonsense concept label to the individual dimensions of the embedding space. The second approach uses a metric, derived by spreading activation, to quantify the coherence of coordinates along the individual axes. We also provide results on the relationship between the two approaches. The results show, for example, that in the individual dimensions of sparse word embeddings, words having high coefficients are more semantically related in terms of path lengths in the knowledge base than the ones having zero coefficients."}}
{"id": "JKKWz9SqsYW", "cdate": 1577836800000, "mdate": null, "content": {"title": "Learning under p-tampering poisoning attacks", "abstract": "Recently, Mahloujifar and Mahmoody (Theory of Cryptography Conference\u201917) studied attacks against learning algorithms using a special case of Valiant\u2019s malicious noise, called p-tampering, in which the adversary gets to change any training example with independent probability p but is limited to only choose \u2018adversarial\u2019 examples with correct labels. They obtained p-tampering attacks that increase the error probability in the so called \u2018targeted\u2019 poisoning model in which the adversary\u2019s goal is to increase the loss of the trained hypothesis over a particular test example. At the heart of their attack was an efficient algorithm to bias the expected value of any bounded real-output function through p-tampering. In this work, we present new biasing attacks for increasing the expected value of bounded real-valued functions. Our improved biasing attacks, directly imply improved p-tampering attacks against learners in the targeted poisoning model. As a bonus, our attacks come with considerably simpler analysis. We also study the possibility of PAC learning under p-tampering attacks in the non-targeted (aka indiscriminate) setting where the adversary\u2019s goal is to increase the risk of the generated hypothesis (for a random test example). We show that PAC learning is possible under p-tampering poisoning attacks essentially whenever it is possible in the realizable setting without the attacks. We further show that PAC learning under \u2018no-mistake\u2019 adversarial noise is not possible, if the adversary could choose the (still limited to only p fraction of) tampered examples that she substitutes with adversarially chosen ones. Our formal model for such \u2018bounded-budget\u2019 tampering attackers is inspired by the notions of adaptive corruption in cryptography."}}
{"id": "8YM2DLbUpI5", "cdate": 1577836800000, "mdate": 1634413172349, "content": {"title": "Lower Bounds for Adversarially Robust PAC Learning under Evasion and Hybrid Attacks", "abstract": "In this work, we study probably approximately correct (PAC) learning under general perturbation-based adversarial attacks. In the most basic setting, referred to as an evasion attack, the adversary's goal is to misclassify an honestly sampled point x by adversarially perturbing it into x\u0303, i.e., h(x\u0303) = \u2260 c(x\u0303), where c is the ground truth concept and h is the learned hypothesis. The only limitation on the adversary is that x\u0303 is not \u201ctoo far\u201d from x, controlled by a metric measure. We first prove that for many theoretically natural input spaces of high dimension n (e.g., isotropic Gaussian in dimension n under \u2113 <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sub> perturbations), if the adversary is allowed to apply up to a sublinear amount of perturbations in the expected norm, PAC learning requires sample complexity that is exponential in the data dimension n. We then formalize hybrid attacks in which the evasion attack is preceded by a poisoning attack in which a poisoning phase is followed by specific evasion attacks. Special forms of hybrid attacks include so-called \u201cbackdoor attacks\u201d but here we focus on the general setting in which adversary's evasion attack is only controlled by a pre-specified amount of perturbation based on data dimension and aim to misclassifying the perturbed instances. We show that PAC learning is sometimes impossible under such hybrid attacks, while it is possible without the attack (e.g., due to the bounded VC dimension)."}}
{"id": "06I2AVfqMv", "cdate": 1577836800000, "mdate": null, "content": {"title": "Foreword to special issue for ISAIM 2018", "abstract": ""}}
{"id": "rXE--ZzeO6r", "cdate": 1546300800000, "mdate": null, "content": {"title": "The Curse of Concentration in Robust Learning: Evasion and Poisoning Attacks from Concentration of Measure.", "abstract": "Many modern machine learning classifiers are shown to be vulnerable to adversarial perturbations of the instances. Despite a massive amount of work focusing on making classifiers robust, the task seems quite challenging. In this work, through a theoretical study, we investigate the adversarial risk and robustness of classifiers and draw a connection to the well-known phenomenon of \u201cconcentration of measure\u201d in metric measure spaces. We show that if the metric probability space of the test instance is concentrated, any classifier with some initial constant error is inherently vulnerable to adversarial perturbations.One class of concentrated metric probability spaces are the so-called L\u00e9vy families that include many natural distributions. In this special case, our attacks only need to perturb the test instance by at most O(\u221an) to make it misclassified, where n is the data dimension. Using our general result about L\u00e9vy instance spaces, we first recover as special case some of the previously proved results about the existence of adversarial examples. However, many more L\u00e9vy families are known (e.g., product distribution under the Hamming distance) for which we immediately obtain new attacks that find adversarial examples of distance O(\u221an).Finally, we show that concentration of measure for product spaces implies the existence of forms of \u201cpoisoning\u201d attacks in which the adversary tampers with the training data with the goal of degrading the classifier. In particular, we show that for any learning algorithm that uses m training examples, there is an adversary who can increase the probability of any \u201cbad property\u201d (e.g., failing on a particular test instance) that initially happens with non-negligible probability to \u2248 1 by substituting only \u00d5e(\u221am) of the examples with other (still correctly labeled) examples."}}
{"id": "GGACyntVFr5", "cdate": 1514764800000, "mdate": null, "content": {"title": "Learning under $p$-Tampering Attacks", "abstract": "Recently, Mahloujifar and Mahmoody (TCC\u201917) studied attacks against learning algorithms using a special case of Valiant\u2019s malicious noise, called $p$-tampering, in which the adversary gets to change any training example with independent probability $p$ but is limited to only choose \u2018adversarial\u2019 examples with correct labels. They obtained $p$-tampering attacks that increase the error probability in the so called \u2018targeted\u2019 poisoning model in which the adversary\u2019s goal is to increase the loss of the trained hypothesis over a particular test example. At the heart of their attack was an efficient algorithm to bias the average output of any bounded real-valued function through $p$-tampering. In this work, we present new biasing attacks for biasing the average output of bounded real-valued functions. Our new biasing attacks achieve in \\emph{polynomial-time} the the best bias achieved by MM16 through an \\emph{exponential} time $p$-tampering attack. Our improved biasing attacks, directly imply improved $p$-tampering attacks against learners in the targeted poisoning model. As a bonus, our attacks come with considerably simpler analysis compared to previous attacks. We also study the possibility of PAC learning under $p$-tampering attacks in the \\emph{non-targeted} (aka indiscriminate) setting where the adversary\u2019s goal is to increase the risk of the generated hypothesis (for a random test example). We show that PAC learning is \\emph{possible} under $p$-tampering poisoning attacks essentially whenever it is possible in the realizable setting without the attacks. We further show that PAC learning under \u2018no-mistake\u2019 adversarial noise is \\emph{not} possible, if the adversary could choose the (still limited to only $p$ fraction of) tampered examples that she substitutes with adversarially chosen ones. Our formal model for such \u2018bounded-budget\u2019 tampering attackers is inspired by the notions of (strong) adaptive corruption in secure multi-party computation."}}
{"id": "B1bK8PbubS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Adversarial Risk and Robustness: General Definitions and Implications for the Uniform Distribution", "abstract": "We study adversarial perturbations when the instances are uniformly distributed over {0,1}^n. We study both \"inherent\" bounds that apply to any problem and any classifier for such a problem as well as bounds that apply to specific problems and specific hypothesis classes. As the current literature contains multiple definitions of adversarial risk and robustness, we start by giving a taxonomy for these definitions based on their direct goals; we identify one of them as the one guaranteeing misclassification by pushing the instances to the error region. We then study some classic algorithms for learning monotone conjunctions and compare their adversarial risk and robustness under different definitions by attacking the hypotheses using instances drawn from the uniform distribution. We observe that sometimes these definitions lead to significantly different bounds. Thus, this study advocates for the use of the error-region definition, even though other definitions, in other contexts with context-dependent assumptions, may coincide with the error-region definition. Using the error-region definition of adversarial perturbations, we then study inherent bounds on risk and robustness of any classifier for any classification problem whose instances are uniformly distributed over {0,1}^n. Using the isoperimetric inequality for the Boolean hypercube, we show that for initial error 0.01, there always exists an adversarial perturbation that changes O(\u221an) bits of the instances to increase the risk to 0.5, making classifier's decisions meaningless. Furthermore, by also using the central limit theorem we show that when n\u2192\u221e, at most c\u221an bits of perturbations, for a universal constant c&lt;1.17, suffice for increasing the risk to 0.5, and the same c\u221an bits of perturbations on average suffice to increase the risk to 1, hence bounding the robustness by c\u221an."}}
