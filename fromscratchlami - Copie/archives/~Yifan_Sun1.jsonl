{"id": "3eWUMYVQo1", "cdate": 1609459200000, "mdate": 1683894361679, "content": {"title": "Screening for a Reweighted Penalized Conditional Gradient Method", "abstract": "The conditional gradient method (CGM) is widely used in large-scale sparse convex optimization, having a low per iteration computational cost for structured sparse regularizers and a greedy approach to collecting nonzeros. We explore the sparsity acquiring properties of a general penalized CGM (P-CGM) for convex regularizers and a reweighted penalized CGM (RP-CGM) for nonconvex regularizers, replacing the usual convex constraints with gauge-inspired penalties. This generalization does not increase the per-iteration complexity noticeably. Without assuming bounded iterates or using line search, we show $O(1/t)$ convergence of the gap of each subproblem, which measures distance to a stationary point. We couple this with a screening rule which is safe in the convex case, converging to the true support at a rate $O(1/(\\delta^2))$ where $\\delta \\geq 0$ measures how close the problem is to degeneracy. In the nonconvex case the screening rule converges to the true support in a finite number of iterations, but is not necessarily safe in the intermediate iterates. In our experiments, we verify the consistency of the method and adjust the aggressiveness of the screening rule by tuning the concavity of the regularizer."}}
{"id": "p7Ugqu8toee", "cdate": 1577836800000, "mdate": 1683894361534, "content": {"title": "Safe Screening for the Generalized Conditional Gradient Method", "abstract": "The conditional gradient method (CGM) has been widely used for fast sparse approximation, having a low per iteration computational cost for structured sparse regularizers. We explore the sparsity acquiring properties of a generalized CGM (gCGM), where the constraint is replaced by a penalty function based on a gauge penalty; this can be done without significantly increasing the per-iteration computation, and applies to general notions of sparsity. Without assuming bounded iterates, we show $O(1/t)$ convergence of the function values and gap of gCGM. We couple this with a safe screening rule, and show that at a rate $O(1/(t\\delta^2))$, the screened support matches the support at the solution, where $\\delta \\geq 0$ measures how close the problem is to being degenerate. In our experiments, we show that the gCGM for these modified penalties have similar feature selection properties as common penalties, but with potentially more stability over the choice of hyperparameter."}}
{"id": "EkvwF37zlcmG", "cdate": 1577836800000, "mdate": 1632868281999, "content": {"title": "Greed Meets Sparsity: Understanding and Improving Greedy Coordinate Descent for Sparse Optimization", "abstract": "We consider greedy coordinate descent (GCD) for composite problems with sparsity inducing regularizers, including 1-norm regularization and non-negative constraints. Empirical evidence strongly sug..."}}
{"id": "Cj1rhAZMePO", "cdate": 1577836800000, "mdate": 1683894361821, "content": {"title": "Atomic Decomposition via Polar Alignment: The Geometry of Structured Optimization", "abstract": ""}}
{"id": "rJeYYJKvDB", "cdate": 1569324929439, "mdate": null, "content": {"title": "Are we there yet? Manifold identification of gradient-related proximal methods", "abstract": "In machine learning, models that generalize better often generate outputs that lie on a low-dimensional manifold. Recently, several works have separately shown finite-time manifold identification by some proximal methods. In this work we provide a unified view by giving a simple condition under which any proximal method using a constant step size can achieve finite-iteration manifold detection. For several key methods (FISTA, DRS, ADMM, SVRG, SAGA, and RDA) we give an iteration bound, characterized in terms of their variable convergence rate and a problem-dependent constant that indicates problem degeneracy. For popular models, this constant is related to certain data assumptions, which gives intuition as to when lower active set complexity may be expected in practice."}}
{"id": "yQIOrlQ_o2", "cdate": 1546300800000, "mdate": 1683894362231, "content": {"title": "Are we there yet? Manifold identification of gradient-related proximal methods", "abstract": "In machine learning, models that generalize better often generate outputs that lie on a low-dimensional manifold. Recently, several works have separately shown finite-time manifold identification b..."}}
{"id": "dyMRpU24Cw", "cdate": 1546300800000, "mdate": 1683894361761, "content": {"title": "Bundle methods for dual atomic pursuit", "abstract": "The aim of structured optimization is to assemble a solution, using a given set of (possibly uncountably infinite) atoms, to fit a model to data. A two-stage algorithm based on gauge duality and bundle method is proposed. The first stage discovers the optimal atomic support for the primal problem by solving a sequence of approximations of the dual problem using a bundle-type method. The second stage recovers the approximate primal solution using the atoms discovered in the first stage. The overall approach leads to implementable and efficient algorithms for large problems."}}
{"id": "XTLxf6lomEf", "cdate": 1546300800000, "mdate": 1683894362861, "content": {"title": "One-shot atomic detection", "abstract": "Feature selection in data science involves identifying the most prominent and uncorrelated features in the data, which can be useful for compression and interpretability. If these feature can be easily extracted, then a model can be trained over a reduced set of weights, which leads to more efficient training and possibly more robust classifiers. There are many approaches to feature selection; in this work, we propose screening the \u201catoms\u201d of a gradient of a loss function taken at a random point. We illustrate this approach on sparse and low-rank optimization problems. Despite the simplicity of the approach, we are often able to select the dominant features easily, and greatly improve the runtime and robustness in training overparametrized models."}}
{"id": "K1Vb7kZhXu", "cdate": 1546300800000, "mdate": 1683894361993, "content": {"title": "Redundancy Techniques for Straggler Mitigation in Distributed Optimization and Learning", "abstract": "Performance of distributed optimization and learning systems is bottlenecked by \u201cstraggler\u201d nodes and slow communication links, which significantly delay computation. We propose a distributed optimization framework where the dataset is \u201cencoded\u201d to have an over-complete representation with built-in redundancy, and the straggling nodes in the system are dynamically treated as missing, or as \u201cerasures\u201d at every iteration, whose loss is compensated by the embedded redundancy. For quadratic loss functions, we show that under a simple encoding scheme, many optimization algorithms (gradient descent, L-BFGS, and proximal gradient) operating under data parallelism converge to an approximate solution even when stragglers are ignored. Furthermore, we show a similar result for a wider class of convex loss functions when operating under model parallelism. The applicable classes of objectives covers several popular learning problems such as linear regression, LASSO, support vector machine, collaborative filtering, and generalized linear models including logistic regression. These convergence results are deterministic, i.e., they establish sample path convergence for arbitrary sequences of delay patterns or distributions on the nodes, and are independent of the tail behavior of the delay distribution. We demonstrate that equiangular tight frames have desirable properties as encoding matrices, and propose efficient mechanisms for encoding large-scale data. We implement the proposed technique on Amazon EC2 clusters, and demonstrate its performance over several learning problems, including matrix factorization, LASSO, ridge regression and logistic regression, and compare the proposed method with uncoded, asynchronous, and data replication strategies."}}
{"id": "I5m5THonps", "cdate": 1546300800000, "mdate": 1683894362032, "content": {"title": "Polar Alignment and Atomic Decomposition", "abstract": "Structured optimization uses a prescribed set of atoms to assemble a solution that fits a model to data. Polarity, which extends the familiar notion of orthogonality from linear sets to general convex sets, plays a special role in a simple and geometric form of convex duality. This duality correspondence yields a general notion of alignment that leads to an intuitive and complete description of how atoms participate in the final decomposition of the solution. The resulting geometric perspective leads to variations of existing algorithms effective for large-scale problems. We illustrate these ideas with many examples, including applications in matrix completion and morphological component analysis for the separation of mixtures of signals."}}
