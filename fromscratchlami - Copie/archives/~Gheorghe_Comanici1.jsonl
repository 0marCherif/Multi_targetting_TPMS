{"id": "TYw-9_rdP", "cdate": 1640995200000, "mdate": 1681695013050, "content": {"title": "Learning how to Interact with a Complex Interface using Hierarchical Reinforcement Learning", "abstract": "Hierarchical Reinforcement Learning (HRL) allows interactive agents to decompose complex problems into a hierarchy of sub-tasks. Higher-level tasks can invoke the solutions of lower-level tasks as if they were primitive actions. In this work, we study the utility of hierarchical decompositions for learning an appropriate way to interact with a complex interface. Specifically, we train HRL agents that can interface with applications in a simulated Android device. We introduce a Hierarchical Distributed Deep Reinforcement Learning architecture that learns (1) subtasks corresponding to simple finger gestures, and (2) how to combine these gestures to solve several Android tasks. Our approach relies on goal conditioning and can be used more generally to convert any base RL agent into an HRL agent. We use the AndroidEnv environment to evaluate our approach. For the experiments, the HRL agent uses a distributed version of the popular DQN algorithm to train different components of the hierarchy. While the native action space is completely intractable for simple DQN agents, our architecture can be used to establish an effective way to interact with different tasks, significantly improving the performance of the same DQN agent over different levels of abstraction."}}
{"id": "LGvlCcMgWqb", "cdate": 1621629748761, "mdate": null, "content": {"title": "Temporally Abstract Partial Models", "abstract": "Humans and animals have the ability to reason and make predictions about different courses of action at many time scales. In reinforcement learning, option models (Sutton, Precup \\& Singh, 1999; Precup, 2000) provide the framework for this kind of temporally abstract prediction and reasoning. Natural intelligent agents are also able to focus their attention on courses of action that are relevant or feasible in a given situation, sometimes termed affordable actions. In this paper, we define a notion of affordances for options, and develop temporally abstract partial option models, that take into account the fact that an option might be affordable only in certain situations. We analyze the trade-offs between estimation and approximation error in planning and learning when using such models, and identify some interesting special cases. Additionally, we empirically demonstrate the ability to learn both affordances and partial option models online resulting in improved sample efficiency and planning time in the Taxi domain."}}
{"id": "r6Tu5ZPsq6m", "cdate": 1609459200000, "mdate": 1681695013020, "content": {"title": "Temporally Abstract Partial Models", "abstract": "title> <link rel=\"stylesheet\" href=\"/static/papers/css/papers.css\" /> <meta name=\"citation_title\" content=\"Temporally Abstract Partial Models\" /> <meta name=\"citation_author\" content=\"Khetarpal, Khimya\" /> <meta name=\"citation_author\" content=\"Ahmed, Zafarali\" /> <meta name=\"citation_author\" content=\"Comanici, Gheorghe\" /> <meta name=\"citation_author\" content=\"Precup, Doina\" /> <meta name=\"citation_journal_title\" content=\"Advances in Neural Information Processing Systems\" /> <meta name=\"citation_volume\" content=\"34\" /> <meta name=\"citation_firstpage\" content=\"1979\" /> <meta name=\"citation_lastpage\" content=\"1991\" /> <meta name=\"citation_pdf_url\" content=\"https://proceedings.neurips.cc/paper_files/paper/2021/file/0f3d014eead934bbdbacb62a01dc4831-Paper.pdf\" /> <meta name=\"citation_publication_date\" content=\"2021-12-06\" /><!-- Bootstrap CSS --> <!-- https://codepen.io/surjithctly/pen/PJqKzQ --> <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\" /> <link href=\"/static/menus/css/menus.css\" rel=\"stylesheet\" id=\"bootstrap-css\" /> <link rel=\"stylesheet\" href=\"https://use.fontawesome.com/releases/v5.8.1/css/all.css\" integrity=\"sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf\" crossorigin=\"anonymous\" /> <script type=\"text/javascript\" async=\"async\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML\"></script> <script type=\"text/x-mathjax-config\"> <![CDATA[ MathJax.Hub.Config({ \"tex2jax\": { \"inlineMath\": [[\"$\",\"$\"], [\"\\\\(\",\"\\\\)\"]], \"displayMath\": [[\"\\\\[\",\"\\\\]\"]], \"processEscapes\": true } } ); ]]> </script> <style> <![CDATA[ @media (prefers-color-scheme: dark) { body { background-color: #333; color: #eee; } } .btn-spacer { margin: 2px; } .footer { position: fixed; left: 0; bottom: 0; width: 100%; background-color: #eee; color: black; } ]]> </style> <nav class=\"navbar navbar-expand-md navbar-light bg-light\"> <button class=\"navbar-toggler\" type=\"button\" data-toggle=\"collapse\" data-target=\"#navbarToggler6\" aria-controls=\"navbarToggler6\" aria-expanded=\"false\" aria-label=\"Toggle navigation\"><span class=\"navbar-toggler-icon\"></span></button> <div class=\"collapse navbar-collapse\" id=\"navbarToggler6\"> <a class=\"navbar-brand\" href=\"/\">NeurIPS Proceedings</a> <ul class=\"navbar-nav mr-auto mt-2 mt-md-0\"> <li class=\"nav-item\"> <a class=\"nav-link\" href=\"/admin/login/?next=/admin/\"><i class=\"fas fa-sign-in-alt\" title=\"Login\"></i></a> <li class=\"nav-item\"> <a class=\"nav-link\" href=\"/admin/logout/?nextp=/admin\"><i class=\"fas fa-sign-out-alt\" title=\"Logout\"></i></a> <form class=\"form-inline my-2 my-lg-0\" method=\"get\" role=\"search\" action=\"/papers/search\"> <input class=\"form-control mr-sm-2\" type=\"text\" name=\"q\" placeholder=\"Search\" aria-label=\"Search\" id=\"navsearch\" /> <button class=\"btn btn-outline-success my-2 my-sm-0\" type=\"submit\">Search</button>"}}
{"id": "RxYoPWtbYMT", "cdate": 1609459200000, "mdate": 1681695013109, "content": {"title": "AndroidEnv: A Reinforcement Learning Platform for Android", "abstract": "We introduce AndroidEnv, an open-source platform for Reinforcement Learning (RL) research built on top of the Android ecosystem. AndroidEnv allows RL agents to interact with a wide variety of apps and services commonly used by humans through a universal touchscreen interface. Since agents train on a realistic simulation of an Android device, they have the potential to be deployed on real devices. In this report, we give an overview of the environment, highlighting the significant features it provides for research, and we present an empirical evaluation of some popular reinforcement learning agents on a set of tasks built on this platform."}}
{"id": "BUQb_tjdsz5", "cdate": 1609459200000, "mdate": 1648229248127, "content": {"title": "Temporally Abstract Partial Models", "abstract": "Humans and animals have the ability to reason and make predictions about different courses of action at many time scales. In reinforcement learning, option models (Sutton, Precup \\& Singh, 1999; Precup, 2000) provide the framework for this kind of temporally abstract prediction and reasoning. Natural intelligent agents are also able to focus their attention on courses of action that are relevant or feasible in a given situation, sometimes termed affordable actions. In this paper, we define a notion of affordances for options, and develop temporally abstract partial option models, that take into account the fact that an option might be affordable only in certain situations. We analyze the trade-offs between estimation and approximation error in planning and learning when using such models, and identify some interesting special cases. Additionally, we demonstrate empirically the potential impact of partial option models on the efficiency of planning."}}
{"id": "5foFf93vd7u", "cdate": 1609459200000, "mdate": 1662327789098, "content": {"title": "The Option Keyboard: Combining Skills in Reinforcement Learning", "abstract": "The ability to combine known skills to create new ones may be crucial in the solution of complex reinforcement learning problems that unfold over extended periods. We argue that a robust way of combining skills is to define and manipulate them in the space of pseudo-rewards (or \"cumulants\"). Based on this premise, we propose a framework for combining skills using the formalism of options. We show that every deterministic option can be unambiguously represented as a cumulant defined in an extended domain. Building on this insight and on previous results on transfer learning, we show how to approximate options whose cumulants are linear combinations of the cumulants of known options. This means that, once we have learned options associated with a set of cumulants, we can instantaneously synthesise options induced by any linear combination of them, without any learning involved. We describe how this framework provides a hierarchical interface to the environment whose abstract actions correspond to combinations of basic skills. We demonstrate the practical benefits of our approach in a resource management problem and a navigation task involving a quadrupedal simulated robot."}}
{"id": "r4lVdYi_izc", "cdate": 1577836800000, "mdate": 1648229248421, "content": {"title": "What can I do here? A Theory of Affordances in Reinforcement Learning", "abstract": "Reinforcement learning algorithms usually assume that all actions are always available to an agent. However, both people and animals understand the general link between the features of their environment and the actions that are feasible. Gibson (1977) coined the term \"affordances\" to describe the fact that certain states enable an agent to do certain actions, in the context of embodied agents. In this paper, we develop a theory of affordances for agents who learn and plan in Markov Decision Processes. Affordances play a dual role in this case. On one hand, they allow faster planning, by reducing the number of actions available in any given situation. On the other hand, they facilitate more efficient and precise learning of transition models from data, especially when such models require function approximation. We establish these properties through theoretical results as well as illustrative examples. We also propose an approach to learn affordances and use it to estimate transition models that are simpler and generalize better."}}
{"id": "1Rpb-OwCLbY", "cdate": 1577836800000, "mdate": null, "content": {"title": "What can I do here? A Theory of Affordances in Reinforcement Learning", "abstract": "Reinforcement learning algorithms usually assume that all actions are always available to an agent. However, both people and animals understand the general link between the features of their enviro..."}}
{"id": "CK4g6H5StdX", "cdate": 1546300800000, "mdate": null, "content": {"title": "The Option Keyboard: Combining Skills in Reinforcement Learning", "abstract": "The ability to combine known skills to create new ones may be crucial in the solution of complex reinforcement learning problems that unfold over extended periods. We argue that a robust way of combining skills is to define and manipulate them in the space of pseudo-rewards (or \"cumulants\"). Based on this premise, we propose a framework for combining skills using the formalism of options. We show that every deterministic option can be unambiguously represented as a cumulant defined in an extended domain. Building on this insight and on previous results on transfer learning, we show how to approximate options whose cumulants are linear combinations of the cumulants of known options. This means that, once we have learned options associated with a set of cumulants, we can instantaneously synthesise options induced by any linear combination of them, without any learning involved. We describe how this framework provides a hierarchical interface to the environment whose abstract actions correspond to combinations of basic skills. We demonstrate the practical benefits of our approach in a resource management problem and a navigation task involving a quadrupedal simulated robot."}}
{"id": "rygvZ2RcYm", "cdate": 1538087934850, "mdate": null, "content": {"title": "Knowledge Representation for Reinforcement Learning using General Value Functions", "abstract": "Reinforcement learning (RL) is a very powerful approach for learning good control strategies from data. Value functions are a key concept for reinforcement learning, as they guide the search for good policies. A lot of effort has been devoted to designing and improving algorithms for learning value functions. In this paper, we argue that value functions are also a very natural way of providing a framework for knowledge representation for reinforcement learning agents. We show that generalized value functions provide a unifying lens for many algorithms, including policy gradient, successor features, option models and policies, and other forms of hierarchical reinforcement learning. We also demonstrate the potential of this representation to provide new, useful algorithms."}}
