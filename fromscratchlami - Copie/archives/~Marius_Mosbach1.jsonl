{"id": "PRgVsmtAW7", "cdate": 1702927048533, "mdate": 1702927048533, "content": {"title": "Weaker Than You Think: A Critical Look at Weakly Supervised Learning", "abstract": "Weakly supervised learning is a popular approach for training machine learning models in low-resource settings. Instead of requesting high-quality yet costly human annotations, it allows training models with noisy annotations obtained from various weak sources. Recently, many sophisticated approaches have been proposed for robust training under label noise, reporting impressive results. In this paper, we revisit the setup of these approaches and find that the benefits brought by these approaches are significantly overestimated. Specifically, we find that the success of existing weakly supervised learning approaches heavily relies on the availability of clean validation samples which, as we show, can be leveraged much more efficiently by simply training on them. After using these clean labels in training, the advantages of using these sophisticated approaches are mostly wiped out. This remains true even when reducing the size of the available clean data to just five samples per class, making these approaches impractical. To understand the true value of weakly supervised learning, we thoroughly analyze diverse NLP datasets and tasks to ascertain when and why weakly supervised approaches work. Based on our findings, we provide recommendations for future research."}}
{"id": "JF8V0bCcGY", "cdate": 1672531200000, "mdate": 1693808748960, "content": {"title": "Weaker Than You Think: A Critical Look at Weakly Supervised Learning", "abstract": ""}}
{"id": "2W1nMs0_xPO", "cdate": 1672531200000, "mdate": 1693808748961, "content": {"title": "Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation", "abstract": ""}}
{"id": "HKW4Kqf4I-9", "cdate": 1646834321458, "mdate": null, "content": {"title": "Multilingual language model Adaptive Fine-Tuning: A Study on African Languages", "abstract": "Multilingual pre-trained language models (PLMs) have demonstrated impressive performance on several downstream tasks on both high-resourced and low-resourced languages. However, there is still a large performance drop for languages unseen during pre-training, especially African languages. One of the most effective approaches to adapt to a new language is language adaptive fine-tuning (LAFT) \u2014 fine-tuning a multilingual PLM on monolingual texts using the pre-training objective. However, African languages with large monolingual texts are few, and adapting to each of them individually takes large disk space and limits the cross-lingual transfer abilities of the resulting models because they have been specialized for a single language. In this paper, we perform multilingual adaptive fine-tuning (MAFT) on 17 most-resourced African languages and three other high-resource languages widely spoken on the continent \u2013 English, French, and Arabic to encourage cross-lingual transfer learning. Additionally, to further specialize the multilingual PLM, we removed vocabulary tokens from the embedding layer that corresponds to non-African writing scripts before MAFT, thus reducing the model size by 50%. Our evaluation on two multilingual PLMs (AfriBERTa and XLM-R) and three NLP tasks (NER, news topic classification, and sentiment classification) shows that our approach is competitive to applying LAFT on individual languages while requiring significantly less disk space."}}
{"id": "dT8ss75DVhY", "cdate": 1640995200000, "mdate": 1682326148723, "content": {"title": "Artefact Retrieval: Overview of NLP Models with Knowledge Base Access", "abstract": "Many NLP models gain performance by having access to a knowledge base. A lot of research has been devoted to devising and improving the way the knowledge base is accessed and incorporated into the model, resulting in a number of mechanisms and pipelines. Despite the diversity of proposed mechanisms, there are patterns in the designs of such systems. In this paper, we systematically describe the typology of artefacts (items retrieved from a knowledge base), retrieval mechanisms and the way these artefacts are fused into the model. This further allows us to uncover combinations of design decisions that had not yet been tried. Most of the focus is given to language models, though we also show how question answering, fact-checking and knowledgable dialogue models fit into this system as well. Having an abstract model which can describe the architecture of specific models also helps with transferring these architectures between multiple NLP tasks."}}
{"id": "UUqTk5NjPuK", "cdate": 1640995200000, "mdate": 1682326148685, "content": {"title": "Knowledge Base Index Compression via Dimensionality and Precision Reduction", "abstract": "Recently neural network based approaches to knowledge-intensive NLP tasks, such as question answering, started to rely heavily on the combination of neural retrievers and readers. Retrieval is typically performed over a large textual knowledge base (KB) which requires significant memory and compute resources, especially when scaled up. On HotpotQA we systematically investigate reducing the size of the KB index by means of dimensionality (sparse random projections, PCA, autoencoders) and numerical precision reduction. Our results show that PCA is an easy solution that requires very little data and is only slightly worse than autoencoders, which are less stable. All methods are sensitive to pre- and post-processing and data should always be centered and normalized both before and after dimension reduction. Finally, we show that it is possible to combine PCA with using 1bit per dimension. Overall we achieve (1) 100$\\times$ compression with 75%, and (2) 24$\\times$ compression with 92% original retrieval performance."}}
{"id": "TqokvtGZprx", "cdate": 1640995200000, "mdate": 1682326148728, "content": {"title": "MCSE: Multimodal Contrastive Learning of Sentence Embeddings", "abstract": "Miaoran Zhang, Marius Mosbach, David Adelani, Michael Hedderich, Dietrich Klakow. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022."}}
{"id": "Si0PKuHSkB", "cdate": 1640995200000, "mdate": 1682326148752, "content": {"title": "Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning", "abstract": ""}}
{"id": "NmUqi_kVcH", "cdate": 1640995200000, "mdate": 1682326148792, "content": {"title": "Fusing Sentence Embeddings Into LSTM-based Autoregressive Language Models", "abstract": "Although masked language models are highly performant and widely adopted by NLP practitioners, they can not be easily used for autoregressive language modelling (next word prediction and sequence probability estimation). We present an LSTM-based autoregressive language model which uses prefix embeddings (from a pretrained masked language model) via fusion (e.g. concatenation) to obtain a richer context representation for language modelling. We find that fusion helps reliably in lowering the perplexity (16.74 $\\rightarrow$ 15.80), which is even preserved after a transfer to a dataset from a different domain than the training data. We also evaluate the best-performing fusion model by correlating its next word surprisal estimates with human reading times. Contradicting our expectation, and despite the improvement in perplexity overall, the correlation remains the same as for the baseline model. Lastly, while we focus on language models pre-trained on text as the sources for the fusion, our approach can be possibly extended to fuse any information represented as a fixed-size vector into an auto-regressive language model. These include e.g. sentence external information retrieved for a knowledge base or representations of multi-modal encoders."}}
{"id": "HaD-D7ILbn", "cdate": 1640995200000, "mdate": 1681661710852, "content": {"title": "Measuring Causal Effects of Data Statistics on Language Model's 'Factual' Predictions", "abstract": "Large amounts of training data are one of the major reasons for the high performance of state-of-the-art NLP models. But what exactly in the training data causes a model to make a certain prediction? We seek to answer this question by providing a language for describing how training data influences predictions, through a causal framework. Importantly, our framework bypasses the need to retrain expensive models and allows us to estimate causal effects based on observational data alone. Addressing the problem of extracting factual knowledge from pretrained language models (PLMs), we focus on simple data statistics such as co-occurrence counts and show that these statistics do influence the predictions of PLMs, suggesting that such models rely on shallow heuristics. Our causal framework and our results demonstrate the importance of studying datasets and the benefits of causality for understanding NLP models."}}
