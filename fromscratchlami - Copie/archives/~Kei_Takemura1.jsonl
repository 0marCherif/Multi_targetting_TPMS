{"id": "bc-jwSgUVf", "cdate": 1609459200000, "mdate": null, "content": {"title": "A Parameter-Free Algorithm for Misspecified Linear Contextual Bandits", "abstract": "We investigate the misspecified linear contextual bandit (MLCB) problem, which is a generalization of the linear contextual bandit (LCB) problem. The MLCB problem is a decision-making problem in which a learner observes $d$-dimensional feature vectors, called arms, chooses an arm from $K$ arms, and then obtains a reward from the chosen arm in each round. The learner aims to maximize the sum of the rewards over $T$ rounds. In contrast to the LCB problem, the rewards in the MLCB problem may not be represented by a linear function in feature vectors; instead, it is approximated by a linear function with additive approximation parameter $\\varepsilon \\geq 0$. In this paper, we propose an algorithm that achieves $\\tilde{O}(\\sqrt{dT\\log(K)} + \\varepsilon\\sqrt{d}T)$ regret, where $\\tilde{O}(\\cdot)$ ignores polylogarithmic factors in $d$ and $T$. This is the first algorithm that guarantees a high-probability regret bound for the MLCB problem without knowledge of the approximation parameter $\\varepsilon$."}}
{"id": "6LEwwly_mqk", "cdate": 1609459200000, "mdate": null, "content": {"title": "Near-Optimal Regret Bounds for Contextual Combinatorial Semi-Bandits with Linear Payoff Functions", "abstract": "The contextual combinatorial semi-bandit problem with linear payoff functions is a decision-making problem in which a learner chooses a set of arms with the feature vectors in each round under given constraints so as to maximize the sum of rewards of arms. Several existing algorithms have regret bounds that are optimal with respect to the number of rounds $T$. However, there is a gap of $\\tilde{O}(\\max(\\sqrt{d}, \\sqrt{k}))$ between the current best upper and lower bounds, where $d$ is the dimension of the feature vectors, $k$ is the number of the chosen arms in a round, and $\\tilde{O}(\\cdot)$ ignores the logarithmic factors. The dependence of $k$ and $d$ is of practical importance because $k$ may be larger than $T$ in real-world applications such as recommender systems. In this paper, we fill the gap by improving the upper and lower bounds. More precisely, we show that the C${}^2$UCB algorithm proposed by Qin, Chen, and Zhu (2014) has the optimal regret bound $\\tilde{O}(d\\sqrt{kT} + dk)$ for the partition matroid constraints. For general constraints, we propose an algorithm that modifies the reward estimates of arms in the C${}^2$UCB algorithm and demonstrate that it enjoys the optimal regret bound for a more general problem that can take into account other objectives simultaneously. We also show that our technique would be applicable to related problems. Numerical experiments support our theoretical results and considerations."}}
{"id": "3OebrhlSBK6", "cdate": 1577836800000, "mdate": null, "content": {"title": "Delay and Cooperation in Nonstochastic Linear Bandits", "abstract": "This paper offers a nearly optimal algorithm for online linear optimization with delayed bandit feedback. Online linear optimization with bandit feedback, or nonstochastic linear bandits, provides a generic framework for sequential decision-making problems with limited information. This framework, however, assumes that feedback can be observed just after choosing the action, and, hence, does not apply directly to many practical applications, in which the feedback can often only be obtained after a while. To cope with such situations, we consider problem settings in which the feedback can be observed $d$ rounds after the choice of an action, and propose an algorithm for which the expected regret is $\\tilde{O}( \\sqrt{m (m + d) T} )$, ignoring logarithmic factors in $m$ and $T$, where $m$ and $T$ denote the dimensionality of the action set and the number of rounds, respectively. This algorithm achieves nearly optimal performance, as we are able to show that arbitrary algorithms suffer the regret of $\\Omega(\\sqrt{m (m+d) T})$ in the worst case. To develop the algorithm, we introduce a technique we refer to as \\textit{distribution truncation}, which plays an essential role in bounding the regret. We also apply our approach to cooperative bandits, as studied by Cesa-Bianchi et al. [17] and Bar-On and Mansour [12], and extend their results to the linear bandits setting."}}
{"id": "abdfJi7rWH9", "cdate": 1546300800000, "mdate": null, "content": {"title": "An Arm-Wise Randomization Approach to Combinatorial Linear Semi-Bandits", "abstract": "Combinatorial linear semi-bandits (CLS) are widely applicable frameworks of sequential decision-making, in which a learner chooses a subset of arms from a given set of arms associated with feature vectors. Existing algorithms work poorly for the clustered case, in which the feature vectors form several large clusters. This shortcoming is critical in practice because it can be found in many applications, including recommender systems. In this paper, we clarify why such a shortcoming occurs, and we introduce a key technique of arm-wise randomization to overcome it. We propose two algorithms with this technique: the perturbed C^2 UCB (PC^2 UCB) and the Thompson sampling (TS). Our empirical evaluation with artificial and real-world datasets demonstrates that the proposed algorithms with the arm-wise randomization technique outperform the existing algorithms without this technique, especially for the clustered case. Our contributions also include theoretical analyses that provide high probability asymptotic regret bounds for our algorithms."}}
{"id": "F-QnkhGxQ-k", "cdate": 1546300800000, "mdate": null, "content": {"title": "Oracle-Efficient Algorithms for Online Linear Optimization with Bandit Feedback", "abstract": "We propose computationally efficient algorithms for \\textit{online linear optimization with bandit feedback}, in which a player chooses an \\textit{action vector} from a given (possibly infinite) set $\\mathcal{A} \\subseteq \\mathbb{R}^d$, and then suffers a loss that can be expressed as a linear function in action vectors. Although existing algorithms achieve an optimal regret bound of $\\tilde{O}(\\sqrt{T})$ for $T$ rounds (ignoring factors of $\\mathrm{poly} (d, \\log T)$), computationally efficient ways of implementing them have not yet been specified, in particular when $|\\mathcal{A}|$ is not bounded by a polynomial size in $d$. A standard way to pursue computational efficiency is to assume that we have an efficient algorithm referred to as \\textit{oracle} that solves (offline) linear optimization problems over $\\mathcal{A}$. Under this assumption, the computational efficiency of a bandit algorithm can then be measured in terms of \\textit{oracle complexity}, i.e., the number of oracle calls. Our contribution is to propose algorithms that offer optimal regret bounds of $\\tilde{O}(\\sqrt{T})$ as well as low oracle complexity for both \\textit{non-stochastic settings} and \\textit{stochastic settings}. Our algorithm for non-stochastic settings has an oracle complexity of $\\tilde{O}( T )$ and is the first algorithm that achieves both a regret bound of $\\tilde{O}( \\sqrt{T} )$ and an oracle complexity of $\\tilde{O} ( \\mathrm{poly} ( T ) )$, given only linear optimization oracles. Our algorithm for stochastic settings calls the oracle only $O( \\mathrm{poly} (d, \\log T))$ times, which is smaller than the current best oracle complexity of $O( T )$ if $T$ is sufficiently large."}}
