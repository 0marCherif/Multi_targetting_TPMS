{"id": "A7EXfIi2au", "cdate": 1668776542657, "mdate": 1668776542657, "content": {"title": "Geometry Aware Constrained Optimization Techniques for Deep Learning", "abstract": "In this paper, we generalize the Stochastic Gradient Descent (SGD) and RMSProp algorithms to the setting of Riemannian optimization. SGD is a popular method for large scale optimization. In particular, it is widely used to train the weights of Deep Neural Networks. However, gradients computed using standard SGD can have large variance, which is detrimental for the convergence rate of the algorithm. Other methods such as RMSProp and ADAM address this issue. Nevertheless, these methods cannot be directly applied to constrained optimization problems. In this paper, we extend some popular optimization algorithm to the Riemannian (constrained) setting. We substantiate our proposed extensions with a range of relevant problems in machine learning such as incremental Principal Component Analysis, computating the Riemannian centroids of SPD matrices, and Deep Metric Learning. We achieve competitive results against the state of the art for fine-grained object recognition datasets."}}
{"id": "z5-chidgZU3", "cdate": 1621630220524, "mdate": null, "content": {"title": "Risk Monotonicity in Statistical Learning", "abstract": "Acquisition of data is a difficult task in many applications of machine learning, and it is only natural that one hopes and expects the population risk to decrease (better performance) monotonically with increasing data points. It turns out, somewhat surprisingly, that this is not the case even for the most standard algorithms that minimize the empirical risk. Non-monotonic behavior of the risk and instability in training have manifested and appeared in the popular deep learning paradigm under the description of double descent. These problems highlight the current lack of understanding of learning algorithms and generalization. It is, therefore, crucial to pursue this concern and provide a characterization of such behavior. In this paper, we derive the first consistent and risk-monotonic (in high probability) algorithms for a general statistical learning setting under weak assumptions, consequently answering some questions posed by Viering et. al. 2019 on how to avoid non-monotonic behavior of risk curves. We further show that risk monotonicity need not necessarily come at the price of worse excess risk rates. To achieve this, we derive new empirical Bernstein-like concentration inequalities of independent interest that hold for certain non-i.i.d.~processes such as Martingale Difference Sequences. "}}
{"id": "Skgx5HBgIB", "cdate": 1567802759757, "mdate": null, "content": {"title": "PAC-Bayes Un-Expected Bernstein Inequality", "abstract": "We present a new PAC-Bayesian generalization bound. Standard bounds contain a $\\sqrt{L_n \\cdot \\KL/n}$ complexity term which dominates unless $L_n$, the empirical error of the learning algorithm's randomized predictions, vanishes. We manage to replace $L_n$ by a term which vanishes in many more situations, essentially whenever the employed learning algorithm is sufficiently stable on the dataset at hand. Our new bound consistently beats state-of-the-art bounds both on a toy example and on UCI datasets (with large enough $n$). Theoretically, unlike existing bounds, our new bound can be expected to converge to $0$ faster whenever a Bernstein/Tsybakov condition holds, thus connecting PAC-Bayesian generalization and excess risk bounds --- for the latter it has long been known that faster convergence can be obtained under Bernstein conditions. Our main technical tool is a new concentration inequality which is like Bernstein's but with $X^2$ taken outside its expectation. "}}
{"id": "ByEIRdWd-S", "cdate": 1514764800000, "mdate": null, "content": {"title": "Constant Regret, Generalized Mixability, and Mirror Descent", "abstract": "We consider the setting of prediction with expert advice; a learner makes predictions by aggregating those of a group of experts. Under this setting, and for the right choice of loss function and ``mixing'' algorithm, it is possible for the learner to achieve a constant regret regardless of the number of prediction rounds. For example, a constant regret can be achieved for \\emph{mixable} losses using the \\emph{aggregating algorithm}. The \\emph{Generalized Aggregating Algorithm} (GAA) is a name for a family of algorithms parameterized by convex functions on simplices (entropies), which reduce to the aggregating algorithm when using the \\emph{Shannon entropy} $\\operatorname{S}$. For a given entropy $\\Phi$, losses for which a constant regret is possible using the \\textsc{GAA} are called $\\Phi$-mixable. Which losses are $\\Phi$-mixable was previously left as an open question. We fully characterize $\\Phi$-mixability and answer other open questions posed by \\cite{Reid2015}. We show that the Shannon entropy $\\operatorname{S}$ is fundamental in nature when it comes to mixability; any $\\Phi$-mixable loss is necessarily $\\operatorname{S}$-mixable, and the lowest worst-case regret of the \\textsc{GAA} is achieved using the Shannon entropy. Finally, by leveraging the connection between the \\emph{mirror descent algorithm} and the update step of the GAA, we suggest a new \\emph{adaptive} generalized aggregating algorithm and analyze its performance in terms of the regret bound."}}
{"id": "BkEzuyMuZr", "cdate": 1514764800000, "mdate": null, "content": {"title": "Geometry Aware Constrained Optimization Techniques for Deep Learning", "abstract": "In this paper, we generalize the Stochastic Gradient Descent (SGD) and RMSProp algorithms to the setting of Riemannian optimization. SGD is a popular method for large scale optimization. In particular, it is widely used to train the weights of Deep Neural Networks. However, gradients computed using standard SGD can have large variance, which is detrimental for the convergence rate of the algorithm. Other methods such as RMSProp and ADAM address this issue. Nevertheless, these methods cannot be directly applied to constrained optimization problems. In this paper, we extend some popular optimization algorithm to the Riemannian (constrained) setting. We substantiate our proposed extensions with a range of relevant problems in machine learning such as incremental Principal Component Analysis, computating the Riemannian centroids of SPD matrices, and Deep Metric Learning. We achieve competitive results against the state of the art for fine-grained object recognition datasets."}}
{"id": "rkEdW7MdZr", "cdate": 1483228800000, "mdate": null, "content": {"title": "Adversarial Generation of Real-time Feedback with Neural Networks for Simulation-based Training", "abstract": "Simulation-based training (SBT) is gaining popularity as a low-cost and convenient training technique in a vast range of applications. However, for a SBT platform to be fully utilized as an effective training tool, it is essential that feedback on performance is provided automatically in real-time during training. It is the aim of this paper to develop an efficient and effective feedback generation method for the provision of real-time feedback in SBT. Existing methods either have low effectiveness in improving novice skills or suffer from low efficiency, resulting in their inability to be used in real-time. In this paper, we propose a neural network based method to generate feedback using the adversarial technique. The proposed method utilizes a bounded adversarial update to minimize a L1 regularized loss via back-propagation. We empirically show that the proposed method can be used to generate simple, yet effective feedback. Also, it was observed to have high effectiveness and efficiency when compared to existing methods, thus making it a promising option for real-time feedback generation in SBT."}}
{"id": "Bk-AhoWd-r", "cdate": 1483228800000, "mdate": null, "content": {"title": "Efficient Orthogonal Parametrisation of Recurrent Neural Networks Using Householder Reflections", "abstract": "The problem of learning long-term dependencies in sequences using Recurrent Neural Networks (RNNs) is still a major challenge. Recent methods have been suggested to solve this problem by constraini..."}}
