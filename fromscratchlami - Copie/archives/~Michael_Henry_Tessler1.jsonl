{"id": "m29t-0opsLb", "cdate": 1681645261911, "mdate": 1681645261911, "content": {"title": "Polite speech emerges from competing social goals", "abstract": "Language is a remarkably efficient tool for transmitting information. Yet human speakers make statements that are inefficient, imprecise, or even contrary to their own beliefs, all in the service of being polite. What rational machinery underlies polite language use? Here, we show that polite speech emerges from the competition of three communicative goals: to convey information, to be kind, and to present oneself in a good light. We formalize this goal tradeoff using a probabilistic model of utterance production, which predicts human utterance choices in socially-sensitive situations with high quantitative accuracy, and we show that our full model is superior to its variants with subsets of the three goals. This utility-theoretic approach to speech acts takes a step towards explaining the richness and subtlety of social language use."}}
{"id": "WyWguq48o2", "cdate": 1681645134974, "mdate": 1681645134974, "content": {"title": "The language of generalization", "abstract": "Language provides simple ways of communicating generalizable knowledge to each other (e.g., \"Birds fly\", \"John hikes\", \"Fire makes smoke\"). Though found in every language and emerging early in development, the language of generalization is philosophically puzzling and has resisted precise formalization. Here, we propose the first formal account of generalizations conveyed with language that makes quantitative predictions about human understanding. We test our model in three diverse domains: generalizations about categories (generic language), events (habitual language), and causes (causal language). The model explains the gradience in human endorsement through the interplay between a simple truth-conditional semantic theory and diverse beliefs about properties, formalized in a probabilistic model of language understanding. This work opens the door to understanding precisely how abstract knowledge is learned from language."}}
{"id": "et9T2bwf_v", "cdate": 1672143474007, "mdate": 1672143474007, "content": {"title": "Can language models learn from explanations in context?", "abstract": "Language Models (LMs) can perform new\ntasks by adapting to a few in-context examples.\nFor humans, explanations that connect examples to task principles can improve learning.\nWe therefore investigate whether explanations\nof few-shot examples can help LMs. We annotate questions from 40 challenging tasks with\nanswer explanations, and various matched control explanations. We evaluate how different\ntypes of explanations, instructions, and controls affect zero- and few-shot performance.\nWe analyze these results using statistical multilevel modeling techniques that account for the\nnested dependencies among conditions, tasks,\nprompts, and models. We find that explanations can improve performance\u2014even without tuning. Furthermore, explanations handtuned for performance on a small validation set\noffer substantially larger benefits, and building a prompt by selecting examples and explanations together substantially improves performance over selecting examples alone. Finally, even untuned explanations outperform\ncarefully matched controls, suggesting that the\nbenefits are due to the link between an example and its explanation, rather than lower-level\nfeatures. However, only large models benefit.\nIn summary, explanations can support the incontext lea"}}
{"id": "OxFoLTKDcNm", "cdate": 1654485227488, "mdate": null, "content": {"title": "Communicating Natural Programs to Humans and Machines", "abstract": "The Abstraction and Reasoning Corpus (ARC) is a set of procedural tasks that tests an agent's ability to flexibly solve novel problems. While most ARC tasks are easy for humans, they are challenging for state-of-the-art AI. What makes building intelligent systems that can generalize to novel situations such as ARC difficult? We posit that the answer might be found by studying the difference of $\\textit{language}$: While humans readily generate and interpret instructions in a general language, computer systems are shackled to a narrow domain-specific language that they can precisely execute. We present LARC, the $\\textit{Language-complete ARC}$: a collection of natural language descriptions by a group of human participants  who instruct each other on how to solve ARC tasks using language alone, which contains successful instructions for 88\\% of the ARC tasks. We analyze the collected instructions as `natural programs', finding that while they resemble computer programs, they are distinct in two ways: First, they contain a wide range of primitives; Second, they frequently leverage communicative strategies beyond directly executable codes. We demonstrate that these two distinctions prevent current program synthesis techniques from leveraging LARC to its full potential, and give concrete suggestions on how to build the next-generation program synthesizers."}}
{"id": "G5ADoRKiTyJ", "cdate": 1652737812422, "mdate": null, "content": {"title": "Fine-tuning language models to find agreement among humans with diverse preferences", "abstract": "Recent work in large language modeling (LLMs) has used fine-tuning to align outputs with the preferences of a prototypical user. This work assumes that human preferences are static and homogeneous across individuals, so that aligning to a single \"generic\" user will confer more general alignment. Here, we embrace the heterogeneity of human preferences to consider a different challenge: how might a machine help people with diverse views find agreement? We fine-tune a 70 billion parameter LLM to generate statements that maximize the expected approval for a group of people with potentially diverse opinions. Human participants provide written opinions on thousands of questions touching on moral and political issues (e.g., \"should we raise taxes on the rich?\"), and rate the LLM's generated candidate consensus statements for agreement and quality. A reward model is then trained to predict individual preferences, enabling it to quantify and rank consensus statements in terms of their appeal to the overall group, defined according to different aggregation (social welfare) functions. The model produces consensus statements that are preferred by human users over those from prompted LLMs ($>70\\%$) and significantly outperforms a tight fine-tuned baseline that lacks the final ranking step. Further, our best model's consensus statements are preferred over the best human-generated opinions ($>65\\%$). We find that when we silently constructed consensus statements from only a subset of group members, those who were excluded were more likely to dissent, revealing the sensitivity of the consensus to individual contributions. These results highlight the potential to use LLMs to help groups of humans align their values with one another."}}
{"id": "Z0XiFAb_WDr", "cdate": 1632875548042, "mdate": null, "content": {"title": "Communicating Natural Programs to Humans and Machines", "abstract": "The Abstraction and Reasoning Corpus (ARC) is a set of procedural tasks that tests an agent's ability to flexibly solve novel problems. While most ARC tasks are easy for humans, they are challenging for state-of-the-art AI. What makes building intelligent systems that can generalize to novel situations such as ARC difficult?\nWe posit that the answer might be found by studying the difference of \\emph{language}: While humans readily generate and interpret instructions in a general language, computer systems are shackled to a narrow domain-specific language that they can precisely execute.\nWe present LARC, the \\textit{Language-complete ARC}: a collection of natural language descriptions by a group of human participants  who instruct each other on how to solve ARC tasks using language alone, which contains successful instructions for 88\\% of the ARC tasks.\nWe analyze the collected instructions as `natural programs', finding that while they resemble computer programs, they are distinct in two ways: First, they contain a wide range of primitives; Second, they frequently leverage communicative strategies beyond directly executable codes. We demonstrate that these two distinctions prevent current program synthesis techniques from leveraging LARC to its full potential, and give concrete suggestions on how to build the next-generation program synthesizers."}}
{"id": "uyKk_avJ-p4", "cdate": 1621630256137, "mdate": null, "content": {"title": "Improving Coherence and Consistency in Neural Sequence Models with Dual-System, Neuro-Symbolic Reasoning", "abstract": "Human reasoning can be understood as an interplay between two systems: the intuitive and associative (\"System 1\") and the deliberative and logical (\"System 2\"). Neural sequence models---which have been increasingly successful at performing complex, structured tasks---exhibit the advantages and failure modes of System 1: they are fast and learn patterns from data, but are often inconsistent and incoherent. In this work, we seek a lightweight, training-free means of improving existing System 1-like sequence models by adding System 2-inspired logical reasoning. We explore several variations on this theme in which candidate generations from a neural sequence model are examined for logical consistency by a symbolic reasoning module, which can either accept or reject the generations. Our approach uses neural inference to mediate between the neural System 1 and the logical System 2. Results in robust story generation and grounded instruction-following show that this approach can increase the coherence and accuracy of neurally-based generations."}}
{"id": "P7GUAXxS3ym", "cdate": 1621630256137, "mdate": null, "content": {"title": "Improving Coherence and Consistency in Neural Sequence Models with Dual-System, Neuro-Symbolic Reasoning", "abstract": "Human reasoning can be understood as an interplay between two systems: the intuitive and associative (\"System 1\") and the deliberative and logical (\"System 2\"). Neural sequence models---which have been increasingly successful at performing complex, structured tasks---exhibit the advantages and failure modes of System 1: they are fast and learn patterns from data, but are often inconsistent and incoherent. In this work, we seek a lightweight, training-free means of improving existing System 1-like sequence models by adding System 2-inspired logical reasoning. We explore several variations on this theme in which candidate generations from a neural sequence model are examined for logical consistency by a symbolic reasoning module, which can either accept or reject the generations. Our approach uses neural inference to mediate between the neural System 1 and the logical System 2. Results in robust story generation and grounded instruction-following show that this approach can increase the coherence and accuracy of neurally-based generations."}}
