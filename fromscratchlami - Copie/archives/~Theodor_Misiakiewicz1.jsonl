{"id": "m5fU5hJaGnC", "cdate": 1684352921123, "mdate": 1684352921123, "content": {"title": "SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics", "abstract": "We investigate the time complexity of SGD learning on fully-connected neural networks with isotropic data. We put forward a complexity measure, {\\it the leap}, which measures how \u201chierarchical\u201d target functions are. \nFor uniform Boolean or isotropic Gaussian data, our main conjecture is that the time complexity for SGD to learn a function $f$ with low-dimensional support is controlled by its leap, i.e., it is $$\\Tilde \\Theta (d^{\\max(\\mathrm{Leap}(f),2)}) \\,\\,.$$   \nWe prove a version of this conjecture for a specific class of functions on Gaussian isotropic data and 2-layer neural networks, under additional technical assumptions on how SGD is run. We show that the training  sequentially learns the function support with a saddle-to-saddle dynamic. Our result departs from [Abbe et al.'22] by going beyond leap 1 (merged-staircase functions), and by going beyond the mean-field and gradient flow approximations that prohibit the full complexity control obtained here. Finally, we note that this gives an SGD complexity scaling that matches that of Correlational Statistical Query (CSQ) lower-bounds."}}
{"id": "e8EkYPDHrsY", "cdate": 1652737814107, "mdate": null, "content": {"title": "Learning with convolution and pooling operations in kernel methods", "abstract": "Recent empirical work has shown that hierarchical convolutional kernels inspired by convolutional neural networks (CNNs) signi\ufb01cantly improve the performance of kernel methods in image classi\ufb01cation tasks. A widely accepted explanation for their success is that these architectures encode hypothesis classes that are suitable for natural images. However, understanding the precise interplay between approximation and generalization in convolutional architectures remains a challenge. In this paper, we consider the stylized setting of covariates (image pixels) uniformly distributed on the hypercube, and characterize exactly the RKHS of kernels composed of single layers of convolution, pooling, and downsampling operations. We use this characterization to compute sharp asymptotics of the generalization error for any given function in high-dimension. In particular, we quantify the gain in sample complexity brought by enforcing locality with the convolution operation and approximate translation invariance with average pooling. Notably, these results provide a precise description of how convolution and pooling operations trade off approximation with generalization power in one layer convolutional kernels."}}
{"id": "HvJC_KsSx8S", "cdate": 1652737640399, "mdate": null, "content": {"title": "Precise Learning Curves and Higher-Order Scalings for Dot-product Kernel Regression  ", "abstract": "As modern machine learning models continue to advance the computational frontier, it has become increasingly important to develop precise estimates for expected performance improvements under different model and data scaling regimes. Currently, theoretical understanding of the learning curves that characterize how the prediction error depends on the number of samples is restricted to either large-sample asymptotics ($m\\to\\infty$) or, for certain simple data distributions, to the high-dimensional asymptotics in which the number of samples scales linearly with the dimension ($m\\propto d$). There is a wide gulf between these two regimes, including all higher-order scaling relations $m\\propto d^r$, which are the subject of the present paper. We focus on the problem of kernel ridge regression for dot-product kernels and present precise formulas for the mean of the test error, bias, and variance, for data drawn uniformly from the sphere with isotropic random labels in the $r$th-order asymptotic scaling regime $m\\to\\infty$ with $m/d^r$ held constant. We observe a peak in the learning curve whenever $m \\approx d^r/r!$ for any integer $r$, leading to multiple sample-wise descent and nontrivial behavior at multiple scales. We include a colab notebook that reproduces the essential results of the paper."}}
{"id": "93SVBUB1r5C", "cdate": 1632875560927, "mdate": null, "content": {"title": "Learning with convolution and pooling operations in kernel methods", "abstract": "Recent empirical work has shown that hierarchical convolutional kernels inspired by convolutional neural networks (CNNs) significantly improve the performance of kernel methods in image classification tasks. A widely accepted explanation for the success of these architectures is that they encode hypothesis classes that are suitable for natural images. However, understanding the precise interplay between approximation and generalization in convolutional architectures remains a challenge. In this paper, we consider the stylized setting of covariates (image pixels) uniformly distributed on the hypercube, and fully characterize the RKHS of kernels composed of single layers of convolution, pooling, and downsampling operations. We then study the gain in sample efficiency of kernel methods using these kernels over standard inner-product kernels. In particular, we show that 1) the convolution layer breaks the curse of dimensionality by restricting the RKHS to `local' functions; 2) local pooling biases learning towards low-frequency functions, which are stable by small translations; 3) downsampling may modify the high-frequency eigenspaces but leaves the low-frequency part approximately unchanged. Notably, our results quantify how choosing an architecture adapted to the target function leads to a large improvement in the sample complexity.\n\n"}}
{"id": "JcbHP7nox8s", "cdate": 1623623776236, "mdate": 1623623776236, "content": {"title": "Solving SDPs for synchronization and MaxCut problems via the Grothendieck inequality", "abstract": "A number of statistical estimation problems can be addressed by semidefinite programs (SDP). While SDPs are solvable in polynomial time using interior point methods, in practice generic SDP solvers do not scale well to high-dimensional problems. In order to cope with this problem, Burer and Monteiro proposed a non-convex rank-constrained formulation, which has good performance in practice but is still poorly understood theoretically. In this paper we study the rank-constrained version of SDPs arising in MaxCut and in Z2 and SO(d) synchronization problems. We establish a Grothendieck-type inequality that proves that all the local maxima and dangerous saddle points are within a small multiplicative gap from the global maximum. We use this structural information to prove that SDPs can be solved within a known accuracy, by applying the Riemannian trust-region method to this non-convex problem, while constraining the rank to be of order one. For the MaxCut problem, our inequality implies that any local maximizer of the rank-constrained SDP provides a (1\u22121/(k\u22121))\u00d70.878 approximation of the MaxCut, when the rank is fixed to k. We then apply our results to data matrices generated according to the Gaussian Z2 synchronization problem, and the two-groups stochastic block model with large bounded degree. We prove that the error achieved by local maximizers undergoes a phase transition at the same threshold as for information-theoretically optimal methods."}}
{"id": "39NTDah-gC-", "cdate": 1609459200000, "mdate": null, "content": {"title": "Learning with invariances in random features and kernel models", "abstract": "A number of machine learning tasks entail a high degree of invariance: the data distribution does not change if we act on the data with a certain group of transformations. For instance, labels of images are invariant under translations of the images. Certain neural network architectures -- for instance, convolutional networks -- are believed to owe their success to the fact that they exploit such invariance properties. With the objective of quantifying the gain achieved by invariant architectures, we introduce two classes of models: invariant random features and invariant kernel methods. The latter includes, as a special case, the neural tangent kernel for convolutional networks with global average pooling. We consider uniform covariates distributions on the sphere and hypercube and a general invariant target function. We characterize the test error of invariant methods in a high-dimensional regime in which the sample size and number of hidden units scale as polynomials in the dimension, for a class of groups that we call `degeneracy $\\alpha$', with $\\alpha \\leq 1$. We show that exploiting invariance in the architecture saves a $d^\\alpha$ factor ($d$ stands for the dimension) in sample size and number of hidden units to achieve the same test error as for unstructured architectures. Finally, we show that output symmetrization of an unstructured kernel estimator does not give a significant statistical improvement; on the other hand, data augmentation with an unstructured kernel estimator is equivalent to an invariant kernel estimator and enjoys the same improvement in statistical efficiency."}}
{"id": "IO9Lp5IFCSD", "cdate": 1577836800000, "mdate": null, "content": {"title": "When Do Neural Networks Outperform Kernel Methods?", "abstract": "For a certain scaling of the initialization of stochastic gradient descent (SGD), wide neural networks (NN) have been shown to be well approximated by reproducing kernel Hilbert space (RKHS) methods. Recent empirical work showed that, for some classification tasks, RKHS methods can replace NNs without a large loss in performance. On the other hand, two-layers NNs are known to encode richer smoothness classes than RKHS and we know of special examples for which SGD-trained NN provably outperform RKHS. This is true even in the wide network limit, for a different scaling of the initialization. How can we reconcile the above claims? For which tasks do NNs outperform RKHS? If covariates are nearly isotropic, RKHS methods suffer from the curse of dimensionality, while NNs can overcome it by learning the best low-dimensional representation. Here we show that this curse of dimensionality becomes milder if the covariates display the same low-dimensional structure as the target function, and we precisely characterize this tradeoff. Building on these results, we present the spiked covariates model that can capture in a unified framework both behaviors observed in earlier works. We hypothesize that such a latent low-dimensional structure is present in image classification. We numerically test this hypothesis by showing that specific perturbations of the training distribution degrade the performances of RKHS methods much more significantly than NNs."}}
{"id": "0syWaH8mbB", "cdate": 1577836800000, "mdate": null, "content": {"title": "When Do Neural Networks Outperform Kernel Methods?", "abstract": "For a certain scaling of the initialization of stochastic gradient descent (SGD), wide neural networks (NN) have been shown to be well approximated by reproducing kernel Hilbert space (RKHS) methods. Recent empirical work showed that, for some classification tasks, RKHS methods can replace NNs without a large loss in performance. On the other hand, two-layers NNs are known to encode richer smoothness classes than RKHS and we know of special examples for which SGD-trained NN provably outperform RKHS. This is true even in the wide network limit, for a different scaling of the initialization. How can we reconcile the above claims? For which tasks do NNs outperform RKHS? If covariates are nearly isotropic, RKHS methods suffer from the curse of dimensionality, while NNs can overcome it by learning the best low-dimensional representation. Here we show that this curse of dimensionality becomes milder if the covariates display the same low-dimensional structure as the target function, and we precisely characterize this tradeoff. Building on these results, we present the spiked covariates model that can capture in a unified framework both behaviors observed in earlier work. We hypothesize that such a latent low-dimensional structure is present in image classification. We test numerically this hypothesis by showing that specific perturbations of the training distribution degrade the performances of RKHS methods much more significantly than NNs."}}
{"id": "awTPNWAFLTx", "cdate": 1546300800000, "mdate": null, "content": {"title": "Limitations of Lazy Training of Two-layers Neural Networks.", "abstract": "We study the supervised learning problem under either of the following two models: (1) Feature vectors ${\\boldsymbol x}_i$ are $d$-dimensional Gaussians and responses are $y_i = f_*({\\boldsymbol x}_i)$ for $f_*$ an unknown quadratic function; (2) Feature vectors ${\\boldsymbol x}_i$ are distributed as a mixture of two $d$-dimensional centered Gaussians, and $y_i$'s are the corresponding class labels. We use two-layers neural networks with quadratic activations, and compare three different learning regimes: the random features (RF) regime in which we only train the second-layer weights; the neural tangent (NT) regime in which we train a linearization of the neural network around its initialization; the fully trained neural network (NN) regime in which we train all the weights in the network. We prove that, even for the simple quadratic model of point (1), there is a potentially unbounded gap between the prediction risk achieved in these three training regimes, when the number of neurons is smaller than the ambient dimension. When the number of neurons is larger than the number of dimensions, the problem is significantly easier and both NT and NN learning achieve zero risk."}}
{"id": "Zfp2E9PPQ-", "cdate": 1546300800000, "mdate": null, "content": {"title": "Limitations of Lazy Training of Two-layers Neural Network.", "abstract": "We study the supervised learning problem under either of the following two models: (1) Feature vectors x<em>i are d-dimensional Gaussian and responses are y</em>i = f<em>*(x</em>i) for f<em>* an unknown quadratic function; (2) Feature vectors x</em>i are distributed as a mixture of two d-dimensional centered Gaussians, and y_i's are the corresponding class labels. We use two-layers neural networks with quadratic activations, and compare three different learning regimes: the random features (RF) regime in which we only train the second-layer weights; the neural tangent (NT) regime in which we train a linearization of the neural network around its initialization; the fully trained neural network (NN) regime in which we train all the weights in the network. We prove that, even for the simple quadratic model of point (1), there is a potentially unbounded gap between the prediction risk achieved in these three training regimes, when the number of neurons is smaller than the ambient dimension. When the number of neurons is larger than the number of dimensions, the problem is significantly easier and both NT and NN learning achieve zero risk."}}
