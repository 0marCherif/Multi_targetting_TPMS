{"id": "Q2171WADNTT", "cdate": 1683892571395, "mdate": null, "content": {"title": "Neural Circuit Synthesis with Pre-trained Language Models", "abstract": "This extended abstract reports preliminary results on fine-tuning pre-trained language models for solving reactive synthesis problems end-to-end. In recent work, hierarchical Transformer neural networks have been successfully trained from scratch to synthesize sequential circuits directly out of formal specifications. We improve over existing approaches by fine-tuning CodeT5 models that have been pre-trained on both natural language and programming languages. Our experiments show improved generalization and sample efficiency compared to the previous approach."}}
{"id": "ft5UoTtXg7", "cdate": 1672531200000, "mdate": 1682345877777, "content": {"title": "Iterative Circuit Repair Against Formal Specifications", "abstract": "We present a deep learning approach for repairing sequential circuits against formal specifications given in linear-time temporal logic (LTL). Given a defective circuit and its formal specification, we train Transformer models to output circuits that satisfy the corresponding specification. We propose a separated hierarchical Transformer for multimodal representation learning of the formal specification and the circuit. We introduce a data generation algorithm that enables generalization to more complex specifications and out-of-distribution datasets. In addition, our proposed repair mechanism significantly improves the automated synthesis of circuits from LTL specifications with Transformers. It improves the state-of-the-art by $6.8$ percentage points on held-out instances and $11.8$ percentage points on an out-of-distribution dataset from the annual reactive synthesis competition."}}
{"id": "YaCO3Lh4ZV", "cdate": 1672531200000, "mdate": 1682345877772, "content": {"title": "nl2spec: Interactively Translating Unstructured Natural Language to Temporal Logics with Large Language Models", "abstract": ""}}
{"id": "ywAjQw-spmY", "cdate": 1663850564594, "mdate": null, "content": {"title": "Formal Specifications from Natural Language", "abstract": "We study the generalization abilities of language models when translating natural language into formal specifications with complex semantics. In particular, we fine-tune language models on three datasets consisting of English sentences and their corresponding formal representation: 1) regular expressions (regex), frequently used in programming and search; 2) First-order logic (FOL), commonly used in software verification and theorem proving; and 3) linear-time temporal logic (LTL), which forms the basis for industrial hardware specification languages. Our experiments show that, in these diverse domains, the language models maintain their generalization capabilities from pre-trained knowledge of natural language to generalize, e.g., to new variable names or operator descriptions. Additionally, they achieve competitive performance, and even outperform the state-of-the-art for translating into regular expressions, with the benefits of being easy to access, efficient to fine-tune, and without a particular need for domain-specific reasoning."}}
{"id": "pcBJT4bgbpH", "cdate": 1663850559049, "mdate": null, "content": {"title": "Attention Flows for General Transformers", "abstract": "In this paper, we study the computation of how much an input token in a Transformer model influences its prediction. We formalize a method to construct a flow network out of the attention values of encoder-only Transformer models and extend it to general Transformer architectures, including an auto-regressive decoder. We show that running a maxflow algorithm on the flow network construction yields Shapley values, which determine a player's impact in cooperative game theory. By interpreting the input tokens in the flow network as players, we can compute their influence on the total attention flow leading to the decoder's decision. Additionally, we provide a library that computes and visualizes the attention flow of arbitrary Transformer models. We show the usefulness of our implementation on various models trained on natural language processing and reasoning tasks."}}
{"id": "SEcSahl0Ql", "cdate": 1663850555585, "mdate": null, "content": {"title": "Iterative Circuit Repair Against Formal Specifications", "abstract": "We present a deep learning approach for repairing sequential circuits against formal specifications given in linear-time temporal logic (LTL). Given a defective circuit and its formal specification, we train Transformer models to output circuits that satisfy the corresponding specification. We propose a separated hierarchical Transformer for multimodal representation learning of the formal specification and the circuit. We introduce a data generation algorithm that enables generalization to more complex specifications and out-of-distribution datasets. In addition, our proposed repair mechanism significantly improves the automated synthesis of circuits from LTL specifications with Transformers. It improves the state-of-the-art by $6.8$ percentage points on held-out instances and $11.8$ percentage points on an out-of-distribution dataset from the annual reactive synthesis competition."}}
{"id": "cKx8lVB2B3", "cdate": 1640995200000, "mdate": 1682345877893, "content": {"title": "Attention Flows for General Transformers", "abstract": "In this paper, we study the computation of how much an input token in a Transformer model influences its prediction. We formalize a method to construct a flow network out of the attention values of encoder-only Transformer models and extend it to general Transformer architectures including an auto-regressive decoder. We show that running a maxflow algorithm on the flow network construction yields Shapley values, which determine the impact of a player in cooperative game theory. By interpreting the input tokens in the flow network as players, we can compute their influence on the total attention flow leading to the decoder's decision. Additionally, we provide a library that computes and visualizes the attention flow of arbitrary Transformer models. We show the usefulness of our implementation on various models trained on natural language processing and reasoning tasks."}}
{"id": "Bwogj2ALEp", "cdate": 1640995200000, "mdate": 1682345877784, "content": {"title": "Formal Specifications from Natural Language", "abstract": "We study the generalization abilities of language models when translating natural language into formal specifications with complex semantics. In particular, we fine-tune language models on three datasets consisting of English sentences and their corresponding formal representation: 1) regular expressions (regex), frequently used in programming and search; 2) First-order logic (FOL), commonly used in software verification and theorem proving; and 3) linear-time temporal logic (LTL), which forms the basis for industrial hardware specification languages. Our experiments show that, in these diverse domains, the language models maintain their generalization capabilities from pre-trained knowledge of natural language to generalize, e.g., to new variable names or operator descriptions. Additionally, they achieve competitive performance, and even outperform the state-of-the-art for translating into regular expressions, with the benefits of being easy to access, efficient to fine-tune, and without a particular need for domain-specific reasoning."}}
{"id": "O4TE57kehc1", "cdate": 1621630343085, "mdate": null, "content": {"title": "Neural Circuit Synthesis from Specification Patterns", "abstract": "We train hierarchical Transformers on the task of synthesizing hardware circuits directly out of high-level logical speci\ufb01cations in linear-time temporal logic (LTL). The LTL synthesis problem is a well-known algorithmic challenge with a long history and an annual competition is organized to track the improvement of algorithms and tooling over time. New approaches using machine learning might open a lot of possibilities in this area, but suffer from the lack of suf\ufb01cient amounts of training data. In this paper, we consider a method to generate large amounts of additional training data, i.e., pairs of speci\ufb01cations and circuits implementing them. We ensure that this synthetic data is suf\ufb01ciently close to human-written speci\ufb01cations by mining common patterns from the speci\ufb01cations used in the synthesis competitions. We show that hierarchical Transformers trained on this synthetic data solve a signi\ufb01cant portion of problems from the synthesis competitions, and even out-of-distribution examples from a recent case study."}}
{"id": "l8qzevGN6J", "cdate": 1609459200000, "mdate": 1681514553099, "content": {"title": "Neural Circuit Synthesis from Specification Patterns", "abstract": ""}}
