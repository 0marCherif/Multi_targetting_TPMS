{"id": "bRNAyEGLhcb", "cdate": 1664575123248, "mdate": 1664575123248, "content": {"title": "Regionalized Optimization", "abstract": "We propose a theoretical framework for non redundant reconstruction of a global loss from a collection of local ones under constraints given by a functor; we call this loss the regionalized loss in honor to Yedidia, Freeman, Weiss' celebrated article `Constructing free-energy approximations and generalized belief propagation algorithms' where a first example of regionalized loss, for entropy and the marginal functor, is built. We show how one can associate to these regionalized losses message passing algorithms for finding their critical points. It is a natural mathematical framework for optimization problems where there are multiple points of views on a dataset and replaces message passing algorithms as canonical ways of finding the optima of these problems. We explain how Generalized Belief propagation algorithms fall into the framework we propose and propose novel message passing algorithms for noisy channel networks."}}
{"id": "MZoyeKrpVYP", "cdate": 1652737552069, "mdate": null, "content": {"title": "On Non-Linear operators for Geometric Deep Learning", "abstract": "This work studies operators mapping vector and scalar fields defined over a manifold $\\mathcal{M}$, and which commute with its group of diffeomorphisms $\\text{Diff}(\\mathcal{M})$. We prove that in the case of scalar fields $L^p_\\omega(\\mathcal{M,\\mathbb{R}})$, those operators correspond to point-wise non-linearities, recovering and extending known results on $\\mathbb{R}^d$. In the context of Neural Networks defined over $\\mathcal{M}$, it indicates that point-wise non-linear operators are the only universal family that commutes with any group of symmetries, and justifies their systematic use in combination with dedicated linear operators commuting with specific symmetries. In the case of vector fields $L^p_\\omega(\\mathcal{M},T\\mathcal{M})$, we show that those operators are solely the scalar multiplication. It indicates that $\\text{Diff}(\\mathcal{M})$ is too rich and that there is no universal class of non-linear operators to motivate the design of Neural Networks over the symmetries of $\\mathcal{M}$."}}
