{"id": "1wVvweK3oIb", "cdate": 1632875470172, "mdate": null, "content": {"title": "Simple GNN Regularisation for 3D Molecular Property Prediction and Beyond", "abstract": "In this paper we show that simple noisy regularisation can be an effective way to address oversmoothing. We first argue that regularisers ad-dressing oversmoothing should both penalise node latent similarity and encourage meaningful node representations. From this observation we derive \u201cNoisy Nodes\u201d,a simple technique in which we corrupt the input graph with noise, and add a noise correcting node-level loss.  The diverse node level loss encourages latent node diversity, and the denoising objective encourages graph manifold learning.  Our regulariser applies well-studied methods in simple, straightforward ways which allow even generic architectures to overcome oversmoothing and achieve state of the art results on quantum chemistry tasks such as QM9 and Open Catalyst, and improve results significantly on Open Graph Benchmark (OGB) datasets.  Our results suggest Noisy Nodes can serve as a complementary building block in the GNN toolkit."}}
{"id": "BJl6AjC5F7", "cdate": 1538087892584, "mdate": null, "content": {"title": "Learning to Represent Edits", "abstract": "We introduce the problem of learning distributed representations of edits. By combining a\n\"neural editor\" with an \"edit encoder\", our models learn to represent the salient\ninformation of an edit and can be used to apply edits to new inputs.\nWe experiment on natural language and source code edit data. Our evaluation yields\npromising results that suggest that our neural network models learn to capture\nthe structure and semantics of edits. We hope that this interesting task and\ndata source will inspire other researchers to work further on this problem."}}
{"id": "Bke4KsA5FX", "cdate": 1538087804139, "mdate": null, "content": {"title": "Generative Code Modeling with Graphs", "abstract": "Generative models forsource code are an interesting structured prediction problem, requiring to reason about both hard syntactic and semantic constraints as well as about natural, likely programs. We present a novel model for this problem that uses a graph to represent the intermediate state of the generated output. Our model generates code by interleaving grammar-driven expansion steps with graph augmentation and neural message passing steps. An experimental evaluation shows that our new model can generate semantically meaningful expressions, outperforming a range of strong baselines."}}
{"id": "B1l08oAct7", "cdate": 1538087765785, "mdate": null, "content": {"title": "Deterministic Variational Inference for Robust Bayesian Neural Networks", "abstract": "Bayesian neural networks (BNNs) hold great promise as a flexible and principled solution to deal with uncertainty when learning from finite data. Among approaches to realize probabilistic inference in deep neural networks, variational Bayes (VB) is theoretically grounded, generally applicable, and computationally efficient. With wide recognition of potential advantages, why is it that variational Bayes has seen very limited practical use for BNNs in real applications? We argue that variational inference in neural networks is fragile: successful implementations require careful initialization and tuning of prior variances, as well as controlling the variance of Monte Carlo gradient estimates. We provide two innovations that aim to turn VB into a robust inference tool for Bayesian neural networks: first, we introduce a novel deterministic method to approximate moments in neural networks, eliminating gradient variance; second, we introduce a hierarchical prior for parameters and a novel Empirical Bayes procedure for automatically selecting prior variances. Combining these two innovations, the resulting method is highly efficient and robust. On the application of heteroscedastic regression we demonstrate good predictive performance over alternative approaches."}}
{"id": "rk4Fz2e0b", "cdate": 1518730179626, "mdate": null, "content": {"title": "Graph Partition Neural Networks for Semi-Supervised Classification", "abstract": "We present graph partition neural networks (GPNN), an extension of graph neural networks (GNNs) able to handle extremely large graphs. GPNNs alternate between locally propagating information between nodes in small subgraphs and globally propagating information between the subgraphs. To efficiently partition graphs, we experiment with spectral partitioning and also propose a modified multi-seed flood fill for fast processing of large scale graphs. We extensively test our model on a variety of semi-supervised node classification tasks. Experimental results indicate that GPNNs are either superior or comparable to state-of-the-art methods on a wide variety of datasets for graph-based semi-supervised classification. We also show that GPNNs can achieve similar performance as standard GNNs with fewer propagation steps."}}
{"id": "HJnQJXbC-", "cdate": 1518730157879, "mdate": null, "content": {"title": "AMPNet: Asynchronous Model-Parallel Training for Dynamic Neural Networks", "abstract": "\nNew types of compute hardware in development and entering the market hold the promise of revolutionizing deep learning in a manner as profound as GPUs. However, existing software frameworks and training algorithms for deep learning have yet to evolve to fully leverage the capability of the new wave of silicon. In particular, models that exploit structured input via complex and instance-dependent control flow are difficult to accelerate using existing algorithms and hardware that typically rely on minibatching. We present an asynchronous model-parallel (AMP) training algorithm that is specifically motivated by training on networks of interconnected devices. Through an implementation on multi-core CPUs, we show that AMP training converges to the same accuracy as conventional synchronous training algorithms in a similar number of epochs, but utilizes the available hardware more efficiently, even for small minibatch sizes, resulting in shorter overall training times. Our framework opens the door for scaling up a new class of deep learning models that cannot be efficiently trained today."}}
{"id": "ByM6TrTUz", "cdate": 1518325178328, "mdate": null, "content": {"title": "Graph Partition Neural Networks for Semi-Supervised Classification", "abstract": "We present graph partition neural networks (GPNN), an extension of graph neural\nnetworks (GNNs) able to handle extremely large graphs.\nGPNNs alternate between locally propagating information between nodes in small\nsubgraphs and globally propagating information between the subgraphs.\nTo efficiently partition graphs, we experiment with spectral partitioning and also\npropose a modified multi-seed flood fill for fast processing of large scale graphs.\nWe extensively test our model on a variety of semi-supervised node\nclassification tasks.\nExperimental results indicate that GPNNs are either superior or comparable to \nstate-of-the-art methods on a wide variety of datasets for graph-based \nsemi-supervised classification. \nWe also show that GPNNs can achieve similar performance as standard GNNs with\nfewer propagation steps."}}
{"id": "B1Z-38W_bH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Constrained Graph Variational Autoencoders for Molecule Design", "abstract": "Graphs are ubiquitous data structures for representing interactions between entities. With an emphasis on applications in chemistry, we explore the task of learning to generate graphs that conform to a distribution observed in training data. We propose a variational autoencoder model in which both encoder and decoder are graph-structured. Our decoder assumes a sequential ordering of graph extension steps and we discuss and analyze design choices that mitigate the potential downsides of this linearization. Experiments compare our approach with a wide range of baselines on the molecule generation task and show that our method is successful at matching the statistics of the original dataset on semantically important metrics. Furthermore, we show that by using appropriate shaping of the latent space, our model allows us to design molecules that are (locally) optimal in desired properties."}}
{"id": "BJWgMhW_bH", "cdate": 1483228800000, "mdate": null, "content": {"title": "Differentiable Programs with Neural Libraries", "abstract": "We develop a framework for combining differentiable programming languages with neural networks. Using this framework we create end-to-end trainable systems that learn to write interpretable algorit..."}}
{"id": "rJNulIVtx", "cdate": null, "mdate": null, "content": {"title": "Lifelong Perceptual Programming By Example", "abstract": "We introduce and develop solutions for the problem of Lifelong Perceptual Programming By Example (LPPBE). The problem is to induce a series of programs that require understanding perceptual data like images or text. LPPBE systems learn from weak supervision (input-output examples) and incrementally construct a shared library of components that grows and improves as more tasks are solved. Methodologically, we extend differentiable interpreters to operate on perceptual data and to share components across tasks. Empirically we show that this leads to a lifelong learning system that transfers knowledge to new tasks more effectively than baselines, and the performance on earlier tasks continues to improve even as the system learns on new, different tasks."}}
