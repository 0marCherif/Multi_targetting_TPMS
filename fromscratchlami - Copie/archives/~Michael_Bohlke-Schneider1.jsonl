{"id": "U_PvlhPbsGS", "cdate": 1672531200000, "mdate": 1682329923739, "content": {"title": "Adaptive Sampling for Probabilistic Forecasting under Distribution Shift", "abstract": "The world is not static: This causes real-world time series to change over time through external, and potentially disruptive, events such as macroeconomic cycles or the COVID-19 pandemic. We present an adaptive sampling strategy that selects the part of the time series history that is relevant for forecasting. We achieve this by learning a discrete distribution over relevant time steps by Bayesian optimization. We instantiate this idea with a two-step method that is pre-trained with uniform sampling and then training a lightweight adaptive architecture with adaptive sampling. We show with synthetic and real-world experiments that this method adapts to distribution shift and significantly reduces the forecasting error of the base model for three out of five datasets."}}
{"id": "7M0DSgjqsFs", "cdate": 1672531200000, "mdate": 1681699644561, "content": {"title": "Deep Learning for Time Series Forecasting: Tutorial and Literature Survey", "abstract": "Deep learning based forecasting methods have become the methods of choice in many applications of time series prediction or forecasting often outperforming other approaches. Consequently, over the last years, these methods are now ubiquitous in large-scale industrial forecasting applications and have consistently ranked among the best entries in forecasting competitions (e.g., M4 and M5). This practical success has further increased the academic interest to understand and improve deep forecasting methods. In this article we provide an introduction and overview of the field: We present important building blocks for deep forecasting in some depth; using these building blocks, we then survey the breadth of the recent deep forecasting literature."}}
{"id": "oPHuNpJl3c", "cdate": 1664928778720, "mdate": null, "content": {"title": "Adaptive Sampling for Probabilistic Forecasting under Distribution Shift", "abstract": "The world is not static: This causes real-world time series to change over time\nthrough external, and potentially disruptive, events such as macroeconomic cycles\nor the COVID-19 pandemic. We present an adaptive sampling strategy that selects\nthe part of the time series history that is relevant for forecasting. We achieve this by\nlearning a discrete distribution over relevant time steps by Bayesian optimization.\nWe instantiate this idea with a two-step method that is pre-trained with uniform\nsampling and then training a lightweight adaptive architecture with adaptive sam-\npling. We show with synthetic and real-world experiments that this method adapts\nto distribution shift and significantly reduces the forecasting error of the base model\nfor three out of five datasets."}}
{"id": "zj3Vm78j9c", "cdate": 1640995200000, "mdate": 1683890532062, "content": {"title": "Criteria for Classifying Forecasting Methods", "abstract": "Classifying forecasting methods as being either of a \"machine learning\" or \"statistical\" nature has become commonplace in parts of the forecasting literature and community, as exemplified by the M4 competition and the conclusion drawn by the organizers. We argue that this distinction does not stem from fundamental differences in the methods assigned to either class. Instead, this distinction is probably of a tribal nature, which limits the insights into the appropriateness and effectiveness of different forecasting methods. We provide alternative characteristics of forecasting methods which, in our view, allow to draw meaningful conclusions. Further, we discuss areas of forecasting which could benefit most from cross-pollination between the ML and the statistics communities."}}
{"id": "un2dOLYwe_", "cdate": 1640995200000, "mdate": 1682318796515, "content": {"title": "Intrinsic Anomaly Detection for Multi-Variate Time Series", "abstract": "We introduce a novel, practically relevant variation of the anomaly detection problem in multi-variate time series: intrinsic anomaly detection. It appears in diverse practical scenarios ranging from DevOps to IoT, where we want to recognize failures of a system that operates under the influence of a surrounding environment. Intrinsic anomalies are changes in the functional dependency structure between time series that represent an environment and time series that represent the internal state of a system that is placed in said environment. We formalize this problem, provide under-studied public and new purpose-built data sets for it, and present methods that handle intrinsic anomaly detection. These address the short-coming of existing anomaly detection methods that cannot differentiate between expected changes in the system's state and unexpected ones, i.e., changes in the system that deviate from the environment's influence. Our most promising approach is fully unsupervised and combines adversarial learning and time series representation learning, thereby addressing problems such as label sparsity and subjectivity, while allowing to navigate and improve notoriously problematic anomaly detection data sets."}}
{"id": "iLaHpvlHDsH", "cdate": 1640995200000, "mdate": 1682329923753, "content": {"title": "PSA-GAN: Progressive Self Attention GANs for Synthetic Time Series", "abstract": "Realistic synthetic time series data of sufficient length enables practical applications in time series modeling tasks, such as forecasting, but remains a challenge. In this paper we present PSA-GAN, a generative adversarial network (GAN) that generates long time series samples of high quality using progressive growing of GANs and self-attention. We show that PSA-GAN can be used to reduce the error in several downstream forecasting tasks over baselines that only use real data. We also introduce a Frechet-Inception Distance-like score for time series, Context-FID, assessing the quality of synthetic time series samples. We find that Context-FID is indicative for downstream performance. Therefore, Context-FID could be a useful tool to develop time series GAN models."}}
{"id": "4kQeOURQri", "cdate": 1640995200000, "mdate": 1682329923742, "content": {"title": "Resilient Neural Forecasting Systems", "abstract": "Industrial machine learning systems face data challenges that are often under-explored in the academic literature. Common data challenges are data distribution shifts, missing values and anomalies. In this paper, we discuss data challenges and solutions in the context of a Neural Forecasting application on labor planning.We discuss how to make this forecasting system resilient to these data challenges. We address changes in data distribution with a periodic retraining scheme and discuss the critical importance of model stability in this setting. Furthermore, we show how our deep learning model deals with missing values natively without requiring imputation. Finally, we describe how we detect anomalies in the input data and mitigate their effect before they impact the forecasts. This results in a fully autonomous forecasting system that compares favorably to a hybrid system consisting of the algorithm and human overrides."}}
{"id": "Ix_mh42xq5w", "cdate": 1632875639189, "mdate": null, "content": {"title": "PSA-GAN: Progressive Self Attention GANs for Synthetic Time Series", "abstract": "Realistic synthetic time series data of sufficient length enables practical applications in time series modeling tasks, such as forecasting, but remains a challenge. In this paper we present PSA-GAN, a generative adversarial network (GAN) that generates long time series samples of high quality using progressive growing of GANs and self-attention. We show that PSA-GAN can be used to reduce the error in several downstream forecasting tasks over baselines that only use real data. We also introduce a Frechet-Inception Distance-like score for time series, Context-FID, assessing the quality of synthetic time series samples. We find that Context-FID is indicative for downstream performance. Therefore, Context-FID could be a useful tool to develop time series GAN models. "}}
{"id": "7sz69eztw9", "cdate": 1632875627569, "mdate": null, "content": {"title": "Context-invariant, multi-variate time series representations", "abstract": "Modern time series corpora, in particular those coming from sensor-based data, exhibit characteristics that have so far not been adequately addressed in the literature on representation learning for time series. In particular, such corpora often allow to distinguish between \\emph{exogenous} signals that describe a context which influences a given appliance and \\emph{endogenous} signals that describe the internal state of the appliance. We propose a temporal convolution network based embedding that improves on the state-of-the-art by incorporating recent advances in contrastive learning to the time series domain and by adopting a multi-resolution approach. Employing techniques borrowed from domain-adversarial learning, we achieve an invariance of the embeddings with respect to the context provided by the exogenous signal. To show the effectiveness of our approach, we contribute new data sets to the research community and use both new as well as existing data sets to empirically verify that we can separate normal from abnormal internal appliance behaviour independent of the external signals in data sets from IoT and DevOps."}}
{"id": "t8zBkMGHMEu", "cdate": 1609459200000, "mdate": 1682329923739, "content": {"title": "PSA-GAN: Progressive Self Attention GANs for Synthetic Time Series", "abstract": "Realistic synthetic time series data of sufficient length enables practical applications in time series modeling tasks, such as forecasting, but remains a challenge. In this paper we present PSA-GAN, a generative adversarial network (GAN) that generates long time series samples of high quality using progressive growing of GANs and self-attention. We show that PSA-GAN can be used to reduce the error in two downstream forecasting tasks over baselines that only use real data. We also introduce a Frechet-Inception Distance-like score, Context-FID, assessing the quality of synthetic time series samples. In our downstream tasks, we find that the lowest scoring models correspond to the best-performing ones. Therefore, Context-FID could be a useful tool to develop time series GAN models."}}
