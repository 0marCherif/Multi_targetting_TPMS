{"id": "pzmwfDLoANS", "cdate": 1621630016862, "mdate": null, "content": {"title": "Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence", "abstract": "We investigate the reasons for the performance degradation incurred with batch-independent normalization. We find that the prototypical techniques of layer normalization and instance normalization both induce the appearance of failure modes in the neural network's pre-activations: (i) layer normalization induces a collapse towards channel-wise constant functions; (ii) instance normalization induces a lack of variability in instance statistics, symptomatic of an alteration of the expressivity. To alleviate failure mode (i) without aggravating failure mode (ii), we introduce the technique \"Proxy Normalization\" that normalizes post-activations using a proxy distribution. When combined with layer normalization or group normalization, this batch-independent normalization emulates batch normalization's behavior and consistently matches or exceeds its performance."}}
{"id": "tpv92FvEuph", "cdate": 1609459200000, "mdate": 1681486382114, "content": {"title": "Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence", "abstract": ""}}
{"id": "iiL-zqz6XSo", "cdate": 1609459200000, "mdate": 1681486272049, "content": {"title": "Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence", "abstract": ""}}
{"id": "XWiImU96Ljh", "cdate": 1609459200000, "mdate": 1681486272052, "content": {"title": "Making EfficientNet More Efficient: Exploring Batch-Independent Normalization, Group Convolutions and Reduced Resolution Training", "abstract": ""}}
{"id": "ryNh9sWu-S", "cdate": 1546300800000, "mdate": null, "content": {"title": "Characterizing Well-Behaved vs. Pathological Deep Neural Networks", "abstract": "We introduce a novel approach, requiring only mild assumptions, for the characterization of deep neural networks at initialization. Our approach applies both to fully-connected and convolutional ne..."}}
{"id": "HylXHhA9Km", "cdate": 1538087995145, "mdate": null, "content": {"title": "Statistical Characterization of Deep Neural Networks and their Sensitivity", "abstract": "Despite their ubiquity, it remains an active area of research to fully understand deep neural networks (DNNs) and the reasons of their empirical success. We contribute to this effort by introducing a principled approach to statistically characterize DNNs and their sensitivity. By distinguishing between randomness from input data and from model parameters, we study how central and non-central moments of network activation and sensitivity evolve during propagation. Thereby, we provide novel statistical insights on the hypothesis space of input-output mappings encoded by different architectures. Our approach applies both to fully-connected and convolutional networks and incorporates most ingredients of modern DNNs: rectified linear unit (ReLU) activation, batch normalization, skip connections."}}
{"id": "qKvIdfQiMw3", "cdate": 1514764800000, "mdate": 1681486382115, "content": {"title": "Characterizing Well-behaved vs. Pathological Deep Neural Network Architectures", "abstract": ""}}
