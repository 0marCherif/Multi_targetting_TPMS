{"id": "Gww4J6ile-", "cdate": 1640995200000, "mdate": 1679901928175, "content": {"title": "Strength of Minibatch Noise in SGD", "abstract": ""}}
{"id": "-y3lFWwyQKQ", "cdate": 1640995200000, "mdate": 1679901928176, "content": {"title": "Power-Law Escape Rate of SGD", "abstract": ""}}
{"id": "rqolQhuq6Hs", "cdate": 1632875749610, "mdate": null, "content": {"title": "Logarithmic landscape and power-law escape rate of SGD", "abstract": "Stochastic gradient descent (SGD) undergoes complicated multiplicative noise for the mean-square loss. We use this property of the SGD noise to derive a stochastic differential equation (SDE) with simpler additive noise by performing a random time change. In the SDE, the loss gradient is replaced by the logarithmized loss gradient. By using this formalism, we obtain the escape rate formula from a local minimum, which is determined not by the loss barrier height $\\Delta L=L(\\theta^s)-L(\\theta^*)$ between a minimum $\\theta^*$ and a saddle $\\theta^s$ but by the logarithmized loss barrier height $\\Delta\\log L=\\log[L(\\theta^s)/L(\\theta^*)]$. Our escape-rate formula strongly depends on the typical magnitude $h^*$ and the number $n$ of the outlier eigenvalues of the Hessian. This result explains an empirical fact that SGD prefers flat minima with low effective dimensions, which gives an insight into implicit biases of SGD."}}
{"id": "uorVGbWV5sw", "cdate": 1632875747449, "mdate": null, "content": {"title": "Strength of Minibatch Noise in SGD", "abstract": "The noise in stochastic gradient descent (SGD), caused by minibatch sampling, is poorly understood despite its practical importance in deep learning. This work presents the first systematic study of the SGD noise and fluctuations close to a local minimum. We first analyze the SGD noise in linear regression in detail and then derive a general formula for approximating SGD noise in different types of minima. For application, our results (1) provide insight into the stability of training a neural network, (2) suggest that a large learning rate can help generalization by introducing an implicit regularization, (3) explain why the linear learning rate-batchsize scaling law fails at a large learning rate or at a small batchsize and (4) can provide an understanding of how discrete-time nature of SGD affects the recently discovered power-law phenomenon of SGD."}}
{"id": "rBqGHRCETXD", "cdate": 1609459200000, "mdate": null, "content": {"title": "On Minibatch Noise: Discrete-Time SGD, Overparametrization, and Bayes", "abstract": "The noise in stochastic gradient descent (SGD), caused by minibatch sampling, is poorly understood despite its practical importance in deep learning. This work presents the first systematic study of the SGD noise and fluctuations close to a local minimum. We first analyze the SGD noise in linear regression in detail and then derive a general formula for approximating SGD noise in different types of minima. For application, our results (1) provide insight into the stability of training a neural network, (2) suggest that a large learning rate can help generalization by introducing an implicit regularization, (3) explain why the linear learning rate-batchsize scaling law fails at a large learning rate or at a small batchsize and (4) can provide an understanding of how discrete-time nature of SGD affects the recently discovered power-law phenomenon of SGD."}}
{"id": "SfZEjKyIl9", "cdate": 1609459200000, "mdate": 1645767068080, "content": {"title": "Noise and Fluctuation of Finite Learning Rate Stochastic Gradient Descent", "abstract": "In the vanishing learning rate regime, stochastic gradient descent (SGD) is now relatively well understood. In this work, we propose to study the basic properties of SGD and its variants in the non..."}}
{"id": "SHrbEoYyUgq", "cdate": 1609459200000, "mdate": 1645767068078, "content": {"title": "Logarithmic landscape and power-law escape rate of SGD", "abstract": "Stochastic gradient descent (SGD) undergoes complicated multiplicative noise for the mean-square loss. We use this property of SGD noise to derive a stochastic differential equation (SDE) with simpler additive noise by performing a random time change. Using this formalism, we show that the log loss barrier $\\Delta\\log L=\\log[L(\\theta^s)/L(\\theta^*)]$ between a local minimum $\\theta^*$ and a saddle $\\theta^s$ determines the escape rate of SGD from the local minimum, contrary to the previous results borrowing from physics that the linear loss barrier $\\Delta L=L(\\theta^s)-L(\\theta^*)$ decides the escape rate. Our escape-rate formula strongly depends on the typical magnitude $h^*$ and the number $n$ of the outlier eigenvalues of the Hessian. This result explains an empirical fact that SGD prefers flat minima with low effective dimensions, giving an insight into implicit biases of SGD."}}
{"id": "wc5kEfQt1Uz", "cdate": 1577836800000, "mdate": null, "content": {"title": "Stochastic Gradient Descent with Large Learning Rate", "abstract": "In the vanishing learning rate regime, stochastic gradient descent (SGD) is now relatively well understood. In this work, we propose to study the basic properties of SGD and its variants in the non-vanishing learning rate regime. The focus is on deriving exactly solvable results and discussing their implications. The main contributions of this work are to derive the stationary distribution for discrete-time SGD in a quadratic loss function with and without momentum; in particular, one implication of our result is that the fluctuation caused by discrete-time dynamics takes a distorted shape and is dramatically larger than a continuous-time theory could predict. Examples of applications of the proposed theory considered in this work include the approximation error of variants of SGD, the effect of minibatch noise, the optimal Bayesian inference, the escape rate from a sharp minimum, and the stationary covariance of a few second-order methods including damped Newton's method, natural gradient descent, and Adam."}}
