{"id": "4NzB00qmiW", "cdate": 1668026837809, "mdate": 1668026837809, "content": {"title": "NeuMips: Neural Mixture of Planar Experts for View Synthesis", "abstract": "We present Neural Mixtures of Planar Experts (NeurMiPs), a novel planar-based scene representation for modeling geometry and appearance. NeurMiPs leverages a\ncollection of local planar experts in 3D space as the scene\nrepresentation. Each planar expert consists of the parameters of the local rectangular shape representing geometry\nand a neural radiance field modeling the color and opacity.\nWe render novel views by calculating ray-plane intersections\nand composite output colors and densities at intersected\npoints to the image. NeurMiPs blends the efficiency of explicit mesh rendering and flexibility of the neural radiance\nfield. Experiments demonstrate superior performance and\nspeed of our proposed method, compared to other 3D representations in novel view synthesis"}}
{"id": "LPBV_khaqiz", "cdate": 1640995200000, "mdate": 1668640405215, "content": {"title": "NeurMiPs: Neural Mixture of Planar Experts for View Synthesis", "abstract": "We present Neural Mixtures of Planar Experts (Neur-MiPs), a novel planar-based scene representation for modeling geometry and appearance. NeurMiPs leverages a collection of local planar experts in 3D space as the scene representation. Each planar expert consists of the parameters of the local rectangular shape representing geometry and a neural radiance field modeling the color and opacity. We render novel views by calculating ray-plane intersections and composite output colors and densities at intersected points to the image. NeurMiPs blends the efficiency of explicit mesh rendering and flexibility of the neural radiance field. Experiments demonstrate superior performance and speed of our proposed method, compared to other 3D representations in novel view synthesis."}}
{"id": "6ipgXgVA04", "cdate": 1640995200000, "mdate": 1668640405218, "content": {"title": "Learning of 3D Graph Convolution Networks for Point Cloud Analysis", "abstract": "Point clouds are among the popular geometry representations in 3D vision. However, unlike 2D images with pixel-wise layouts, such representations containing unordered data points which make the processing and understanding the associated semantic information quite challenging. Although a number of previous works attempt to analyze point clouds and achieve promising performances, their performances would degrade significantly when data variations like shift and scale changes are presented. In this paper, we propose <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">3D graph convolution networks (3D-GCN)</i> , which uniquely learns 3D kernels with graph max-pooling mechanisms for extracting geometric features from point cloud data across different scales. We show that, with the proposed 3D-GCN, satisfactory shift and scale invariance can be jointly achieved. We show that 3D-GCN can be applied to point cloud classification and segmentation tasks, with ablation studies and visualizations verifying the design of 3D-GCN."}}
{"id": "9vYRjdGf_T", "cdate": 1577836800000, "mdate": 1668640405219, "content": {"title": "Convolution in the Cloud: Learning Deformable Kernels in 3D Graph Convolution Networks for Point Cloud Analysis", "abstract": "Point clouds are among the popular geometry representations for 3D vision applications. However, without regular structures like 2D images, processing and summarizing information over these unordered data points are very challenging. Although a number of previous works attempt to analyze point clouds and achieve promising performances, their performances would degrade significantly when data variations like shift and scale changes are presented. In this paper, we propose 3D Graph Convolution Networks (3D-GCN), which is designed to extract local 3D features from point clouds across scales, while shift and scale-invariance properties are introduced. The novelty of our 3D-GCN lies in the definition of learnable kernels with a graph max-pooling mechanism. We show that 3D-GCN can be applied to 3D classification and segmentation tasks, with ablation studies and visualizations verifying the design of 3D-GCN."}}
