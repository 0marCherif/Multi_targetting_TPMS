{"id": "euxp53tkZaT", "cdate": 1673021625998, "mdate": null, "content": {"title": "A Probabilistic Explanation for VoE-based Evaluation", "abstract": "Visual grounded \\emph{Violation of expectations} (VoE) paradigm is widely used to evaluate the physics learning capability of both humans and machines. It does this by measuring the prediction error, or \\emph{surprise}, of a physics learning model in a given scene. Despite intuitive formulation and perfect alignment with developmental psychology, the design of evaluation protocol based on \\textit{surprise} score is empirical. We point out the potential risks behind the traditional \\textit{surprise} score design and provide a probabilistic explanation of VoE paradigm based on \\textit{likelihood ratio theory}. Guided by the theoretical framework, we propose two novel and extensible surprise scores that are theoretically sounded. Furthermore, we implement a simple yet novel baseline based on PredRNN~\\cite{wang2017predrnn} that demonstrates the ability to perform physical reasoning through direct \\emph{pixel-level prediction}. Our model outperforms a strong \\emph{object-level prediction} baseline PLATO, achieving an overall accuracy of 90.0\\% on the \\texttt{Probe} dataset, compared to 73.4\\% for PLATO (with overall accuracy $73.4\\%$). Additionally, we conduct experiments using our newly proposed metric."}}
{"id": "zDjtZZBZtqK", "cdate": 1663849899398, "mdate": null, "content": {"title": "Denoising Masked Autoencoders Help Robust Classification", "abstract": "In this paper, we propose a new self-supervised method, which is called denoising masked autoencoders (DMAE), for learning certified robust classifiers of images. In DMAE, we corrupt each image by adding Gaussian noises to each pixel value and randomly masking several patches. A Transformer-based encoder-decoder model is then trained to reconstruct the original image from the corrupted one. In this learning paradigm, the encoder will learn to capture relevant semantics for the downstream tasks, which is also robust to Gaussian additive noises. We show that the pre-trained encoder can naturally be used as the base classifier in Gaussian smoothed models, where we can analytically compute the certified radius for any data point. Although the proposed method is simple, it yields significant performance improvement in downstream classification tasks. We show that the DMAE ViT-Base model, which just uses 1/10 parameters of the model developed in recent work (Carlini et al., 2022), achieves competitive or better certified accuracy in various settings. The DMAE ViT-Large model significantly surpasses all previous results, establishing a new state-of-the-art on ImageNet dataset. We further demonstrate that the pre-trained model has good transferability to the CIFAR-10 dataset, suggesting its wide adaptability. Models and code are available at\nhttps://github.com/quanlin-wu/dmae."}}
{"id": "crE5pyeb6c_", "cdate": 1640995200000, "mdate": 1683881811529, "content": {"title": "Denoising Masked AutoEncoders are Certifiable Robust Vision Learners", "abstract": "In this paper, we propose a new self-supervised method, which is called Denoising Masked AutoEncoders (DMAE), for learning certified robust classifiers of images. In DMAE, we corrupt each image by adding Gaussian noises to each pixel value and randomly masking several patches. A Transformer-based encoder-decoder model is then trained to reconstruct the original image from the corrupted one. In this learning paradigm, the encoder will learn to capture relevant semantics for the downstream tasks, which is also robust to Gaussian additive noises. We show that the pre-trained encoder can naturally be used as the base classifier in Gaussian smoothed models, where we can analytically compute the certified radius for any data point. Although the proposed method is simple, it yields significant performance improvement in downstream classification tasks. We show that the DMAE ViT-Base model, which just uses 1/10 parameters of the model developed in recent work arXiv:2206.10550, achieves competitive or better certified accuracy in various settings. The DMAE ViT-Large model significantly surpasses all previous results, establishing a new state-of-the-art on ImageNet dataset. We further demonstrate that the pre-trained model has good transferability to the CIFAR-10 dataset, suggesting its wide adaptability. Models and code are available at https://github.com/quanlin-wu/dmae."}}
{"id": "-xHXmIlXND", "cdate": 1640995200000, "mdate": 1681813822348, "content": {"title": "Faster VoxelPose: Real-time 3D Human Pose Estimation by Orthographic Projection", "abstract": "While the voxel-based methods have achieved promising results for multi-person 3D pose estimation from multi-cameras, they suffer from heavy computation burdens, especially for large scenes. We present Faster VoxelPose to address the challenge by re-projecting the feature volume to the three two-dimensional coordinate planes and estimating X,\u00a0Y,\u00a0Z coordinates from them separately. To that end, we first localize each person by a 3D bounding box by estimating a 2D box and its height based on the volume features projected to the xy-plane and z-axis, respectively. Then for each person, we estimate partial joint coordinates from the three coordinate planes separately which are then fused to obtain the final 3D pose. The method is free from costly 3D-CNNs and improves the speed of VoxelPose by ten times and meanwhile achieves competitive accuracy as the state-of-the-art methods, proving its potential in real-time applications."}}
{"id": "-gMaxLcEz_z", "cdate": 1640995200000, "mdate": 1683881811545, "content": {"title": "Guided Diffusion Model for Adversarial Purification from Random Noise", "abstract": "In this paper, we propose a novel guided diffusion purification approach to provide a strong defense against adversarial attacks. Our model achieves 89.62% robust accuracy under PGD-L_inf attack (eps = 8/255) on the CIFAR-10 dataset. We first explore the essential correlations between unguided diffusion models and randomized smoothing, enabling us to apply the models to certified robustness. The empirical results show that our models outperform randomized smoothing by 5% when the certified L2 radius r is larger than 0.5."}}
