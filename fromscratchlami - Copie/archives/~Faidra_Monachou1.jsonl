{"id": "vFKvKIwcHw9", "cdate": 1621630071666, "mdate": null, "content": {"title": "Counterbalancing Learning and Strategic Incentives in Allocation Markets", "abstract": "Motivated by the high discard rate of donated organs in the United States, we study an allocation problem in the presence of learning and strategic incentives. We consider a setting where a benevolent social planner decides whether and how to allocate a single indivisible object to a queue of strategic agents.  The object has a common true quality, good or bad,  which is ex-ante unknown to everyone. Each agent holds an informative, yet noisy, private signal about the quality. To make a correct allocation decision the planner attempts to learn the object quality by truthfully eliciting agents' signals. Under the commonly applied sequential offering mechanism, we show that learning is hampered by the presence of strategic incentives as herding may emerge. This can result in incorrect allocation and welfare loss. To overcome these issues, we propose a novel class of incentive-compatible mechanisms. Our mechanism involves a batch-by-batch, dynamic voting process using a majority rule. We prove that the proposed voting mechanisms improve the probability of correct allocation whenever agents are sufficiently well informed. Particularly, we show that such an improvement can be achieved via a simple greedy algorithm. We quantify the improvement using simulations."}}
{"id": "alT0KR1FDp", "cdate": 1617980582152, "mdate": null, "content": {"title": "Bridging Machine Learning and Mechanism Design Towards Algorithmic Fairness", "abstract": "Decision-making systems increasingly orchestrate our world: how to intervene on thealgorithmic components to build fair and equitable systems is therefore a question of ut-most importance; one that is substantially complicated by the context-dependent natureof fairness and discrimination. Modern decision-making systems that involve allocatingresources or information to people (e.g., school choice, advertising) incorporate machine-learned predictions in their pipelines, raising concerns about potential strategic behavioror constrained allocation, concerns usually tackled in the context of mechanism design.Although both machine learning and mechanism design have developedframeworks foraddressing issues of fairness and equity, in some complex decision-making systems, nei-ther framework is individually sufficient.  In this paper, we develop the position that building fair decision-making systems requires overcoming these limitations which, weargue, are inherent to each field.  Our ultimate objective is to build anencompassingframework that cohesively bridges the individual frameworks of mechanism design and machine learning. We begin to lay the ground work towards this goal by comparing the perspective each discipline takes on fair decision-making, teasing out the lessons each field has taught and can teach the other, and highlighting application domains that require a strong collaboration between these disciplines."}}
{"id": "dorfYEiBtm2", "cdate": 1609459200000, "mdate": null, "content": {"title": "Standardized Tests and Affirmative Action: The Role of Bias and Variance", "abstract": "The University of California suspended through 2024 the requirement that applicants from California submit SAT scores, upending the major role standardized testing has played in college admissions. We study the impact of such decisions and its interplay with other policies---such as affirmative action---on admitted class composition. This paper considers a theoretical framework to study the effect of requiring test scores on academic merit and diversity in college admissions. The model has a college and set of potential students. Each student has observed application components and group membership, as well as an unobserved noisy skill level generated from an observed distribution. The college is Bayesian and maximizes an objective that depends on both diversity and merit. It estimates each applicant's true skill level using the observed features and potentially their group membership, and then admits students with or without affirmative action. We characterize the trade-off between the (potentially positive) informational role of standardized testing in college admissions and its (negative) exclusionary nature. Dropping test scores may exacerbate disparities by decreasing the amount of information available for each applicant, especially those from non-traditional backgrounds. However, if there are substantial barriers to testing, removing the test improves both academic merit and diversity by increasing the size of the applicant pool. Finally, using application and transcript data from the University of Texas at Austin, we demonstrate how an admissions committee could measure the trade-off in practice to better decide whether to drop their test scores requirement. The full paper can be found at https://arxiv.org/abs/2010.04396."}}
{"id": "H1eWLVrl8B", "cdate": 1567802440544, "mdate": null, "content": {"title": "Discrimination in Online Markets: Effects of Social Bias on Learning from Reviews and Policy Design", "abstract": " The increasing popularity of online two-sided markets such as ride-sharing, accommodation and freelance labor platforms, goes hand in hand with new socioeconomic challenges.  A major issue remains the existence of bias and discrimination against certain social groups.  We study this problem using a two-sided large market model of employers and workers mediated by a platform. Employers who seek to hire  workers face uncertainty about a candidate worker's  skill level.Therefore, they  base their hiring decision  on learning from past reviews about an individual worker as well as on their (possibly misspecified)  prior beliefs about the ability level of the social group the worker belongs to. Drawing upon the  social learning literature with bounded rationality and limited information, we find that uncertainty combined with social bias leads to  unequal hiring opportunities between workers of different social groups. Consistent with  empirical findings, we show that the effect of social bias decreases as the number of reviews increases. Furthermore, we quantify discrimination in terms of welfare inequality showing that minority workers have lower expected payoff. Finally, we assume a balanced market and design a simple directed matching policy (DM) which combines matching and learning to make better matching decisions for minority workers. We prove that there exists a steady-state equilibrium such that DM reduces the discrimination gap."}}
