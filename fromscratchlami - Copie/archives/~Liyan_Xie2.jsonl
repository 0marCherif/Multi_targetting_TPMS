{"id": "TYJo_wRlXQ", "cdate": 1680307200000, "mdate": 1684121939174, "content": {"title": "PERCEPT: A New Online Change-Point Detection Method using Topological Data Analysis", "abstract": "Topological data analysis (TDA) provides a set of data analysis tools for extracting embedded topological structures from complex high-dimensional datasets. In recent years, TDA has been a rapidly ..."}}
{"id": "hwod0VI3ysQ", "cdate": 1672531200000, "mdate": 1681651236049, "content": {"title": "Sequential Change-Point Detection for Mutually Exciting Point Processes", "abstract": ""}}
{"id": "quCOIL8JQnp", "cdate": 1663850339597, "mdate": null, "content": {"title": "Improving Adversarial Robustness by Contrastive Guided Diffusion Process", "abstract": "Synthetic data generation has become an emerging tool to help improve the adversarial robustness in classification tasks since robust learning requires a significantly larger amount of training samples compared with standard classification tasks. Among various deep generative models, the diffusion model has been shown to produce high-quality synthetic images and has achieved good performance in improving the adversarial robustness. However, diffusion-type methods are typically slow in data generation as compared with other generative models. Although different acceleration techniques have been proposed recently, it is also of great importance to study how to improve the sample efficiency of generated data for the downstream task. In this paper, we first analyze the optimality condition of synthetic distribution for achieving non-trivial robust accuracy. We show that enhancing the distinguishability among the generated data is critical for improving adversarial robustness. Thus, we propose the Contrastive-Guided Diffusion Process (Contrastive-DP), which adopts the contrastive loss to guide the diffusion model in data generation. We verify our theoretical results using simulations and demonstrate the good performance of Contrastive-DP on image datasets."}}
{"id": "Yg2CRGUln5k", "cdate": 1652737541976, "mdate": null, "content": {"title": "Distributionally robust weighted k-nearest neighbors", "abstract": "Learning a robust classifier from a few samples remains a key challenge in machine learning. A major thrust of research has been focused on developing k-nearest neighbor (k-NN) based algorithms combined with metric learning that captures similarities between samples. When the samples are limited, robustness is especially crucial to ensure the generalization capability of the classifier. In this paper, we study a minimax distributionally robust formulation of weighted k-nearest neighbors, which aims to find the optimal weighted k-NN classifiers that hedge against feature uncertainties. We develop an algorithm, Dr.k-NN, that efficiently solves this functional optimization problem and features in assigning minimax optimal weights to training samples when performing classification. These weights are class-dependent, and are determined by the similarities of sample features under the least favorable scenarios. When the size of the uncertainty set is properly tuned, the robust classifier has a smaller Lipschitz norm than the vanilla k-NN, and thus improves the generalization capability. We also couple our framework with neural-network-based feature embedding. We demonstrate the competitive performance of our algorithm compared to the state-of-the-art in the few-training-sample setting with various real-data experiments."}}
{"id": "oGE_nHXxNh", "cdate": 1640995200000, "mdate": 1681651236036, "content": {"title": "Generalizing to Unseen Domains with Wasserstein Distributional Robustness under Limited Source Knowledge", "abstract": ""}}
{"id": "gWyO2CdYSmL", "cdate": 1640995200000, "mdate": 1684121939201, "content": {"title": "Distributionally robust weighted k-nearest neighbors", "abstract": "Learning a robust classifier from a few samples remains a key challenge in machine learning. A major thrust of research has been focused on developing k-nearest neighbor (k-NN) based algorithms combined with metric learning that captures similarities between samples. When the samples are limited, robustness is especially crucial to ensure the generalization capability of the classifier. In this paper, we study a minimax distributionally robust formulation of weighted k-nearest neighbors, which aims to find the optimal weighted k-NN classifiers that hedge against feature uncertainties. We develop an algorithm, Dr.k-NN, that efficiently solves this functional optimization problem and features in assigning minimax optimal weights to training samples when performing classification. These weights are class-dependent, and are determined by the similarities of sample features under the least favorable scenarios. When the size of the uncertainty set is properly tuned, the robust classifier has a smaller Lipschitz norm than the vanilla k-NN, and thus improves the generalization capability. We also couple our framework with neural-network-based feature embedding. We demonstrate the competitive performance of our algorithm compared to the state-of-the-art in the few-training-sample setting with various real-data experiments."}}
{"id": "aumFxkHbZ6O", "cdate": 1640995200000, "mdate": 1681651236106, "content": {"title": "Minimax Robust Quickest Change Detection using Wasserstein Ambiguity Sets", "abstract": ""}}
{"id": "W1TpeoE0uF", "cdate": 1640995200000, "mdate": 1681651236030, "content": {"title": "Early Detection of COVID-19 Hotspots Using Spatio-Temporal Data", "abstract": ""}}
{"id": "N_MyIiflrl", "cdate": 1640995200000, "mdate": 1681651236282, "content": {"title": "Improving Adversarial Robustness by Contrastive Guided Diffusion Process", "abstract": ""}}
{"id": "Mr_m0ew76Sq", "cdate": 1620880747184, "mdate": null, "content": {"title": "Uncertainty Quantification for Inferring Hawkes Networks", "abstract": "Multivariate Hawkes processes are commonly used to model streaming networked event data in a wide variety of applications. However, it remains a challenge to extract reliable inference from complex datasets with uncertainty quantification. Aiming towards this, we develop a statistical inference framework to learn causal relationships between nodes from networked data, where the underlying directed graph implies Granger causality. We provide uncertainty quantification for the maximum likelihood estimate of the network multivariate Hawkes process by providing a non-asymptotic confidence set. The main technique is based on the concentration inequalities of continuous-time martingales. We compare our method to the previously-derived asymptotic Hawkes process confidence interval, and demonstrate the strengths of our method in an application to neuronal connectivity reconstruction."}}
