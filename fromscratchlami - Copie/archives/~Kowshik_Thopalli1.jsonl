{"id": "mmlix0UucTh", "cdate": 1633790970120, "mdate": null, "content": {"title": "Multi-Domain Ensembles for Domain Generalization", "abstract": "In this paper, we consider the challenging problem of multi-source zero shot domain generalization (MDG), where labeled training data from multiple source domains are available but with no access to data from the target domain. Many methods have been proposed to address this problem, but surprisingly the naiive solution of pooling all source data together and training a single ERM model is highly competitive. Constructing an ensemble of deep classifiers is a popular approach for building models that are calibrated under challenging distribution shifts. Hence, we propose MulDEns (Multi-Domain Deep Ensembles), a new approach for constructing deep ensembles in multi-domain problems that does not require to construct domain-specific models. Our empirical studies on multiple standard benchmarks show that MulDEns significantly outperforms ERM and existing ensembling solutions for MDG."}}
{"id": "UQGYhou3oEi", "cdate": 1633790969245, "mdate": null, "content": {"title": "Re-labeling Domains Improves Multi-Domain Generalization", "abstract": "Domain generalization (DG) methods aim to develop models that generalize to settings where the test distribution is different from the training data. In this paper, we focus on the challenging problem of multi-source zero-shot DG, where labeled training data from multiple source domains is available but with no access to data from the target domain. Though this problem has become an important topic of research, surprisingly, the naive solution of pooling all source data together and training a single classifier is highly competitive on standard benchmarks. More importantly, even sophisticated approaches that explicitly optimize for invariance across different domains do not necessarily provide non-trivial gains over ERM. We hypothesize that this behavior arises due to the poor definitions of the domain splits itself. In this paper, we make a first attempt to understand the role pre-defined domain labels play in the success of domain-aware DG methods. To this end, we ignore the domain labels that come with the dataset but instead alternatively perform unsupervised clustering to infer domain splits and train the DG method with these domain labels. We also introduce a novel regularization to improve the behavior of this alternating optimization process. We conduct analysis on two standard benchmarks PACS and VLCS and demonstrate the benefit of re-categorizing samples into new domain groups on DG performance."}}
{"id": "qXBpzbrH7xS", "cdate": 1626450727628, "mdate": 1626450727628, "content": {"title": "MULTIPLE SUBSPACE ALIGNMENT IMPROVES DOMAIN ADAPTATION", "abstract": "We present a novel unsupervised domain adaptation (DA)\nmethod for cross-domain visual recognition. Though subspace methods have found success in DA, their performance\nis often limited due to the assumption of approximating an\nentire dataset using a single low-dimensional subspace. Instead, we develop a method to effectively represent the source\nand target datasets via a collection of low-dimensional subspaces, and subsequently align them by exploiting the natural\ngeometry of the space of subspaces, on the Grassmann manifold. We demonstrate the effectiveness of this approach,\nusing empirical studies on two widely used benchmarks, with\nstate of the art domain adaptation performance."}}
{"id": "rkZ7O9WOWH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Perturbation Robust Representations of Topological Persistence Diagrams", "abstract": "Topological methods for data analysis present opportunities for enforcing certain invariances of broad interest in computer vision, including view-point in activity analysis, articulation in shape analysis, and measurement invariance in non-linear dynamical modeling. The increasing success of these methods is attributed to the complementary information that topology provides, as well as availability of tools for computing topological summaries such as persistence diagrams. However, persistence diagrams are multi-sets of points and hence it is not straightforward to fuse them with features used for contemporary machine learning tools like deep-nets. In this paper we present theoretically well-grounded approaches to develop novel perturbation robust topological representations, with the long-term view of making them amenable to fusion with contemporary learning architectures. We term the proposed representation as Perturbed Topological Signatures, which live on a Grassmann manifold and hence can be efficiently used in machine learning pipelines. We explore the use of the proposed descriptor on three applications: 3D shape analysis, view-invariant activity analysis, and non-linear dynamical modeling. We show favorable results in both high-level recognition performance and time-complexity when compared to other baseline methods."}}
