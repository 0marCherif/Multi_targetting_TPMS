{"id": "gPxd1tTvoaC", "cdate": 1663850269017, "mdate": null, "content": {"title": "Combating noisy labels with stochastic noise-tolerated supervised contrastive learning", "abstract": "Learning with noisy labels (LNL) aims to achieve good generalization performance given a label-corrupted training set. In this work, we consider a more challenging situation of LNL on \\emph{fine-grained} datasets (LNL-FG). Due to large inter-class ambiguity among those fine-grained classes, deep models are more prone to overfitting to noisy labels, leading to poor generalization performance. To handle this problem, we propose a novel framework called stochastic noise-tolerated supervised contrastive learning (SNSCL) that can enhance discriminability of deep models. Specifically, SNSCL contains a noise-tolerated contrastive loss and a stochastic module. To play against fitting noisy labels, we design a noise-tolerated supervised contrastive learning loss that incorporates a weight-aware mechanism for noisy label correction and selectively updating momentum queue lists. By this mechanism, SCL mitigates the effects of noisy anchors and avoids inserting noisy labels into the momentum-updated queue. Besides, to avoid manually-defined augmentation strategies in SCL, we propose an efficient stochastic module that samples feature embeddings from a generated distribution, which can also enhance the representation ability of SCL. Our proposed SNSCL is general and compatible with prevailing robust LNL strategies to improve their performance for LNL-FG. Extensive experiments on four noisy benchmarks and an open-world dataset with variant noise ratios demonstrate that our proposed framework significantly improves the performance of current LNL methods for LNL-FG."}}
{"id": "z7u6N23ByA8", "cdate": 1640995200000, "mdate": 1668132577044, "content": {"title": "SNIP-FSL: Finding task-specific lottery jackpots for few-shot learning", "abstract": ""}}
{"id": "yGlX1n2SgM", "cdate": 1640995200000, "mdate": 1668132577340, "content": {"title": "Self-Filtering: A Noise-Aware Sample Selection for Label Noise with Confidence Penalization", "abstract": ""}}
{"id": "qrZQuK9Lnb", "cdate": 1640995200000, "mdate": 1668132577039, "content": {"title": "Learning to rectify for robust learning with noisy labels", "abstract": ""}}
{"id": "Z8n2XwgNio", "cdate": 1640995200000, "mdate": 1668132577041, "content": {"title": "Learning Transferable Parameters for Unsupervised Domain Adaptation", "abstract": ""}}
{"id": "FrQvBTVkm9", "cdate": 1609459200000, "mdate": 1668132577339, "content": {"title": "Attentional Prototype Inference for Few-Shot Semantic Segmentation", "abstract": ""}}
{"id": "1zwxyD5wf8r", "cdate": 1609459200000, "mdate": 1668132577341, "content": {"title": "MetaKernel: Learning Variational Random Features with Limited Labels", "abstract": ""}}
{"id": "SKQiIT9TrwL", "cdate": 1577836800000, "mdate": 1668132577337, "content": {"title": "Learning to Learn Kernels with Variational Random Features", "abstract": ""}}
{"id": "rJebgkSFDB", "cdate": 1569439465003, "mdate": null, "content": {"title": "Learning to Learn Kernels with Variational Random Features", "abstract": "Meta-learning for few-shot learning involves a meta-learner that acquires shared knowledge from a set of prior tasks to improve the performance of a base-learner on new tasks with a small amount of data. Kernels are commonly used in machine learning due to their strong nonlinear learning capacity, which have not yet been fully investigated in the meta-learning scenario for few-shot learning. In this work, we explore kernel approximation with random Fourier features in the meta-learning framework for few-shot learning. We propose learning adaptive kernels by meta variational random features (MetaVRF), which is formulated as a variational inference problem. To explore shared knowledge across diverse tasks, our MetaVRF deploys an LSTM inference network to generate informative features, which can establish kernels of highly representational power with low spectral sampling rates, while also being able to quickly adapt to specific tasks for improved performance. We evaluate MetaVRF on a variety of few-shot learning tasks for both regression and classification. Experimental results demonstrate that our MetaVRF can deliver much better or competitive performance than recent meta-learning algorithms."}}
{"id": "aH8D2xW3K4x", "cdate": 1546300800000, "mdate": 1668132577336, "content": {"title": "DUAL-GLOW: Conditional Flow-Based Generative Model for Modality Transfer", "abstract": ""}}
