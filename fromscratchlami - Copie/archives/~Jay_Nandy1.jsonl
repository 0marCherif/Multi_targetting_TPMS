{"id": "AeDI0CZ8Wa", "cdate": 1671916755721, "mdate": 1671916755721, "content": {"title": "Domain-Agnostic Constrastive Representations for Learning from Label Proportions", "abstract": "We study the weak supervision learning problem of Learning from\nLabel Proportions (LLP) where the goal is to learn an instance-level\nclassifier using proportions of various class labels in a bag \u2013 a collection of input instances that often can be highly correlated. While\nrepresentation learning for weakly-supervised tasks is found to\nbe effective, they often require domain knowledge. To the best of\nour knowledge, representation learning for tabular data (unstructured data containing both continuous and categorical features)\nare not studied. In this paper, we propose to learn diverse representations of instances within the same bags to effectively utilize\nthe weak bag-level supervision. We propose a domain agnostic LLP\nmethod, called \"Self Contrastive Representation Learning for LLP\"\n(SelfCLR-LLP) that incorporates a novel self\u2013contrastive function\nas an auxiliary loss to learn representations on tabular data for LLP.\nWe show that diverse representations for instances within the same\nbags aid efficient usage of the weak bag-level LLP supervision. We\nevaluate the proposed method through extensive experiments on\nreal-world LLP datasets from e-commerce applications to demonstrate the effectiveness of our proposed SelfCLR-LLP."}}
{"id": "9_VrvV7d-FK", "cdate": 1663850395221, "mdate": null, "content": {"title": "Unsupervised Adaptation for Fairness under Covariate Shift", "abstract": "Training fair models typically involves optimizing a composite objective accounting for both prediction accuracy and some fairness measure. However, due to a shift in the distribution of the covariates at test time, the learnt fairness tradeoffs may no longer be valid, which we verify experimentally. To address this, we consider an unsupervised adaptation problem of training fair classifiers when only a small set of unlabeled test samples is available along with a large labeled training set. We propose a novel modification to the traditional composite objective by adding a weighted entropy objective on the unlabeled test dataset. This involves a min-max optimization where weights are optimized to mimic the importance weighting ratios followed by classifier optimization. We demonstrate that our weighted entropy objective provides an upper bound on the standard importance sampled training objective common in covariate shift formulations under some mild conditions. Experimentally, we demonstrate that Wasserstein distance based penalty for representation matching across protected sub groups together with the above loss outperforms existing baselines. Our method achieves the best accuracy-equalized odds tradeoff under the covariate shift setup. We find that, for the same accuracy, we get upto 2x improvement in equalized odds on notable benchmarks."}}
{"id": "idtsGjGjh4r", "cdate": 1662928260035, "mdate": 1662928260035, "content": {"title": "Multi-Variate Time Series Forecasting on Variable Subsets", "abstract": "We formulate a new inference task in the domain of multivariate time series forecasting (MTSF), called Variable Subset Forecast\n(VSF), where only a small subset of the variables is available during\ninference. Variables are absent during inference because of longterm data loss (eg. sensor failures) or high\u2192low-resource domain\nshift between train / test. To the best of our knowledge, robustness\nof MTSF models in presence of such failures, has not been studied\nin the literature. Through extensive evaluation, we first show that\nthe performance of state of the art methods degrade significantly in\nthe VSF setting. We propose a non-parametric, wrapper technique\nthat can be applied on top any existing forecast models. Through\nsystematic experiments across 4 datasets and 5 forecast models,\nwe show that our technique is able to recover close to 95% performance of the models even when only 15% of the original variables\nare present."}}
{"id": "RFGkzxMFqby", "cdate": 1632875480900, "mdate": null, "content": {"title": "Adversarially Trained Models with Test-Time Covariate Shift Adaptation", "abstract": "Existing defense models against adversarial examples typically provide either empirical or certified robustness. Adversarially trained models empirically demonstrate state-of-the-art defense while providing no robustness guarantees for large classifiers or higher-dimensional inputs.\nIn contrast, a randomized smoothing framework provides state-of-the-art certification while significantly degrades the empirical performance against adversarial attacks. \nIn this work, we propose a novel \\textit{certification through adaptation} technique that transforms an adversarially trained model into a randomized smoothing classifier during inference to provide certified robustness for $\\ell_2$ norm without affecting their empirical robustness against adversarial attacks. One advantage of our proposed technique is that it allows us to separately choose the appropriate noise level for certifying each test example during inference. It also leads to outperform the existing randomized smoothing models for $\\ell_2$ certification on CIFAR-10. Therefore, our work is a step towards bridging the gap between the empirical and certified robustness against adversarial examples by achieving both using the same classifier for the first time."}}
{"id": "NjjY_SJXpH", "cdate": 1609459200000, "mdate": null, "content": {"title": "Adversarially Robust Classifier with Covariate Shift Adaptation", "abstract": "The current state-of-the-art defense methods against adversarial examples typically focus on improving either empirical or certified robustness. Among them, adversarially trained (AT) models produce empirical state-of-the-art defense against adversarial examples without providing any robustness guarantees for large classifiers or higher-dimensional inputs. In contrast, existing randomized smoothing based models achieve state-of-the-art certified robustness while significantly degrading the empirical robustness against adversarial examples. In this paper, we propose a novel method, called \\emph{Certification through Adaptation}, that transforms an AT model into a randomized smoothing classifier during inference to provide certified robustness for $\\ell_2$ norm without affecting their empirical robustness against adversarial attacks. We also propose \\emph{Auto-Noise} technique that efficiently approximates the appropriate noise levels to flexibly certify the test examples using randomized smoothing technique. Our proposed \\emph{Certification through Adaptation} with \\emph{Auto-Noise} technique achieves an \\textit{average certified radius (ACR) scores} up to $1.102$ and $1.148$ respectively for CIFAR-10 and ImageNet datasets using AT models without affecting their empirical robustness or benign accuracy. Therefore, our paper is a step towards bridging the gap between the empirical and certified robustness against adversarial examples by achieving both using the same classifier."}}
{"id": "yGD18tz1U0o", "cdate": 1577836800000, "mdate": null, "content": {"title": "Towards Maximizing the Representation Gap between In-Domain \\& Out-of-Distribution Examples", "abstract": "Among existing uncertainty estimation approaches, Dirichlet Prior Network (DPN) distinctly models different predictive uncertainty types. However, for in-domain examples with high data uncertainties among multiple classes, even a DPN model often produces indistinguishable representations from the out-of-distribution (OOD) examples, compromising their OOD detection performance. We address this shortcoming by proposing a novel loss function for DPN to maximize the \\textit{representation gap} between in-domain and OOD examples. Experimental results demonstrate that our proposed approach consistently improves OOD detection performance."}}
{"id": "uoLjAn-xZEu", "cdate": 1577836800000, "mdate": null, "content": {"title": "Approximate Manifold Defense Against Multiple Adversarial Perturbations", "abstract": "Existing defenses against adversarial attacks are typically tailored to a specific perturbation type. Using adversarial training to defend against multiple types of perturbation requires expensive adversarial examples from different perturbation types at each training step. In contrast, manifold-based defense incorporates a generative network to project an input sample onto the clean data manifold. This approach eliminates the need to generate expensive adversarial examples while achieving robustness against multiple perturbation types. However, the success of this approach relies on whether the generative network can capture the complete clean data manifold, which remains an open problem for complex input domain. In this work, we devise an approximate manifold defense mechanism, called RBF-CNN, for image classification. Instead of capturing the complete data manifold, we use an RBF layer to learn the density of small image patches. RBF-CNN also utilizes a reconstruction layer that mitigates any minor adversarial perturbations. Further, incorporating our proposed reconstruction process for training improves the adversarial robustness of our RBF-CNN models. Experiment results on MNIST and CIFAR-10 datasets indicate that RBF-CNN offers robustness for multiple perturbations without the need for expensive adversarial training."}}
{"id": "aHNTbxZ8Car", "cdate": 1577836800000, "mdate": null, "content": {"title": "Approximate Manifold Defense Against Multiple Adversarial Perturbations", "abstract": "Existing defenses against adversarial attacks are typically tailored to a specific perturbation type. Using adversarial training to defend against multiple types of perturbation requires expensive adversarial examples from different perturbation types at each training step. In contrast, manifold-based defense incorporates a generative network to project an input sample onto the clean data manifold. This approach eliminates the need to generate expensive adversarial examples while achieving robustness against multiple perturbation types. However, the success of this approach relies on whether the generative network can capture the complete clean data manifold, which remains an open problem for complex input domain. In this work, we devise an approximate manifold defense mechanism, called RBF-CNN, for image classification. Instead of capturing the complete data manifold, we use an RBF layer to learn the density of small image patches. RBF-CNN also utilizes a reconstruction layer that mitigates any minor adversarial perturbations. Further, incorporating our proposed reconstruction process for training improves the adversarial robustness of our RBF-CNN models. Experiment results on MNIST and CIFAR-10 datasets indicate that RBF-CNN offers robustness for multiple perturbations without the need for expensive adversarial training."}}
{"id": "AEhHoiMPSJ9", "cdate": 1577836800000, "mdate": null, "content": {"title": "Towards Maximizing the Representation Gap between In-Domain & Out-of-Distribution Examples", "abstract": "Among existing uncertainty estimation approaches, Dirichlet Prior Network (DPN) distinctly models different predictive uncertainty types. However, for in-domain examples with high data uncertainties among multiple classes, even a DPN model often produces indistinguishable representations from the out-of-distribution (OOD) examples, compromising their OOD detection performance. We address this shortcoming by proposing a novel loss function for DPN to maximize the representation gap between in-domain and OOD examples. Experimental results demonstrate that our proposed approach consistently improves OOD detection performance."}}
{"id": "Bye4iaEFwr", "cdate": 1569439131954, "mdate": null, "content": {"title": "Improving Dirichlet Prior Network for Out-of-Distribution Example Detection", "abstract": "Determining the source of uncertainties in the predictions of AI systems are important. It allows the users to act in an informative manner to improve the safety of such systems, applied to the real-world sensitive applications. Predictive uncertainties can originate from the uncertainty in model parameters, data uncertainty or due to distributional mismatch between training and test examples. While recently, significant progress has been made to improve the predictive uncertainty estimation of deep learning models, most of these approaches either conflate the distributional uncertainty with model uncertainty or data uncertainty. In contrast, the Dirichlet Prior Network (DPN)  can model distributional uncertainty distinctly by parameterizing a prior Dirichlet over the predictive categorical distributions. However, their complex loss function by explicitly incorporating KL divergence between Dirichlet distributions often makes the error surface \nill-suited to optimize for challenging datasets with multiple classes. In this paper, we present an improved DPN framework by proposing a novel loss function using the standard cross-entropy loss along with a regularization term to control the sharpness of the output Dirichlet distributions from the network. Our proposed loss function aims to improve the training efficiency of the DPN framework for challenging classification tasks with large number of classes. In our experiments using synthetic and real datasets, we demonstrate that our DPN models can distinguish the distributional uncertainty from other uncertainty types. Our proposed approach significantly improves DPN frameworks and outperform the existing OOD detectors on CIFAR-10 and CIFAR-100 dataset while also being able to recognize distributional uncertainty distinctly."}}
