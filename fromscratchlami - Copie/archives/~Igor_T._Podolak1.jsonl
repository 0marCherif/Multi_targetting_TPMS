{"id": "hxA53wtPmo", "cdate": 1640995200000, "mdate": 1681658137342, "content": {"title": "Generative models with kernel distance in data space", "abstract": ""}}
{"id": "gx6qkpKP7sF", "cdate": 1640995200000, "mdate": 1681658137557, "content": {"title": "Batch Size Reconstruction-Distribution Trade-Off In Kernel Based Generative Autoencoders", "abstract": "Most autoencoder-based generative machine learning models use a two-factor cost function composed of reconstruction error and prior distribution distance. The latter is often evaluated with kernel\u2013based methods. We notice that the impact of the batch size is different on each of the factors: kernel distribution profits from larger batches, while the reconstruction term achieves peak performance on smaller ones. Thus, we define a batch size reconstruction-distribution trade-off. Instead of searching for an optimum global size of the batch, we propose to use small batches for the sake of reconstruction together with a vector enhanced with previously computed latent data for the sake of prior distribution optimization. We evaluate our method on standard benchmarks and illustrate that it can improve the model\u2019s generative scores."}}
{"id": "7ktHTjV9FHw", "cdate": 1632875651672, "mdate": null, "content": {"title": "Relative Molecule Self-Attention Transformer", "abstract": "Self-supervised learning holds promise to revolutionize molecule property prediction - a central task to drug discovery and many more industries - by enabling data efficient learning from scarce experimental data. Despite significant progress, non-pretrained methods can be still competitive in certain settings. We reason that architecture might be a key bottleneck. In particular, enriching the backbone architecture with domain-specific inductive biases has been key for the success of self-supervised learning in other domains. In this spirit, we methodologically explore the design space of the self-attention mechanism tailored to molecular data. We identify a novel variant of self-attention adapted to processing molecules, inspired by the relative self-attention layer, which involves fusing embedded graph and distance relationships between atoms. Our main contribution is Relative Molecule Attention Transformer (R-MAT): a novel Transformer-based model based on the developed self-attention layer that achieves state-of-the-art or very competitive results across a~wide range of molecule property prediction tasks. "}}
{"id": "7AiFm-cB-ac", "cdate": 1621629922396, "mdate": null, "content": {"title": "Zero Time Waste: Recycling Predictions in Early Exit Neural Networks", "abstract": "The problem of reducing processing time of large deep learning models is a fundamental challenge in many real-world applications. Early exit methods strive towards this goal by attaching additional Internal Classifiers (ICs) to intermediate layers of a neural network. ICs can quickly return predictions for easy examples and, as a result, reduce the average inference time of the whole model. However, if a particular IC does not decide to return an answer early, its predictions are discarded, with its computations effectively being wasted. To solve this issue, we introduce Zero Time Waste (ZTW), a novel approach in which each IC reuses predictions returned by its predecessors by (1) adding direct connections between ICs and (2) combining previous outputs in an ensemble-like manner. We conduct extensive experiments across various datasets and architectures to demonstrate that ZTW achieves a significantly better accuracy vs. inference time trade-off than other recently proposed early exit methods."}}
{"id": "14-dXLRn4fE", "cdate": 1621629922396, "mdate": null, "content": {"title": "Zero Time Waste: Recycling Predictions in Early Exit Neural Networks", "abstract": "The problem of reducing processing time of large deep learning models is a fundamental challenge in many real-world applications. Early exit methods strive towards this goal by attaching additional Internal Classifiers (ICs) to intermediate layers of a neural network. ICs can quickly return predictions for easy examples and, as a result, reduce the average inference time of the whole model. However, if a particular IC does not decide to return an answer early, its predictions are discarded, with its computations effectively being wasted. To solve this issue, we introduce Zero Time Waste (ZTW), a novel approach in which each IC reuses predictions returned by its predecessors by (1) adding direct connections between ICs and (2) combining previous outputs in an ensemble-like manner. We conduct extensive experiments across various datasets and architectures to demonstrate that ZTW achieves a significantly better accuracy vs. inference time trade-off than other recently proposed early exit methods."}}
{"id": "tBIyjlRJB-", "cdate": 1609459200000, "mdate": 1682329714819, "content": {"title": "Zero Time Waste: Recycling Predictions in Early Exit Neural Networks", "abstract": "The problem of reducing processing time of large deep learning models is a fundamental challenge in many real-world applications. Early exit methods strive towards this goal by attaching additional Internal Classifiers (ICs) to intermediate layers of a neural network. ICs can quickly return predictions for easy examples and, as a result, reduce the average inference time of the whole model. However, if a particular IC does not decide to return an answer early, its predictions are discarded, with its computations effectively being wasted. To solve this issue, we introduce Zero Time Waste (ZTW), a novel approach in which each IC reuses predictions returned by its predecessors by (1) adding direct connections between ICs and (2) combining previous outputs in an ensemble-like manner. We conduct extensive experiments across various datasets and architectures to demonstrate that ZTW achieves a significantly better accuracy vs. inference time trade-off than other recently proposed early exit methods."}}
{"id": "OF2k2xsGlW", "cdate": 1609459200000, "mdate": 1681661505622, "content": {"title": "Zero Time Waste: Recycling Predictions in Early Exit Neural Networks", "abstract": "The problem of reducing processing time of large deep learning models is a fundamental challenge in many real-world applications. Early exit methods strive towards this goal by attaching additional Internal Classifiers (ICs) to intermediate layers of a neural network. ICs can quickly return predictions for easy examples and, as a result, reduce the average inference time of the whole model. However, if a particular IC does not decide to return an answer early, its predictions are discarded, with its computations effectively being wasted. To solve this issue, we introduce Zero Time Waste (ZTW), a novel approach in which each IC reuses predictions returned by its predecessors by (1) adding direct connections between ICs and (2) combining previous outputs in an ensemble-like manner. We conduct extensive experiments across various datasets and architectures to demonstrate that ZTW achieves a significantly better accuracy vs. inference time trade-off than other recently proposed early exit methods."}}
{"id": "FmiOviZ-lct", "cdate": 1609459200000, "mdate": 1682329714748, "content": {"title": "Relative Molecule Self-Attention Transformer", "abstract": "Self-supervised learning holds promise to revolutionize molecule property prediction - a central task to drug discovery and many more industries - by enabling data efficient learning from scarce experimental data. Despite significant progress, non-pretrained methods can be still competitive in certain settings. We reason that architecture might be a key bottleneck. In particular, enriching the backbone architecture with domain-specific inductive biases has been key for the success of self-supervised learning in other domains. In this spirit, we methodologically explore the design space of the self-attention mechanism tailored to molecular data. We identify a novel variant of self-attention adapted to processing molecules, inspired by the relative self-attention layer, which involves fusing embedded graph and distance relationships between atoms. Our main contribution is Relative Molecule Attention Transformer (R-MAT): a novel Transformer-based model based on the developed self-attention layer that achieves state-of-the-art or very competitive results across a~wide range of molecule property prediction tasks."}}
{"id": "DUcrDRdU1fA", "cdate": 1609459200000, "mdate": 1682329714699, "content": {"title": "2D SIFt: a matrix of ligand-receptor interactions", "abstract": "Depicting a ligand-receptor complex via Interaction Fingerprints has been shown to be both a viable data visualization and an analysis tool. The spectrum of its applications ranges from simple visualization of the binding site through analysis of molecular dynamics runs, to the evaluation of the homology models and virtual screening. Here we present a novel tool derived from the Structural Interaction Fingerprints providing a detailed and unique insight into the interactions between receptor and specific regions of the ligand (grouped into pharmacophore features) in the form of a matrix, a 2D-SIFt descriptor. The provided implementation is easy to use and extends the python library, allowing the generation of interaction matrices and their manipulation (reading and writing as well as producing the average 2D-SIFt). The library for handling the interaction matrices is available via repository http://bitbucket.org/zchl/sift2d ."}}
{"id": "ybbJu2xJwwq", "cdate": 1577836800000, "mdate": 1681658137525, "content": {"title": "Generative models with kernel distance in data space", "abstract": "Generative models dealing with modeling a~joint data distribution are generally either autoencoder or GAN based. Both have their pros and cons, generating blurry images or being unstable in training or prone to mode collapse phenomenon, respectively. The objective of this paper is to construct a~model situated between above architectures, one that does not inherit their main weaknesses. The proposed LCW generator (Latent Cramer-Wold generator) resembles a classical GAN in transforming Gaussian noise into data space. What is of utmost importance, instead of a~discriminator, LCW generator uses kernel distance. No adversarial training is utilized, hence the name generator. It is trained in two phases. First, an autoencoder based architecture, using kernel measures, is built to model a manifold of data. We propose a Latent Trick mapping a Gaussian to latent in order to get the final model. This results in very competitive FID values."}}
