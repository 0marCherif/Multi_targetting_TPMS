{"id": "98mksfFJROk", "cdate": 1667472488876, "mdate": 1667472488876, "content": {"title": "Few-Shot Object Detection with Model Calibration", "abstract": "Few-shot object detection (FSOD) targets at transferring knowledge from known to unknown classes to detect objects of novel classes. However, previous works ignore the model bias problem inherent in the transfer learning paradigm. Such model bias causes overfitting toward the training classes and destructs the well-learned transferable knowledge. In this paper, we pinpoint and comprehensively investigate the model bias problem in FSOD models and propose a simple yet effective method to address the model bias problem with the facilitation of model calibrations in three levels: 1) Backbone calibration to preserve the well-learned prior knowledge and relieve the model bias toward base classes, 2) RPN calibration to rescue unlabeled objects of novel classes and, 3) Detector calibration to prevent the model bias toward a few training samples for novel classes. Specifically, we leverage the overlooked classification dataset to facilitate our model calibration procedure, which has only been used for pre-training in other related works. We validate the effectiveness of our model calibration method on the popular Pascal VOC and MS COCO datasets, where our method achieves very promising performance."}}
{"id": "rrxDSvp-rWn", "cdate": 1667472453635, "mdate": 1667472453635, "content": {"title": "Self-Support Few-Shot Semantic Segmentation", "abstract": "Existing few-shot segmentation methods have achieved great progress based on the support-query matching framework. But they still heavily suffer from the limited coverage of intra-class variations from the few-shot supports provided. Motivated by the simple Gestalt principle that pixels belonging to the same object are more similar than those to different objects of same class, we propose a novel self-support matching strategy to alleviate this problem, which uses query prototypes to match query features, where the query prototypes are collected from high-confidence query predictions. This strategy can effectively capture the consistent underlying characteristics of the query objects, and thus fittingly match query features. We also propose an adaptive self-support background prototype generation module and self-support loss to further facilitate the self-support matching procedure. Our self-support network substantially improves the prototype quality, benefits more improvement from stronger backbones and more supports, and achieves SOTA on multiple datasets. Codes are at \\url{https://github.com/fanq15/SSP}."}}
{"id": "odVhX3QDtO", "cdate": 1667472415741, "mdate": 1667472415741, "content": {"title": "Few-shot video object detection", "abstract": "We introduce Few-Shot Video Object Detection (FSVOD)\nwith three contributions to real-world visual learning challenge in our\nhighly diverse and dynamic world: 1) a large-scale video dataset FSVOD500 comprising of 500 classes with class-balanced videos in each category for few-shot learning; 2) a novel Tube Proposal Network (TPN) to\ngenerate high-quality video tube proposals for aggregating feature representation for the target video object which can be highly dynamic;\n3) a strategically improved Temporal Matching Network (TMN+) for\nmatching representative query tube features with better discriminative\nability thus achieving higher diversity. Our TPN and TMN+ are jointly\nand end-to-end trained. Extensive experiments demonstrate that our\nmethod produces significantly better detection results on two few-shot\nvideo object detection datasets compared to image-based methods and\nother naive video-based extensions. "}}
{"id": "k5XML9l9aR", "cdate": 1667472355008, "mdate": 1667472355008, "content": {"title": "Commonality-parsing network across shape and appearance for partially supervised instance segmentation", "abstract": "Partially supervised instance segmentation aims to perform\nlearning on limited mask-annotated categories of data thus eliminating\nexpensive and exhaustive mask annotation. The learned models are expected to be generalizable to novel categories. Existing methods either\nlearn a transfer function from detection to segmentation, or cluster shape\npriors for segmenting novel categories. We propose to learn the underlying class-agnostic commonalities that can be generalized from maskannotated categories to novel categories. Specifically, we parse two types\nof commonalities: 1) shape commonalities which are learned by performing supervised learning on instance boundary prediction; and 2) appearance commonalities which are captured by modeling pairwise affinities\namong pixels of feature maps to optimize the separability between instance and the background. Incorporating both the shape and appearance commonalities, our model significantly outperforms the state-ofthe-art methods on both partially supervised setting and few-shot setting for instance segmentation on COCO dataset."}}
{"id": "zwTt7b1nsl", "cdate": 1667472182935, "mdate": 1667472182935, "content": {"title": "Group collaborative learning for co-salient object detection", "abstract": "We present a novel group collaborative learning framework (GCNet) capable of detecting co-salient objects in real time (16ms), by simultaneously mining consensus representations at group level based on the two necessary criteria: 1) intra-group compactness to better formulate the consistency among co-salient objects by capturing their inherent shared attributes using our novel group affinity module; 2) inter-group separability to effectively suppress the influence of noisy objects on the output by introducing our new group collaborating module conditioning the inconsistent consensus. To learn a better embedding space without extra computational overhead, we explicitly employ auxiliary classification supervision. Extensive experiments on three challenging benchmarks, ie, CoCA, CoSOD3k, and Cosal2015, demonstrate that our simple GCNet outperforms 10 cutting-edge models and achieves the new state-of-the-art. We demonstrate this paper's new technical contributions on a number of important downstream computer vision applications including content aware co-segmentation, co-localization based automatic thumbnails, etc. Our research code with two applications will be released."}}
{"id": "0somR4DXJ3", "cdate": 1667472115559, "mdate": 1667472115559, "content": {"title": "Few-shot object detection with attention-RPN and multi-relation detector", "abstract": "Conventional methods for object detection typically require a substantial amount of training data and preparing such high-quality training data is very labor-intensive. In this paper, we propose a novel few-shot object detection network that aims at detecting objects of unseen categories with only a few annotated examples. Central to our method are our Attention-RPN, Multi-Relation Detector and Contrastive Training strategy, which exploit the similarity between the few shot support set and query set to detect novel objects while suppressing false detection in the background. To train our network, we contribute a new dataset that contains 1000 categories of various objects with high-quality annotations. To the best of our knowledge, this is one of the first datasets specifically designed for few-shot object detection. Once our few-shot network is trained, it can detect objects of unseen categories without further training or fine-tuning. Our method is general and has a wide range of potential applications. We produce a new state-of-the-art performance on different datasets in the few-shot setting. The dataset link is https://github. com/fanq15/Few-Shot-Object-Detection-Dataset."}}
{"id": "-DEv_3tIK2", "cdate": 1667402913382, "mdate": 1667402913382, "content": {"title": "Learning Sequence Representations by Non-local Recurrent Neural Memory", "abstract": "The key challenge of sequence representation learning is to capture the long-range temporal dependencies. Typical methods for supervised sequence representation learning are built upon recurrent neural networks to capture temporal dependencies. One potential limitation of these methods is that they only model one-order information interactions explicitly between adjacent time steps in a sequence, hence the high-order interactions between nonadjacent time steps are not fully exploited. It greatly limits the capability of modeling the long-range temporal dependencies since the temporal features learned by one-order interactions cannot be maintained for a long term due to temporal information dilution and gradient vanishing. To tackle this limitation, we propose the non-local recurrent neural memory (NRNM) for supervised sequence representation learning, which performs non-local operations by means of self-attention mechanism to learn full-order interactions within a sliding temporal memory block and models global interactions between memory blocks in a gated recurrent manner. Consequently, our model is able to capture long-range dependencies. Besides, the latent high-level features contained in high-order interactions can be distilled by our model. We validate the effectiveness and generalization of our NRNM on three types of sequence applications across different modalities, including sequence classification, step-wise sequential prediction and sequence similarity learning. Our model compares favorably against other state-of-the-art methods specifically designed for each of these sequence applications."}}
{"id": "DDoyY3T-yw8", "cdate": 1667353696543, "mdate": 1667353696543, "content": {"title": "Pyramid Multi-view Stereo Net with Self-adaptive View Aggregation", "abstract": " In this paper, we propose an effective and efficient pyramid\nmulti-view stereo (MVS) net with self-adaptive view aggregation for accurate and complete dense point cloud reconstruction. Different from\nusing mean square variance to generate cost volume in previous deeplearning based MVS methods, our VA-MVSNet incorporates the cost\nvariances in different views with small extra memory consumption by\nintroducing two novel self-adaptive view aggregations: pixel-wise view\naggregation and voxel-wise view aggregation. To further boost the robustness and completeness of 3D point cloud reconstruction, we extend\nVA-MVSNet with pyramid multi-scale images input as PVA-MVSNet,\nwhere multi-metric constraints are leveraged to aggregate the reliable\ndepth estimation at the coarser scale to fill in the mismatched regions at\nthe finer scale. Experimental results show that our approach establishes\na new state-of-the-art on the DTU dataset with significant improvements in the completeness and overall quality, and has strong generalization by achieving a comparable performance as the state-of-the-art\nmethods on the Tanks and Temples benchmark. Our codebase is at\nhttps://github.com/yhw-yhw/PVAMVSNet"}}
{"id": "k-JvYGkA9o", "cdate": 1663849986956, "mdate": null, "content": {"title": "How Normalization and Weight Decay Can Affect SGD? Insights from a Simple Normalized Model", "abstract": "Recent works(Li et al., 2020, Wan et al., 2021) characterize an important mechanism of normalized model trained with SGD and WD (Weight Decay), called Spherical Motion Dynamics (SMD), confirming its widespread effects in practice. However, no theoretical study is available on the influence of SMD on the training process of normalized models in literature. In this work, we seek to understand the effect of SMD by theoretically analyzing a simple normalized model, named as Noisy Rayleigh Quotient (NRQ). On NRQ, We theoretically prove SMD can dominate the whole training process via controlling the evolution of angular update (AU), an essential feature of SMD. Specifically, we show: 1) within equilibrium state of SMD, the convergence rate and limiting risk of NRQ are mainly determined by the theoretical value of AU; and 2) beyond equilibrium state, the evolution of AU can interfere the optimization trajectory, causing odd phenomena such as ``escape'' behavior. We further show the insights drawn from NRQ is consistent with empirical observations in experiments on real datasets. We believe our theoretical results shed new light on the role of normalization techniques during the training of modern deep learning models."}}
{"id": "vqSyt8D3ny", "cdate": 1663849932016, "mdate": null, "content": {"title": "Towards Robust Object Detection Invariant to Real-World Domain Shifts", "abstract": "Safety-critical applications such as autonomous driving require robust object detection invariant to real-world domain shifts. Such shifts can be regarded as different domain styles, which can vary substantially due to environment changes and sensor noises, but deep models only know the training domain style. Such domain style gap impedes object detection generalization on diverse real-world domains. Existing classification domain generalization (DG) methods cannot effectively solve the robust object detection problem, because they either rely on multiple source domains with large style variance or destroy the content structures of the original images. In this paper, we analyze and investigate effective solutions to overcome domain style overfitting for robust object detection without the above shortcomings. Our method, dubbed as Normalization Perturbation (NP), perturbs the channel statistics of source domain low-level features to synthesize various latent styles, so that the trained deep model can perceive diverse potential domains and generalizes well even without observations of target domain data in training. This approach is motivated by the observation that feature channel statistics of the target domain images deviate around the source domain statistics. We further explore the style-sensitive channels for effective style synthesis. Normalization Perturbation only relies on a single source domain and is surprisingly simple and effective, contributing a practical solution by effectively adapting or generalizing classification DG methods to robust object detection. Extensive experiments demonstrate the effectiveness of our method for generalizing object detectors under real-world domain shifts."}}
