{"id": "_zPG0ShaZTc", "cdate": 1652737366365, "mdate": null, "content": {"title": "The Unreasonable Effectiveness of Fully-Connected Layers for Low-Data Regimes", "abstract": "Convolutional neural networks were the standard for solving many computer vision tasks until recently, when Transformers of MLP-based architectures have started to show competitive performance. These architectures typically have a vast number of weights and need to be trained on massive datasets; hence, they are not suitable for their use in low-data regimes. In this work, we propose a simple yet effective framework to improve generalization from small amounts of data. We augment modern CNNs with fully-connected (FC) layers and show the massive impact this architectural change has in low-data regimes. We further present an online joint knowledge-distillation method to utilize the extra FC layers at train time but avoid them during test time. This allows us to improve the generalization of a CNN-based model without any increase in the number of weights at test time. We perform classification experiments for a large range of network backbones and several standard datasets on supervised learning and active learning. Our experiments significantly outperform the networks without fully-connected layers, reaching a relative improvement of up to $16\\%$ validation accuracy in the supervised setting without adding any extra parameters during inference. "}}
{"id": "sTAV84tMr3O", "cdate": 1640995200000, "mdate": 1681493093287, "content": {"title": "Generalization In Multi-Objective Machine Learning", "abstract": ""}}
{"id": "HjfehdI9bZL", "cdate": 1640995200000, "mdate": 1668229410334, "content": {"title": "The Unreasonable Effectiveness of Fully-Connected Layers for Low-Data Regimes", "abstract": "Convolutional neural networks were the standard for solving many computer vision tasks until recently, when Transformers of MLP-based architectures have started to show competitive performance. These architectures typically have a vast number of weights and need to be trained on massive datasets; hence, they are not suitable for their use in low-data regimes. In this work, we propose a simple yet effective framework to improve generalization from small amounts of data. We augment modern CNNs with fully-connected (FC) layers and show the massive impact this architectural change has in low-data regimes. We further present an online joint knowledge-distillation method to utilize the extra FC layers at train time but avoid them during test time. This allows us to improve the generalization of a CNN-based model without any increase in the number of weights at test time. We perform classification experiments for a large range of network backbones and several standard datasets on supervised learning and active learning. Our experiments significantly outperform the networks without fully-connected layers, reaching a relative improvement of up to $16\\%$ validation accuracy in the supervised setting without adding any extra parameters during inference."}}
{"id": "8RW8bvk8sIi", "cdate": 1640995200000, "mdate": 1681985615096, "content": {"title": "Intriguing Properties of Input-Dependent Randomized Smoothing", "abstract": "Randomized smoothing is currently considered the state-of-the-art method to obtain certifiably robust classifiers. Despite its remarkable performance, the method is associated with various serious ..."}}
{"id": "aJ9BXxg352", "cdate": 1632875526965, "mdate": null, "content": {"title": "Intriguing Properties of Input-dependent Randomized Smoothing", "abstract": "Randomized smoothing is currently considered the state-of-the-art method to obtain certifiably robust classifiers. Despite its remarkable performance, the method is associated with various serious problems such as ``certified accuracy waterfalls'', certification vs.\\ accuracy trade-off, or even fairness issues. Input-dependent smoothing approaches have been proposed to overcome these flaws. However, we demonstrate that these methods lack formal guarantees and so the resulting certificates are not justified. We show that the input-dependent smoothing, in general, suffers from the curse of dimensionality, forcing the variance function to have low semi-elasticity. On the other hand, we provide a theoretical and practical framework that enables the usage of input-dependent smoothing even in the presence of the curse of dimensionality, under strict restrictions. We present one concrete design of the smoothing variance and test it on CIFAR10 and MNIST. Our design solves some of the problems of classical smoothing and is formally underlined, yet further improvement of the design is still necessary."}}
