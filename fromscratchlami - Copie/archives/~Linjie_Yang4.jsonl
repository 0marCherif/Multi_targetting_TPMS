{"id": "1hBxkv3iM4", "cdate": 1668186726897, "mdate": 1668186726897, "content": {"title": "Progressive Temporal Feature Alignment Network for Video Inpainting", "abstract": "Video inpainting aims to fill spatio-temporal\" corrupted\" regions with plausible content. To achieve this goal, it is necessary to find correspondences from neighbouring frames to faithfully hallucinate the unknown content. Current methods achieve this goal through attention, flow-based warping, or 3D temporal convolution. However, flow-based warping can create artifacts when optical flow is not accurate, while temporal convolution may suffer from spatial misalignment. We proposeProgressive Temporal Feature Alignment Network', which progressively enriches features extracted from the current frame with the feature warped from neighbouring frames using optical flow. Our approach corrects the spatial misalignment in the temporal feature propagation stage, greatly improving visual quality and temporal consistency of the inpainted videos. Using the proposed architecture, we achieve state-of-the-art performance on the DAVIS and FVI datasets compared to existing deep learning approaches. Code is available at https://github. com/MaureenZOU/TSAM."}}
{"id": "O4L8A8a-M9", "cdate": 1667357382332, "mdate": 1667357382332, "content": {"title": "Is in-domain data really needed? A pilot study on cross-domain calibration for network quantization", "abstract": "Post-training quantization methods use a set of calibration data to compute quantization ranges for network parameters and activations. The calibration data usually comes from the training dataset which could be inaccessible due to sensitivity of the data. In this work, we want to study such a problem: can we use out-of-domain data to calibrate the trained networks without knowledge of the original dataset? Specifically, we go beyond the domain of natural images to include drastically different domains such as X-ray images, satellite images and ultrasound images. We find cross-domain calibration leads to surprisingly stable performance of quantized models on 10 tasks in different image domains with 13 different calibration datasets. We also find that the performance of quantized models is correlated with the similarity of the Gram matrices between the source and calibration domains, which can be used as a criterion to choose calibration set for better performance. We believe our research opens the door to borrow cross-domain knowledge for network quantization and compression."}}
{"id": "IahIsM9mV7", "cdate": 1667340176136, "mdate": 1667340176136, "content": {"title": "Video Instance Segmentation", "abstract": "In this paper we present a new computer vision task,\nnamed video instance segmentation. The goal of this new\ntask is simultaneous detection, segmentation and tracking\nof instances in videos. In words, it is the first time that\nthe image instance segmentation problem is extended to\nthe video domain. To facilitate research on this new task,\nwe propose a large-scale benchmark called YouTube-VIS,\nwhich consists of 2,883 high-resolution YouTube videos, a\n40-category label set and 131k high-quality instance masks.\nIn addition, we propose a novel algorithm called Mask-\nTrack R-CNN for this task. Our new method introduces\na new tracking branch to Mask R-CNN to jointly perform\nthe detection, segmentation and tracking tasks simultaneously.\nFinally, we evaluate the proposed method and several\nstrong baselines on our new dataset. Experimental results\nclearly demonstrate the advantages of the proposed algorithm\nand reveal insight for future improvement. We believe\nthe video instance segmentation task will motivate the community\nalong the line of research for video understanding."}}
{"id": "FI5IysDR8pG", "cdate": 1663850021843, "mdate": null, "content": {"title": "Learning Dynamic Query Combinations for Transformer-based Object Detection and Segmentation", "abstract": "Transformer-based detection and segmentation methods use a list of learned detection queries to retrieve information from the transformer network and learn to predict the location and category of one specific object from each query. We empirically find that random convex combinations of the learned queries are still good queries for the corresponding models. We then propose to learn a convex combination with dynamic coefficients based on the high-level semantics of the image. The generated dynamic queries better capture the prior of object locations and categories in the different images. Equipped with our dynamic queries, a wide range of DETR-based models achieve consistent and superior performance across multiple tasks (object detection, instance segmentation, panoptic segmentation) and on different benchmarks (MS COCO, CityScapes, YoutubeVIS)."}}
{"id": "kAuP4Gl8Yt", "cdate": 1640995200000, "mdate": 1667340823365, "content": {"title": "Learning Versatile Neural Architectures by Propagating Network Codes", "abstract": "This work explores how to design a single neural network capable of adapting to multiple heterogeneous vision tasks, such as image segmentation, 3D detection, and video recognition. This goal is challenging because both network architecture search (NAS) spaces and methods in different tasks are inconsistent. We solve this challenge from both sides. We first introduce a unified design space for multiple tasks and build a multitask NAS benchmark (NAS-Bench-MR) on many widely used datasets, including ImageNet, Cityscapes, KITTI, and HMDB51. We further propose Network Coding Propagation (NCP), which back-propagates gradients of neural predictors to directly update architecture codes along the desired gradient directions to solve various tasks. In this way, optimal architecture configurations can be found by NCP in our large search space in seconds. Unlike prior arts of NAS that typically focus on a single task, NCP has several unique benefits. (1) NCP transforms architecture optimization from data-driven to architecture-driven, enabling joint search an architecture among multitasks with different data distributions. (2) NCP learns from network codes but not original data, enabling it to update the architecture efficiently across datasets. (3) In addition to our NAS-Bench-MR, NCP performs well on other NAS benchmarks, such as NAS-Bench-201. (4) Thorough studies of NCP on inter-, cross-, and intra-tasks highlight the importance of cross-task neural architecture design, i.e., multitask neural architectures and architecture transferring between different tasks. Code is available at https://github.com/dingmyu/NCP."}}
{"id": "YcOVHajN-m9", "cdate": 1640995200000, "mdate": 1667340823446, "content": {"title": "Robust High-Resolution Video Matting with Temporal Guidance", "abstract": "We introduce a robust, real-time, high-resolution human video matting method that achieves new state-of-the-art performance. Our method is much lighter than previous approaches and can process 4K at 76 FPS and HD at 104 FPS on an Nvidia GTX 1080Ti GPU. Unlike most existing methods that perform video matting frame-by-frame as independent images, our method uses a recurrent architecture to exploit temporal information in videos and achieves significant improvements in temporal coherence and matting quality. Furthermore, we propose a novel training strategy that enforces our network on both matting and segmentation objectives. This significantly improves our model\u2019s robustness. Our method does not require any auxiliary inputs such as a trimap or a pre-captured background image, so it can be widely applied to existing human matting applications. Our code is available at https://peterl1n.github.io/RobustVideoMatting/"}}
{"id": "ER5iF_5bcv", "cdate": 1640995200000, "mdate": 1667340823371, "content": {"title": "Dynamic Proposals for Efficient Object Detection", "abstract": "Object detection is a basic computer vision task to loccalize and categorize objects in a given image. Most state-of-the-art detection methods utilize a fixed number of proposals as an intermediate representation of object candidates, which is unable to adapt to different computational constraints during inference. In this paper, we propose a simple yet effective method which is adaptive to different computational resources by generating dynamic proposals for object detection. We first design a module to make a single query-based model to be able to inference with different numbers of proposals. Further, we extend it to a dynamic model to choose the number of proposals according to the input image, greatly reducing computational costs. Our method achieves significant speed-up across a wide range of detection models including two-stage and query-based models while obtaining similar or even better accuracy."}}
{"id": "KEQl-MZ5fg7", "cdate": 1632875479244, "mdate": null, "content": {"title": "Learning Versatile Neural Architectures by Propagating Network Codes", "abstract": "This work explores how to design a single neural network capable of adapting to multiple heterogeneous vision tasks, such as image segmentation, 3D detection, and video recognition. This goal is challenging because both network architecture search (NAS) spaces and methods in different tasks are inconsistent. We solve this challenge from both sides. We first introduce a unified design space for multiple tasks and build a multitask NAS benchmark (NAS-Bench-MR) on many widely used datasets, including ImageNet, Cityscapes, KITTI, and HMDB51. We further propose Network Coding Propagation (NCP), which back-propagates gradients of neural predictors to directly update architecture codes along the desired gradient directions to solve various tasks. In this way, optimal architecture configurations can be found by NCP in our large search space in seconds.\n\nUnlike prior arts of NAS that typically focus on a single task, NCP has several unique benefits. (1) NCP transforms architecture optimization from data-driven to architecture-driven, enabling joint search an architecture among multitasks with different data distributions. (2) NCP learns from network codes but not original data, enabling it to update the architecture efficiently across datasets. (3) In addition to our NAS-Bench-MR, NCP performs well on other NAS benchmarks, such as NAS-Bench-201. (4) Thorough studies of NCP on inter-, cross-, and intra-tasks highlight the importance of cross-task neural architecture design, i.e., multitask neural architectures and architecture transferring between different tasks. Code is available at https://github.com/dingmyu/NCP."}}
{"id": "y5Gd0qqdOw-", "cdate": 1609459200000, "mdate": 1667340823339, "content": {"title": "AutoSpace: Neural Architecture Search with Less Human Interference", "abstract": "Current neural architecture search (NAS) algorithms still require expert knowledge and effort to design a search space for network construction. In this paper, we consider automating the search space design to minimize human interference, which however faces two challenges: the ex-plosive complexity of the exploration space and the expensive computation cost to evaluate the quality of different search spaces. To solve them, we propose a novel differentiable evolutionary framework named AutoSpace, which evolves the search space to an optimal one with following novel techniques: a differentiable fitness scoring function to efficiently evaluate the performance of cells and a reference architecture to speedup the evolution procedure and avoid falling into sub-optimal solutions. The frame-work is generic and compatible with additional computational constraints, making it feasible to learn specialized search spaces that fit different computational bud-gets. With the learned search space, the performance of recent NAS algorithms can be improved significantly compared with using previously manually designed spaces. Remarkably, the models generated from the new search space achieve 77.8% top-1 accuracy on ImageNet under the mobile setting (MAdds 500M), outperforming previous SOTA EfficientNet-B0 by\u22640.7%. https://github.com/zhoudaquan/AutoSpace.git."}}
{"id": "rn4lWIuSFW6", "cdate": 1609459200000, "mdate": 1667340823446, "content": {"title": "HR-NAS: Searching Efficient High-Resolution Neural Architectures With Lightweight Transformers", "abstract": "High-resolution representations (HR) are essential for dense prediction tasks such as segmentation, detection, and pose estimation. Learning HR representations is typically ignored in previous Neural Architecture Search (NAS) methods that focus on image classification. This work proposes a novel NAS method, called HR-NAS, which is able to find efficient and accurate networks for different tasks, by effectively encoding multiscale contextual information while maintaining high-resolution representations. In HR-NAS, we renovate the NAS search space as well as its searching strategy. To better encode multiscale image contexts in the search space of HR-NAS, we first carefully design a lightweight transformer, whose computational complexity can be dynamically changed with respect to different objective functions and computation budgets. To maintain high-resolution representations of the learned networks, HR-NAS adopts a multi-branch architecture that provides convolutional encoding of multiple feature resolutions, inspired by HRNet. Last, we proposed an efficient fine-grained search strategy to train HR-NAS, which effectively explores the search space, and finds optimal architectures given various tasks and computation resources. HR-NAS is capable of achieving state-of-the-art trade-offs between performance and FLOPs for three dense prediction tasks and an image classification task, given only small computational budgets. For example, HR-NAS surpasses SqueezeNAS that is specially designed for semantic segmentation by a large margin of 3.61% while improving efficiency by 45.9%. Code is available at https://github.com/dingmyu/HR-NAS."}}
