{"id": "h6j82iBkWix", "cdate": 1640995200000, "mdate": 1682407237404, "content": {"title": "Multi-scale Feature Learning Dynamics: Insights for Double Descent", "abstract": "An intriguing phenomenon that arises from the high-dimensional learning dynamics of neural networks is the phenomenon of \u201cdouble descent\u201d. The more commonly studied aspect of this phenomenon corres..."}}
{"id": "JmU7lyDxTpc", "cdate": 1632875748191, "mdate": null, "content": {"title": "Multi-scale Feature Learning Dynamics: Insights for Double Descent", "abstract": "A key challenge in building theoretical foundations for deep learning is the complex optimization dynamics of neural networks, resulting from the high-dimensional interactions between the large number of network parameters. Such non-trivial interactions lead to intriguing model behaviors such as the phenomenon  of \"double descent\" of the generalization error. The more commonly studied aspect of this phenomenon corresponds to model-wise double descent where the test error exhibits a second descent with increasing model complexity, beyond the classical U-shaped error curve. In this work, we investigate the origins of the less studied epoch-wise double descent in which the test error undergoes two non-monotonous transitions, or descents as the training time increases. We study a linear teacher-student setup exhibiting epoch-wise double descent similar to that in deep neural networks. In this setting, we derive closed-form analytical expressions for the evolution of generalization error over training. We find that double descent can be attributed to distinct features being learned at different scales: as fast-learning features overfit, slower-learning features start to fit, resulting in a second descent in test error. We validate our findings through numerical experiments where our theory accurately predicts empirical findings and remains consistent with observations in deep neural networks."}}
{"id": "KYGnoFQq8d", "cdate": 1609459200000, "mdate": 1682407237868, "content": {"title": "Topics in Quantum-Hall Physics, Game-Optimization and Generalization in Neural Networks", "abstract": "Author(s): Mitra, Amartya | Advisor(s): Mulligan, Michael | Abstract: This thesis contains research conducted on various topics in quantum Hall physics and deep learning theory. The first chapter studies a particular aspect of quantum Hall systems, namely their behavior around the $\\nu = 1/2$ Landau level (LL) state. This work is motivated by the need to understand better this particular state in light of the two proposed distinct theoretical descriptions existing for the same. Specifically, we analyze quantum oscillations around the $\\nu = 1/2$ LL state using one of the propositions to support the latter. The second and third chapters study two distinct domains in deep learning, multi and single-objective models. In particular, the second considers a specific type of multi-objective model, zero-sum games, to demonstrate existing issues in training such setups and develop an efficient optimization scheme. The final chapter involves studying a particular aspect of the generalization behavior of deep neural networks (DNNs). Specifically, it attempts to provide a theoretical framework to explain the recently observed phenomenon of \"epoch-wise double descent\" in such DNNs."}}
{"id": "HU7b_zJUHg5", "cdate": 1609459200000, "mdate": 1645727504229, "content": {"title": "Multi-scale Feature Learning Dynamics: Insights for Double Descent", "abstract": "A key challenge in building theoretical foundations for deep learning is the complex optimization dynamics of neural networks, resulting from the high-dimensional interactions between the large number of network parameters. Such non-trivial dynamics lead to intriguing behaviors such as the phenomenon of \"double descent\" of the generalization error. The more commonly studied aspect of this phenomenon corresponds to model-wise double descent where the test error exhibits a second descent with increasing model complexity, beyond the classical U-shaped error curve. In this work, we investigate the origins of the less studied epoch-wise double descent in which the test error undergoes two non-monotonous transitions, or descents as the training time increases. By leveraging tools from statistical physics, we study a linear teacher-student setup exhibiting epoch-wise double descent similar to that in deep neural networks. In this setting, we derive closed-form analytical expressions for the evolution of generalization error over training. We find that double descent can be attributed to distinct features being learned at different scales: as fast-learning features overfit, slower-learning features start to fit, resulting in a second descent in test error. We validate our findings through numerical experiments where our theory accurately predicts empirical findings and remains consistent with observations in deep neural networks."}}
{"id": "ta_ZtCLZOZR", "cdate": 1577836800000, "mdate": null, "content": {"title": "LEAD: Least-Action Dynamics for Min-Max Optimization", "abstract": "Adversarial formulations such as generative adversarial networks (GANs) have rekindled interest in two-player min-max games. A central obstacle in the optimization of such games is the rotational dynamics that hinder their convergence. Existing methods typically employ intuitive, carefully hand-designed mechanisms for controlling such rotations. In this paper, we take a novel approach to address this issue by casting min-max optimization as a physical system. We leverage tools from physics to introduce LEAD (Least-Action Dynamics), a second-order optimizer for min-max games. Next, using Lyapunov stability theory and spectral analysis, we study LEAD's convergence properties in continuous and discrete-time settings for bilinear games to demonstrate linear convergence to the Nash equilibrium. Finally, we empirically evaluate our method on synthetic setups and CIFAR-10 image generation to demonstrate improvements over baseline methods."}}
