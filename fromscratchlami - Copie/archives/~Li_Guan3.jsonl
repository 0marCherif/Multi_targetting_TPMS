{"id": "d_Q7YiGpkoz", "cdate": 1668539657960, "mdate": 1668539657960, "content": {"title": "Glissando-Net: Deep sinGLe vIew category level poSe eStimation ANd 3D recOnstruction", "abstract": "We present a deep learning model, dubbed Glissando-Net, to simultaneously estimate the pose and reconstruct the 3D\nshape of objects at the category level from a single RGB image. Previous works predominantly focused on either estimating poses\n(often at the instance level), or reconstructing shapes, but not both. Glissando-Net is composed of two auto-encoders that are jointly\ntrained, one for RGB images and the other for point clouds. We embrace two key design choices in Glissando-Net to achieve a more\naccurate prediction of the 3D shape and pose of the object given a single RGB image as input. First, we augment the feature maps of\nthe point cloud encoder and decoder with transformed feature maps from the image decoder, enabling effective 2D-3D interaction in\nboth training and prediction. Second, we predict both the 3D shape and pose of the object in the decoder stage. This way, we better\nutilize the information in the 3D point clouds presented only in the training stage to train the network for more accurate prediction. We\njointly train the two encoder-decoders for RGB and point cloud data to learn how to pass latent features to the point cloud decoder\nduring inference. In testing, the encoder of the 3D point cloud is discarded. The design of Glissando-Net is inspired by codeSLAM.\nUnlike codeSLAM, which targets 3D reconstruction of scenes, we focus on pose estimation and shape reconstruction of objects, and\ndirectly predict the object pose and a pose invariant 3D reconstruction without the need of the code optimization step. Extensive\nexperiments, involving both ablation studies and comparison with competing methods, demonstrate the efficacy of our proposed\nmethod, and compare favorably with the state-of-the-art."}}
{"id": "00kPgkoahtO", "cdate": 1663850463652, "mdate": null, "content": {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "abstract": "Autoencoding has been a popular topic across many fields and recently emerged in the 3D domain. However, many 3D representations (e.g., point clouds) are discrete samples of the underlying continuous 3D surface which makes them different from other data modalities. This process inevitably introduces sampling variations on the underlying 3D shapes. In learning 3D representation, a desirable goal is to disregard such sampling variations while focusing on capturing transferable knowledge of the underlying 3D shape. This aim poses a grand challenge to existing representation learning paradigms. For example, the standard autoencoding paradigm forces the encoder to capture such sampling variations as the decoder has to reconstruct the original point cloud. In this paper, we introduce the Implicit Autoencoder (IAE). This simple yet effective method addresses this challenge by replacing the point cloud decoder with an implicit decoder. The implicit decoder can output a continuous representation that is shared among different point cloud samplings of the same model. Reconstructing under the implicit representation can prioritize that the encoder discards sampling variations, introducing appropriate inductive bias to learn more generalizable feature representations. We validate this claim experimentally and show a theoretical analysis under a simple linear autoencoder. Moreover, our implicit decoder offers excellent flexibility in designing suitable implicit representations for different tasks. We demonstrate the usefulness of IAE across various self-supervised learning tasks for both 3D objects and 3D scenes. Experimental results show that IAE consistently outperforms the state-of-the-art in each task. "}}
{"id": "x9qXVIsT2N7", "cdate": 1609459200000, "mdate": 1664351365112, "content": {"title": "Beyond Visual Attractiveness: Physically Plausible Single Image HDR Reconstruction for Spherical Panoramas", "abstract": "HDR reconstruction is an important task in computer vision with many industrial needs. The traditional approaches merge multiple exposure shots to generate HDRs that correspond to the physical quantity of illuminance of the scene. However, the tedious capturing process makes such multi-shot approaches inconvenient in practice. In contrast, recent single-shot methods predict a visually appealing HDR from a single LDR image through deep learning. But it is not clear whether the previously mentioned physical properties would still hold, without training the network to explicitly model them. In this paper, we introduce the physical illuminance constraints to our single-shot HDR reconstruction framework, with a focus on spherical panoramas. By the proposed physical regularization, our method can generate HDRs which are not only visually appealing but also physically plausible. For evaluation, we collect a large dataset of LDR and HDR images with ground truth illuminance measures. Extensive experiments show that our HDR images not only maintain high visual quality but also top all baseline methods in illuminance prediction accuracy."}}
{"id": "2SXoWg-L4h", "cdate": 1609459200000, "mdate": 1664351365102, "content": {"title": "Learning View Selection for 3D Scenes", "abstract": "Efficient 3D space sampling to represent an underlying3D object/scene is essential for 3D vision, robotics, and be-yond. A standard approach is to explicitly sample a densecollection of views and formulate it as a view selection prob-lem, or, more generally, a set cover problem. In this paper,we introduce a novel approach that avoids dense view sam-pling. The key idea is to learn a view prediction networkand a trainable aggregation module that takes the predictedviews as input and outputs an approximation of their genericscores (e.g., surface coverage, viewing angle from surfacenormals). This methodology allows us to turn the set coverproblem (or multi-view representation optimization) into acontinuous optimization problem. We then explain how toeffectively solve the induced optimization problem using con-tinuation, i.e., aggregating a hierarchy of smoothed scoringmodules. Experimental results show that our approach ar-rives at similar or better solutions with about 10 x speed upin running time, comparing with the standard methods."}}
{"id": "nRg5tWYnmCq", "cdate": 1546300800000, "mdate": 1664351365112, "content": {"title": "Pano Popups: Indoor 3D Reconstruction with a Plane-Aware Network", "abstract": "In this work we present a method to train a plane-aware convolutional neural network for dense depth and surface normal estimation as well as plane boundaries from a single indoor $360^\\circ$ image. Using our proposed loss function, our network outperforms existing methods for single-view, indoor, omnidirectional depth estimation and provides an initial benchmark for surface normal prediction from $360^\\circ$ images. Our improvements are due to the use of a novel plane-aware loss that leverages principal curvature as an indicator of planar boundaries. We also show that including geodesic coordinate maps as network priors provides a significant boost in surface normal prediction accuracy. Finally, we demonstrate how we can combine our network's outputs to generate high quality 3D \"pop-up\" models of indoor scenes."}}
{"id": "WeB0GkDAFUo", "cdate": 1546300800000, "mdate": 1664351365105, "content": {"title": "Pano Popups: Indoor 3D Reconstruction with a Plane-Aware Network", "abstract": "In this work we present a method to train a plane-aware convolutional neural network for dense depth and surface normal estimation as well as plane boundaries from a single indoor 360 image. Using our proposed loss function, our network outperforms existing methods for single-view, indoor, omnidirectional depth estimation and provides an initial benchmark for surface normal prediction from 360 images. Our improvements are due to the use of a novel plane-aware loss that leverages principal curvature as an indicator of planar boundaries. We also show that including geodesic coordinate maps as network priors provides a significant boost in surface normal prediction accuracy. Finally, we demonstrate how we can combine our network's outputs to generate high quality 3D \"pop-up\" models of indoor scenes."}}
{"id": "KSX5MsdQow", "cdate": 1356998400000, "mdate": 1664351365103, "content": {"title": "Automatic Registration of Smooth Object Image to 3D CAD Model for Industrial Inspection Applications", "abstract": "We describe an algorithmic pipeline for automatic registration of smooth object image on 3D CAD model, which has many applications in industrial inspections. This enables camera pose estimation and 3D measurement of structures of interest from 2D images. In this work, when given a query image, we leverage image retrieval to find a similar historical image, in which its 2D points' corresponding locations on the 3D model has already been computed offline. By matching features between the query and retrieved image, the latter's feature locations on the 3D model can be transferred to the former. Camera pose of the query image can then be computed for overlaying it on the 3D model. A big challenge arises when the object of interest, such as one with smooth metallic surface, has extremely limited number of reliable features, which is especially true in industrial inspection. This causes popular recognition and detection algorithms as well as 3D calibration algorithms that depend on a sufficient number of good features to fail. We describe how we construct a pipeline to overcome this challenge. Comparative results and accuracy of 3D measurements made on real datasets with the proposed pipeline validate our approach."}}
{"id": "rk-KBT-_bH", "cdate": 1325376000000, "mdate": null, "content": {"title": "Computer vision aided target linked radiation imaging", "abstract": "In this paper, we demonstrated an application of video tracking to radiation detection, where a vision-based tracking system enables a traditional CZT (cadmium zinc telluride)-based radiation imaging device to detect radioactive targets that are in motion. An integrated real-time system consisting of multiple fixed cameras and radiation detectors was implemented and tested. The multi-camera tracking system combines multiple feature cues (such as silhouette, appearance, and geometry) from different viewing angles to ensure consistent target identities under challenging tracking conditions. Experimental results show that both the video tracking and the integrated systems perform accurately and persistently under various scenarios involving multiple vehicles, driving speeds, and driving patterns. The results also validate and reiterate the importance of video tracking as an enabling technology in the field of radiation imaging."}}
{"id": "NOXmDyqkEXz", "cdate": 1325376000000, "mdate": 1664351365121, "content": {"title": "Exploring High-Level Plane Primitives for Indoor 3D Reconstruction with a Hand-held RGB-D Camera", "abstract": "Given a hand-held RGB-D camera (e.g.\u00a0Kinect), methods such as Structure from Motion (SfM) and Iterative Closest Point (ICP), perform poorly when reconstructing indoor scenes with few image features or little geometric structure information. In this paper, we propose to extract high level primitives\u2013planes\u2013from an RGB-D camera, in addition to low level image features (e.g.\u00a0SIFT), to better constrain the problem and help improve indoor 3D reconstruction. Our work has two major contributions: first, for frame to frame matching, we propose a new scheme which takes into account both low-level appearance feature correspondences in RGB image and high-level plane correspondences in depth image. Second, in the global bundle adjustment step, we formulate a novel error measurement that not only takes into account the traditional 3D point re-projection errors, but also the planar surface alignment errors. We demonstrate with real datasets that our method with plane constraints achieves more accurate and more appealing results comparing with other state-of-the-art scene reconstruction algorithms in aforementioned challenging indoor scenarios."}}
{"id": "H1Z27gf_-r", "cdate": 1325376000000, "mdate": null, "content": {"title": "Simultaneous image segmentation and 3D plane fitting for RGB-D sensors - An iterative framework", "abstract": "In this paper, we segment RGB-D sensor (e.g. Microsoft Kinect camera) images into 3D planar surfaces. We initialize a set of plane equations based solely from the depth (point cloud) information. We then iteratively refine the pixel-to-plane assignment and plane equations. During this process, the number of planes are also reduced by merging adjacent local planes with similar orientations. For the pixel-to-plane assignment, we treat the image as a Markov Random Field (MRF), and solve the association problem using graph-based global energy minimization. We design the energy terms to encapsulate both appearance cues from the RGB (color) channels and shape cues from the D (depth) channel. Experiments show that the use of both appearance and geometry information significantly improves the segmentation quality, especially so at genuine plane edges and plane intersections. As a byproduct, the framework also automatically fills in missing depth information."}}
