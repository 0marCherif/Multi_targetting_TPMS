{"id": "2OETPKmG4S0", "cdate": 1663849853709, "mdate": null, "content": {"title": "On the Lower Bound of Minimizing Polyak-\u0141ojasiewicz functions", "abstract": "Polyak-\u0141ojasiewicz (PL) [Polyak, 1963] condition is a weaker condition than the strong convexity but suffices to ensure a global convergence for the Gradient Descent algorithm. In this paper, we study the lower bound of algorithms using first-order oracles to find an approximate optimal solution. We show that any first-order algorithm requires at least $\\Omega\\left((L/\\mu)^{1-\\alpha} \\right)$ gradient costs to find an $\\epsilon$-approximate optimal solution for a general $L$-smooth function that has an $\\mu$-PL constant for any $\\alpha>0$. This result demonstrates the near optimality of the Gradient Descent algorithm to minimize smooth PL functions in the sense that there exists a ``hard'' PL function such that no first-order algorithm can be faster by a polynomial order. In contrast, it is well-known that the momentum technique, e.g. [Nesterov, 2003, chap. 2] can provably accelerate Gradient Descent to $O\\left(\\sqrt{L/\\hat{\\mu}}\\log\\frac{1}{\\epsilon}\\right)$ gradient costs for functions that are $L$-smooth and $\\hat{\\mu}$-strongly convex. Therefore, our result distinguishes the hardness of minimizing a smooth PL function and a smooth strongly convex function as the complexity of the former cannot be improved by any polynomial order in general. "}}
