{"id": "woz0lAxV7O1", "cdate": 1546300800000, "mdate": 1666857385929, "content": {"title": "Anticipating Where People will Look Using Adversarial Networks", "abstract": "We introduce a new problem of gaze anticipation on future frames which extends the conventional gaze prediction problem to go beyond current frames. To solve this problem, we propose a new generative adversarial network based model, Deep Future Gaze (DFG), encompassing two pathways: DFG-P is to anticipate gaze prior maps conditioned on the input frame which provides task influences; DFG-G is to learn to model both semantic and motion information in future frame generation. DFG-P and DFG-G are then fused to anticipate future gazes. DFG-G consists of two networks: a generator and a discriminator. The generator uses a two-stream spatial-temporal convolution architecture (3D-CNN) for explicitly untangling the foreground and background to generate future frames. It then attaches another 3D-CNN for gaze anticipation based on these synthetic frames. The discriminator plays against the generator by distinguishing the synthetic frames of the generator from the real frames. Experimental results on the publicly available egocentric and third person video datasets show that DFG significantly outperforms all competitive baselines. We also demonstrate that DFG achieves better performance of gaze prediction on current frames in egocentric and third person videos than state-of-the-art methods."}}
{"id": "SQ0-SrBxd6B", "cdate": 1546300800000, "mdate": null, "content": {"title": "Anticipating Where People will Look Using Adversarial Networks.", "abstract": "We introduce a new problem of gaze anticipation on future frames which extends the conventional gaze prediction problem to go beyond current frames. To solve this problem, we propose a new generative adversarial network based model, Deep Future Gaze (DFG), encompassing two pathways: DFG-P is to anticipate gaze prior maps conditioned on the input frame which provides task influences; DFG-G is to learn to model both semantic and motion information in future frame generation. DFG-P and DFG-G are then fused to anticipate future gazes. DFG-G consists of two networks: a generator and a discriminator. The generator uses a two-stream spatial-temporal convolution architecture (3D-CNN) for explicitly untangling the foreground and background to generate future frames. It then attaches another 3D-CNN for gaze anticipation based on these synthetic frames. The discriminator plays against the generator by distinguishing the synthetic frames of the generator from the real frames. Experimental results on the publicly available egocentric and third person video datasets show that DFG significantly outperforms all competitive baselines. We also demonstrate that DFG achieves better performance of gaze prediction on current frames in egocentric and third person videos than state-of-the-art methods."}}
{"id": "S7tvIJXg_Tr", "cdate": 1546300800000, "mdate": null, "content": {"title": "An End-To-End Network for Generating Social Relationship Graphs.", "abstract": "Socially-intelligent agents are of growing interest in artificial intelligence. To this end, we need systems that can understand social relationships in diverse social contexts. Inferring the social context in a given visual scene not only involves recognizing objects, but also demands a more in-depth understanding of the relationships and attributes of the people involved. To achieve this, one computational approach for representing human relationships and attributes is to use an explicit knowledge graph, which allows for high-level reasoning. We introduce a novel end-to-end-trainable neural network that is capable of generating a Social Relationship Graph - a structured, unified representation of social relationships and attributes - from a given input image. Our Social Relationship Graph Generation Network (SRG-GN) is the first to use memory cells like Gated Recurrent Units (GRUs) to iteratively update the social relationship states in a graph using scene and attribute context. The neural network exploits the recurrent connections among the GRUs to implement message passing between nodes and edges in the graph, and results in significant improvement over previous methods for social relationship recognition."}}
{"id": "NhFAb-vMEPL", "cdate": 1546300800000, "mdate": 1666857385847, "content": {"title": "Which Body Is Mine?", "abstract": "In the light of the human studies that report a strong correlation between head circumference and body size, we propose a new research problem: head-body matching. Given an image of a person's head, we want to match it with his body (headless) image. We propose a dual-pathway framework which computes head and body discriminating features independently, and learns the correlation between such features. We introduce a comprehensive evaluation of our proposed framework for this problem using different features including anthropometric features and deep-CNN features, different experimental setting such as head-body scale variations, and different body parts. We demonstrate the usefulness of our framework with two novel applications: head/body recognition, and T-shirt sizing from a head image. Our evaluations for head/body recognition application on the challenging large scale PIPA dataset (contains high variations of pose, viewpoint, and occlusion) show up to 53% of performance improvement using deep-CNN features, over the global model features in which head and body features are not separated or correlated. For T-shirt sizing application, we use anthropometric features for head-body matching. We achieve promising experimental results on small and challenging datasets."}}
{"id": "1Tc1mb_ghFB", "cdate": 1546300800000, "mdate": 1666857385885, "content": {"title": "An End-To-End Network for Generating Social Relationship Graphs", "abstract": "Socially-intelligent agents are of growing interest in artificial intelligence. To this end, we need systems that can understand social relationships in diverse social contexts. Inferring the social context in a given visual scene not only involves recognizing objects, but also demands a more in-depth understanding of the relationships and attributes of the people involved. To achieve this, one computational approach for representing human relationships and attributes is to use an explicit knowledge graph, which allows for high-level reasoning. We introduce a novel end-to-end-trainable neural network that is capable of generating a Social Relationship Graph - a structured, unified representation of social relationships and attributes - from a given input image. Our Social Relationship Graph Generation Network (SRG-GN) is the first to use memory cells like Gated Recurrent Units (GRUs) to iteratively update the social relationship states in a graph using scene and attribute context. The neural network exploits the recurrent connections among the GRUs to implement message passing between nodes and edges in the graph, and results in significant improvement over previous methods for social relationship recognition."}}
{"id": "SkmM6M_pW", "cdate": 1518730191274, "mdate": null, "content": {"title": "Egocentric Spatial Memory Network", "abstract": "Inspired by neurophysiological discoveries of navigation cells in the mammalian\nbrain, we introduce the first deep neural network architecture for modeling Egocentric\nSpatial Memory (ESM). It learns to estimate the pose of the agent and\nprogressively construct top-down 2D global maps from egocentric views in a spatially\nextended environment. During the exploration, our proposed ESM network\nmodel updates belief of the global map based on local observations using a recurrent\nneural network. It also augments the local mapping with a novel external\nmemory to encode and store latent representations of the visited places based on\ntheir corresponding locations in the egocentric coordinate. This enables the agents\nto perform loop closure and mapping correction. This work contributes in the\nfollowing aspects: first, our proposed ESM network provides an accurate mapping\nability which is vitally important for embodied agents to navigate to goal locations.\nIn the experiments, we demonstrate the functionalities of the ESM network in\nrandom walks in complicated 3D mazes by comparing with several competitive\nbaselines and state-of-the-art Simultaneous Localization and Mapping (SLAM)\nalgorithms. Secondly, we faithfully hypothesize the functionality and the working\nmechanism of navigation cells in the brain. Comprehensive analysis of our model\nsuggests the essential role of individual modules in our proposed architecture and\ndemonstrates efficiency of communications among these modules. We hope this\nwork would advance research in the collaboration and communications over both\nfields of computer science and computational neuroscience."}}
{"id": "sYinR8ACQOU", "cdate": 1514764800000, "mdate": 1666857385928, "content": {"title": "Egocentric Spatial Memory", "abstract": "Egocentric spatial memory (ESM) defines a memory system with encoding, storing, recognizing and recalling the spatial information about the environment from an egocentric perspective. We introduce an integrated deep neural network architecture for modeling ESM. It learns to estimate the occupancy state of the world and progressively construct top-down 2D global maps from egocentric views in a spatially extended environment. During the exploration, our proposed ESM model updates belief of the global map based on local observations using a recurrent neural network. It also augments the local mapping with a novel external memory to encode and store latent representations of the visited places over longterm exploration in large environments which enables agents to perform place recognition and hence, loop closure. Our proposed ESM network contributes in the following aspects: (1) without feature engineering, our model predicts free space based on egocentric views efficiently in an end-to-end manner; (2) different from other deep learning-based mapping system, ESMN deals with continuous actions and states which is vitally important for robotic control in real applications. In the experiments, we demonstrate its accurate and robust global mapping capacities in 3D virtual mazes and realistic indoor environments by comparing with several competitive baselines."}}
{"id": "S1OzdUP-v_B", "cdate": 1514764800000, "mdate": 1666857385870, "content": {"title": "Finding any Waldo: zero-shot invariant and efficient visual search", "abstract": "Searching for a target object in a cluttered scene constitutes a fundamental challenge in daily vision. Visual search must be selective enough to discriminate the target from distractors, invariant to changes in the appearance of the target, efficient to avoid exhaustive exploration of the image, and must generalize to locate novel target objects with zero-shot training. Previous work has focused on searching for perfect matches of a target after extensive category-specific training. Here we show for the first time that humans can efficiently and invariantly search for natural objects in complex scenes. To gain insight into the mechanisms that guide visual search, we propose a biologically inspired computational model that can locate targets without exhaustive sampling and generalize to novel objects. The model provides an approximation to the mechanisms integrating bottom-up and top-down signals during search in natural scenes."}}
{"id": "MbiT42IOckC", "cdate": 1514764800000, "mdate": 1666857385861, "content": {"title": "Egocentric Spatial Memory", "abstract": "Egocentric spatial memory (ESM) defines a memory system with encoding, storing, recognizing and recalling the spatial information about the environment from an egocentric perspective. We introduce an integrated deep neural network architecture for modeling ESM. It learns to estimate the occupancy state of the world and progressively construct top-down 2D global maps from egocentric views in a spatially extended environment. During the exploration, our proposed ESM model updates belief of the global map based on local observations using a recurrent neural network. It also augments the local mapping with a novel external memory to encode and store latent representations of the visited places over long-term exploration in large environments which enables agents to perform place recognition and hence, loop closure. Our proposed ESM network contributes in the following aspects: (1) without feature engineering, our model predicts free space based on egocentric views efficiently in an end-to-end manner; (2) different from other deep learning-based mapping system, ESMN deals with continuous actions and states which is vitally important for robotic control in real applications. In the experiments, we demonstrate its accurate and robust global mapping capacities in 3D virtual mazes and realistic indoor environments by comparing with several competitive baselines."}}
{"id": "0Ru-fVV84c", "cdate": 1514764800000, "mdate": 1666857385870, "content": {"title": "Evaluating an augmented remote assistance platform to support industrial applications", "abstract": "Remote assistance provides a communication bridge for users engaged in different locations. However, understanding how to design such systems in IoT is a challenging issue given digital representations are not the same as sharing a physical space. In this paper, we present a Remote Assistance Platform (RAP) that is designed to facilitate task guidance between an instructor and one or more remote operators. This includes the support of visual communication using annotation tools that augment information from a live video stream. Two user studies were performed to evaluate co-located and remote interaction. In the first study, dyads interacted with paper-based instructions while situated in the same location. In the second study, different dyads remotely performed the same tasks, assisted by using a smartphone or smart glass display. Overall, our findings found significant differences in communication behaviour based on the type of collaborative environment and information modality used. A short review of these results is discussed."}}
