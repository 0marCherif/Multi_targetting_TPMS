{"id": "drr08w5Bakj", "cdate": 1672531200000, "mdate": 1681510244177, "content": {"title": "Semantic Strengthening of Neuro-Symbolic Learning", "abstract": ""}}
{"id": "GPJVuyX4p_h", "cdate": 1663850450929, "mdate": null, "content": {"title": "SIMPLE: A Gradient Estimator for k-Subset Sampling", "abstract": "$k$-subset sampling is ubiquitous in machine learning, enabling regularization and interpretability through sparsity. The challenge lies in rendering $k$-subset sampling amenable to end-to-end learning. This has typically involved relaxing the reparameterized samples to allow for backpropagation, but introduces both bias and variance. In this work, we fall back to discrete $k$-subset sampling on the forward pass. This is coupled with using the gradient with respect to the exact marginals, computed efficiently, as a proxy for the true gradient. We show that our gradient estimator exhibits lower bias and variance compared to state-of-the-art estimators. Empirical results show improved performance on learning to explain and sparse models benchmarks. We provide an algorithm for computing the exact ELBO for the $k$-subset distribution, obtaining significantly lower loss compared to state-of-the-art discrete sparse VAEs. All of our algorithms are exact and efficient."}}
{"id": "Ai2HIoiU1DQ", "cdate": 1655187837378, "mdate": null, "content": {"title": " Neuro-Symbolic Entropy Regularization", "abstract": "In structured prediction, the goal is to jointly predict many output variables that together encode a structured object \u2013 a path in a graph, an entity-relation triple, or an ordering of objects. Such a large output space makes learning hard and requires vast amounts of labeled data. Different approaches leverage alternate sources of supervision. One approach \u2013 entropy regularization \u2013 posits that decision boundaries should lie in low-probability regions. It extracts supervision from unlabeled examples, but remains agnostic to the structure of the output space. Conversely, neuro-symbolic approaches exploit the knowledge that not every prediction corresponds to a valid structure in the output space. Yet, they does not further restrict the learned output distribution. This paper introduces a framework that unifies both approaches. We propose a loss, neuro-symbolic entropy regularization, that encourages the model to confidently predict a valid object. It is obtained by restricting entropy regularization to the distribution over only valid structures. This loss is efficiently computed when the output constraint is expressed as a tractable logic circuit. Moreover, it seamlessly integrates with other neuro-symbolic losses that eliminate invalid predictions. We demonstrate the efficacy of our approach on a series of semi-supervised and fully-supervised structured-prediction experiments, where we find that it leads to models whose predictions are more accurate and more likely to be valid."}}
{"id": "qJY8R7rCjDr", "cdate": 1655187653378, "mdate": null, "content": {"title": "Semantic Probabilistic Layers for Neuro-Symbolic Learning", "abstract": "We design a predictive layer for structured-output prediction (SOP) that can be plugged into any neural network guaranteeing its predictions are consistent with a set of predefined symbolic constraints. Our Semantic Probabilistic Layer (SPL) can model intricate correlations, and hard constraints, over a structured output space while being amenable to end-to-end learning via maximum likelihood. SPLs combine exact probabilistic inference with logical reasoning in a clean and modular way, learning complex distributions and restricting their support to solutions of the constraint. As such, they can faithfully, and efficiently, model complex SOP tasks beyond the reach of alternative neuro-symbolic approaches. We show SPLs outperform such competitors in terms of accuracy on challenging SOP tasks including hierarchical multi-label classification, pathfinding and preference learning, while retaining perfect constraint satisfaction."}}
{"id": "o-mxIWAY1T8", "cdate": 1652737842524, "mdate": null, "content": {"title": "Semantic Probabilistic Layers for Neuro-Symbolic Learning", "abstract": "We design a predictive layer for structured-output prediction (SOP) that can be plugged into any neural network guaranteeing its predictions are consistent with a set of predefined symbolic constraints. Our Semantic Probabilistic Layer (SPL) can model intricate correlations, and hard constraints, over a structured output space all while being amenable to end-to-end learning via maximum likelihood.\nSPLs combine exact probabilistic inference with logical reasoning in a clean and modular way, learning complex distributions and restricting their support to solutions of the constraint. As such, they can faithfully, and efficiently, model complex SOP tasks beyond the reach of alternative neuro-symbolic approaches. We empirically demonstrate that SPLs outperform these competitors in terms of accuracy on challenging SOP tasks such as hierarchical multi-label classification, pathfinding and preference learning, while retaining perfect constraint satisfaction."}}
{"id": "rlbz4_Io5g9", "cdate": 1646077547912, "mdate": null, "content": {"title": "Neuro-Symbolic Entropy Regularization", "abstract": "In structured prediction, the goal is to jointly predict many output variables that together encode a structured object -- a path in a graph, an entity-relation triple, or an ordering of objects. Such a large output space makes learning hard and requires vast amounts of labeled data. Different approaches leverage alternate sources of supervision. One approach -- entropy regularization -- posits that decision boundaries should lie in low-probability regions. It extracts supervision from unlabeled examples, but remains agnostic to the structure of the output space. Conversely, neuro-symbolic approaches exploit the knowledge that not every prediction corresponds to a \\emph{valid} structure in the output space. Yet, they do not further restrict the learned output distribution. This paper introduces a framework that unifies both approaches. We propose a loss, neuro-symbolic entropy regularization, that encourages the model to confidently predict a valid object. It is obtained by restricting entropy regularization to the distribution over only the valid structures. This loss can be computed efficiently when the output constraint is expressed as a tractable logic circuit. Moreover, it seamlessly integrates with other neuro-symbolic losses that eliminate invalid predictions. We demonstrate the efficacy of our approach on a series of semi-supervised and fully-supervised structured-prediction experiments, where it leads to models whose predictions are more accurate as well as more likely to be valid."}}
{"id": "ptiU3Q47Pd", "cdate": 1640995200000, "mdate": 1681510244733, "content": {"title": "Neuro-symbolic entropy regularization", "abstract": ""}}
{"id": "nx49SXuy5l", "cdate": 1640995200000, "mdate": 1681510243769, "content": {"title": "Neuro-Symbolic Entropy Regularization", "abstract": ""}}
{"id": "caIu4TiRwq", "cdate": 1640995200000, "mdate": 1681510244912, "content": {"title": "PYLON: A PyTorch Framework for Learning with Constraints", "abstract": ""}}
{"id": "O9Ulec_Hwke", "cdate": 1640995200000, "mdate": 1681510245172, "content": {"title": "SIMPLE: A Gradient Estimator for k-Subset Sampling", "abstract": ""}}
