{"id": "SUqrM7WR7W5", "cdate": 1646678298083, "mdate": null, "content": {"title": "Emergent Communication Fine-tuning (EC-FT) for Pretrained Language Models", "abstract": "It has recently been argued that the currently dominant paradigm in NLP of pretraining on text-only corpora will not yield robust natural language understanding systems.  One strain of this argumentation highlights the need for grounded, goal-oriented, and interactive language learning.  In this position paper, we articulate how Emergent Communication (EC) can be used in conjunction with large pretrained language models as a `Fine-Tuning' (FT) step (hence, EC-FT) in order to provide them with supervision from such learning scenarios.  We discuss methodological issues and difficulties with making this work, and then illustrate the overall idea with a case study in unsupervised machine translation, before concluding with a discussion on the relation to multimodal pretraining."}}
{"id": "DyCqKVSyhmb", "cdate": 1640995200000, "mdate": 1682279433241, "content": {"title": "Learning to translate by learning to communicate", "abstract": "We formulate and test a technique to use Emergent Communication (EC) with a pretrained multilingual model to improve on modern Unsupervised NMT systems, especially for low-resource languages. It has been argued that the currently dominant paradigm in NLP of pretraining on text-only corpora will not yield robust natural language understanding systems, and the need for grounded, goal-oriented, and interactive language learning has been highlighted. In our approach, we embed a modern multilingual model (mBART, Liu et. al. 2020) into an EC image-reference game, in which the model is incentivized to use multilingual generations to accomplish a vision-grounded task, with the hypothesis that this will align multiple languages to a shared task space. We present two variants of EC Fine-Tuning (Steinert-Threlkeld et. al. 2022), one of which outperforms a backtranslation-based baseline in 6/8 translation settings, and proves especially beneficial for the very low-resource languages of Nepali and Sinhala."}}
{"id": "CyKHTn8nn9t", "cdate": 1640995200000, "mdate": 1682279433236, "content": {"title": "Extracting and Inferring Personal Attributes from Dialogue", "abstract": ""}}
{"id": "3ap4f9nYyC", "cdate": 1640995200000, "mdate": 1680542917590, "content": {"title": "Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection", "abstract": ""}}
{"id": "MZjidkc9Tvn", "cdate": 1609459200000, "mdate": null, "content": {"title": "Challenges in Automated Debiasing for Toxic Language Detection", "abstract": "Biased associations have been a challenge in the development of classifiers for detecting toxic language, hindering both fairness and accuracy. As potential solutions, we investigate recently introduced debiasing methods for text classification datasets and models, as applied to toxic language detection. Our focus is on lexical (e.g., swear words, slurs, identity mentions) and dialectal markers (specifically African American English). Our comprehensive experiments establish that existing methods are limited in their ability to prevent biased behavior in current toxicity detectors. We then propose an automatic, dialect-aware data correction method, as a proof-of-concept. Despite the use of synthetic labels, this method reduces dialectal associations with toxicity. Overall, our findings show that debiasing a model trained on biased toxic language data is not as effective as simply relabeling the data to remove existing biases."}}
{"id": "ZvsFbA8S0s-", "cdate": 1577836800000, "mdate": 1637091636954, "content": {"title": "Multilevel Text Alignment with Cross-Document Attention", "abstract": "Xuhui Zhou, Nikolaos Pappas, Noah A. Smith. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020."}}
{"id": "Hk1lk_sg4pX", "cdate": 1577836800000, "mdate": 1637033650959, "content": {"title": "Evaluating Commonsense in Pre-Trained Language Models", "abstract": "Contextualized representations trained over large raw text data have given remarkable improvements for NLP tasks including question answering and reading comprehension. There have been works showing that syntactic, semantic and word sense knowledge are contained in such representations, which explains why they benefit such tasks. However, relatively little work has been done investigating commonsense knowledge contained in contextualized representations, which is crucial for human question answering and reading comprehension. We study the commonsense ability of GPT, BERT, XLNet, and RoBERTa by testing them on seven challenging benchmarks, finding that language modeling and its variants are effective objectives for promoting models' commonsense ability while bi-directional context and larger training set are bonuses. We additionally find that current models do poorly on tasks require more necessary inference steps. Finally, we test the robustness of models by making dual test cases, which are correlated so that the correct prediction of one sample should lead to correct prediction of the other. Interestingly, the models show confusion on these test cases, which suggests that they learn commonsense at the surface rather than the deep level. We release a test set, named CATs publicly, for future research."}}
{"id": "E0JhxizxbUp", "cdate": 1577836800000, "mdate": null, "content": {"title": "RPD: A Distance Function Between Word Embeddings", "abstract": "It is well-understood that different algorithms, training processes, and corpora produce different word embeddings. However, less is known about the relation between different embedding spaces, i.e. how far different sets of em-beddings deviate from each other. In this paper, we propose a novel metric called Relative Pairwise Inner Product Distance (RPD) to quantify the distance between different sets of word embeddings. This unitary-invariant metric has a unified scale for comparing different sets of word embeddings. Based on the properties of RPD, we study the relations of word embeddings of different algorithms systematically and investigate the influence of different training processes and corpora. The results shed light on the poorly understood word embeddings and justify RPD as a measure of the distance of embedding space."}}
{"id": "8VKAqTMOVmj", "cdate": 1577836800000, "mdate": 1637091637065, "content": {"title": "Linguistically-Informed Transformations (LIT): A Method for Automatically Generating Contrast Sets", "abstract": "Chuanrong Li, Lin Shengshuo, Zeyu Liu, Xinyi Wu, Xuhui Zhou, Shane Steinert-Threlkeld. Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. 2020."}}
