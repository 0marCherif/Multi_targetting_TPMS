{"id": "JyYrptqOD_", "cdate": 1696365865398, "mdate": 1696365865398, "content": {"title": "Large Language Models with Controllable Working Memory", "abstract": "Large language models (LLMs) have led to a series of breakthroughs in natural language processing (NLP), owing to their excellent understanding and generation abilities. Remarkably, what further sets these models apart is the massive amounts of world knowledge they internalize during pretraining. While many downstream applications provide the model with an informational context to aid its performance on the underlying task, how the model's world knowledge interacts with the factual information presented in the context remains under explored. As a desirable behavior, an LLM should give precedence to the context whenever it contains task-relevant information that conflicts with the model's memorized knowledge. This enables model predictions to be grounded in the context, which can then be used to update or correct specific model predictions without frequent retraining. By contrast, when the context is irrelevant to the task, the model should ignore it and fall back on its internal knowledge. In this paper, we undertake a first joint study of the aforementioned two properties, namely controllability and robustness, in the context of LLMs. We demonstrate that state-of-the-art T5 and PaLM (both pretrained and finetuned) could exhibit poor controllability and robustness, which do not scale with increasing model size. As a solution, we propose a novel method - Knowledge Aware FineTuning (KAFT) - to strengthen both controllability and robustness by incorporating counterfactual and irrelevant contexts to standard supervised datasets. Our comprehensive evaluation showcases the utility of KAFT across model architectures and sizes."}}
{"id": "o_sYD-1T2ZZ", "cdate": 1676827101524, "mdate": null, "content": {"title": "Robust Distillation for Worst-class Performance: On the Interplay Between Teacher and Student Objectives", "abstract": "Knowledge distillation is a popular technique that has been shown to produce remarkable gains in average accuracy. However, recent work has shown that these gains are not uniform across subgroups in the data, and can often come at the cost of accuracy on rare subgroups and classes. Robust optimization is a common remedy to improve worst-class accuracy in standard learning settings, but in distillation it is unknown whether it is best to apply robust objectives when training the teacher, the student, or both. This work studies the interplay between robust objectives for the teacher and student. Empirically, we show that that jointly modifying the teacher and student objectives can lead to better worst-class student performance and even Pareto improvement in the tradeoff between worst-class and overall performance. Theoretically, we show that the *per-class calibration* of teacher scores is key when training a robust student. Both the theory and experiments support the surprising finding that applying a robust teacher training objective does not always yield a more robust student."}}
{"id": "4lZZjsC5UG", "cdate": 1674767276505, "mdate": 1674767276505, "content": {"title": "Robust Distillation for Wost-class Performance", "abstract": "Knowledge distillation has proven to be an effective technique in improving the\nperformance a student model using predictions from a teacher model. However,\nrecent work has shown that gains in average efficiency are not uniform across\nsubgroups in the data, and in particular can often come at the cost of accuracy\non rare subgroups and classes. To preserve strong performance across classes\nthat may follow a long-tailed distribution, we develop distillation techniques that\nare tailored to improve the student\u2019s worst-class performance. Specifically, we\nintroduce robust optimization objectives in different combinations for the teacher\nand student, and further allow for training with any tradeoff between the overall\naccuracy and the robust worst-class objective. We show empirically that our robust\ndistillation techniques not only achieve better worst-class performance, but also\nlead to Pareto improvement in the tradeoff between overall performance and worst-class \nperformance compared to other baseline methods. Theoretically, we provide\ninsights into what makes a good teacher when the goal is to train a robust student."}}
{"id": "QcA9iGaLpH4", "cdate": 1663850481146, "mdate": null, "content": {"title": "What do large networks memorize?", "abstract": "The success of modern neural models has prompted renewed study of the connection between memorisation and generalisation: such models typically generalise well, despite being able to perfectly fit (\"memorise\") completely random labels.\nTo more carefully study this issue, Feldman (2019); Feldman & Zhang (2020) provided a simple metric to quantify the degree of memorisation of a specific training example, and empirically quantified the corresponding memorisation profile of a ResNet model on image classification benchmarks.\nWhile an exciting first glimpse into how real-world models memorise, these studies leave open several questions about memorisation of practical networks.\nIn particular, how is memorisation affected by increasing model size, and by distilling a large model into a smaller one?\nWe present a systematic empirical analysis of these questions.\nOn standard image classification benchmarks, we find that training examples exhibit a diverse set of memorisation trajectories across model sizes, with some samples having increased memorisation under larger models.\nFurther, we find that distillation tends to inhibit memorisation of the student model, while also improving generalisation.\nFinally, we show that computationally tractable measures of memorisation do not capture the properties we identify for memorisation in the sense of Feldman (2019), despite highly correlating to the latter. "}}
{"id": "sVV0KK3COzD", "cdate": 1663850440636, "mdate": null, "content": {"title": "Preserving In-Context Learning Ability in Large Language Model Fine-tuning", "abstract": "Pretrained large language models (LLMs) are strong in-context learners that are able to perform few-shot learning without changing model parameters. However, as we show, fine-tuning an LLM on any specific task generally destroys its in-context ability. We discover an important cause of this loss, format specialization, where the model overfits to the format of the fine-tuned task and is unable to output anything beyond this format. We further show that format specialization happens at the beginning of fine-tuning. To solve this problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet effective two-stage fine-tuning framework that preserves in-context abilities of the pretrained model substantially better than vanilla fine-tuning. ProMoT first trains a soft prompt for the fine-tuning target task, and then fine-tunes the model itself with this soft prompt attached. ProMoT offloads task-specific formats into the soft prompt that can be easily removed when doing other in-context tasks. We fine-tune mT5 XXL with ProMoT on natural language inference (NLI) and English-French translation and evaluate the in-context abilities of the resulting models on 8 different NLP datasets including classification, summarization, translation and question answering. ProMoT achieves similar performance on the fine-tuned tasks compared with vanilla fine-tuning, but with much less reduction of in-context learning performances across the board. More importantly, ProMoT shows remarkable generalization ability on tasks that have different formats, e.g. fine-tuning on a NLI binary classification task improves the model's in-context ability to do summarization (+0.53 Rouge-2 score compared to the pretrained model), making ProMoT a promising method to build general purpose capabilities such as grounding and reasoning into LLMs with small but high quality datasets."}}
{"id": "WDBo7y8lcJm", "cdate": 1632875737755, "mdate": null, "content": {"title": "Teacher's pet: understanding and mitigating biases in distillation", "abstract": "Knowledge distillation is widely used as a means of improving the performance of a relatively simple \u201cstudent\u201d model using the predictions from a complex \u201cteacher\u201d model.  Several works have shown that distillation significantly boosts the student\u2019s overall performance; however, are these gains uniform across all data sub-groups? In this paper, we show that distillation can harm performance on certain subgroups, e.g., classes with few associated samples, compared to the vanilla student trained using the one-hot labels. We trace this behavior to errors made by the teacher distribution being transferred to and amplified by the student model. To mitigate this problem, we present techniques which soften the teacher influence for subgroups where it is less reliable. Experiments on several image classification benchmarks show that these modifications of distillation maintain boost in overall accuracy, while additionally ensuring improvement in subgroup performance."}}
{"id": "2yITmG7YIFT", "cdate": 1632875735469, "mdate": null, "content": {"title": "HD-cos Networks: Efficient Neural Architechtures for Secure Multi-Party Computation", "abstract": "Multi-party computation (MPC) is a branch of cryptography where multiple non-colluding  parties execute a well designed protocol to securely compute a function. With the non-colluding party assumption, MPC has a cryptographic guarantee that the parties will not learn sensitive information from the computation process, making it an appealing framework for applications that involve privacy-sensitive user data.\nIn this paper, we study  training and inference of neural networks under the MPC setup. This is challenging because the elementary operations of neural networks such as the ReLU activation function and matrix-vector multiplications are very expensive to compute due to the added multi-party communication overhead. \nTo address this, we propose the HD-cos network that uses 1) cosine as activation function, 2) the Hadamard-Diagonal transformation to replace the unstructured linear transformations. We show that both of the approaches enjoy strong theoretical motivations and efficient computation under the MPC setup. We demonstrate on multiple public datasets that HD-cos matches the quality of the more expensive baselines. "}}
{"id": "V37YFd_fFgN", "cdate": 1632875549147, "mdate": null, "content": {"title": "Leveraging Redundancy in Attention with Reuse Transformers", "abstract": "Pairwise dot product-based attention allows Transformers to exchange information between tokens in an input-dependent way, and is key to their success across diverse applications in language and vision. However, a typical Transformer model computes such pairwise attention scores repeatedly for the same sequence, in multiple heads in multiple layers. We systematically analyze the empirical similarity of these scores across heads and layers and find them to be considerably redundant, especially adjacent layers showing high similarity. Motivated by these findings, we propose a novel architecture that reuses attention scores computed in one layer in multiple subsequent layers. Experiments on a number of standard benchmarks show that reusing attention delivers performance equivalent to or better than standard transformers, while reducing both compute and memory usage."}}
{"id": "ryNpmMGu-S", "cdate": 1514764800000, "mdate": null, "content": {"title": "Content Explorer: Recommending Novel Entities for a Document Writer", "abstract": ""}}
{"id": "rJbvCnx_Wr", "cdate": 1451606400000, "mdate": null, "content": {"title": "Hawkes Processes for Continuous Time Sequence Classification: an Application to Rumour Stance Classification in Twitter", "abstract": "Classification of temporal textual data sequences is a common task in various domains such as social media and the Web. In this paper we propose to use Hawkes Processes for classifying sequences of temporal textual data, which exploit both temporal and textual information. Our experiments on rumour stance classification on four Twitter datasets show the importance of using the temporal information of tweets along with the textual content."}}
