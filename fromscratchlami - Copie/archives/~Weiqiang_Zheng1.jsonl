{"id": "yKv8Qqr8RYK", "cdate": 1664731445289, "mdate": null, "content": {"title": "Accelerated Single-Call Methods for Constrained Min-Max Optimization", "abstract": "We study first-order methods for constrained min-max optimization. Existing methods either requires two gradient calls or two projections in each iteration, which may be costly in applications. In this paper, we first show that the \\emph{Optimistic Gradient (OG)} method, a \\emph{single-call single-projection} algorithm,  has $O(\\frac{1}{\\sqrt{T}})$ convergence rate for inclusion problems with operators that satisfy the weak Minty variation inequality (MVI). Our second result is the first single-call single-projection algorithm -- the \\emph{Accelerated Reflected Gradient (ARG)} method that achieves the \\emph{optimal $O(\\frac{1}{T})$} convergence rate for inclusion problems that satisfy negative comonotonicity. Both the weak MVI and negative comonotonicity are well-studied assumptions and capture a rich set of non-convex non-concave min-max optimization problems. Finally, we show that the \\emph{Reflected Gradient (RG)} method, another \\emph{single-call single-projection} algorithm, has $O(\\frac{1}{\\sqrt{T}})$ last-iterate convergence rate for constrained convex-concave min-max optimization, answering an open problem of (Hsieh et al., 2019). "}}
{"id": "SfjzfqUAsiw", "cdate": 1664731445174, "mdate": null, "content": {"title": "Accelerated Algorithms for Monotone Inclusion and Constrained Nonconvex-Nonconcave Min-Max Optimization", "abstract": "We study monotone inclusions and monotone variational inequalities, as well as their generalizations to non-monotone settings. We first show that the \\emph{Extra Anchored Gradient (EAG)} algorithm, originally proposed by [Yoon and Ryu, 2021] for unconstrained convex-concave min-max optimization, can be applied to solve the more general problem of Lipschitz monotone inclusion. More specifically, we prove that the EAG solves Lipschitz monotone inclusion problems with an \\emph{accelerated convergence rate} of $O(\\frac{1}{T})$, which is \\emph{optimal among all first-order methods} [Diakonikolas, 2020, Yoon and Ryu, 2021]. Our second result is an {accelerated forward-backward splitting algorithm (AS),} which not only achieves the accelerated $O(\\frac{1}{T})$ convergence rate for all monotone inclusion problems, but also exhibits the same accelerated rate for a family of general (non-monotone) inclusion problems that concern negative comonotone operators. As a special case of our second result, AS enjoys the $O(\\frac{1}{T})$ convergence rate for solving a non-trivial class of nonconvex-nonconcave min-max optimization problems. Our analyses are based on simple potential function arguments, which might be useful for analysing other accelerated algorithms."}}
{"id": "HRwN7IQLUKA", "cdate": 1663850380309, "mdate": null, "content": {"title": "Accelerated Single-Call Methods for Constrained Min-Max Optimization", "abstract": "We study first-order methods for constrained min-max optimization. Existing methods either require two gradient calls or two projections in each iteration, which may be costly in some applications. In this paper, we first show that a variant of the \\emph{Optimistic Gradient (OG)} method, a \\emph{single-call single-projection} algorithm,  has $O(\\frac{1}{\\sqrt{T}})$ best-iterate convergence rate for inclusion problems with operators that satisfy the weak Minty variation inequality (MVI). Our second result is the first single-call single-projection algorithm -- the \\emph{Accelerated Reflected Gradient (ARG)} method that achieves the \\emph{optimal $O(\\frac{1}{T})$} last-iterate convergence rate for inclusion problems that satisfy negative comonotonicity. Both the weak MVI and negative comonotonicity are well-studied assumptions and capture a rich set of non-convex non-concave min-max optimization problems. Finally, we show that the \\emph{Reflected Gradient (RG)} method, another \\emph{single-call single-projection} algorithm,  has $O(\\frac{1}{\\sqrt{T}})$ last-iterate convergence rate for constrained convex-concave min-max optimization, answering an open problem of [Hsieh et al., 2019]. Our convergence rates hold for standard measures such as the tangent residual and the natural residual. "}}
{"id": "snUOkDdJypm", "cdate": 1652737840991, "mdate": null, "content": {"title": "Finite-Time Last-Iterate Convergence for Learning in Multi-Player Games", "abstract": "We study the question of last-iterate convergence rate of the extragradient algorithm by Korpelevich [1976] and the optimistic gradient algorithm by Popov [1980] in multi-player games. We show that both algorithms with constant step-size have last-iterate convergence rate of $O(\\frac{1}{\\sqrt{T}})$ to a Nash equilibrium in terms of the gap function in smooth monotone games, where each player's action set is an arbitrary convex set. Previous results only study the unconstrained setting, where each player's action set is the entire Euclidean space.  Our results address an open question raised in several recent work by Hsieh et al. [2019], Golowich et al. [2020a,b], who ask for last-iterate convergence rate of either the extragradient or the optimistic gradient algorithm in the constrained setting. Our convergence rates for both algorithms are tight and match the lower bounds by Golowich et al. [2020a,b]. At the core of our results lies a new notion -- the tangent residual, which we use to measure the proximity to equilibrium. We use the tangent residual (or a slight variation of the tangent residual) as the the potential function in our analysis of the extragradient algorithm (or the optimistic gradient algorithm) and prove that it is non-increasing between two consecutive iterates."}}
{"id": "qfWs6CKXl9S", "cdate": 1640995200000, "mdate": 1678675713269, "content": {"title": "Beyond the Worst Case: Semi-random Complexity Analysis of Winner Determination", "abstract": ""}}
{"id": "KS8P55pv29t", "cdate": 1640995200000, "mdate": 1678675713342, "content": {"title": "Nash Convergence of Mean-Based Learning Algorithms in First Price Auctions", "abstract": ""}}
{"id": "TQ78v9gmYrT", "cdate": 1609459200000, "mdate": 1678675713329, "content": {"title": "The Smoothed Complexity of Computing Kemeny and Slater Rankings", "abstract": ""}}
