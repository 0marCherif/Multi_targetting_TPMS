{"id": "DClS-1HQ_0P", "cdate": 1663850113786, "mdate": null, "content": {"title": "Jointist: Simultaneous Improvement of Multi-instrument Transcription and Music Source Separation via Joint Training", "abstract": "In this paper, we introduce Jointist, an instrument-aware multi-instrument framework that is capable of transcribing, recognizing, and separating multiple musical instruments from an audio clip.\nJointist consists of an instrument recognition module that conditions the other two modules: a transcription module that outputs instrument-specific piano rolls, and a source separation module that utilizes instrument information and transcription results. The joint training of the transcription and source separation modules serves to improve the performance of both tasks. The instrument module is optional and can be directly controlled by human users. This makes Jointist a flexible user-controllable framework.\n\nOur challenging problem formulation makes the model highly useful in the real world given that modern popular music typically consists of multiple instruments. Its novelty, however, necessitates a new perspective on how to evaluate such a model. In our experiments, we assess the proposed model from various aspects, providing a new evaluation perspective for multi-instrument transcription. Subjective listening test shows that Jointist achieves state-of-the-art performance on popular music, outperforming existing multi-instrument transcription models such as MT3. %We also argue that transcription models can be used as a preprocessing module for other music analysis tasks. We conducted experiments on several downstream tasks, and found that the proposed method improved transcription by more than 1 percentage points (ppt.); source separation by 5 SDR, downbeat detection by 1.8 ppt., chord recognition by 1.4 ppt., and key estimation by 1.4 ppt., when utilizing transcription results obtained from Jointist.  "}}
{"id": "J_5Oc-rr28J", "cdate": 1577836800000, "mdate": null, "content": {"title": "Deep Composer Classification Using Symbolic Representation", "abstract": "In this study, we train deep neural networks to classify composer on a symbolic domain. The model takes a two-channel two-dimensional input, i.e., onset and note activations of time-pitch representation, which is converted from MIDI recordings and performs a single-label classification. On the experiments conducted on MAESTRO dataset, we report an F1 value of 0.8333 for the classification of 13~classical composers."}}
{"id": "yhbjGtqZ8pB", "cdate": 1546300800000, "mdate": null, "content": {"title": "Deep Learning for Audio-Based Music Classification and Tagging: Teaching Computers to Distinguish Rock from Bach", "abstract": "Over the last decade, music-streaming services have grown dramatically. Pandora, one company in the field, has pioneered and popularized streaming music by successfully deploying the Music Genome Project [1] (https://www.pandora.com/about/mgp) based on human-annotated content analysis. Another company, Spotify, has a catalog of over 40 million songs and over 180 million users as of mid-2018 (https://press.spotify.com/us/about/), making it a leading music service provider worldwide. Giant technology companies such as Apple, Google, and Amazon have also been strengthening their music service platforms. Furthermore, artificial intelligence speakers, such as Amazon Echo, are gaining popularity, providing listeners with a new and easily accessible way to listen to music."}}
{"id": "sH5aRI2wCgm", "cdate": 1546300800000, "mdate": null, "content": {"title": "mirdata: Software for Reproducible Usage of Datasets", "abstract": ""}}
{"id": "dv8dS-f34x", "cdate": 1546300800000, "mdate": null, "content": {"title": "Deep Unsupervised Drum Transcription", "abstract": ""}}
{"id": "YUH9bo5uvjd", "cdate": 1514764800000, "mdate": null, "content": {"title": "Revisiting Singing Voice Detection: A quantitative review and the future outlook", "abstract": ""}}
{"id": "TkXOxyupV9x", "cdate": 1514764800000, "mdate": null, "content": {"title": "The Effects of Noisy Labels on Deep Convolutional Neural Networks for Music Tagging", "abstract": "Deep neural networks (DNNs) have been successfully applied to music classification including music tagging. However, there are several open questions regarding the training, evaluation, and analysis of DNNs. In this paper, we investigate specific aspects of neural networks, the effects of noisy labels, to deepen our understanding of their properties. We analyze and (re-)validate a large music tagging dataset to investigate the reliability of training and evaluation. Using a trained network, we compute label vector similarities, which are compared to groundtruth similarity. The results highlight several important aspects of music tagging and neural networks. We show that networks can be effective despite relatively large error rates in groundtruth datasets, while conjecturing that label noise can be the cause of varying tag-wise performance differences. Finally, the analysis of our trained network provides valuable insight into the relationships between music tags. These results highlight the benefit of using data-driven methods to address automatic music tagging."}}
{"id": "Mz0Ot8CSV3d", "cdate": 1514764800000, "mdate": null, "content": {"title": "A Comparison of Audio Signal Preprocessing Methods for Deep Neural Networks on Music Tagging", "abstract": "In this paper, we empirically investigate the effect of audio preprocessing on music tagging with deep neural networks. We perform comprehensive experiments involving audio preprocessing using different time-frequency representations, logarithmic magnitude compression, frequency weighting, and scaling. We show that many commonly used input preprocessing techniques are redundant except magnitude compression."}}
{"id": "xHcUxbewrCw", "cdate": 1483228800000, "mdate": null, "content": {"title": "Kapre: On-GPU Audio Preprocessing Layers for a Quick Implementation of Deep Neural Network Models with Keras", "abstract": "We introduce Kapre, Keras layers for audio and music signal preprocessing. Music research using deep neural networks requires a heavy and tedious preprocessing stage, for which audio processing parameters are often ignored in parameter optimisation. To solve this problem, Kapre implements time-frequency conversions, normalisation, and data augmentation as Keras layers. We report simple benchmark results, showing real-time on-GPU preprocessing adds a reasonable amount of computation."}}
{"id": "HSWThdLFCW1", "cdate": 1483228800000, "mdate": null, "content": {"title": "Transfer Learning for Music Classification and Regression Tasks", "abstract": ""}}
