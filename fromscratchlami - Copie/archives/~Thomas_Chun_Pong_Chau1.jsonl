{"id": "nSj-BqjYIPj", "cdate": 1672531200000, "mdate": 1695949108124, "content": {"title": "Zero-Cost Operation Scoring in Differentiable Architecture Search", "abstract": "We formalize and analyze a fundamental component of dif- ferentiable neural architecture search (NAS): local \u201copera- tion scoring\u201d at each operation choice. We view existing operation scoring functions as inexact proxies for accuracy, and we find that they perform poorly when analyzed empir- ically on NAS benchmarks. From this perspective, we intro- duce a novel perturbation-based zero-cost operation scor- ing (Zero-Cost-PT) approach, which utilizes zero-cost prox- ies that were recently studied in multi-trial NAS but de- grade significantly on larger search spaces, typical for dif- ferentiable NAS. We conduct a thorough empirical evalu- ation on a number of NAS benchmarks and large search spaces, from NAS-Bench-201, NAS-Bench-1Shot1, NAS- Bench-Macro, to DARTS-like and MobileNet-like spaces, showing significant improvements in both search time and accuracy. On the ImageNet classification task on the DARTS search space, our approach improved accuracy compared to the best current training-free methods (TE-NAS) while be- ing over 10\u00d7 faster (total searching time 25 minutes on a single GPU), and observed significantly better transferabil- ity on architectures searched on the CIFAR-10 dataset with an accuracy increase of 1.8 pp. Our code is available at: https://github.com/zerocostptnas/zerocost operation score."}}
{"id": "IIbJ9m5G73t", "cdate": 1654506763060, "mdate": null, "content": {"title": "BLOX: Macro Neural Architecture Search Benchmark and Algorithms", "abstract": "Neural architecture search (NAS) has been successfully used to design numerous high-performance neural networks. However, NAS is typically compute-intensive, so most existing approaches restrict the search to decide the operations and topological structure of a single block only, then the same block is stacked repeatedly to form an end-to-end model. Although such an approach reduces the size of search space, recent studies show that a macro search space, which allows blocks in a model to be different, can lead to better performance. To provide a systematic study of the performance of NAS algorithms on a macro search space, we release Blox \u2013 a benchmark that consists of 91k unique models trained on the CIFAR-100 dataset. The dataset also includes runtime measurements of all the models on a diverse set of hardware platforms. We perform extensive experiments to compare existing algorithms that are well studied on cell-based search spaces, with the emerging blockwise approaches that aim to make NAS scalable to much larger macro search spaces. The Blox benchmark and code are available at https://github.com/SamsungLabs/blox."}}
{"id": "Ew6FDnjo_HE", "cdate": 1640995200000, "mdate": 1681650332414, "content": {"title": "Adaptable Butterfly Accelerator for Attention-based NNs via Hardware and Algorithm Co-design", "abstract": ""}}
{"id": "BqG0ypjAEe", "cdate": 1640995200000, "mdate": 1681650332621, "content": {"title": "BLOX: Macro Neural Architecture Search Benchmark and Algorithms", "abstract": ""}}
{"id": "1c-5I0ak_CS", "cdate": 1640995200000, "mdate": 1681650332424, "content": {"title": "Adaptable Butterfly Accelerator for Attention-based NNs via Hardware and Algorithm Co-design", "abstract": ""}}
{"id": "8QE3pwEVc8P", "cdate": 1632875655315, "mdate": null, "content": {"title": "Zero-Cost Operation Scoring in Differentiable Architecture Search", "abstract": "Differentiable neural architecture search (NAS) has attracted significant attention in recent years due to its ability to quickly discover promising architectures of deep neural networks even in very large search spaces. Despite its success, many differentiable NAS methods lack robustness and may degenerate to trivial architectures with excessive parameter-free operations such as skip connections thus leading to inferior performance. In fact, selecting operations based on the magnitude of architectural parameters was recently proven to be fundamentally wrong, showcasing the need to rethink how operation scoring and selection occurs in differentiable NAS. To this end, we formalize and analyze a fundamental component of differentiable NAS: local \"operation scoring\" that occurs at each choice of operation.\nWhen comparing existing operation scoring functions, we find that existing methods can be viewed as inexact proxies for accuracy.\nWe also find that existing methods perform poorly when analyzed empirically on NAS benchmarks. From this perspective, we introduce new training-free proxies to the context of differentiable NAS, and show that we can significantly speed up the search process while improving accuracy on multiple search spaces. We take inspiration from zero-cost proxies that were recently studied in the context of sample-based NAS but shown to degrade significantly for larger search spaces like DARTS. Our novel \"perturbation-based zero-cost operation scoring\" (Zero-Cost-PT) improves searching time and accuracy compared to the best available differentiable architecture search for many search space sizes, including very large ones. Specifically, we are able improve accuracy compared to the best current method (DARTS-PT) on the DARTS CNN search space while being over 40x faster (total searching time 25 minutes on a single GPU). Our code is available at: https://github.com/avail-upon-acceptance."}}
{"id": "vXwz80IHtcR", "cdate": 1609459200000, "mdate": 1681650332348, "content": {"title": "Zero-Cost Proxies Meet Differentiable Architecture Search", "abstract": ""}}
{"id": "Ah0PAPpZZ0", "cdate": 1609459200000, "mdate": 1681650332628, "content": {"title": "NAS-Bench-ASR: Reproducible Neural Architecture Search for Speech Recognition", "abstract": ""}}
{"id": "CU0APx9LMaL", "cdate": 1601308391415, "mdate": null, "content": {"title": "NAS-Bench-ASR: Reproducible Neural Architecture Search for Speech Recognition", "abstract": "Powered by innovations in novel architecture design, noise tolerance techniques and increasing model capacity, Automatic Speech Recognition (ASR) has made giant strides in reducing word-error-rate over the past decade. ASR models are often trained with tens of thousand hours of high quality speech data to produce state-of-the-art (SOTA) results. Industry-scale ASR model training thus remains computationally heavy and time-consuming, and consequently has attracted little attention in adopting automatic techniques. On the other hand, Neural Architecture Search (NAS) has gained a lot of interest in the recent years thanks to its successes in discovering efficient architectures, often outperforming handcrafted alternatives. However, by changing the standard training process into a bi-level optimisation problem, NAS approaches often require significantly more time and computational power compared to single-model training, and at the same time increase complexity of the overall process. As a result, NAS has been predominately applied to problems which do not require as extensive training as ASR, and even then reproducibility of NAS algorithms is often problematic. Lately, a number of benchmark datasets has been introduced to address reproducibility issues by pro- viding NAS researchers with information about performance of different models obtained through exhaustive evaluation. However, these datasets focus mainly on computer vision and NLP tasks and thus suffer from limited coverage of application domains. In order to increase diversity in the existing NAS benchmarks, and at the same time provide systematic study of the effects of architectural choices for ASR, we release NAS-Bench-ASR \u2013 the first NAS benchmark for ASR models. The dataset consists of 8, 242 unique models trained on the TIMIT audio dataset for three different target epochs, and each starting from three different initializations. The dataset also includes runtime measurements of all the models on a diverse set of hardware platforms. Lastly, we show that identified good cell structures in our search space for TIMIT transfer well to a much larger LibriSpeech dataset."}}
{"id": "bsRGLz-s2NY", "cdate": 1577836800000, "mdate": 1681650332725, "content": {"title": "Best of Both Worlds: AutoML Codesign of a CNN and its Hardware Accelerator", "abstract": ""}}
