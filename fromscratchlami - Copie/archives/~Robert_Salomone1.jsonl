{"id": "D4hRCcOdBxM", "cdate": 1672531200000, "mdate": 1682321809751, "content": {"title": "Federated Variational Inference Methods for Structured Latent Variable Models", "abstract": "Federated learning methods enable model training across distributed data sources without data leaving their original locations and have gained increasing interest in various fields. However, existing approaches are limited, excluding many structured probabilistic models. We present a general and elegant solution based on structured variational inference, widely used in Bayesian machine learning, adapted for the federated setting. Additionally, we provide a communication-efficient variant analogous to the canonical FedAvg algorithm. The proposed algorithms' effectiveness is demonstrated, and their performance is compared with hierarchical Bayesian neural networks and topic models."}}
{"id": "RHa77BXv6k", "cdate": 1652737597129, "mdate": null, "content": {"title": "Continuously Tempered PDMP samplers", "abstract": "New sampling algorithms based on simulating continuous-time stochastic processes called piece-wise deterministic Markov processes (PDMPs) have shown considerable promise. However, these methods can struggle to sample from multi-modal or heavy-tailed distributions. We show how tempering ideas can improve the mixing of PDMPs in such cases. We introduce an extended distribution defined over the state of the posterior distribution and an inverse temperature, which interpolates between a tractable distribution when the inverse temperature is 0 and the posterior when the inverse temperature is 1. The marginal distribution of the inverse temperature is a mixture of a continuous distribution on $[0,1)$ and a point mass at 1: which means that we obtain samples when the inverse temperature is 1, and these are draws from the posterior, but sampling algorithms will also explore distributions at lower temperatures which will improve mixing. We show how PDMPs, and particularly the Zig-Zag sampler, can be implemented to sample from such an extended distribution. The resulting algorithm is easy to implement and we show empirically that it can outperform existing PDMP-based samplers on challenging multimodal posteriors."}}
{"id": "EpXUaeHgY1", "cdate": 1609459200000, "mdate": 1681605661532, "content": {"title": "Implicit Langevin Algorithms for Sampling From Log-concave Densities", "abstract": ""}}
{"id": "baBbnmj1jyz", "cdate": 1577836800000, "mdate": 1682321809878, "content": {"title": "Spectral Subsampling MCMC for Stationary Time Series", "abstract": "Bayesian inference using Markov Chain Monte Carlo (MCMC) on large datasets has developed rapidly in recent years. However, the underlying methods are generally limited to relatively simple settings..."}}
{"id": "jI-sFKYU2N4", "cdate": 1546300800000, "mdate": 1682321809739, "content": {"title": "Monte Carlo estimation of the density of the sum of dependent random variables", "abstract": ""}}
{"id": "Z7KZ_kth9qS", "cdate": 1546300800000, "mdate": 1682321809742, "content": {"title": "Fast and accurate computation of the distribution of sums of dependent log-normals", "abstract": ""}}
