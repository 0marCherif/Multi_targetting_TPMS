{"id": "md4ZiV4m21y", "cdate": 1577836800000, "mdate": 1623940416818, "content": {"title": "Refer360$^\\circ$: A Referring Expression Recognition Dataset in 360$^\\circ$ Images", "abstract": "Volkan Cirik, Taylor Berg-Kirkpatrick, Louis-Philippe Morency. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020."}}
{"id": "Sy43zmZ_bB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Visual Referring Expression Recognition: What Do Systems Actually Learn?", "abstract": ""}}
{"id": "Sy-u6axO-H", "cdate": 1514764800000, "mdate": null, "content": {"title": "Using Syntax to Ground Referring Expressions in Natural Images", "abstract": "We introduce GroundNet, a neural network for referring expression recognition -- the task of localizing (or grounding) in an image the object referred to by a natural language expression. Our approach to this task is the first to rely on a syntactic analysis of the input referring expression in order to inform the structure of the computation graph. Given a parse tree for an input expression, we explicitly map the syntactic constituents and relationships present in the tree to a composed graph of neural modules that defines our architecture for performing localization. This syntax-based approach aids localization of \\textit{both} the target object and auxiliary supporting objects mentioned in the expression. As a result, GroundNet is more interpretable than previous methods: we can (1) determine which phrase of the referring expression points to which object in the image and (2) track how the localization of the target object is determined by the network. We study this property empirically by introducing a new set of annotations on the GoogleRef dataset to evaluate localization of supporting objects. Our experiments show that GroundNet achieves state-of-the-art accuracy in identifying supporting objects, while maintaining comparable performance in the localization of target objects."}}
{"id": "HJbboDZ_br", "cdate": 1514764800000, "mdate": null, "content": {"title": "Speaker-Follower Models for Vision-and-Language Navigation", "abstract": "Navigation guided by natural language instructions presents a challenging reasoning problem for instruction followers. Natural language instructions typically identify only a few high-level decisions and landmarks rather than complete low-level motor behaviors; much of the missing information must be inferred based on perceptual context. In machine learning settings, this is doubly challenging: it is difficult to collect enough annotated data to enable learning of this reasoning process from scratch, and also difficult to implement the reasoning process using generic sequence models. Here we describe an approach to vision-and-language navigation that addresses both these issues with an embedded speaker model. We use this speaker model to (1) synthesize new instructions for data augmentation and to (2) implement pragmatic reasoning, which evaluates how well candidate action sequences explain an instruction. Both steps are supported by a panoramic action space that reflects the granularity of human-generated instructions. Experiments show that all three components of this approach---speaker-driven data augmentation, pragmatic reasoning and panoramic action space---dramatically improve the performance of a baseline instruction follower, more than doubling the success rate over the best existing approach on a standard benchmark."}}
{"id": "_8KXZWoXZA", "cdate": 1483228800000, "mdate": 1623940416917, "content": {"title": "SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine", "abstract": "We publicly release a new large-scale dataset, called SearchQA, for machine comprehension, or question-answering. Unlike recently released datasets, such as DeepMind CNN/DailyMail and SQuAD, the proposed SearchQA was constructed to reflect a full pipeline of general question-answering. That is, we start not from an existing article and generate a question-answer pair, but start from an existing question-answer pair, crawled from J! Archive, and augment it with text snippets retrieved by Google. Following this approach, we built SearchQA, which consists of more than 140k question-answer pairs with each pair having 49.6 snippets on average. Each question-answer-context tuple of the SearchQA comes with additional meta-data such as the snippet's URL, which we believe will be valuable resources for future research. We conduct human evaluation as well as test two baseline methods, one simple word selection and the other deep learning based, on the SearchQA. We show that there is a meaningful gap between the human and machine performances. This suggests that the proposed dataset could well serve as a benchmark for question-answering."}}
{"id": "fgdKgE98NEl", "cdate": 1451606400000, "mdate": 1623940416837, "content": {"title": "Visualizing and Understanding Curriculum Learning for Long Short-Term Memory Networks", "abstract": "Curriculum Learning emphasizes the order of training instances in a computational learning setup. The core hypothesis is that simpler instances should be learned early as building blocks to learn more complex ones. Despite its usefulness, it is still unknown how exactly the internal representation of models are affected by curriculum learning. In this paper, we study the effect of curriculum learning on Long Short-Term Memory (LSTM) networks, which have shown strong competency in many Natural Language Processing (NLP) problems. Our experiments on sentiment analysis task and a synthetic task similar to sequence prediction tasks in NLP show that curriculum learning has a positive effect on the LSTM's internal states by biasing the model towards building constructive representations i.e. the internal representation at the previous timesteps are used as building blocks for the final prediction. We also find that smaller models significantly improves when they are trained with curriculum learning. Lastly, we show that curriculum learning helps more when the amount of training data is limited."}}
{"id": "8Fij9FKeFKr", "cdate": 1451606400000, "mdate": 1623940416870, "content": {"title": "Learning grammatical categories using paradigmatic representations: Substitute words for language acquisition", "abstract": "Mehmet Ali Yatbaz, Volkan Cirik, Aylin K\u00fcntay, Deniz Yuret. Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. 2016."}}
{"id": "zC6sbQjSIHg", "cdate": 1388534400000, "mdate": 1623940416862, "content": {"title": "Substitute Based SCODE Word Embeddings in Supervised NLP Tasks", "abstract": "We analyze a word embedding method in supervised tasks. It maps words on a sphere such that words co-occurring in similar contexts lie closely. The similarity of contexts is measured by the distribution of substitutes that can fill them. We compared word embeddings, including more recent representations, in Named Entity Recognition (NER), Chunking, and Dependency Parsing. We examine our framework in multilingual dependency parsing as well. The results show that the proposed method achieves as good as or better results compared to the other word embeddings in the tasks we investigate. It achieves state-of-the-art results in multilingual dependency parsing. Word embeddings in 7 languages are available for public use."}}
{"id": "yiha6VmO0CP", "cdate": 1356998400000, "mdate": 1623940416925, "content": {"title": "AI-KU: Using Substitute Vectors and Co-Occurrence Modeling For Word Sense Induction and Disambiguation", "abstract": "Osman Ba\u015fkaya, Enis Sert, Volkan Cirik, Deniz Yuret. Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013). 2013."}}
{"id": "Ma1CyAgUCc", "cdate": 1356998400000, "mdate": 1623940416893, "content": {"title": "The AI-KU System at the SPMRL 2013 Shared Task : Unsupervised Features for Dependency Parsing", "abstract": "Volkan Cirik, Husnu Sensoy. Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages. 2013."}}
