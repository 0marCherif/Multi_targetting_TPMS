{"id": "PK2debCKaG", "cdate": 1686324867426, "mdate": null, "content": {"title": "Language Conditioned Traffic Generation", "abstract": "Simulation forms the backbone of modern self-driving development. Simulators help develop, test, and improve driving systems without putting humans, vehicles, or their environment at risk. However, simulators face a major challenge: They rely on realistic, scalable, yet interesting content. While recent advances in rendering and scene reconstruction make great strides in creating static scene assets, modeling their layout, dynamics, and behaviors remains challenging. In this work, we turn to language as a source of supervision for dynamic traffic scene generation. Our model, LCTGen, combines a large language model with a transformer-based decoder architecture that selects likely map locations from a dataset of maps, and produces an initial traffic distribution, as well as the dynamics of each vehicle. LCTGen outperforms prior work in both unconditional and conditional traffic scene generation in terms of realism and fidelity."}}
{"id": "IzI055GrvG", "cdate": 1663849804527, "mdate": null, "content": {"title": "Object Tracking by Hierarchical Part-Whole Attention", "abstract": "We present in this paper that hierarchical representations of objects can provide an informative and low-noisy proxy to associate objects of interest in multi-object tracking. This is aligned with our intuition that we usually only need to compare a little region of the body of target objects to distinguish them from other objects. We build the hierarchical representation in levels of (1) target body parts, (2) the whole target body, and  (3) the union area of the target and other objects of overlap.  Furthermore, with the spatio-temporal attention mechanism by transformer, we can solve the tracking in a global fashion and keeps the process online.  We design our method by combining the representation with the transformer and name it Hierarchical Part-Whole Attention, or HiPWA for short. The experiments on multiple datasets suggest its good effectiveness.  Moreover, previous methods mostly focus on leveraging transformers to exploit long temporal context during association which requires heavy computation resources. But HiPWA focuses on a more informative representation of objects on every single frame instead. So it is more robust with the length of temporal context and more computationally economic. "}}
{"id": "nId8ZtIXub", "cdate": 1663849804407, "mdate": null, "content": {"title": "Observation-Centric SORT: Rethinking SORT for Robust Multi-Object Tracking", "abstract": "Recent advances in object detection and re-identification have greatly improved the performance of Multi-Object Tracking (MOT) methods, but progress in motion modeling has been limited. The motion model is a key component of many MOT methods and is commonly used to predict an object's future position. However, mainstream motion models in MOT naively assume that object motion is linear. They rely on detections on each frame as the observation value to supervise motion models. However, in practice, the observations can be noisy and even missing, especially in crowded scenes, which greatly degrade the performance of existing MOT methods. In this work, we show that a simple filtering-based motion model can still obtain state-of-the-art tracking performance if proper care is given to missing observations and noisy estimates. We emphasize the role of observations when recovering tracks from being lost and reducing the error accumulated by the assumption of linear motion when the target is lost. In contrast to the popular motion-based method SORT, which is estimation-centric, we name our method Observation-Centric SORT (OC-SORT). It remains simple, online, and real-time but improves robustness over occlusion and non-linear motion. It achieves state-of-the-art on multiple MOT benchmarks, including MOT17, MOT20, KITTI, head tracking, and especially DanceTrack where the object motion is highly non-linear."}}
{"id": "uhhA2OryTjj", "cdate": 1655376328710, "mdate": null, "content": {"title": "Robust Trajectory Prediction against Adversarial Attacks", "abstract": "Trajectory prediction using deep neural networks (DNNs) is an essential component of autonomous driving (AD) systems.  However, these methods are vulnerable to adversarial attacks, leading to serious consequences such as collisions. In this work, we identify two key ingredients to defend trajectory prediction models against adversarial attacks including (1) designing effective adversarial training methods and (2) adding domain-specific data augmentation to mitigate the performance degradation on clean data. We demonstrate that our method is able to improve the performance by 46\\% on adversarial data and at the cost of only 3\\% performance degradation on clean data, compared to the model trained with clean data. Additionally, compared to existing robust methods, our method can improve performance by 21\\% on adversarial examples and 9\\% on clean data. Our robust model is evaluated with a planner to study its downstream impacts. We demonstrate that our model can significantly reduce the severe accident rates (e.g., collisions and off-road driving)."}}
{"id": "yl9aThYT9W", "cdate": 1623084972503, "mdate": null, "content": {"title": "All-In-One Drive: A Comprehensive Perception Dataset with High-Density Long-Range Point Clouds", "abstract": "Developing datasets that cover comprehensive sensors, annotations, and out-of-distribution data is important for innovating robust multi-sensor multi-task perception systems in autonomous driving. Though many datasets have been released, they target different use-cases such as 3D segmentation (SemanticKITTI), radar data (nuScenes), large-scale training and evaluation (Waymo). As a result, we are still in need of a dataset that forms a union of various strengths of existing datasets. To address this challenge, we present the AIODrive dataset, a synthetic large-scale dataset that provides comprehensive sensors, annotations, and environmental variations. Specifically, we provide (1) eight sensor modalities (RGB, Stereo, Depth, LiDAR, SPAD-LiDAR, Radar, IMU, GPS), (2) annotations for all mainstream perception tasks (e.g., detection, tracking, prediction, segmentation, depth estimation, etc), and (3) out-of-distribution driving scenarios such as adverse weather and lighting, crowded scenes, high-speed driving, violation of traffic rules, and vehicle crash. In addition to comprehensive data, long-range perception is also important to perception systems as early detection of faraway objects can help prevent collision in high-speed driving scenarios. However, due to the sparsity and limited range of point cloud data in prior datasets, developing and evaluating long-range perception algorithms is not feasible. To address the issue, we provide high-density long-range point clouds for LiDAR and SPAD-LiDAR sensors (10x than Velodyne-64), to enable research in long-range perception. Our dataset is released and free to use for both research and commercial purpose: http://www.aiodrive.org/"}}
{"id": "pkDRk0z5l0", "cdate": 1578685615144, "mdate": null, "content": {"title": "A Baseline for 3D Multi-Object Tracking", "abstract": "3D multi-object tracking (MOT) is an essential component technology for many real-time applications such as autonomous driving or assistive robotics. Recent work on 3D MOT tend to focus more on developing accurate systems giving less regard to computational cost and system complexity. In contrast, this work proposes a simple yet accurate real-time 3D MOT system. We use an off-theshelf 3D object detector to obtain oriented 3D bounding boxes from the LiDAR point cloud. Then, a combination of 3D Kalman filter and Hungarian algorithm is used for state estimation and data association. Although our baseline system is a straightforward combination of standard methods, we obtain the state-of-the-art results. To evaluate our baseline system, we propose a new 3D MOT extension to the official KITTI 2D MOT evaluation along with a set of new metrics. Our proposed baseline method for 3D MOT establishes new state-of-the-art performance on 3D MOT for KITTI. Surprisingly, although our baseline system does not use any 2D data as input, we place 2nd on the official KITTI 2D MOT leaderboard. Also, our proposed 3D MOT method runs at a rate of 214.7 FPS, achieving the fastest speed among all modern MOT systems. Our code is publicly available at https://github.com/xinshuoweng/AB3DMOT"}}
{"id": "7RcI6ZnxxF", "cdate": 1578685535985, "mdate": null, "content": {"title": "Monocular 3D Object Detection with Pseudo-LiDAR Point Cloud", "abstract": "Monocular 3D scene understanding tasks, such as object size estimation, heading angle estimation and 3D localization, is challenging. Successful modern day methods for 3D scene understanding require the use of a 3D sensor. On the other hand, single image based methods have significantly worse performance. In this work, we aim at bridging the performance gap between 3D sensing and 2D sensing for 3D object detection by enhancing LiDAR-based algorithms to work with single image input. Specifically, we perform monocular depth estimation and lift the input image to a point cloud representation, which we call pseudoLiDAR point cloud. Then we can train a LiDAR-based 3D detection network with our pseudo-LiDAR end-to-end. Following the pipeline of two-stage 3D detection algorithms, we detect 2D object proposals in the input image and extract a point cloud frustum from the pseudo-LiDAR for each proposal. Then an oriented 3D bounding box is detected for each frustum. To handle the large amount of noise in the pseudo-LiDAR, we propose two innovations: (1) use a 2D-3D bounding box consistency constraint, adjusting the predicted 3D bounding box to have a high overlap with its corresponding 2D proposal after projecting onto the image; (2) use the instance mask instead of the bounding box as the representation of 2D proposals, in order to reduce the number of points not belonging to the object in the point cloud frustum. Through our evaluation on the KITTI benchmark, we achieve the top-ranked performance on both bird\u2019s eye view and 3D object detection among all monocular methods, effectively quadrupling the performance over previous state-of-the-art. Our code is available at https://github.com/xinshuoweng/Mono3D_PLiDAR"}}
{"id": "S1N7hyfOZH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Supervision-by-Registration: An Unsupervised Approach to Improve the Precision of Facial Landmark Detectors", "abstract": "In this paper, we present supervision-by-registration, an unsupervised approach to improve the precision of facial landmark detectors on both images and video. Our key observation is that the detections of the same landmark in adjacent frames should be coherent with registration, i.e., optical flow. Interestingly, coherency of optical flow is a source of supervision that does not require manual labeling, and can be leveraged during detector training. For example, we can enforce in the training loss function that a detected landmark at frame t-1 followed by optical flow tracking from frame t-1 to frame t should coincide with the location of the detection at frame t. Essentially, supervision-by-registration augments the training loss function with a registration loss, thus training the detector to have output that is not only close to the annotations in labeled images, but also consistent with registration on large amounts of unlabeled videos. End-to-end training with the registration loss is made possible by a differentiable Lucas-Kanade operation, which computes optical flow registration in the forward pass, and back-propagates gradients that encourage temporal coherency in the detector. The output of our method is a more precise image-based facial landmark detector, which can be applied to single images or video. With supervision-by-registration, we demonstrate (1) improvements in facial landmark detection on both images (300W, ALFW) and video (300VW, Youtube-Celebrities), and (2) significant reduction of jittering in video detections."}}
