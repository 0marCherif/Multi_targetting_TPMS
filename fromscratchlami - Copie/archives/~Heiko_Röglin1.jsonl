{"id": "VNa_c9MtVXZ", "cdate": 1672531200000, "mdate": 1701846263943, "content": {"title": "The smoothed number of Pareto-optimal solutions in bicriteria integer optimization", "abstract": "A well-established heuristic approach for solving bicriteria optimization problems is to enumerate the set of Pareto-optimal solutions. The heuristics following this principle are often successful in practice. Their running time, however, depends on the number of enumerated solutions, which is exponential in the worst case. We study bicriteria integer optimization problems in the model of smoothed analysis, in which inputs are subject to a small amount of random noise, and we prove an almost tight polynomial bound on the expected number of Pareto-optimal solutions. Our results give rise to tight polynomial bounds for the expected running time of the Nemhauser-Ullmann algorithm for the knapsack problem and they improve known results on the running times of heuristics for the bounded knapsack problem and the bicriteria shortest path problem."}}
{"id": "S3FMZJYRYNn", "cdate": 1672531200000, "mdate": 1701846263929, "content": {"title": "Connected k-Center and k-Diameter Clustering", "abstract": "Motivated by an application from geodesy, we study the connected k-center problem and the connected k-diameter problem. These problems arise from the classical k-center and k-diameter problems by adding a side constraint. For the side constraint, we are given an undirected connectivity graph G on the input points, and a clustering is now only feasible if every cluster induces a connected subgraph in G. Usually in clustering problems one assumes that the clusters are pairwise disjoint. We study this case but additionally also the case that clusters are allowed to be non-disjoint. This can help to satisfy the connectivity constraints. Our main result is an O(1)-approximation algorithm for the disjoint connected k-center and k-diameter problem for Euclidean spaces of low dimension (constant d) and for metrics with constant doubling dimension. For general metrics, we get an O(log\u00b2k)-approximation. Our algorithms work by computing a non-disjoint connected clustering first and transforming it into a disjoint connected clustering. We complement these upper bounds by several upper and lower bounds for variations and special cases of the model."}}
{"id": "8QWgGA2dVgC", "cdate": 1672531200000, "mdate": 1681720017090, "content": {"title": "Worst Case and Probabilistic Analysis of the 2-Opt Algorithm for the TSP", "abstract": "2-Opt is probably the most basic local search heuristic for the TSP. This heuristic achieves amazingly good results on real world Euclidean instances both with respect to running time and approximation ratio. There are numerous experimental studies on the performance of 2-Opt. However, the theoretical knowledge about this heuristic is still very limited. Not even its worst case running time on 2-dimensional Euclidean instances was known so far. We clarify this issue by presenting, for every $p\\in\\mathbb{N}$, a family of $L_p$ instances on which 2-Opt can take an exponential number of steps. Previous probabilistic analyses were restricted to instances in which $n$ points are placed uniformly at random in the unit square $[0,1]^2$. We consider a more advanced model in which the points can be placed independently according to general distributions on $[0,1]^d$, for an arbitrary $d\\ge 2$. In particular, we allow different distributions for different points. We study the expected number of local improvements in terms of the number $n$ of points and the maximal density $\\phi$ of the probability distributions. We show an upper bound on the expected length of any 2-Opt improvement path of $\\tilde{O}(n^{4+1/3}\\cdot\\phi^{8/3})$. When starting with an initial tour computed by an insertion heuristic, the upper bound on the expected number of steps improves even to $\\tilde{O}(n^{4+1/3-1/d}\\cdot\\phi^{8/3})$. If the distances are measured according to the Manhattan metric, then the expected number of steps is bounded by $\\tilde{O}(n^{4-1/d}\\cdot\\phi)$. In addition, we prove an upper bound of $O(\\sqrt[d]{\\phi})$ on the expected approximation factor with respect to all $L_p$ metrics. Let us remark that our probabilistic analysis covers as special cases the uniform input model with $\\phi=1$ and a smoothed analysis with Gaussian perturbations of standard deviation $\\sigma$ with $\\phi\\sim1/\\sigma^d$."}}
{"id": "zLo3ESId1-", "cdate": 1640995200000, "mdate": 1681720017100, "content": {"title": "Minimum-Error Triangulations for Sea Surface Reconstruction", "abstract": "We apply state-of-the-art computational geometry methods to the problem of reconstructing a time-varying sea surface from tide gauge records. Our work builds on a recent article by Nitzke et al. (Computers & Geosciences, 157:104920, 2021) who have suggested to learn a triangulation D of a given set of tide gauge stations. The objective is to minimize the misfit of the piecewise linear surface induced by D to a reference surface that has been acquired with satellite altimetry. The authors restricted their search to k-order Delaunay (k-OD) triangulations and used an integer linear program in order to solve the resulting optimization problem. In geometric terms, the input to our problem consists of two sets of points in \u211d\u00b2 with elevations: a set \ud835\udcae that is to be triangulated, and a set \u211b of reference points. Intuitively, we define the error of a triangulation as the average vertical distance of a point in \u211b to the triangulated surface that is obtained by interpolating elevations of \ud835\udcae linearly in each triangle. Our goal is to find the triangulation of \ud835\udcae that has minimum error with respect to \u211b. In our work, we prove that the minimum-error triangulation problem is NP-hard and cannot be approximated within any multiplicative factor in polynomial time unless P = NP. At the same time we show that the problem instances that occur in our application (considering sea level data from several hundreds of tide gauge stations worldwide) can be solved relatively fast using dynamic programming when restricted to k-OD triangulations for k \u2264 7. In particular, instances for which the number of connected components of the so-called k-OD fixed-edge graph is small can be solved within few seconds."}}
{"id": "pt5fkkL5iQF", "cdate": 1640995200000, "mdate": 1681720017096, "content": {"title": "Minimum-Error Triangulations for Sea Surface Reconstruction", "abstract": "We apply state-of-the-art computational geometry methods to the problem of reconstructing a time-varying sea surface from tide gauge records. Our work builds on a recent article by Nitzke et al.~(Computers \\& Geosciences, 157:104920, 2021) who have suggested to learn a triangulation $D$ of a given set of tide gauge stations. The objective is to minimize the misfit of the piecewise linear surface induced by $D$ to a reference surface that has been acquired with satellite altimetry. The authors restricted their search to k-order Delaunay ($k$-OD) triangulations and used an integer linear program in order to solve the resulting optimization problem. In geometric terms, the input to our problem consists of two sets of points in $\\mathbb{R}^2$ with elevations: a set $\\mathcal{S}$ that is to be triangulated, and a set $\\mathcal{R}$ of reference points. Intuitively, we define the error of a triangulation as the average vertical distance of a point in $\\mathcal{R}$ to the triangulated surface that is obtained by interpolating elevations of $\\mathcal{S}$ linearly in each triangle. Our goal is to find the triangulation of $\\mathcal{S}$ that has minimum error with respect to $\\mathcal{R}$. In our work, we prove that the minimum-error triangulation problem is NP-hard and cannot be approximated within any multiplicative factor in polynomial time unless $P=NP$. At the same time we show that the problem instances that occur in our application (considering sea level data from several hundreds of tide gauge stations worldwide) can be solved relatively fast using dynamic programming when restricted to $k$-OD triangulations for $k\\le 7$. In particular, instances for which the number of connected components of the so-called $k$-OD fixed-edge graph is small can be solved within few seconds."}}
{"id": "lxAgn0zWYQ", "cdate": 1640995200000, "mdate": 1681720017091, "content": {"title": "The Price of Hierarchical Clustering", "abstract": "Hierarchical Clustering is a popular tool for understanding the hereditary properties of a data set. Such a clustering is actually a sequence of clusterings that starts with the trivial clustering in which every data point forms its own cluster and then successively merges two existing clusters until all points are in the same cluster. A hierarchical clustering achieves an approximation factor of \u03b1 if the costs of each k-clustering in the hierarchy are at most \u03b1 times the costs of an optimal k-clustering. We study as cost functions the maximum (discrete) radius of any cluster (k-center problem) and the maximum diameter of any cluster (k-diameter problem). In general, the optimal clusterings do not form a hierarchy and hence an approximation factor of 1 cannot be achieved. We call the smallest approximation factor that can be achieved for any instance the price of hierarchy. For the k-diameter problem we improve the upper bound on the price of hierarchy to 3+2\u221a2\u2248 5.83. Moreover we significantly improve the lower bounds for k-center and k-diameter, proving a price of hierarchy of exactly 4 and 3+2\u221a2, respectively."}}
{"id": "_Rw8eeIH1u4", "cdate": 1640995200000, "mdate": 1681720017107, "content": {"title": "Connected k-Center and k-Diameter Clustering", "abstract": "Motivated by an application from geodesy, we introduce a novel clustering problem which is a $k$-center (or k-diameter) problem with a side constraint. For the side constraint, we are given an undirected connectivity graph $G$ on the input points, and a clustering is now only feasible if every cluster induces a connected subgraph in $G$. We call the resulting problems the connected $k$-center problem and the connected $k$-diameter problem. We prove several results on the complexity and approximability of these problems. Our main result is an $O(\\log^2{k})$-approximation algorithm for the connected $k$-center and the connected $k$-diameter problem. For Euclidean metrics and metrics with constant doubling dimension, the approximation factor of this algorithm improves to $O(1)$. We also consider the special cases that the connectivity graph is a line or a tree. For the line we give optimal polynomial-time algorithms and for the case that the connectivity graph is a tree, we either give an optimal polynomial-time algorithm or a $2$-approximation algorithm for all variants of our model. We complement our upper bounds by several lower bounds."}}
{"id": "TI1U9-PW4o", "cdate": 1640995200000, "mdate": 1681720017095, "content": {"title": "The Price of Hierarchical Clustering", "abstract": "Hierarchical Clustering is a popular tool for understanding the hereditary properties of a data set. Such a clustering is actually a sequence of clusterings that starts with the trivial clustering in which every data point forms its own cluster and then successively merges two existing clusters until all points are in the same cluster. A hierarchical clustering achieves an approximation factor of $\\alpha$ if the costs of each $k$-clustering in the hierarchy are at most $\\alpha$ times the costs of an optimal $k$-clustering. We study as cost functions the maximum (discrete) radius of any cluster ($k$-center problem) and the maximum diameter of any cluster ($k$-diameter problem). In general, the optimal clusterings do not form a hierarchy and hence an approximation factor of $1$ cannot be achieved. We call the smallest approximation factor that can be achieved for any instance the price of hierarchy. For the $k$-diameter problem we improve the upper bound on the price of hierarchy to $3+2\\sqrt{2}\\approx 5.83$. Moreover we significantly improve the lower bounds for $k$-center and $k$-diameter, proving a price of hierarchy of exactly $4$ and $3+2\\sqrt{2}$, respectively."}}
{"id": "oqG0KN_QrAp", "cdate": 1609459200000, "mdate": 1681720017094, "content": {"title": "Bicriteria Aggregation of Polygons via Graph Cuts", "abstract": "We present a new method for the task of detecting groups of polygons in a given geographic data set and computing a representative polygon for each group. This task is relevant in map generalization where the aim is to derive a less detailed map from a given map. Following a classical approach, we define the output polygons by merging the input polygons with a set of triangles that we select from a constrained Delaunay triangulation of the input polygons' exterior. The innovation of our method is to compute the selection of triangles by solving a bicriteria optimization problem. While on the one hand we aim at minimizing the total area of the outputs polygons, we aim on the other hand at minimizing their total perimeter. We combine these two objectives in a weighted sum and study two computational problems that naturally arise. In the first problem, the parameter that balances the two objectives is fixed and the aim is to compute a single optimal solution. In the second problem, the aim is to compute a set containing an optimal solution for every possible value of the parameter. We present efficient algorithms for these problems based on computing a minimum cut in an appropriately defined graph. Moreover, we show how the result set of the second problem can be approximated with few solutions. In an experimental evaluation, we finally show that the method is able to derive settlement areas from building footprints that are similar to reference solutions."}}
{"id": "6-yrGZ4q5qV", "cdate": 1609459200000, "mdate": 1681720017120, "content": {"title": "Upper and Lower Bounds for Complete Linkage in General Metric Spaces", "abstract": "In a hierarchical clustering problem the task is to compute a series of mutually compatible clusterings of a finite metric space (P,dist). Starting with the clustering where every point forms its own cluster, one iteratively merges two clusters until only one cluster remains. Complete linkage is a well-known and popular algorithm to compute such clusterings: in every step it merges the two clusters whose union has the smallest radius (or diameter) among all currently possible merges. We prove that the radius (or diameter) of every k-clustering computed by complete linkage is at most by factor O(k) (or O(k\u00b2)) worse than an optimal k-clustering minimizing the radius (or diameter). Furthermore we give a negative answer to the question proposed by Dasgupta and Long [Sanjoy Dasgupta and Philip M. Long, 2005], who show a lower bound of \u03a9(log(k)) and ask if the approximation guarantee is in fact \u0398(log(k)). We present instances where complete linkage performs poorly in the sense that the k-clustering computed by complete linkage is off by a factor of \u03a9(k) from an optimal solution for radius and diameter. We conclude that in general metric spaces complete linkage does not perform asymptotically better than single linkage, merging the two clusters with smallest inter-cluster distance, for which we prove an approximation guarantee of O(k)."}}
