{"id": "IpBjWtJp40j", "cdate": 1652737695110, "mdate": null, "content": {"title": "The Hessian Screening Rule", "abstract": "Predictor screening rules, which discard predictors before fitting a model, have had considerable impact on the speed with which sparse regression problems, such as the lasso, can be solved. In this paper we present a new screening rule for solving the lasso path: the Hessian Screening Rule. The rule uses second-order information from the model to provide both effective screening, particularly in the case of high correlation, as well as accurate warm starts. The proposed rule outperforms all alternatives we study on simulated data sets with both low and high correlation for \\(\\ell_1\\)-regularized least-squares (the lasso) and logistic regression. It also performs best in general on the real data sets that we examine. "}}
{"id": "d2CejHDZJh", "cdate": 1621629859597, "mdate": null, "content": {"title": "Efficient methods for Gaussian Markov random fields under sparse linear constraints", "abstract": "Methods for inference and simulation of linearly constrained Gaussian Markov Random Fields (GMRF) are computationally prohibitive when the number of constraints is large. In some cases, such as for intrinsic GMRFs, they may even be unfeasible. We propose a new class of methods to overcome these challenges in the common case of sparse constraints, where one has a large number of constraints and each only involves a few elements. Our methods rely on a basis transformation into blocks of constrained versus non-constrained subspaces, and we show that the methods greatly outperform existing alternatives in terms of computational cost. By combining the proposed methods with the stochastic partial differential equation approach for Gaussian random fields, we also show how to formulate Gaussian process regression with linear constraints in a GMRF setting to reduce computational cost. This is illustrated in two applications with simulated data."}}
