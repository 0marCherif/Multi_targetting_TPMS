{"id": "RKl992IImOA", "cdate": 1640995200000, "mdate": 1681674145403, "content": {"title": "Dyna-bAbI: unlocking bAbI's potential with dynamic synthetic benchmarking", "abstract": ""}}
{"id": "LNpT3zz9FCr", "cdate": 1640995200000, "mdate": 1681674145404, "content": {"title": "Identifying the Limits of Cross-Domain Knowledge Transfer for Pretrained Models", "abstract": ""}}
{"id": "8ZtHs0gRpZ", "cdate": 1640995200000, "mdate": 1681674145400, "content": {"title": "Are Sample-Efficient NLP Models More Robust?", "abstract": "Recent results in image classification and extractive question answering have observed that pre-trained models trained on less in-distribution data have better out-of-distribution performance. However, it is unclear how broadly these trends hold. We conduct a large empirical study across three tasks, three broadly-applicable modeling interventions (increasing model size, using a different adaptation method, and pre-training on more data), and 14 diverse datasets to investigate the relationship between sample efficiency (amount of data needed to reach a given ID accuracy) and robustness (how models fare on OOD evaluation). We find that higher sample efficiency is only correlated with better average OOD robustness on some modeling interventions and tasks, but not others. On individual datasets, models with lower sample efficiency can even be more robust. These results suggest that general-purpose methods for improving sample efficiency are unlikely to yield universal OOD robustness improvements, since such improvements are highly dataset- and task-dependent. Even in an era of large, multi-purpose pretrained models, task-specific decisions may often be necessary for OOD generalization."}}
{"id": "luO6l9cP6b6", "cdate": 1632875480759, "mdate": null, "content": {"title": "Identifying the Limits of Cross-Domain Knowledge Transfer for Pretrained Models", "abstract": "There is growing evidence that pretrained language models improve task-specific fine-tuning even where the task examples are radically different from those seen in training. What is the nature of this surprising cross-domain transfer? We offer a partial answer via a systematic exploration of how much transfer occurs when models are denied any information about word identity via random scrambling. In four classification tasks and two sequence labeling tasks, we evaluate LSTMs using GloVe embeddings, BERT, and baseline models. Among these models, we find that only BERT shows high rates of transfer into our scrambled domains, and for classification but not sequence labeling tasks. Our analyses seek to explain why transfer succeeds for some tasks but not others, to isolate the separate contributions of pretraining versus fine-tuning, to show that the fine-tuning process is not merely learning to unscramble the scrambled inputs, and to quantify the role of word frequency. These findings help explain where and why cross-domain transfer occurs, which can guide future studies and practical fine-tuning efforts."}}
{"id": "stusRZWANTH", "cdate": 1609459200000, "mdate": null, "content": {"title": "Can Small and Synthetic Benchmarks Drive Modeling Innovation? A Retrospective Study of Question Answering Modeling Approaches", "abstract": "Do question answering (QA) modeling improvements (e.g., choice of architecture and training procedure) hold across the diverse landscape of QA benchmarks? To study this question, we introduce the notion of concurrence -- two benchmarks have high concurrence on a set of modeling approaches if they rank the modeling approaches similarly. We measure the concurrence between 32 QA benchmarks on a set of 20 diverse modeling approaches and find that human-constructed benchmarks have high concurrence amongst themselves, even if their passage and question distributions are very different. Surprisingly, even downsampled human-constructed benchmarks (i.e., collecting less data) and programmatically-generated benchmarks (e.g., cloze-formatted examples) have high concurrence with human-constructed benchmarks. These results indicate that, despite years of intense community focus on a small number of benchmarks, the modeling improvements studied hold broadly."}}
{"id": "kEZf0QpZMAc", "cdate": 1609459200000, "mdate": 1681674145403, "content": {"title": "Lexical Semantic Recognition", "abstract": ""}}
{"id": "8InyM8Pporr", "cdate": 1609459200000, "mdate": 1681674145411, "content": {"title": "Making Heads and Tails of Models with Marginal Calibration for Sparse Tagsets", "abstract": ""}}
{"id": "DcDs2RRRWm", "cdate": 1580837738360, "mdate": null, "content": {"title": "QUOREF: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning", "abstract": "Machine comprehension of texts longer than a single sentence often requires coreference resolution. However, most current reading comprehension benchmarks do not contain complex coreferential phenomena and hence fail to evaluate the ability of models to resolve coreference. We present a new crowdsourced dataset containing more than 24K span-selection questions that require resolving coreference among entities in over 4.7K English paragraphs from Wikipedia. Obtaining questions focused on such phenomena is challenging, because it is hard to avoid lexical cues that shortcut complex reasoning. We deal with this issue by using a strong baseline model as an adversary in the crowdsourcing loop, which helps crowdworkers avoid writing questions with exploitable surface cues. We show that state-of-the-art reading comprehension models perform significantly worse than humans on this benchmark\u2014the best model performance is 70.5 F1, while the estimated human performance is 93.4 F1."}}
{"id": "LJA68XdwUvW", "cdate": 1577836800000, "mdate": null, "content": {"title": "Evaluating NLP Models via Contrast Sets", "abstract": "Standard test sets for supervised learning evaluate in-distribution generalization. Unfortunately, when a dataset has systematic gaps (e.g., annotation artifacts), these evaluations are misleading: a model can learn simple decision rules that perform well on the test set but do not capture a dataset's intended capabilities. We propose a new annotation paradigm for NLP that helps to close systematic gaps in the test data. In particular, after a dataset is constructed, we recommend that the dataset authors manually perturb the test instances in small but meaningful ways that (typically) change the gold label, creating contrast sets. Contrast sets provide a local view of a model's decision boundary, which can be used to more accurately evaluate a model's true linguistic capabilities. We demonstrate the efficacy of contrast sets by creating them for 10 diverse NLP datasets (e.g., DROP reading comprehension, UD parsing, IMDb sentiment analysis). Although our contrast sets are not explicitly adversarial, model performance is significantly lower on them than on the original test sets---up to 25\\% in some cases. We release our contrast sets as new evaluation benchmarks and encourage future dataset construction efforts to follow similar annotation processes."}}
{"id": "ItFcIqKqEfS", "cdate": 1577836800000, "mdate": null, "content": {"title": "Evaluating Models' Local Decision Boundaries via Contrast Sets", "abstract": "Matt Gardner, Yoav Artzi, Victoria Basmov, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Nitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco, Daniel Khashabi, Kevin Lin, Jiangming Liu, Nelson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A. Smith, Sanjay Subramanian, Reut Tsarfaty, Eric Wallace, Ally Zhang, Ben Zhou. Findings of the Association for Computational Linguistics: EMNLP 2020. 2020."}}
