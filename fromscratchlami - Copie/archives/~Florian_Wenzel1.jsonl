{"id": "57Ryl7lLD4h", "cdate": 1652737766542, "mdate": null, "content": {"title": "Assaying Out-Of-Distribution Generalization in Transfer Learning", "abstract": "Since out-of-distribution generalization is a generally ill-posed problem, various proxy targets (e.g., calibration, adversarial robustness, algorithmic corruptions, invariance across shifts) were studied across different research programs resulting in different recommendations. While sharing the same aspirational goal, these approaches have never been tested under the same experimental conditions on real data. In this paper, we take a unified view of previous work, highlighting message discrepancies that we address empirically, and providing recommendations on how to measure the robustness of a model and how to improve it. To this end, we collect 172 publicly available dataset pairs for training and out-of-distribution evaluation of accuracy, calibration error, adversarial attacks, environment invariance, and synthetic corruptions. We fine-tune over 31k networks, from nine different architectures in the many- and few-shot setting. Our findings confirm that in- and out-of-distribution accuracies tend to increase jointly, but show that their relation is largely dataset-dependent, and in general more nuanced and more complex than posited by previous, smaller scale studies."}}
{"id": "0Tnl8uBHfQw", "cdate": 1632875690933, "mdate": null, "content": {"title": "Deep Classifiers with Label Noise Modeling and Distance Awareness", "abstract": "Uncertainty estimation in deep learning has recently emerged as a crucial area of interest to advance reliability and robustness in safety-critical applications. While there have been many proposed methods that either focus on distance-aware model uncertainties for out-of-distribution detection or on input-dependent label uncertainties for in-distribution calibration, both of these types of uncertainty are often necessary. In this work, we propose the HetSNGP method for jointly modeling the model and data uncertainty. We show that our proposed model affords a favorable combination between these two complementary types of uncertainty and thus outperforms the baseline methods on some challenging out-of-distribution datasets, including CIFAR-100C, Imagenet-C, and Imagenet-A. Moreover, we propose HetSNGP Ensemble, an ensembled version of our method which adds an additional type of uncertainty and also outperforms other ensemble baselines."}}
{"id": "xkjqJYqRJy", "cdate": 1632875650081, "mdate": null, "content": {"title": "Bayesian Neural Network Priors Revisited", "abstract": "Isotropic Gaussian priors are the de facto standard for modern Bayesian neural network inference. However, it is unclear whether these priors accurately reflect our true beliefs about the weight distributions or give optimal performance. To find better priors, we study summary statistics of neural network weights in networks trained using stochastic gradient descent (SGD). We find that convolutional neural network (CNN) and ResNet weights display strong spatial correlations, while fully connected networks (FCNNs) display heavy-tailed weight distributions. We show that building these observations into priors can lead to improved performance on a variety of image classification datasets. Surprisingly, these priors mitigate the cold posterior effect in FCNNs, but slightly increase the cold posterior effect in ResNets."}}
{"id": "TD-5kgf13mH", "cdate": 1632875465616, "mdate": null, "content": {"title": "Sparse MoEs meet Efficient Ensembles", "abstract": "Machine learning models based on the aggregated outputs of submodels, either at the activation or prediction levels, lead to strong performance. We study the interplay of two popular classes of such models: ensembles of neural networks and sparse mixture of experts (sparse MoEs). First, we show that these two approaches have complementary features whose combination is beneficial. Then, we present partitioned batch ensembles, an efficient ensemble of sparse MoEs that takes the best of both classes of models. Extensive experiments on fine-tuned vision transformers demonstrate the accuracy, log-likelihood, few-shot learning, robustness, and uncertainty calibration improvements of our approach over several challenging baselines. Partitioned batch ensembles not only scale to models with up to 2.7B parameters, but also provide larger performance gains for larger models. "}}
{"id": "Lzi5IMyJTFX", "cdate": 1606146135119, "mdate": null, "content": {"title": "Distilling Ensembles Improves Uncertainty Estimates", "abstract": "We seek to bridge the performance gap between batch ensembles (ensembles of deep networks with shared parameters)  and deep ensembles on tasks which require not only predictions, but also uncertainty estimates for these predictions. We obtain negative theoretical results on the possibility of approximating deep ensemble weights by  batch ensemble weights, and so turn to distillation. Training a batch ensemble on the outputs of deep ensembles improves accuracy and uncertainty estimates, without requiring hyper-parameter tuning. This result is specific to the choice of batch ensemble architectures: distilling deep ensembles to a single network is unsuccessful, despite single networks having only marginally fewer parameters than batch ensembles."}}
{"id": "xaqKWHcoOGP", "cdate": 1606146131784, "mdate": null, "content": {"title": "Bayesian Neural Network Priors Revisited", "abstract": "Isotropic Gaussian priors are the de facto standard for modern Bayesian neural network inference.\nHowever, such simplistic priors are unlikely to either accurately reflect our true beliefs about the weight distributions, or to give optimal performance.\nWe study summary statistics of (convolutional) neural network weights in networks trained using SGD.\nWe find that in certain circumstances, these networks have heavy-tailed weight distributions, while convolutional neural network weights often display strong spatial correlations.\nBuilding these observations into the respective priors, we get improved performance on MNIST classification.   \nRemarkably, we find that using a more accurate prior partially mitigates the cold posterior effect, by improving performance at high temperatures corresponding to exact Bayesian inference, while having less of an effect at small temperatures."}}
{"id": "KWF4Slxui0s", "cdate": 1603119168241, "mdate": null, "content": {"title": "Bayesian Neural Network Priors Revisited", "abstract": "Isotropic Gaussian priors are the de facto standard for modern Bayesian neural network inference. However, such simplistic priors are unlikely to either accurately reflect our true beliefs about the weight distributions, or to give optimal performance. We study summary statistics of (convolutional) neural network weights in networks trained using SGD. We find that in certain circumstances, these networks have heavy-tailed weight distributions, while convolutional neural network weights often display strong spatial correlations. Building these observations into the respective priors, we get improved performance on MNIST classification. Remarkably, we find that using a more accurate prior partially mitigates the cold posterior effect, by improving performance at high temperatures corresponding to exact Bayesian inference, while having less of an effect at small temperatures."}}
{"id": "IlO4QEQe-Ap", "cdate": 1598859246414, "mdate": null, "content": {"title": "Hyperparameter Ensembles for Robustness and Uncertainty Quantification", "abstract": "Ensembles over neural network weights trained from different random initialization, known as deep ensembles, achieve state-of-the-art accuracy and calibration. The recently introduced batch ensembles provide a drop-in replacement that is more parameter efficient. In this paper, we design ensembles not only over weights, but over hyperparameters to improve the state of the art in both settings. For best performance independent of budget, we propose hyper-deep ensembles, a simple procedure that involves a random search over different hyperparameters, themselves stratified across multiple random initializations. Its strong performance highlights the benefit of combining models with both weight and hyperparameter diversity. We further propose a parameter efficient version, hyper-batch ensembles, which builds on the layer structure of batch ensembles and self-tuning networks. The computational and memory costs of our method are notably lower than typical ensembles. On image classification tasks, with MLP, LeNet, and Wide ResNet 28-10 architectures, our methodology improves upon both deep and batch ensembles."}}
{"id": "Bii4-Xze_6r", "cdate": 1546300800000, "mdate": null, "content": {"title": "Efficient Gaussian Process Classification Using P\u00f3lya-Gamma Data Augmentation.", "abstract": "We propose a scalable stochastic variational approach to GP classification building on P\u00f3lya-Gamma data augmentation and inducing points. Unlike former approaches, we obtain closed-form updates based on natural gradients that lead to efficient optimization. We evaluate the algorithm on real-world datasets containing up to 11 million data points and demonstrate that it is up to two orders of magnitude faster than the state-of-the-art while being competitive in terms of prediction performance."}}
{"id": "ByZVls-uZH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Quasi-Monte Carlo Variational Inference", "abstract": "Many machine learning problems involve Monte Carlo gradient estimators. As a prominent example, we focus on Monte Carlo variational inference (MCVI) in this paper. The performance of MCVI crucially..."}}
