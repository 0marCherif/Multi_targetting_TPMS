{"id": "i8zQLneFvn", "cdate": 1675849635632, "mdate": null, "content": {"title": "Labeling EEG Components with a Bag of Waveforms from Learned Dictionaries", "abstract": "Electroencephalograms (EEGs) are useful for analyzing brain activity, and spatiotemporal patterns in the EEG signal have clinical value, serving for example as biomarkers of diseases such as epilepsy. EEGs are a combination of components from multiple sources within the brain, the electrical activity of muscles, including the heart, and artifacts due to movement and external signals (e.g, line noise). Separating and classifying the sources of these components is important for analyzing the brain patterns in the EEG data. We propose \\textit{bag-of-waves} (BoWav), a new feature for the classification of EEG independent components (ICs). BoWav represents the IC time series through the distribution of counts of waveforms from a learned shift-invariant dictionary based reconstruction. We found that BoWav has a promising predictive performance, outperforming the state-of-the-art method for IC classification, ICLabel, in two of three classes of interest."}}
{"id": "ljTldBQhK8q", "cdate": 1672531200000, "mdate": 1682346299204, "content": {"title": "DiME: Maximizing Mutual Information by a Difference of Matrix-Based Entropies", "abstract": "We introduce an information-theoretic quantity with similar properties to mutual information that can be estimated from data without making explicit assumptions on the underlying distribution. This quantity is based on a recently proposed matrix-based entropy that uses the eigenvalues of a normalized Gram matrix to compute an estimate of the eigenvalues of an uncentered covariance operator in a reproducing kernel Hilbert space. We show that a difference of matrix-based entropies (DiME) is well suited for problems involving the maximization of mutual information between random variables. While many methods for such tasks can lead to trivial solutions, DiME naturally penalizes such outcomes. We compare DiME to several baseline estimators of mutual information on a toy Gaussian dataset. We provide examples of use cases for DiME, such as latent factor disentanglement and a multiview representation learning problem where DiME is used to learn a shared representation among views with high mutual information."}}
{"id": "-PXr3ajKZOE", "cdate": 1640995200000, "mdate": 1682346299365, "content": {"title": "The Representation Jensen-R\u00e9nyi Divergence", "abstract": "We introduce a divergence measure between data distributions based on operators in reproducing kernel Hilbert spaces defined by kernels. The empirical estimator of the divergence is computed using the eigenvalues of positive definite Gram matrices that are obtained by evaluating the kernel over pairs of data points. The new measure shares similar properties to Jensen-Shannon divergence. Convergence of the proposed estimators follows from concentration results based on the difference between the ordered spectrum of the Gram matrices and the integral operators associated with the population quantities. The proposed measure of divergence avoids the estimation of the probability distribution underlying the data. Numerical experiments involving comparing distributions and applications to sampling unbalanced data for classification show that the proposed divergence can achieve state of the art results."}}
{"id": "Wu5hMMQ76OE", "cdate": 1633790969388, "mdate": null, "content": {"title": "Kernel Landmarks: An Empirical Statistical Approach to Detect Covariate Shift", "abstract": "Training an effective predictive model with empirical risk minimization requires a distribution of the  input training data that matches the testing data. Covariate shift can occur when the testing cases are not class-balanced, but the training is. In order to detect when class imbalance is present in a test sample (without labels), we propose to use statistical divergence based on the Wasserstein distance and optimal transport. Recently, slicing techniques have been proposed that provide computational and statistical advantages for the Wasserstein distance for high-dimensional spaces.  In this work we presented a computationally simple approach to perform generalized slicing of the kernel-based Wasserstein distance and apply it as a two-sample test. The proposed landmark-based slicing chooses a single point to be the sole support vector to represent the witness function. We run pseudo-real experiments using the MNIST dataset and compare our method with maximum mean discrepancy (MMD). We have shown that our proposed methods perform better than MMD on these synthetic simulations of covariate shift."}}
{"id": "y_s0M6OtyH_", "cdate": 1633790968407, "mdate": null, "content": {"title": "Identifying the Instances Associated with Distribution Shifts using the Max-Sliced Bures Divergence", "abstract": "We investigate an interpretable approach to compare two distributions. The approach, max-sliced Bures divergence, approximates the max-sliced Wasserstein distance and projects the distributions into a one-dimensional subspace defined by a `slicing' vector. Unlike heuristic algorithms for the max-sliced Wasserstein-2 distance that are not guaranteed to find the optimal slice, we detail a tractable algorithm that finds the global optimal slice and scales to large sample sizes, due to its expression in terms of second moments. However, it is unable to detect changes in higher-order statistics. To overcome this, we explore using a non-linear mapping provided by the internal representation of a pre-trained neural network (Inception Net). Our approach provides an interpretation of the Fr\u00e9chet Inception distance by identifying the instances that are either overrepresented or underrepresented with respect to the other sample. We apply the proposed measure to detect class imbalances and underrepresentation within data sets."}}
{"id": "rRMbaujWoxq", "cdate": 1609459200000, "mdate": 1646103413183, "content": {"title": "The Representation Jensen-R\u00e9nyi Divergence", "abstract": "We introduce a divergence measure between data distributions based on operators in reproducing kernel Hilbert spaces defined by kernels. The empirical estimator of the divergence is computed using the eigenvalues of positive definite Gram matrices that are obtained by evaluating the kernel over pairs of data points. The new measure shares similar properties to Jensen-Shannon divergence. Convergence of the proposed estimators follows from concentration results based on the difference between the ordered spectrum of the Gram matrices and the integral operators associated with the population quantities. The proposed measure of divergence avoids the estimation of the probability distribution underlying the data. Numerical experiments involving comparing distributions and applications to sampling unbalanced data for classification show that the proposed divergence can achieve state of the art results."}}
{"id": "S6v-6uoZjxq", "cdate": 1609459200000, "mdate": 1646103413197, "content": {"title": "Searching for waveforms on spatially-filtered epileptic ECoG", "abstract": "Seizures are one of the defining symptoms in patients with epilepsy, and due to their unannounced occurrence, they can pose a severe risk for the individual that suffers it. New research efforts are showing a promising future for the prediction and preemption of imminent seizures, and with those efforts, a vast and diverse set of features have been proposed for seizure prediction algorithms. However, the data-driven discovery of nonsinusoidal waveforms for seizure prediction is lacking in the literature, which is in stark contrast with recent works that show the close connection between the waveform morphology of neural oscillations and the physiology and pathophysiology of the brain, and especially its use in effectively discriminating between normal and abnormal oscillations in electrocorticographic (ECoG) recordings of epileptic patients. Here, we explore a scalable, energy-guided waveform search strategy on spatially-projected continuous multi-day ECoG data sets. Our work shows that data-driven waveform learning methods have the potential to not only contribute features with predictive power for seizure prediction, but also to facilitate the discovery of oscillatory patterns that could contribute to our understanding of the pathophysiology and etiology of seizures."}}
{"id": "HSebado-jxc", "cdate": 1609459200000, "mdate": 1646103413185, "content": {"title": "Shift-invariant waveform learning on epileptic ECoG", "abstract": "Seizure detection algorithms must discriminate abnormal neuronal activity associated with a seizure from normal neural activity in a variety of conditions. Our approach is to seek spatiotemporal waveforms with distinct morphology in electrocorticographic (ECoG) recordings of epileptic patients that are indicative of a subsequent seizure (preictal) versus non-seizure segments (interictal). To find these waveforms we apply a shift-invariant k-means algorithm to segments of spatially filtered signals to learn codebooks of prototypical waveforms. The frequency of the cluster labels from the codebooks is then used to train a binary classifier that predicts the class (preictal or interictal) of a test ECoG segment. We use the Matthews correlation coefficient to evaluate the performance of the classifier and the quality of the codebooks. We found that our method finds recurrent non-sinusoidal waveforms that could be used to build interpretable features for seizure prediction and that are also physiologically meaningful."}}
{"id": "HNg-p_iWox9", "cdate": 1609459200000, "mdate": 1646103413187, "content": {"title": "Shift-invariant waveform learning on epileptic ECoG", "abstract": "Seizure detection algorithms must discriminate abnormal neuronal activity associated with a seizure from normal neural activity in a variety of conditions. Our approach is to seek spatiotemporal waveforms with distinct morphology in electrocorticographic (ECoG) recordings of epileptic patients that are indicative of a subsequent seizure (preictal) versus non-seizure segments (interictal). To find these waveforms we apply a shift-invariant k-means algorithm to segments of spatially filtered signals to learn codebooks of prototypical waveforms. The frequency of the cluster labels from the codebooks is then used to train a binary classifier that predicts the class (preictal or interictal) of a test ECoG segment. We use the Matthews correlation coefficient to evaluate the performance of the classifier and the quality of the codebooks. We found that our method finds recurrent non-sinusoidal waveforms that could be used to build interpretable features for seizure prediction and that are also physiologically meaningful."}}
{"id": "Bbbpui-jec", "cdate": 1609459200000, "mdate": 1646103413181, "content": {"title": "Local and Sparse Linear Causal Models for fMRI Resting-State Signals", "abstract": "Modeling the human brain as a complex network (operating at many different scales) is a powerful tool to analyze both its structural and functional connections. Neuroimaging techniques, such as fMRI, capture the metabolic response to neural activity within voxels encompassing hundreds of thousands of neurons. Graph theory and graph signal processing provide a principled methodology to analyze the brain's functional interactions evidenced by the spatiotemporal patterns revealed by the neuroimaging. In this paper, we propose a methodology to identify a linear, first-order auto-regressive model describing the causal dependence among the fMRI activity on a subset of voxels. We assume the matrix of linear coefficients capturing the voxel-to-voxel interactions is a sum of two components: one low rank and one sparse. The low-rank component represents the dense local connections and the sparse component represents long-range connections that mediate or coordinate disparate brain regions. To enforce the dense connections to be local we use prior knowledge about the spatial proximity of voxels. We apply the proposed methodology on synthetic data and fMRI data captured during resting state. Our results show that the proposed methodology is able to capture causal structure explaining the variance of the resting-state activity. In particular, our methodology can predict intra-subject resting-state activity across different sessions (test-retest reliability)."}}
