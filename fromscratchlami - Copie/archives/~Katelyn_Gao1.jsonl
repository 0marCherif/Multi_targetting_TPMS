{"id": "fFvo-H_5XTN", "cdate": 1640995200000, "mdate": 1660248341008, "content": {"title": "Generalizing Gaussian Smoothing for Random Search", "abstract": "Gaussian smoothing (GS) is a derivative-free optimization (DFO) algorithm that estimates the gradient of an objective using perturbations of the current parameters sampled from a standard normal di..."}}
{"id": "exSkmMyuld", "cdate": 1577836800000, "mdate": null, "content": {"title": "Modeling and Optimization Trade-off in Meta-learning", "abstract": "By searching for shared inductive biases across tasks, meta-learning promises to accelerate learning on novel tasks, but with the cost of solving a complex bilevel optimization problem. We introduce and rigorously define the trade-off between accurate modeling and optimization ease in meta-learning. At one end, classic meta-learning algorithms account for the structure of meta-learning but solve a complex optimization problem, while at the other end domain randomized search (otherwise known as joint training) ignores the structure of meta-learning and solves a single level optimization problem. Taking MAML as the representative meta-learning algorithm, we theoretically characterize the trade-off for general non-convex risk functions as well as linear regression, for which we are able to provide explicit bounds on the errors associated with modeling and optimization. We also empirically study this trade-off for meta-reinforcement learning benchmarks."}}
{"id": "BklYixAN2B", "cdate": 1574391968992, "mdate": null, "content": {"title": "Efficient moment calculations for variance components in large unbalanced crossed random effects models", "abstract": "Large crossed data sets, often modeled by generalized linear mixed models, have become increasingly common and provide challenges for statistical analysis. At very large sizes it becomes desirable to have the computational costs of estimation, inference and prediction (both space and time) grow at most linearly with sample size. Both traditional maximum likelihood estimation and numerous Markov chain Monte Carlo Bayesian algorithms take superlinear time in order to obtain good parameter estimates in the simple two-factor crossed random effects model. We propose moment based algorithms that, with at most lin- ear cost, estimate variance components, measure the uncertainties of those estimates, and generate shrinkage based predictions for missing observa- tions. When run on simulated normally distributed data, our algorithm performs competitively with maximum likelihood methods."}}
{"id": "r1xh8e0N2S", "cdate": 1574391892405, "mdate": null, "content": {"title": "Estimation and inference for very large linear mixed effects models", "abstract": "Linear mixed models with large imbalanced crossed random effects structures pose severe computational problems for maximum likeli- hood estimation and for Bayesian analysis. The costs can grow as fast as N^3/2 when there are N observations. Such problems arise in any setting where the underlying factors satisfy a many to many relationship (instead of a nested one) and in electronic commerce applications, the N can be quite large. Methods that do not account for the correlation structure can greatly underestimate uncertainty. We propose a method of moments approach that takes account of the correlation structure and that can be computed at O(N) cost. The method of moments is very amenable to parallel computa- tion and it does not require parametric distributional assumptions, tuning parameters or convergence diagnostics. For the regression coefficients, we give conditions for consistency and asymptotic normality as well as a consis- tent variance estimate. For the variance components, we give conditions for consistency and we use consistent estimates of a mildly conservative vari- ance estimate. All of these computations can be done in O(N) work. We illustrate the algorithm with some data from Stitch Fix where the crossed random effects correspond to clients and items."}}
{"id": "rylKB3A9Fm", "cdate": 1538088000857, "mdate": null, "content": {"title": "Assessing Generalization in Deep Reinforcement Learning", "abstract": "Deep reinforcement learning (RL) has achieved breakthrough results on many tasks, but has been shown to be sensitive to system changes at test time. As a result, building deep RL agents that generalize has become an active research area. Our aim is to catalyze and streamline community-wide progress on this problem by providing the first benchmark and a common experimental protocol for investigating generalization in RL. Our benchmark contains a diverse set of environments and our evaluation methodology covers both in-distribution and out-of-distribution generalization. To provide a set of baselines for future research, we conduct a systematic evaluation of state-of-the-art algorithms, including those that specifically tackle the problem of generalization. The experimental results indicate that in-distribution generalization may be within the capacity of current algorithms, while out-of-distribution generalization is an exciting challenge for future work."}}
{"id": "fLD8QAZjtv9", "cdate": 1420070400000, "mdate": null, "content": {"title": "Online One-Class SVMs with Active-Set Optimization for Data Streams", "abstract": "A great advantage of support vector machines (SVMs) is its capability to learn decision borders, represented by a set of particular data points called margin support vectors. The real-time or nearly real-time online learning and detection from data streams poses stringent time and space constraints for the learner. We consider solving online one-class SVMs with an active-set method for quadratic programming (QP). At each iteration, the problem size is the size of the estimated support vectors so far. Active-set programming has the nice property that the solution of a previous problem can serve as a warm start of the next and computation time can thereby be greatly reduced. In general, finding a good warm-start point is difficult. We propose a method to find a good warm start by exploiting the structure of the SVM optimization problem."}}
