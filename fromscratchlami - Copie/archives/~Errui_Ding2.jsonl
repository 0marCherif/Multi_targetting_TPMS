{"id": "zO_zLnhYRER", "cdate": 1681698883287, "mdate": 1681698883287, "content": {"title": "The Devil is in the Task: Exploiting Reciprocal Appearance-Localization Features for Monocular 3D Object Detection", "abstract": "Low-cost monocular 3D object detection plays a fundamental role in autonomous driving, whereas its accuracy is still far from satisfactory. In this paper, we dig into the 3D object detection task and reformulate it as the sub-tasks of object localization and appearance perception, which benefits to a deep excavation of reciprocal information underlying the entire task. We introduce a Dynamic Feature Reflecting Network, named DFR-Net, which contains two novel standalone modules: (i) the Appearance-Localization Feature Reflecting module (ALFR) that first separates taskspecific features and then self-mutually reflects the reciprocal features; (ii) the Dynamic Intra-Trading module (DIT) that adaptively realigns the training processes of various sub-tasks via a self-learning manner. Extensive experiments on the challenging KITTI dataset demonstrate the effectiveness and generalization of DFR-Net. We rank 1st among all the monocular 3D object detectors in the KITTI test set (till March 16th, 2021). The proposed method is also easy to be plug-and-play in many cutting-edge 3D detection frameworks at negligible cost to boost"}}
{"id": "e_ZX40rJjt_", "cdate": 1681698618197, "mdate": 1681698618197, "content": {"title": "Paint and Distill: Boosting 3D Object Detection with Semantic Passing Network", "abstract": "3D object detection task from lidar or camera sensors is essential for autonomous driving. Pioneer attempts at multi-modality fusion\ncomplement the sparse lidar point clouds with rich semantic texture information from images at the cost of extra network designs and overhead. In this work, we propose a novel semantic passing framework, named SPNet, to boost the performance of existing lidar-based 3D detection models with the guidance of rich context painting, with no extra computation cost during inference. Our key design is to first exploit the potential instructive semantic knowledge within the ground-truth labels by training a semantic-painted teacher model and then guide the pure-lidar network to learn the semantic-painted representation via knowledge passing modules at different granularities: class-wise passing, pixel-wise passing and instance-wise passing. Experimental results show that the proposed SPNet can seamlessly cooperate with most existing 3D detection frameworks with 1\u223c5% AP gain and even achieve new state-of-theart 3D detection performance on the KITTI test benchmark. Code is available at: https://github.com/jb892/SPNet"}}
{"id": "O7qGCOrgQoN", "cdate": 1668517255828, "mdate": 1668517255828, "content": {"title": "Image Inpainting with Learnable Bidirectional Attention Maps", "abstract": "Most convolutional network (CNN)-based inpainting methods adopt standard convolution to indistinguishably treat valid pixels and holes, making them limited in handling irregular holes and more likely to generate inpainting results with color discrepancy and blurriness. Partial\nconvolution has been suggested to address this issue, but it adopts handcrafted feature re-normalization, and only considers forward mask-updating. In this paper, we present a learnable attention map module for learning feature re-normalization and mask-updating in an end-to-end manner, which is effective in adapting to irregular holes and propagation of convolution layers. Furthermore, learnable reverse attention maps are introduced to allow the decoder of U-Net to concentrate on filling in irregular holes instead of reconstructing both holes and known regions, resulting in our learnable bidirectional attention maps. Qualitative and quantitative experiments show that our method performs favorably against state-of-the-arts in generating sharper, more coherent and visually plausible inpainting results. The source code and pre-trained models will be available at: https://github.com/Vious/LBAM_inpainting/.\n"}}
{"id": "w9aS8WVXXaf", "cdate": 1667357900567, "mdate": 1667357900567, "content": {"title": "Group DETR: Fast DETR Training with Group-Wise One-to-Many Assignment", "abstract": "Detection Transformer (DETR) relies on One-to-One assignment, i.e., assigning one ground-truth object to only one positive object query, for end-to-end object detection and lacks the capability of exploiting multiple positive object queries. We present a novel DETR training approach, named {\\em Group DETR}, to support Group-wise One-to-Many assignment. We make simple modifications during training: (i) adopt K groups of object queries; (ii) conduct decoder self-attention on each group of object queries with the same parameters; (iii) perform One-to-One label assignment for each group, leading to K positive object queries for each ground-truth object. In inference, we only use one group of object queries, making no modifications to DETR architecture and processes. We validate the effectiveness of the proposed approach on DETR variants, including Conditional DETR, DAB-DETR, DN-DETR, and DINO. Code will be available."}}
{"id": "cKGZQ3EoyJT", "cdate": 1667357721076, "mdate": 1667357721076, "content": {"title": "MixFormer: Mixing Features across Windows and Dimensions", "abstract": "While local-window self-attention performs notably in\nvision tasks, it suffers from limited receptive field and weak\nmodeling capability issues. This is mainly because it performs self-attention within non-overlapped windows and\nshares weights on the channel dimension. We propose MixFormer to find a solution. First, we combine local-window\nself-attention with depth-wise convolution in a parallel design, modeling cross-window connections to enlarge the receptive fields. Second, we propose bi-directional interactions across branches to provide complementary clues in\nthe channel and spatial dimensions. These two designs are\nintegrated to achieve efficient feature mixing among windows and dimensions. Our MixFormer provides competitive results on image classification with EfficientNet and\nshows better results than RegNet and Swin Transformer.\nPerformance in downstream tasks outperforms its alternatives by significant margins with less computational costs\nin 5 dense prediction tasks on MS COCO, ADE20k, and\nLVIS. Code is available at https://github.com/\nPaddlePaddle/PaddleClas."}}
{"id": "q23TzayryBG", "cdate": 1663849973478, "mdate": null, "content": {"title": "MAFormer: A Transformer Network with Multi-scale Attention Fusion for Visual Recognition", "abstract": "Vision Transformer and its variants have demonstrated great potential in various computer vision tasks. But conventional vision transformers often focus on global dependency at a coarse level, which suffer from a learning challenge on global relationships and fine-grained representation at a token level. In this paper, we introduce Multi-scale Attention Fusion into transformer (MAFormer), which explores local aggregation and global feature extraction in a dual-stream framework for visual recognition. We develop a simple but effective module to explore the full potential of transformers for visual representation by learning fine-grained and coarse-grained features at a token level and dynamically fusing them. Our Multi-scale Attention Fusion (MAF) block consists of: i) a local window attention branch that learns short-range interactions within windows, aggregating fine-grained local features; ii) global feature extraction through a novel Global Learning with Down-sampling (GLD) operation to efficiently capture long-range context information within the whole image; iii) a fusion module that self-explores the integration of both features via attention. MAFormer achieves state-of-the-art performance on common vision tasks. In particular, MAFormer-L achieves 85.9$\\%$ Top-1 accuracy on ImageNet, surpassing CSWin-B and LV-ViT-L by 1.7$\\%$ and 0.6$\\%$ respectively. On MSCOCO, MAFormer outperforms the prior art CSWin by 1.7$\\%$  mAPs on object detection and 1.4$\\%$  on instance segmentation with similar-sized parameters, demonstrating the potential to be a general backbone network."}}
{"id": "HE_75XY5Ljh", "cdate": 1663849912359, "mdate": null, "content": {"title": "StrucTexTv2: Masked Visual-Textual Prediction for Document Image Pre-training", "abstract": "In this paper, we present StrucTexTv2, an effective document image pre-training framework, by performing masked visual-textual prediction. It consists of two self-supervised pre-training tasks: masked image modeling and masked language modeling, based on text region-level image masking. The proposed method randomly masks some image regions according to the bounding box coordinates of text words. The objectives of our pre-training tasks are reconstructing the pixels of masked image regions and the corresponding masked tokens simultaneously. Hence the pre-trained encoder can capture more textual semantics in comparison to the masked image modeling that usually predicts the masked image patches. Compared to the masked multi-modal modeling methods for document image understanding that rely on both the image and text modalities, StrucTexTv2 models image-only input and potentially deals with more application scenarios free from OCR pre-processing. Extensive experiments on mainstream benchmarks of document image understanding demonstrate the effectiveness of StrucTexTv2. It achieves competitive or even new state-of-the-art performance in various downstream tasks such as image classification, layout analysis, table structure recognition, document OCR, and information extraction under the end-to-end scenario."}}
{"id": "PLUXnnxUdr4", "cdate": 1663849864502, "mdate": null, "content": {"title": "Graph Contrastive Learning for Skeleton-based Action Recognition", "abstract": "In the field of skeleton-based action recognition, current top-performing graph convolutional networks (GCNs) exploit intra-sequence context to construct adaptive graphs for feature aggregation. However, we argue that such context is still $\\textit{local}$ since the rich cross-sequence relations have not been explicitly investigated. In this paper, we propose a graph contrastive learning framework for skeleton-based action recognition ($\\textit{SkeletonGCL}$) to explore the $\\textit{global}$ context across all sequences. In specific, SkeletonGCL associates graph learning across sequences by enforcing graphs to be class-discriminative, i.e., intra-class compact and inter-class dispersed, which improves the GCN capacity to distinguish various action patterns. Besides, two memory banks are designed to enrich cross-sequence context from two complementary levels, i.e., instance and semantic levels, enabling graph contrastive learning in multiple context scales. Consequently, SkeletonGCL establishes a new training paradigm, and it can be seamlessly incorporated into current GCNs. Without loss of generality, we combine SkeletonGCL with three GCNs (2S-ACGN, CTR-GCN, and InfoGCN), and achieve consistent improvements on NTU60, NTU120, and NW-UCLA benchmarks. "}}
{"id": "ptbePrczhRt", "cdate": 1663849811949, "mdate": null, "content": {"title": "Group DETR: Fast DETR Training with Group-Wise One-to-Many Assignment", "abstract": "Detection Transformer (DETR) relies on one-to-one assignment for end-to-end object detection and lacks the capability of exploiting multiple positive object queries. We present a novel DETR training approach, named {\\em Group DETR}, to support one-to-many assignment in a group-wise manner. To achieve it, we make simple modifications during training: (i) adopt $K$ groups of object queries; (ii) conduct decoder self-attention on each group of object queries with the same parameters; (iii) perform one-to-one assignment for each group, leading to $K$ positive object queries for each ground-truth object. In inference, we only use one group of object queries, making no modifications to model architectures and inference processes. Group DETR is a versatile training method and is applicable to various DETR variants. Our experiments show that Group DETR significantly speeds up the training convergences and improves the performances of various DETR-based methods."}}
{"id": "kMiL9hWbD1z", "cdate": 1652737557015, "mdate": null, "content": {"title": "RTFormer: Efficient Design for Real-Time Semantic Segmentation with Transformer", "abstract": "Recently, transformer-based networks have shown impressive results in semantic segmentation. Yet for real-time semantic segmentation, pure CNN-based approaches still dominate in this field, due to the time-consuming computation mechanism of transformer. We propose RTFormer, an efficient dual-resolution transformer for real-time semantic segmenation, which achieves better trade-off between performance and efficiency than CNN-based models. To achieve high inference efficiency on GPU-like devices, our RTFormer leverages GPU-Friendly Attention with linear complexity and discards the multi-head mechanism. Besides, we find that cross-resolution attention is more efficient to gather global context information for high-resolution branch by spreading the high level knowledge learned from low-resolution branch. Extensive experiments on mainstream benchmarks demonstrate the effectiveness of our proposed RTFormer, it achieves state-of-the-art on Cityscapes, CamVid and COCOStuff, and shows promising results on ADE20K."}}
