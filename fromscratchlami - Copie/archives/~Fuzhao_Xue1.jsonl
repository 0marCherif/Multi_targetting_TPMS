{"id": "gb6VM_pTd5E", "cdate": 1682227363265, "mdate": null, "content": {"title": "ParaGAN: A Cloud Training Framework for Generative Adversarial Networks", "abstract": "Generative Adversarial Network (GAN) has shown tremendous success in synthesizing realistic photos and videos in recent years. However, training GAN to convergence is still a challenging task that requires significant computing power and is subject to training instability. To address these challenges, we propose ParaGAN, a cloud training framework for GAN optimized from both system and numerical perspectives. To achieve this, ParaGAN implements a congestion-aware pipeline for latency hiding, hardware-aware layout transformation for improved accelerator utilization, and an asynchronous update scheme to optimize system performance. Additionally, from a numerical perspective, we introduce an asymmetric optimization policy to stabilize training. Our preliminary experiments show that ParaGAN reduces the training time of BigGAN from 15 days to just 14 hours on 1024 TPUs, achieving 91\\% scaling efficiency. Moreover, we demonstrate that ParaGAN enables the generation of unprecedented high-resolution ($1024\\times1024$) images on BigGAN."}}
{"id": "1PW_txDkX7", "cdate": 1677713807787, "mdate": null, "content": {"title": "One Student Knows All Experts Know: From Sparse to Dense", "abstract": "Human education system trains one student by multiple experts. Mixture-of-experts (MoE) is a powerful sparse architecture including multiple experts. However, sparse MoE model is easy to overfit, hard to deploy, and not hardware-friendly for practitioners. In this work, inspired by the human education model, we propose a novel task, knowledge integration, to obtain a dense student model (OneS) as knowledgeable as one sparse MoE. We investigate this task by exploring 4 different ways to gather knowledge from MoE to initialize a dense student model, and we then refine the dense student by knowledge distillation. We evaluate our model on both vision and language tasks. Experimental results show, with $3.7 \\times$ inference speedup, the dense student can still preserve $88.2\\%$ benefits from MoE counterpart."}}
{"id": "Peb3QdR8zzP", "cdate": 1677713800890, "mdate": null, "content": {"title": "Hierarchical Dialogue Understanding with Special Tokens and Turn-level Attention", "abstract": "Compared with standard text, understanding dialogue is more challenging for machines as the dynamic and unexpected semantic changes in each turn. To model such inconsistent semantics, we propose a simple but effective Hierarchical Dialogue Understanding model, HiDialog. Specifically, we first insert multiple special tokens into a dialogue and propose the turn-level attention to learn turn embeddings hierarchically. Then, a heterogeneous graph module is leveraged to polish the learned embeddings. We evaluate our model on various dialogue understanding tasks including dialogue relation extraction, dialogue emotion recognition, and dialogue act classification. Results show that our simple approach achieves state-of-the-art performance on all three tasks above. All our source code is publicly available at https://github.com/ShawX825/HiDialog. "}}
{"id": "Z7DlKK-2Rug", "cdate": 1668043406191, "mdate": 1668043406191, "content": {"title": "Modeling Motion with Multi-Modal Features for Text-Based Video Segmentation", "abstract": "Text-based video segmentation aims to segment the target object in a video based on a describing sentence. Incorporating motion information from optical flow maps with\nappearance and linguistic modalities is crucial yet has been\nlargely ignored by previous work. In this paper, we design a\nmethod to fuse and align appearance, motion, and linguistic\nfeatures to achieve accurate segmentation. Specifically, we\npropose a multi-modal video transformer, which can fuse\nand aggregate multi-modal and temporal features between\nframes. Furthermore, we design a language-guided feature\nfusion module to progressively fuse appearance and motion\nfeatures in each feature level with guidance from linguistic\nfeatures. Finally, a multi-modal alignment loss is proposed\nto alleviate the semantic gap between features from different\nmodalities. Extensive experiments on A2D Sentences and\nJ-HMDB Sentences verify the performance and the generalization ability of our method compared to the state-of-theart methods"}}
{"id": "FkRMv-mlSTy", "cdate": 1663850543734, "mdate": null, "content": {"title": "Adaptive Computation with Elastic Input Sequence", "abstract": "When solving a problem, human beings have the adaptive ability in terms of the type of information they use, the procedure they take, and the amount of time they spend approaching and solving the problem. However, most standard neural networks have the same function type and fixed computation budget on different samples regardless of their nature and difficulty. Adaptivity is a powerful paradigm as it not only imbues practitioners with flexibility pertaining to the downstream usage of these models but can also serve as a powerful inductive bias for solving certain challenging classes of problems. In this work, we propose a new strategy, AdaTape, that enables dynamic computation in neural networks via adaptive tape tokens. AdaTape employs an elastic input sequence by equipping an existing architecture with a dynamic read and write tape. Specifically, we adaptively generate input sequences using tape tokens obtained from a tape bank that can either be trainable or generated from input data. We analyze the challenges and requirements to obtain dynamic sequence content and length, and propose the Adaptive Tape Reader (ATR) algorithm to achieve both objectives. Via extensive experiments on image recognition tasks, we show that AdaTape can achieve better performance while maintaining the computational cost."}}
{"id": "vZ4kWo05wOR", "cdate": 1640995200000, "mdate": 1668592973781, "content": {"title": "Deeper vs Wider: A Revisit of Transformer Configuration", "abstract": "Transformer-based models have delivered impressive results on many tasks, particularly vision and language tasks. In many model training situations, conventional configurations are typically adopted. For example, we often set the base model with hidden dimensions (i.e. model width) to be 768 and the number of transformer layers (i.e. model depth) to be 12. In this paper, we revisit these conventional configurations. Through theoretical analysis and experimental evaluation, we show that the masked autoencoder is effective in alleviating the over-smoothing issue in deep transformer training. Based on this finding, we propose Bamboo, an idea of using deeper and narrower transformer configurations, for masked autoencoder training. On ImageNet, with such a simple change in configuration, re-designed model achieves 87.1% top-1 accuracy and outperforms SoTA models like MAE and BEiT. On language tasks, re-designed model outperforms BERT with default setting by 1.1 points on average, on GLUE datasets."}}
{"id": "uwA7BCmPdd", "cdate": 1640995200000, "mdate": 1668592973896, "content": {"title": "RACP: A network with attention corrected prototype for few-shot speaker recognition using indefinite distance metric", "abstract": ""}}
{"id": "lKFhJecHxRg", "cdate": 1640995200000, "mdate": 1668592973772, "content": {"title": "One Student Knows All Experts Know: From Sparse to Dense", "abstract": "Human education system trains one student by multiple experts. Mixture-of-experts (MoE) is a powerful sparse architecture including multiple experts. However, sparse MoE model is easy to overfit, hard to deploy, and not hardware-friendly for practitioners. In this work, inspired by the human education model, we propose a novel task, knowledge integration, to obtain a dense student model (OneS) as knowledgeable as one sparse MoE. We investigate this task by proposing a general training framework including knowledge gathering and knowledge distillation. Specifically, to gather key knowledge from different pre-trained experts, we first investigate four different possible knowledge gathering methods, \\ie summation, averaging, Top-K Knowledge Gathering (Top-KG), and Singular Value Decomposition Knowledge Gathering (SVD-KG) proposed in this paper. We then refine the dense student model by knowledge distillation to offset the noise from gathering. On ImageNet, our OneS preserves $61.7\\%$ benefits from MoE and achieves $78.4\\%$ top-1 accuracy ImageNet with only $15$M parameters. On four natural language processing datasets, OneS obtains $88.2\\%$ MoE benefits and outperforms the best baseline by $51.7\\%$ using the same architecture and training data. In addition, compared with the MoE counterpart, OneS can achieve $3.7 \\times$ inference speedup due to less computation and hardware-friendly architecture."}}
{"id": "XrZ5aVaMxB", "cdate": 1640995200000, "mdate": 1668592973897, "content": {"title": "CowClip: Reducing CTR Prediction Model Training Time from 12 hours to 10 minutes on 1 GPU", "abstract": "The click-through rate (CTR) prediction task is to predict whether a user will click on the recommended item. As mind-boggling amounts of data are produced online daily, accelerating CTR prediction model training is critical to ensuring an up-to-date model and reducing the training cost. One approach to increase the training speed is to apply large batch training. However, as shown in computer vision and natural language processing tasks, training with a large batch easily suffers from the loss of accuracy. Our experiments show that previous scaling rules fail in the training of CTR prediction neural networks. To tackle this problem, we first theoretically show that different frequencies of ids make it challenging to scale hyperparameters when scaling the batch size. To stabilize the training process in a large batch size setting, we develop the adaptive Column-wise Clipping (CowClip). It enables an easy and effective scaling rule for the embeddings, which keeps the learning rate unchanged and scales the L2 loss. We conduct extensive experiments with four CTR prediction networks on two real-world datasets and successfully scaled 128 times the original batch size without accuracy loss. In particular, for CTR prediction model DeepFM training on the Criteo dataset, our optimization framework enlarges the batch size from 1K to 128K with over 0.1% AUC improvement and reduces training time from 12 hours to 10 minutes on a single V100 GPU. Our code locates at https://github.com/bytedance/LargeBatchCTR."}}
{"id": "SbQEar2sDIM", "cdate": 1640995200000, "mdate": 1668592973807, "content": {"title": "Modeling Motion with Multi-Modal Features for Text-Based Video Segmentation", "abstract": "Text-based video segmentation aims to segment the target object in a video based on a describing sentence. Incorporating motion information from optical flow maps with appearance and linguistic modalities is crucial yet has been largely ignored by previous work. In this paper, we design a method to fuse and align appearance, motion, and linguistic features to achieve accurate segmentation. Specifically, we propose a multi-modal video transformer, which can fuse and aggregate multi-modal and temporal features between frames. Furthermore, we design a language-guided feature fusion module to progressively fuse appearance and motion features in each feature level with guidance from linguistic features. Finally, a multi-modal alignment loss is proposed to alleviate the semantic gap between features from different modalities. Extensive experiments on A2D Sentences and J-HMDB Sentences verify the performance and the generalization ability of our method compared to the state-of-the-art methods."}}
