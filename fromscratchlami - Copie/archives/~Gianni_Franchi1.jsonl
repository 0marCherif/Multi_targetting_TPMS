{"id": "LYz2bFSFV7", "cdate": 1668594639342, "mdate": 1668594639342, "content": {"title": "Learning deep morphological networks with neural architecture search", "abstract": "Deep Neural Networks (DNNs) are generated by sequentially performing linear and non-linear processes. The combination of linear and non-linear procedures is critical for generating a sufficiently deep feature space. Most non-linear operators are derivations of activation functions or pooling functions. Mathematical morphology is a branch of mathematics that provides non-linear operators for various image processing problems. This paper investigates the utility of integrating these operations into an end-to-end deep learning framework. DNNs are designed to acquire a realistic representation for a particular job. Morphological operators give topological descriptors that convey salient information about the shapes of objects depicted in images. We propose a method based on meta-learning to incorporate morphological operators into DNNs. The learned architecture demonstrates how our novel morphological operations significantly increase DNN performance on various tasks, including picture classification, edge detection, and semantic segmentation. Our codes are available at https://nao-morpho.github.io/."}}
{"id": "uH8SZnNGbY", "cdate": 1668594444839, "mdate": 1668594444839, "content": {"title": "Deep morphological networks", "abstract": "Mathematical morphology provides powerful nonlinear operators for a variety of image processing tasks such as filtering, segmentation, and edge detection. In this paper, we propose a way to use these nonlinear operators in an end-to-end deep learning framework and illustrate them on different applications. We demonstrate on various examples that new layers making use of the morphological non-linearities are complementary to convolution layers. These new layers can be used to integrate the non-linear operations and pooling into a joint operation. We finally enhance results obtained in boundary detection using this new family of layers with just 0.01% of the parameters of competing state-of-the-art methods."}}
{"id": "3tMyzeoDK1", "cdate": 1668594355882, "mdate": 1668594355882, "content": {"title": "TRADI: Tracking deep neural network weight distributions", "abstract": "During training, the weights of a Deep Neural Network (DNN) are optimized from a random initialization towards a nearly optimum\nvalue minimizing a loss function. Only this final state of the weights is typically kept for testing, while the wealth of information on the geometry of the weight space, accumulated over the descent towards the minimum is discarded. In this work we propose to make use of this knowledge and leverage it for computing the distributions of the weights of the DNN. This can be further used for estimating the epistemic uncertainty of the DNN by aggregating predictions from an ensemble of networks sampled from these distributions. To this end we introduce a method for tracking the trajectory of the weights during optimization, that does neither require any change in the architecture, nor in the training procedure. We evaluate our method, TRADI, on standard classification and regression benchmarks, and on out-of-distribution detection for classification and semantic segmentation. We achieve competitive results, while preserving computational efficiency in comparison to ensemble approaches."}}
{"id": "NzvLxXGYLm", "cdate": 1668594204311, "mdate": 1668594204311, "content": {"title": "Latent Discriminant deterministic Uncertainty", "abstract": " Predictive uncertainty estimation is essential for deploying Deep Neural Networks in real-world autonomous systems. However, most\nsuccessful approaches are computationally intensive. In this work, we attempt to address these challenges in the context of autonomous driving perception tasks. Recently proposed Deterministic Uncertainty Methods (DUM) can only partially meet such requirements as their scalability to complex computer vision tasks is not obvious. In this work we advance a scalable and effective DUM for high-resolution semantic segmentation, that relaxes the Lipschitz constraint typically hindering practicality of such architectures. We learn a discriminant latent space by leveraging a distinction maximization layer over an arbitrarily-sized set of trainable prototypes. Our approach achieves competitive results over Deep Ensembles, the state of the art for uncertainty prediction, on image classification, segmentation and monocular depth estimation tasks. Our code is available at https://github.com/ENSTA-U2IS/LDU\n.\n"}}
{"id": "XXTyv1zD9zD", "cdate": 1663849952926, "mdate": null, "content": {"title": "Packed Ensembles for efficient uncertainty estimation", "abstract": "Deep Ensembles (DE) are a prominent approach for achieving excellent performance on key metrics such as accuracy, calibration, uncertainty estimation, and out-of-distribution detection. However, hardware limitations of real-world systems constrain to smaller ensembles and lower-capacity networks, significantly deteriorating their performance and properties. We introduce Packed-Ensembles (PE), a strategy to design and train lightweight structured ensembles by carefully modulating the dimension of their encoding space. We leverage grouped convolutions to parallelize the ensemble into a single shared backbone and forward pass to improve training and inference speeds. PE is designed to operate within the memory limits of a standard neural network. Our extensive research indicates that PE accurately preserves the properties of DE, such as diversity, and performs equally well in terms of accuracy, calibration, out-of-distribution detection, and robustness to distribution shift. We make our code available at https://github.com/ENSTA-U2IS/torch-uncertainty."}}
{"id": "zBbBJ5WQpCf", "cdate": 1640995200000, "mdate": 1668610852192, "content": {"title": "Greybox XAI: a Neural-Symbolic learning framework to produce interpretable predictions for image classification", "abstract": "Although Deep Neural Networks (DNNs) have great generalization and prediction capabilities, their functioning does not allow a detailed explanation of their behavior. Opaque deep learning models are increasingly used to make important predictions in critical environments, and the danger is that they make and use predictions that cannot be justified or legitimized. Several eXplainable Artificial Intelligence (XAI) methods that separate explanations from machine learning models have emerged, but have shortcomings in faithfulness to the model actual functioning and robustness. As a result, there is a widespread agreement on the importance of endowing Deep Learning models with explanatory capabilities so that they can themselves provide an answer to why a particular prediction was made. First, we address the problem of the lack of universal criteria for XAI by formalizing what an explanation is. We also introduced a set of axioms and definitions to clarify XAI from a mathematical perspective. Finally, we present the Greybox XAI, a framework that composes a DNN and a transparent model thanks to the use of a symbolic Knowledge Base (KB). We extract a KB from the dataset and use it to train a transparent model (i.e., a logistic regression). An encoder-decoder architecture is trained on RGB images to produce an output similar to the KB used by the transparent model. Once the two models are trained independently, they are used compositionally to form an explainable predictive model. We show how this new architecture is accurate and explainable in several datasets."}}
{"id": "smu-XbsWIbk", "cdate": 1640995200000, "mdate": 1669128783287, "content": {"title": "Greybox XAI: A Neural-Symbolic learning framework to produce interpretable predictions for image classification", "abstract": ""}}
{"id": "qEqoLWe0P9r", "cdate": 1640995200000, "mdate": 1668610852202, "content": {"title": "Learning deep morphological networks with neural architecture search", "abstract": ""}}
{"id": "nVU_kTjfnh", "cdate": 1640995200000, "mdate": 1668610852191, "content": {"title": "A study of deep perceptual metrics for image quality assessment", "abstract": "Several metrics exist to quantify the similarity between images, but they are inefficient when it comes to measure the similarity of highly distorted images. In this work, we propose to empirically investigate perceptual metrics based on deep neural networks for tackling the Image Quality Assessment (IQA) task. We study deep perceptual metrics according to different hyperparameters like the network's architecture or training procedure. Finally, we propose our multi-resolution perceptual metric (MR-Perceptual), that allows us to aggregate perceptual information at different resolutions and outperforms standard perceptual metrics on IQA tasks with varying image deformations. Our code is available at https://github.com/ENSTA-U2IS/MR_perceptual"}}
{"id": "g0dxNL_vik", "cdate": 1640995200000, "mdate": 1668610852205, "content": {"title": "On Monocular Depth Estimation and Uncertainty Quantification using Classification Approaches for Regression", "abstract": "Monocular depth is important in many tasks, such as 3D reconstruction and autonomous driving. Deep learning based models achieve state-of-the-art performance in this field. A set of novel approaches for estimating monocular depth consists of transforming the regression task into a classification one. However, there is a lack of detailed descriptions and comparisons for Classification Approaches for Regression (CAR) in the community and no in-depth exploration of their potential for uncertainty estimation. To this end, this paper will introduce a taxonomy and summary of CAR approaches, a new uncertainty estimation solution for CAR, and a set of experiments on depth accuracy and uncertainty quantification for CAR-based models on KITTI dataset. The experiments reflect the differences in the portability of various CAR methods on two backbones. Meanwhile, the newly proposed method for uncertainty estimation can outperform the ensembling method with only one forward propagation."}}
