{"id": "HjOo2k8lhFl", "cdate": 1663850326731, "mdate": null, "content": {"title": "Learning Rationalizable Equilibria in Multiplayer Games", "abstract": "A natural goal in multi-agent learning is to learn \\emph{rationalizable} behavior, where players learn to avoid any Iteratively Dominated Action (IDA). However, standard no-regret based equilibria-finding algorithms could take exponential samples to find such rationalizable strategies. In this paper, we first propose a simple yet sample-efficient algorithm for finding a rationalizable action profile in multi-player general-sum games under bandit feedback, which substantially improves over the results of Wu et al. We further develop algorithms with the first efficient guarantees for learning rationalizable Coarse Correlated Equilibria (CCE) and Correlated Equilibria (CE). Our algorithms incorporate several novel techniques to guarantee the elimination of IDA and no (swap-)regret simultaneously, including a correlated exploration scheme and adaptive learning rates, which may be of independent interest. We complement our results with a sample complexity lower bound showing the sharpness of our guarantees."}}
{"id": "B2rqx0w63U", "cdate": 1652737666077, "mdate": null, "content": {"title": "Provably Feedback-Efficient Reinforcement Learning via Active Reward Learning", "abstract": "An appropriate reward function is of paramount importance in specifying a task in reinforcement learning (RL). Yet, it is known to be extremely challenging in practice to design a correct reward function for even simple tasks. Human-in-the-loop (HiL) RL allows humans to communicate complex goals to the RL agent by providing various types of feedback. However, despite achieving great empirical successes, HiL RL usually requires \\emph{too much} feedback from a human teacher and also suffers from insufficient theoretical understanding. In this paper, we focus on addressing this issue from a theoretical perspective, aiming to provide provably feedback-efficient algorithmic frameworks that take human-in-the-loop to specify rewards of given tasks. We provide an \\emph{active-learning}-based RL algorithm that first explores the environment without specifying a reward function and then asks a human teacher for only a few queries about the rewards of a task at some state-action pairs. After that, the algorithm guarantees to provide a nearly optimal policy for the task with high probability. We show that, even with the presence of random noise in the feedback, the algorithm only takes $\\tilde{O}(H{\\dim_{R}^2})$ queries on the reward function to provide an $\\epsilon$-optimal policy for any $\\epsilon > 0$. Here $H$ is the horizon of the RL environment, and $\\dim_{R}$ specifies the complexity of the function class representing the reward function. In contrast, standard RL algorithms require to query the reward function for at least $\\Omega(\\operatorname{poly}(d, 1/\\epsilon))$ state-action pairs where $d$ depends on the complexity of the environmental transition."}}
{"id": "wmjs9NoG9uC", "cdate": 1640995200000, "mdate": 1681497515862, "content": {"title": "Learning Rationalizable Equilibria in Multiplayer Games", "abstract": ""}}
{"id": "ayU4YRX0k-x", "cdate": 1609459200000, "mdate": 1681751391525, "content": {"title": "Online Sub-Sampling for Reinforcement Learning with General Function Approximation", "abstract": "Most of the existing works for reinforcement learning (RL) with general function approximation (FA) focus on understanding the statistical complexity or regret bounds. However, the computation complexity of such approaches is far from being understood -- indeed, a simple optimization problem over the function class might be as well intractable. In this paper, we tackle this problem by establishing an efficient online sub-sampling framework that measures the information gain of data points collected by an RL algorithm and uses the measurement to guide exploration. For a value-based method with complexity-bounded function class, we show that the policy only needs to be updated for $\\propto\\operatorname{poly}\\log(K)$ times for running the RL algorithm for $K$ episodes while still achieving a small near-optimal regret bound. In contrast to existing approaches that update the policy for at least $\\Omega(K)$ times, our approach drastically reduces the number of optimization calls in solving for a policy. When applied to settings in \\cite{wang2020reinforcement} or \\cite{jin2021bellman}, we improve the overall time complexity by at least a factor of $K$. Finally, we show the generality of our online sub-sampling technique by applying it to the reward-free RL setting and multi-agent RL setting."}}
