{"id": "M-seILmeISn", "cdate": 1652737627009, "mdate": null, "content": {"title": "Tight Mutual Information Estimation With Contrastive Fenchel-Legendre Optimization", "abstract": "Successful applications of InfoNCE (Information Noise-Contrastive Estimation) and its variants have popularized the use of contrastive variational mutual information (MI) estimators in machine learning . While featuring superior stability, these estimators crucially depend on costly large-batch training, and they sacrifice bound tightness for variance reduction. To overcome these limitations, we revisit the mathematics of popular variational MI bounds from the lens of unnormalized statistical modeling and convex optimization. Our investigation yields a new unified theoretical framework encompassing popular variational MI bounds, and leads to a novel, simple, and powerful contrastive MI estimator we name FLO. Theoretically, we show that the FLO estimator is tight, and it converges under stochastic gradient descent. Empirically, the proposed FLO estimator overcomes the limitations of its predecessors and learns more efficiently. The utility of FLO is verified using extensive benchmarks, and we further inspire the community with novel applications in meta-learning. Our presentation underscores the foundational importance of variational MI estimation in data-efficient learning."}}
{"id": "kpTMw7ZMJB", "cdate": 1621629931357, "mdate": null, "content": {"title": "Supercharging Imbalanced Data Learning With Energy-based Contrastive Representation Transfer", "abstract": "Dealing with severe class imbalance poses a major challenge for many real-world applications, especially when the accurate classification and generalization of minority classes are of primary interest.\nIn computer vision and NLP, learning from datasets with long-tail behavior is a recurring theme, especially for naturally occurring labels. Existing solutions mostly appeal to sampling or weighting adjustments to alleviate the extreme imbalance, or impose inductive bias to prioritize generalizable associations. Here we take a novel perspective to promote sample efficiency and model generalization based on the invariance principles of causality. Our contribution posits a meta-distributional scenario, where the causal generating mechanism for label-conditional features is invariant across different labels. Such causal assumption enables efficient knowledge transfer from the dominant classes to their under-represented counterparts, even if their feature distributions show apparent disparities. This allows us to leverage a causal data augmentation procedure to enlarge the representation of minority classes. Our development is orthogonal to the existing imbalanced data learning techniques thus can be seamlessly integrated. The proposed approach is validated on an extensive set of synthetic and real-world tasks against state-of-the-art solutions. "}}
{"id": "RgdpJq320fL", "cdate": 1594394548977, "mdate": null, "content": {"title": "GO Hessian for Expectation-Based Objectives", "abstract": "An unbiased low-variance gradient estimator, termed GO gradient, was proposed recently for expectation-based objectives $E_{q_{\\gamma}(y)} [f(y)]$, where the random variable (RV) y may be drawn from a stochastic computation graph with continuous (non-reparameterizable) internal nodes and continuous/discrete leaves. Upgrading the GO gradient, we present for $E_{q_{\\gamma}(y)} [f(y)]$ an unbiased low-variance Hessian estimator, named GO Hessian. Considering practical implementation, we reveal that GO Hessian is easy-to-use with auto-differentiation and Hessian-vector products, enabling efficient cheap exploitation of curvature information over stochastic computation graphs. As representative examples, we present the GO Hessian for non-reparameterizable gamma and negative binomial RVs/nodes. Based on the GO Hessian, we design a new second-order method for $E_{q_{\\gamma}(y)} [f(y)]$, with rigorous experiments conducted to verify its effectiveness and efficiency."}}
{"id": "rkNvbsWObB", "cdate": 1546300800000, "mdate": null, "content": {"title": "Variational Annealing of GANs: A Langevin Perspective", "abstract": "The generative adversarial network (GAN) has received considerable attention recently as a model for data synthesis, without an explicit specification of a likelihood function. There has been comme..."}}
{"id": "bHa0TAOSL6T", "cdate": 1546300800000, "mdate": null, "content": {"title": "Variational Annealing of GANs: A Langevin Perspective", "abstract": "The generative adversarial network (GAN) has received considerable attention recently as a model for data synthesis, without an explicit specification of a likelihood function. There has been comme..."}}
{"id": "Ce3IXz5Bvb3", "cdate": 1546300800000, "mdate": null, "content": {"title": "On Fenchel Mini-Max Learning", "abstract": "Inference, estimation, sampling and likelihood evaluation are four primary goals of probabilistic modeling. Practical considerations often force modeling approaches to make compromises between these objectives. We present a novel probabilistic learning framework, called Fenchel Mini-Max Learning (FML), that accommodates all four desiderata in a flexible and scalable manner. Our derivation is rooted in classical maximum likelihood estimation, and it overcomes a longstanding challenge that prevents unbiased estimation of unnormalized statistical models. By reformulating MLE as a mini-max game, FML enjoys an unbiased training objective that (i) does not explicitly involve the intractable normalizing constant and (ii) is directly amendable to stochastic gradient descent optimization. To demonstrate the utility of the proposed approach, we consider learning unnormalized statistical models, nonparametric density estimation and training generative models, with encouraging empirical results presented."}}
{"id": "31NxYYdCpoO", "cdate": 1514764800000, "mdate": null, "content": {"title": "A Wiener Causality Defined by Relative Entropy", "abstract": "In this paper, we propose a novel definition of Wiener causality to describe the intervene between time series, based on relative entropy. In comparison to the classic Granger causality, by which the interdependence of the statistic moments beside the second moments are concerned, this definition of causality theoretically takes all statistic aspects into considerations. Furthermore under the Gaussian assumption, not only the intervenes between the co-variances but also those between the means are involved in the causality. This provides an integrated description of statistic causal intervene. Additionally, our implementation also requires minimum assumption on data, which allows one to easily combine modern predictive model with causality inference. We demonstrate that REC outperform the standard causality method on a series of simulations under various conditions."}}
