{"id": "QCcj1wnJK_", "cdate": 1672531200000, "mdate": 1683725092211, "content": {"title": "Understanding Transformer Memorization Recall Through Idioms", "abstract": ""}}
{"id": "eiCb_PE1oN", "cdate": 1640995200000, "mdate": 1681532152726, "content": {"title": "SCROLLS: Standardized CompaRison Over Long Language Sequences", "abstract": ""}}
{"id": "3XvKtSao5N", "cdate": 1640995200000, "mdate": 1680419401276, "content": {"title": "Transformer Language Models without Positional Encodings Still Learn Positional Information", "abstract": ""}}
{"id": "uGgt5QCtgh", "cdate": 1609459200000, "mdate": 1631256224651, "content": {"title": "BERTese: Learning to Speak to BERT", "abstract": "Adi Haviv, Jonathan Berant, Amir Globerson. Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 2021."}}
{"id": "3bKQnCQ77i3", "cdate": 1609459200000, "mdate": 1635479308941, "content": {"title": "Can Latent Alignments Improve Autoregressive Machine Translation?", "abstract": "Adi Haviv, Lior Vassertail, Omer Levy. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021."}}
