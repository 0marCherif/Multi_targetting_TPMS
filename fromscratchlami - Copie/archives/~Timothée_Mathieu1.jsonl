{"id": "EyFhAS5mdO", "cdate": 1686250300283, "mdate": null, "content": {"title": "AdaStop: sequential testing for efficient and reliable comparisons of Deep RL Agents", "abstract": "The reproducibility of many experimental results in Deep Reinforcement Learning (RL) is under question. To solve this reproducibility crisis, we propose a theoretically sound methodology to compare multiple Deep RL algorithms. The performance of one execution of a Deep RL algorithm is random so that independent executions are needed to assess it precisely. When comparing several RL algorithms, a major question is how many executions must be made and how can we assure that the results of such a comparison is theoretically sound. Researchers in Deep RL often use less than 5 independent executions to compare algorithms: we claim that this is not enough in general. Moreover, when comparing several algorithms at once, the error of each comparison accumulates and must be taken into account with a multiple tests procedure to preserve low error guarantees. To address this problem in a statistically sound way, we introduce AdaStop, a new statistical test based on multiple group sequential tests. When comparing algorithms, AdaStop adapts the number of executions to stop as early as possible while ensuring that we have enough information to distinguish algorithms that perform better than the others in a statistical significant way. We prove both theoretically and empirically that AdaStop has a low probability of making an error (Family-Wise Error). Finally, we illustrate the effectiveness of AdaStop in multiple use-cases, including toy examples and difficult cases such as Mujoco environments."}}
{"id": "Ev4NOEeTYL", "cdate": 1668480871630, "mdate": null, "content": {"title": "Farm-gym: A modular reinforcement learning platform for stochastic agronomic games", "abstract": "We introduce Farm-gym, an open-source farming environment written in Python, that models sequential decision-making in farms using Reinforcement Learning (RL). Farm-gym conceptualizes a farm as a dynamical system with many interacting entities. Leveraging a modular design, it enables us to instantiate from very simple to highly complicated environments. \nContrasting many available gym environments, Farm-gym features intrinsically stochastic games, using stochastic growth models and weather data. Further, it enables to create farm games in a modular way, activating or not the entities (e.g. weeds, pests, pollinators), and yielding non-trivial coupled dynamics. Finally, every game can be customized with .yaml files for rewards, feasible actions, and initial/end-game conditions. We illustrate some interesting features on simple farms. We also showcase the challenges posed by Farm-gym to the deep RL algorithms, in order to stimulate studies in the RL community."}}
{"id": "Uy3SsRvtHHi", "cdate": 1577836800000, "mdate": null, "content": {"title": "Robust classification via MOM minimization", "abstract": "We present an extension of Chervonenkis and Vapnik\u2019s classical empirical risk minimization (ERM) where the empirical risk is replaced by a median-of-means (MOM) estimator of the risk. The resulting new estimators are called MOM minimizers. While ERM is sensitive to corruption of the dataset for many classical loss functions used in classification, we show that MOM minimizers behave well in theory, in the sense that it achieves Vapnik\u2019s (slow) rates of convergence under weak assumptions: the functions in the hypothesis class are only required to have a finite second moment and some outliers may also have corrupted the dataset. We propose algorithms, inspired by MOM minimizers, which may be interpreted as MOM version of block stochastic gradient descent (BSGD). The key point of these algorithms is that the block of data onto which a descent step is performed is chosen according to its \u201c centrality\u201d among the other blocks. This choice of \u201c descent block\u201d makes these algorithms robust to outliers; also, this is the only extra step added to classical BSGD algorithms. As a consequence, classical BSGD algorithms can be easily turn into robust MOM versions. Moreover, MOM algorithms perform a smart subsampling which may help to reduce substantially time computations and memory resources when applied to non linear algorithms. These empirical performances are illustrated on both simulated and real datasets."}}
{"id": "myyAzcrO48L", "cdate": 1546300800000, "mdate": null, "content": {"title": "Excess risk bounds in robust empirical risk minimization", "abstract": "This paper investigates robust versions of the general empirical risk minimization algorithm, one of the core techniques underlying modern statistical methods. Success of the empirical risk minimization is based on the fact that for a \"well-behaved\" stochastic process $\\left\\{ f(X), \\ f\\in \\mathcal F\\right\\}$ indexed by a class of functions $f\\in \\mathcal F$, averages $\\frac{1}{N}\\sum_{j=1}^N f(X_j)$ evaluated over a sample $X_1,\\ldots,X_N$ of i.i.d. copies of $X$ provide good approximation to the expectations $\\mathbb E f(X)$ uniformly over large classes $f\\in \\mathcal F$. However, this might no longer be true if the marginal distributions of the process are heavy-tailed or if the sample contains outliers. We propose a version of empirical risk minimization based on the idea of replacing sample averages by robust proxies of the expectation, and obtain high-confidence bounds for the excess risk of resulting estimators. In particular, we show that the excess risk of robust estimators can converge to $0$ at fast rates with respect to the sample size. We discuss implications of the main results to the linear and logistic regression problems, and evaluate the numerical performance of proposed methods on simulated and real data."}}
{"id": "ByWBr2WOZS", "cdate": 1546300800000, "mdate": null, "content": {"title": "MONK Outlier-Robust Mean Embedding Estimation by Median-of-Means", "abstract": "Mean embeddings provide an extremely flexible and powerful tool in machine learning and statistics to represent probability distributions and define a semi-metric (MMD, maximum mean discrepancy; al..."}}
