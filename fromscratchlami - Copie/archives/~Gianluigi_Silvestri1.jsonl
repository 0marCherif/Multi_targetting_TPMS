{"id": "g8wBdhnstYz", "cdate": 1663849936758, "mdate": null, "content": {"title": "Deterministic training of generative autoencoders using invertible layers", "abstract": "In this work, we provide a deterministic alternative to the stochastic variational training of generative autoencoders. We refer to these new generative autoencoders as AutoEncoders within Flows (AEF), since the encoder and decoder are defined as affine layers of an overall invertible architecture. This results in a deterministic encoding of the data, as opposed to the stochastic encoding of VAEs. The paper introduces two related families of AEFs. The first family relies on a partition of the ambient space and is trained by exact maximum-likelihood. The second family exploits a deterministic expansion of the ambient space and is trained by maximizing the log-probability in this extended space. This latter case leaves complete freedom in the choice of encoder, decoder and prior architectures, making it a drop-in replacement for the training of existing VAEs and VAE-style models. We show that these AEFs can have strikingly higher performance than architecturally identical VAEs in terms of log-likelihood and sample quality, especially for low dimensional latent spaces. Importantly, we show that AEF samples are substantially sharper than VAE samples. "}}
{"id": "caMzWWsaO2G", "cdate": 1640995200000, "mdate": 1682422253949, "content": {"title": "Closing the gap: Exact maximum likelihood training of generative autoencoders using invertible layers", "abstract": "In this work, we provide a deterministic alternative to the stochastic variational training of generative autoencoders. We refer to these new generative autoencoders as AutoEncoders within Flows (AEF), since the encoder and decoder are defined as affine layers of an overall invertible architecture. This results in a deterministic encoding of the data, as opposed to the stochastic encoding of VAEs. The paper introduces two related families of AEFs. The first family relies on a partition of the ambient space and is trained by exact maximum-likelihood. The second family exploits a deterministic expansion of the ambient space and is trained by maximizing the log-probability in this extended space. This latter case leaves complete freedom in the choice of encoder, decoder and prior architectures, making it a drop-in replacement for the training of existing VAEs and VAE-style models. We show that these AEFs can have strikingly higher performance than architecturally identical VAEs in terms of log-likelihood and sample quality, especially for low dimensional latent spaces. Importantly, we show that AEF samples are substantially sharper than VAE samples."}}
{"id": "HTxqYW9Hd5t", "cdate": 1640995200000, "mdate": 1682422253946, "content": {"title": "Embedded-model flows: Combining the inductive biases of model-free deep learning and explicit probabilistic modeling", "abstract": "Normalizing flows have shown great success as general-purpose density estimators. However, many real world applications require the use of domain-specific knowledge, which normalizing flows cannot readily incorporate. We propose embedded-model flows (EMF), which alternate general-purpose transformations with structured layers that embed domain-specific inductive biases. These layers are automatically constructed by converting user-specified differentiable probabilistic models into equivalent bijective transformations. We also introduce gated structured layers, which allow bypassing the parts of the models that fail to capture the statistics of the data. We demonstrate that EMFs can be used to induce desirable properties such as multimodality and continuity. Furthermore, we show that EMFs enable a high performance form of variational inference where the structure of the prior model is embedded in the variational architecture. In our experiments, we show that this approach outperforms a large number of alternative methods in common structured inference problems."}}
{"id": "loKJxxVNI8f", "cdate": 1637576008466, "mdate": null, "content": {"title": "Embedded-model flows: Combining the inductive biases of model-free deep learning and explicit probabilistic modeling", "abstract": "Normalizing flows have shown great success as general-purpose density estimators. However, many real-world applications require the use of domain-specific knowledge, which normalizing flows cannot readily incorporate. We propose embedded-model flows (EMF), which alternate general-purpose transformations with structured layers that embed domain-specific inductive biases. These layers are automatically constructed by converting user-specified differentiable probabilistic models into equivalent bijective transformations. We also introduce gated structured layers, which allow bypassing the parts of the models that fail to capture the statistics of the data. We demonstrate that EMFs can be used to induce desirable properties such as multimodality, hierarchical coupling and continuity. Furthermore, we show that EMFs enable a high-performance form of variational inference where the structure of the prior model is embedded in the variational architecture. In our experiments, we show that this approach outperforms state-of-the-art methods in common structured inference problems."}}
{"id": "9pEJSVfDbba", "cdate": 1632875494693, "mdate": null, "content": {"title": "Embedded-model flows: Combining the inductive biases of model-free deep learning and explicit probabilistic modeling", "abstract": "Normalizing flows have shown great success as general-purpose density estimators. However, many real world applications require the use of domain-specific knowledge, which normalizing flows cannot readily incorporate. We propose embedded-model flows (EMF), which alternate general-purpose transformations with structured layers that embed domain-specific inductive biases. These layers are automatically constructed by converting user-specified differentiable probabilistic models into equivalent bijective transformations. We also introduce gated structured layers, which allow bypassing the parts of the models that fail to capture the statistics of the data. We demonstrate that EMFs can be used to induce desirable properties such as multimodality and continuity. Furthermore, we show that EMFs enable a high performance form of variational inference where the structure of the prior model is embedded in the variational architecture. In our experiments, we show that this approach outperforms a large number of alternative methods in common structured inference problems."}}
{"id": "8TSLv9L2l0", "cdate": 1623413376660, "mdate": null, "content": {"title": "Automatic variational inference with cascading flows", "abstract": "The automation of probabilistic reasoning is one\nof the primary aims of machine learning. Recently, the confluence of variational inference and\ndeep learning has led to powerful and flexible automatic inference methods that can be trained by\nstochastic gradient descent. In particular, normalizing flows are highly parameterized deep models\nthat can fit arbitrarily complex posterior densities.\nHowever, normalizing flows struggle in highly\nstructured probabilistic programs as they need\nto relearn the forward-pass of the program. Automatic structured variational inference (ASVI)\nremedies this problem by constructing variational\nprograms that embed the forward-pass. Here, we\ncombine the flexibility of normalizing flows and\nthe prior-embedding property of ASVI in a new\nfamily of variational programs, which we named\ncascading flows. A cascading flows program interposes a newly designed highway flow architecture in between the conditional distributions\nof the prior program such as to steer it toward\nthe observed data. These programs can be constructed automatically from an input probabilistic program and can also be amortized automatically. We evaluate the performance of the new\nvariational programs in a series of structured inference problems. We find that cascading flows\nhave much higher performance than both normalizing flows and ASVI in a large set of structured\ninference problems."}}
{"id": "cGrDPO0aHqt", "cdate": 1609459200000, "mdate": 1682422253949, "content": {"title": "Automatic variational inference with cascading flows", "abstract": "The automation of probabilistic reasoning is one of the primary aims of machine learning. Recently, the confluence of variational inference and deep learning has led to powerful and flexible automatic inference methods that can be trained by stochastic gradient descent. In particular, normalizing flows are highly parameterized deep models that can fit arbitrarily complex posterior densities. However, normalizing flows struggle in highly structured probabilistic programs as they need to relearn the forward-pass of the program. Automatic structured variational inference (ASVI) remedies this problem by constructing variational programs that embed the forward-pass. Here, we combine the flexibility of normalizing flows and the prior-embedding property of ASVI in a new family of variational programs, which we named cascading flows. A cascading flows program interposes a newly designed highway flow architecture in between the conditional distributions of the prior program such as to steer it toward the observed data. These programs can be constructed automatically from an input probabilistic program and can also be amortized automatically. We evaluate the performance of the new variational programs in a series of structured inference problems. We find that cascading flows have much higher performance than both normalizing flows and ASVI in a large set of structured inference problems."}}
{"id": "LalYzIbQqk", "cdate": 1609459200000, "mdate": 1682422253951, "content": {"title": "Embedded-model flows: Combining the inductive biases of model-free deep learning and explicit probabilistic modeling", "abstract": "Normalizing flows have shown great success as general-purpose density estimators. However, many real world applications require the use of domain-specific knowledge, which normalizing flows cannot readily incorporate. We propose embedded-model flows (EMF), which alternate general-purpose transformations with structured layers that embed domain-specific inductive biases. These layers are automatically constructed by converting user-specified differentiable probabilistic models into equivalent bijective transformations. We also introduce gated structured layers, which allow bypassing the parts of the models that fail to capture the statistics of the data. We demonstrate that EMFs can be used to induce desirable properties such as multimodality, hierarchical coupling and continuity. Furthermore, we show that EMFs enable a high performance form of variational inference where the structure of the prior model is embedded in the variational architecture. In our experiments, we show that this approach outperforms state-of-the-art methods in common structured inference problems."}}
{"id": "Dp6cn--yRuK", "cdate": 1609459200000, "mdate": 1682422253953, "content": {"title": "Automatic variational inference with cascading flows", "abstract": "The automation of probabilistic reasoning is one of the primary aims of machine learning. Recently, the confluence of variational inference and deep learning has led to powerful and flexible automa..."}}
{"id": "Gffuav8hf-q", "cdate": 1483228800000, "mdate": 1682422253950, "content": {"title": "Searching Relevant Variable Subsets in Complex Systems Using K-Means PSO", "abstract": "The Relevance Index method has been shown to be effective in identifying Relevant Sets in complex systems, i.e., variable sub-sets that exhibit a coordinated behavior, along with a clear independence from the remaining variables. The need for computing the Relevance Index for each possible variable sub-set makes such a computation unfeasible, as the size of the system increases. Because of this, smart search methods are needed to analyze large-size systems using such an approach. Niching metaheuristics provide an effective solution to this problem, as they join search capabilities to good exploration properties, which allow them to explore different regions of the search space in parallel and converge onto several local/global minima. In this paper, we describe the application of a niching metaheuristic, K-means PSO, to a set of complex systems of different size, comparing, when possible, its results with the ground truth represented by the results of an exhaustive search, while we rely on the analysis of a domain expert to assess the results of larger systems. In all cases, we also compare the results of K-means PSO to another metaheuristic, based on a niching genetic algorithm, that we had previously developed."}}
