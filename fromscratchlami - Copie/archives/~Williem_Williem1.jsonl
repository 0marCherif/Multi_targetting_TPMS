{"id": "56g9BZxAoA", "cdate": 1640995200000, "mdate": 1698559127723, "content": {"title": "Holistic 3D Body Reconstruction From a Blurred Single Image", "abstract": "Holistic human pose and shape reconstruction receive huge interest since it restores detailed human pose and shape including facial expression and finger-level hand shape. Existing deep 3D holistic human pose and shape reconstruction methods utilize sharp images as their input which leads to inaccurate human mesh when given a blurred image. Although there exist lots of image deblurring methods, a simple cascaded approach could not produce satisfactory results. In this paper, we introduce D2R (Deblurring-to-Reconstruction), a novel joint framework of human motion deblurring and 3D holistic body reconstruction to solve both problems simultaneously. In addition, we generate a new large-scale dataset that contains sharp/blur image pairs and corresponding 3D body pose/shape. We train the proposed joint framework in an alternating scheme to refine each module\u2019s performance by utilizing an additional structure-aware module. Experimental results show that the proposed method achieves outperforming results 3D holistic human reconstruction qualitatively as well as quantitatively while input image is deblurred."}}
{"id": "L6j2JO0vxQj", "cdate": 1577836800000, "mdate": 1698559127845, "content": {"title": "Face Mask Invariant End-to-End Face Recognition", "abstract": "This paper introduces an end-to-end face recognition network that is invariant to face images with face masks. Conventional face recognition networks have degraded performance on images with face masks due to inaccurate landmark prediction and alignment results. Thus, an end-to-end network is proposed to solve the problem. We generate face mask synthesized datasets by properly aligning the face mask to images on available public datasets, such as CASIA-Webface, LFW, CALFW, CPLFW, and CFP. Then, we utilize those datasets as training and testing datasets. Second, we introduce a network that contains two modules: alignment and feature extraction modules. These modules are trained end-to-end, which makes the network invariant to face images with a face mask. Experimental results show that the proposed method achieves significant improvement from state-of-the-art face recognition network in face mask synthesized datasets."}}
{"id": "3K2dZeOsKH", "cdate": 1577836800000, "mdate": 1698559127819, "content": {"title": "Joint Light Field Spatial and Angular Super-Resolution From a Single Image", "abstract": "Synthesizing a densely sampled light field from a single image is highly beneficial for many applications. Moreover, jointly solving both angular and spatial super-resolution problem also introduces new possibilities in light field imaging. The conventional method relies on physical-based rendering and a secondary network to solve the angular super-resolution problem. In addition, pixel-based loss limits the network capability to infer scene geometry globally. In this paper, we show that both super-resolution problems can be solved jointly from a single image by proposing a single end-to-end deep neural network that does not require a physical-based approach. Two novel loss functions based on known light field domain knowledge are proposed to enable the network to consider the relation between sub-aperture images. Experimental results show that the proposed model successfully synthesizes dense high resolution light field and it outperforms the state-of-the-art method in both quantitative and qualitative criteria. The method can be generalized to various scenes, rather than focusing on a particular subject. The synthesized light field can be used as if it has been captured by a light field camera, such as depth estimation and refocusing."}}
{"id": "oaI7RLKRb7A", "cdate": 1546300800000, "mdate": 1667633918765, "content": {"title": "6-DOF motion blur synthesis and performance evaluation of light field deblurring", "abstract": "Motion deblurring is essential for reconstructing sharp images from given a blurry input caused by the camera motion. The complexity of this problem increases in a light field due to its depth-dependent blur constraint. A method of generating synthetic 3 degree-of-freedom (3-DOF) translation blur on a light field image without camera rotation has been introduced. In this study, we generate a camera translation and rotation (6-DOF) motion blur model that preserves the consistency of the light field image. Our experiment results show that the proposed blur model can maintain the parallax information (depth-dependent blur) in a light field image. Furthermore, we produce a synthetic blurry light field dataset based on the 6-DOF model. Finally, to validate the usability of the synthetic dataset, we conduct extensive benchmarking using state-of-the-art motion deblurring algorithms."}}
{"id": "g4TFoqrNfL", "cdate": 1546300800000, "mdate": 1698559127817, "content": {"title": "Joint Spatial and Angular Super-Resolution from a Single Image", "abstract": "Synthesizing a densely sampled light field from a single image is highly beneficial for many applications. Moreover, jointly solving both angular and spatial super-resolution problem also introduces new possibilities in light field imaging. The conventional method relies on physical-based rendering and a secondary network to solve the angular super-resolution problem. In addition, pixel-based loss limits the network capability to infer scene geometry globally. In this paper, we show that both super-resolution problems can be solved jointly from a single image by proposing a single end-to-end deep neural network that does not require a physical-based approach. Two novel loss functions based on known light field domain knowledge are proposed to enable the network to preserve the spatio-angular consistency between sub-aperture images. Experimental results show that the proposed model successfully synthesizes dense high resolution light field and it outperforms the state-of-the-art method in both quantitative and qualitative criteria. The method can be generalized to arbitrary scenes, rather than focusing on a particular subject. The synthesized light field can be used for various applications, such as depth estimation and refocusing."}}
{"id": "3P6zIxkKZj", "cdate": 1546300800000, "mdate": 1698559127850, "content": {"title": "Synthesizing a 4D Spatio-Angular Consistent Light Field from a Single Image", "abstract": "Synthesizing a densely sampled light field from a single image is highly beneficial for many applications. The conventional method reconstructs a depth map and relies on physical-based rendering and a secondary network to improve the synthesized novel views. Simple pixel-based loss also limits the network by making it rely on pixel intensity cue rather than geometric reasoning. In this study, we show that a different geometric representation, namely, appearance flow, can be used to synthesize a light field from a single image robustly and directly. A single end-to-end deep neural network that does not require a physical-based approach nor a post-processing subnetwork is proposed. Two novel loss functions based on known light field domain knowledge are presented to enable the network to preserve the spatio-angular consistency between sub-aperture images effectively. Experimental results show that the proposed model successfully synthesizes dense light fields and qualitatively and quantitatively outperforms the previous model . The method can be generalized to arbitrary scenes, rather than focusing on a particular class of object. The synthesized light field can be used for various applications, such as depth estimation and refocusing."}}
{"id": "tirQxiaTSH", "cdate": 1514764800000, "mdate": 1698559127820, "content": {"title": "Deep self-guided cost aggregation for stereo matching", "abstract": ""}}
{"id": "H14bTRbuZB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Light Field Depth Estimation on Off-the-Shelf Mobile GPU", "abstract": "While novel light processing algorithms have been continuously introduced, it is still challenging to perform light field processing on a mobile device with limited computation resource due to the high dimensionality of light field data. Recently, the performance of mobile graphics processing unit (GPU) increases rapidly and GPGPU on mobile GPU utilizes massive parallel computation to solve various computer vision problems with high computational complexity. To show the potential capability of light field processing on mobile GPU, we parallelize and optimize the state-of-the-art light field depth estimation which is essential to many light field applications. We employ both algorithm and kernel-based optimization to enable light field processing on mobile GPU. Light field processing involves independent pixel processing with intensive floating-point operations that can be vectorized to match single instruction multiple data (SIMD) style of GPU architecture. We design efficient memory access, caching, and prefetching to exploit light field properties. The experimental result shows that the light field depth estimation on mobile GPU obtains comparable performance as on the desktop CPU. The proposed optimization method gains up to 25 times speedup compared to the naive baseline method."}}
{"id": "9wUujLtDUe5", "cdate": 1514764800000, "mdate": 1698559127767, "content": {"title": "Cost aggregation benchmark for light field depth estimation", "abstract": ""}}
{"id": "5TsTnLTvaWS5", "cdate": 1483228800000, "mdate": 1668046265431, "content": {"title": "Visual-Inertial RGB-D SLAM for Mobile Augmented Reality", "abstract": "This paper presents a practical framework for occlusion-aware augmented reality application using visual-inertial RGB-D SLAM. First, an efficient visual SLAM framework with map merging based relocalization is introduced. When the pose estimation fails, a new environment map is generated. Then, a map merging is performed to merge the current and previous environment maps if a loop closure is detected. The framework is then integrated with the inertial information to solve the missing environment map problem. Camera pose is approximated using the angular velocity and translational acceleration value when the pose estimation fails. Experimental results show that the proposed method can perform well in the presence of missing pose. Finally, an occlusion-aware augmented reality application is built over the SLAM framework."}}
