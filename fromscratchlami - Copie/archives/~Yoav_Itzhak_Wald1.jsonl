{"id": "6ckp3HWUjDU", "cdate": 1676827103571, "mdate": null, "content": {"title": "Birds of an Odd Feather: Guaranteed Out-of-Distribution (OOD) Novel Category Detection", "abstract": "In this work, we solve the problem of novel category detection under distribution shift. This problem is critical to ensuring the safety and efficacy of machine learning models, particularly in domains such as healthcare where timely detection of novel subgroups of patients is crucial.\n\nTo address this problem, we propose a method based on constrained learning. Our approach is guaranteed to detect a novel category under a relatively weak assumption, namely that rare events in past data have bounded frequency under the shifted distribution. Prior works on the problem do not provide such guarantees, as they either attend to very specific types of distribution shift or make stringent assumptions that limit their guarantees.\n\nWe demonstrate favorable performance of our method on challenging novel category detection problems over real world datasets."}}
{"id": "1xadmcm2CC", "cdate": 1664928789658, "mdate": null, "content": {"title": "Malign Overfitting: Interpolation and Invariance are Fundamentally at Odds", "abstract": "Learned classifiers should often possess certain invariance properties meant to encourage fairness, robustness, or out-of-distribution generalization. \nHowever, multiple recent works empirically demonstrate that common invariance-inducing regularizers are ineffective in the over-parameterized regime, in which classifiers perfectly fit (i.e. interpolate) the training data. This suggests that the phenomenon of ``benign overfitting\", in which models generalize well despite interpolating, might not favorably extend to settings in which robustness or fairness are desirable. \n\nIn this work we provide a theoretical justification for these observations. We prove that - even in the simplest of settings - any interpolating classifier (with nonzero margin) will not satisfy these invariance properties. We then propose and analyze an algorithm that - in the same setting - successfully learns a non-interpolating classifier that is provably invariant. We validate our theoretical observations regarding the conflict between interpolation and invariance on simulated data and the Waterbirds dataset."}}
{"id": "dQNL7Zsta3", "cdate": 1663850169264, "mdate": null, "content": {"title": "Malign Overfitting: Interpolation and Invariance are Fundamentally at Odds", "abstract": "Learned classifiers should often possess certain invariance properties meant to encourage fairness, robustness, or out-of-distribution generalization. \nHowever, multiple recent works empirically demonstrate that common invariance-inducing regularizers are ineffective in the over-parameterized regime, in which classifiers perfectly fit (i.e. interpolate) the training data. This suggests that the phenomenon of ``benign overfitting,\" in which models generalize well despite interpolating, might not favorably extend to settings in which robustness or fairness are desirable. \n\nIn this work, we provide a theoretical justification for these observations. We prove that---even in the simplest of settings---any interpolating learning rule (with an arbitrarily small margin) will not satisfy these invariance properties. We then propose and analyze an algorithm that---in the same setting---successfully learns a non-interpolating classifier that is provably invariant. We validate our theoretical observations on simulated data and the Waterbirds dataset."}}
{"id": "8FsfyRnb8b", "cdate": 1653904143120, "mdate": null, "content": {"title": "In the Eye of the Beholder: Robust Prediction with Causal User Modeling", "abstract": "Accurately predicting the relevance of items to users is crucial to the success of many social platforms. Conventional approaches train models on logged historical data; but recommendation systems, media services, and online marketplaces all exhibit a constant influx of new content---making relevancy a moving target, to which standard predictive models are not robust. In this paper, we propose a learning framework for relevance prediction that is robust to changes in the data distribution. Our key observation is that robustness can be obtained by accounting for \\emph{how users causally perceive the environment}. We model users as boundedly-rational decision makers whose causal beliefs are encoded by a causal graph, and show how minimal information regarding the graph can be used to contend with distributional changes. Experiments in multiple settings demonstrate the effectiveness of our approach."}}
{"id": "ikXoMuy_H4", "cdate": 1652737299344, "mdate": null, "content": {"title": "In the Eye of the Beholder: Robust Prediction with Causal User Modeling", "abstract": "Accurately predicting the relevance of items to users is crucial to the success of many social platforms. Conventional approaches train models on logged historical data; but recommendation systems, media services, and online marketplaces all exhibit a constant influx of new content---making relevancy a moving target, to which standard predictive models are not robust. In this paper, we propose a learning framework for relevance prediction that is robust to changes in the data distribution. Our key observation is that robustness can be obtained by accounting for \\emph{how users causally perceive the environment}. We model users as boundedly-rational decision makers whose causal beliefs are encoded by a causal graph, and show how minimal information regarding the graph can be used to contend with distributional changes. Experiments in multiple settings demonstrate the effectiveness of our approach."}}
{"id": "XWYJ25-yTRS", "cdate": 1621630049565, "mdate": null, "content": {"title": "On Calibration and Out-of-Domain Generalization", "abstract": "Out-of-domain (OOD) generalization is a significant challenge for machine learning models. Many techniques have been proposed to overcome this challenge, often focused on learning models with certain invariance properties. In this work, we draw a link between OOD performance and model calibration, arguing that calibration across multiple domains can be viewed as a special case of an invariant representation leading to better OOD generalization. Specifically, we show that under certain conditions, models which achieve \\emph{multi-domain calibration} are provably free of spurious correlations. This leads us to propose multi-domain calibration as a measurable and trainable surrogate for the OOD performance of a classifier. We therefore introduce methods that are easy to apply and allow practitioners to improve multi-domain calibration by training or modifying an existing model, leading to better performance on unseen domains. Using four datasets from the recently proposed WILDS OOD benchmark, as well as the Colored MNIST, we demonstrate that training or tuning models so they are calibrated across multiple domains leads to significantly improved performance on unseen test domains. We believe this intriguing connection between calibration and OOD generalization is promising from both a practical and theoretical point of view.\n\n"}}
{"id": "BJGusHre8B", "cdate": 1567802784402, "mdate": null, "content": {"title": "Globally Optimal Learning for Structured Elliptical Losses", "abstract": "Heavy tailed and contaminated data are common in various applications of machine learning. A standard technique to handle regression tasks that involve such data, is to use robust losses, e.g., the popular Huber's loss.  In structured problems, however, where there are multiple labels and structural constraints on the labels are imposed (or learned), robust optimization is challenging, and more often than not the loss used is simply the negative log-likelihood of a Gaussian Markov random field. In this work, we analyze robust alternatives.  Theoretical understanding of such problems is quite limited, with guarantees on optimization given only for special cases and non-structured settings. The core of the difficulty is the non-convexity of the objective function, implying that standard optimization algorithms may converge to sub-optimal critical points.  Our analysis focuses on loss functions that arise from  elliptical distributions, which appealingly include most loss functions proposed in the literature as special cases. We prove that, even though these problems are non-convex, they can be optimized efficiently. Concretely, we show that at the limit of infinite training data, due to algebraic properties of the problem, all stationary points are globally optimal. Finally, we demonstrate the empirical appeal of using these losses for regression on synthetic and real-life data."}}
{"id": "S1ZfHDZOZH", "cdate": 1483228800000, "mdate": null, "content": {"title": "Robust Conditional Probabilities", "abstract": "Conditional probabilities are a core concept in machine learning. For example, optimal prediction of a label $Y$ given an input $X$ corresponds to maximizing the conditional probability of $Y$ given $X$. A common approach to inference tasks is learning a model of conditional probabilities. However, these models are often based on strong assumptions (e.g., log-linear models), and hence their estimate of conditional probabilities is not robust and is highly dependent on the validity of their assumptions. Here we propose a framework for reasoning about conditional probabilities without assuming anything about the underlying distributions, except knowledge of their second order marginals, which can be estimated from data. We show how this setting leads to guaranteed bounds on conditional probabilities, which can be calculated efficiently in a variety of settings, including structured-prediction. Finally, we apply them to semi-supervised deep learning, obtaining results competitive with variational autoencoders."}}
