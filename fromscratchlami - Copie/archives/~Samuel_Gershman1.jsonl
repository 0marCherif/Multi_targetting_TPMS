{"id": "QNZ9GSeGEya", "cdate": 1683926966102, "mdate": 1683926966102, "content": {"title": "Action chunking as policy compression", "abstract": "Many skills in our everyday lives are learned by sequencing actions toward a desired goal. The action sequence can become a \\chunk\" when individual actions are grouped together and executed as one unit, making them more efficient to store and execute. While chunking has been studied extensively across various domains, a puzzle remains as to why and under what conditions action chunking occurs. To tackle these questions, we apply the concept of policy compression|the reduction in cognitive cost by making policies simpler to action selection in order to explain the origin of chunking. We argue that chunking is a result of optimizing the trade-off between reward and policy\ncomplexity. Chunking compresses policies when there is structure in the mapping from environment states to optimal action sequences, reducing the amount of memory necessary to encode the policy. We experimentally con\u2000rm our model's predictions, showing that chunking reduces policy complexity and reaction time. Chunking also increases with working memory load, consistent with the hypothesis that the degree of policy compression scales with the scarcity of cognitive resources."}}
{"id": "R4lQmXwUZfQ", "cdate": 1675209600000, "mdate": 1682326662778, "content": {"title": "The molecular memory code and synaptic plasticity: A synthesis", "abstract": ""}}
{"id": "I7WpCrjWYTV", "cdate": 1669852800000, "mdate": 1682326662924, "content": {"title": "Compositional Sequence Generation in the Entorhinal-Hippocampal System", "abstract": "Neurons in the medial entorhinal cortex exhibit multiple, periodically organized, firing fields which collectively appear to form an internal representation of space. Neuroimaging data suggest that this grid coding is also present in other cortical areas such as the prefrontal cortex, indicating that it may be a general principle of neural functionality in the brain. In a recent analysis through the lens of dynamical systems theory, we showed how grid coding can lead to the generation of a diversity of empirically observed sequential reactivations of hippocampal place cells corresponding to traversals of cognitive maps. Here, we extend this sequence generation model by describing how the synthesis of multiple dynamical systems can support compositional cognitive computations. To empirically validate the model, we simulate two experiments demonstrating compositionality in space or in time during sequence generation. Finally, we describe several neural network architectures supporting various types of compositionality based on grid coding and highlight connections to recent work in machine learning leveraging analogous techniques."}}
{"id": "ahNbktjeAJH", "cdate": 1654933348450, "mdate": 1654933348450, "content": {"title": "Language-Mediated, Object-Centric Representation Learning", "abstract": "We present Language-mediated, Object-centric Representation Learning (LORL), a paradigm for learning disentangled, object-centric scene representations from vision and language. LORL builds upon recent advances in unsupervised object discovery and segmentation, notably MONet and Slot Attention. While these algorithms learn an object-centric representation just by reconstructing the input image, LORL enables them to further learn to associate the learned representations to concepts, i.e., words for object categories, properties, and spatial relationships, from language input. These object-centric concepts derived from language facilitate the learning of object-centric representations. LORL can be integrated with various unsupervised object discovery algorithms that are language-agnostic. Experiments show that the integration of LORL consistently improves the performance of unsupervised object discovery methods on two datasets via the help of language. We also show that concepts learned by LORL, in conjunction with object discovery methods, aid downstream tasks such as referring expression comprehension."}}
{"id": "C_ABgIB4_-", "cdate": 1640995200000, "mdate": 1682326662930, "content": {"title": "Hybrid Memoised Wake-Sleep: Approximate Inference at the Discrete-Continuous Interface", "abstract": "Modeling complex phenomena typically involves the use of both discrete and continuous variables. Such a setting applies across a wide range of problems, from identifying trends in time-series data to performing effective compositional scene understanding in images. Here, we propose Hybrid Memoised Wake-Sleep (HMWS), an algorithm for effective inference in such hybrid discrete-continuous models. Prior approaches to learning suffer as they need to perform repeated expensive inner-loop discrete inference. We build on a recent approach, Memoised Wake-Sleep (MWS), which alleviates part of the problem by memoising discrete variables, and extend it to allow for a principled and effective way to handle continuous variables by learning a separate recognition model used for importance-sampling based approximate inference and marginalization. We evaluate HMWS in the GP-kernel learning and 3D scene understanding domains, and show that it outperforms current state-of-the-art inference methods."}}
{"id": "auOPcdAcoy", "cdate": 1632875678015, "mdate": null, "content": {"title": "Hybrid Memoised Wake-Sleep: Approximate Inference at the Discrete-Continuous Interface", "abstract": "Modeling complex phenomena typically involves the use of both discrete and continuous variables. Such a setting applies across a wide range of problems, from identifying trends in time-series data to performing effective compositional scene understanding in images. Here, we propose Hybrid Memoised Wake-Sleep (HMWS), an algorithm for effective inference in such hybrid discrete-continuous models. Prior approaches to learning suffer as they need to perform repeated expensive inner-loop discrete inference. We build on a recent approach, Memoised Wake-Sleep (MWS), which alleviates part of the problem by memoising discrete variables, and extend it to allow for a principled and effective way to handle continuous variables by learning a separate recognition model used for importance-sampling based approximate inference and marginalization. We evaluate HMWS in the GP-kernel learning and 3D scene understanding domains, and show that it outperforms current state-of-the-art inference methods."}}
{"id": "1vC5GFOXuhM", "cdate": 1622646892727, "mdate": null, "content": {"title": "CCNLab: A Benchmarking Framework for Computational Cognitive Neuroscience", "abstract": "CCNLab is a benchmark for evaluating computational cognitive neuroscience models on empirical data. As a starting point, its focus is classical conditioning, which studies how animals predict reward and punishment in the environment. CCNLab includes a collection of simulations of seminal experiments expressed under a common API, as wells as tools for visualizing and comparing simulated data with empirical data. CCNLab is broad, incorporating representative experiments from different categories of phenomena; flexible, allowing the straightforward addition of new experiments; and easy-to-use, so researchers can focus on developing better models. We envision CCNLab as a testbed for unifying computational theories of learning in the brain. We also hope that it can broadly accelerate neuroscience research and facilitate interaction between the fields of neuroscience, psychology, and artificial intelligence."}}
{"id": "_p_SPo2YANV", "cdate": 1609459200000, "mdate": 1652290068375, "content": {"title": "Hybrid Memoised Wake-Sleep: Approximate Inference at the Discrete-Continuous Interface", "abstract": "Modeling complex phenomena typically involves the use of both discrete and continuous variables. Such a setting applies across a wide range of problems, from identifying trends in time-series data to performing effective compositional scene understanding in images. Here, we propose Hybrid Memoised Wake-Sleep (HMWS), an algorithm for effective inference in such hybrid discrete-continuous models. Prior approaches to learning suffer as they need to perform repeated expensive inner-loop discrete inference. We build on a recent approach, Memoised Wake-Sleep (MWS), which alleviates part of the problem by memoising discrete variables, and extend it to allow for a principled and effective way to handle continuous variables by learning a separate recognition model used for importance-sampling based approximate inference and marginalization. We evaluate HMWS in the GP-kernel learning and 3D scene understanding domains, and show that it outperforms current state-of-the-art inference methods."}}
{"id": "MK-EZhhqAAQ", "cdate": 1609459200000, "mdate": 1682326662932, "content": {"title": "Neural signatures of arbitration between Pavlovian and instrumental action selection", "abstract": "Author summary Using a combination of computational modeling, neuroimaging (both EEG and fMRI), and behavioral analysis, we present evidence for a dual-process architecture in which Pavlovian and instrumental action values are adaptively combined through a Bayesian arbitration mechanism. Building on prior research, we find neural signatures of this arbitration mechanism in frontal cortex. In particular, we show that trial-by-trial changes in Pavlovian influences on action can be predicted by our computational model, and are reflected in midfrontal theta power, as well as inferior frontal and ventromedial prefrontal cortex fMRI responses."}}
{"id": "EjHdR_-d0Te", "cdate": 1609459200000, "mdate": 1667342742346, "content": {"title": "Language-Mediated, Object-Centric Representation Learning", "abstract": ""}}
