{"id": "SgZ-WUSHxc", "cdate": 1609459200000, "mdate": 1645725177338, "content": {"title": "Filtered poisson process bandit on a continuum", "abstract": "Highlights \u2022 We propose a continuum-armed bandit model of surveillance resource allocation. \u2022 Poisson process data is given as feedback, with filtering depending on the actions. \u2022 We derive an effective upper confidence bound approach with adaptive discretisation. \u2022 We show matching upper and lower bounds on the regret. Abstract We consider a version of the continuum armed bandit where an action induces a filtered realisation of a non-homogeneous Poisson process. Point data in the filtered sample are then revealed to the decision-maker, whose reward is the total number of revealed points. Using knowledge of the function governing the filtering, but without knowledge of the Poisson intensity function, the decision-maker seeks to maximise the expected number of revealed points over T rounds. We propose an upper confidence bound algorithm for this problem utilising data-adaptive discretisation of the action space. This approach enjoys O \u02dc ( T 2 / 3 ) regret under a Lipschitz assumption on the reward function. We provide lower bounds on the regret of any algorithm for the problem, via new lower bounds for related finite-armed bandits, and show that the orders of the upper and lower bounds match up to a logarithmic factor."}}
{"id": "SRMWb-8rHxq", "cdate": 1609459200000, "mdate": 1645725177338, "content": {"title": "Apple Tasting Revisited: Bayesian Approaches to Partially Monitored Online Binary Classification", "abstract": "We consider a variant of online binary classification where a learner sequentially assigns labels ($0$ or $1$) to items with unknown true class. If, but only if, the learner chooses label $1$ they immediately observe the true label of the item. The learner faces a trade-off between short-term classification accuracy and long-term information gain. This problem has previously been studied under the name of the `apple tasting' problem. We revisit this problem as a partial monitoring problem with side information, and focus on the case where item features are linked to true classes via a logistic regression model. Our principal contribution is a study of the performance of Thompson Sampling (TS) for this problem. Using recently developed information-theoretic tools, we show that TS achieves a Bayesian regret bound of an improved order to previous approaches. Further, we experimentally verify that efficient approximations to TS and Information Directed Sampling via P\\'{o}lya-Gamma augmentation have superior empirical performance to existing methods."}}
{"id": "sofMrwT5nRj", "cdate": 1577836800000, "mdate": null, "content": {"title": "Filtered Poisson Process Bandit on a Continuum", "abstract": "We consider a version of the continuum armed bandit where an action induces a filtered realisation of a non-homogeneous Poisson process. Point data in the filtered sample are then revealed to the decision-maker, whose reward is the total number of revealed points. Using knowledge of the function governing the filtering, but without knowledge of the Poisson intensity function, the decision-maker seeks to maximise the expected number of revealed points over T rounds. We propose an upper confidence bound algorithm for this problem utilising data-adaptive discretisation of the action space. This approach enjoys O(T^(2/3)) regret under a Lipschitz assumption on the reward function. We provide lower bounds on the regret of any algorithm for the problem, via new lower bounds for related finite-armed bandits, and show that the orders of the upper and lower bounds match up to a logarithmic factor."}}
{"id": "n9WAUetVvtQ", "cdate": 1577836800000, "mdate": null, "content": {"title": "Adaptive policies for perimeter surveillance problems", "abstract": "Highlights \u2022 Sequential search problem is formulated as a combinatorial multi-armed bandit. \u2022 We consider a novel setting with action-dependent filtering of Poisson rewards. \u2022 A new upper confidence bound approach based on martingale inequalities is proposed. \u2022 We derive order-optimal upper bounds on the regret of the approach. \u2022 We verify its strong performance and robustness in extensive numerical experiments. Abstract We consider the problem of sequentially choosing observation regions along a line, with an aim of maximising the detection of events of interest. Such a problem may arise when monitoring the movements of endangered or migratory species, detecting crossings of a border, policing activities at sea, and in many other settings. In each case, the key operational challenge is to learn an allocation of surveillance resources which maximises successful detection of events of interest. We present a combinatorial multi-armed bandit model with Poisson rewards and a novel filtered feedback mechanism - arising from the failure to detect certain intrusions - where reward distributions are dependent on the actions selected. Our solution method is an upper confidence bound approach and we derive upper and lower bounds on its expected performance. We prove that the gap between these bounds is of constant order, and demonstrate empirically that our approach is more reliable in simulated problems than competing algorithms."}}
{"id": "bNttHHoo9Ad", "cdate": 1577836800000, "mdate": null, "content": {"title": "On Thompson Sampling for Smoother-than-Lipschitz Bandits", "abstract": "Thompson Sampling is a well established approach to bandit and reinforcement learning problems. However its use in continuum armed bandit problems has received relatively little attention. We provide the first bounds on the regret of Thompson Sampling for continuum armed bandits under weak conditions on the function class containing the true function and sub-exponential observation noise. The eluder dimension is a recently proposed measure of the complexity of a function class, which has been demonstrated to be useful in bounding the Bayesian regret of Thompson Sampling for simpler bandit problems under sub-Gaussian observation noise. We derive a new bound on the eluder dimension for classes of functions with Lipschitz derivatives, and generalise previous analyses in multiple regards."}}
{"id": "PSs9lk8h6_", "cdate": 1577836800000, "mdate": null, "content": {"title": "Learning to Rank under Multinomial Logit Choice", "abstract": "Learning the optimal ordering of content is an important challenge in website design. The learning to rank (LTR) framework models this problem as a sequential problem of selecting lists of content and observing where users decide to click. Most previous work on LTR assumes that the user considers each item in the list in isolation, and makes binary choices to click or not on each. We introduce a multinomial logit (MNL) choice model to the LTR framework, which captures the behaviour of users who consider the ordered list of items as a whole and make a single choice among all the items and a no-click option. Under the MNL model, the user favours items which are either inherently more attractive, or placed in a preferable position within the list. We propose upper confidence bound algorithms to minimise regret in two settings - where the position dependent parameters are known, and unknown. We present theoretical analysis leading to an $\\Omega(\\sqrt{T})$ lower bound for the problem, an $\\tilde{O}(\\sqrt{T})$ upper bound on regret for the known parameter version. Our analyses are based on tight new concentration results for Geometric random variables, and novel functional inequalities for maximum likelihood estimators computed on discrete data."}}
{"id": "ELAAY9UzGQG", "cdate": 1577836800000, "mdate": null, "content": {"title": "On Thompson Sampling for Smoother-than-Lipschitz Bandits", "abstract": "Thompson Sampling is a well established approach to bandit and reinforcement learning problems. However its use in continuum armed bandit problems has received relatively little attention. We provide the first bounds on the regret of Thompson Sampling for continuum armed bandits under weak conditions on the function class containing the true function and sub-exponential observation noise. Our bounds are realised by analysis of the eluder dimension, a recently proposed measure of the complexity of a function class, which has been demonstrated to be useful in bounding the Bayesian regret of Thompson Sampling for simpler bandit problems under sub-Gaussian observation noise. We derive a new bound on the eluder dimension for classes of functions with Lipschitz derivatives, and generalise previous analyses in multiple regards."}}
{"id": "SkVnxibdWH", "cdate": 1546300800000, "mdate": null, "content": {"title": "Adaptive Sensor Placement for Continuous Spaces", "abstract": "We consider the problem of adaptively placing sensors along an interval to detect stochastically-generated events. We present a new formulation of the problem as a continuum-armed bandit problem wi..."}}
{"id": "XCpX8_IqrP4", "cdate": 1514764800000, "mdate": null, "content": {"title": "Adaptive Policies for Perimeter Surveillance Problems", "abstract": "Maximising the detection of intrusions is a fundamental and often critical aim of perimeter surveillance. Commonly, this requires a decision-maker to optimally allocate multiple searchers to segments of the perimeter. We consider a scenario where the decision-maker may sequentially update the searchers' allocation, learning from the observed data to improve decisions over time. In this work we propose a formal model and solution methods for this sequential perimeter surveillance problem. Our model is a combinatorial multi-armed bandit (CMAB) with Poisson rewards and a novel filtered feedback mechanism - arising from the failure to detect certain intrusions. Our solution method is an upper confidence bound approach and we derive upper and lower bounds on its expected performance. We prove that the gap between these bounds is of constant order, and demonstrate empirically that our approach is more reliable in simulated problems than competing algorithms."}}
{"id": "kbXvbDsUbjD", "cdate": 1483228800000, "mdate": null, "content": {"title": "Combinatorial Multi-Armed Bandits with Filtered Feedback", "abstract": "Motivated by problems in search and detection we present a solution to a Combinatorial Multi-Armed Bandit (CMAB) problem with both heavy-tailed reward distributions and a new class of feedback, filtered semibandit feedback. In a CMAB problem an agent pulls a combination of arms from a set $\\{1,...,k\\}$ in each round, generating random outcomes from probability distributions associated with these arms and receiving an overall reward. Under semibandit feedback it is assumed that the random outcomes generated are all observed. Filtered semibandit feedback allows the outcomes that are observed to be sampled from a second distribution conditioned on the initial random outcomes. This feedback mechanism is valuable as it allows CMAB methods to be applied to sequential search and detection problems where combinatorial actions are made, but the true rewards (number of objects of interest appearing in the round) are not observed, rather a filtered reward (the number of objects the searcher successfully finds, which must by definition be less than the number that appear). We present an upper confidence bound type algorithm, Robust-F-CUCB, and associated regret bound of order $\\mathcal{O}(\\ln(n))$ to balance exploration and exploitation in the face of both filtering of reward and heavy tailed reward distributions."}}
