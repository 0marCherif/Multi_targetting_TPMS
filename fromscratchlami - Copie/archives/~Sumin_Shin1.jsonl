{"id": "tS9MLN6e9W", "cdate": 1672531200000, "mdate": 1672799694303, "content": {"title": "Learning Polymorphic Neural ODEs With Time-Evolving Mixture", "abstract": ""}}
{"id": "N-4TbS1_OI", "cdate": 1640995200000, "mdate": 1672799694303, "content": {"title": "Music2Video: Automatic Generation of Music Video with fusion of audio and text", "abstract": ""}}
{"id": "ceXR2Dn4db", "cdate": 1609459200000, "mdate": 1672799694299, "content": {"title": "FedMix: Approximation of Mixup under Mean Augmented Federated Learning", "abstract": ""}}
{"id": "6BptWktRPK", "cdate": 1609459200000, "mdate": 1672799694302, "content": {"title": "FedMix: Approximation of Mixup under Mean Augmented Federated Learning", "abstract": ""}}
{"id": "Ogga20D2HO-", "cdate": 1601308326864, "mdate": null, "content": {"title": "FedMix: Approximation of Mixup under Mean Augmented Federated Learning", "abstract": "Federated learning (FL) allows edge devices to collectively learn a model without directly sharing data within each device, thus preserving privacy and eliminating the need to store data globally. While there are promising results under the assumption of independent and identically distributed (iid) local data, current state-of-the-art algorithms suffer a performance degradation as the heterogeneity of local data across clients increases. To resolve this issue, we propose a simple framework, \\emph{Mean Augmented Federated Learning (MAFL)}, where clients send and receive \\emph{averaged} local data, subject to the privacy requirements of target applications. Under our framework, we propose a new augmentation algorithm, named \\emph{FedMix}, which is inspired by a phenomenal yet simple data augmentation method, Mixup, but does not require local raw data to be directly shared among devices. Our method shows greatly improved performance in the standard benchmark datasets of FL, under highly non-iid federated settings, compared to conventional algorithms."}}
