{"id": "ydlmuadiFT", "cdate": 1672531200000, "mdate": 1683614567319, "content": {"title": "Depth-aware inverted refinement network for RGB-D salient object detection", "abstract": ""}}
{"id": "DUC0Tl_dYB", "cdate": 1672531200000, "mdate": 1683614567291, "content": {"title": "Adaptive Spatial Tokenization Transformer for Salient Object Detection in Optical Remote Sensing Images", "abstract": "Convolutional neural network (CNN)-based salient object detection (SOD) models have achieved promising performance in optical remote sensing images (ORSIs) in recent years. However, the restriction concerning the local sliding window operation of CNN has caused many existing CNN-based ORSI SOD models to still struggle with learning long-range relationships. To this end, a novel transformer framework is proposed for ORSI SOD, which is inspired by the powerful global dependency relationships of transformer networks. This is the first attempt to explore global and local details using transformer architecture for SOD in ORSIs. Concretely, we design an adaptive spatial tokenization transformer encoder to extract global\u2013local features, which can accurately sparsify tokens for each input image and achieve competitive performance in ORSI SOD tasks. Then, a specific dense token aggregation decoder (DTAD) is proposed to generate saliency results, including three cascade decoders to integrate the global\u2013local tokens and contextual dependencies. Extensive experiments indicate that the proposed model greatly surpasses 20 state-of-the-art (SOTA) SOD approaches on two standard ORSI SOD datasets under seven evaluation metrics. We also report comparison results to demonstrate the generalization capacity on the latest challenging ORSI datasets. In addition, we validate the contributions of different modules through a series of ablation analyses, especially the proposed adaptive spatial tokenization module (ASTM), which can halve the computational budget."}}
{"id": "wj5VLrdt8xR", "cdate": 1640995200000, "mdate": 1683614567295, "content": {"title": "A novel dynamic graph evolution network for salient object detection", "abstract": "The advanced deep convolution neural networks (CNNs) based salient object detection (SOD) models still suffer from the coarse object edge. The traditional graph-based SOD models can preserve the object edge well benefitting from the superpixels, but they perform weaker in highlighting the whole object compared to recent deep learning models. To tackle this problem, we attempt to find a new way to address this issue under the framework of graph convolution networks (GCNs). Specifically, we first model the image as a set of superpixels and construct the graph structure by connecting the k nearest neighbors for each node. For the connected nodes, rather than only leveraging on the predefined edges, we propose a multi-relations edge convolution operation, expecting to learn multiple implicit relations in the pair-wise nodes and aggregate the information from their neighbors relying on the learned edges. Then, a channel-wise attention operation is also proposed to boost the intra-node massage propagation across channels within the same node. In order to optimize the graph structure from layer to layer, we learn a new metric to re-measure the distance between any pair of nodes, and the graph structure evolves dynamically by recomputing the k nearest neighbors in the saliency feature space at different layers. Finally, a residual structure is applied to enable our graph network to go as deep as CNNs models. The graph nodes (superpixels) inherently belonging to the same class will be ideally clustered together in the learned embedding space. Experiments show that this work is a good practice for designing GCNs for image SOD and achieves the comparable performance with the recent state-of-the-art deep CNNs-based models."}}
{"id": "qPyvSsEQMLh", "cdate": 1640995200000, "mdate": 1683614567315, "content": {"title": "A novel spatiotemporal attention enhanced discriminative network for video salient object detection", "abstract": "In contrast to image salient object detection, on which many achievements have been made, video salient object detection remains a considerable challenge. Not all features are useful in salient object detection, and some even cause interferences. In this paper, we propose a novel multiscale spatiotemporal ConvLSTM model based on an attention mechanism, which introduces space-based and channel-based attention mechanisms and improves the network\u2019s capability to extract high-level semantic information and low-level spatial structural features. First, to obtain more effective spatiotemporal information, a ConvLSTM module embedded with an attention mechanism (CSAtt-ConvLSTM) is designed at higher layers of the network to weight salient features of the extracted spatiotemporal consistency. Second, a multiscale attention (MSA) module for distinguishing features is designed, which introduces two attention mechanisms: channel-wise attention (CA) units and spatial-wise attention (SA) units. The CA and SA units are used after high-level feature mapping obtained by the CSAtt-ConvLSTM module and shallow feature mapping, respectively, and then their outputs are fused as final output feature maps. A large number of experiments on multiple datasets verified the effectiveness of our proposed model, which reached a real-time speed on a single GPU of 20 fps."}}
{"id": "oOgqc7sDP0r", "cdate": 1640995200000, "mdate": 1683614567315, "content": {"title": "Visual tracking via dynamic saliency discriminative correlation filter", "abstract": "The discriminative correlation filter (DCF) is one of the crucial visual tracking methods, and it has outstanding performance. Nevertheless, DCF-based methods have an unavoidable boundary effect, which results in poor tracking performance in an abrupt scene, such as fast motion or deformation. To address this problem, we propose a novel dynamic saliency discriminative correlation filter for visual tracking. In our approach, a response guided saliency map is constructed to introduce saliency information into the filter. The method effectively highlights the target by further increasing the number of positive samples to alleviate the boundary effect. We also investigate an effective multifeature integration method to extract the target feature by employing the Felzenszwalb Histograms of Oriented Gradients (fHOG) from each color space. Finally, we apply a novel update approach to prevent filter model degradation, which uses a temporal regularization term to update the filter model. Extensive experiments on the standard OTB-2015 benchmark validate that our approach achieves competitive performance compared to other state-of-the-art trackers. Moreover, we conducted an ablation study to evaluate the effectiveness of the components in our tracker."}}
{"id": "CuRxKssJOo", "cdate": 1609459200000, "mdate": 1683614567312, "content": {"title": "Multi-Stream Attention-Aware Graph Convolution Network for Video Salient Object Detection", "abstract": "Recent advances in deep convolution neural networks (CNNs) boost the development of video salient object detection (SOD), and many remarkable deep-CNNs video SOD models have been proposed. However, many existing deep-CNNs video SOD models still suffer from coarse boundaries of the salient object, which may be attributed to the loss of high-frequency information. The traditional graph-based video SOD models can preserve object boundaries well by conducting superpixels/supervoxels segmentation in advance, but they perform weaker in highlighting the whole object than the latest deep-CNNs models, limited by heuristic graph clustering algorithms. To tackle this problem, we find a new way to address this issue under the framework of graph convolution networks (GCNs), taking advantage of graph model and deep neural network. Specifically, a superpixel-level spatiotemporal graph is first constructed among multiple frame-pairs by exploiting the motion cues implied in the frame-pairs. Then the graph data is imported into the devised multi-stream attention-aware GCN, where a novel Edge-Gated graph convolution (GC) operation is proposed to boost the saliency information aggregation on the graph data. A novel attention module is designed to encode the spatiotemporal sematic information via adaptive selection of graph nodes and fusion of the static-specific and the motion-specific graph embedding. Finally, a smoothness-aware regularization term is proposed to enhance the uniformity of salient object. Graph nodes (superpixels) inherently belonging to the same class will be ideally clustered together in the learned embedding space. Extensive experiments have been conducted on three widely used datasets. Compared with fourteen state-of-the-art video SOD models, our proposed method can well retain the salient object boundaries and possess a strong learning ability, which shows that this work is a good practice for designing GCNs for video SOD."}}
{"id": "6Wz_1YYLzEG", "cdate": 1609459200000, "mdate": 1683614567316, "content": {"title": "Co-Saliency Detection Via Unified Hierarchical Graph Neural Network With Geometric Attention", "abstract": "Co-saliency detection aims to identify the common and salient objects from a group of relevant images. The main challenge for co-saliency detection is how to mine and exploit the saliency cues of both intra-image and inter-image. In this paper, we present a novel unified hierarchical neural network (UHGNN). We first construct the graph model by segmenting the images into super-pixels and extracting the intra-image hierarchical saliency cues. Then, the inter-image hierarchical saliency representation is mined to form the unified two-dimensional hierarchical feature setup. We further propose the geometric attention module to make the most of the intra-image and inter-image cues. Our UHGNN model competes or outperforms the state-of-the-art methods on two co-saliency detection benchmark datasets (MSRC, iCoSeg)."}}
{"id": "LYZ25-ZqBr", "cdate": 1577836800000, "mdate": 1683614567283, "content": {"title": "Video Salient Object Detection via Robust Seeds Extraction and Multi-Graphs Manifold Propagation", "abstract": "Video salient object detection aims at distinguishing the salient objects from the complex background and highlighting them uniformly in the spatiotemporal domain, which still suffers from the interference of the complicated dynamic background in unconstrained videos. To address this problem, we propose a novel coarse-to-fine spatiotemporal salient object detection method. Specifically, we first model a novel motion energy to exclude the motion noise by exploiting the motion magnitude and motion orientation. Then, a supervoxel-level inter-frame graph model is constructed for each pair of adjacent frames independently, and a robust graph clustering-based saliency seed generation method is proposed to produce a coarse saliency map. Furthermore, the supervoxel-level inter-frame graph model is reconstructed by considering the regional spatiotemporal consistency constraint based on the coarse saliency map. The prior information obtained from pixel clustering is also taken into account to optimize the weight of the inter-frame graph model. Finally, a multi-graphs saliency propagation method is exploited under the manifold regularization framework by fusing the motion energy and appearance feature to refine the coarse saliency map. The extensive experiments on two widely used datasets validate the effectiveness and superiority of the proposed method against 13 state-of-the-art methods in terms of PR-curves, scores of S-measure, <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$F_{\\beta }$ </tex-math></inline-formula> , and MAE."}}
{"id": "pPlrPEN6Ht", "cdate": 1546300800000, "mdate": 1683614567254, "content": {"title": "Video Saliency Detection via Graph Clustering With Motion Energy and Spatiotemporal Objectness", "abstract": "We present a novel, robust estimation method to distinguish salient objects from complicated, dynamic backgrounds in videos. In this method, we propose a novel approach to model motion energy based on motion magnitude, motion orientation, gradient flow field, and spatial gradient of the video frame. Furthermore, an effective spatiotemporal objectness map is also proposed to estimate a compact object-like region in the current video frame leveraging both the objectness proposals and the saliency map of the previous frame. Then the current video frame is oversegmented into the granularity of superpixels using the simple linear iterative clustering algorithm. Each superpixel is designated as a node of a graph. The similarity between adjacent superpixels will be assigned as the weight of an edge that connects these two nodes. The feature values of motion energy and spatiotemporal objectness within each superpixel will be averaged respectively, and used to graphically cluster similar superpixels to form the detected salient object. Extensive experiments comparing this proposed new method against twelve existing salient object detection (SOD) methods have been performed using the benchmark datasets unconstrained video saliency detection and densely annotated video segmentation. Superior performance of this proposed SOD method has been observed through three well-known performance metrics: precision-recall curves, F-measure curves, and the mean absolute error."}}
