{"id": "gsDz6B2BjSa", "cdate": 1620425776844, "mdate": null, "content": {"title": "Learning Isometric Separation Maps", "abstract": "Maximum Variance Unfolding (MVU) and its variants have been very successful in embedding data-manifolds in lower dimensional spaces, often revealing the true intrinsic dimension. In this paper we show how to also incorporate supervised class information into an MVU-like method without breaking its convexity. We call this method the Isometric Separation Map and we show that the resulting kernel matrix can be used as a binary/multiclass Support Vector Machine-like method in a semi-supervised (transductive) framework. We also show that the method always finds a kernel matrix that linearly separates the training data exactly without projecting them in infinite dimensional spaces. In traditional SVMs we choose a kernel and hope that the data become linearly separable in the kernel space. In this paper we show how the hyperplane can be chosen ad-hoc and the kernel is trained so that data are always linearly separable. Comparisons with Large Margin SVMs show comparable performance.\n"}}
{"id": "19f1b2bxBDp", "cdate": 1620425730549, "mdate": null, "content": {"title": "Spectrum of Fractal Interpolation Functions", "abstract": "In this paper we compute the Fourier spectrum of the Fractal Interpolation Functions FIFs as introduced by Michael Barnsley. We show that there is an analytical way to compute them. In this paper we attempt to solve the inverse problem of FIF by using the spectrum"}}
{"id": "TnlIDpE20xb", "cdate": 1620425681410, "mdate": null, "content": {"title": "Data Science with Linear Programming", "abstract": "The standard process of data science tasks is to prepare features inside a database, export them as a denormalized data frame and then apply machine learning algorithms. This process is not optimal for two reasons. First, it requires denormalization of the database that can convert a small data problem into a big data problem. The second problem is that it assumes that the machine learning algorithm is disentangled from the relational model of the problem. That seems to be a serious limitation since the relational model contains very valuable domain expertise. In this paper we explore the use of convex optimization and specifically linear programming as a data science tool that can express most of the common machine learning algorithms and at the same time it can be natively integrated inside a declarative database. We are using SolverBlox, a framework that accepts as an input Datalog code and feeds it into a linear programming solver. We demonstrate the expression of three common machine learning algorithms, Linear Regression, Factorization Machines and Spectral Clustering, and present use case scenarios where data processing and modelling of optimization problems can be done step by step inside the database"}}
{"id": "wit30N_amLU", "cdate": 1620425634518, "mdate": null, "content": {"title": "Human in the Loop Enrichment of Product Graphs with Probabilistic Soft Logic", "abstract": "Product graphs have emerged as a powerful tool for online retailers\nto enhance product semantic search, catalog navigation, and recommendations. Their versatility stems from the fact that they can\nuniformly store and represent different relationships between products, their attributes, concepts or abstractions etc, in an actionable\nform. Such information may come from many, heterogeneous, disparate, and mostly unstructured data sources, rendering the product\ngraph creation task a major undertaking. Our work complements\nexisting efforts on product graph creation, by enabling field experts\nto directly control the graph completion process. We focus on the\nsubtask of enriching product graphs with product attributes and we\nemploy statistical relational learning coupled with a novel human\nin the loop enhanced inference workflow based on Probabilistic Soft\nLogic (PSL), to reliably predict product-attribute relationships. Our\npreliminary experiments demonstrate the viability, practicality and\neffectiveness of our approach and its competitiveness comparing\nwith alternative methods. As a by-product, our method generates\nprobabilistic fact validity labels from an originally unlabeled database that can subsequently be leveraged by other graph completion\nmethods.\n"}}
{"id": "i8U7_c-uw2i", "cdate": 1620425570272, "mdate": null, "content": {"title": "Optimizing Training Data for Image Classifiers", "abstract": "In this paper, we propose a robust method for outlier removal to\nimprove the performance for image classification. Increasing the\nsize of training data does not necessarily raise prediction accuracy,\ndue to instances that may be poor representatives of their respective classes. Four separate experiments are tested to evaluate the\neffectiveness of outlier removal for several classifiers. Embeddings\nare generated from a pre-trained neural network, a fine-tuned network, as well as a Siamese network. Subsequently, outlier detection\nis evaluated based on clustering quality and classifier performance\nfrom a fully-connected feed-forward network, K-Nearest Neighbors\nand gradient boosting model."}}
{"id": "RTsxYHEtkoM", "cdate": 1620425521510, "mdate": null, "content": {"title": "Hyperkernel Based Density Estimation", "abstract": "We focus on solving the problem of learning an optimal smoothing kernel for the\nunsupervised learning problem of kernel density estimation(KDE) by using hyperkernels. The optimal kernel is the one which minimizes the regularized negative\nleave-one-out-log likelihood score of the train set. We demonstrate that \u201dfixed\nbandwidth\u201d and \u201dvariable bandwidth\u201d KDE are special cases of our algorithm."}}
{"id": "X7ROgWVlUn_", "cdate": 1620425477466, "mdate": null, "content": {"title": "Extending Datalog with Analytics in LogicBlox", "abstract": "LogicBlox is a database product designed for enterprise software development, combining transactions and analytics. The underying data model is a relational database,\nand the query language, LogiQL, is an extension of Datalog [13]. As such, LogiQL\nfeatures a simple and unified syntax for traditional relational manipulation as well as\ndeeper analytics. Moreover, its declarative nature allows for substantial static analysis\nfor optimizing evaluation schemes, parallelization, and incremental maintenance, and it\nallows for sophisticated transactional management [11]. In this paper, we describe various extensions of Datalog for supporting prescriptive and predictive analytics. These extensions come in the form of mathematical optimization (mixed integer programming),\nmachine-learning capabilities, statistical relational models, and probabilistic programming. Some of these extensions are currently"}}
{"id": "NoRXEgqTnhH", "cdate": 1620425431222, "mdate": null, "content": {"title": "Learning distances to improve phoneme classification", "abstract": "In this work we aim to learn a Mahalanobis distance to improve the performance of phoneme classification using the standard 39-dimensional MFCC features. To learn and to evaluate the performance of our distance, we use the simple k-nearest-neighbors (k-NN) classifier. Although this classifier exhibits low performance relative to state-of-the-art phoneme classifiers, it can be used to determine a distance metric that is applicable to many other better-performing machine learning methods. We devise a novel optimization method that minimizes the error function of the k-NN classifier with respect to the covariance matrix of the Mahalanobis distance, based on finite-difference stochastic approximation (FDSA) gradient estimates combined with a random perturbation term to avoid local minima. We apply our method to the problem of phoneme classification with the k-NN classifier and show that our learned distance provides performance improvement of up to 8:19% over the standard k-NN classifier, and additionally outperforms other state-of-the-art distance learning methods by approximately 4 percentage points. We also find that the computational complexity of our method, while not optimal, is better than other distance learning methods. The performance improvements for individual phoneme classes are given. The distances learned are applicable to other scale-variant machine learning methods, such as support vector machines, multidimensional scaling, and maximum variance unfolding, as well as others."}}
{"id": "IaUM_fRBsIY", "cdate": 1620425381366, "mdate": null, "content": {"title": "Parameter Estimation for Manifold Learning, Through Density Estimation", "abstract": "Manifold learning turns out to be a very useful tool for many applications of machine learning, such as classification. Unfortunately the existing algorithms use ad hoc selection of the parameters that define the geometry of the manifold. The parameter choice affects significantly the performance of manifold learning algorithms. Recent theoretical work has proven the equivalence between the Mercer kernel learning methods and the kernel in kernel density estimation. Based on this fact the problem of kernel parameter estimation for manifold learning is addressed based on the nonparametric statistical theory estimation. An automatic way of determining the local bandwidths that define the geometry is introduced. The results show that the automatic bandwidth selection leads to improved clustering performance and reduces the computational load versus ad hoc selection."}}
{"id": "1CkBeBteTjO", "cdate": 1620425335378, "mdate": null, "content": {"title": "Scalable semidefinite manifold learning", "abstract": "Maximum variance unfolding (MVU) is among the state of the art manifold learning (ML) algorithms and experimentally proven to be the best method to unfold a manifold to its intrinsic dimension. Unfortunately it doesnpsilat scale for more than a few hundred points. A non convex formulation of MVU made it possible to scale up to a few thousand points with the risk of getting trapped in local minima. In this paper we demonstrate techniques based on the dual-tree algorithm and L-BFGS that allow MVU to scale up to 100,000 points. We also present a new variant called maximum furthest neighbor unfolding (MFNU) which performs even better than MVU in terms of avoiding local minima."}}
