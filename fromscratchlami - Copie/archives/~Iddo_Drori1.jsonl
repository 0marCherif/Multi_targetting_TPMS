{"id": "wAa3ch5miVf", "cdate": 1672531200000, "mdate": 1696347876895, "content": {"title": "From Human Days to Machine Seconds: Automatically Answering and Generating Machine Learning Final Exams", "abstract": "A final exam in machine learning at a top institution such as MIT, Harvard, or Cornell typically takes faculty days to write, and students hours to solve. We demonstrate that large language models pass machine learning finals at a human level on finals available online and automatically generate new human-quality final exam questions in seconds. Previous work has developed program synthesis and few-shot learning methods to solve university-level problem set questions in mathematics and STEM courses. In this work, we develop and compare methods that solve final exams, which differ from problem sets in several ways: the questions are longer, have multiple parts, are more complicated, and span a broader set of topics. We curate a dataset and benchmark of questions from machine learning final exams available online and code for answering these questions and generating new questions. We show how to generate new questions from other questions and course notes. For reproducibility and future research on this final exam benchmark, we use automatic checkers for multiple-choice, numeric, and questions with expression answers. A student survey comparing the quality, appropriateness, and difficulty of machine-generated questions with human-written questions shows that across multiple aspects, machine-generated questions are indistinguishable from human-generated questions and are suitable for final exams. We perform ablation studies comparing zero-shot learning with few-shot learning and chain-of-thought prompting using GPT-3, OPT, Codex, and ChatGPT across machine learning topics and find that few-shot learning methods perform best. We highlight the transformative potential of language models to streamline the writing and solution of large-scale assessments, significantly reducing the workload from human days to mere machine seconds. Our results suggest that rather than banning large language models such as ChatGPT in class, instructors should teach students to harness them by asking students meta-questions about correctness, completeness, and originality of the responses generated, encouraging critical thinking in academic studies."}}
{"id": "cRuoZeTalYm", "cdate": 1672531200000, "mdate": 1699690678063, "content": {"title": "A Dataset for Learning University STEM Courses at Scale and Generating Questions at a Human Level", "abstract": "We present a new dataset for learning to solve, explain, and generate university-level STEM questions from 27 courses across a dozen departments in seven universities. We scale up previous approaches to questions from courses in the departments of Mechanical Engineering, Materials Science and Engineering, Chemistry, Electrical Engineering, Computer Science, Physics, Earth Atmospheric and Planetary Sciences, Economics, Mathematics, Biological Engineering, Data Systems, and Society, and Statistics. We visualize similarities and differences between questions across courses. We demonstrate that a large foundation model is able to generate questions that are as appropriate and at the same difficulty level as human-written questions."}}
{"id": "MT1Pcdo8sGG", "cdate": 1663850534063, "mdate": null, "content": {"title": "Automatically Answering and Generating Machine Learning Final Exams", "abstract": "Can a machine learn machine learning? We propose to answer this question using the same criteria we use to answer a similar question: can a human learn machine learning? We automatically answer final exams in MIT's recent large machine learning course and generate new questions at a human level. Recently, program synthesis and few-shot learning solved university-level problem set questions in mathematics and STEM courses at a human level. In this work, we solve questions from final exams that differ from problem sets in several ways: the questions are longer, have multiple parts, are more complicated, and span a broader set of topics. We provide a new dataset and benchmark of questions from machine learning final exams and code for automatically answering these questions and generating new questions. To make our dataset a reproducible benchmark, we use automatic checkers for multiple choice questions, questions with numeric answers, and questions with expression answers, and evaluate a large free language model, Meta\u2019s OPT, and compare the results with Open AI\u2019s GPT-3 and Codex. A student survey comparing the quality, appropriateness, and difficulty of machine-generated questions with human-written questions shows that across multiple aspects, machine-generated questions are indistinguishable from human-generated questions and are suitable for final exams. We perform ablation studies comparing zero-shot learning with few-shot learning, chain-of-thought prompting, GPT-3 and OPT pre-trained on text and Codex fine-tuned on code on a range of machine learning topics and find that few-shot learning methods perform best. We make our data and code publicly available for the machine learning community."}}
{"id": "x-XKh_LaB3", "cdate": 1640995200000, "mdate": 1682001888915, "content": {"title": "Human Evaluation of Text-to-Image Models on a Multi-Task Benchmark", "abstract": "We provide a new multi-task benchmark for evaluating text-to-image models. We perform a human evaluation comparing the most common open-source (Stable Diffusion) and commercial (DALL-E 2) models. Twenty computer science AI graduate students evaluated the two models, on three tasks, at three difficulty levels, across ten prompts each, providing 3,600 ratings. Text-to-image generation has seen rapid progress to the point that many recent models have demonstrated their ability to create realistic high-resolution images for various prompts. However, current text-to-image methods and the broader body of research in vision-language understanding still struggle with intricate text prompts that contain many objects with multiple attributes and relationships. We introduce a new text-to-image benchmark that contains a suite of thirty-two tasks over multiple applications that capture a model's ability to handle different features of a text prompt. For example, asking a model to generate a varying number of the same object to measure its ability to count or providing a text prompt with several objects that each have a different attribute to identify its ability to match objects and attributes correctly. Rather than subjectively evaluating text-to-image results on a set of prompts, our new multi-task benchmark consists of challenge tasks at three difficulty levels (easy, medium, and hard) and human ratings for each generated image."}}
{"id": "KD4rbE8KqIk", "cdate": 1640995200000, "mdate": 1668714959090, "content": {"title": "Solving Probability and Statistics Problems by Probabilistic Program Synthesis at Human Level and Predicting Solvability", "abstract": "We use probabilistic program synthesis to solve questions in MIT and Harvard Probability and Statistics courses. Traditional approaches using the latest GPT-3 language model without program synthesis achieve a solve rate of 0.2 in these classes. In contrast, by turning course questions into probabilistic programs using the latest program synthesis Transformer, OpenAI Codex, and executing the programs, our solve rates are 0.9 and 0.88, which are on par with human performance."}}
{"id": "Hy-cEZLWn2g", "cdate": 1640995200000, "mdate": 1668957814716, "content": {"title": "Generalizing Imaging Through Scattering Media With Uncertainty Estimates", "abstract": "Imaging through scattering media is challenging: object features are hidden under highly-scattered photons. Conventional methods that characterize scattering properties, such as the media input-output transmission matrix, are susceptible to environmental disturbance that is not ideal for many imaging scenarios, especially in biomedical imaging. Learning from examples is ideal for imaging in highly scattered regimes because it is adaptable and accurate even when the microstructures of the scattering media change. In current approaches, network output on unseen scattering media contain artifacts that inhibit meaningful object recognition. We present a network architecture that is able to generate high quality images over a range of different scattering media and image sizes with minimal artifacts. Our network learns the statistical information within highly scattered speckle intensity patterns. This allows us to compute an accurate mapping from different speckle patterns to their corresponding objects given scattering media with varying microstructures. Our network demonstrates superior performance compared to similar models, especially when trained on a single scattering medium and then tested on unseen scattering media. We estimate the uncertainty of our approach and use the available data efficiently, increasing the generalizability of predicting objects from unseen scattering media with multiple different diffusers."}}
{"id": "OG78-TuPcvL", "cdate": 1633790965318, "mdate": null, "content": {"title": "Quantifying and Alleviating Distribution Shifts in Foundation Models on Review Classification", "abstract": "This work quantifies the extent to which accuracy degrades on review classification when state-of-the-art Transformer models are subjected to distribution shifts, and offers a solution to significantly decrease this degradation. We find differences in the extent of degradation depending on the independent variable across which the shift is created. Specifically, in our experiments time and sentiment shifts show upto 10% drops in accuracy; whereas shifts between industry and product sectors show 20-40% drops in accuracy. We provide ablation experiments with different Transformer architectures, such as BERT, T5 and Jurassic-I, and study their relationship with this degradation. The suggested solution reuses the base of the model trained on one distribution, in addition to fine-tuning the final dense layer in the model to support the new distribution that is seen once the model is deployed. This uses just 100-300 samples compared to the previous 10,000 samples from the unseen distribution, while decreasing the accuracy drops in half."}}
{"id": "wPon886vV_", "cdate": 1609459200000, "mdate": 1668957814701, "content": {"title": "Predicting Critical Biogeochemistry of the Southern Ocean for Climate Monitoring", "abstract": "The Biogeochemical-Argo (BGC-Argo) program is building a network of globally distributed, sensor-equipped robotic profiling floats, improving our understanding of the climate system and how it is changing. These floats, however, are limited in the number of variables measured. In this study, we train neural networks to predict silicate and phosphate values in the Southern Ocean from temperature, pressure, salinity, oxygen, nitrate, and location and apply these models to earth system model (ESM) and BGC-Argo data to expand the utility of this ocean observation network. We trained our neural networks on observations from the Global Ocean Ship-Based Hydrographic Investigations Program (GO-SHIP) and use dropout regularization to provide uncertainty bounds around our predicted values. Our neural network significantly improves upon linear regression but shows variable levels of uncertainty across the ranges of predicted variables. We explore the generalization of our estimators to test data outside our training distribution from both ESM and BGC-Argo data. Our use of out-of-distribution test data to examine shifts in biogeochemical parameters and calculate uncertainty bounds around estimates advance the state-of-the-art in oceanographic data and climate monitoring. We make our data and code publicly available."}}
{"id": "tOw69XyY6n", "cdate": 1609459200000, "mdate": 1668225827446, "content": {"title": "Solving Machine Learning Problems", "abstract": "Can a machine learn Machine Learning? This work trains a machine learning model to solve machine learning problems from a University undergraduate level course. We generate a new training set of qu..."}}
{"id": "rwfxSti1Ija", "cdate": 1609459200000, "mdate": 1668957814670, "content": {"title": "Image2Lego: Customized LEGO Set Generation from Images", "abstract": "Although LEGO sets have entertained generations of children and adults, the challenge of designing customized builds matching the complexity of real-world or imagined scenes remains too great for the average enthusiast. In order to make this feat possible, we implement a system that generates a LEGO brick model from 2D images. We design a novel solution to this problem that uses an octree-structured autoencoder trained on 3D voxelized models to obtain a feasible latent representation for model reconstruction, and a separate network trained to predict this latent representation from 2D images. LEGO models are obtained by algorithmic conversion of the 3D voxelized model to bricks. We demonstrate first-of-its-kind conversion of photographs to 3D LEGO models. An octree architecture enables the flexibility to produce multiple resolutions to best fit a user's creative vision or design needs. In order to demonstrate the broad applicability of our system, we generate step-by-step building instructions and animations for LEGO models of objects and human faces. Finally, we test these automatically generated LEGO sets by constructing physical builds using real LEGO bricks."}}
