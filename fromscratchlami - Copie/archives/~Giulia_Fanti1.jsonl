{"id": "b1tvKVCtEbQ", "cdate": 1695999679762, "mdate": null, "content": {"title": "Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity", "abstract": "While deep learning (DL) models are state-of-the-art in text and image domains, they have not yet consistently outperformed Gradient Boosted Decision Trees (GBDTs) on tabular Learning-To-Rank (LTR) problems. Most of the recent performance gains attained by DL models in text and image tasks have used unsupervised pretraining, which exploits orders of magnitude more unlabeled data than labeled data. To the best of our knowledge, unsupervised pretraining has not been applied to the LTR problem, which often produces vast amounts of unlabeled data. In this work, we study whether unsupervised pretraining can improve LTR performance over GBDTs and other non-pretrained models. Using simple design choices--including SimCLR-Rank, our ranking-specific modification of SimCLR (an unsupervised pretraining method for images)--we produce pretrained deep learning models that soundly outperform GBDTs (and other non-pretrained models) in the case where labeled data is vastly outnumbered by unlabeled data. We also show that pretrained models also often achieve significantly better robustness than non-pretrained models (GBDTs or DL models) in ranking outlier data."}}
{"id": "teAB2Ft5NXt", "cdate": 1677628800000, "mdate": 1681745112150, "content": {"title": "Batching of Tasks by Users of Pseudonymous Forums: Anonymity Compromise and Protection", "abstract": "There are a number of forums where people participate under pseudonyms. One example is peer review, where the identity of reviewers for any paper is confidential. When participating in these forums, people frequently engage in \"batching\": executing multiple related tasks (e.g., commenting on multiple papers) at nearly the same time. Our empirical analysis shows that batching is common in two applications we consider -- peer review and Wikipedia edits. In this paper, we identify and address the risk of deanonymization arising from linking batched tasks. To protect against linkage attacks, we take the approach of adding delay to the posting time of batched tasks. We first show that under some natural assumptions, no delay mechanism can provide a meaningful differential privacy guarantee. We therefore propose a \"one-sided\" formulation of differential privacy for protecting against linkage attacks. We design a mechanism that adds zero-inflated uniform delay to events and show it can preserve privacy. We prove that this noise distribution is in fact optimal in minimizing expected delay among mechanisms adding independent noise to each event, thereby establishing the Pareto frontier of the trade-off between the expected delay for batched and unbatched events. Finally, we conduct a series of experiments on Wikipedia and Bitcoin data that corroborate the practical utility of our algorithm in obfuscating batching without introducing onerous delay to a system."}}
{"id": "OWb_lZuEwyI", "cdate": 1676472364709, "mdate": null, "content": {"title": "Privately Customizing Prefinetuning to Better Match User Data in Federated Learning", "abstract": "In Federated Learning (FL), accessing private client data incurs communication and privacy costs.  As a result, FL deployments commonly prefinetune pretrained foundation models on a (large, possibly public) dataset that is held by the central server; they then FL-finetune the model on a private, federated dataset held by clients.  Evaluating prefinetuning dataset quality reliably and privately (with respect to its usefulness on the user datasets) is therefore of high importance.  To this end, we propose FreD (Federated Private Fr\u00e9chet Distance) --- a privately computed distance between a prefinetuning dataset and federated datasets. Intuitively, it privately computes and compares a Fr\u00e9chet distance between embeddings generated by a large language model on both the central (public) dataset and the federated private client data.  To make this computation privacy-preserving,  we use distributed, differentially-private mean and covariance estimators.  We show empirically that FreD accurately predicts the best prefinetuning dataset at minimal privacy cost.  Altogether, using FreD we demonstrate a proof-of-concept for a new approach in private FL training: (1) customize a prefinetuning dataset to better match user data (2) prefinetune (3) perform FL-finetuning."}}
{"id": "ITe5n0oFO_", "cdate": 1672531200000, "mdate": 1681678426428, "content": {"title": "Summary Statistic Privacy in Data Sharing", "abstract": "Data sharing between different parties has become increasingly common across industry and academia. An important class of privacy concerns that arises in data sharing scenarios regards the underlying distribution of data. For example, the total traffic volume of data from a networking company can reveal the scale of its business, which may be considered a trade secret. Unfortunately, existing privacy frameworks (e.g., differential privacy, anonymization) do not adequately address such concerns. In this paper, we propose summary statistic privacy, a framework for analyzing and protecting these summary statistic privacy concerns. We propose a class of quantization mechanisms that can be tailored to various data distributions and statistical secrets, and analyze their privacy-distortion trade-offs under our framework. We prove corresponding lower bounds on the privacy-utility tradeoff, which match the tradeoffs of the quantization mechanism under certain regimes, up to small constant factors. Finally, we demonstrate that the proposed quantization mechanisms achieve better privacy-distortion tradeoffs than alternative privacy mechanisms on real-world datasets."}}
{"id": "IQBQ2xgetD_", "cdate": 1672531200000, "mdate": 1681745112196, "content": {"title": "Privately Customizing Prefinetuning to Better Match User Data in Federated Learning", "abstract": "In Federated Learning (FL), accessing private client data incurs communication and privacy costs. As a result, FL deployments commonly prefinetune pretrained foundation models on a (large, possibly public) dataset that is held by the central server; they then FL-finetune the model on a private, federated dataset held by clients. Evaluating prefinetuning dataset quality reliably and privately is therefore of high importance. To this end, we propose FreD (Federated Private Fr\\'echet Distance) -- a privately computed distance between a prefinetuning dataset and federated datasets. Intuitively, it privately computes and compares a Fr\\'echet distance between embeddings generated by a large language model on both the central (public) dataset and the federated private client data. To make this computation privacy-preserving, we use distributed, differentially-private mean and covariance estimators. We show empirically that FreD accurately predicts the best prefinetuning dataset at minimal privacy cost. Altogether, using FreD we demonstrate a proof-of-concept for a new approach in private FL training: (1) customize a prefinetuning dataset to better match user data (2) prefinetune (3) perform FL-finetuning."}}
{"id": "6oVAzFsHLFK", "cdate": 1664816287496, "mdate": null, "content": {"title": "Distributional Privacy for Data Sharing", "abstract": "Data sharing between different parties has become an important engine powering modern research and development processes. An important class of privacy concerns in data sharing regards the underlying distribution of data. For example, the total traffic volume of data from a networking company reveals the scale of its business. Unfortunately, existing privacy frameworks do not adequately address this class of concerns. In this paper, we propose distributional privacy, a framework for analyzing and protecting these distributional privacy concerns in data sharing scenarios. Distributional privacy is applicable in multiple data sharing settings, including synthetic data release. Theoretically, we analyze the lower and upper bounds of privacy-distortion trade-offs. Practically, we propose data release mechanism for protecting distributional privacy concerns, and demonstrate that they achieve better privacy-distortion trade-offs than alternative privacy mechanisms on real-world datasets."}}
{"id": "zo2wPrZuLko", "cdate": 1650860483015, "mdate": null, "content": {"title": "FedChain: Chained Algorithms for Near-optimal Communication Cost in Federated Learning", "abstract": "Federated learning (FL) aims to minimize the communication complexity of training a model over heterogeneous data distributed across many clients. A common approach is local methods, where clients take multiple optimization steps over local data before communicating with the server (e.g., FedAvg).  Local methods can exploit similarity between clients' data. However, in existing analyses, this comes at the cost of slow convergence in terms of the dependence on the number of communication rounds R.  On the other hand, global methods, where clients simply return a gradient vector in each round (e.g., SGD), converge faster in terms of R but fail to exploit the similarity between clients even when clients are homogeneous.  We propose FedChain, an algorithmic framework that combines the strengths of local methods and global methods to achieve fast convergence in terms of R while  leveraging the similarity between clients.  Using FedChain, we instantiate algorithms that improve upon previously known rates in the general convex and PL settings, and are near-optimal (via an algorithm-independent lower bound that we show) for problems that satisfy strong convexity.  Empirical results support this theoretical gain over existing methods. "}}
{"id": "sLePlKyl6Y", "cdate": 1640995200000, "mdate": 1681678426668, "content": {"title": "RareGAN: Generating Samples for Rare Classes", "abstract": "We study the problem of learning generative adversarial networks (GANs) for a rare class of an unlabeled dataset subject to a labeling budget. This problem is motivated from practical applications in domains including security (e.g., synthesizing packets for DNS amplification attacks), systems and networking (e.g., synthesizing workloads that trigger high resource usage), and machine learning (e.g., generating images from a rare class). Existing approaches are unsuitable, either requiring fully-labeled datasets or sacrificing the fidelity of the rare class for that of the common classes. We propose RareGAN, a novel synthesis of three key ideas: (1) extending conditional GANs to use labelled and unlabelled data for better generalization; (2) an active learning approach that requests the most useful labels; and (3) a weighted loss function to favor learning the rare class. We show that RareGAN achieves a better fidelity-diversity tradeoff on the rare class than prior work across different applications, budgets, rare class fractions, GAN losses, and architectures."}}
{"id": "kQWIC-3O-pT", "cdate": 1640995200000, "mdate": 1681745112195, "content": {"title": "The Effect of Network Topology on Credit Network Throughput", "abstract": "The global economy relies on digital transactions between entities who do not trust one another. Today, such transactions are handled by intermediaries who extract fees (e.g., credit card providers). A natural question is how to build financial systems that limit the need for such middlemen."}}
{"id": "eZxaCnh6cHC", "cdate": 1640995200000, "mdate": 1681745112194, "content": {"title": "Locally Differentially Private Sparse Vector Aggregation", "abstract": "Vector mean estimation is a central primitive in federated analytics. In vector mean estimation, each user $i \\in[n]$ holds a real-valued vector $v_{i} \\in[-1,1]^{d}$, and a server wants to estimate the mean of all n vectors; we would additionally like to protect each user\u2019s privacy. In this paper, we consider the k-sparse version of the vector mean estimation problem. That is, suppose each user\u2019s vector has at most k non-zero coordinates in its d-dimensional vector, and moreover, $k \\ll d$. In practice, since the universe size d can be very large (e.g., the space of all possible URLs), we would like the per-user communication to be succinct, i.e., independent of or (poly-)logarithmic in the universe size.In this paper, we show matching upper- and lower-bounds for the k-sparse vector mean estimation problem under local differential privacy (LDP). Specifically, we construct new mechanisms that achieve asymptotically optimal error as well as succinct communication, either under user-level-LDP or event-level-LDP. We implement our algorithms and evaluate them on synthetic and real-world datasets. Our experiments show that we can often achieve one or two orders of magnitude reduction in error compared with prior work under typical choices of parameters, while incurring insignificant communication cost."}}
