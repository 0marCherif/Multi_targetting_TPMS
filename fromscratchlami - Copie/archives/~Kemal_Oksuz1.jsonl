{"id": "HKIH2YA2JrO", "cdate": 1617690222403, "mdate": null, "content": {"title": "A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection", "abstract": "We propose average Localisation-Recall-Precision (aLRP), a unified, bounded,\nbalanced and ranking-based loss function for both classification and localisation\ntasks in object detection. aLRP extends the Localisation-Recall-Precision (LRP)\nperformance metric (Oksuz et al., 2018) inspired from how Average Precision (AP)\nLoss extends precision to a ranking-based loss function for classification (Chen et\nal., 2020). aLRP has the following distinct advantages: (i) aLRP is the first ranking based loss function for both classification and localisation tasks. (ii) Thanks to\nusing ranking for both tasks, aLRP naturally enforces high-quality localisation\nfor high-precision classification. (iii) aLRP provides provable balance between\npositives and negatives. (iv) Compared to on average \u223c6 hyperparameters in the\nloss functions of state-of-the-art detectors, aLRP Loss has only one hyperparameter,\nwhich we did not tune in practice. On the COCO dataset, aLRP Loss improves\nits ranking-based predecessor, AP Loss, up to around 5 AP points, achieves 48.9\nAP without test time augmentation and outperforms all one-stage detectors. Code\navailable at: https://github.com/kemaloksuz/aLRPLoss."}}
{"id": "rxYWSbOEaC", "cdate": 1609459200000, "mdate": 1668447933734, "content": {"title": "Rank & Sort Loss for Object Detection and Instance Segmentation", "abstract": "We propose Rank & Sort (RS) Loss, a ranking-based loss function to train deep object detection and instance segmentation methods (i.e. visual detectors). RS Loss supervises the classifier, a sub-network of these methods, to rank each positive above all negatives as well as to sort positives among themselves with respect to (wrt.) their localisation qualities (e.g. Intersection-over-Union - IoU). To tackle the non-differentiable nature of ranking and sorting, we reformulate the incorporation of error-driven update with back-propagation as Identity Update, which enables us to model our novel sorting error among positives. With RS Loss, we significantly simplify training: (i) Thanks to our sorting objective, the positives are prioritized by the classifier without an additional auxiliary head (e.g. for centerness, IoU, mask-IoU), (ii) due to its ranking-based nature, RS Loss is robust to class imbalance, and thus, no sampling heuristic is required, and (iii) we address the multi-task nature of visual detectors using tuning-free task-balancing coefficients. Using RS Loss, we train seven diverse visual detectors only by tuning the learning rate, and show that it consistently outperforms baselines: e.g. our RS Loss improves (i) Faster R-CNN by \u223c 3 box AP and aLRP Loss (ranking-based baseline) by \u223c 2 box AP on COCO dataset, (ii) Mask R-CNN with repeat factor sampling (RFS) by 3.5 mask AP (\u223c 7 AP for rare classes) on LVIS dataset; and also outperforms all counterparts. Code is available at: https://github.com/kemaloksuz/RankSortLoss."}}
{"id": "chgFfUBUAL", "cdate": 1609459200000, "mdate": 1668447933863, "content": {"title": "Mask-aware IoU for Anchor Assignment in Real-time Instance Segmentation", "abstract": "This paper presents Mask-aware Intersection-over-Union (maIoU) for assigning anchor boxes as positives and negatives during training of instance segmentation methods. Unlike conventional IoU or its variants, which only considers the proximity of two boxes; maIoU consistently measures the proximity of an anchor box with not only a ground truth box but also its associated ground truth mask. Thus, additionally considering the mask, which, in fact, represents the shape of the object, maIoU enables a more accurate supervision during training. We present the effectiveness of maIoU on a state-of-the-art (SOTA) assigner, ATSS, by replacing IoU operation by our maIoU and training YOLACT, a SOTA real-time instance segmentation method. Using ATSS with maIoU consistently outperforms (i) ATSS with IoU by $\\sim 1$ mask AP, (ii) baseline YOLACT with fixed IoU threshold assigner by $\\sim 2$ mask AP over different image sizes and (iii) decreases the inference time by $25 \\%$ owing to using less anchors. Then, exploiting this efficiency, we devise maYOLACT, a faster and $+6$ AP more accurate detector than YOLACT. Our best model achieves $37.7$ mask AP at $25$ fps on COCO test-dev establishing a new state-of-the-art for real-time instance segmentation. Code is available at https://github.com/kemaloksuz/Mask-aware-IoU"}}
{"id": "b8S4lsqQJu3", "cdate": 1609459200000, "mdate": 1668447933728, "content": {"title": "Imbalance Problems in Object Detection: A Review", "abstract": "In this paper, we present a comprehensive review of the imbalance problems in object detection. To analyze the problems in a systematic manner, we introduce a problem-based taxonomy. Following this taxonomy, we discuss each problem in depth and present a unifying yet critical perspective on the solutions in the literature. In addition, we identify major open issues regarding the existing imbalance problems as well as imbalance problems that have not been discussed before. Moreover, in order to keep our review up to date, we provide an accompanying webpage which catalogs papers addressing imbalance problems, according to our problem-based taxonomy. Researchers can track newer studies on this webpage available at: https://github.com/kemaloksuz/ObjectDetectionImbalance."}}
{"id": "NDO73s9WOt", "cdate": 1609459200000, "mdate": 1668447933739, "content": {"title": "Rank & Sort Loss for Object Detection and Instance Segmentation", "abstract": "We propose Rank & Sort (RS) Loss, a ranking-based loss function to train deep object detection and instance segmentation methods (i.e. visual detectors). RS Loss supervises the classifier, a sub-network of these methods, to rank each positive above all negatives as well as to sort positives among themselves with respect to (wrt.) their localisation qualities (e.g. Intersection-over-Union - IoU). To tackle the non-differentiable nature of ranking and sorting, we reformulate the incorporation of error-driven update with backpropagation as Identity Update, which enables us to model our novel sorting error among positives. With RS Loss, we significantly simplify training: (i) Thanks to our sorting objective, the positives are prioritized by the classifier without an additional auxiliary head (e.g. for centerness, IoU, mask-IoU), (ii) due to its ranking-based nature, RS Loss is robust to class imbalance, and thus, no sampling heuristic is required, and (iii) we address the multi-task nature of visual detectors using tuning-free task-balancing coefficients. Using RS Loss, we train seven diverse visual detectors only by tuning the learning rate, and show that it consistently outperforms baselines: e.g. our RS Loss improves (i) Faster R-CNN by ~ 3 box AP and aLRP Loss (ranking-based baseline) by ~ 2 box AP on COCO dataset, (ii) Mask R-CNN with repeat factor sampling (RFS) by 3.5 mask AP (~ 7 AP for rare classes) on LVIS dataset; and also outperforms all counterparts. Code is available at: https://github.com/kemaloksuz/RankSortLoss"}}
{"id": "KWOlGcCUPx", "cdate": 1609459200000, "mdate": 1668447933712, "content": {"title": "Mask-aware IoU for Anchor Assignment in Real-time Instance Segmentation", "abstract": ""}}
{"id": "abkZbWG5EJ", "cdate": 1581763030361, "mdate": null, "content": {"title": "Imbalance Problems in Object Detection: A Review", "abstract": "In this paper, we present a comprehensive review of the imbalance problems in object detection. To analyze the problems in\na systematic manner, we introduce a problem-based taxonomy. Following this taxonomy, we discuss each problem in depth and\npresent a unifying yet critical perspective on the solutions in the literature. In addition, we identify major open issues regarding the\nexisting imbalance problems as well as imbalance problems that have not been discussed before. Moreover, in order to keep our\nreview up to date, we provide an accompanying webpage which catalogs papers addressing imbalance problems, according to our\nproblem-based taxonomy. Researchers can track newer studies on this webpage available at:\nhttps://github.com/kemaloksuz/ObjectDetectionImbalance."}}
{"id": "atxV0BA5Qp", "cdate": 1581762467940, "mdate": null, "content": {"title": "Generating Positive Bounding Boxes for Balanced Training of Object Detectors", "abstract": "Two-stage deep object detectors generate a set of\nregions-of-interest (RoI) in the first stage, then, in the second stage, identify objects among the proposed RoIs that\nsufficiently overlap with a ground truth (GT) box. The second stage is known to suffer from a bias towards RoIs that\nhave low intersection-over-union (IoU) with the associated\nGT boxes. To address this issue, we first propose a sampling method to generate bounding boxes (BB) that overlap\nwith a given reference box more than a given IoU threshold. Then, we use this BB generation method to develop\na positive RoI (pRoI) generator that produces RoIs following any desired spatial or IoU distribution, for the secondstage. We show that our pRoI generator is able to simulate\nother sampling methods for positive examples such as hard\nexample mining and prime sampling. Using our generator as an analysis tool, we show that (i) IoU imbalance has\nan adverse effect on performance, (ii) hard positive example\nmining improves the performance only for certain input IoU\ndistributions, and (iii) the imbalance among the foreground\nclasses has an adverse effect on performance and that it\ncan be alleviated at the batch level. Finally, we train Faster\nR-CNN using our pRoI generator and, compared to conventional training, obtain better or on-par performance for low\nIoUs and significant improvements when trained for higher\nIoUs for Pascal VOC and MS COCO datasets. The code is\navailable at: https://github.com/kemaloksuz/\nBoundingBoxGenerator."}}
{"id": "qNossKdr-2", "cdate": 1577836800000, "mdate": 1668447933850, "content": {"title": "A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection", "abstract": "We propose average Localisation-Recall-Precision (aLRP), a unified, bounded, balanced and ranking-based loss function for both classification and localisation tasks in object detection. aLRP extends the Localisation-Recall-Precision (LRP) performance metric (Oksuz et al., 2018) inspired from how Average Precision (AP) Loss extends precision to a ranking-based loss function for classification (Chen et al., 2020). aLRP has the following distinct advantages: (i) aLRP is the first ranking-based loss function for both classification and localisation tasks. (ii) Thanks to using ranking for both tasks, aLRP naturally enforces high-quality localisation for high-precision classification. (iii) aLRP provides provable balance between positives and negatives. (iv) Compared to on average $\\sim$6 hyperparameters in the loss functions of state-of-the-art detectors, aLRP Loss has only one hyperparameter, which we did not tune in practice. On the COCO dataset, aLRP Loss improves its ranking-based predecessor, AP Loss, up to around $5$ AP points, achieves $48.9$ AP without test time augmentation and outperforms all one-stage detectors. Code available at: https://github.com/kemaloksuz/aLRPLoss ."}}
{"id": "ff-UXdHporN", "cdate": 1577836800000, "mdate": 1668447933729, "content": {"title": "One Metric to Measure them All: Localisation Recall Precision (LRP) for Evaluating Visual Detection Tasks", "abstract": "Despite being widely used as a performance measure for visual detection tasks, Average Precision (AP) is limited in (i) reflecting localisation quality, (ii) interpretability and (iii) robustness to the design choices regarding its computation, and its applicability to outputs without confidence scores. Panoptic Quality (PQ), a measure proposed for evaluating panoptic segmentation (Kirillov et al., 2019), does not suffer from these limitations but is limited to panoptic segmentation. In this paper, we propose Localisation Recall Precision (LRP) Error as the average matching error of a visual detector computed based on both its localisation and classification qualities for a given confidence score threshold. LRP Error, initially proposed only for object detection by Oksuz et al. (2018), does not suffer from the aforementioned limitations and is applicable to all visual detection tasks. We also introduce Optimal LRP (oLRP) Error as the minimum LRP Error obtained over confidence scores to evaluate visual detectors and obtain optimal thresholds for deployment. We provide a detailed comparative analysis of LRP Error with AP and PQ, and use nearly 100 state-of-the-art visual detectors from seven visual detection tasks (i.e. object detection, keypoint detection, instance segmentation, panoptic segmentation, visual relationship detection, zero-shot detection and generalised zero-shot detection) using ten datasets to empirically show that LRP Error provides richer and more discriminative information than its counterparts. Code available at: https://github.com/kemaloksuz/LRP-Error"}}
