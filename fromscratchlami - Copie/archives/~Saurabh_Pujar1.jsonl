{"id": "w5_pkq5T1y6", "cdate": 1672531200000, "mdate": 1707244091496, "content": {"title": "Learning Transfers over Several Programming Languages", "abstract": "Large language models (LLMs) have recently become remarkably good at improving developer productivity for high-resource programming languages. These models use two kinds of data: large amounts of unlabeled code samples for pretraining and relatively smaller amounts of labeled code samples for fine-tuning or in-context learning. Unfortunately, many programming languages are low-resource, lacking labeled samples for most tasks and often even lacking unlabeled samples. Therefore, users of low-resource languages (e.g., legacy or new languages) miss out on the benefits of LLMs. Cross-lingual transfer learning uses data from a source language to improve model performance on a target language. It has been well-studied for natural languages, but has received little attention for programming languages. This paper reports extensive experiments on four tasks using a transformer-based LLM and 11 to 41 programming languages to explore the following questions. First, how well cross-lingual transfer works for a given task across different language pairs. Second, given a task and target language, how to best choose a source language. Third, the characteristics of a language pair that are predictive of transfer performance, and fourth, how that depends on the given task."}}
{"id": "qeyKfQKIe9k", "cdate": 1672531200000, "mdate": 1707244091483, "content": {"title": "Invited: Automated Code generation for Information Technology Tasks in YAML through Large Language Models", "abstract": "The recent improvement in code generation capabilities due to the use of large language models has mainly benefited general purpose programming languages. Domain specific languages, such as the ones used for IT Automation, received far less attention, despite involving many active developers and being an essential component of modern cloud platforms. This work focuses on the generation of Ansible YAML, a widely used markup language for IT Automation. We present Ansible Wisdom, a natural-language to Ansible YAML code generation tool, aimed at improving IT automation productivity. Results show that Ansible Wisdom can accurately generate Ansible script from natural language prompts with performance comparable or better than existing state of the art code generation models."}}
{"id": "WGTPgNO9aw", "cdate": 1672531200000, "mdate": 1707244091479, "content": {"title": "CONCORD: Clone-Aware Contrastive Learning for Source Code", "abstract": "Deep Learning (DL) models to analyze source code have shown immense promise during the past few years. More recently, self-supervised pre-training has gained traction for learning generic code representations valuable for many downstream SE tasks, such as clone and bug detection. While previous work successfully learned from different code abstractions (e.g., token, AST, graph), we argue that it is also essential to factor in how developers code day-to-day for learning general-purpose representation. On the one hand, human developers tend to write repetitive programs referencing existing code snippets from the current codebase or online resources (e.g., Stack Overflow website) rather than implementing functions from scratch; such behaviors result in a vast number of code clones. In contrast, a deviant clone by mistake might trigger malicious program behaviors. Thus, as a proxy to incorporate developers' coding behavior into the pre-training scheme, we propose to include code clones and their deviants. In particular, we propose CONCORD, a self-supervised pre-training strategy to place benign clones closer in the representation space while moving deviants further apart. We show that CONCORD's clone-aware pre-training drastically reduces the need for expensive pre-training resources while improving the performance of downstream SE tasks. We also empirically demonstrate that CONCORD can improve existing pre-trained models to learn better representations that consequently become more efficient in both identifying semantically equivalent programs and differentiating buggy from non-buggy code."}}
{"id": "MFitXKyYGSE", "cdate": 1672531200000, "mdate": 1707244091502, "content": {"title": "Can Large Language Models Identify And Reason About Security Vulnerabilities? Not Yet", "abstract": "Large Language Models (LLMs) have been suggested for use in automated vulnerability repair, but benchmarks showing they can consistently identify security-related bugs are lacking. We thus perform the most detailed investigation to date on whether LLMs can reliably identify security-related bugs. We construct a series of 228 code scenarios and analyze eight of the most capable LLMs across eight different investigative dimensions in an automated framework. Our evaluation shows LLMs provide non-deterministic responses, incorrect and unfaithful reasoning, and perform poorly in real-world scenarios outside their knowledge cut-off date. Most importantly, our findings reveal significant non-robustness in even the most advanced models like `PaLM2' and `GPT-4': by merely changing function or variable names, or by the addition of library functions in the source code, these models can yield incorrect answers in 26% and 17% of cases, respectively. These findings demonstrate that further LLM advances are needed before LLMs can be used as general purpose security assistants."}}
{"id": "GOasjCfEQi0", "cdate": 1672531200000, "mdate": 1707244091481, "content": {"title": "Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain", "abstract": "Code Large Language Models (Code LLMs) are being increasingly employed in real-life applications, so evaluating them is critical. While the conventional accuracy evaluates the performance of Code LLMs on a set of individual tasks, their self-consistency across different tasks is overlooked. Intuitively, a trustworthy model should be self-consistent when generating natural language specifications for its own code and generating code for its own specifications. Failure to preserve self-consistency reveals a lack of understanding of the shared semantics underlying natural language and programming language, and therefore undermines the trustworthiness of a model. In this paper, we first formally define the self-consistency of Code LLMs and then design a framework, IdentityChain, which effectively and efficiently evaluates the self-consistency and conventional accuracy of a model at the same time. We study eleven Code LLMs and show that they fail to preserve self-consistency, which is indeed a distinct aspect from conventional accuracy. Furthermore, we show that IdentityChain can be used as a model debugging tool to expose weaknesses of Code LLMs by demonstrating three major weaknesses that we identify in current models using IdentityChain. Our code is available at https://github.com/marcusm117/IdentityChain."}}
{"id": "E7meKSIOYO", "cdate": 1672531200000, "mdate": 1707244091508, "content": {"title": "CONCORD: Clone-aware Contrastive Learning for Source Code", "abstract": "Deep Learning (DL) models to analyze source code have shown immense promise during the past few years. More recently, self-supervised pre-training has gained traction for learning generic code representations valuable for many downstream SE tasks, such as clone and bug detection. While previous work successfully learned from different code abstractions (e.g., token, AST, graph), we argue that it is also essential to factor in how developers code day-to-day for general-purpose representation learning. On the one hand, human developers tend to write repetitive programs referencing existing code snippets from the current codebase or online resources (e.g., Stack Overflow website) rather than implementing functions from scratch; such behaviors result in a vast number of code clones. In contrast, a deviant clone by mistake might trigger malicious program behaviors. Thus, as a proxy to incorporate developers' coding behavior into the pre-training scheme, we propose to include code clones and their deviants. In particular, we propose CONCORD, a self-supervised, contrastive learning strategy to place benign clones closer in the representation space while moving deviants further apart. We show that CONCORD's clone-aware contrastive learning drastically reduces the need for expensive pre-training resources while improving the performance of downstream SE tasks. We also empirically demonstrate that CONCORD can improve existing pre-trained models to learn better representations that consequently become more efficient in both identifying semantically equivalent programs and differentiating buggy from non-buggy code."}}
{"id": "1FL73bcjfh5", "cdate": 1672531200000, "mdate": 1707244091510, "content": {"title": "Automated Code generation for Information Technology Tasks in YAML through Large Language Models", "abstract": "The recent improvement in code generation capabilities due to the use of large language models has mainly benefited general purpose programming languages. Domain specific languages, such as the ones used for IT Automation, have received far less attention, despite involving many active developers and being an essential component of modern cloud platforms. This work focuses on the generation of Ansible-YAML, a widely used markup language for IT Automation. We present Ansible Wisdom, a natural-language to Ansible-YAML code generation tool, aimed at improving IT automation productivity. Ansible Wisdom is a transformer-based model, extended by training with a new dataset containing Ansible-YAML. We also develop two novel performance metrics for YAML and Ansible to capture the specific characteristics of this domain. Results show that Ansible Wisdom can accurately generate Ansible script from natural language prompts with performance comparable or better than existing state of the art code generation models. In few-shot settings we asses the impact of training with Ansible, YAML data and compare with different baselines including Codex-Davinci-002. We also show that after finetuning, our Ansible specific model (BLEU: 66.67) can outperform a much larger Codex-Davinci-002 (BLEU: 50.4) model, which was evaluated in few shot settings."}}
{"id": "D95qW5yUcP", "cdate": 1640995200000, "mdate": 1682679363335, "content": {"title": "Varangian: A Git Bot for Augmented Static Analysis", "abstract": "The complexity and scale of modern software programs often lead to overlooked programming errors and security vulnerabilities. Developers often rely on automatic tools, like static analysis tools, to look for bugs and vulnerabilities. Static analysis tools are widely used because they can understand nontrivial program behaviors, scale to millions of lines of code, and detect subtle bugs. However, they are known to generate an excess of false alarms which hinder their utilization as it is counterproductive for developers to go through a long list of reported issues, only to find a few true positives. One of the ways proposed to suppress false positives is to use machine learning to identify them. However, training machine learning models requires good quality labeled datasets. For this purpose, we developed D2A [3], a differential analysis based approach that uses the commit history of a code repository to create a labeled dataset of Infer [2] static analysis output. The data generated by D2A can be used to train AI Models like Voting, Stacking Ensembles and C-BERT [1], which learn to identify False Positives. Ensembles are built on top of the Boosting and Tree based classifiers which use hand-crafted features for classifying static analyzer output as True Positives. C-BERT is a BERT-base language model pretrained from scratch on C source code extracted from 10,000 C repositories, thus leveraging \"big code\". This model is then fine-tuned on D2A labeled data. This approach views source code as language and automates the extraction of features. The output of the models is a prioritized list of defects ordered by the likelihood of being True Positive. One way to use the Augmented Static Analyzer to improve developer productivity would be to insert the application in the developer workflow so that its use would seem natural. With this idea in mind, we created Varangian, which is a Git bot that automatically creates issues on the repository based on defects prioritized by the Augmented Static Analyzer. Models trained on the D2A dataset created from the same repository are used to create the prioritized list. The models achieve about 0.9 AUC when tested on historical data of 7 different Open Source projects. In terms of FP/TP ratio the improvement was at least 20 times for most projects. It is important to note that the test data will generally contain defects from many commits and will have a different distribution from any individual commit, which may contain fewer and less varied defects. We test the models on the latest commit of a project by first absorbing the commit by the inference pipeline and then manually validating the output. The inference pipeline which produces the prioritized list for a single commit, first applies the Infer static analyzer to the latest commit of the repository. Relevant source code is then extracted based on the bug report for each defect highlighted by the static analyzer. Bug reports and relevant source code extracted from repositories are used to generate features for AI Models, which then assign a likelihood of being a True Positive to each defect. The git bot then takes the defects most likely to be True Positive and creates issues for each defect. The issue created by Varangian has a lot of information the developer can use for debugging. On a recent commit of an opensource project, the Varangian bot created 5 issues out of which 1 was a TP. This gives us an FP/TP ratio of 4/1 which is five times better than the 20/1 ratio we observe for Infer on the test set of the same project. In this presentation, we will showcase Varangian, compare different model training approaches, discuss the challenges involved in building a training and inference pipeline based on code repository, and the impact on performance when moving from the test set to the latest commit."}}
{"id": "8KoNkXaf-Kd", "cdate": 1640995200000, "mdate": 1681672495540, "content": {"title": "Towards Learning (Dis)-Similarity of Source Code from Program Contrasts", "abstract": "Yangruibo Ding, Luca Buratti, Saurabh Pujar, Alessandro Morari, Baishakhi Ray, Saikat Chakraborty. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022."}}
{"id": "7KgeqhkbZab", "cdate": 1632875729869, "mdate": null, "content": {"title": "Contrastive Learning for Source Code with Structural and Functional Properties", "abstract": "Pre-trained transformer models have recently shown promises for understanding the source code. Most existing works expect to understand code from the textual features and limited structural knowledge of code. However, the program functionalities sometimes cannot be fully revealed by the code sequence, even with structure information. Programs can contain very different tokens and structures while sharing the same functionality, but changing only one or a few code tokens can introduce unexpected or malicious program behaviors while preserving the syntax and most tokens. In this work, we present BOOST, a novel self-supervised model to focus pre-training based on the characteristics of source code. We first employ automated, structure-guided code transformation algorithms that generate (i.) functionally equivalent code that looks drastically different from the original one, and (ii.) textually and syntactically very similar code that is functionally distinct from the original. We train our model in a way that brings the functionally equivalent code closer and distinct code further through a contrastive learning objective. To encode the structure information, we introduce a new node-type masked language model objective that helps the model learn about structural context. We pre-train BOOST with a much smaller dataset than the state-of-the-art models, but our small models can still match or outperform these large models in code understanding and generation tasks."}}
