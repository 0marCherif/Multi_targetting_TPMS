{"id": "Ik8iimy4oFF", "cdate": 1652737669986, "mdate": null, "content": {"title": "Tractable Optimality in Episodic Latent MABs", "abstract": "We consider a multi-armed bandit problem with $M$ latent contexts, where an agent interacts with the environment for an episode of $H$ time steps. Depending on the length of the episode, the learner may not be able to estimate accurately the latent context. The resulting partial observation of the environment makes the learning task significantly more challenging. \nWithout any additional structural assumptions, existing techniques to tackle partially observed settings imply the decision maker can learn a near-optimal policy with $O(A)^H$ episodes, but do not promise more. \nIn this work, we show that learning with {\\em polynomial} samples in $A$ is possible. We achieve this by using techniques from experiment design. Then, through a method-of-moments approach, we design a procedure that provably learns a near-optimal policy with $O(\\poly(A) + \\poly(M,H)^{\\min(M,H)})$ interactions. In practice, we show that we can formulate the moment-matching via maximum likelihood estimation. In our experiments, this significantly outperforms the worst-case guarantees, as well as existing practical methods.\n"}}
{"id": "XHHxE-KOK7", "cdate": 1621630202406, "mdate": null, "content": {"title": "Reinforcement Learning in Reward-Mixing MDPs", "abstract": "Learning a near optimal policy in a partially observable system remains an elusive challenge in contemporary reinforcement learning. In this work, we consider episodic reinforcement learning in a reward-mixing Markov decision process (MDP). There, a reward function is drawn from one of $M$ possible reward models at the beginning of every episode, but the identity of the chosen reward model is not revealed to the agent. Hence, the latent state space, for which the dynamics are Markovian, is not given to the agent. We study the problem of learning a near optimal policy for two reward-mixing MDPs. Unlike existing approaches that rely on strong assumptions on the dynamics, we make no assumptions and study the problem in full generality. Indeed, with no further assumptions, even for two switching reward-models, the problem requires several new ideas beyond existing algorithmic and analysis techniques for efficient exploration. We provide the first polynomial-time algorithm that finds an $\\epsilon$-optimal policy after exploring $\\tilde{O}(poly(H,\\epsilon^{-1}) \\cdot S^2 A^2)$ episodes, where $H$ is time-horizon and $S, A$ are the number of states and actions respectively. This is the first efficient algorithm that does not require any assumptions in partially observed environments where the observation space is smaller than the latent state space. "}}
{"id": "CLCVcl1rSPP", "cdate": 1621630010793, "mdate": null, "content": {"title": "RL for Latent MDPs: Regret Guarantees and a Lower Bound", "abstract": "In this work, we consider the regret minimization problem for reinforcement learning in latent Markov Decision Processes (LMDP). In an LMDP, an MDP is randomly drawn from a set of $M$ possible MDPs at the beginning of the interaction, but the identity of the chosen MDP is not revealed to the agent. We first show that a general instance of LMDPs requires at least $\\Omega((SA)^M)$ episodes to even approximate the optimal policy. Then, we consider sufficient assumptions under which learning good policies requires polynomial number of episodes. We show that the key link is a notion of separation between the MDP system dynamics. With sufficient separation, we provide an efficient algorithm with local guarantee, {\\it i.e.,} providing a sublinear regret guarantee when we are given a good initialization. Finally, if we are given standard statistical sufficiency assumptions common in the Predictive State Representation (PSR) literature (e.g., \\cite{boots2011online}) and a reachability assumption, we show that the need for initialization can be removed. "}}
{"id": "cbTUzR3ZLXP", "cdate": 1609459200000, "mdate": null, "content": {"title": "On the Minimax Optimality of the EM Algorithm for Learning Two-Component Mixed Linear Regression", "abstract": "We study the convergence rates of the EM algorithm for learning two-component mixed linear regression under all regimes of signal-to-noise ratio (SNR). We resolve a long-standing question that many recent results have attempted to tackle: we completely characterize the convergence behavior of EM, and show that the EM algorithm achieves minimax optimal sample complexity under all SNR regimes. In particular, when the SNR is sufficiently large, the EM updates converge to the true parameter $\\theta^{*}$ at the standard parametric convergence rate $\\calo((d/n)^{1/2})$ after $\\calo(\\log(n/d))$ iterations. In the regime where the SNR is above $\\calo((d/n)^{1/4})$ and below some constant, the EM iterates converge to a $\\calo({\\rm SNR}^{-1} (d/n)^{1/2})$ neighborhood of the true parameter, when the number of iterations is of the order $\\calo({\\rm SNR}^{-2} \\log(n/d))$. In the low SNR regime where the SNR is below $\\calo((d/n)^{1/4})$, we show that EM converges to a $\\calo((d/n)^{1/4})$ neighborhood of the true parameters, after $\\calo((n/d)^{1/2})$ iterations. Notably, these results are achieved under mild conditions of either random initialization or an efficiently computable local initialization. By providing tight convergence guarantees of the EM algorithm in middle-to-low SNR regimes, we fill the remaining gap in the literature, and significantly, reveal that in low SNR, EM changes rate, matching the $n^{-1/4}$ rate of the MLE, a behavior that previous work had been unable to show."}}
{"id": "Yrll88E1aaR", "cdate": 1609459200000, "mdate": null, "content": {"title": "On the computational and statistical complexity of over-parameterized matrix sensing", "abstract": "We consider solving the low rank matrix sensing problem with Factorized Gradient Descend (FGD) method when the true rank is unknown and over-specified, which we refer to as over-parameterized matrix sensing. If the ground truth signal $\\mathbf{X}^* \\in \\mathbb{R}^{d*d}$ is of rank $r$, but we try to recover it using $\\mathbf{F} \\mathbf{F}^\\top$ where $\\mathbf{F} \\in \\mathbb{R}^{d*k}$ and $k>r$, the existing statistical analysis falls short, due to a flat local curvature of the loss function around the global maxima. By decomposing the factorized matrix $\\mathbf{F}$ into separate column spaces to capture the effect of extra ranks, we show that $\\|\\mathbf{F}_t \\mathbf{F}_t - \\mathbf{X}^*\\|_{F}^2$ converges to a statistical error of $\\tilde{\\mathcal{O}} ({k d \\sigma^2/n})$ after $\\tilde{\\mathcal{O}}(\\frac{\\sigma_{r}}{\\sigma}\\sqrt{\\frac{n}{d}})$ number of iterations where $\\mathbf{F}_t$ is the output of FGD after $t$ iterations, $\\sigma^2$ is the variance of the observation noise, $\\sigma_{r}$ is the $r$-th largest eigenvalue of $\\mathbf{X}^*$, and $n$ is the number of sample. Our results, therefore, offer a comprehensive picture of the statistical and computational complexity of FGD for the over-parameterized matrix sensing problem."}}
{"id": "HvJZwBr3GJD", "cdate": 1609459200000, "mdate": null, "content": {"title": "RL for Latent MDPs: Regret Guarantees and a Lower Bound", "abstract": "In this work, we consider the regret minimization problem for reinforcement learning in latent Markov Decision Processes (LMDP). In an LMDP, an MDP is randomly drawn from a set of $M$ possible MDPs at the beginning of the interaction, but the identity of the chosen MDP is not revealed to the agent. We first show that a general instance of LMDPs requires at least $\\Omega((SA)^M)$ episodes to even approximate the optimal policy. Then, we consider sufficient assumptions under which learning good policies requires polynomial number of episodes. We show that the key link is a notion of separation between the MDP system dynamics. With sufficient separation, we provide an efficient algorithm with local guarantee, {\\it i.e.,} providing a sublinear regret guarantee when we are given a good initialization. Finally, if we are given standard statistical sufficiency assumptions common in the Predictive State Representation (PSR) literature (e.g., Boots et al.) and a reachability assumption, we show that the need for initialization can be removed."}}
{"id": "zp2iZMP9JHy", "cdate": 1577836800000, "mdate": null, "content": {"title": "The EM Algorithm gives Sample-Optimality for Learning Mixtures of Well-Separated Gaussians", "abstract": "We consider the problem of spherical Gaussian Mixture models with $k \\geq 3$ components when the components are well separated. A fundamental previous result established that separation of $\\Omega(\\sqrt{\\log k})$ is necessary and sufficient for identifiability of the parameters with \\textit{polynomial} sample complexity (Regev and Vijayaraghavan, 2017). In the same context, we show that $\\tilde{O} (kd/\\epsilon^2)$ samples suffice for any $\\epsilon \\lesssim 1/k$, closing the gap from polynomial to linear, and thus giving the first optimal sample upper bound for the parameter estimation of well-separated Gaussian mixtures. We accomplish this by proving a new result for the Expectation-Maximization (EM) algorithm: we show that EM converges locally, under separation $\\Omega(\\sqrt{\\log k})$. The previous best-known guarantee required $\\Omega(\\sqrt{k})$ separation (Yan, et al., 2017). Unlike prior work, our results do not assume or use prior knowledge of the (potentially different) mixing weights or variances of the Gaussian components. Furthermore, our results show that the finite-sample error of EM does not depend on non-universal quantities such as pairwise distances between means of Gaussian components."}}
{"id": "9RclcyU_kMe", "cdate": 1546300800000, "mdate": null, "content": {"title": "EM Converges for a Mixture of Many Linear Regressions", "abstract": "We study the convergence of the Expectation-Maximization (EM) algorithm for mixtures of linear regressions with an arbitrary number $k$ of components. We show that as long as signal-to-noise ratio (SNR) is $\\tilde{\\Omega}(k)$, well-initialized EM converges to the true regression parameters. Previous results for $k \\geq 3$ have only established local convergence for the noiseless setting, i.e., where SNR is infinitely large. Our results enlarge the scope to the environment with noises, and notably, we establish a statistical error rate that is independent of the norm (or pairwise distance) of the regression parameters. In particular, our results imply exact recovery as $\\sigma \\rightarrow 0$, in contrast to most previous local convergence results for EM, where the statistical error scaled with the norm of parameters. Standard moment-method approaches may be applied to guarantee we are in the region where our local convergence guarantees apply."}}
