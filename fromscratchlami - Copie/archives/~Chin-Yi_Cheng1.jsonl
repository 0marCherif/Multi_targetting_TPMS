{"id": "uqpQ8BnJUv", "cdate": 1672531200000, "mdate": 1681743661843, "content": {"title": "PLay: Parametrically Conditioned Layout Generation using Latent Diffusion", "abstract": "Layout design is an important task in various design fields, including user interface, document, and graphic design. As this task requires tedious manual effort by designers, prior works have attempted to automate this process using generative models, but commonly fell short of providing intuitive user controls and achieving design objectives. In this paper, we build a conditional latent diffusion model, PLay, that generates parametrically conditioned layouts in vector graphic space from user-specified guidelines, which are commonly used by designers for representing their design intents in current practices. Our method outperforms prior works across three datasets on metrics including FID and FD-VG, and in user study. Moreover, it brings a novel and interactive experience to professional layout design processes."}}
{"id": "4eC4CSqTcw", "cdate": 1672531200000, "mdate": 1681490133915, "content": {"title": "IKEA-Manual: Seeing Shape Assembly Step by Step", "abstract": ""}}
{"id": "ZeeswGSOw7r", "cdate": 1654427645687, "mdate": null, "content": {"title": "IKEA-Manual: Seeing Shape Assembly Step by Step", "abstract": "Human-designed visual manuals are crucial components in shape assembly activities. They provide step-by-step guidance on how we should move and connect different parts in a convenient and physically-realizable way. While there has been an ongoing effort in building agents that perform assembly tasks, the information in human-design manuals has been largely overlooked. We identify that this is due to 1) a lack of realistic 3D assembly objects that have paired manuals and 2) the difficulty of extracting structured information from purely image-based manuals. Motivated by this observation, we present IKEA-Manual, a dataset consisting of 102 IKEA objects paired with assembly manuals. We provide fine-grained annotations on the IKEA objects and assembly manuals, including decomposed assembly parts, assembly plans, manual segmentation, and 2D-3D correspondence between 3D parts and visual manuals. We illustrate the broad application of our dataset on four tasks related to shape assembly: assembly plan generation, part segmentation, pose estimationand 3D part assembly."}}
{"id": "sB93rMmmJu", "cdate": 1640995200000, "mdate": 1668670782062, "content": {"title": "SkexGen: Autoregressive Generation of CAD Construction Sequences with Disentangled Codebooks", "abstract": "We present SkexGen, a novel autoregressive generative model for computer-aided design (CAD) construction sequences containing sketch-and-extrude modeling operations. Our model utilizes distinct Tra..."}}
{"id": "olNu_BEVlh", "cdate": 1640995200000, "mdate": 1668670782061, "content": {"title": "CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation", "abstract": "Generating shapes using natural language can enable new ways of imagining and creating the things around us. While significant recent progress has been made in text-to-image generation, text-to-shape generation remains a challenging problem due to the unavailability of paired text and shape data at a large scale. We present a simple yet effective method for zero-shot text-to-shape gener-ation that circumvents such data scarcity. Our proposed method, named CLIP-Forge, is based on a two-stage training process, which only depends on an unlabelled shape dataset and a pre-trained image-text network such as CLIP. Our method has the benefits of avoiding expensive inference time optimization, as well as the ability to generate multiple shapes for a given text. We not only demonstrate promising zero-shot generalization of the CLIP-Forge model qualitatively and quantitatively, but also provide extensive compar-ative evaluations to better understand its behavior."}}
{"id": "dMu7Y290nh-", "cdate": 1640995200000, "mdate": 1668670782094, "content": {"title": "Translating a Visual LEGO Manual to a Machine-Executable Plan", "abstract": "We study the problem of translating an image-based, step-by-step assembly manual created by human designers into machine-interpretable instructions. We formulate this problem as a sequential prediction task: at each step, our model reads the manual, locates the components to be added to the current shape, and infers their 3D poses. This task poses the challenge of establishing a 2D-3D correspondence between the manual image and the real 3D object, and 3D pose estimation for unseen 3D objects, since a new component to be added in a step can be an object built from previous steps. To address these two challenges, we present a novel learning-based framework, the Manual-to-Executable-Plan Network (MEPNet), which reconstructs the assembly steps from a sequence of manual images. The key idea is to integrate neural 2D keypoint detection modules and 2D-3D projection algorithms for high-precision prediction and strong generalization to unseen components. The MEPNet outperforms existing methods on three newly collected LEGO manual datasets and a Minecraft house dataset."}}
{"id": "_5qrebDRNi", "cdate": 1609459200000, "mdate": 1668670782072, "content": {"title": "Inferring CAD Modeling Sequences Using Zone Graphs", "abstract": "In computer-aided design (CAD), the ability to \"reverse engineer\" the modeling steps used to create 3D shapes is a long-sought-after goal. This process can be decomposed into two sub-problems: converting an input mesh or point cloud into a boundary representation (or B-rep), and then inferring modeling operations which construct this B-rep. In this paper, we present a new system for solving the second sub-problem. Central to our approach is a new geometric representation: the zone graph. Zones are the set of solid regions formed by extending all B-Rep faces and partitioning space with them; a zone graph has these zones as its nodes, with edges denoting geometric adjacencies between them. Zone graphs allow us to tractably work with industry-standard CAD operations, unlike prior work using CSG with parametric primitives. We focus on CAD programs consisting of sketch + extrude + Boolean operations, which are common in CAD practice. We phrase our problem as search in the space of such extrusions permitted by the zone graph, and we train a graph neural network to score potential extrusions in order to accelerate the search. We show that our approach outperforms an existing CSG inference baseline in terms of geometric reconstruction accuracy and reconstruction time, while also creating more plausible modeling sequences."}}
{"id": "S7jkedrMJib", "cdate": 1609459200000, "mdate": 1668670782096, "content": {"title": "Building-GAN: Graph-Conditioned Architectural Volumetric Design Generation", "abstract": "Volumetric design is the first and critical step for professional building design, where architects not only depict the rough 3D geometry of the building but also specify the programs to form a 2D layout on each floor. Though 2D layout generation for a single story has been widely studied, there is no developed method for multi-story buildings. This paper focuses on volumetric design generation conditioned on an input program graph. Instead of outputting dense 3D voxels, we propose a new 3D representation named voxel graph that is both compact and expressive for building geometries. Our generator is a cross-modal graph neural network that uses a pointer mechanism to connect the input program graph and the output voxel graph, and the whole pipeline is trained using the adversarial framework. The generated designs are evaluated qualitatively by a user study and quantitatively using three metrics: quality, diversity, and connectivity accuracy. We show that our model generates realistic 3D volumetric designs and outperforms previous methods and baselines."}}
{"id": "NgfZYtDcJfC", "cdate": 1609459200000, "mdate": 1668670782071, "content": {"title": "House-GAN++: Generative Adversarial Layout Refinement Network towards Intelligent Computational Agent for Professional Architects", "abstract": "This paper proposes a generative adversarial layout refinement network for automated floorplan generation. Our architecture is an integration of a graph-constrained relational GAN and a conditional GAN, where a previously generated layout becomes the next input constraint, enabling iterative refinement. A surprising discovery of our research is that a simple non-iterative training process, dubbed component-wise GT-conditioning, is effective in learning such a generator. The iterative generator further allows us to improve a metric of choice via meta-optimization techniques by controlling when to pass which input constraints during iterative refinement. Our qualitative and quantitative evaluation based on the three standard metrics demonstrate that the proposed system makes significant improvements over the current state-of-the-art, even competitive against the ground-truth floorplans, designed by professional architects. Code, model, and data are available at https://ennauata.github.io/houseganpp/page.html."}}
{"id": "74nWzF_tads", "cdate": 1609459200000, "mdate": 1668670782078, "content": {"title": "SketchOpt: Sketch-based Parametric Model Retrieval for Generative Design", "abstract": "Developing fully parametric building models for performance-based generative design tasks often requires proficiency in many advanced 3D modeling and visual programming software, limiting its use for many building designers. Moreover, iterations of such models can be time-consuming tasks and sometimes limiting depending on the the design stage, as major changes in the layout design may result in remodeling the entire parametric definition. To address these challenges, we introduce a novel automated generative design system, which takes a basic floor plan sketch as an input and provides a parametric model prepared for multi-objective building optimization as output. In addition, the user-designer can assign various design variables for its desired building elements by using simple annotations in the drawing. We take advantage of a asymmetric convolutional module combined with a parametrizer to allow real-time parametric sketch-retrieval for a performance-based generative workflow. The system would recognize the corresponding element and define variable constraints to prepare for a multi-objective optimization problem. We illustrate the the use case of our proposed system by running a real-time structural optimization form-finding study. Our findings indicate the system can be utilized as a promising generative design tool for novice users."}}
