{"id": "TGfj2P_410X", "cdate": 1632875581783, "mdate": null, "content": {"title": "On the Effect of Input Perturbations for Graph Neural Networks", "abstract": "The expressive power of a message passing graph neural network (MPGNN) depends on its architecture and the input node attributes. In this work, we study how this interplay is affected by input perturbations. First, perturbations of node attributes may act as noise and hinder predictive power. But, perturbations can also aid expressiveness, by making nodes more identifiable. Recent works show that unique node IDs are necessary to represent certain functions with MPGNNs. Our results relate properties of the noise, smoothness of the model and the geometry of the input graphs and task. In particular, we take the perspective of lower bounding smoothness for achieving discrimination: how much output variation is needed for exploiting random node IDs, or for retaining discriminability? Our theoretical results imply constraints on the model for exploiting random node IDs, and, conversely, insights into the tolerance of a given model class for retaining discrimination with perturbations of node attributes."}}
{"id": "-RAFyM-YPj", "cdate": 1632875571154, "mdate": null, "content": {"title": "Counting Substructures with Higher-Order Graph Neural Networks:  Possibility and Impossibility Results", "abstract": "While message passing Graph Neural Networks (GNNs) have become increasingly popular architectures for learning with graphs, recent works have revealed important shortcomings in their expressive power. In response, several higher-order GNNs have been proposed that substantially increase the expressive power, albeit at a large computational cost. Motivated by this gap, we explore alternative strategies and lower bounds. In particular, we analyze a new recursive pooling technique of local neighborhoods that allows different tradeoffs of computational cost and expressive power. First, we prove that this model can count subgraphs of size $k$, and thereby overcomes a known limitation of low-order GNNs. Second, we show how recursive pooling can exploit sparsity to reduce the computational complexity compared to the existing higher-order GNNs. More generally, we provide a (near) matching information-theoretic lower bound for counting subgraphs with graph representations that pool over representations of derived (sub-)graphs. We also discuss lower bounds on time complexity."}}
{"id": "QmSvURro98l", "cdate": 1609459200000, "mdate": 1671926467716, "content": {"title": "The Capacity of Associated Subsequence Retrieval", "abstract": "The objective of a genome-wide association study (GWAS) is to associate subsequences of individuals\u2019 genomes to the observable characteristics called phenotypes (e.g., high blood pressure). Motivated by the GWAS problem, in this paper we introduce the information-theoretic problem of <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">associated subsequence retrieval</i> , where a dataset of N (possibly high-dimensional) sequences of length G, and their corresponding observable (binary) characteristics is given. The sequences are chosen independently and uniformly at random from <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\mathcal {X}^{\\text {G}}$ </tex-math></inline-formula> , where <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\mathcal {X}$ </tex-math></inline-formula> is a finite alphabet. The observable (binary) characteristic is only related to a specific unknown subsequence of length <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$L$ </tex-math></inline-formula> of the sequences, called <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">associated subsequence</i> . For each sequence, if the associated subsequence of it belongs to a universal finite set, then it is more likely to display the observable characteristic (i.e., it is more likely that the observable characteristic is one). The goal is to retrieve the associated subsequence using a dataset of N sequences and their observable characteristics. We demonstrate that as the parameters N, G, and L grow, a threshold effect appears in the curve of probability of error versus the rate which is defined as <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">${{\\it\\text { Gh}}(\\text {L}/\\text {G})}/{\\text {N}}$ </tex-math></inline-formula> , where <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\text {h}(\\cdot )$ </tex-math></inline-formula> is the binary entropy function. This effect allows us to define the capacity of associated subsequence retrieval. We develop an achievable scheme and a matching converse for this problem, and thus characterize its capacity in two scenarios: the zero-error-rate and the <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\epsilon $ </tex-math></inline-formula> -error-rate."}}
{"id": "jH7wTMOYvbw", "cdate": 1601308336944, "mdate": null, "content": {"title": "Recursive Neighborhood Pooling for Graph Representation Learning", "abstract": "While massage passing based Graph Neural Networks (GNNs) have become increasingly popular architectures for learning with graphs, recent works have revealed important shortcomings in their expressive power. In response, several higher-order GNNs have been proposed, which substantially increase the expressive power, but at a large computational cost.\n  Motivated by this gap, we introduce and analyze a new recursive pooling technique of local neighborhoods that allows different tradeoffs of computational cost and expressive power. First, we show that this model can count subgraphs of size $k$, and thereby overcomes a known limitation of low-order GNNs. Second, we prove that, in several cases, RNP-GNNs can greatly reduce computational complexity compared to the existing higher-order $k$-GNN and Local Relational Pooling (LRP) networks. \n "}}
{"id": "xUu6kqhFlrx", "cdate": 1577836800000, "mdate": 1671926467770, "content": {"title": "Counting Substructures with Higher-Order Graph Neural Networks: Possibility and Impossibility Results", "abstract": "While message passing Graph Neural Networks (GNNs) have become increasingly popular architectures for learning with graphs, recent works have revealed important shortcomings in their expressive power. In response, several higher-order GNNs have been proposed that substantially increase the expressive power, albeit at a large computational cost. Motivated by this gap, we explore alternative strategies and lower bounds. In particular, we analyze a new recursive pooling technique of local neighborhoods that allows different tradeoffs of computational cost and expressive power. First, we prove that this model can count subgraphs of size $k$, and thereby overcomes a known limitation of low-order GNNs. Second, we show how recursive pooling can exploit sparsity to reduce the computational complexity compared to the existing higher-order GNNs. More generally, we provide a (near) matching information-theoretic lower bound for counting subgraphs with graph representations that pool over representations of derived (sub-)graphs. We also discuss lower bounds on time complexity."}}
{"id": "OqhVbARJDY", "cdate": 1577836800000, "mdate": 1671926467679, "content": {"title": "Private Function Computation", "abstract": "In this paper, we study the problem of private function computation, where a user wants to compute a function of some inputs, using N \u2208 N servers, where the function is a private combination/composition of some K \u2208 N public basic functions {f <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sub> , f <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sub> , ... , f <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">K</sub> }. More precisely, for some inputs W <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">m</sub> , m \u2208 [1 : M], the user's goal is to calculate h(W <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">m</sub> ) = \u03a3j=1J \u03b1 <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">j</sub> h <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">j</sub> (W <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">m</sub> ), for some J \u2208 N, some scalers \u03b1 <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">j</sub> , j \u2208 [1 : J], and some functions h <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">j</sub> (.), j \u2208 [1 : J], where each is an arbitrary compositions of the basic functions {f <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sub> , f <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sub> , ... , f <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">K</sub> }. The computation is done through a sequence of queries to N servers. In each query, the user sends an input W, which is a (possibly randomized) function of W <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sub> :M and the answers to the previous queries, to one of the servers, and asks the server to return f <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">k</sub> (W), for some k \u2208 [1 : K]. The servers should not obtain any information about the structure of the function h(.), i.e., the way the basic functions are combined to form h(.), from the sequence of queries they received, even if T of them collude, for some T \u2208 N. In this paper, we focus on the cases, where basic functions are linear and can be represented by (possibly large-scale) full-rank matrices, and each basic function may contribute in function h(.) for at most once. We prove that C, defined as the supremum of the number of desired computations of the basic functions, normalized by the number of queries, in asymptotic regimes of large M, satisfies the following inequality: min{(1-T/N)/(1-1/K), (1- T-1/N)}\u2264C\u22641. The key idea is that in the proposed scheme, each server is asked to compute a specific order of basic functions, independent from the user's desired function. In addition, some random vectors are added to the inputs of the queries such that the sequence of the queries does not leak any information."}}
{"id": "yPbSjo8shM", "cdate": 1514764800000, "mdate": 1671926467689, "content": {"title": "Genome-Wide Association Studies: Information Theoretic Limits of Reliable Learning", "abstract": "In the problems of Genome-Wide Association Study (GWAS), the objective is to associate subsequences of individual's genomes to the observable characteristics called phenotypes. The genome containing the biological information of an individual can be represented by a sequence of length <i xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">G</i> . Many observable characteristics of the individuals can be related to a subsequence of a given length <i xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">L</i> , called <i xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">causal subsequence</i> . The environmental affects make the relation between the causal subsequence and the observable characteristics a stochastic function. Our objective in this paper is to detect the causal subsequence of a specific phenotype using a dataset of <i xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">N</i> individuals and their observed characteristics. We introduce an abstract formulation of GWAS which allows us to investigate the problem from an information theoretic perspective. In particular, as the parameters <i xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">N</i> , <i xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">G</i> , and <i xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">L</i> grow, we observe a threshold effect at [( <i xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Gh</i> ( <i xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">L</i> / <i xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">G</i> ))/ <i xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">N</i> ], where <i xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">h</i> (.) is the binary entropy function. This effect allows us to define the capacity of recovering the causal subsequence by denoting the rate of the GWAS problem as [( <i xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Gh</i> ( <i xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">L</i> / <i xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">G</i> ))/ <i xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">N</i> ]. We develop an achievable scheme and a matching converse for this problem, and thus characterize its capacity in two scenarios: the zero-error-rate and the \u03b5-error-rate."}}
{"id": "rCYTRDPqUT", "cdate": 1514764800000, "mdate": 1671926467749, "content": {"title": "Optimum Transmission Delay for Function Computation in NFV-based Networks: the role of Network Coding and Redundant Computing", "abstract": "In this paper, we study the problem of delay minimization in NFV-based networks. In such systems, the ultimate goal of any request is to compute a sequence of functions in the network, where each function can be computed at only a specific subset of network nodes. In conventional approaches, for each function, we choose one node from the corresponding subset of the nodes to compute that function. In contrast, in this work, we allow each function to be computed in more than one node, redundantly in parallel, to respond to a given request. We argue that such redundancy in computation not only improves the reliability of the network, but would also, perhaps surprisingly, reduce the overall transmission delay. In particular, we establish that by judiciously choosing the subset of nodes which compute each function, in conjunction with a linear network coding scheme to deliver the result of each computation, we can characterize and achieve the optimal end-to-end transmission delay. In addition, we show that using such technique, we can significantly reduce the transmission delay as compared to the conventional approach. In some scenarios, such reduction can even scale with the size of the network. More precisely, by increasing the number of nodes that can compute the given function in parallel by a multiplicative factor, the end-to-end delay will also decrease by the same factor. Moreover, we show that while finding the subset of nodes for each computation, in general, is a complex integer program, approximation algorithms can be proposed to reduce the computational complexity. In fact, for the case where the number of computing nodes for a given function is upper-bounded by a constant, a dynamic programming scheme can be proposed to find the optimum subsets in polynomial times. Our numerical simulations confirm the achieved gain in performance in comparison with conventional approaches."}}
{"id": "lnjyoqGitVC", "cdate": 1514764800000, "mdate": 1671926467785, "content": {"title": "On the Identifiability of Parameters in the Population Stratification Problem: A Worst-Case Analysis", "abstract": "In the problem of population stratification, each data instance is generated based on a finite mixture model with <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$K$</tex> mixture components and <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$L$</tex> observed variables. Each variable takes its value in a finite state space with cardinality M. The variables are drawn independently in each mixture component. In this paper, we study the problem of the identifiability of parameters in this model, i.e. interpolation of the parameters of a mixture model from its mixture distribution. First we define the notion of informative variables. Then, we prove that the parameters of the problem are identifiable in the worst-case regime, if and only if the number of informative variables is greater than or equal to 2K \u2212 1. As a result, in the worst-case analysis of the identifiability problem of finite mixture models, the number of required informative variables is \u0398(K) and it is independent of the state space size."}}
{"id": "hMA75g7huqK", "cdate": 1514764800000, "mdate": 1671926467779, "content": {"title": "Information theoretic limits of learning of the causal features in a linear model", "abstract": "In this paper, we study the problem of causal features detection in a linear model. In a mathematical model, we consider a dataset of N samples, each represented by a sequence of G binary features. Associated to each sample, there is a binary label. It is assumed that the labels are related to a latent subset of the features, called causal features, via a linear function. More precisely, in our model, each label is the result of a noisy observation of a linear function of the causal features. We assume that the number of the causal features is bounded by L, where L is a given positive integer. In this paper, our objective is to detect the set of the causal features. In this way, at the limits of the parameters N, G and L, we observe a threshold effect at Gh(L/G)/N, where h(.) is the binary entropy function. Hence, we define the rate of the problem of causal features detection as Gh(L/G)/N and we characterize the capacity, using an achievable scheme and a matching converse."}}
