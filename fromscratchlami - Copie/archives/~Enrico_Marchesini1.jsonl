{"id": "rRTrlKOkZq", "cdate": 1672531200000, "mdate": 1709775940021, "content": {"title": "Entropy Maximization in High Dimensional Multiagent State Spaces", "abstract": "Underwater or planetary exploration are prime examples of missions that can benefit from autonomous agents working together. However, discovering effective team-level behaviors (i.e., coordinated joint actions) is challenging in these domains as agents typically receive a sparse reward (zero-or constant-for the majority of the interactions). To address this issue, intrinsic rewards encourage agents to explore diverse policies to visit the state space more effectively. Unfortunately, as the agents\u2019 state space grows, intrinsic reward-based (i.e., curiosity) approaches become less effective as they cannot effectively distinguish a diverse set of states. In this direction, we introduce state entropy maximization for multiagent learning where agents explore using local (dense) rewards and learn to solve the coordination task by leveraging global (sparse) rewards. Because of the intrinsic ability to balance local and global rewards, our approach enables the state entropy function to remain effective in high dimensional state spaces. Experiments in tightly coupled tasks requiring complex joint actions, show that local entropy-based rewards enable agents to discover successful team behaviors in high dimensional spaces where previous hand-tuned count-based rewards fail."}}
{"id": "qO3oyaKOb4", "cdate": 1672531200000, "mdate": 1709775940030, "content": {"title": "Improving Deep Policy Gradients with Value Function Search", "abstract": ""}}
{"id": "mx2eVdXbVx", "cdate": 1672531200000, "mdate": 1709775940017, "content": {"title": "Enumerating Safe Regions in Deep Neural Networks with Provable Probabilistic Guarantees", "abstract": "Identifying safe areas is a key point to guarantee trust for systems that are based on Deep Neural Networks (DNNs). To this end, we introduce the AllDNN-Verification problem: given a safety property and a DNN, enumerate the set of all the regions of the property input domain which are safe, i.e., where the property does hold. Due to the #P-hardness of the problem, we propose an efficient approximation method called epsilon-ProVe. Our approach exploits a controllable underestimation of the output reachable sets obtained via statistical prediction of tolerance limits, and can provide a tight (with provable probabilistic guarantees) lower estimate of the safe areas. Our empirical evaluation on different standard benchmarks shows the scalability and effectiveness of our method, offering valuable insights for this new type of verification of DNNs."}}
{"id": "iQjYCIaBcJR", "cdate": 1672531200000, "mdate": 1681491644288, "content": {"title": "Safe Deep Reinforcement Learning by Verifying Task-Level Properties", "abstract": ""}}
{"id": "fvdpVZl8E4r", "cdate": 1672531200000, "mdate": 1681491644288, "content": {"title": "Improving Deep Policy Gradients with Value Function Search", "abstract": ""}}
{"id": "eVZ8gB5odFS", "cdate": 1672531200000, "mdate": 1709775940086, "content": {"title": "Online Safety Property Collection and Refinement for Safe Deep Reinforcement Learning in Mapless Navigation", "abstract": "Safety is essential for deploying Deep Reinforcement Learning (DRL) algorithms in real-world scenarios. Recently, verification approaches have been proposed to allow quantifying the number of violations of a DRL policy over input-output relationships, called properties. However, such properties are hard-coded and require task-level knowledge, making their application intractable in challenging safety-critical tasks. To this end, we introduce the Collection and Refinement of Online Properties (CROP) framework to design properties at training time. CROP employs a cost signal to identify unsafe interactions and use them to shape safety properties. Hence, we propose a refinement strategy to combine properties that model similar unsafe interactions. Our evaluation compares the benefits of computing the number of violations using standard hard-coded properties and the ones generated with CROP. We evaluate our approach in several robotic mapless navigation tasks and demonstrate that the violation metric computed with CROP allows higher returns and lower violations over previous Safe DRL approaches."}}
{"id": "PxQ5r3UJFs", "cdate": 1672531200000, "mdate": 1709775940029, "content": {"title": "Online Safety Property Collection and Refinement for Safe Deep Reinforcement Learning in Mapless Navigation", "abstract": "Safety is essential for deploying Deep Reinforcement Learning (DRL) algorithms in real-world scenarios. Recently, verification approaches have been proposed to allow quantifying the number of violations of a DRL policy over input-output relationships, called properties. However, such properties are hard-coded and require task-level knowledge, making their application intractable in challenging safety-critical tasks. To this end, we introduce the Collection and Refinement of Online Properties (CROP) framework to design properties at training time. CROP employs a cost signal to identify unsafe interactions and use them to shape safety properties. Hence, we propose a refinement strategy to combine properties that model similar unsafe interactions. Our evaluation compares the benefits of computing the number of violations using standard hard-coded properties and the ones generated with CROP. We evaluate our approach in several robotic mapless navigation tasks and demonstrate that the violation metric computed with CROP allows higher returns and lower violations over previous Safe DRL approaches."}}
{"id": "DnPTViTfAt", "cdate": 1672531200000, "mdate": 1709775940012, "content": {"title": "Safe Deep Reinforcement Learning by Verifying Task-Level Properties", "abstract": "Cost functions are commonly employed in Safe Deep Reinforcement Learning (DRL). However, the cost is typically encoded as an indicator function due to the difficulty of quantifying the risk of policy decisions in the state space. Such an encoding requires the agent to visit numerous unsafe states to learn a cost-value function to drive the learning process toward safety. Hence, increasing the number of unsafe interactions and decreasing sample efficiency. In this paper, we investigate an alternative approach that uses domain knowledge to quantify the risk in the proximity of such states by defining aviolation metric. This metric is computed by verifying task-level properties, shaped as input-output conditions, and it is used as a penalty to bias the policy away from unsafe states without learning an additional value function. We investigate the benefits of using the violation metric in standard Safe DRL benchmarks and robotic mapless navigation tasks. The navigation experiments bridge the gap between Safe DRL and robotics, introducing a framework that allows rapid testing on real robots. Our experiments show that policies trained with the violation penalty achieve higher performance over Safe DRL baselines and significantly reduce the number of visited unsafe states."}}
{"id": "6qZC7pfenQm", "cdate": 1663850126652, "mdate": null, "content": {"title": "Improving Deep Policy Gradients with Value Function Search", "abstract": "Deep Policy Gradient (PG) algorithms employ value networks to drive the learning of parameterized policies and reduce the variance of the gradient estimates. However, value function approximation gets stuck in local optima and struggles to fit the actual return, limiting the variance reduction efficacy and leading policies to sub-optimal performance. This paper focuses on improving value approximation and analyzing the effects on Deep PG primitives such as value prediction, variance reduction, and correlation of gradient estimates with the true gradient. To this end, we introduce a Value Function Search that employs a population of perturbed value networks to search for a better approximation. Our framework does not require additional environment interactions, gradient computations, or ensembles, providing a computationally inexpensive approach to enhance the supervised learning task on which value networks train. Crucially, we show that improving Deep PG primitives results in improved sample efficiency and policies with higher returns using common continuous control benchmark domains."}}
{"id": "mO8ZfUmMH1k", "cdate": 1640995200000, "mdate": 1709775940019, "content": {"title": "Curriculum learning for safe mapless navigation", "abstract": "This work investigates the effects of Curriculum Learning (CL)-based approaches on the agent's performance. In particular, we focus on the safety aspect of robotic mapless navigation, comparing over a standard end-to-end (E2E) training strategy. To this end, we present a CL approach that leverages Transfer of Learning (ToL) and fine-tuning in a Unity-based simulation with the Robotnik Kairos as a robotic agent. For a fair comparison, our evaluation considers an equal computational demand for every learning approach (i.e., the same number of interactions and difficulty of the environments) and confirms that our CL-based method that uses ToL outperforms the E2E methodology. In particular, we improve the average success rate and the safety of the trained policy, resulting in 10% fewer collisions in unseen testing scenarios. To further confirm these results, we employ a formal verification tool to quantify the number of correct behaviors of Reinforcement Learning policies over desired specifications."}}
