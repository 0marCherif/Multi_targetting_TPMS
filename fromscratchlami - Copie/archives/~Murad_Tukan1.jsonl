{"id": "bth6XbnDmib", "cdate": 1663850423094, "mdate": null, "content": {"title": "Approximating any Function via Coreset for Radial Basis Functions: Towards Provable Data Subset Selection For Efficient Neural Networks training", "abstract": "Radial basis function neural networks (\\emph{RBFNN}) are notoriously known for their capability to approximate any continuous function on a closed bounded set with arbitrary precision given enough hidden neurons. Coreset is a small weighted subset of an input set of items, that provably approximates their loss function for a given set of queries (models, classifiers, etc.). In this paper, we suggest the first coreset construction algorithm for \\emph{RBFNNs}, i.e., a small weighted subset which approximates the loss of the input data on any radial basis function network and thus approximates any function defined by an \\emph{RBFNN} on the big input data. This is done by constructing coresets for radial basis and Laplacian loss functions. We use our coreset to suggest a provable data subset selection algorithm for training deep neural networks, since our coreset approximates every function, it should approximate the gradient of each weight in a neural network as it is defined as a function on the input. Experimental results on function approximation and dataset subset selection on popular network architectures and data sets are presented, demonstrating the efficacy and accuracy of our coreset construction."}}
{"id": "btpIaJiRx6z", "cdate": 1652737516563, "mdate": null, "content": {"title": "Pruning Neural Networks via Coresets and Convex Geometry: Towards No Assumptions", "abstract": "Pruning is one of the predominant approaches for compressing deep neural networks (DNNs). Lately, coresets (provable data summarizations) were leveraged for pruning DNNs, adding the advantage of theoretical guarantees on the trade-off between the compression rate and the approximation error. However, coresets in this domain were either data dependant or generated under restrictive assumptions on both the model's weights and inputs. In real-world scenarios, such assumptions are rarely satisfied, limiting the applicability of coresets. To this end, we suggest a novel and robust framework for computing such coresets under mild assumptions on the model's weights and without any assumption on the training data. The idea is to compute the importance of each neuron in each layer with respect to the output of the following layer. This is achieved by an elegant combination of L\\\"{o}wner ellipsoid and Caratheodory theorem.\nOur method is simultaneously data-independent, applicable to various networks and datasets (due to the simplified assumptions), and theoretically supported. Experimental results show that our method outperforms existing coreset based neural pruning approaches across a wide range of networks and datasets. For example, our method achieved a $62\\%$ compression rate on ResNet50 on ImageNet with $1.09\\%$ drop in accuracy."}}
{"id": "yMHOo-Kn-kG", "cdate": 1577836800000, "mdate": null, "content": {"title": "On Coresets for Support Vector Machines", "abstract": "We present an efficient coreset construction algorithm for large-scale Support Vector Machine (SVM) training in Big Data and streaming applications. A coreset is a small, representative subset of the original data points such that a models trained on the coreset are provably competitive with those trained on the original data set. Since the size of the coreset is generally much smaller than the original set, our preprocess-then-train scheme has potential to lead to significant speedups when training SVM models. We prove lower and upper bounds on the size of the coreset required to obtain small data summaries for the SVM problem. As a corollary, we show that our algorithm can be used to extend the applicability of any off-the-shelf SVM solver to streaming, distributed, and dynamic data settings. We evaluate the performance of our algorithm on real-world and synthetic data sets. Our experimental results reaffirm the favorable theoretical properties of our algorithm and demonstrate its practical effectiveness in accelerating SVM training."}}
{"id": "uDUyFEjMa5Y", "cdate": 1577836800000, "mdate": null, "content": {"title": "Sets Clustering", "abstract": "The input to the \\emph{sets-$k$-means} problem is an integer $k\\geq 1$ and a set $\\mathcal{P}=\\{P_1,\\cdots,P_n\\}$ of sets in $\\mathbb{R}^d$. The goal is to compute a set $C$ of $k$ centers (points) in $\\mathbb{R}^d$ that minimizes the sum $\\sum_{P\\in \\mathcal{P}} \\min_{p\\in P, c\\in C}\\left\\| p-c \\right\\|^2$ of squared distances to these sets. An \\emph{$\\varepsilon$-core-set} for this problem is a weighted subset of $\\mathcal{P}$ that approximates this sum up to $1\\pm\\varepsilon$ factor, for \\emph{every} set $C$ of $k$ centers in $\\mathbb{R}^d$. We prove that such a core-set of $O(\\log^2{n})$ sets always exists, and can be computed in $O(n\\log{n})$ time, for every input $\\mathcal{P}$ and every fixed $d,k\\geq 1$ and $\\varepsilon \\in (0,1)$. The result easily generalized for any metric space, distances to the power of $z>0$, and M-estimators that handle outliers. Applying an inefficient but optimal algorithm on this coreset allows us to obtain the first PTAS ($1+\\varepsilon$ approximation) for the sets-$k$-means problem that takes time near linear in $n$. This is the first result even for sets-mean on the plane ($k=1$, $d=2$). Open source code and experimental results for document classification and facility locations are also provided."}}
{"id": "bFNXSVlIQsQ", "cdate": 1577836800000, "mdate": null, "content": {"title": "On Coresets for Support Vector Machines", "abstract": "We present an efficient coreset construction algorithm for large-scale Support Vector Machine (SVM) training in Big Data and streaming applications. A coreset is a small, representative subset of the original data points such that a models trained on the coreset are provably competitive with those trained on the original data set. Since the size of the coreset is generally much smaller than the original set, our preprocess-then-train scheme has potential to lead to significant speedups when training SVM models. We prove lower and upper bounds on the size of the coreset required to obtain small data summaries for the SVM problem. As a corollary, we show that our algorithm can be used to extend the applicability of any off-the-shelf SVM solver to streaming, distributed, and dynamic data settings. We evaluate the performance of our algorithm on real-world and synthetic data sets. Our experimental results reaffirm the favorable theoretical properties of our algorithm and demonstrate its practical effectiveness in accelerating SVM training."}}
{"id": "XdbILVddFja", "cdate": 1577836800000, "mdate": null, "content": {"title": "Coresets for Near-Convex Functions", "abstract": "Coreset is usually a small weighted subset of $n$ input points in $\\mathbb{R}^d$, that provably approximates their loss function for a given set of queries (models, classifiers, etc.). Coresets become increasingly common in machine learning since existing heuristics or inefficient algorithms may be improved by running them possibly many times on the small coreset that can be maintained for streaming distributed data. Coresets can be obtained by sensitivity (importance) sampling, where its size is proportional to the total sum of sensitivities. Unfortunately, computing the sensitivity of each point is problem dependent and may be harder to compute than the original optimization problem at hand. We suggest a generic framework for computing sensitivities (and thus coresets) for wide family of loss functions which we call near-convex functions. This is by suggesting the $f$-SVD factorization that generalizes the SVD factorization of matrices to functions. Example applications include coresets that are either new or significantly improves previous results, such as SVM, Logistic regression, M-estimators, and $\\ell_z$-regression. Experimental results and open source are also provided."}}
{"id": "WhFpbKs3kSt", "cdate": 1577836800000, "mdate": null, "content": {"title": "Coresets for Near-Convex Functions", "abstract": "Coreset is usually a small weighted subset of $n$ input points in $\\mathbb{R}^d$, that provably approximates their loss function for a given set of queries (models, classifiers, etc.). Coresets become increasingly common in machine learning since existing heuristics or inefficient algorithms may be improved by running them possibly many times on the small coreset that can be maintained for streaming distributed data. Coresets can be obtained by sensitivity (importance) sampling, where its size is proportional to the total sum of sensitivities. Unfortunately, computing the sensitivity of each point is problem dependent and may be harder to compute than the original optimization problem at hand. We suggest a generic framework for computing sensitivities (and thus coresets) for wide family of loss functions which we call near-convex functions. This is by suggesting the $f$-SVD factorization that generalizes the SVD factorization of matrices to functions. Example applications include coresets that are either new or significantly improves previous results, such as SVM, Logistic regression, M-estimators, and $\\ell_z$-regression. Experimental results and open source are also provided."}}
{"id": "Q7Q93E6LB3", "cdate": 1577836800000, "mdate": null, "content": {"title": "Compressed Deep Networks: Goodbye SVD, Hello Robust Low-Rank Approximation", "abstract": "A common technique for compressing a neural network is to compute the $k$-rank $\\ell_2$ approximation $A_{k,2}$ of the matrix $A\\in\\mathbb{R}^{n\\times d}$ that corresponds to a fully connected layer (or embedding layer). Here, $d$ is the number of the neurons in the layer, $n$ is the number in the next one, and $A_{k,2}$ can be stored in $O((n+d)k)$ memory instead of $O(nd)$. This $\\ell_2$-approximation minimizes the sum over every entry to the power of $p=2$ in the matrix $A - A_{k,2}$, among every matrix $A_{k,2}\\in\\mathbb{R}^{n\\times d}$ whose rank is $k$. While it can be computed efficiently via SVD, the $\\ell_2$-approximation is known to be very sensitive to outliers (\"far-away\" rows). Hence, machine learning uses e.g. Lasso Regression, $\\ell_1$-regularization, and $\\ell_1$-SVM that use the $\\ell_1$-norm. This paper suggests to replace the $k$-rank $\\ell_2$ approximation by $\\ell_p$, for $p\\in [1,2]$. We then provide practical and provable approximation algorithms to compute it for any $p\\geq1$, based on modern techniques in computational geometry. Extensive experimental results on the GLUE benchmark for compressing BERT, DistilBERT, XLNet, and RoBERTa confirm this theoretical advantage. For example, our approach achieves $28\\%$ compression of RoBERTa's embedding layer with only $0.63\\%$ additive drop in the accuracy (without fine-tuning) in average over all tasks in GLUE, compared to $11\\%$ drop using the existing $\\ell_2$-approximation. Open code is provided for reproducing and extending our results."}}
{"id": "PBWDeMnPIn", "cdate": 1577836800000, "mdate": null, "content": {"title": "Sets Clustering", "abstract": "The input to the \\emph{sets-$k$-means} problem is an integer $k\\geq 1$ and a set $\\mathcal{P}=\\{P_1,\\cdots,P_n\\}$ of fixed sized sets in $\\mathbb{R}^d$. The goal is to compute a set $C$ of $k$ cent..."}}
{"id": "2zNyP2kAnF-", "cdate": 1577836800000, "mdate": null, "content": {"title": "Faster PAC Learning and Smaller Coresets via Smoothed Analysis", "abstract": "PAC-learning usually aims to compute a small subset ($\\varepsilon$-sample/net) from $n$ items, that provably approximates a given loss function for every query (model, classifier, hypothesis) from a given set of queries, up to an additive error $\\varepsilon\\in(0,1)$. Coresets generalize this idea to support multiplicative error $1\\pm\\varepsilon$. Inspired by smoothed analysis, we suggest a natural generalization: approximate the \\emph{average} (instead of the worst-case) error over the queries, in the hope of getting smaller subsets. The dependency between errors of different queries implies that we may no longer apply the Chernoff-Hoeffding inequality for a fixed query, and then use the VC-dimension or union bound. This paper provides deterministic and randomized algorithms for computing such coresets and $\\varepsilon$-samples of size independent of $n$, for any finite set of queries and loss function. Example applications include new and improved coreset constructions for e.g. streaming vector summarization [ICML'17] and $k$-PCA [NIPS'16]. Experimental results with open source code are provided."}}
