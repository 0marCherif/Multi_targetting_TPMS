{"id": "RM2IoZtWWhM", "cdate": 1693526400000, "mdate": 1694786928580, "content": {"title": "Refining neural network predictions using background knowledge", "abstract": "Recent work has shown learning systems can use logical background knowledge to compensate for a lack of labeled training data. Many methods work by creating a loss function that encodes this knowledge. However, often the logic is discarded after training, even if it is still helpful at test time. Instead, we ensure neural network predictions satisfy the knowledge by refining the predictions with an extra computation step. We introduce differentiable refinement functions that find a corrected prediction close to the original prediction. We study how to effectively and efficiently compute these refinement functions. Using a new algorithm called iterative local refinement (ILR), we combine refinement functions to find refined predictions for logical formulas of any complexity. ILR finds refinements on complex SAT formulas in significantly fewer iterations and frequently finds solutions where gradient descent can not. Finally, ILR produces competitive results in the MNIST addition task."}}
{"id": "CVTGEbttq1x", "cdate": 1672531200000, "mdate": 1694786928553, "content": {"title": "IntelliGraphs: Datasets for Benchmarking Knowledge Graph Generation", "abstract": "Knowledge Graph Embedding (KGE) models are used to learn continuous representations of entities and relations. A key task in the literature is predicting missing links between entities. However, Knowledge Graphs are not just sets of links but also have semantics underlying their structure. Semantics is crucial in several downstream tasks, such as query answering or reasoning. We introduce the subgraph inference task, where a model has to generate likely and semantically valid subgraphs. We propose IntelliGraphs, a set of five new Knowledge Graph datasets. The IntelliGraphs datasets contain subgraphs with semantics expressed in logical rules for evaluating subgraph inference. We also present the dataset generator that produced the synthetic datasets. We designed four novel baseline models, which include three models based on traditional KGEs. We evaluate their expressiveness and show that these models cannot capture the semantics. We believe this benchmark will encourage the development of machine learning models that emphasize semantic understanding."}}
{"id": "chUYLpHXWr", "cdate": 1640995200000, "mdate": 1681720620589, "content": {"title": "Prompting as Probing: Using Language Models for Knowledge Base Construction", "abstract": "Language Models (LMs) have proven to be useful in various downstream applications, such as summarisation, translation, question answering and text classification. LMs are becoming increasingly important tools in Artificial Intelligence, because of the vast quantity of information they can store. In this work, we present ProP (Prompting as Probing), which utilizes GPT-3, a large Language Model originally proposed by OpenAI in 2020, to perform the task of Knowledge Base Construction (KBC). ProP implements a multi-step approach that combines a variety of prompting techniques to achieve this. Our results show that manual prompt curation is essential, that the LM must be encouraged to give answer sets of variable lengths, in particular including empty answer sets, that true/false questions are a useful device to increase precision on suggestions generated by the LM, that the size of the LM is a crucial factor, and that a dictionary of entity aliases improves the LM score. Our evaluation study indicates that these proposed techniques can substantially enhance the quality of the final predictions: ProP won track 2 of the LM-KBC competition, outperforming the baseline by 36.4 percentage points. Our implementation is available on https://github.com/HEmile/iswc-challenge."}}
{"id": "IqikJ31H0TD", "cdate": 1640995200000, "mdate": 1681720620589, "content": {"title": "A-NeSI: A Scalable Approximate Method for Probabilistic Neurosymbolic Inference", "abstract": "We study the problem of combining neural networks with symbolic reasoning. Recently introduced frameworks for Probabilistic Neurosymbolic Learning (PNL), such as DeepProbLog, perform exponential-time exact inference, limiting the scalability of PNL solutions. We introduce Approximate Neurosymbolic Inference (A-NeSI): a new framework for PNL that uses neural networks for scalable approximate inference. A-NeSI 1) performs approximate inference in polynomial time without changing the semantics of probabilistic logics; 2) is trained using data generated by the background knowledge; 3) can generate symbolic explanations of predictions; and 4) can guarantee the satisfaction of logical constraints at test time, which is vital in safety-critical applications. Our experiments show that A-NeSI is the first end-to-end method to solve three neurosymbolic tasks with exponential combinatorial scaling. Finally, our experiments show that A-NeSI achieves explainability and safety without a penalty in performance."}}
{"id": "GZ-mMeCmBu", "cdate": 1640995200000, "mdate": 1681720620591, "content": {"title": "Refining neural network predictions using background knowledge", "abstract": "Recent work has shown logical background knowledge can be used in learning systems to compensate for a lack of labeled training data. Many methods work by creating a loss function that encodes this knowledge. However, often the logic is discarded after training, even if it is still useful at test time. Instead, we ensure neural network predictions satisfy the knowledge by refining the predictions with an extra computation step. We introduce differentiable refinement functions that find a corrected prediction close to the original prediction. We study how to effectively and efficiently compute these refinement functions. Using a new algorithm called Iterative Local Refinement (ILR), we combine refinement functions to find refined predictions for logical formulas of any complexity. ILR finds refinements on complex SAT formulas in significantly fewer iterations and frequently finds solutions where gradient descent can not. Finally, ILR produces competitive results in the MNIST addition task."}}
{"id": "7AlZWe3WRld", "cdate": 1640995200000, "mdate": 1668613555285, "content": {"title": "Analyzing Differentiable Fuzzy Logic Operators", "abstract": ""}}
{"id": "1VSSkP0XJm7", "cdate": 1640995200000, "mdate": 1681720620590, "content": {"title": "Analysis of Measure-Valued Derivatives in a Reinforcement Learning Actor-Critic Framework", "abstract": "Policy gradient methods are successful for a wide range of reinforcement learning tasks. Traditionally, such methods utilize the score function as stochastic gradient estimator. We investigate the effect of replacing the score function with a measure-valued derivative within an on-policy actor-critic algorithm. The hypothesis is that measure-valued derivatives reduce the need for score function variance reduction techniques that are common in policy gradient algorithms. We adapt the actor-critic to measure-valued derivatives and develop a novel algorithm. This method keeps the computational complexity of the measure-valued derivative within bounds by using a parameterized state-value function approximation. We show empirically that measure-valued derivatives have comparable performance to score functions on the environments Pendulum and MountainCar. The empirical results of this study suggest that measure-valued derivatives can serve as low-variance alternative to score functions in on-policy actor-critic and indeed reduce the need for variance reduction techniques."}}
{"id": "KAFyFabsK88", "cdate": 1621630167304, "mdate": null, "content": {"title": "Storchastic: A Framework for General Stochastic Automatic Differentiation", "abstract": "Modelers use automatic differentiation (AD) of computation graphs to implement complex Deep Learning models without defining gradient computations. Stochastic AD extends AD to stochastic computation graphs with sampling steps, which arise when modelers handle the intractable expectations common in Reinforcement Learning and Variational Inference. However, current methods for stochastic AD are limited: They are either only applicable to continuous random variables and differentiable functions, or can only use simple but high variance score-function estimators. To overcome these limitations, we introduce Storchastic, a new framework for AD of stochastic computation graphs. Storchastic allows the modeler to choose from a wide variety of gradient estimation methods at each sampling step, to optimally reduce the variance of the gradient estimates. Furthermore, Storchastic is provably unbiased for estimation of any-order gradients, and generalizes variance reduction techniques to higher-order gradient estimates. Finally, we implement Storchastic as a PyTorch library at github.com/HEmile/storchastic."}}
{"id": "lVmR_9SBJS", "cdate": 1609459200000, "mdate": 1681720620595, "content": {"title": "Storchastic: A Framework for General Stochastic Automatic Differentiation", "abstract": "Modelers use automatic differentiation (AD) of computation graphs to implement complex Deep Learning models without defining gradient computations. Stochastic AD extends AD to stochastic computation graphs with sampling steps, which arise when modelers handle the intractable expectations common in Reinforcement Learning and Variational Inference. However, current methods for stochastic AD are limited: They are either only applicable to continuous random variables and differentiable functions, or can only use simple but high variance score-function estimators. To overcome these limitations, we introduce Storchastic, a new framework for AD of stochastic computation graphs. Storchastic allows the modeler to choose from a wide variety of gradient estimation methods at each sampling step, to optimally reduce the variance of the gradient estimates. Furthermore, Storchastic is provably unbiased for estimation of any-order gradients, and generalizes variance reduction techniques to higher-order gradient estimates. Finally, we implement Storchastic as a PyTorch library at github.com/HEmile/storchastic."}}
{"id": "DJ1Bc526-y9", "cdate": 1602617099145, "mdate": null, "content": {"title": "Type-driven Neural Programming by Example", "abstract": "We propose a method to incorporate programming types into a neural program synthesis approach for programming by example (PBE). We introduce Typed Neuro-Symbolic Program Synthesis (TNSPS), and test it in a functional programming context to empirically verify whether type information helps to improve generalization in neural synthesizers on limited-size datasets.\n\nOur TNSPS model builds upon the existing Neuro-Symbolic Program Synthesis (NSPS) model, by incorporating information on types of input-output examples, of grammar production rules, as well as of the next node to expand in the program.\n\nAdditionally, we introduce a generation method for programs written in a limited subset of the Haskell language. Our experiments show that incorporating type information using TNSPS improves the accuracy of the synthesized programs. This suggests that hybrid approaches that use both neural synthesis and strong type-checking is a fruitful research line.\n"}}
