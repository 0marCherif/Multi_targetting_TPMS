{"id": "7Z2a2XCm889", "cdate": 1640995200000, "mdate": 1667344747487, "content": {"title": "Ego4D: Around the World in 3, 000 Hours of Egocentric Video", "abstract": "We introduce Ego4D, a massive-scale egocentric video dataset and benchmark suite. It offers 3,670 hours of dailylife activity video spanning hundreds of scenarios (household, outdoor, workplace, leisure, etc.) captured by 931 unique camera wearers from 74 worldwide locations and 9 different countries. The approach to collection is designed to uphold rigorous privacy and ethics standards, with consenting participants and robust de-identification procedures where relevant. Ego4D dramatically expands the volume of diverse egocentric video footage publicly available to the research community. Portions of the video are accompanied by audio, 3D meshes of the environment, eye gaze, stereo, and/or synchronized videos from multiple egocentric cameras at the same event. Furthermore, we present a host of new benchmark challenges centered around understanding the first-person visual experience in the past (querying an episodic memory), present (analyzing hand-object manipulation, audio-visual conversation, and social interactions), and future (forecasting activities). By publicly sharing this massive annotated dataset and benchmark suite, we aim to push the frontier of first-person perception. Project page: https://ego4d-data.org/"}}
{"id": "3vIs247jJfi", "cdate": 1640995200000, "mdate": 1667344747470, "content": {"title": "Toward Learning Robust and Invariant Representations with Alignment Regularization and Data Augmentation", "abstract": "Data augmentation has been proven to be an effective technique for developing machine learning models that are robust to known classes of distributional shifts (e.g., rotations of images), and alignment regularization is a technique often used together with data augmentation to further help the model learn representations invariant to the shifts used to augment the data. In this paper, motivated by a proliferation of options of alignment regularizations, we seek to evaluate the performances of several popular design choices along the dimensions of robustness and invariance, for which we introduce a new test procedure. Our synthetic experiment results speak to the benefits of squared \u21132 norm regularization. Further, we also formally analyze the behavior of alignment regularization to complement our empirical study under assumptions we consider realistic. Finally, we test this simple technique we identify (worst-case data augmentation with squared \u21132 norm alignment regularization) and show that the benefits of this method outrun those of the specially designed methods. We also release a software package in both TensorFlow and PyTorch for users to use the method with a couple of lines at https://github.com/jyanln/AlignReg."}}
{"id": "QpU7n-6l0n", "cdate": 1601308162659, "mdate": null, "content": {"title": "On the Consistency Loss for Leveraging Augmented Data to Learn Robust and Invariant Representations", "abstract": "Data augmentation is one of the most popular techniques for improving the robustness of neural networks. In addition to directly training the model with original samples and augmented samples, a torrent of methods regularizing the distance between embeddings/representations of the original samples and their augmented counterparts have been introduced. In this paper, we explore these various regularization choices, seeking to provide a general understanding of how we should regularize the embeddings. Our analysis suggests how the ideal choices of regularization correspond to various assumptions. With an invariance test, we show that regularization is important if the model is to be used in a broader context than the in-lab setting because non-regularized approaches are limited in learning the concept of invariance, despite equally high accuracy. Finally, we also show that the generic approach we identified (squared $\\ell_2$ norm regularized augmentation) performs better than several recent methods, which are each specially designed for one task and significantly more complicated than ours, over three different tasks."}}
{"id": "ys4JmpXP4i1", "cdate": 1577836800000, "mdate": 1667344747485, "content": {"title": "Squared \ud835\udcc12 Norm as Consistency Loss for Leveraging Augmented Data to Learn Robust and Invariant Representations", "abstract": "Data augmentation is one of the most popular techniques for improving the robustness of neural networks. In addition to directly training the model with original samples and augmented samples, a torrent of methods regularizing the distance between embeddings/representations of the original samples and their augmented counterparts have been introduced. In this paper, we explore these various regularization choices, seeking to provide a general understanding of how we should regularize the embeddings. Our analysis suggests the ideal choices of regularization correspond to various assumptions. With an invariance test, we argue that regularization is important if the model is to be used in a broader context than the accuracy-driven setting because non-regularized approaches are limited in learning the concept of invariance, despite equally high accuracy. Finally, we also show that the generic approach we identified (squared $\\ell_2$ norm regularized augmentation) outperforms several recent methods, which are each specially designed for one task and significantly more complicated than ours, over three different tasks."}}
{"id": "qhAUUpOGYUB", "cdate": 1577836800000, "mdate": 1667344747486, "content": {"title": "High-Frequency Component Helps Explain the Generalization of Convolutional Neural Networks", "abstract": "We investigate the relationship between the frequency spectrum of image data and the generalization behavior of convolutional neural networks (CNN). We first notice CNN's ability in capturing the high-frequency components of images. These high-frequency components are almost imperceptible to a human. Thus the observation leads to multiple hypotheses that are related to the generalization behaviors of CNN, including a potential explanation for adversarial examples, a discussion of CNN's trade-off between robustness and accuracy, and some evidence in understanding training heuristics."}}
{"id": "BJerUCEtPB", "cdate": 1569439309340, "mdate": null, "content": {"title": "Smooth Kernels Improve Adversarial Robustness and Perceptually-Aligned Gradients", "abstract": "Recent research has shown that CNNs are often overly sensitive to high-frequency textural patterns. Inspired by the intuition that humans are more sensitive to the lower-frequency (larger-scale) patterns we design a regularization scheme that penalizes large differences between adjacent components within each convolutional kernel. We apply our regularization onto several popular training methods, demonstrating that the models with the proposed smooth kernels enjoy improved adversarial robustness. Further, building on recent work establishing connections between adversarial robustness and interpretability, we show that our method appears to give more perceptually-aligned gradients. "}}
{"id": "fBEbUpsyTvA", "cdate": 1546300800000, "mdate": 1667344747484, "content": {"title": "Regularized Adversarial Training (RAT) for Robust Cellular Electron Cryo Tomograms Classification", "abstract": "Cellular Electron Cryo Tomography (CECT) 3D imaging has permitted biomedical community to study macromolecule structures inside single cells with deep learning approaches. Many deep learning-based methods have since been developed to classify macromolecule structures from tomograms with high accuracy. However, several recent studies have demonstrated the lack of robustness in these models against often-imperceptible, designed changes of input. Therefore, making existing subtomogram-classification models robust remains a serious challenge. In this paper, we study the robustness of the state-of-the-art subtomogram classifier on CECT images and propose a method called Regularized Adversarial Training (RAT) to defend the classifier against a wide range of designed threats. Our results show that RAT improves robustness for CECT image classification over the previous methods."}}
{"id": "cNJODPRzfH", "cdate": 1546300800000, "mdate": 1667344747515, "content": {"title": "Deep Self-Paced Learning for Semi-Supervised Person Re-Identification Using Multi-View Self-Paced Clustering", "abstract": "Semi-supervised person re-identification (Re-ID) is an extension of the existing popular Re-ID research, which only uses a small portion of labeled data, while the majority of the training samples are unlabeled. This paper approaches the problem by constructing a set of heterogeneous convolutional neural networks (CNNs) fine-tuned by utilizing the labeled training samples, and then propagating the labels to the unlabeled portion for further fine-tuning the overall system in a self-paced manner. In this work, a novel self-paced multi-view clustering is presented to generate pseudo labels for unlabeled training samples, which combines multiple heterogeneous CNNs features to cluster. In our clustering method, we introduce a self-paced regularizer to select reliable samples for fine-tuning each CNNs by minimizing ranking loss and identification loss. Specifically, we select a small portion of unlabeled training data when multiple CNNs are weak. With CNNs become stronger, more and more unlabeled samples are selected. Pseudo label estimation and CNNs training are improved simultaneously, which optimize alternatively until all the unlabeled training samples are selected. In our framework, both the optimization of multiple CNNs training and multi-view clustering on unlabeled training samples are self-paced optimizing procedure. Extensive experiments have been conducted on two large-scale Re-ID datasets to demonstrate the superiority of the proposed method."}}
{"id": "oU3SbG2c45", "cdate": 1514764800000, "mdate": 1667344747515, "content": {"title": "Multitask Learning With Enhanced Modules", "abstract": "In multitask learning (MTL) paradigm, modularity is an effective way to achieve component and parameter reuse as well as system extensibility. In this work, we introduce two enhanced modules named res-fire module (RF) and dimension reduction module(DR) to improve the performance of modular MTL network - PathNet. In addition, in order to further improve the transfer ability of the network, we apply learnable scale parameters to merge the outputs of the modules in the same layer and then scatter to the next layer. Experiments on MNIST, CIFAR, SVHN and MiniImageNet demonstrate that, with the similar scale as PathNet, our architecture achieves remarkable improvement in both transfer ability and expression ability. Our design used x5.23 fewer generations to achieve 99% accuracy on a source-to-target MNIST classification task compared with DeepMind's PathNet. We also increase the accuracy of CIFARSVHN transfer task by x1.9. Also we get 70.75% accuracy on miniImageNet."}}
