{"id": "05g7mnKJyJ7", "cdate": 1683910859733, "mdate": 1683910859733, "content": {"title": "Is Local SGD Better than Minibatch SGD?", "abstract": "We study local SGD (also known as parallel SGD and federated averaging), a natural and frequently\nused stochastic distributed optimization method. Its theoretical foundations are currently lacking and\nwe highlight how all existing error guarantees in the convex setting are dominated by a simple baseline,\nminibatch SGD. (1) For quadratic objectives we prove that local SGD strictly dominates minibatch SGD\nand that accelerated local SGD is minimax optimal for quadratics; (2) For general convex objectives we\nprovide the first guarantee that at least sometimes improves over minibatch SGD; (3) We show that\nindeed local SGD does not dominate minibatch SGD by presenting a lower bound on the performance\nof local SGD that is worse than the minibatch SGD guarantee."}}
{"id": "4XP0ZuQKXmV", "cdate": 1652737683455, "mdate": null, "content": {"title": "Asynchronous SGD Beats Minibatch SGD Under Arbitrary Delays", "abstract": "The existing analysis of asynchronous stochastic gradient descent (SGD) degrades dramatically when any delay is large, giving the impression that performance depends primarily on the delay. On the contrary, we prove much better guarantees for the same asynchronous SGD algorithm regardless of the delays in the gradients, depending instead just on the number of parallel devices used to implement the algorithm. Our guarantees are strictly better than the existing analyses, and we also argue that asynchronous SGD outperforms synchronous minibatch SGD in the settings we consider. For our analysis, we introduce a novel recursion based on ``virtual iterates'' and delay-adaptive stepsizes, which allow us to derive state-of-the-art guarantees for both convex and non-convex objectives. "}}
{"id": "SNElc7QmMDe", "cdate": 1652737602689, "mdate": null, "content": {"title": "Towards Optimal Communication Complexity in Distributed Non-Convex Optimization", "abstract": "We study the problem of distributed stochastic non-convex optimization with intermittent communication. We consider the full participation setting where $M$ machines work in parallel over $R$ communication rounds and the partial participation setting where $M$ machines are sampled independently every round from some meta-distribution over machines. We propose and analyze a new algorithm that improves existing methods by requiring fewer and lighter variance reduction operations. We also present lower bounds, showing our algorithm is either $\\textit{optimal}$ or $\\textit{almost optimal}$ in most settings. Numerical experiments demonstrate the superior performance of our algorithm."}}
{"id": "ui0sz9Y2x9X", "cdate": 1621630318303, "mdate": null, "content": {"title": "A Stochastic Newton Algorithm for Distributed Convex Optimization", "abstract": "We propose and analyze a stochastic Newton algorithm for homogeneous distributed stochastic convex optimization, where each machine can calculate stochastic gradients of the same population objective, as well as stochastic Hessian-vector products (products of an independent unbiased estimator of the Hessian of the population objective with arbitrary vectors), with many such stochastic computations performed between rounds of communication.  We show that our method can reduce the number, and frequency, of required communication rounds, compared to existing methods without hurting performance, by proving convergence guarantees for quasi-self-concordant objectives (e.g., logistic regression), alongside empirical evidence."}}
{"id": "5BD4_awH4Fd", "cdate": 1621630318303, "mdate": null, "content": {"title": "A Stochastic Newton Algorithm for Distributed Convex Optimization", "abstract": "We propose and analyze a stochastic Newton algorithm for homogeneous distributed stochastic convex optimization, where each machine can calculate stochastic gradients of the same population objective, as well as stochastic Hessian-vector products (products of an independent unbiased estimator of the Hessian of the population objective with arbitrary vectors), with many such stochastic computations performed between rounds of communication.  We show that our method can reduce the number, and frequency, of required communication rounds, compared to existing methods without hurting performance, by proving convergence guarantees for quasi-self-concordant objectives (e.g., logistic regression), alongside empirical evidence."}}
{"id": "iBHiqlbFvLb", "cdate": 1621630282643, "mdate": null, "content": {"title": "An Even More Optimal Stochastic Optimization Algorithm: Minibatching and Interpolation Learning", "abstract": "We present and analyze an algorithm for optimizing smooth and convex or strongly convex objectives using minibatch stochastic gradient estimates. The algorithm is optimal with respect to its dependence on both the minibatch size and minimum expected loss simultaneously. This improves over the optimal method of Lan, which is insensitive to the minimum expected loss; over the optimistic acceleration of Cotter et al., which has suboptimal dependence on the minibatch size; and over the algorithm of Liu and Belkin, which is limited to least squares problems and is also similarly suboptimal.  Applied to interpolation learning, the improvement over Cotter et al.~and Liu and Belkin translates to a linear, rather than square-root, parallelization speedup.  "}}
{"id": "Byg9bxrtwS", "cdate": 1569439745966, "mdate": null, "content": {"title": "Kernel and Rich Regimes in Overparametrized Models", "abstract": "A recent line of work studies overparametrized neural networks in the \"kernel regime,\" i.e. when the network behaves during training as a kernelized linear predictor, and thus training with gradient descent has the effect of finding the minimum RKHS norm solution.  This stands in contrast to other studies which demonstrate how gradient descent on overparametrized multilayer networks can induce rich implicit biases that are not RKHS norms.  Building on an observation by Chizat and Bach, we show how the scale of the initialization controls the transition between the \"kernel\" (aka lazy) and \"rich\" (aka active) regimes and affects generalization properties in multilayer homogeneous models.  We provide a complete and detailed analysis for a simple two-layer model that already exhibits an interesting and meaningful transition between the kernel and rich regimes, and we demonstrate the transition for more complex matrix factorization models and multilayer non-linear networks. "}}
{"id": "Hy-TijbOWS", "cdate": 1546300800000, "mdate": null, "content": {"title": "Training Well-Generalizing Classifiers for Fairness Metrics and Other Data-Dependent Constraints", "abstract": "Classifiers can be trained with data-dependent constraints to satisfy fairness goals, reduce churn, achieve a targeted false positive rate, or other policy goals. We study the generalization perfor..."}}
{"id": "HJWygtW_ZB", "cdate": 1514764800000, "mdate": null, "content": {"title": "The Everlasting Database: Statistical Validity at a Fair Price", "abstract": "The problem of handling adaptivity in data analysis, intentional or not, permeates a variety of fields, including test-set overfitting in ML challenges and the accumulation of invalid scientific discoveries. We propose a mechanism for answering an arbitrarily long sequence of potentially adaptive statistical queries, by charging a price for each query and using the proceeds to collect additional samples. Crucially, we guarantee statistical validity without any assumptions on how the queries are generated. We also ensure with high probability that the cost for $M$ non-adaptive queries is $O(\\log M)$, while the cost to a potentially adaptive user who makes $M$ queries that do not depend on any others is $O(\\sqrt{M})$."}}
{"id": "BJ4ePv-_-H", "cdate": 1514764800000, "mdate": null, "content": {"title": "Graph Oracle Models, Lower Bounds, and Gaps for Parallel Stochastic Optimization", "abstract": "We suggest a general oracle-based framework that captures parallel stochastic optimization in different parallelization settings described by a dependency graph, and derive generic lower bounds in terms of this graph. We then use the framework and derive lower bounds to study several specific parallel optimization settings, including delayed updates and parallel processing with intermittent communication. We highlight gaps between lower and upper bounds on the oracle complexity, and cases where the ``natural'' algorithms are not known to be optimal."}}
