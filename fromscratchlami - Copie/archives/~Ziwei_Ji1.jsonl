{"id": "3LMI8CHDb0g", "cdate": 1652737470259, "mdate": null, "content": {"title": "Reproducibility in Optimization: Theoretical Framework and Limits", "abstract": " We initiate a formal study of reproducibility in optimization. We define a quantitative measure of reproducibility of optimization procedures in the face of noisy or error-prone operations such as inexact or stochastic gradient computations or inexact initialization. We then analyze several convex optimization settings of interest such as smooth, non-smooth, and strongly-convex objective functions and establish tight bounds on the limits of reproducibility in each setting. Our analysis reveals a fundamental trade-off between computation and reproducibility: more computation is necessary (and sufficient) for better reproducibility."}}
{"id": "vEZyTBRPP6o", "cdate": 1632875642413, "mdate": null, "content": {"title": "Actor-critic is implicitly biased towards high entropy optimal policies", "abstract": "We show that the simplest actor-critic method \u2014 a linear softmax policy updated with TD through interaction with a linear MDP, but featuring no explicit regularization or exploration \u2014 does not merely find an optimal policy, but moreover prefers high entropy optimal policies. To demonstrate the strength of this bias, the algorithm not only has no regularization, no projections, and no exploration like $\\epsilon$-greedy, but is moreover trained on a single trajectory with no resets. The key consequence of the high entropy bias is that uniform mixing assumptions on the MDP, which exist in some form in all prior work, can be dropped: the implicit regularization of the high entropy bias is enough to ensure that all chains mix and an optimal policy is reached with high probability. As auxiliary contributions, this work decouples concerns between the actor and critic by writing the actor update as an explicit mirror descent, provides tools to uniformly bound mixing times within KL balls of policy space, and provides a projection-free TD analysis with its own implicit bias which can be run from an unmixed starting distribution.\n"}}
{"id": "vPVTsuJtGky", "cdate": 1621630206404, "mdate": null, "content": {"title": "Early-stopped neural networks are consistent", "abstract": "This work studies the behavior of shallow ReLU networks trained with the logistic loss via gradient descent on binary classification data where the underlying data distribution is general, and the (optimal) Bayes risk is not necessarily zero.  In this setting, it is shown that gradient descent with early stopping achieves population risk arbitrarily close to optimal in terms of not just logistic and misclassification losses, but also in terms of calibration, meaning the sigmoid mapping of its outputs approximates the true underlying conditional distribution arbitrarily finely.  Moreover, the necessary iteration, sample, and architectural complexities of this analysis all scale naturally with a certain complexity measure of the true conditional model.  Lastly, while it is not shown that early stopping is necessary, it is shown that any classifier satisfying a basic local interpolation property is inconsistent.\n"}}
{"id": "rMKTq-ca0qu", "cdate": 1621630206404, "mdate": null, "content": {"title": "Early-stopped neural networks are consistent", "abstract": "This work studies the behavior of shallow ReLU networks trained with the logistic loss via gradient descent on binary classification data where the underlying data distribution is general, and the (optimal) Bayes risk is not necessarily zero.  In this setting, it is shown that gradient descent with early stopping achieves population risk arbitrarily close to optimal in terms of not just logistic and misclassification losses, but also in terms of calibration, meaning the sigmoid mapping of its outputs approximates the true underlying conditional distribution arbitrarily finely.  Moreover, the necessary iteration, sample, and architectural complexities of this analysis all scale naturally with a certain complexity measure of the true conditional model.  Lastly, while it is not shown that early stopping is necessary, it is shown that any classifier satisfying a basic local interpolation property is inconsistent.\n"}}
{"id": "EGdFhBzmAwB", "cdate": 1601308352774, "mdate": null, "content": {"title": "Generalization bounds via distillation", "abstract": "This paper theoretically investigates the following empirical phenomenon: given a high-complexity network with poor generalization bounds, one can distill it into a network with nearly identical predictions but low complexity and vastly smaller generalization bounds.  The main contribution is an analysis showing that the original network inherits this good generalization bound from its distillation, assuming the use of well-behaved data augmentation.  This bound is presented both in an abstract and in a concrete form, the latter complemented by a reduction technique to handle modern computation graphs featuring convolutional layers, fully-connected layers, and skip connections, to name a few.  To round out the story, a (looser) classical uniform convergence analysis of compression is also presented, as well as a variety of experiments on cifar and mnist demonstrating similar generalization performance between the original network and its distillation.  \n"}}
{"id": "HklQYxBKwS", "cdate": 1569439867091, "mdate": null, "content": {"title": "Neural tangent kernels, transportation mappings, and universal approximation", "abstract": "This paper establishes rates of universal approximation for the shallow neural tangent kernel (NTK): network weights are only allowed microscopic changes from random initialization, which entails that activations are mostly unchanged, and the network is nearly equivalent to its linearization. Concretely, the paper has two main contributions: a generic scheme to approximate functions with the NTK by sampling from transport mappings between the initial weights and their desired values, and the construction of transport mappings via Fourier transforms. Regarding the first contribution, the proof scheme provides another perspective on how the NTK regime arises from rescaling: redundancy in the weights due to resampling allows individual weights to be scaled down. Regarding the second contribution, the most notable transport mapping asserts that roughly $1 / \\delta^{10d}$ nodes are sufficient to approximate continuous functions, where $\\delta$ depends on the continuity properties of the target function. By contrast, nearly the same proof yields a bound of $1 / \\delta^{2d}$ for shallow ReLU networks; this gap suggests a tantalizing direction for future work, separating shallow ReLU networks and their linearization.\n"}}
{"id": "HygegyrYwH", "cdate": 1569439463698, "mdate": null, "content": {"title": "Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks", "abstract": "Recent theoretical work has guaranteed that overparameterized networks trained by gradient descent achieve arbitrarily low training error, and sometimes even low test error.\nThe required width, however, is always polynomial in at least one of the sample size $n$, the (inverse) target error $1/\\epsilon$, and the (inverse) failure probability $1/\\delta$. \nThis work shows that $\\widetilde{\\Theta}(1/\\epsilon)$ iterations of gradient descent with $\\widetilde{\\Omega}(1/\\epsilon^2)$ training examples on two-layer ReLU networks of any width exceeding $\\textrm{polylog}(n,1/\\epsilon,1/\\delta)$ suffice to achieve a test misclassification error of $\\epsilon$. \nWe also prove that stochastic gradient descent can achieve $\\epsilon$ test error with polylogarithmic width and $\\widetilde{\\Theta}(1/\\epsilon)$ samples. \nThe analysis relies upon the separation margin of the limiting kernel, which is guaranteed positive, can distinguish between true labels and random labels, and can give a tight sample-complexity analysis in the infinite-width setting."}}
{"id": "HJflg30qKX", "cdate": 1538087911980, "mdate": null, "content": {"title": "Gradient descent aligns the layers of deep linear networks", "abstract": "This paper establishes risk convergence and asymptotic weight matrix alignment --- a form of implicit regularization --- of gradient flow and gradient descent when applied to deep linear networks on linearly separable data. In more detail, for gradient flow applied to strictly decreasing loss functions (with similar results for gradient descent with particular decreasing step sizes):\n(i) the risk converges to 0;\n(ii) the normalized i-th weight matrix asymptotically equals its rank-1 approximation u_iv_i^T;\n(iii) these rank-1 matrices are aligned across layers, meaning |v_{i+1}^T u_i| -> 1.\nIn the case of the logistic loss (binary cross entropy), more can be said: the linear function induced by the network --- the product of its weight matrices --- converges to the same direction as the maximum margin solution. This last property was identified in prior work, but only under assumptions on gradient descent which here are implied by the alignment phenomenon."}}
