{"id": "cLcXUvH1qm", "cdate": 1676827072722, "mdate": null, "content": {"title": "Semi-supervised Learning of Partial Differential Operators and Dynamical Flows", "abstract": "The evolution of many dynamical systems is generically governed by nonlinear partial differential equations (PDEs), whose solution, in a simulation framework, requires vast amounts of computational resources. In this work, we present a novel method that combines a hyper-network solver with a Fourier Neural Operator architecture. Our method treats time and space separately and as a result, it successfully propagates initial conditions in continuous time steps by employing the general composition properties of the partial differential operators. Following previous works, supervision is provided at a specific time point. We test our method on various time evolution PDEs, including nonlinear fluid flows in one, two, or three spatial dimensions. The results show that the new method improves the learning accuracy at the time of the supervision point, and can interpolate the solutions to any intermediate time. Our implementation is available at \\href{https://github.com/rotmanmi/Semi-Supervised-Learning-of-Dynamical-Flows}{https://github.com/rotmanmi/Semi-Supervised-Learning-of-Dynamical-Flows}."}}
{"id": "dbH8I7SHsv", "cdate": 1672531200000, "mdate": 1681696582659, "content": {"title": "Energy Regularized RNNs for Solving Non-Stationary Bandit Problems", "abstract": "We consider a Multi-Armed Bandit problem in which the rewards are non-stationary and are dependent on past actions and potentially on past contexts. At the heart of our method, we employ a recurrent neural network, which models these sequences. In order to balance between exploration and exploitation, we present an energy minimization term that prevents the neural network from becoming too confident in support of a certain action. This term provably limits the gap between the maximal and minimal probabilities assigned by the network. In a diverse set of experiments, we demonstrate that our method is at least as effective as methods suggested to solve the sub-problem of Rotting Bandits, and can solve intuitive extensions of various benchmark problems. We share our implementation at https://github.com/rotmanmi/Energy-Regularized-RNN."}}
{"id": "GlEj4FJfV1", "cdate": 1672531200000, "mdate": 1681696582747, "content": {"title": "Gradient Adjusting Networks for Domain Inversion", "abstract": "StyleGAN2 was demonstrated to be a powerful image generation engine that supports semantic editing. However, in order to manipulate a real-world image, one first needs to be able to retrieve its corresponding latent representation in StyleGAN's latent space that is decoded to an image as close as possible to the desired image. For many real-world images, a latent representation does not exist, which necessitates the tuning of the generator network. We present a per-image optimization method that tunes a StyleGAN2 generator such that it achieves a local edit to the generator's weights, resulting in almost perfect inversion, while still allowing image editing, by keeping the rest of the mapping between an input latent representation tensor and an output image relatively intact. The method is based on a one-shot training of a set of shallow update networks (aka. Gradient Modification Modules) that modify the layers of the generator. After training the Gradient Modification Modules, a modified generator is obtained by a single application of these networks to the original parameters, and the previous editing capabilities of the generator are maintained. Our experiments show a sizable gap in performance over the current state of the art in this very active domain. Our code is available at \\url{https://github.com/sheffier/gani}."}}
{"id": "-i73LPWa3bD", "cdate": 1663849943365, "mdate": null, "content": {"title": "Semi-supervised learning of partial differential operators and dynamical flows", "abstract": "The evolution of dynamical systems is generically governed by nonlinear partial differential equations (PDEs), whose solution, in a simulation framework, requires vast amounts of computational resources. In this work, we present a novel method that combines a hyper-network solver with a Fourier Neural Operator architecture. Our method treats time and space separately and as a result, it successfully propagates initial conditions in continuous time steps by employing the general composition properties of the partial differential operators. Following previous works, supervision is provided at a specific time point. We test our method on various time evolution PDEs, including nonlinear fluid flows in one, two, or three spatial dimensions. The results show that the new method improves the learning accuracy at the time of the supervision point, and can interpolate the solutions to any intermediate time."}}
{"id": "YGuExB3tKy", "cdate": 1640995200000, "mdate": 1663769513018, "content": {"title": "Unsupervised Disentanglement with Tensor Product Representations on the Torus", "abstract": "The current methods for learning representations with auto-encoders almost exclusively employ vectors as the latent representations. In this work, we propose to employ a tensor product structure for this purpose. This way, the obtained representations are naturally disentangled. In contrast to the conventional variations methods, which are targeted toward normally distributed features, the latent space in our representation is distributed uniformly over a set of unit circles. We argue that the torus structure of the latent space captures the generative factors effectively. We employ recent tools for measuring unsupervised disentanglement, and in an extensive set of experiments demonstrate the advantage of our method in terms of disentanglement, completeness, and informativeness. The code for our proposed method is available at https://github.com/rotmanmi/Unsupervised-Disentanglement-Torus."}}
{"id": "4_ePJqln_hS", "cdate": 1640995200000, "mdate": 1663769513204, "content": {"title": "Semi-supervised Learning of Partial Differential Operators and Dynamical Flows", "abstract": "The evolution of dynamical systems is generically governed by nonlinear partial differential equations (PDEs), whose solution, in a simulation framework, requires vast amounts of computational resources. In this work, we present a novel method that combines a hyper-network solver with a Fourier Neural Operator architecture. Our method treats time and space separately. As a result, it successfully propagates initial conditions in continuous time steps by employing the general composition properties of the partial differential operators. Following previous work, supervision is provided at a specific time point. We test our method on various time evolution PDEs, including nonlinear fluid flows in one, two, and three spatial dimensions. The results show that the new method improves the learning accuracy at the time point of supervision point, and is able to interpolate and the solutions to any intermediate time."}}
{"id": "dKLoUvtnq0C", "cdate": 1632875492840, "mdate": null, "content": {"title": "Semi-supervised learning of partial differential operators and dynamical flows", "abstract": "The evolution of dynamical systems is generically governed by nonlinear partial differential equations (PDEs), whose solution, in a simulation framework, requires vast amounts of computational resources. For a growing number of specific cases, neural network-based solvers have been shown to provide comparable results to other numerical methods while utilizing fewer resources.\nIn this work, we present a novel method that combines a hyper-network solver with a Fourier Neural Operator architecture. Our method treats time and space separately. As a result, it successfully propagates initial conditions in discrete time steps by employing the general composition properties of the partial differential operators. Following previous work, supervision is provided at a specific time point. We test our method on various time evolution PDEs, including nonlinear fluid flows in one, two, and three spatial dimensions. The results show that the new method improves the learning accuracy at the time point of supervision point, and is also able to interpolate and extrapolate the solutions to arbitrary times."}}
{"id": "neqU3HWDgE", "cdate": 1632875492763, "mdate": null, "content": {"title": "Unsupervised Disentanglement with Tensor Product Representations on the Torus", "abstract": "The current methods for learning representations with auto-encoders almost exclusively employ vectors as the latent representations.  In this work, we propose to employ a tensor product structure for this purpose. This way, the obtained representations are naturally disentangled. In contrast to the conventional variations methods, which are targeted toward normally distributed features, the latent space in our representation is distributed uniformly over a set of unit circles. We argue that the torus structure of the latent space captures the generative factors effectively. We employ recent tools for measuring unsupervised disentanglement, and in an extensive set of experiments demonstrate the advantage of our method in terms of disentanglement, completeness, and informativeness. The code for our proposed method is available at https://github.com/rotmanmi/Unsupervised-Disentanglement-Torus."}}
{"id": "ws4m60xG3s", "cdate": 1609459200000, "mdate": 1663769513169, "content": {"title": "Shuffling Recurrent Neural Networks", "abstract": "We propose a novel recurrent neural network model, where the hidden state h\u209c is obtained by permuting the vector elements of the previous hidden state h\u209c\u208b\u2081 and adding the output of a learned function \u03b2(x\u209c) of the input x\u209c at time t. In our model, the prediction is given by a second learned function, which is applied to the hidden state s(h\u209c). The method is easy to implement, extremely efficient, and does not suffer from vanishing nor exploding gradients. In an extensive set of experiments, the method shows competitive results, in comparison to the leading literature baselines. We share our implementation at https://github.com/rotmanmi/SRNN."}}
{"id": "qQ4pGyzk1m", "cdate": 1609459200000, "mdate": 1663769513149, "content": {"title": "Effectively using unsupervised machine learning in next generation astronomical surveys", "abstract": ""}}
