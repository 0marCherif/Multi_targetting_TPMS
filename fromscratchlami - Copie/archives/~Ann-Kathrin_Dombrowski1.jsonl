{"id": "s8msJGU1Ef4", "cdate": 1632901648872, "mdate": 1632901648872, "content": {"title": "Fairwashing explanations with off-manifold detergent", "abstract": " Explanation methods promise to make black-box classifiers more transparent. As a result, it is hoped that they can act as proof for a sensible, fair and trustworthy decision-making process of the algorithm and thereby increase its acceptance by the end-users. In this paper, we show both theoretically and experimentally that these hopes are presently unfounded. Specifically, we show that, for any classifier g, one can always construct another classifier g~ which has the same behavior on the data (same train, validation, and test error) but has arbitrarily manipulated explanation maps. We derive this statement theoretically using differential geometry and demonstrate it experimentally for various explanation methods, architectures, and datasets. Motivated by our theoretical insights, we then propose a modification of existing explanation methods which makes them significantly more robust."}}
{"id": "FI4p3JtrOG", "cdate": 1632901220868, "mdate": 1632901220868, "content": {"title": "Towards robust explanations for deep neural networks", "abstract": "Explanation methods shed light on the decision process of black-box classifiers such as deep neural networks. But their usefulness can be compromised because they are susceptible to manipulations. With this work, we aim to enhance the resilience of explanations. We develop a unified theoretical framework for deriving bounds on the maximal manipulability of a model. Based on these theoretical insights, we present three different techniques to boost robustness against manipulation: training with weight decay, smoothing activation functions, and minimizing the Hessian of the network. Our experimental results confirm the effectiveness of these approaches."}}
{"id": "DDqQtL50cdZ", "cdate": 1632901115147, "mdate": 1632901115147, "content": {"title": "Explanations can be manipulated and geometry is to blame", "abstract": "Explanation methods aim to make neural networks more trustworthy and interpretable. In this paper, we demonstrate a property of explanation methods which is disconcerting for both of these purposes. Namely, we show that explanations can be manipulated arbitrarily by applying visually hardly perceptible perturbations to the input that keep the network\u2019s output approximately constant. We establish theoretically that this phenomenon can be related to certain geometrical properties of neural networks.  This allows us to derive an upper bound on the susceptibility of explanations to manipulations.  Based on this result, we propose effective mechanisms to enhance the robustness of explanations."}}
{"id": "ZBR9EpEl6G4", "cdate": 1622637628064, "mdate": null, "content": {"title": "Diffeomorphic Explanations with Normalizing Flows", "abstract": "Normalizing flows are diffeomorphisms which are parameterized by neural networks. As a result, they can induce coordinate transformations in the tangent space of the data manifold. In this work, we demonstrate that such transformations can be used to generate interpretable explanations for decisions of neural networks. More specifically, we perform gradient ascent in the base space of the flow to generate counterfactuals which are classified with great confidence as a specified target class. We analyze this generation process theoretically using Riemannian differential geometry and establish a rigorous theoretical connection between gradient ascent on the data manifold and in the base space of the flow."}}
