{"id": "-_I3i2orAV", "cdate": 1652737745196, "mdate": null, "content": {"title": "Look where you look! Saliency-guided Q-networks for generalization in visual Reinforcement Learning", "abstract": "Deep reinforcement learning policies, despite their outstanding efficiency in simulated visual control tasks, have shown disappointing ability to generalize across disturbances in the input training images. \nChanges in image statistics or distracting background elements are pitfalls that prevent generalization and real-world applicability of such control policies.\nWe elaborate on the intuition that a good visual policy should be able to identify which pixels are important for its decision, and preserve this identification of important sources of information across images. \nThis implies that training of a policy with small generalization gap should focus on such important pixels and ignore the others. \nThis leads to the introduction of saliency-guided Q-networks (SGQN), a generic method for visual reinforcement learning, that is compatible with any value function learning method. \nSGQN vastly improves the generalization capability of Soft Actor-Critic agents and outperforms existing state-of-the-art methods on the Deepmind Control Generalization benchmark, setting a new reference in terms of training efficiency, generalization gap, and policy interpretability."}}
{"id": "xqxCpOn8DIE", "cdate": 1640995200000, "mdate": 1680858279621, "content": {"title": "Look where you look! Saliency-guided Q-networks for visual RL tasks", "abstract": ""}}
{"id": "pyhlNh0iQzQ", "cdate": 1640995200000, "mdate": 1680858279620, "content": {"title": "Local Feature Swapping for Generalization in Reinforcement Learning", "abstract": ""}}
{"id": "iZn-V2bDJ4", "cdate": 1640995200000, "mdate": 1680858279471, "content": {"title": "Local Feature Swapping for Generalization in Reinforcement Learning", "abstract": ""}}
{"id": "WxLxCUYyLGS", "cdate": 1640995200000, "mdate": 1682322527854, "content": {"title": "Autonomous Drone Interception with Reinforcement Learning", "abstract": ""}}
{"id": "Sq0-tgDyHe4", "cdate": 1632875618794, "mdate": null, "content": {"title": "Local Feature Swapping for Generalization in Reinforcement Learning", "abstract": "Over the past few years, the acceleration of computing resources and research in Deep Learning has led to significant practical successes in a range of tasks, including in particular in computer vision. Building on these advances, reinforcement learning has also seen a leap forward with the emergence of agents capable of making decisions directly from visual observations. Despite these successes, the over-parametrization of neural architectures leads to memorization of the data used during training and thus to a lack of generalization.\nReinforcement learning agents based on visual inputs also suffer from this phenomenon by erroneously correlating rewards with unrelated visual features such as background elements. To alleviate this problem, we introduce a new regularization layer consisting of channel-consistent local permutations (CLOP) of the feature maps. The proposed permutations induce robustness to spatial correlations and help prevent overfitting behaviors in RL. We demonstrate, on the OpenAI Procgen Benchmark, that RL agents trained with the CLOP layer exhibit robustness to visual changes and better generalization properties than agents trained using other state-of-the-art regularization techniques."}}
{"id": "urrcVI-_jRm", "cdate": 1621629919875, "mdate": null, "content": {"title": "Numerical influence of ReLU\u2019(0) on backpropagation", "abstract": "In theory, the choice of ReLU(0) in [0, 1] for a neural network has a negligible influence both on backpropagation and training. Yet, in the real world, 32 bits default precision combined with the size of deep learning problems makes it a hyperparameter of training methods. We investigate the importance of the value of ReLU'(0) for several precision levels (16, 32, 64 bits), on various networks (fully connected, VGG, ResNet) and datasets (MNIST, CIFAR10, SVHN, ImageNet). We observe considerable variations of backpropagation outputs which occur around half of the time in 32 bits precision. The effect disappears with double precision, while it is systematic at 16 bits. For vanilla SGD training, the choice ReLU'(0) = 0 seems to be the most efficient. For our experiments on ImageNet the gain in test accuracy over ReLU'(0) = 1 was more than 10 points (two runs). We also evidence that reconditioning approaches as batch-norm or ADAM tend to buffer the influence of ReLU'(0)\u2019s value. Overall, the message we convey is that algorithmic differentiation of nonsmooth problems potentially hides parameters that could be tuned advantageously."}}
{"id": "YS-ZrtblwX", "cdate": 1609459200000, "mdate": 1680858279590, "content": {"title": "Disentanglement by Cyclic Reconstruction", "abstract": ""}}
{"id": "1OCwJdJSnSA", "cdate": 1601308102671, "mdate": null, "content": {"title": "Disentangled cyclic reconstruction for domain adaptation", "abstract": "The domain adaptation problem involves learning a unique classification or regression model capable of performing on both a source and a target domain. Although the labels for the source data are available during training, the labels in the target domain are unknown. An effective way to tackle this problem lies in extracting insightful features invariant to the source and target domains. In this work, we propose splitting the information for each domain into a task-related representation and its complimentary context representation. We propose an original method to disentangle these two representations in the single-domain supervised case. We then adapt this method to the unsupervised domain adaptation problem. In particular, our method allows disentanglement in the target domain, despite the absence of training labels. This enables the isolation of task-specific information from both domains and a projection into a common representation. The task-specific representation allows efficient transfer of knowledge acquired from the source domain to the target domain. We validate the proposed method on several classical domain adaptation benchmarks and illustrate the benefits of disentanglement for domain adaptation."}}
