{"id": "TiGIyQuYvz", "cdate": 1696620731917, "mdate": 1696620731917, "content": {"title": "RARR: Researching and Revising What Language Models Say, Using Language Models", "abstract": "Language models (LMs) now excel at many tasks such as question answering, reasoning, and dialog. However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence. To enable attribution while still preserving all the powerful advantages of recent generation models, we propose RARR (Retrofit Attribution using Research and Revision), a system that 1) automatically finds attribution for the output of any text generation model, and 2) post-edits the output to fix unsupported content while preserving the original output as much as possible. When applied to the output of several state-of-the-art LMs on a diverse set of generation tasks, we find that RARR significantly improves attribution while otherwise preserving the original input to a much greater degree than previously explored edit models. Furthermore, the implementation of RARR requires only a handful of training examples, a large language model, and standard web search."}}
{"id": "4Go-I0jf4a", "cdate": 1654124927150, "mdate": null, "content": {"title": "Dialog Inpainting: Turning Documents into Dialogs", "abstract": "Many important questions (e.g. \"How to eat healthier?\") require conversation to establish context and explore in depth. However, conversational question answering (ConvQA) systems have long been stymied by scarce training data that is expensive to collect. To address this problem, we propose a new technique for synthetically generating diverse and high-quality dialog data: dialog inpainting. Our approach takes the text of any document and transforms it into a two-person dialog between the writer and an imagined reader: we treat sentences from the article as utterances spoken by the writer, and then use a dialog inpainter to predict what the imagined reader asked or said in between each of the writer's utterances. By applying this approach to passages from Wikipedia and the web, we produce WikiDialog and WebDialog, two datasets totalling 19 million diverse information-seeking dialogs---1,000x larger than the largest existing ConvQA dataset. Furthermore, human raters judge the answer adequacy and conversationality of WikiDialog to be as good or better than existing manually-collected datasets. Using our inpainted data to pre-train ConvQA retrieval systems, we significantly advance state-of-the-art across three benchmarks, yielding up to 40% relative gains on standard evaluation metrics."}}
{"id": "-GiW96Rz8Rw", "cdate": 1577836800000, "mdate": 1635526354312, "content": {"title": "Conformal retrofitting via Riemannian manifolds: distilling task-specific graphs into pretrained embeddings", "abstract": "Pretrained (language) embeddings are versatile, task-agnostic feature representations of entities, like words, that are central to many machine learning applications. These representations can be enriched through retrofitting, a class of methods that incorporate task-specific domain knowledge encoded as a graph over a subset of these entities. However, existing retrofitting algorithms face two limitations: they overfit the observed graph by failing to represent relationships with missing entities; and they underfit the observed graph by only learning embeddings in Euclidean manifolds, which cannot faithfully represent even simple tree-structured or cyclic graphs. We address these problems with two key contributions: (i) we propose a novel regularizer, a conformality regularizer, that preserves local geometry from the pretrained embeddings---enabling generalization to missing entities and (ii) a new Riemannian feedforward layer that learns to map pre-trained embeddings onto a non-Euclidean manifold that can better represent the entire graph. Through experiments on WordNet, we demonstrate that the conformality regularizer prevents even existing (Euclidean-only) methods from overfitting on link prediction for missing entities, and---together with the Riemannian feedforward layer---learns non-Euclidean embeddings that outperform them."}}
{"id": "PUrh9Eb1RVB", "cdate": 1546300800000, "mdate": 1635526354325, "content": {"title": "Mimic and Rephrase: Reflective Listening in Open-Ended Dialogue", "abstract": "Justin Dieter, Tian Wang, Arun Tejasvi Chaganty, Gabor Angeli, Angel X. Chang. Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL). 2019."}}
{"id": "r1W0M2x_-r", "cdate": 1514764800000, "mdate": null, "content": {"title": "The price of debiasing automatic metrics in natural language evalaution", "abstract": "For evaluating generation systems, automatic metrics such as BLEU cost nothing to run but have been shown to correlate poorly with human judgment, leading to systematic bias against certain model improvements. On the other hand, averaging human judgments, the unbiased gold standard, is often too expensive. In this paper, we use control variates to combine automatic metrics with human evaluation to obtain an unbiased estimator with lower cost than human evaluation alone. In practice, however, we obtain only a 7-13% cost reduction on evaluating summarization and open-response question answering systems. We then prove that our estimator is optimal: there is no unbiased estimator with lower cost. Our theory further highlights the two fundamental bottlenecks---the automatic metric and the prompt shown to human evaluators---both of which need to be improved to obtain greater cost savings."}}
{"id": "Hk-XszzdWS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Textual Analogy Parsing: What's Shared and What's Compared among Analogous Facts", "abstract": "Author Summary How do neurons learn to extract information from their inputs, and perform meaningful computations? Neurons receive inputs as continuous streams of action potentials or &ldquo;spikes&rdquo; that arrive at thousands of synapses. The strength of these synapses - the synaptic weight - undergoes constant modification. It has been demonstrated in numerous experiments that this modification depends on the temporal order of spikes in the pre- and postsynaptic neuron, a rule known as STDP, but it has remained unclear, how this contributes to higher level functions in neural network architectures. In this paper we show that STDP induces in a commonly found connectivity motif in the cortex - a winner-take-all (WTA) network - autonomous, self-organized learning of probabilistic models of the input. The resulting function of the neural circuit is Bayesian computation on the input spike trains. Such unsupervised learning has previously been studied extensively on an abstract, algorithmical level. We show that STDP approximates one of the most powerful learning methods in machine learning, Expectation-Maximization (EM). In a series of computer simulations we demonstrate that this enables STDP in WTA circuits to solve complex learning tasks, reaching a performance level that surpasses previous uses of spiking neural networks."}}
{"id": "r14F0ZGuWS", "cdate": 1483228800000, "mdate": null, "content": {"title": "Importance sampling for unbiased on-demand evaluation of knowledge base population", "abstract": "The rapid developments in sensor technology and mobile devices bring a flourish of social images, and large-scale social images have attracted increasing attention to researchers. Existing approaches generally rely on recognizing object instances individually with geo-tags, visual patterns, etc. However, the social image represents a web of interconnected relations; these relations between entities carry semantic meaning and help a viewer differentiate between instances of a substance. This article forms the perspective of the spatial relationship to exploring the joint learning of social images. Precisely, the model consists of three parts: (a) a module for deep semantic understanding of images based on residual network (ResNet); (b) a deep semantic analysis module of text beyond traditional word bag methods; (c) a joint reasoning module from which the text weights obtained using image features on self-attention and a novel tree-based clustering algorithm. The experimental results demonstrate the effectiveness of using Flickr30k and Microsoft COCO datasets. Meanwhile, our method considers spatial relations while matching."}}
{"id": "d5ACrOwJgT6", "cdate": 1483228800000, "mdate": null, "content": {"title": "Stanford at TAC KBP 2017: Building a Trilingual Relational Knowledge Graph", "abstract": ""}}
{"id": "JyaHBS7k-Q", "cdate": 1483228800000, "mdate": null, "content": {"title": "TinkerBell: Cross-lingual Cold-Start Knowledge Base Construction", "abstract": ""}}
{"id": "OZ1X6STyHxz", "cdate": 1451606400000, "mdate": null, "content": {"title": "Stanford at TAC KBP 2016: Sealing Pipeline Leaks and Understanding Chinese", "abstract": ""}}
