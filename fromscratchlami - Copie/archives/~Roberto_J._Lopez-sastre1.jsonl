{"id": "z_AEvlLQYVy", "cdate": 1577836800000, "mdate": 1623749689949, "content": {"title": "Learning to Learn Face-PAD: a lifelong learning approach", "abstract": "A face presentation attack detection (face-PAD) system is in charge of determining whether a face corresponds to a presentation attack or not. The vast majority of proposed solutions consider a static scenario, where models are trained and evaluated in datasets where all types of attacks and conditions are known beforehand. However, in a real-world scenario, the situation is very different. There, for instance, the types of attacks change over time, with new impersonation situations appearing for which little training data is available. In this paper we propose to tackle these problems presenting for the first time a con-tinuallearning framework for PAD. We introduce a continual meta-learning PAD solution that can be trained on new attack scenarios, following the continual few-shot learning paradigm, where the model uses only a small number of training samples. We also provide a thorough experimental evaluation using the GRAD-GPAD benchmark. Our results confirm the benefits of applying a continual meta-learning model to the real-world PAD scenario. Interestingly, the accuracy of our solution, which is continuously trained, where data from new attacks arrive sequentially, is capable of recovering the accuracy achieved by a traditional solution that has all the data from all possible attacks from the beginning. In addition, our experiments show that when these traditional PAD solutions are trained on new attacks, using a standard fine-tuning process, they suffer from catastrophic forgetting while our model does not."}}
{"id": "yDui4FqFt5K", "cdate": 1577836800000, "mdate": 1623749689855, "content": {"title": "Rethinking Online Action Detection in Untrimmed Videos: A Novel Online Evaluation Protocol", "abstract": "The Online Action Detection (OAD) problem needs to be revisited. Unlike traditional offline action detection approaches, where the evaluation metrics are clear and well established, in the OAD setting we find very few works and no consensus on the evaluation protocols to be used. In this work we propose to rethink the OAD scenario, clearly defining the problem itself and the main characteristics that the models which are considered online must comply with. We also introduce a novel metric: the Instantaneous Accuracy (IA). This new metric exhibits an online nature and solves most of the limitations of the previous metrics. We conduct a thorough experimental evaluation on 3 challenging datasets, where the performance of various baseline methods is compared to that of the state-of-the-art. Our results confirm the problems of the previous evaluation protocols, and suggest that an IA-based protocol is more adequate to the online scenario. The baselines models and a development kit with the novel evaluation protocol will be made publicly available."}}
{"id": "rz1LFYvOwEf", "cdate": 1577836800000, "mdate": 1623749689959, "content": {"title": "Rethinking Online Action Detection in Untrimmed Videos: A Novel Online Evaluation Protocol", "abstract": "The Online Action Detection (OAD) problem needs to be revisited. Unlike traditional offline action detection approaches, where the evaluation metrics are clear and well established, in the OAD setting we find very few works and no consensus on the evaluation protocols to be used. In this work we propose to rethink the OAD scenario, clearly defining the problem itself and the main characteristics that the models which are considered online must comply with. We also introduce a novel metric: the Instantaneous Accuracy ($IA$). This new metric exhibits an \\emph{online} nature and solves most of the limitations of the previous metrics. We conduct a thorough experimental evaluation on 3 challenging datasets, where the performance of various baseline methods is compared to that of the state-of-the-art. Our results confirm the problems of the previous evaluation protocols, and suggest that an IA-based protocol is more adequate to the online scenario. The baselines models and a development kit with the novel evaluation protocol are publicly available: https://github.com/gramuah/ia."}}
{"id": "ht4-NVTYrOF", "cdate": 1577836800000, "mdate": 1623749689954, "content": {"title": "The Instantaneous Accuracy: a Novel Metric for the Problem of Online Human Behaviour Recognition in Untrimmed Videos", "abstract": "The problem of Online Human Behaviour Recognition in untrimmed videos, aka Online Action Detection (OAD), needs to be revisited. Unlike traditional offline action detection approaches, where the evaluation metrics are clear and well established, in the OAD setting we find few works and no consensus on the evaluation protocols to be used. In this paper we introduce a novel online metric, the Instantaneous Accuracy ($IA$), that exhibits an \\emph{online} nature, solving most of the limitations of the previous (offline) metrics. We conduct a thorough experimental evaluation on TVSeries dataset, comparing the performance of various baseline methods to the state of the art. Our results confirm the problems of previous evaluation protocols, and suggest that an IA-based protocol is more adequate to the online scenario for human behaviour understanding. Code of the metric available https://github.com/gramuah/ia"}}
{"id": "crrMN032dJ-", "cdate": 1577836800000, "mdate": 1623749689858, "content": {"title": "Unsupervised Action Proposals Using Support Vector Classifiers for Online Video Processing", "abstract": "In this work, we introduce an intelligent video sensor for the problem of Action Proposals (AP). AP consists of localizing temporal segments in untrimmed videos that are likely to contain actions. Solving this problem can accelerate several video action understanding tasks, such as detection, retrieval, or indexing. All previous AP approaches are supervised and offline, i.e., they need both the temporal annotations of the datasets during training and access to the whole video to effectively cast the proposals. We propose here a new approach which, unlike the rest of the state-of-the-art models, is unsupervised. This implies that we do not allow it to see any labeled data during learning nor to work with any pre-trained feature on the used dataset. Moreover, our approach also operates in an online manner, which can be beneficial for many real-world applications where the video has to be processed as soon as it arrives at the sensor, e.g., robotics or video monitoring. The core of our method is based on a Support Vector Classifier (SVC) module which produces candidate segments for AP by distinguishing between sets of contiguous video frames. We further propose a mechanism to refine and filter those candidate segments. This filter optimizes a learning-to-rank formulation over the dynamics of the segments. An extensive experimental evaluation is conducted on Thumos&rsquo;14 and ActivityNet datasets, and, to the best of our knowledge, this work supposes the first unsupervised approach on these main AP benchmarks. Finally, we also provide a thorough comparison to the current state-of-the-art supervised AP approaches. We achieve 41% and 59% of the performance of the best-supervised model on ActivityNet and Thumos&rsquo;14, respectively, confirming our unsupervised solution as a correct option to tackle the AP problem. The code to reproduce all our results will be publicly released upon acceptance of the paper."}}
{"id": "yJhbbAJ2asW", "cdate": 1546300800000, "mdate": 1623749689961, "content": {"title": "On-Board Correction of Systematic Odometry Errors in Differential Robots", "abstract": "An easier method for the calibration of differential drive robots is presented. Most calibration is done on-board and it is not necessary to spend too much time taking note of the robot&#x2019;s position. The calibration method does not need a large free space to perform the tests. The bigger space is merely in a straight line, which is easy to find. The results with the method presented are compared with those from UMB for reference, and they show very little deviation while the proposed calibration is much simpler."}}
{"id": "L4FIcEhtTLm", "cdate": 1546300800000, "mdate": 1623749689957, "content": {"title": "Unsupervised learning from videos using temporal coherency deep networks", "abstract": "Highlights \u2022 Results for the action and scene discovery problems are presented. \u2022 Our models are learned in a fully unsupervised fashion. \u2022 Evaluation of the use of unlabeled video as a prior for supervised recognition. \u2022 Our unsupervised models can surpass a supervised pre-training approach. Abstract In this work we address the challenging problem of unsupervised learning from videos. Existing methods utilize the spatio-temporal continuity in contiguous video frames as regularization for the learning process. Typically, this temporal coherence of close frames is used as a free form of annotation, encouraging the learned representations to exhibit small differences between these frames. But this type of approach fails to capture the dissimilarity between videos with different content, hence learning less discriminative features. We here propose two Siamese architectures for Convolutional Neural Networks, and their corresponding novel loss functions, to learn from unlabeled videos, which jointly exploit the local temporal coherence between contiguous frames, and a global discriminative margin used to separate representations of different videos. An extensive experimental evaluation is presented, where we validate the proposed models on various tasks. First, we show how the learned features can be used to discover actions and scenes in video collections. Second, we show the benefits of such an unsupervised learning from just unlabeled videos, which can be directly used as a prior for the supervised recognition tasks of actions and objects in images, where our results further show that our features can even surpass a traditional and heavily supervised pre-training plus fine-tuning strategy."}}
{"id": "KU4c-RIn1Pz", "cdate": 1546300800000, "mdate": 1623749689973, "content": {"title": "The Instantaneous Accuracy: a Novel Metric for the Problem of Online Human Behaviour Recognition in Untrimmed Videos", "abstract": "The problem of Online Human Behavior Recognition in untrimmed videos, aka Online Action Detection (OAD), needs to be revisited. Unlike traditional off-line action detection approaches, where the evaluation metrics are clear and well established, in the OAD setting we find few works and no consensus on evaluation protocols to be used. In this paper we introduce a novel online metric, the Instantaneous Accuracy (IA), that exhibits an online nature, solving most of the limitations of the previous (off-line) metrics. We conduct a thorough experimental evaluation on the TVSeries dataset, comparing the performance of various baseline methods with the state of the art. Our results confirm the problems of the previous evaluation protocols, and suggest that an IA-based protocol is more adequate to the online scenario for human behaviour understanding."}}
{"id": "KB8jr0GgjIS", "cdate": 1546300800000, "mdate": 1623749689985, "content": {"title": "Boosting Multi-Vehicle Tracking with a Joint Object Detection and Viewpoint Estimation Sensor", "abstract": "In this work, we address the problem of multi-vehicle detection and tracking for traffic monitoring applications. We preset a novel intelligent visual sensor for tracking-by-detection with simultaneous pose estimation. Essentially, we adapt an Extended Kalman Filter (EKF) to work not only with the detections of the vehicles but also with their estimated coarse viewpoints, directly obtained with the vision sensor. We show that enhancing the tracking with observations of the vehicle pose, results in a better estimation of the vehicles trajectories. For the simultaneous object detection and viewpoint estimation task, we present and evaluate two independent solutions. One is based on a fast GPU implementation of a Histogram of Oriented Gradients (HOG) detector with Support Vector Machines (SVMs). For the second, we adequately modify and train the Faster R-CNN deep learning model, in order to recover from it not only the object localization but also an estimation of its pose. Finally, we publicly release a challenging dataset, the GRAM Road Traffic Monitoring (GRAM-RTM), which has been especially designed for evaluating multi-vehicle tracking approaches within the context of traffic monitoring applications. It comprises more than 700 unique vehicles annotated across more than 40.300 frames of three videos. We expect the GRAM-RTM becomes a benchmark in vehicle detection and tracking, providing the computer vision and intelligent transportation systems communities with a standard set of images, annotations and evaluation procedures for multi-vehicle tracking. We present a thorough experimental evaluation of our approaches with the GRAM-RTM, which will be useful for establishing further comparisons. The results obtained confirm that the simultaneous integration of vehicle localizations and pose estimations as observations in an EKF, improves the tracking results."}}
{"id": "F1m58IQwGH3", "cdate": 1546300800000, "mdate": 1623749689880, "content": {"title": "Generalized Presentation Attack Detection: a face anti-spoofing evaluation proposal", "abstract": "Over the past few years, Presentation Attack Detection (PAD) has become a fundamental part of facial recognition systems. Although much effort has been devoted to anti-spoofing research, generalization in real scenarios remains a challenge. In this paper we present a new open-source evaluation framework to study the generalization capacity of face PAD methods, coined here as face-GPAD. This framework facilitates the creation of new protocols focused on the generalization problem establishing fair procedures of evaluation and comparison between PAD solutions. We also introduce a large aggregated and categorized dataset to address the problem of incompatibility between publicly available datasets. Finally, we propose a benchmark adding two novel evaluation protocols: one for measuring the effect introduced by the variations in face resolution, and the second for evaluating the influence of adversarial operating conditions."}}
