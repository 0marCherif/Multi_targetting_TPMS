{"id": "AeIzVxdJgeb", "cdate": 1591922546097, "mdate": null, "content": {"title": "Task Agnostic Continual Learning via Meta Learning", "abstract": "Most continual learning approaches implicitly assume that there exists a multi-task solution for the sequence of tasks. In this work, we motivate and discuss realistic scenarios when this assumption does not hold. We argue that the traditional metric of zero-shot remembering is not appropriate in such settings, and, inspired by the meta-learning literature, we focus on the speed of remembering previous tasks.  A natural approach to deal with this case is to separate the concerns into what task is currently being solved and how the task should be solved. At each step, the What algorithm performs task inference, which allows our framework to work in absence of task boundaries. The How algorithm is conditioned on the inferred task, allowing for task-specific behaviour, hence relaxing the assumption of a multi-task solution.  From the perspective of meta-learning, our framework is able to deal with a sequential presentation of tasks, rather than having access to the distribution of all tasks. We empirically validate the effectiveness of our approach and discuss variations of the proposed algorithm."}}
{"id": "FOyk6uDpNS7", "cdate": 1591922545844, "mdate": null, "content": {"title": "Continual Learning from the Perspective of Compression", "abstract": "Connectionist models such as neural networks suffer from catastrophic forgetting. In this work, we study this problem from the perspective of information theory and define forgetting as the increase of description lengths of previous data when they are compressed with a sequentially learned model. In addition, we show that continual learning approaches based on variational posterior approximation and generative replay can be considered as approximations to two prequential coding methods in compression, namely, the Bayesian mixture code and maximum likelihood (ML) plug-in code. We compare these approaches in terms of both compression and forgetting and empirically study the reasons that limit the performance of continual learning methods based on variational posterior approximation. To address these limitations, we propose a new continual learning method that combines ML plug-in and Bayesian mixture codes."}}
{"id": "B1al7jg0b", "cdate": 1518730180080, "mdate": null, "content": {"title": "Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation", "abstract": "Catastrophic interference has been a major roadblock in the research of continual learning. Here we propose a variant of the back-propagation algorithm, \"Conceptor-Aided Backprop\" (CAB), in which gradients are shielded by conceptors against degradation of previously learned tasks. Conceptors have their origin in reservoir computing, where they have been previously shown to overcome catastrophic forgetting. CAB extends these results to deep feedforward networks. On the disjoint and permuted MNIST tasks, CAB outperforms two other methods for coping with catastrophic interference that have recently been proposed."}}
