{"id": "3WW4WXieYz", "cdate": 1675048877048, "mdate": 1675048877048, "content": {"title": "Towards Enabling Learnware to Handle Heterogeneous Feature Spaces", "abstract": "The learnware paradigm was recently proposed by Zhou (2016) with the wish of developing the learnware market to help users build models more efciently by reusing existing well-performed models rather than starting from scratch. Specifcally, a learnware in the learnware market is a well-performed pre-trained model with a specifcation describing its specialty and utility, and the market identifes helpful learnware(s) for the user\u2019s task based on the specifcation. Recent studies have attempted to realize a homogeneous  prototype learnware market initially through Reduced Kernel Mean Embedding (RKME) specifcation, which requires all models in the market to share the same feature space. However, this limits the application scope of the learnware paradigm because various pre-trained models are often obtained from diferent feature spaces in real-world scenarios. In this paper, we make the frst attempt to enable the learnware to handle heterogeneous feature spaces. We propose a more powerful specifcation to manage heterogeneous learnwares by integrating subspace learning in the specifcation design, along with a practical approach for identifying and reusing helpful learnwares for the user\u2019s task. Empirical studies on both synthetic data and real-world tasks validate the efcacy of our approach."}}
{"id": "ekzhf_MkB5Q", "cdate": 1672531200000, "mdate": 1696081611049, "content": {"title": "Handling Learnwares Developed from Heterogeneous Feature Spaces without Auxiliary Data", "abstract": "The learnware paradigm proposed by Zhou [2016] devotes to constructing a market of numerous well-performed models, enabling users to solve problems by reusing existing efforts rather than starting from scratch. A learnware comprises a trained model and the specification which enables the model to be adequately identified according to the user's requirement. Previous studies concentrated on the homogeneous case where models share the same feature space based on Reduced Kernel Mean Embedding (RKME) specification. However, in real-world scenarios, models are typically constructed from different feature spaces. If such a scenario can be handled by the market, all models built for a particular task even with different feature spaces can be identified and reused for a new user task. Generally, this problem would be easier if there were additional auxiliary data connecting different feature spaces, however, obtaining such data in reality is challenging. In this paper, we present a general framework for accommodating heterogeneous learnwares without requiring additional auxiliary data. The key idea is to utilize the submitted RKME specifications to establish the relationship between different feature spaces. Additionally, we give a matrix factorization-based implementation and propose the overall procedure for constructing and exploiting the heterogeneous learnware market. Experiments on real-world tasks validate the efficacy of our method."}}
{"id": "VDtkN_FzXx", "cdate": 1672531200000, "mdate": 1705501260956, "content": {"title": "Identifying Helpful Learnwares Without Examining the Whole Market", "abstract": "The learnware paradigm aims to construct a market of numerous well-performing machine learning models, which enables users to leverage these models to accomplish specific tasks without having to build models from scratch. Each learnware in the market is a model associated with a specification, representing the model\u2019s utility and enabling it to be identified according to future users\u2019 requirements. In the learnware paradigm, due to the vast and ever-increasing number of models in the market, a significant challenge is to identify helpful learnwares efficiently for a specific user task without leaking data privacy. However, existing identification methods require examining the whole market, which is computationally unaffordable in a large market. In this paper, we propose a new framework for identifying helpful learnwares without examining the whole market. Specifically, using the Reduced Kernel Mean Embedding (RKME) specification, we derive a novel learnware scoring criterion for assessing the helpfulness of a learnware, based on which we design an anchor-based framework to identify helpful learnwares by examining only a small portion of learnwares in the market. Theoretical analyses are provided for both the criterion and the anchor-based method. Empirical studies on market containing thousands of learnwares from real-world datasets confirm the effectiveness of our proposed approach."}}
{"id": "Jd2RfKd4Mjz", "cdate": 1652737724129, "mdate": null, "content": {"title": "Real-Valued Backpropagation is Unsuitable for Complex-Valued Neural Networks", "abstract": "Recently complex-valued neural networks have received increasing attention due to successful applications in various tasks and the potential advantages of better theoretical properties and richer representational capacity. However, the training dynamics of complex networks compared to real networks remains an open problem. In this paper, we investigate the dynamics of deep complex networks during real-valued backpropagation in the infinite-width limit via neural tangent kernel (NTK). We first extend the Tensor Program to the complex domain, to show that the dynamics of any basic complex network architecture is governed by its NTK under real-valued backpropagation. Then we propose a way to investigate the comparison of training dynamics between complex and real networks by studying their NTKs. As a result, we surprisingly prove that for most complex activation functions, the commonly used real-valued backpropagation reduces the training dynamics of complex networks to that of ordinary real networks as the widths tend to infinity, thus eliminating the characteristics of complex-valued neural networks. Finally, the experiments validate our theoretical findings numerically."}}
{"id": "yysmlK7Yu_e", "cdate": 1640995200000, "mdate": 1683901478900, "content": {"title": "Real-Valued Backpropagation is Unsuitable for Complex-Valued Neural Networks", "abstract": "Recently complex-valued neural networks have received increasing attention due to successful applications in various tasks and the potential advantages of better theoretical properties and richer representational capacity. However, the training dynamics of complex networks compared to real networks remains an open problem. In this paper, we investigate the dynamics of deep complex networks during real-valued backpropagation in the infinite-width limit via neural tangent kernel (NTK). We first extend the Tensor Program to the complex domain, to show that the dynamics of any basic complex network architecture is governed by its NTK under real-valued backpropagation. Then we propose a way to investigate the comparison of training dynamics between complex and real networks by studying their NTKs. As a result, we surprisingly prove that for most complex activation functions, the commonly used real-valued backpropagation reduces the training dynamics of complex networks to that of ordinary real networks as the widths tend to infinity, thus eliminating the characteristics of complex-valued neural networks. Finally, the experiments validate our theoretical findings numerically."}}
{"id": "0UkY1AWA1v", "cdate": 1640995200000, "mdate": 1675048366048, "content": {"title": "Learnware: Small Models Do Big", "abstract": "There are complaints about current machine learning techniques such as the requirement of a huge amount of training data and proficient training skills, the difficulty of continual learning, the risk of catastrophic forgetting, the leaking of data privacy/proprietary, etc. Most research efforts have been focusing on one of those concerned issues separately, paying less attention to the fact that most issues are entangled in practice. The prevailing big model paradigm, which has achieved impressive results in natural language processing and computer vision applications, has not yet addressed those issues, whereas becoming a serious source of carbon emissions. This article offers an overview of the learnware paradigm, which attempts to enable users not need to build machine learning models from scratch, with the hope of reusing small models to do things even beyond their original purposes, where the key ingredient is the specification which enables a trained model to be adequately identified to reuse according to the requirement of future users who know nothing about the model in advance."}}
{"id": "Q4htm_aG3-N", "cdate": 1577836800000, "mdate": 1649834468073, "content": {"title": "Multi-label optimal margin distribution machine", "abstract": "Multi-label support vector machine (Rank-SVM) is a classic and effective algorithm for multi-label classification. The pivotal idea is to maximize the minimum margin of label pairs, which is extended from SVM. However, recent studies disclosed that maximizing the minimum margin does not necessarily lead to better generalization performance, and instead, it is more crucial to optimize the margin distribution. Inspired by this idea, in this paper, we first introduce margin distribution to multi-label learning and propose multi-label Optimal margin Distribution Machine (mlODM), which optimizes the margin mean and variance of all label pairs efficiently. Extensive experiments in multiple multi-label evaluation metrics illustrate that mlODM outperforms SVM-style multi-label methods. Moreover, empirical study presents the best margin distribution and verifies the fast convergence of our method."}}
{"id": "Otvs6TgP8M", "cdate": 1546300800000, "mdate": 1675048383795, "content": {"title": "Coreset Stochastic Variance-Reduced Gradient with Application to Optimal Margin Distribution Machine", "abstract": "A major problem for kernel-based predictors is the prohibitive computational complexity, which limits their application in large-scale datasets. Coreset, an approximation method which tries to cover the given examples with a small set of points, can be used to remain the prominent information and accelerate the kernel method. In this paper, we provide perhaps the first coreset-based kernel-accelerating optimization method that has a linear convergence rate, which is much faster than existing approaches. Our method can be used to train kernel SVM-style problems and obtain sparse solutions efficiently. Specifically, the method uses SVRG as the framework, and utilizes the core points to approximate the gradients, so it can significantly reduce the complexity of the kernel method. Furthermore, we apply the method to train ODM, a kernel machine enjoying better statistical property than SVM, so that we can reduce the risk of compromising the performance while encouraging the sparsity. We conduct extensive experiments on several large-scale datasets and the results verify that our method outperforms the state-of-the-art coreset approximation method in both efficiency and generalization, while simultaneously achieving significant speed-up compared to non-approximation baselines."}}
