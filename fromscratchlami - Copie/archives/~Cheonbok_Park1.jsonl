{"id": "yuiHUPmTmP", "cdate": 1672531200000, "mdate": 1684116937814, "content": {"title": "PePe: Personalized Post-editing Model utilizing User-generated Post-edits", "abstract": ""}}
{"id": "FDivyTQ1kbU", "cdate": 1672531200000, "mdate": 1681654968720, "content": {"title": "A Visual Analytics System for Improving Attention-based Traffic Forecasting Models", "abstract": ""}}
{"id": "upNEMwaXh0", "cdate": 1640995200000, "mdate": 1684116937816, "content": {"title": "DaLC: Domain Adaptation Learning Curve Prediction for Neural Machine Translation", "abstract": ""}}
{"id": "kiU3oRaW6d", "cdate": 1640995200000, "mdate": 1681654969183, "content": {"title": "Residual Correction in Real-Time Traffic Forecasting", "abstract": ""}}
{"id": "i-5sG2bF0pC", "cdate": 1640995200000, "mdate": 1681654968936, "content": {"title": "PASTA: PArallel Spatio-Temporal Attention with Spatial Auto-Correlation Gating for Fine-Grained Crowd Flow Prediction", "abstract": ""}}
{"id": "caEC4WECc0", "cdate": 1640995200000, "mdate": 1684116937929, "content": {"title": "DaLC: Domain Adaptation Learning Curve Prediction for Neural Machine Translation", "abstract": "Domain Adaptation (DA) of Neural Machine Translation (NMT) model often relies on a pre-trained general NMT model which is adapted to the new domain on a sample of in-domain parallel data. Without parallel data, there is no way to estimate the potential benefit of DA, nor the amount of parallel samples it would require. It is however a desirable functionality that could help MT practitioners to make an informed decision before investing resources in dataset creation. We propose a Domain adaptation Learning Curve prediction (DaLC) model that predicts prospective DA performance based on in-domain monolingual samples in the source language. Our model relies on the NMT encoder representations combined with various instance and corpus-level features. We demonstrate that instance-level is better able to distinguish between different domains compared to corpus-level frameworks proposed in previous studies. Finally, we perform in-depth analyses of the results highlighting the limitations of our approach, and provide directions for future research."}}
{"id": "_W5qqVCpAKX", "cdate": 1640995200000, "mdate": 1684116938058, "content": {"title": "Specializing Multi-domain NMT via Penalizing Low Mutual Information", "abstract": "Multi-domain Neural Machine Translation (NMT) trains a single model with multiple domains. It is appealing because of its efficacy in handling multiple domains within one model. An ideal multi-domain NMT should learn distinctive domain characteristics simultaneously, however, grasping the domain peculiarity is a non-trivial task. In this paper, we investigate domain-specific information through the lens of mutual information (MI) and propose a new objective that penalizes low MI to become higher. Our method achieved the state-of-the-art performance among the current competitive multi-domain NMT models. Also, we empirically show our objective promotes low MI to be higher resulting in domain-specialized multi-domain NMT."}}
{"id": "YGTDRLF5gx", "cdate": 1640995200000, "mdate": 1681654969188, "content": {"title": "A Visual Analytics System for Improving Attention-based Traffic Forecasting Models", "abstract": ""}}
{"id": "WMdiKNC_wJR", "cdate": 1640995200000, "mdate": 1681654969179, "content": {"title": "PePe: Personalized Post-editing Model utilizing User-generated Post-edits", "abstract": ""}}
{"id": "DfkOeDXuYm5", "cdate": 1640995200000, "mdate": 1681654968942, "content": {"title": "Reversible Instance Normalization for Accurate Time-Series Forecasting against Distribution Shift", "abstract": ""}}
