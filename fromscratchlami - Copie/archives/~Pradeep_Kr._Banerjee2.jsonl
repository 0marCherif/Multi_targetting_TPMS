{"id": "3YjQfCLdrzz", "cdate": 1663850034826, "mdate": null, "content": {"title": "FoSR: First-order spectral rewiring for addressing oversquashing in GNNs", "abstract": "Graph neural networks (GNNs) are able to leverage the structure of graph data by passing messages along the edges of the graph. While this allows GNNs to learn features depending on the graph structure, for certain graph topologies it leads to inefficient information propagation and a problem known as oversquashing. This has recently been linked with the curvature and spectral gap of the graph. On the other hand, adding edges to the message-passing graph can lead to increasingly similar node representations and a problem known as oversmoothing. We propose a computationally efficient algorithm that prevents oversquashing by systematically adding edges to the graph based on spectral expansion. We combine this with a relational architecture, which lets the GNN preserve the original graph structure and provably prevents oversmoothing. We find experimentally that our algorithm outperforms existing graph rewiring methods in several graph classification tasks."}}
{"id": "eKc91g8K2nm", "cdate": 1663624350380, "mdate": 1663624350380, "content": {"title": "Oversquashing in GNNs through the lens of information contraction and graph expansion", "abstract": "The quality of signal propagation in message-passing graph neural networks (GNNs) strongly influences their expressivity as has been observed in recent works. In particular, for prediction tasks relying on long-range interactions, recursive aggregation of node features can lead to an undesired phenomenon called \"oversquashing\". We present a framework for analyzing oversquashing based on information contraction. Our analysis is guided by a model of reliable computation due to von Neumann that lends a new insight into oversquashing as signal quenching in noisy computation graphs. Building on this, we propose a graph rewiring algorithm aimed at alleviating oversquashing. Our algorithm employs a random local edge flip primitive motivated by an expander graph construction. We compare the spectral expansion properties of our algorithm with that of an existing curvature-based non-local rewiring strategy. Synthetic experiments show that while our algorithm in general has a slower rate of expansion, it is overall computationally cheaper, preserves the node degrees exactly and never disconnects the graph."}}
{"id": "KeI9E-gsoB", "cdate": 1632875536651, "mdate": null, "content": {"title": "Learning Curves for Gaussian Process Regression with Power-Law Priors and Targets", "abstract": "We characterize the power-law asymptotics of learning curves for Gaussian process regression (GPR) under the assumption that the eigenspectrum of the prior and the eigenexpansion coefficients of the target function follow a power law. Under similar assumptions, we leverage the equivalence between GPR and kernel ridge regression (KRR) to show the generalization error of KRR. Infinitely wide neural networks can be related to GPR with respect to the neural network GP kernel and the neural tangent kernel, which in several cases is known to have a power-law spectrum. Hence our methods can be applied to study the generalization error of infinitely wide neural networks. We present toy experiments demonstrating the theory."}}
{"id": "LPw-isa6Ngb", "cdate": 1614887119243, "mdate": null, "content": {"title": "PAC-Bayes and Information Complexity", "abstract": "We point out that a number of well-known PAC-Bayesian-style and information-theoretic generalization bounds for randomized learning algorithms can be derived under a common framework starting from a fundamental information exponential inequality. We also obtain new bounds for data-dependent priors and unbounded loss functions. Optimizing these bounds naturally gives rise to a method called Information Complexity Minimization for which we discuss two practical examples for learning with neural networks, namely Entropy- and PAC-Bayes- SGD. "}}
{"id": "KHhzeqGfMqK", "cdate": 1546300800000, "mdate": null, "content": {"title": "Unique Information and Secret Key Decompositions", "abstract": "The unique information (UI) is an information measure that quantifies a deviation from the Blackwell order. We have recently shown that this quantity is an upper bound on the one-way secret key rate. In this paper, we prove a triangle inequality for the UI, which implies that the UI is never greater than one of the best known upper bounds on the two-way secret key rate. We conjecture that the UI lower bounds the two-way rate and discuss implications of the conjecture."}}
{"id": "rygjN3C9F7", "cdate": 1538087987360, "mdate": null, "content": {"title": "The Variational Deficiency Bottleneck", "abstract": "We introduce a bottleneck method for learning data representations based on channel deficiency, rather than the more traditional information sufficiency. A variational upper bound allows us to implement this method efficiently. The bound itself is bounded above by the variational information bottleneck objective, and the two methods coincide in the regime of single-shot Monte Carlo approximations. The notion of deficiency provides a principled way of approximating complicated channels by relatively simpler ones. The deficiency of one channel w.r.t. another has an operational interpretation in terms of the optimal risk gap of decision problems, capturing classification as a special case. Unsupervised generalizations are possible, such as the deficiency autoencoder, which can also be formulated in a variational form. Experiments demonstrate that the deficiency bottleneck can provide advantages in terms of minimal sufficiency as measured by information bottleneck curves, while retaining a good test performance in classification and reconstruction tasks. "}}
{"id": "vMeBr2icn8F", "cdate": 1514764800000, "mdate": null, "content": {"title": "Computing the Unique Information", "abstract": "Given a pair of predictor variables and a response variable, how much information do the predictors have about the response, and how is this information distributed between unique, redundant, and synergistic components? Recent work has proposed to quantify the unique component of the decomposition as the minimum value of the conditional mutual information over a constrained set of information channels. We present an efficient iterative divergence minimization algorithm to solve this optimization problem with convergence guarantees and evaluate its performance against other techniques. A full version of this paper is accessible at: https://arxiv.org/abs/1709.07487."}}
{"id": "uACzeHONW_S", "cdate": 1514764800000, "mdate": null, "content": {"title": "Unique Informations and Deficiencies", "abstract": "Given two channels that convey information about the same random variable, we introduce two measures of the unique information of one channel with respect to the other. The two quantities are based on the notion of generalized weighted Le Cam deficiencies and differ on whether one channel can approximate the other by a randomization at either its input or output. We relate the proposed quantities to an existing measure of unique information which we call the minimum-synergy unique information. We give an operational interpretation of the latter in terms of an upper bound on the one-way secret key rate and discuss the role of the unique informations in the context of nonnegative mutual information decompositions into unique, redundant and synergistic components."}}
{"id": "lQ6t-PJhVyC", "cdate": 1514764800000, "mdate": null, "content": {"title": "The Variational Deficiency Bottleneck", "abstract": "We introduce a bottleneck method for learning data representations based on information deficiency, rather than the more traditional information sufficiency. A variational upper bound allows us to implement this method efficiently. The bound itself is bounded above by the variational information bottleneck objective, and the two methods coincide in the regime of single-shot Monte Carlo approximations. The notion of deficiency provides a principled way of approximating complicated channels by relatively simpler ones. We show that the deficiency of one channel with respect to another has an operational interpretation in terms of the optimal risk gap of decision problems, capturing classification as a special case. Experiments demonstrate that the deficiency bottleneck can provide advantages in terms of minimal sufficiency as measured by information bottleneck curves, while retaining robust test performance in classification tasks."}}
{"id": "w-nIiBajMvi", "cdate": 1483228800000, "mdate": null, "content": {"title": "Coarse-Graining and the Blackwell Order", "abstract": "Suppose we have a pair of information channels, \u03ba 1 , \u03ba 2 , with a common input. The Blackwell order is a partial order over channels that compares \u03ba 1 and \u03ba 2 by the maximal expected utility an agent can obtain when decisions are based on the channel outputs. Equivalently, \u03ba 1 is said to be Blackwell-inferior to \u03ba 2 if and only if \u03ba 1 can be constructed by garbling the output of \u03ba 2 . A related partial order stipulates that \u03ba 2 is more capable than \u03ba 1 if the mutual information between the input and output is larger for \u03ba 2 than for \u03ba 1 for any distribution over inputs. A Blackwell-inferior channel is necessarily less capable. However, examples are known where \u03ba 1 is less capable than \u03ba 2 but not Blackwell-inferior. We show that this may even happen when \u03ba 1 is constructed by coarse-graining the inputs of \u03ba 2 . Such a coarse-graining is a special kind of \u201cpre-garbling\u201d of the channel inputs. This example directly establishes that the expected value of the shared utility function for the coarse-grained channel is larger than it is for the non-coarse-grained channel. This contradicts the intuition that coarse-graining can only destroy information and lead to inferior channels. We also discuss our results in the context of information decompositions."}}
