{"id": "bFR5AqVdvlo", "cdate": 1679527481116, "mdate": 1679527481116, "content": {"title": "FLEX: Full-Body Grasping Without Full-Body Grasps", "abstract": "Synthesizing 3D human avatars interacting realistically with a scene is an important problem with applications in AR/VR, video games, and robotics. Towards this goal, we address the task of generating a virtual human -- hands and full body -- grasping everyday objects. Existing methods approach this problem by collecting a 3D dataset of humans interacting with objects and training on this data. However, 1) these methods do not generalize to different object positions and orientations or to the presence of furniture in the scene, and 2) the diversity of their generated full-body poses is very limited. In this work, we address all the above challenges to generate realistic, diverse full-body grasps in everyday scenes without requiring any 3D full-body grasping data. Our key insight is to leverage the existence of both full-body pose and hand-grasping priors, composing them using 3D geometrical constraints to obtain full-body grasps. We empirically validate that these constraints can generate a variety of feasible human grasps that are superior to baselines both quantitatively and qualitatively."}}
{"id": "Ve-_MrYkfnr", "cdate": 1667336555421, "mdate": 1667336555421, "content": {"title": "It\u2019s Time for Artistic Correspondence in Music and Video", "abstract": "We present an approach for recommending a music track\nfor a given video, and vice versa, based on both their temporal alignment and their correspondence at an artistic level.\nWe propose a self-supervised approach that learns this correspondence directly from data, without any need of human annotations. In order to capture the high-level concepts that are required to solve the task, we propose modeling the long-term temporal context of both the video and the\nmusic signals, using Transformer networks for each modality. Experiments show that this approach strongly outperforms alternatives that do not exploit the temporal context.\nThe combination of our contributions improve retrieval accuracy up to 10\u00d7 over prior state of the art. This strong improvement allows us to introduce a wide range of analyses\nand applications. For instance, we can condition music retrieval based on visually defined attributes."}}
{"id": "kWH6BPb7sr", "cdate": 1667336464450, "mdate": 1667336464450, "content": {"title": "Learning the Predictability of the Future", "abstract": "We introduce a framework for learning from unlabeled video what is predictable in the future. Instead of committing up front to features to predict, our approach learns from data which features are predictable. Based on the observation that hyperbolic geometry naturally and compactly encodes hierarchical structure, we propose a predictive model in hyperbolic space. When the model is most confident, it will predict at a concrete level of the hierarchy, but when the model is not confident, it learns to automatically select a higher level of abstraction. Experiments on two established datasets show the key role of hierarchical representations for action prediction. Although our representation is trained with unlabeled video, visualizations show that action hierarchies emerge in the representation."}}
{"id": "nJWcpq2fco3", "cdate": 1652737300688, "mdate": null, "content": {"title": "Representing Spatial Trajectories as Distributions", "abstract": "We introduce a representation learning framework for spatial trajectories. We represent partial observations of trajectories as probability distributions in a learned latent space, which characterize the uncertainty about unobserved parts of the trajectory. Our framework allows us to obtain samples from a trajectory for any continuous point in time\u2014both interpolating and extrapolating. Our flexible approach supports directly modifying specific attributes of a trajectory, such as its pace, as well as combining different partial observations into single representations. Experiments show our method's superiority over baselines in prediction tasks."}}
{"id": "cU0a02VF8ZG", "cdate": 1601308279417, "mdate": null, "content": {"title": "Globetrotter: Unsupervised Multilingual Translation from Visual Alignment", "abstract": "Machine translation in a multi-language scenario requires large-scale parallel corpora  for  every  language  pair.   Unsupervised  translation  is  challenging  because there is no explicit connection between languages, and the existing methods have to rely on topological properties of the language representations.  We introduce a framework that leverages visual similarity to align multiple languages, using images as the bridge between them. We estimate the cross-modal alignment between language and images, and use this estimate to guide the learning of cross-lingual representations.   Our  language  representations  are  trained  jointly  in  one  model with a single stage.  Experiments with fifty-two languages show that our method outperforms prior work on unsupervised word-level and sentence-level translation using retrieval."}}
