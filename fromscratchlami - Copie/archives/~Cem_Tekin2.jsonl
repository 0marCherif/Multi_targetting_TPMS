{"id": "FUYe1Rahxc", "cdate": 1672531200000, "mdate": 1682018847507, "content": {"title": "Online Context-Aware Task Assignment in Mobile Crowdsourcing via Adaptive Discretization", "abstract": "Mobile crowdsourcing is rapidly boosting the Internet of Things revolution. Its natural development leads to an adaptation to various real-world scenarios, thus imposing a need for wide generality on data-processing and task-assigning methods. We consider the task assignment problem in mobile crowdsourcing while taking into consideration the following: (i) we assume that additional information is available for both tasks and workers, such as location, device parameters, or task parameters, and make use of such information; (ii) as an important consequence of the worker-location factor, we assume that some workers may not be available for selection at given times; (iii) the workers' characteristics may change over time. To solve the task assignment problem in this setting, we propose <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Adaptive Optimistic Matching for Mobile Crowdsourcing</i> (AOM-MC), an online learning algorithm that incurs <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\tilde{O}(T^{(\\bar{D}+1)/(\\bar{D}+2)+\\epsilon })$</tex-math></inline-formula> regret in <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$T$</tex-math></inline-formula> rounds, for any <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\epsilon &gt;0$</tex-math></inline-formula> , under mild continuity assumptions. Here, <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\bar{D}$</tex-math></inline-formula> is a notion of dimensionality which captures the structure of the problem. We also present extensive simulations that illustrate the advantage of adaptive discretization when compared with uniform discretization, and a time- and location-dependent crowdsourcing simulation using a real-world dataset, clearly demonstrating our algorithm's superiority to the current state-of-the-art and baseline algorithms."}}
{"id": "TAwKEXLWRGk", "cdate": 1652877354938, "mdate": 1652877354938, "content": {"title": "Pareto Active Learning with Gaussian Processes and Adaptive Discretization", "abstract": "We consider the problem of optimizing a vector-valued objective function $\\bs{f}$ sampled from a Gaussian Process (GP) whose index set is a well-behaved, compact metric space $(\\XX,d)$ of designs. We assume that $\\bs{f}$ is not known beforehand and that evaluating $\\bs{f}$ at design $x$ results in a noisy observation of $\\bs{f}(x)$. Since identifying the Pareto optimal designs via exhaustive search is infeasible when the cardinality of $\\XX$ is large, we propose an algorithm, called Adaptive $\\bs{\\epsilon}$-PAL, that exploits the smoothness of the GP-sampled function and the structure of $(\\XX,d)$ to learn fast. In essence, Adaptive $\\bs{\\epsilon}$-PAL employs a tree-based adaptive discretization technique to identify an $\\bs{\\epsilon}$-accurate Pareto set of designs in as few evaluations as possible. We provide both information-type and metric dimension-type bounds on the sample complexity of $\\bs{\\epsilon}$-accurate Pareto set identification. We also experimentally show that our algorithm outperforms other Pareto set identification methods on several benchmark datasets."}}
{"id": "PfPgymeGDxt", "cdate": 1652877220284, "mdate": 1652877220284, "content": {"title": "Contextual Combinatorial Volatile Bandits via Gaussian Processes", "abstract": "We consider a contextual bandit problem with a combinatorial action set and time-varying base arm availability. At the beginning of each round, the agent observes the set of available base arms and their contexts and then selects an action that is a feasible subset of the set of available base arms to maximize its cumulative reward in the long run. We assume that the mean outcomes of base arms are samples from a Gaussian Process indexed by the context set ${\\cal X}$, and the expected reward is Lipschitz continuous in expected base arm outcomes. For this setup, we propose an algorithm called Optimistic Combinatorial Learning and Optimization with Kernel Upper Confidence Bounds (O'CLOK-UCB) and prove that it incurs $\\tilde{O}(K\\sqrt{T\\overline{\\gamma}_{T}} )$ regret with high probability, where $\\overline{\\gamma}_{T}$ is the maximum information gain associated with the set of base arm contexts that appeared in the first $T$ rounds and $K$ is the maximum cardinality of any feasible action over all rounds. To dramatically speed up the algorithm, we also propose a variant of O'CLOK-UCB that uses sparse GPs. Finally, we experimentally show that both algorithms exploit inter-base arm outcome correlation and vastly outperform the previous state-of-the-art UCB-based algorithms in realistic setups."}}
{"id": "eJ230b6n3rD", "cdate": 1652877131458, "mdate": 1652877131458, "content": {"title": "Contextual Combinatorial Volatile Bandits with Satisfying via Gaussian Processes", "abstract": "In many real-world applications of combinatorial bandits such as content caching, rewards must be maximized while satisfying minimum service requirements. In addition, base arm availabilities vary over time, and actions need to be adapted to the situation to maximize the rewards. We propose a new bandit model called Contextual Combinatorial Volatile Bandits with Group Thresholds to address these challenges. Our model subsumes combinatorial bandits by considering super arms to be subsets of groups of base arms. We seek to maximize super arm rewards while satisfying thresholds of all base arm groups that constitute a super arm. To this end, we define a new notion of regret that merges super arm reward maximization with group reward satisfaction. To facilitate learning, we assume that the mean outcomes of base arms are samples from a Gaussian Process indexed by the context set ${\\cal X}$, and the expected reward is Lipschitz continuous in expected base arm outcomes. We propose an algorithm, called Thresholded Combinatorial Gaussian Process Upper Confidence Bounds (TCGP-UCB), that balances between maximizing cumulative reward and satisfying group reward thresholds and prove that it incurs $\\tilde{O}(K\\sqrt{T\\overline{\\gamma}_{T}} )$ regret with high probability, where $\\overline{\\gamma}_{T}$ is the maximum information gain associated with the set of base arm contexts that appeared in the first $T$ rounds and $K$ is the maximum super arm cardinality of any feasible action over all rounds. We show in experiments that our algorithm accumulates a reward comparable with that of the state-of-the-art combinatorial bandit algorithm while picking actions whose groups satisfy their thresholds."}}
{"id": "taGwHpqJNZt", "cdate": 1652856436696, "mdate": 1652856436696, "content": {"title": "Multi-objective Contextual Bandit Problem with Similarity Information", "abstract": " In this paper we propose the multi-objective contextual bandit problem with similarity information. This problem extends the classical contextual bandit problem with similarity information by introducing multiple and possibly conflicting objectives. Since the best arm in each objective can be different given the context, learning the best arm based on a single objective can jeopardize the rewards obtained from the other objectives. To handle this issue, we define a new performance metric, called the contextual Pareto regret, to evaluate the performance of the learner. Essentially, the contextual Pareto regret is the sum of the distances of the arms chosen by the learner to the context dependent Pareto front. For this problem, we develop a new online learning algorithm called Pareto Contextual Zooming (PCZ), which exploits the idea of contextual zooming to learn the arms that are close to the Pareto front for each observed context by adaptively partitioning the joint context-arm set according to the observed rewards and locations of the  context-arm pairs selected in the past. Then, we prove that PCZ achieves $\\tilde O (T^{(1+d_p)/(2+d_p)})$ Pareto regret where $d_p$ is the Pareto zooming dimension that depends on the size of the set of near-optimal context-arm pairs. Moreover, we show that this regret bound is nearly optimal by providing an almost matching $\u03a9(T^{(1+d_p)/(2+d_p)})$ lower bound.\n"}}
{"id": "nEe2ifk4wS", "cdate": 1652856280722, "mdate": 1652856280722, "content": {"title": "Contextual Combinatorial Volatile Multi-armed Bandit with Adaptive Discretization", "abstract": "We consider contextual combinatorial volatile multi-armed bandit (CCV-MAB), in which at each round, the learner observes a set of available base arms and their contexts, and then, selects a super arm that contains $K$ base arms in order to maximize its cumulative reward. Under the semi-bandit feedback setting and assuming that the contexts lie in a space ${\\cal X}$ endowed with the Euclidean norm and that the expected base arm outcomes (expected rewards) are Lipschitz continuous in the contexts (expected base arm outcomes), we propose an algorithm called Adaptive Contextual Combinatorial Upper Confidence Bound (ACC-UCB). This algorithm, which adaptively discretizes ${\\cal X}$ to form estimates of base arm outcomes and uses an $\\alpha$-approximation oracle as a subroutine to select a super arm in each round, achieves $\\tilde{O} ( T^{(\\bar{D}+1)/(\\bar{D}+2) + \\epsilon}  )$ regret for any $\\epsilon&gt;0$, where $\\bar{D}$ represents the approximate optimality dimension related to ${\\cal X}$. This dimension captures both the benignness of the base arm arrivals and the structure of the expected reward. In addition, we provide a recipe for obtaining more optimistic regret bounds by taking into account the volatility of the base arms and show that ACC-UCB achieves significant performance gains compared to the state-of-the-art for worker selection in mobile crowdsourcing.\n\n"}}
{"id": "OHYikHrlVHp", "cdate": 1652856195422, "mdate": 1652856195422, "content": {"title": "Combinatorial Gaussian Process Bandits with Probabilistically Triggered Arms", "abstract": "Combinatorial bandit models and algorithms are used in many sequential decision-making tasks ranging from item list recommendation to influence maximization. Typical algorithms proposed for combinatorial bandits, including combinatorial UCB (CUCB) and combinatorial Thompson sampling (CTS) do not exploit correlations between base arms during the learning process. Moreover, their regret is usually analyzed under independent base arm outcomes. In this paper, we use Gaussian Processes (GPs) to model correlations between base arms. In particular, we consider a combinatorial bandit model with probabilistically triggered arms, and assume that the expected base arm outcome function is a sample from a GP. We assume that the learner has access to an exact computation oracle, which returns an optimal solution given expected base arm outcomes, and analyze the regret of Combinatorial Gaussian Process Upper Confidence Bound (ComGP-UCB) algorithm for this setting. Under (triggering probability modulated) Lipschitz continuity assumption on the expected reward function, we derive ($O( \\sqrt{m T \\log T \\gamma_{T, \\boldsymbol{\\mu}}^{PTA}})$) $O(m \\sqrt{\\frac{T \\log T}{p^*}})$ upper bounds for the regret of ComGP-UCB that hold with high probability, where $m$ denotes the number of base arms, $p^*$ denotes the minimum non-zero triggering probability, and $\\gamma_{T, \\boldsymbol{\\mu}}^{PTA}$ denotes the pseudo-information gain. Finally, we show via simulations that when the correlations between base arm outcomes are strong, ComGP-UCB significantly outperforms CUCB and CTS. "}}
{"id": "JokpPqA294", "cdate": 1652737737804, "mdate": null, "content": {"title": "ESCADA: Efficient Safety and Context Aware Dose Allocation for Precision Medicine", "abstract": "Finding an optimal individualized treatment regimen is considered one of the most challenging precision medicine problems. Various patient characteristics influence the response to the treatment, and hence, there is no one-size-fits-all regimen. Moreover, the administration of an unsafe dose during the treatment can have adverse effects on health. Therefore, a treatment model must ensure patient \\emph{safety} while \\emph{efficiently} optimizing the course of therapy. We study a prevalent medical problem where the treatment aims to keep a physiological variable in a safe range and preferably close to a target level, which we refer to as \\emph{leveling}. Such a task may be relevant in numerous other domains as well. We propose ESCADA, a novel and generic multi-armed bandit (MAB) algorithm tailored for the leveling task, to make safe, personalized, and context-aware dose recommendations. We derive high probability upper bounds on its cumulative regret and safety guarantees. Following ESCADA's design, we also describe its Thompson sampling-based counterpart. We discuss why the straightforward adaptations of the classical MAB algorithms such as GP-UCB may not be a good fit for the leveling task. Finally, we make \\emph{in silico} experiments on the bolus-insulin dose allocation problem in type-1 diabetes mellitus disease and compare our algorithms against the famous GP-UCB algorithm, the rule-based dose calculators, and a clinician."}}
{"id": "muxdvW4OK2v", "cdate": 1640995200000, "mdate": 1682018847494, "content": {"title": "Federated Multi-Armed Bandits Under Byzantine Attacks", "abstract": "Multi-armed bandits (MAB) is a simple reinforcement learning model where the learner controls the trade-off between exploration versus exploitation to maximize its cumulative reward. Federated multi-armed bandits (FMAB) is a recently emerging framework where a cohort of learners with heterogeneous local models play a MAB game and communicate their aggregated feedback to a parameter server to learn the global feedback model. Federated learning models are vulnerable to adversarial attacks such as model-update attacks or data poisoning. In this work, we study an FMAB problem in the presence of Byzantine clients who can send false model updates that pose a threat to the learning process. We borrow tools from robust statistics and propose a median-of-means-based estimator: Fed-MoM-UCB, to cope with the Byzantine clients. We show that if the Byzantine clients constitute at most half the cohort, it is possible to incur a cumulative regret on the order of ${\\cal O} (\\log T)$ with respect to an unavoidable error margin, including the communication cost between the clients and the parameter server. We analyze the interplay between the algorithm parameters, unavoidable error margin, regret, communication cost, and the arms' suboptimality gaps. We demonstrate Fed-MoM-UCB's effectiveness against the baselines in the presence of Byzantine attacks via experiments."}}
{"id": "mXMGFhvkxWs", "cdate": 1640995200000, "mdate": 1682018847487, "content": {"title": "Feedback Adaptive Learning for Medical and Educational Application Recommendation", "abstract": "Recommending applications (apps) to improve health or educational outcomes requires long-term planning and adaptation based on the user feedback, as it is imperative to recommend the right app at the right time to improve engagement and benefit. We model the challenging task of app recommendation for these specific categories of apps\u2014or alike\u2014using a new reinforcement learning method referred to as episodic multi-armed bandit (eMAB). In eMAB, the learner recommends apps to individual users and observes their interactions with the recommendations on a weekly basis. It then uses this data to maximize the total payoff of all users by learning to recommend specific apps. Since computing the optimal recommendation sequence is intractable, as a benchmark, we define an oracle that sequentially recommends apps to maximize the expected immediate gain. Then, we propose our online learning algorithm, named FeedBack Adaptive Learning (FeedBAL), and prove that its regret with respect to the benchmark increases logarithmically in expectation. We demonstrate the effectiveness of FeedBAL on recommending mental health apps based on data from an app suite and show that it results in a substantial increase in the number of app sessions compared with episodic versions of <inline-formula><tex-math notation=\"LaTeX\">$\\epsilon _n$</tex-math></inline-formula> -greedy, Thompson sampling, and collaborative filtering methods."}}
