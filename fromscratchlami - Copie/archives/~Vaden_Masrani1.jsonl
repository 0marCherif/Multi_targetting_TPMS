{"id": "0RTJcuvHtIu", "cdate": 1652737679203, "mdate": null, "content": {"title": "Flexible Diffusion Modeling of Long Videos", "abstract": "We present a framework for video modeling based on denoising diffusion probabilistic models that produces long-duration video completions in a variety of realistic environments. We introduce a generative model that can at test-time sample any arbitrary subset of video frames conditioned on any other subset and present an architecture adapted for this purpose. Doing so allows us to efficiently compare and optimize a variety of schedules for the order in which frames in a long video are sampled and use selective sparse and long-range conditioning on previously sampled frames.  We demonstrate improved video modeling over prior work on a number of datasets and sample temporally coherent videos over 25 minutes in length.  We additionally release a new video modeling dataset and semantically meaningful metrics based on videos generated in the CARLA autonomous driving simulator."}}
{"id": "VrkuIX7-_t", "cdate": 1640995200000, "mdate": 1681494432415, "content": {"title": "Flexible Diffusion Modeling of Long Videos", "abstract": ""}}
{"id": "Lmq6dWvpx6", "cdate": 1640995200000, "mdate": 1668749224469, "content": {"title": "Beyond Simple Meta-Learning: Multi-Purpose Models for Multi-Domain, Active and Continual Few-Shot Learning", "abstract": "Modern deep learning requires large-scale extensively labelled datasets for training. Few-shot learning aims to alleviate this issue by learning effectively from few labelled examples. In previously proposed few-shot visual classifiers, it is assumed that the feature manifold, where classifier decisions are made, has uncorrelated feature dimensions and uniform feature variance. In this work, we focus on addressing the limitations arising from this assumption by proposing a variance-sensitive class of models that operates in a low-label regime. The first method, Simple CNAPS, employs a hierarchically regularized Mahalanobis-distance based classifier combined with a state of the art neural adaptive feature extractor to achieve strong performance on Meta-Dataset, mini-ImageNet and tiered-ImageNet benchmarks. We further extend this approach to a transductive learning setting, proposing Transductive CNAPS. This transductive method combines a soft k-means parameter refinement procedure with a two-step task encoder to achieve improved test-time classification accuracy using unlabelled data. Transductive CNAPS achieves state of the art performance on Meta-Dataset. Finally, we explore the use of our methods (Simple and Transductive) for \"out of the box\" continual and active learning. Extensive experiments on large scale benchmarks illustrate robustness and versatility of this, relatively speaking, simple class of models. All trained model checkpoints and corresponding source codes have been made publicly available."}}
{"id": "3GlFvO5KkY", "cdate": 1620639552689, "mdate": null, "content": {"title": "Planning as Inference in Epidemiological Models", "abstract": "In this work we demonstrate how existing software tools can be used to automate parts of infectious disease-control policy-making via performing inference in existing epidemiological dynamics models. The kind of inference tasks undertaken include computing, for planning purposes, the posterior distribution over putatively controllable, via direct policy-making choices, simulation model parameters that give rise to acceptable disease progression outcomes. Neither the full capabilities of such inference automation software tools nor their utility for planning is widely disseminated at the current time. Timely gains in understanding about these tools and how they can be used may lead to more fine-grained and less economically damaging policy prescriptions, particularly during the current COVID-19 pandemic."}}
{"id": "gDkbR0Jc6u", "cdate": 1609459200000, "mdate": 1682618070841, "content": {"title": "Proof of the impossibility of probabilistic induction", "abstract": ""}}
{"id": "H2cRJYKK8QHG", "cdate": 1609459200000, "mdate": 1663989925342, "content": {"title": "q-Paths: Generalizing the Geometric Annealing Path using Power Means", "abstract": "Many common machine learning methods involve the geometric annealing path, a sequence of intermediate densities between two distributions of interest constructed using the geometric average. While alternatives such as the moment-averaging path have demonstrated performance gains in some settings, their practical applicability remains limited by exponential family endpoint assumptions and a lack of closed form energy function. In this work, we introduce $q$-paths, a family of paths which is derived from a generalized notion of the mean, includes the geometric and arithmetic mixtures as special cases, and admits a simple closed form involving the deformed logarithm function from nonextensive thermodynamics. Following previous analysis of the geometric path, we interpret our $q$-paths as corresponding to a $q$-exponential family of distributions, and provide a variational representation of intermediate densities as minimizing a mixture of $\\alpha$-divergences to the endpoints. We show that small deviations away from the geometric path yield empirical gains for Bayesian inference using Sequential Monte Carlo and generative model evaluation using Annealed Importance Sampling."}}
{"id": "7hRdrxbg7", "cdate": 1609459200000, "mdate": 1663989925047, "content": {"title": "q-Paths: Generalizing the geometric annealing path using power means", "abstract": "Many common machine learning methods involve the geometric annealing path, a sequence of intermediate densities between two distributions of interest constructed using the geometric average. While ..."}}
{"id": "0paONRRCwCX", "cdate": 1609459200000, "mdate": 1681494432411, "content": {"title": "Differentiable Particle Filtering without Modifying the Forward Pass", "abstract": ""}}
{"id": "ZBJ20FRVPD", "cdate": 1603141809315, "mdate": null, "content": {"title": "Annealed Importance Sampling with q-Paths", "abstract": "Annealed importance sampling (AIS) is the gold standard for estimating partition functions or marginal likelihoods, corresponding to importance sampling over a path of distributions between a tractable base and an unnormalized target.  While AIS yields an unbiased estimator for any path, existing literature has been limited to the geometric mixture or moment-averaged paths associated with the KL divergence and exponential family.   We explore using $q$-paths for AIS, which are related to the homogeneous power means, deformed exponential family, and $\\alpha$-divergence, and include the geometric path as a special case."}}
{"id": "kShU4LS5Bza", "cdate": 1577836800000, "mdate": 1681494432406, "content": {"title": "Gaussian Process Bandit Optimization of the Thermodynamic Variational Objective", "abstract": ""}}
