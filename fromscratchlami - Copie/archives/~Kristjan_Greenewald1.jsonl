{"id": "snsTK5_BqvQ", "cdate": 1672531200000, "mdate": 1681491219510, "content": {"title": "Minimum-Entropy Coupling Approximation Guarantees Beyond the Majorization Barrier", "abstract": ""}}
{"id": "czL6NLxJsx", "cdate": 1663850476324, "mdate": null, "content": {"title": "Outlier-Robust Group Inference via Gradient Space Clustering", "abstract": "Traditional machine learning models focus on achieving good performance on the overall training distribution, but they often underperform on minority groups. Existing methods can improve the worst-group performance, but they can have several limitations: (i) they require group annotations, which are often expensive and sometimes infeasible to obtain, and/or (ii) they are sensitive to outliers. Most related works fail to solve these two issues simultaneously as they focus on conflicting perspectives of minority groups and outliers. We address the problem of learning group annotations in the presence of outliers by clustering the data in the space of gradients of the model parameters. We show that data in the gradient space has a simpler structure while preserving information about minority groups and outliers, making it suitable for standard clustering methods like DBSCAN. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art both in terms of group identification and downstream worst-group performance."}}
{"id": "PzBGIu-llo7", "cdate": 1663849963669, "mdate": null, "content": {"title": "Learning Proximal Operators to Discover Multiple Optima", "abstract": "Finding multiple solutions of non-convex optimization problems is a ubiquitous yet challenging task. Most past algorithms either apply single-solution optimization methods from multiple random initial guesses or search in the vicinity of found solutions using ad hoc heuristics. We present an end-to-end method to learn the proximal operator of a family of training problems so that multiple local minima can be quickly obtained from initial guesses by iterating the learned operator, emulating the proximal-point algorithm that has fast convergence. The learned proximal operator can be further generalized to recover multiple optima for unseen problems at test time, enabling applications such as object detection. The key ingredient in our formulation is a proximal regularization term, which elevates the convexity of our training loss: by applying recent theoretical results, we show that for weakly-convex objectives with Lipschitz gradients, training of the proximal operator converges globally with a practical degree of over-parameterization. We further present an exhaustive benchmark for multi-solution optimization to demonstrate the effectiveness of our method."}}
{"id": "L-ceBdl2DPb", "cdate": 1652737844028, "mdate": null, "content": {"title": "$k$-Sliced Mutual Information: A Quantitative Study of Scalability with Dimension", "abstract": "Sliced mutual information (SMI) is defined as an average of mutual information (MI) terms between one-dimensional random projections of the random variables. It serves as a surrogate measure of dependence to classic MI that preserves many of its properties but is more scalable to high dimensions. However, a quantitative characterization of how SMI itself and estimation rates thereof depend on the ambient dimension, which is crucial to the understanding of scalability, remain obscure. \nThis work provides a multifaceted account of the dependence of SMI on dimension, under a broader framework termed $k$-SMI, which considers projections to $k$-dimensional subspaces. Using a new result on the continuity of differential entropy in the 2-Wasserstein metric, we derive sharp bounds on the error of Monte Carlo (MC)-based estimates of $k$-SMI, with explicit dependence on $k$ and the ambient dimension, revealing their interplay with the number of samples. We then combine the MC integrator with the neural estimation framework to provide an end-to-end $k$-SMI estimator, for which optimal convergence rates are established. We also explore asymptotics of the population $k$-SMI as dimension grows, providing Gaussian approximation results with a residual that decays under appropriate moment bounds. All our results trivially apply to SMI by setting $k=1$. Our theory is validated with numerical experiments and is applied to sliced InfoGAN, which altogether provide a comprehensive quantitative account of the scalability question of $k$-SMI, including SMI as a special case when $k=1$."}}
{"id": "eM9Fhlys95", "cdate": 1640995200000, "mdate": 1681491220510, "content": {"title": "Log-Euclidean Signatures for Intrinsic Distances Between Unaligned Datasets", "abstract": ""}}
{"id": "Y_cTWVHlqax", "cdate": 1640995200000, "mdate": 1681491220169, "content": {"title": "Entropic Causal Inference: Graph Identifiability", "abstract": ""}}
{"id": "NByuhr2bFd", "cdate": 1640995200000, "mdate": 1681491220207, "content": {"title": "$k$-Variance: A Clustered Notion of Variance", "abstract": ""}}
{"id": "Cx-0a12i22", "cdate": 1640995200000, "mdate": 1681491220874, "content": {"title": "Outlier-Robust Group Inference via Gradient Space Clustering", "abstract": ""}}
{"id": "B1h1DexAQ6R", "cdate": 1640995200000, "mdate": 1681491220348, "content": {"title": "k-Sliced Mutual Information: A Quantitative Study of Scalability with Dimension", "abstract": ""}}
{"id": "7QDKDcHplT", "cdate": 1640995200000, "mdate": 1681491220575, "content": {"title": "Log-Euclidean Signatures for Intrinsic Distances Between Unaligned Datasets", "abstract": ""}}
