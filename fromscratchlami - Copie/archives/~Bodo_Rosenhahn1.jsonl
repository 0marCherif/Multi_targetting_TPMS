{"id": "DJgHzXv61b", "cdate": 1686250303304, "mdate": null, "content": {"title": "Contextualize Me \u2013 The Case for Context in Reinforcement Learning", "abstract": "While Reinforcement Learning (RL) has shown successes in a variety of domains, including game playing, robot manipulation and nuclear fusion, modern RL algorithms are not designed with generalization in mind, making them brittle when faced with even slight variations of their environment. \nTo address this limitation, recent research has increasingly focused on the generalization capabilities of RL agents.\nIdeally, general agents should be capable of zero-shot transfer to previously unseen environments and robust to changes in the problem setting while interacting with an environment.\nSteps in this direction have been taken by proposing new problem settings where agents can test their transfer performance, e.g.~the Arcade Learning Environment's flavors or benchmarks utilizing Procedural Content Generation (PCG) to increase task variation, e.g. ProcGen, NetHack or Alchemy.\n\nWhile these extended problem settings in RL have expanded the possibilities for benchmarking agents in diverse environments, the degree of task variation is often either unknown or cannot be controlled precisely.\nWe believe that generalization in RL is held back by these factors, stemming in part from a lack of problem formalization.\nIn order to facilitate generalization in RL, contextual RL (cRL) proposes to explicitly take environment characteristics, the so-called context into account.\nThis inclusion enables precise design of train and test distributions with respect to this context.\nThus, cRL allows us to reason about the generalization capabilities of RL agents and to quantify their generalization performance.\nOverall, cRL provides a framework for both theoretical analysis and practical improvements.\n\nIn order to empirically study cRL, we introduce our benchmark library CARL, short for Context-Adaptive Reinforcement Learning.\nCARL collects well-established environments from the RL community and extends them with the notion of context.\nWe use our benchmark library to empirically show how different context variations can drastically increase the difficulty of training RL agents, even in simple environments.\nWe further verify the intuition that allowing RL agents access to context information is beneficial for generalization tasks in theory and practice."}}
{"id": "J8EQzzpoAj", "cdate": 1665069631072, "mdate": null, "content": {"title": "Take 5: Interpretable Image Classification with a Handful of Features", "abstract": "Deep Neural Networks use thousands of mostly incomprehensible features to identify a single class, a decision no human can follow. We propose an interpretable sparse and low dimensional final decision layer in a deep neural network with measurable aspects of interpretability and demonstrate it on fine-grained image classification. We argue that a human can only understand the decision of a machine learning model, if the features are interpretable and only very few of them are used for a single decision. For that matter, the final layer has to be sparse and \u2013 to make interpreting the features feasible \u2013 low dimensional. We call a model with a Sparse Low-Dimensional Decision \u201cSLDD-Model\u201d. We show that a SLDD-Model is easier to interpret locally and globally than a dense high-dimensional decision layer while being able to maintain competitive accuracy. Additionally, we propose a loss function that improves a model\u2019s feature diversity and accuracy. Our more interpretable SLDD-Model only uses 5 out of just 50 features per class, while maintaining 97% to 100% of the accuracy on four common benchmark datasets compared to the baseline model with 2048 features."}}
{"id": "KY4FIwBrly", "cdate": 1664723764819, "mdate": null, "content": {"title": "Take 5: Interpretable Image Classification with a Handful of Features", "abstract": "Deep Neural Networks use thousands of mostly incomprehensible features to identify a single class, a decision no human can follow. We propose an interpretable sparse and low dimensional final decision layer in a deep neural network with measurable aspects of interpretability and demonstrate it on fine-grained image classification. We argue that a human can only understand the decision of a machine learning model, if the features are interpretable and only very few of them are used for a single decision. For that matter, the final layer has to be sparse and \u2013 to make interpreting the features feasible \u2013 low dimensional. We call a model with a Sparse Low-Dimensional Decision \u201cSLDD-Model\u201d. We show that a SLDD-Model is easier to interpret locally and globally than a dense high-dimensional decision layer while being able to maintain competitive accuracy. Additionally, we propose a loss function that improves a model\u2019s feature diversity and accuracy. Our more interpretable SLDD-Model only uses 5 out of just 50 features per class, while maintaining 97% to 100% of the accuracy on four common benchmark datasets compared to the baseline model with 2048 features."}}
{"id": "C__2aUY0_3w", "cdate": 1663850029592, "mdate": null, "content": {"title": "Semantic Video Synthesis from Video Scene Graphs", "abstract": "Video synthesis has recently attracted a lot of attention, as the natural extension to the image synthesis task. Most image synthesis works use class labels or text as guidance. However, neither labels nor text can provide explicit temporal guidance, such as when action starts or ends. To overcome this limitation, we introduce video scene graphs as input for video synthesis, as they represent the spatial and temporal relationships between objects in the scene. Since video scene graphs are usually temporally discrete annotations, we propose a video scene graph (VSG) encoder that not only encodes the existing video scene graphs but also predicts the graph representations for unlabeled frames. The VSG encoder is pre-trained with different contrastive multi-modal losses. A video scene graph-to-video synthesis framework (SGVS) based on the pre-trained VSG encoder, VQ-VAE, and auto-regressive Transformer is proposed to synthesize a semantic video given an initial scene image and a non-fixed number of video scene graphs. We evaluate SGVS and other state-of-the-art video synthesis models on Action Genome dataset and demonstrate the positive significance of video scene graphs in video synthesis."}}
{"id": "9EcAsB7wgM", "cdate": 1663850011281, "mdate": null, "content": {"title": "Take 5: Interpretable Image Classification with a Handful of Features", "abstract": "Deep Neural Networks use thousands of mostly incomprehensible features to identify a single class, a decision no human can follow. We propose an interpretable sparse and low dimensional final decision layer in a deep neural network with measurable aspects of interpretability and demonstrate it on fine-grained image classification. We argue that a human can only understand the decision of a machine learning model, if the input features are interpretable and only very few of them are used for a single decision. For that matter, the final layer has to be sparse and \u2013 to make interpreting the features feasible \u2013 low dimensional. We call a model with a Sparse Low-Dimensional Decision \u201cSLDD-Model\u201d. We show that a SLDD-Model is easier to interpret locally and globally than a dense high-dimensional decision layer while being able to maintain competitive accuracy. Additionally, we propose a loss function that improves a model\u2019s feature diversity and accuracy. Our interpretable SLDD-Model only uses 5 out of just 50 features per class, while maintaining 97% to 100% of the accuracy on four common benchmark datasets compared to the baseline model with 2048 features."}}
{"id": "V9eoE9pF9I", "cdate": 1633418177461, "mdate": 1633418177461, "content": {"title": "Spatial-Temporal Transformer for Dynamic Scene Graph Generation", "abstract": "Dynamic scene graph generation aims at generating a scene graph of the given video. Compared to the task of scene graph generation from images, it is more challenging because of the dynamic relationships between objects and the temporal dependencies between frames allowing for a richer semantic interpretation. In this paper, we propose Spatial-temporal Transformer (STTran), a neural network that consists of two core modules: (1) a spatial encoder that takes an input frame to extract spatial context and reason about the visual relationships within a frame, and (2) a temporal decoder which takes the output of the spatial encoder as input in order to capture the temporal dependencies between frames and infer the dynamic relationships. Furthermore, STTran is flexible to take varying lengths of videos as input without clipping, which is especially important for long videos. Our method is validated on the benchmark dataset Action Genome (AG). The experimental results demonstrate the superior performance of our method in terms of dynamic scene graphs. Moreover, a set of ablative studies is conducted and the effect of each proposed module is justified."}}
{"id": "auVSpl_moc0", "cdate": 1633418055712, "mdate": 1633418055712, "content": {"title": "Cuboids Revisited: Learning Robust 3D Shape Fitting to Single RGB Images", "abstract": "Humans perceive and construct the surrounding world as an arrangement of simple parametric models. In particular, man-made environments commonly consist of volumetric primitives such as cuboids or cylinders. Inferring these primitives is an important step to attain high-level, abstract scene descriptions. Previous approaches directly estimate shape parameters from a 2D or 3D input, and are only able to reproduce simple objects, yet unable to accurately parse more complex 3D scenes. In contrast, we propose a robust estimator for primitive fitting, which can meaningfully abstract real-world environments using cuboids. A RANSAC estimator guided by a neural network fits these primitives to 3D features, such as a depth map. We condition the network on previously detected parts of the scene, thus parsing it one-by-one. To obtain 3D features from a single RGB image, we additionally optimise a feature extraction CNN in an end-to-end manner. However, naively minimising point-to-primitive distances leads to large or spurious cuboids occluding parts of the scene behind. We thus propose an occlusion-aware distance metric correctly handling opaque scenes. The proposed algorithm does not require labour-intensive labels, such as cuboid annotations, for training. Results on the challenging NYU Depth v2 dataset demonstrate that the proposed algorithm successfully abstracts cluttered real-world 3D scene layouts."}}
{"id": "ilPaFZaMzo", "cdate": 1580908118270, "mdate": null, "content": {"title": "CONSAC: Robust Multi-Model Fitting by Conditional Sample Consensus", "abstract": "We present a robust estimator for fitting multiple parametric models of the same form to noisy measurements. Applications include finding multiple vanishing points in man-made scenes, fitting planes to architectural imagery, or estimating multiple rigid motions within the same sequence. In contrast to previous works, which resorted to hand-crafted\nsearch strategies for multiple model detection, we learn the search strategy from data. A neural network conditioned\non previously detected models guides a RANSAC estimator to different subsets of all measurements, thereby finding\nmodel instances one after another. We train our method supervised, as well as, self-supervised. For supervised training of the search strategy, we contribute a new dataset for vanishing point detection. Leveraging this dataset, the proposed algorithm is superior with respect to other robust estimators, as well as, to designated vanishing point detection algorithms. For self-supervised learning of the search, we evaluate the proposed algorithm on multi-homography estimation and demonstrate an accuracy that is superior to state-of-the-art methods.\n"}}
{"id": "5ZAXZozawv", "cdate": 1580907890706, "mdate": null, "content": {"title": "NODIS: Neural Ordinary Differential Scene Understanding", "abstract": "Semantic image understanding is a challenging topic in computer vision. It requires to detect all objects in an im-\nage, but also to identify all the relations between them. Detected objects, their labels and the discovered relations\ncan be used to construct a scene graph which provides an abstract semantic interpretation of an image. In previous\nworks, relations were identified by solving an assignment problem formulated as Mixed-Integer Linear Programs. In\nthis work, we interpret that formulation as Ordinary Differential Equation (ODE). The proposed architecture performs scene graph inference by solving a neural variant of an ODE by end-to-end learning. It achieves state-of-the-art results on all three benchmark tasks: scene graph generation (SGGen), classification (SGCls) and visual relationship detection (PredCls) on Visual Genome benchmark.\n"}}
{"id": "TZAy6F19Iu", "cdate": 1580907684392, "mdate": null, "content": {"title": "On support relations and semantic scene graphs", "abstract": "Scene understanding is one of the essential and challenging topics in computer vision and photogrammetry. Scene graph provides valuable information for such scene understanding. This paper proposes a novel framework for automatic generation of semantic scene graphs which interpret indoor environments. First, a Convolutional Neural Network is used to detect objects of interest in the given image. Then, the precise support relations between objects are inferred by taking two important auxiliary information in the indoor environments: the physical stability and the prior support knowledge between object categories. Finally, a semantic scene graph describing the contextual relations within a cluttered indoor scene is constructed. In contrast to the previous methods for extracting support relations, our approach provides more accurate results. Furthermore, we do not use pixel-wise segmentation to obtain objects, which is computation costly. We also propose different methods to evaluate the generated scene graphs, which lacks in this community. Our experiments are carried out on the NYUv2 dataset. The experimental results demonstrated that our approach outperforms the state-of-the-art methods in inferring support relations. The estimated scene graphs are accurately compared with ground truth.\n"}}
