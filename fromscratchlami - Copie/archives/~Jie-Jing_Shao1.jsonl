{"id": "gEPYdogG1W", "cdate": 1683880997285, "mdate": null, "content": {"title": "Open\u2010set learning under covariate shift", "abstract": "Open-set learning deals with the testing distribution where there exist samples from the classes that are unseen during training. They aim to classify the seen classes and recog- nize the unseen classes. Previous studies typically assume that the marginal distribution of the seen classes is fixed across the training and testing distributions. In many real-world applications, however, there may exist covariate shift between them, i.e., the marginal distribution of seen classes may shift. We call this kind of problem as open-set learning under covariate shift, aim to robustly classify the seen classes under covariate shift and be aware of the unseen classes.We present a new open-set learning framework with covari- ate generalization based on supervised contrastive learning, called SC\u2013OSG, inspired by the latent connection between contrastive learning and representation invariance. Specifi- cally, we theoretically justify supervised contrastive learning that could promote the con- ditional invariance of representations, a critical condition for covariate generalization. SC\u2013 OSG generates multi-source samples to promote the representation invariance and improve the covariate generalization. Based on this, we propose a detection score that is specific to the proposed training scheme. We evaluate the effectiveness of our method on several real-world datasets, on all of which we achieve competitive results with state-of-the-art methods."}}
{"id": "lDohSFOHr0", "cdate": 1652737538239, "mdate": null, "content": {"title": "Robust Semi-Supervised Learning when Not All Classes have Labels", "abstract": "Semi-supervised learning (SSL) provides a powerful framework for leveraging unlabeled data. Existing SSL typically requires all classes have labels. However, in many real-world applications, there may exist some classes that are difficult to label or newly occurred classes that cannot be labeled in time, resulting in there are unseen classes in unlabeled data. Unseen classes will be misclassified as seen classes, causing poor classification performance. The performance of seen classes is also harmed by the existence of unseen classes. This limits the practical and wider application of SSL. To address this problem, this paper proposes a new SSL approach that can classify not only seen classes but also unseen classes. Our approach consists of two modules: unseen class classification and learning pace synchronization. Specifically, we first enable the SSL methods to classify unseen classes by exploiting pairwise similarity between examples and then synchronize the learning pace between seen and unseen classes by proposing an adaptive threshold with distribution alignment. Extensive empirical results show our approach achieves significant performance improvement in both seen and unseen classes compared with previous studies."}}
{"id": "VdQWVdT_8v", "cdate": 1652737477376, "mdate": null, "content": {"title": "LOG: Active Model Adaptation for Label-Efficient OOD Generalization", "abstract": "This work discusses how to achieve worst-case Out-Of-Distribution (OOD) generalization for a variety of distributions based on a relatively small labeling cost. The problem has broad applications, especially in non-i.i.d. open-world scenarios. Previous studies either rely on a large amount of labeling cost or lack of guarantees about the worst-case generalization. In this work, we show for the first time that active model adaptation could achieve both good performance and robustness based on the invariant risk minimization principle. We propose \\textsc{Log}, an interactive model adaptation framework, with two sub-modules: active sample selection and causal invariant learning. Specifically, we formulate the active selection as a mixture distribution separation problem and present an unbiased estimator, which could find the samples that violate the current invariant relationship, with a provable guarantee. The theoretical analysis supports that both sub-modules contribute to generalization. A large number of experimental results confirm the promising performance of the new algorithm."}}
{"id": "8VLGWu658K", "cdate": 1640995200000, "mdate": 1668049032555, "content": {"title": "Active Model Adaptation Under Unknown Shift", "abstract": "Successful machine learning typically relies on fixed data distribution. However, due to unforeseen situations in the open world, distribution shift often occurs in applications. For instance, in the image recognition task, an unpredictable distributional shift may occur due to changes in background or lighting. Furthermore, to alleviate the harm of distribution shift, the resource budget is not infinite and often constrained. To cope with such a novel problem Resource Constrained Adaptation under Unknown Shift, in this paper we study active model adaptation both theoretically and empirically. First, we present a generalization analysis of active model adaptation for distribution shift. In theory, we show that active model adaptation could improve the generalization error from O(1/N) to O(1/N), with only a few queried samples. Second, based on the theoretical analysis, we present a systemic solution Auto, consisting of three sub-steps, that is, distribution tracking, sample selection and model adaptation. Specifically, we design a shifted distribution detection module to locate the distributional shifted samples. To fit the labeling budget, we employ a core-set algorithm to enhance the informativeness of the selected samples. Finally, we update the model through the newly queried labeled data. We conduct empirical studies of nine existing active strategies on diverse real world data sets and the results show that Auto could remarkably outperform all the baselines."}}
{"id": "h0WLyJQnrdH", "cdate": 1609459200000, "mdate": 1668049045098, "content": {"title": "Towards Robust Model Reuse in the Presence of Latent Domains", "abstract": "Model reuse tries to adapt well pre-trained models to a new target task, without access of raw data. It attracts much attention since it reduces the learning resources. Previous model reuse studies typically operate in a single-domain scenario, i.e., the target samples arise from one single domain. However, in practice the target samples often arise from multiple latent or unknown domains, e.g., the images for cars may arise from latent domains such as photo, line drawing, cartoon, etc. The methods based on single-domain may no longer be feasible for multiple latent domains and may sometimes even lead to performance degeneration. To address the above issue, in this paper we propose the MRL (Model Reuse for multiple Latent domains) method. Both domain characteristics and pre-trained models are considered for the exploration of instances in the target task. Theoretically, the overall considerations are packed in a bi-level optimization framework with a reliable generalization. Moreover, through an ensemble of multiple models, the model robustness is improved with a theoretical guarantee. Empirical results on diverse real-world data sets clearly validate the effectiveness of proposed algorithms."}}
{"id": "-awAwSj-1h", "cdate": 1609459200000, "mdate": 1675407505307, "content": {"title": "Learning from Imbalanced and Incomplete Supervision with Its Application to Ride-Sharing Liability Judgment", "abstract": "In multi-label tasks, sufficient and class-balanced label is usually hard to obtain, which makes it challenging to train a good classifier. In this paper, we consider the problem of learning from imbalanced and incomplete supervision, where only a small subset of labeled data is available and the label distribution is highly imbalanced. This setting is of importance and commonly appears in a variety of real applications. For instance, considering the ride-sharing liability judgment task, liability disputes usually due to a variety of reasons, however, it is expensive to manually annotate the reasons, meanwhile, the distribution of reason is often seriously imbalanced. In this paper, we present a systemic framework Limi consisting of three sub-steps, that is, Label Separating, Correlation Mining and Label Completion. Specifically, we propose an effective two-classifier strategy to separately tackle head and tail labels so as to alleviate the performance degradation on tail labels while maintaining high performance on head labels. Then, a novel label correlation network is adopted to explore the label relation knowledge with flexible aggregators. Moreover, the Limi framework completes the label on unlabeled instances in a semi-supervised fashion. The framework is general, flexible, and effective. Extensive experiments on diverse applications, such as the ride-sharing liability judgment task from Didi and various benchmark tasks, demonstrate that our solution is clearly better than many competitive methods."}}
