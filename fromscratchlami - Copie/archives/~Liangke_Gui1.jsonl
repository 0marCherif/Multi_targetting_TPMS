{"id": "qNLe3iq2El", "cdate": 1663849947991, "mdate": null, "content": {"title": "Mega: Moving Average Equipped Gated Attention", "abstract": "The design choices in the Transformer attention mechanism, including weak inductive bias and quadratic computational complexity, have limited its application for modeling long sequences. In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism.  We further propose a variant of Mega that offers linear time and space complexity yet yields only minimal quality loss, by efficiently splitting the whole sequence into multiple chunks with fixed length. Extensive experiments on a wide range of sequence modeling benchmarks, including the Long Range Arena, neural machine translation, auto-regressive language modeling, and image and speech classification, show that Mega achieves significant improvements over other sequence models, including variants of Transformers and recent state space models.\n"}}
{"id": "gk17vkem7S", "cdate": 1640995200000, "mdate": 1667334716457, "content": {"title": "KAT: A Knowledge Augmented Transformer for Vision-and-Language", "abstract": "Liangke Gui, Borui Wang, Qiuyuan Huang, Alexander Hauptmann, Yonatan Bisk, Jianfeng Gao. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022."}}
{"id": "G_EgQ5om37m", "cdate": 1640995200000, "mdate": 1667334716453, "content": {"title": "Training Vision-Language Transformers from Captions Alone", "abstract": "We show that Vision-Language Transformers can be learned without human labels (e.g. class labels, bounding boxes, etc). Existing work, whether explicitly utilizing bounding boxes or patches, assumes that the visual backbone must first be trained on ImageNet class prediction before being integrated into a multimodal linguistic pipeline. We show that this is not necessary and introduce a new model Vision-Language from Captions (VLC) built on top of Masked Auto-Encoders that does not require this supervision. In fact, in a head-to-head comparison between ViLT, the current state-of-the-art patch-based vision-language transformer which is pretrained with supervised object classification, and our model, VLC, we find that our approach 1. outperforms ViLT on standard benchmarks, 2. provides more interpretable and intuitive patch visualizations, and 3. is competitive with many larger models that utilize ROIs trained on annotated bounding-boxes."}}
{"id": "2bD3QerZmTm", "cdate": 1640995200000, "mdate": 1667334716442, "content": {"title": "Mega: Moving Average Equipped Gated Attention", "abstract": "The design choices in the Transformer attention mechanism, including weak inductive bias and quadratic computational complexity, have limited its application for modeling long sequences. In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism. We further propose a variant of Mega that offers linear time and space complexity yet yields only minimal quality loss, by efficiently splitting the whole sequence into multiple chunks with fixed length. Extensive experiments on a wide range of sequence modeling benchmarks, including the Long Range Arena, neural machine translation, auto-regressive language modeling, and image and speech classification, show that Mega achieves significant improvements over other sequence models, including variants of Transformers and recent state space models."}}
{"id": "BhtTa9zNnPB", "cdate": 1609459200000, "mdate": 1667334716453, "content": {"title": "Learning to Hallucinate Examples from Extrinsic and Intrinsic Supervision", "abstract": "Learning to hallucinate additional examples has recently been shown as a promising direction to address few-shot learning tasks. This work investigates two important yet overlooked natural supervision signals for guiding the hallucination process \u2013 (i) extrinsic: classifiers trained on hallucinated examples should be close to strong classifiers that would be learned from a large amount of real examples; and (ii) intrinsic: clusters of hallucinated and real examples belonging to the same class should be pulled together, while simultaneously pushing apart clusters of hallucinated and real examples from different classes. We achieve (i) by introducing an additional mentor model on data-abundant base classes for directing the hallucinator, and achieve (ii) by performing contrastive learning between hallucinated and real examples. As a general, model-agnostic framework, our dual mentor-and self-directed (DMAS) hallucinator significantly improves few-shot learning performance on widely-used benchmarks in various scenarios."}}
{"id": "cj_kBTFuG2z", "cdate": 1577836800000, "mdate": 1667334716454, "content": {"title": "Argus: Efficient Activity Detection System for Extended Video Analysis", "abstract": "We propose an Efficient Activity Detection System, Argus, for Extended Video Analysis in the surveillance scenario. For the spatial-temporal event detection in the surveillance video, we first generate video proposals by applying object detection and tracking algorithm which shared the detection features. After that, we extract several different features and apply sequential activity classification with them. Finally, we eliminate inaccurate events and fuse all the predictions from different features. The proposed system wins Trecvid Activities in Extended Video (ActEV <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> ) challenge 2019. It achieves the first place with 60.5 mean weighted P <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">miss</sub> , outperforming the second place system by 14.5 and the baseline R-C3D by 29.0. In TRECVID 2019 Challenge <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup> , the proposed system wins the first place with pAUDC@0.2tfa 0.48407."}}
{"id": "_y1qjAMrgK", "cdate": 1546300800000, "mdate": 1667334716497, "content": {"title": "MMVG-INF-Etrol@TRECVID 2019: Activities in Extended Video", "abstract": ""}}
{"id": "ma_KW4P1KE", "cdate": 1514764800000, "mdate": 1667334716507, "content": {"title": "Factorized Convolutional Networks: Unsupervised Fine-Tuning for Image Clustering", "abstract": "Deep convolutional neural networks (CNNs) have recognized promise as universal representations for various image recognition tasks. One of their properties is the ability to transfer knowledge from a large annotated source dataset (e.g., ImageNet) to a (typically smaller) target dataset. This is usually accomplished through supervised fine-tuning on labeled new target data. In this work, we address \"unsupervised fine-tuning\" that transfers a pre-trained network to target tasks with unlabeled data such as image clustering tasks. To this end, we introduce group-sparse non-negative matrix factorization (GSNMF), a variant of NMF, to identify a rich set of high-level latent variables that are informative on the target task. The resulting \"factorized convolutional network\" (FCN) can itself be seen as a feed-forward model that combines CNN and two-layer structured NMF. We empirically validate our approach and demonstrate state-of-the-art image clustering performance on challenging scene (MIT-67) and fine-grained (Birds-200, Flowers-102) benchmarks. We further show that, when used as unsupervised initialization, our approach improves image classification performance as well."}}
{"id": "R5O09FvJl-B", "cdate": 1514764800000, "mdate": 1667334716496, "content": {"title": "Adaptive Context-aware Reinforced Agent for Handwritten Text Recognition", "abstract": ""}}
{"id": "r1VK3TxdbH", "cdate": 1483228800000, "mdate": null, "content": {"title": "Integrating Verbal and Nonvebval Input into a Dynamic Response Spoken Dialogue System", "abstract": "In this work, we present a dynamic response spoken dialogue system (DRSDS). It is capable of understanding the verbal and nonverbal language of users and making instant, situation-aware response. Incorporating with two external systems, MultiSense and email summarization, we built an email reading agent on mobile device to show the functionality of DRSDS."}}
