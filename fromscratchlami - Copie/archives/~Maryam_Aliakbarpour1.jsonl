{"id": "kF0991ySpfC", "cdate": 1672531200000, "mdate": 1695350561058, "content": {"title": "Testing Tail Weight of a Distribution Via Hazard Rate", "abstract": "Understanding the shape of a distribution of data is of interest to people in a great variety of fields, as it may affect the types of algorithms used for that data. We study one such problem in th..."}}
{"id": "6Z1Nk2bBZH", "cdate": 1672531200000, "mdate": 1695350561551, "content": {"title": "Differentially Private Medians and Interior Points for Non-Pathological Data", "abstract": "We construct differentially private estimators with low sample complexity that estimate the median of an arbitrary distribution over $\\mathbb{R}$ satisfying very mild moment conditions. Our result stands in contrast to the surprising negative result of Bun et al. (FOCS 2015) that showed there is no differentially private estimator with any finite sample complexity that returns any non-trivial approximation to the median of an arbitrary distribution."}}
{"id": "pV7f1Rq71I5", "cdate": 1652737825552, "mdate": null, "content": {"title": "Estimation of Entropy in Constant Space with Improved Sample Complexity", "abstract": "Recent work of Acharya et al.~(NeurIPS 2019) showed how to estimate the entropy of a distribution $\\mathcal D$ over an alphabet of size $k$ up to $\\pm\\epsilon$ additive error by streaming over $(k/\\epsilon^3) \\cdot \\text{polylog}(1/\\epsilon)$ i.i.d.\\ samples and using only $O(1)$ words of memory. In this work, we give a new constant memory scheme that reduces the sample complexity to $(k/\\epsilon^2)\\cdot \\text{polylog}(1/\\epsilon)$. We conjecture that this is optimal up to $\\text{polylog}(1/\\epsilon)$ factors."}}
{"id": "oasnzSimJ3", "cdate": 1640995200000, "mdate": 1695350561395, "content": {"title": "Estimation of Entropy in Constant Space with Improved Sample Complexity", "abstract": "Recent work of Acharya et al.~(NeurIPS 2019) showed how to estimate the entropy of a distribution $\\mathcal D$ over an alphabet of size $k$ up to $\\pm\\epsilon$ additive error by streaming over $(k/\\epsilon^3) \\cdot \\text{polylog}(1/\\epsilon)$ i.i.d.\\ samples and using only $O(1)$ words of memory. In this work, we give a new constant memory scheme that reduces the sample complexity to $(k/\\epsilon^2)\\cdot \\text{polylog}(1/\\epsilon)$. We conjecture that this is optimal up to $\\text{polylog}(1/\\epsilon)$ factors."}}
{"id": "4LQaBaDW08H", "cdate": 1640995200000, "mdate": 1695350561693, "content": {"title": "Estimation of Entropy in Constant Space with Improved Sample Complexity", "abstract": "Recent work of Acharya et al. (NeurIPS 2019) showed how to estimate the entropy of a distribution $\\mathcal D$ over an alphabet of size $k$ up to $\\pm\\epsilon$ additive error by streaming over $(k/\\epsilon^3) \\cdot \\text{polylog}(1/\\epsilon)$ i.i.d. samples and using only $O(1)$ words of memory. In this work, we give a new constant memory scheme that reduces the sample complexity to $(k/\\epsilon^2)\\cdot \\text{polylog}(1/\\epsilon)$. We conjecture that this is optimal up to $\\text{polylog}(1/\\epsilon)$ factors."}}
{"id": "suVe70NghH", "cdate": 1609459200000, "mdate": 1684373644815, "content": {"title": "Rapid Approximate Aggregation with Distribution-Sensitive Interval Guarantees", "abstract": "Aggregating data is fundamental to data analytics, data exploration, and OLAP. Approximate query processing (AQP) techniques are often used to accelerate computation of aggregates using samples, for which confidence intervals (CIs) are widely used to quantify the associated error. CIs used in practice fall into two categories: techniques that are tight but not correct, i.e., they yield tight intervals but only offer asymptoticguarantees,makingthem unreliable, or techniques that are correct but not tight, i.e., they offer rigorous guarantees, but are overly conservative, leading to confidence intervals that are too loose to be useful. In this paper, we develop a CI technique that is both correct and tighter than traditional approaches. Starting from conservative CIs, we identify two issues they often face: pessimistic mass allocation (PMA) and phantom outlier sensitivity (PHOS). By developing a novel range-trimming technique for eliminating PHOS and pairing it with known CI techniques without PMA, we develop a technique for computing CIs with strong guarantees that requires fewer samples for the same width. We implement our techniques underneath a sampling-optimized in-memory column store and show how they accelerate queries involving aggregates on real datasets with typical speedups on the order of 10\u00d7 over both traditional AQP-with-guarantees and exact methods, all while obeying accuracy constraints."}}
{"id": "WHgY0Nka56", "cdate": 1609459200000, "mdate": 1682717216996, "content": {"title": "Local Differential Privacy Is Equivalent to Contraction of an $f$-Divergence", "abstract": "We investigate the local differential privacy (LDP) guarantees of a randomized privacy mechanism via its contraction properties. We first show that LDP constraints can be equivalently cast in terms of the contraction coefficient of the <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathsf{E}_{\\gamma}$</tex> -divergence. We then use this equivalent formula to express LDP guarantees of privacy mechanisms in terms of contraction coefficients of arbitrary <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$f$</tex> -divergences. When combined with standard estimation-theoretic tools (such as Le Cam's and Fano's converse methods), this result allows us to study the trade-off between privacy and utility in several testing and minimax and Bayesian estimation problems."}}
{"id": "HNirJwJACfj", "cdate": 1609459200000, "mdate": 1682717216997, "content": {"title": "Local Differential Privacy Is Equivalent to Contraction of E\u03b3-Divergence", "abstract": "We investigate the local differential privacy (LDP) guarantees of a randomized privacy mechanism via its contraction properties. We first show that LDP constraints can be equivalently cast in terms of the contraction coefficient of the $E_\\gamma$-divergence. We then use this equivalent formula to express LDP guarantees of privacy mechanisms in terms of contraction coefficients of arbitrary $f$-divergences. When combined with standard estimation-theoretic tools (such as Le Cam's and Fano's converse methods), this result allows us to study the trade-off between privacy and utility in several testing and minimax and Bayesian estimation problems."}}
{"id": "Rdtjf1Ld5RJ", "cdate": 1599699500548, "mdate": null, "content": {"title": "Testing Determinantal Point Processes", "abstract": "Determinantal point processes (DPPs) are popular probabilistic models of diversity. In this paper,\nwe investigate DPPs from a new perspective: property testing of distributions. Given sample access\nto an unknown distribution q over the subsets of a ground set, we aim to distinguish whether q is\na DPP distribution, or \u01eb-far from all DPP distributions in \u21131-distance. In this work, we propose the\nfirst algorithm for testing DPPs. Furthermore, we establish a matching lower bound on the sample\ncomplexity of DPP testing. This lower bound also extends to showing a new hardness result for the\nproblem of testing the more general class of log-submodular distributions."}}
{"id": "ejy65I4mTC9", "cdate": 1599699334537, "mdate": null, "content": {"title": "Testing Properties of Multiple Distributions with Few Samples", "abstract": "We propose a new setting for testing properties of distributions while receiving samples from several distributions, but few samples per distribution. Given samples from s distributions, p1, p2, . . . , ps,\nwe design testers for the following problems: (1) Uniformity Testing: Testing whether all the pi\u2019s are\nuniform or \u01eb-far from being uniform in \u21131-distance (2) Identity Testing: Testing whether all the pi\u2019s\nare equal to an explicitly given distribution q or \u01eb-far from q in \u21131-distance, and (3) Closeness Testing:\nTesting whether all the pi\u2019s are equal to a distribution q which we have sample access to, or \u01eb-far from\nq in \u21131-distance. By assuming an additional natural condition about the source distributions, we provide\nsample optimal testers for all of these problems."}}
