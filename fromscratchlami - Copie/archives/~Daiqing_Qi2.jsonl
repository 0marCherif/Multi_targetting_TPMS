{"id": "CcgqO7j5yWE", "cdate": 1672531200000, "mdate": 1683878932322, "content": {"title": "Better Generative Replay for Continual Federated Learning", "abstract": "Federated learning is a technique that enables a centralized server to learn from distributed clients via communications without accessing the client local data. However, existing federated learning works mainly focus on a single task scenario with static data. In this paper, we introduce the problem of continual federated learning, where clients incrementally learn new tasks and history data cannot be stored due to certain reasons, such as limited storage and data retention policy. Generative replay based methods are effective for continual learning without storing history data, but adapting them for this setting is challenging. By analyzing the behaviors of clients during training, we find that the unstable training process caused by distributed training on non-IID data leads to a notable performance degradation. To address this problem, we propose our FedCIL model with two simple but effective solutions: model consolidation and consistency enforcement. Our experimental results on multiple benchmark datasets demonstrate that our method significantly outperforms baselines."}}
{"id": "cRxYWKiTan", "cdate": 1663850075992, "mdate": null, "content": {"title": "Better Generative Replay for Continual Federated Learning", "abstract": "Federated Learning (FL) aims to develop a centralized server that learns from distributed clients via communications without accessing the clients\u2019 local data. However, existing works mainly focus on federated learning in a single task sce- nario with static data. In this paper, we introduce the continual federated learning (CFL) problem, where clients incrementally learn new tasks and history data can- not be stored due to certain reasons, such as limited storage and data retention policy 1. Generative replay (GR) based methods are effective for continual learning without storing history data. However, we fail when trying to intuitively adapt GR models for this setting. By analyzing the behaviors of clients during training, we find the unstable training process caused by distributed training on non-IID data leads to a notable performance degradation. To address this problem, we propose our FedCIL model with two simple but effective solutions: 1. model consolidation and 2. consistency enforcement. Experimental results on multiple benchmark datasets demonstrate that our method significantly outperforms baselines. Code is available at: https://github.com/daiqing98/FedCIL."}}
{"id": "RtB4CXS1Jxv", "cdate": 1663850075123, "mdate": null, "content": {"title": "Data-Free Continual Graph Learning ", "abstract": "Graph Neural Networks (GNNs), which effectively learn from static graph-structured data become ineffective when directly applied to streaming data in a continual learning (CL) scenario. A few recent works study this so-called \u201ccatastrophic forgetting\u201d problem in GNNs, where historical data are not available during the training stage. However, they make a strong assumption that full access of historical data is provided during the inference stage. This assumption could make the graph learning system impractical to deploy due to a number of reasons, such as limited storage, GDPR1 data retention policy, to name a few. In this work, we study continual graph learning without this strong assumption. Moreover, in practical continual learning, models are sometimes trained with accumulated batch data but required to do on-the-fly inference with a stream of test samples. In this case, without being re-inserted into previous training graphs for inference, streaming test nodes are often very sparsely connected. It makes the inference more difficult as the model is trained on a much more dense graph while required to infer on a sparse graph with insufficient neighborhood information. We propose a simple Replay GNN (ReGNN) to jointly solve the above two challenges without memory buffers (i.e., data-free): catastrophic forgetting and poor neighbour information during inference. Extensive experiments demonstrate the effectiveness of our model over baseline models, including competitive baselines with memory buffers."}}
{"id": "SM-fwAOzYcX", "cdate": 1640995200000, "mdate": 1683878932435, "content": {"title": "Knowledge-Guided Article Embedding Refinement for Session-Based News Recommendation", "abstract": "Personalized news recommendation aims to recommend news articles to customers, by exploiting the personal preferences and short-term reading interest of users. A practical challenge in personalized news recommendations is the lack of logged user interactions. Recently, the session-based news recommendation has attracted increasing attention, which tries to recommend the next news article given previous articles in an active session. Current session-based news recommendation methods mainly extract latent embeddings from news articles and user-item interactions. However, many existing methods could not exploit the semantic-level structural information among news articles. And the feature learning process simply relies on the news articles in training data, which may not be sufficient to learn semantically rich embeddings. This brief presents a context-aware graph embedding (CAGE) approach for session-based news recommendation. It employs external knowledge graphs to improve the semantic-level representations of news articles. Moreover, graph neural networks are incorporated to further enhance the article embeddings. In addition, we consider the similarity among sessions and design attention neural networks to model the short-term user preferences. Extensive results on multiple news recommendation benchmark datasets show that CAGE performs better than some competitive baselines in most cases."}}
{"id": "_5CTXGI1kv1", "cdate": 1577836800000, "mdate": null, "content": {"title": "Less is More: Data-Efficient Complex Question Answering over Knowledge Bases", "abstract": "Question answering is an effective method for obtaining information from knowledge bases (KB). In this paper, we propose the Neural-Symbolic Complex Question Answering (NS-CQA) model, a data-efficient reinforcement learning framework for complex question answering by using only a modest number of training samples. Our framework consists of a neural generator and a symbolic executor that, respectively, transforms a natural-language question into a sequence of primitive actions, and executes them over the knowledge base to compute the answer. We carefully formulate a set of primitive symbolic actions that allows us to not only simplify our neural network design but also accelerate model convergence. To reduce search space, we employ the copy and masking mechanisms in our encoder-decoder architecture to drastically reduce the decoder output vocabulary and improve model generalizability. We equip our model with a memory buffer that stores high-reward promising programs. Besides, we propose an adaptive reward function. By comparing the generated trial with the trials stored in the memory buffer, we derive the curriculum-guided reward bonus, i.e., the proximity and the novelty. To mitigate the sparse reward problem, we combine the adaptive reward and the reward bonus, reshaping the sparse reward into dense feedback. Also, we encourage the model to generate new trials to avoid imitating the spurious trials while making the model remember the past high-reward trials to improve data efficiency. Our NS-CQA model is evaluated on two datasets: CQA, a recent large-scale complex question answering dataset, and WebQuestionsSP, a multi-hop question answering dataset. On both datasets, our model outperforms the state-of-the-art models. Notably, on CQA, NS-CQA performs well on questions with higher complexity, while only using approximately 1% of the total training samples."}}
{"id": "KBA2LQeCQX", "cdate": 1577836800000, "mdate": 1667372008067, "content": {"title": "Less is more: Data-efficient complex question answering over knowledge bases", "abstract": ""}}
{"id": "ZNWpPWphl5g", "cdate": 1514764800000, "mdate": 1648731987226, "content": {"title": "Semantic Parsing for Multiple-relation Chinese Question Answering", "abstract": ""}}
