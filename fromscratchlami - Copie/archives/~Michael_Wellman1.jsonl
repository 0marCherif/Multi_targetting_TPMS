{"id": "sdIEwUcB7V", "cdate": 1672531200000, "mdate": 1681686720913, "content": {"title": "Regularization for Strategy Exploration in Empirical Game-Theoretic Analysis", "abstract": "In iterative approaches to empirical game-theoretic analysis (EGTA), the strategy space is expanded incrementally based on analysis of intermediate game models. A common approach to strategy exploration, represented by the double oracle algorithm, is to add strategies that best-respond to a current equilibrium. This approach may suffer from overfitting and other limitations, leading the developers of the policy-space response oracle (PSRO) framework for iterative EGTA to generalize the target of best response, employing what they term meta-strategy solvers (MSSs). Noting that many MSSs can be viewed as perturbed or approximated versions of Nash equilibrium, we adopt an explicit regularization perspective to the specification and analysis of MSSs. We propose a novel MSS called regularized replicator dynamics (RRD), which simply truncates the process based on a regret criterion. We show that RRD is more adaptive than existing MSSs and outperforms them in various games. We extend our study to three-player games, for which the payoff matrix is cubic in the number of strategies and so exhaustively evaluating profiles may not be feasible. We propose a profile search method that can identify solutions from incomplete models, and combine this with iterative model construction using a regularized MSS. Finally, and most importantly, we reveal that the regret of best response targets has a tremendous influence on the performance of strategy exploration through experiments, which provides an explanation for the effectiveness of regularization in PSRO."}}
{"id": "YlXnJAL-Yf", "cdate": 1672531200000, "mdate": 1681686720786, "content": {"title": "Exploiting Extensive-Form Structure in Empirical Game-Theoretic Analysis", "abstract": "Empirical game-theoretic analysis (EGTA) is a general framework for reasoning about complex games using agent-based simulation. Data from simulating select strategy profiles is employed to estimate a cogent and tractable game model approximating the underlying game. To date, EGTA methodology has focused on game models in normal form; though the simulations play out in sequential observations and decisions over time, the game model abstracts away this temporal structure. Richer models of \\textit{extensive-form games} (EFGs) provide a means to capture temporal patterns in action and information, using tree representations. We propose \\textit{tree-exploiting EGTA} (TE-EGTA), an approach to incorporate EFG models into EGTA\\@. TE-EGTA constructs game models that express observations and temporal organization of activity, albeit at a coarser grain than the underlying agent-based simulation model. The idea is to exploit key structure while maintaining tractability. We establish theoretically and experimentally that exploiting even a little temporal structure can vastly reduce estimation error in strategy-profile payoffs compared to the normal-form model. Further, we explore the implications of EFG models for iterative approaches to EGTA, where strategy spaces are extended incrementally. Our experiments on several game instances demonstrate that TE-EGTA can also improve performance in the iterative setting, as measured by the quality of equilibrium approximation as the strategy spaces are expanded."}}
{"id": "MLC1TLxOWwY", "cdate": 1672531200000, "mdate": 1681559408528, "content": {"title": "Combining Tree-Search, Generative Models, and Nash Bargaining Concepts in Game-Theoretic Reinforcement Learning", "abstract": ""}}
{"id": "xBFyJfm5WXR", "cdate": 1640995200000, "mdate": 1681686721403, "content": {"title": "Exploiting Extensive-Form Structure in Empirical Game-Theoretic Analysis", "abstract": "Empirical game-theoretic analysis (EGTA) is a general framework for reasoning about complex games using agent-based simulation. Data from simulating select strategy profiles is employed to estimate a cogent and tractable game model approximating the underlying game. To date, EGTA methodology has focused on game models in normal form; though the simulations play out in sequential observations and decisions over time, the game model abstracts away this temporal structure. Richer models of extensive-form games (EFGs) provide a means to capture temporal patterns in action and information, using tree representations. We propose tree-exploiting EGTA (TE-EGTA), an approach to incorporate EFG models into EGTA. TE-EGTA constructs game models that express observations and temporal organization of activity, albeit at a coarser grain than the underlying agent-based simulation model. The idea is to exploit key structure while maintaining tractability. We establish theoretically and experimentally that exploiting even a little temporal structure can vastly reduce estimation error in strategy-profile payoffs compared to the normal-form model. Further, we explore the implications of EFG models for iterative approaches to EGTA, where strategy spaces are extended incrementally. Our experiments on several game instances demonstrate that TE-EGTA can also improve performance in the iterative setting, as measured by the quality of equilibrium approximation as the strategy spaces are expanded."}}
{"id": "KNRCCNUaOL_", "cdate": 1640995200000, "mdate": 1681686719222, "content": {"title": "Evaluating Strategy Exploration in Empirical Game-Theoretic Analysis", "abstract": ""}}
{"id": "KdWnM6Xj8KX", "cdate": 1632875520898, "mdate": null, "content": {"title": "Regularization for Strategy Exploration in Empirical Game-Theoretic Analysis", "abstract": "In iterative approaches to empirical game-theoretic analysis (EGTA), the strategy space is expanded incrementally based on analysis of intermediate game models. A common approach to strategy exploration, represented by the double oracle algorithm, is to add strategies that best-respond to a current equilibrium. This approach may suffer from overfitting and other limitations, leading the developers of the policy-space response oracle (PSRO) framework for iterative EGTA to generalize the target of best response, employing what they term meta-strategy solvers (MSSs). Noting that many MSSs can be viewed as perturbed or approximated versions of Nash equilibrium, we adopt an explicit regularization perspective to the specification and analysis of MSSs. We propose a novel MSS called regularized replicator dynamics (RRD), which simply truncates the process based on a regret criterion. We show that the regularization approach exhibits desired properties for strategy exploration and RRD outperforms existing MSSs in various games. We extend our study to three-player games, for which the payoff matrix is cubic in the number of strategies and so exhaustively evaluating profiles may not be feasible. We propose a profile search method that can identify solutions from incomplete models, and combine this with iterative model construction using a regularized MSS. Finally, we suggest an explanation for the effectiveness of regularization demonstrated in our experiments.\n"}}
{"id": "AxWLtSsRtyd", "cdate": 1628612052849, "mdate": 1628612052849, "content": {"title": "Learning to Play Against Any Mixture of Opponents", "abstract": "Intuitively, experience playing against one mixture\nof opponents in a given domain should be relevant\nfor a different mixture in the same domain.\nWe propose a transfer learning method, Q-Mixing,\nthat starts by learning Q-values against each purestrategy\nopponent. Then a Q-value for any distribution\nof opponent strategies is approximated\nby appropriately averaging the separately learned\nQ-values. From these components, we construct\npolicies against all opponent mixtures without\nany further training. We empirically validate QMixing\nin two environments: a simple grid-world\nsoccer environment, and a social dilemma game.\nWe find that Q-Mixing is able to successfully\ntransfer knowledge across any mixture of opponents.\nWe next consider the use of observations\nduring play to update the believed distribution of\nopponents. We introduce an opponent classifier\u2014\ntrained in parallel to Q-learning, reusing data\u2014\nand use the classifier results to refine the mixing\nof Q-values. We find that Q-Mixing augmented\nwith the opponent policy classifier performs better,\nwith higher variance, than training directly\nagainst a mixed-strategy opponent."}}
{"id": "y5zl2jMtb9Sl", "cdate": 1609459200000, "mdate": 1681686721858, "content": {"title": "A strategic analysis of portfolio compression", "abstract": "Portfolio compression, the elimination of debt cycles in a financial network, is employed in over-the-counter derivative markets as a method for simplifying balance sheets. Canceling debts through compression can sometimes promote financial stability by stopping the spread of contagion from an insolvent firm hit by a negative financial shock. However, previous work has demonstrated that in some cases compression can exacerbate systemic risk by removing paths of shock absorption. We analyze portfolio compression as a strategic decision made by firms within a debt network. We define a network game in which firms represented as nodes have only local information and we ask what criteria the firms should consider in their decision to compress. We propose a variety of heuristic strategies and evaluate them using agent-based simulation and empirical game-theoretic analysis. In our experiments, compression may or may not benefit nodes on a cycle, and our results show that simple strategies based on local information can effectively improve firms' decision making. We further examine which features are most useful under various conditions, and find that the results depend on the rate at which assets can be recovered from insolvent nodes. When recovery rates are low, firms focus on avoiding contagion, and when they are high they consider the benefits of retaining cycles to cushion shocks. Finally, we then analyze the effects on systemic risk in equilibrium, finding that when nodes strategically choose to compress, the outcome is more likely to be beneficial than harmful."}}
{"id": "y1YgOKlgnNW", "cdate": 1609459200000, "mdate": 1681686721141, "content": {"title": "Iterative Empirical Game Solving via Single Policy Best Response", "abstract": "Policy-Space Response Oracles (PSRO) is a general algorithmic framework for learning policies in multiagent systems by interleaving empirical game analysis with deep reinforcement learning (Deep RL). At each iteration, Deep RL is invoked to train a best response to a mixture of opponent policies. The repeated application of Deep RL poses an expensive computational burden as we look to apply this algorithm to more complex domains. We introduce two variations of PSRO designed to reduce the amount of simulation required during Deep RL training. Both algorithms modify how PSRO adds new policies to the empirical game, based on learned responses to a single opponent policy. The first, Mixed-Oracles, transfers knowledge from previous iterations of Deep RL, requiring training only against the opponent's newest policy. The second, Mixed-Opponents, constructs a pure-strategy opponent by mixing existing strategy's action-value estimates, instead of their policies. Learning against a single policy mitigates variance in state outcomes that is induced by an unobserved distribution of opponents. We empirically demonstrate that these algorithms substantially reduce the amount of simulation during training required by PSRO, while producing equivalent or better solutions to the game."}}
{"id": "qAOFt0Gfk39", "cdate": 1609459200000, "mdate": 1681686721101, "content": {"title": "Spoofing the Limit Order Book: A Strategic Agent-Based Analysis", "abstract": "We present an agent-based model of manipulating prices in financial markets through spoofing: submitting spurious orders to mislead traders who learn from the order book. Our model captures a complex market environment for a single security, whose common value is given by a dynamic fundamental time series. Agents trade through a limit-order book, based on their private values and noisy observations of the fundamental. We consider background agents following two types of trading strategies: the non-spoofable zero intelligence (ZI) that ignores the order book and the manipulable heuristic belief learning (HBL) that exploits the order book to predict price outcomes. We conduct empirical game-theoretic analysis upon simulated agent payoffs across parametrically different environments and measure the effect of spoofing on market performance in approximate strategic equilibria. We demonstrate that HBL traders can benefit price discovery and social welfare, but their existence in equilibrium renders a market vulnerable to manipulation: simple spoofing strategies can effectively mislead traders, distort prices and reduce total surplus. Based on this model, we propose to mitigate spoofing from two aspects: (1) mechanism design to disincentivize manipulation; and (2) trading strategy variations to improve the robustness of learning from market information. We evaluate the proposed approaches, taking into account potential strategic responses of agents, and characterize the conditions under which these approaches may deter manipulation and benefit market welfare. Our model provides a way to quantify the effect of spoofing on trading behavior and market efficiency, and thus it can help to evaluate the effectiveness of various market designs and trading strategies in mitigating an important form of market manipulation."}}
