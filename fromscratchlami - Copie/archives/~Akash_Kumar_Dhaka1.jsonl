{"id": "kQtxXZsFtr", "cdate": 1680191544289, "mdate": null, "content": {"title": "Robust, Accurate Stochastic Optimization for Variational Inference", "abstract": "We consider the problem of fitting variational posterior approximations using\nstochastic optimization methods. The performance of these approximations de-\npends on (1) how well the variational family matches the true posterior distribution,\n(2) the choice of divergence, and (3) the optimization of the variational objective.\nWe show that even in the best-case scenario when the exact posterior belongs to\nthe assumed variational family, common stochastic optimization methods lead to\npoor variational approximations if the problem dimension is moderately large. We\nalso demonstrate that these methods are not robust across diverse model types.\nMotivated by these findings, we develop a more robust and accurate stochastic\noptimization framework by viewing the underlying optimization algorithm as pro-\nducing a Markov chain. Our approach is theoretically motivated and includes a\ndiagnostic for convergence and a novel stopping rule, both of which are robust to\nnoisy evaluations of the objective function. We show empirically that the proposed\nframework works well on a diverse set of models: it can automatically detect\nstochastic optimization failure or inaccurate variational approximation."}}
{"id": "23WtTkwyzLJ", "cdate": 1622637630599, "mdate": null, "content": {"title": "Challenges for BBVI with Normalizing Flows", "abstract": "Current black-box variational inference (BBVI) methods require the user to make numerous design choices---such as the selection of variational objective and approximating family---yet there is little principled guidance on how to do so. We develop a conceptual framework and set of experimental tools to understand the effects of these choices, which we leverage to propose best practices for maximizing posterior approximation accuracy. Our approach is based on studying the pre-asymptotic tail behavior of the density ratios between the joint distribution and the variational approximation, then exploiting insights and tools from the importance sampling literature. We focus on normalizing flow models and give recommendations on how to be used(and diagnostics) in BBVI, though we are not limited to them."}}
{"id": "_A4-JP8d_f", "cdate": 1621629760547, "mdate": null, "content": {"title": "Challenges and Opportunities in High Dimensional Variational Inference", "abstract": "Current black-box variational inference (BBVI) methods require the user to make numerous design choices \u2013 such as the selection of variational objective and approximating family \u2013 yet there is little principled guidance on how to do so. We develop a conceptual framework and set of experimental tools to understand the effects of these choices, which we leverage to propose best practices for maximizing posterior approximation accuracy. Our approach is based on studying the pre-asymptotic tail behavior of the density ratios between the joint distribution and the variational approximation, then exploiting insights and tools from the importance sampling literature. Our framework and supporting experiments help to distinguish between the behavior of BBVI methods for approximating low-dimensional versus moderate-to-high-dimensional posteriors. In the latter case, we show that mass-covering variational objectives are difficult to optimize and do not improve accuracy, but flexible variational families can improve accuracy and the effectiveness of importance sampling \u2013 at the cost of additional optimization challenges. Therefore, for moderate-to-high-dimensional posteriors we recommend using the (mode-seeking) exclusive KL divergence since it is the easiest to optimize, and improving the variational family or using model parameter transformations to make the posterior and optimal variational approximation more similar. On the other hand, in low-dimensional settings, we show that heavy-tailed variational families and mass-covering divergences are effective and can increase the chances that the approximation can be improved by importance sampling. "}}
