{"id": "fDWNnSiHeka", "cdate": 1652737808394, "mdate": null, "content": {"title": "Sketching based Representations for Robust Image Classification with Provable Guarantees", "abstract": "How do we provably represent images succinctly so that their essential latent attributes are correctly captured by the representation to as high level of detail as possible? While today's deep networks (such as CNNs)  produce image embeddings they do not have any provable properties and seem to work in mysterious non-interpretable ways. In this work we theoretically study synthetic images that are composed of a union or intersection of several mathematically specified shapes using thresholded polynomial functions (for e.g. ellipses, rectangles).  We show how to produce a succinct sketch of such an image so that the sketch \u201csmoothly\u201d maps to the latent-coefficients producing the different shapes in the image.  We prove several important properties  such as: easy reconstruction of the image from the sketch, similarity preservation (similar shapes produce similar sketches), being able to index sketches so that other similar images and parts of other images can be retrieved,  being able to store the sketches into a dictionary of concepts and shapes so parts of the same or different images that refer to the same shape can point to the same entry in this dictionary of common shape attributes."}}
{"id": "AODVskSug8", "cdate": 1652737802610, "mdate": null, "content": {"title": "A Theoretical View on Sparsely Activated Networks", "abstract": "Deep and wide neural networks successfully fit very complex functions today, but dense models are starting to be prohibitively expensive for inference. To mitigate this, one promising research direction is networks that activate a sparse subgraph of the network. The subgraph is chosen by a data-dependent routing function, enforcing a fixed mapping of inputs to subnetworks (e.g., the Mixture of Experts (MoE) paradigm in Switch Transformers). However, there is no theoretical grounding for these sparsely activated models. As our first contribution, we present a formal model of data-dependent sparse networks that captures salient aspects of popular architectures. Then, we show how to construct sparse networks that provably match the approximation power and total size of dense networks on Lipschitz functions. The sparse networks use much fewer inference operations than dense networks, leading to a faster forward pass. The key idea is to use locality sensitive hashing on the input vectors and then interpolate the function in subregions of the input space. This offers a theoretical insight into why sparse networks work well in practice. Finally, we present empirical findings that support our theory; compared to dense networks, sparse networks give a favorable trade-off between number of active units and approximation quality."}}
{"id": "uut_j3UrRCg", "cdate": 1632875690143, "mdate": null, "content": {"title": "Provable hierarchical lifelong learning with a sketch-based modular architecture", "abstract": "We propose a modular architecture for lifelong learning of hierarchically structured tasks. Specifically, we prove that our architecture is theoretically able to learn tasks that can be solved by functions that are learnable given access to functions for other, previously learned tasks as subroutines. We show that some tasks that we can learn in this way are not learned by standard training methods in practice; indeed, prior work suggests that some such tasks cannot be learned by \\emph{any} efficient method without the aid of the simpler tasks. We also consider methods for identifying the tasks automatically, without relying on explicitly given indicators."}}
{"id": "ZTZa78mCbie", "cdate": 1632875470617, "mdate": null, "content": {"title": "For Manifold Learning, Deep Neural Networks Can be Locality Sensitive Hash Functions", "abstract": "It is well established that training deep neural networks gives useful representations that capture essential features of the inputs. However, these representations are poorly understood in theory and practice. In the context of supervised learning an important question is whether these representations capture features informative for classification, while filtering out non-informative noisy ones. We present a formal framework to study this question by considering a generative process where each class is associated with a high-dimensional manifold and different classes define different manifolds. Under this model, each input is produced using two latent vectors: (i) a ``manifold identifier\"  $\\gamma$ and; (ii)~a ``transformation parameter\" $\\theta$ that shifts examples along the surface of a manifold. E.g., $\\gamma$ might represent a canonical image of a dog, and $\\theta$ might stand for variations in pose, background or lighting. We provide theoretical evidence that neural representations can be viewed as LSH-like functions that map each input to an embedding that is a function of solely the informative $\\gamma$ and invariant to $\\theta$, effectively recovering the manifold identifier . We formally show that we get one-shot learning to unseen classes as an important consequence of this behavior."}}
{"id": "uz5uw6gM0m", "cdate": 1601308048675, "mdate": null, "content": {"title": "One Network Fits All? Modular versus Monolithic Task Formulations in Neural Networks", "abstract": "Can deep learning solve multiple, very different tasks simultaneously? We investigate how the representations of the underlying tasks affect the ability of a single neural network to learn them jointly. We present theoretical and empirical findings that a single neural network is capable of simultaneously learning multiple tasks from a combined data set, for a variety of methods for representing tasks---for example, when the distinct tasks are encoded by well-separated clusters or decision trees over some task-code attributes. Indeed, more strongly, we present a novel analysis that shows that families of simple programming-like constructs for the codes encoding the tasks are learnable by two-layer neural networks with standard training. We study more generally how the complexity of learning such combined tasks grows with the complexity of the task codes; we find that learning many tasks can be provably hard, even though the individual tasks are easy to learn. We provide empirical support for the usefulness of the learning bounds by training networks on clusters, decision trees, and SQL-style aggregation."}}
{"id": "rkEmH2WdbB", "cdate": 1546300800000, "mdate": null, "content": {"title": "Recursive Sketches for Modular Deep Learning", "abstract": "We present a mechanism to compute a sketch (succinct summary) of how a complex modular deep network processes its inputs. The sketch summarizes essential information about the inputs and outputs of..."}}
{"id": "SkWc6qbObB", "cdate": 1546300800000, "mdate": null, "content": {"title": "Faster Algorithms for Binary Matrix Factorization", "abstract": "We give faster approximation algorithms for well-studied variants of Binary Matrix Factorization (BMF), where we are given a binary $m \\times n$ matrix $A$ and would like to find binary rank-$k$ ma..."}}
{"id": "BkgFqiAqFX", "cdate": 1538087824525, "mdate": null, "content": {"title": "Recovering the Lowest Layer of Deep Networks with High Threshold Activations", "abstract": "Giving provable guarantees for learning neural networks is a core challenge of machine learning theory. Most prior work gives parameter recovery guarantees for one hidden layer networks, however, the networks used in practice have multiple non-linear layers. In this work, we show how we can strengthen such results to deeper networks -- we address the problem of uncovering the lowest layer in a deep neural network under the assumption that the lowest layer uses a high threshold before applying the activation, the upper network can be modeled as a well-behaved polynomial and the input distribution is gaussian."}}
{"id": "H1ZRViWdZH", "cdate": 1483228800000, "mdate": null, "content": {"title": "Algorithms for $\\ell_p$ Low-Rank Approximation", "abstract": "We consider the problem of approximating a given matrix by a low-rank matrix so as to minimize the entrywise $\\ell_p$-approximation error, for any $p \\geq 1$; the case $p = 2$ is the classical SVD ..."}}
{"id": "rJWcknZOZH", "cdate": 1388534400000, "mdate": null, "content": {"title": "Learning Polynomials with Neural Networks", "abstract": "We study the effectiveness of learning low degree polynomials using neural networks by the gradient descent method. While neural networks have been shown to have great expressive power, and gradient descent has been widely used in practice for learning neural networks, few theoretical guarantees are known for such methods. In particular, it is well known that gradient descent can get stuck at local minima, even for simple classes of target functions. In this paper, we present several positive theoretical results to support the effectiveness of neural networks. We focus on two-layer neural networks (i.e. one hidden layer) where the top layer node is a linear function, similar to\u00a0\\citebarron93. First we show that for a randomly initialized neural network with sufficiently many hidden units, the gradient descent method can learn any low degree polynomial. Secondly, we show that if we use complex-valued weights (the target function can still be real), then under suitable conditions, there are no \u201crobust local minima\u201d: the neural network can always escape a local minimum by performing a random perturbation. This property does not hold for real-valued weights. Thirdly, we discuss whether sparse polynomials can be learned with \\emphsmall neural networks, where the size is dependent on the sparsity of the target function."}}
