{"id": "Wxj9U0ySU-s", "cdate": 1683834494805, "mdate": null, "content": {"title": "Compositional Program Generation for Systematic Generalization", "abstract": "Compositional generalization remains a difficult problem for neural models. There has been progress but the hardest benchmark problems remain intractable without additional task-specific semantic information. In this paper we describe a neuro-symbolic architecture Compositional Program Generator (CPG) which generalizes systematically and productively for sequence-to-sequence language tasks, given a context-free grammar of the input language and a dictionary mapping each input word to its interpretation in the output language. Our approach learns to generate type-specific symbolic semantic functions composed in an input-dependent way to produce the output sequence. In experiments with SCAN, CPG solves all splits and few-shot generalizes on the systematicity (\"add jump\") split. On the COGS benchmark the model achieves perfect generalization in 2 epochs of training on sentences of length less than or equal 12."}}
{"id": "OImyRhNLv3", "cdate": 1683834494609, "mdate": null, "content": {"title": "How Compositional is a Model?", "abstract": "We present a formal definition of a general class of compositional functions, and show how various existing models fit this definition. Given this general definition, we discuss a notion of complexity of these compositional functions, and show how this complexity affects the expressivity and compositional generalization of these functions."}}
{"id": "CF7FzuUkUck", "cdate": 1679417876329, "mdate": null, "content": {"title": "Oversampling to Repair Bias and Imbalance Simultaneously", "abstract": "Both group bias and class imbalance occur when instances with certain characteristics are under-represented in the data.  Group bias causes estimators to be unfair and class imbalance causes estimators to be inaccurate.  Oversampling ought to address both kinds of under-representation.  Unfortunately, it is hard to pick a level of oversampling that yields the best fairness and accuracy for a given estimator.  This paper introduces Orbis, an oversampling algorithm that can be precisely tuned for both fairness and accuracy.  Orbis is a pre-estimator bias mitigator that modifies the data used to train downstream estimators.  This paper demonstrates how to use automated machine learning to tune Orbis along with the choice of estimator that follows it and empirically compares various approaches for blending multiple metrics into a single optimizer objective.  Overall, this paper introduces a new bias mitigator along with a methodology for training and tuning it."}}
{"id": "7Nbd1Ru1M_t", "cdate": 1679417875117, "mdate": null, "content": {"title": "Searching for Fairer Machine Learning Ensembles", "abstract": "Bias mitigators can improve algorithmic fairness in machine learning models, but their effect on fairness is often not stable across data splits.  A popular approach to train more stable models is ensemble learning, but unfortunately, it is unclear how to combine ensembles with mitigators to best navigate trade-offs between fairness and predictive performance.  To that end, we extended the open-source library Lale to enable the modular composition of 8 mitigators, 4 ensembles, and their corresponding hyperparameters, and we empirically explored the space of configurations on 13 datasets.  We distilled our insights from this exploration in the form of a guidance diagram that can serve as a starting point for practitioners that we demonstrate is robust and reproducible.  We also ran automatic combined algorithm selection and hyperparmeter tuning (or CASH) over ensembles with mitigators.  The solutions from the guidance diagram perform similar to those from CASH on many datasets."}}
{"id": "M9jO8V8FUMU", "cdate": 1672531200000, "mdate": 1674492694274, "content": {"title": "Toward Theoretical Guidance for Two Common Questions in Practical Cross-Validation based Hyperparameter Selection", "abstract": "We show, to our knowledge, the first theoretical treatments of two common questions in cross-validation based hyperparameter selection: (1) After selecting the best hyperparameter using a held-out set, we train the final model using {\\em all} of the training data -- since this may or may not improve future generalization error, should one do this? (2) During optimization such as via SGD (stochastic gradient descent), we must set the optimization tolerance $\\rho$ -- since it trades off predictive accuracy with computation cost, how should one set it? Toward these problems, we introduce the {\\em hold-in risk} (the error due to not using the whole training data), and the {\\em model class mis-specification risk} (the error due to having chosen the wrong model class) in a theoretical view which is simple, general, and suggests heuristics that can be used when faced with a dataset instance. In proof-of-concept studies in synthetic data where theoretical quantities can be controlled, we show that these heuristics can, respectively, (1) always perform at least as well as always performing retraining or never performing retraining, (2) either improve performance or reduce computational overhead by $2\\times$ with no loss in predictive performance."}}
{"id": "PvDY71zKsvP", "cdate": 1663850463805, "mdate": null, "content": {"title": "Min-Max Multi-objective Bilevel Optimization with Applications in Robust Machine Learning", "abstract": "We consider a generic min-max multi-objective bilevel optimization problem with applications in robust machine learning such as representation learning and hyperparameter optimization. We design MORBiT, a novel single-loop gradient descent-ascent bilevel optimization algorithm, to solve the generic problem and present a novel analysis showing that MORBiT converges to the first-order stationary point at a rate of $\\widetilde{\\mathcal{O}}(n^{1/2} K^{-2/5})$ for a class of weakly convex problems with $n$ objectives upon $K$ iterations of the algorithm. Our analysis utilizes novel results to handle the non-smooth min-max multi-objective setup and to obtain a sublinear dependence in the number of objectives $n$. Experimental results on robust representation learning and robust hyperparameter optimization showcase (i) the advantages of considering the min-max multi-objective setup, and (ii) convergence properties of the proposed \\morbit."}}
{"id": "MjsDeTcDEy", "cdate": 1663849954850, "mdate": null, "content": {"title": "What Is Missing in IRM Training and Evaluation? Challenges and Solutions", "abstract": "Invariant risk minimization (IRM) has received increasing attention as a way to acquire environment-agnostic data representations and predictions, and also a principled solution for preventing spurious correlations from being learned and improving models\u2019 out-of-distribution generalization. Yet, recent works have found that the optimality of the originally-proposed IRM optimization (IRMV1) may be compromised in practice or could be impossible to achieve in some scenarios. Therefore, a series of advanced IRM algorithms have been developed that show practical improvement over IRMV1. In this work, we revisit these recent IRM advancements and identify and resolve three practical limitations in IRM training and evaluation. First, we find that the effect of batch size during training has been chronically overlooked in previous studies, leaving room for further improvement. We propose small-batch training and highlight the improvements over a set of large-batch optimization techniques. Second, we find that improper selection of evaluation environments could give a false sense of invariance for IRM. To alleviate this effect, we leverage diversified test-time environments to precisely characterize the invariance of IRM when applied in practice. Third, we revisit Ahuja et al. (2020)\u2019s proposal to convert IRM into an ensemble game and identify a limitation when a single invariant predictor is desired instead of an ensemble of individual predictors. We propose a new IRM variant to address this limitation based on a novel viewpoint of ensemble IRM games as consensus-constrained bi-level optimization. Lastly, we conduct extensive experiments (covering 7 existing IRM variants and 7 datasets) to justify the practical significance of revisiting IRM training and evaluation in a principled manner."}}
{"id": "3RhuF8foyPW", "cdate": 1663849902765, "mdate": null, "content": {"title": "Single-shot General Hyper-parameter Optimization for Federated Learning", "abstract": "We address the problem of hyper-parameter optimization (HPO) for federated learning (FL-HPO). We introduce Federated Loss SuRface Aggregation (FLoRA), a general FL-HPO solution framework that can address use cases of tabular data and any Machine Learning (ML) model including gradient boosting training algorithms, SVMs, neural networks, among others and thereby further expands the scope of FL-HPO. FLoRA enables single-shot FL-HPO: identifying a single set of good hyper-parameters that are subsequently used in a single FL training. Thus, it enables FL-HPO solutions with minimal additional communication overhead compared to FL training without HPO. Utilizing standard smoothness assumptions, we theoretically characterize the optimality gap of FLoRA for any convex and non-convex loss functions, which explicitly accounts for the heterogeneous nature of the parties' local data distributions, a dominant characteristic of FL systems. Our empirical evaluation of FLoRA for multiple FL algorithms on seven OpenML datasets demonstrates significant model accuracy improvements over the baselines, and robustness to increasing number of parties involved in FL-HPO training."}}
{"id": "t6O08FxvtBY", "cdate": 1652737285623, "mdate": null, "content": {"title": "Advancing Model Pruning via Bi-level Optimization", "abstract": "The deployment constraints in practical applications necessitate the pruning of large-scale deep learning models, i.e., promoting their weight sparsity. As illustrated by the Lottery Ticket Hypothesis (LTH), pruning also has the potential of improving their generalization ability. At the core of LTH, iterative magnitude pruning (IMP) is the predominant pruning method to successfully find \u2018winning tickets\u2019. Yet, the computation cost of IMP grows prohibitively as the targeted pruning ratio increases. To reduce the computation overhead, various efficient \u2018one-shot\u2019 pruning methods have been developed, but these schemes are usually unable to find winning tickets as good as IMP. This raises the question of how to close the gap between pruning accuracy and pruning efficiency? To tackle it, we pursue the algorithmic advancement of model pruning. Specifically, we formulate the pruning problem from a fresh and novel viewpoint, bi-level optimization (BLO). We show that the BLO interpretation provides a technically-grounded optimization base for an efficient implementation of the pruning-retraining learning paradigm used in IMP. We also show that the proposed bi-level optimization-oriented pruning method (termed BiP) is a special class of BLO problems with a bi-linear problem structure. By leveraging such bi-linearity, we theoretically show that BiP can be solved as easily as first-order optimization, thus inheriting the computation efficiency. Through extensive experiments on both structured and unstructured pruning with 5 model architectures and 4 data sets, we demonstrate that BiP can find better winning tickets than IMP in most cases, and is computationally as efficient as the one-shot pruning schemes, demonstrating $2-7\\times$ speedup over IMP for the same level of model accuracy and sparsity."}}
{"id": "rEeLCearIx5", "cdate": 1645792502200, "mdate": null, "content": {"title": "On the Optimality Gap of Warm-Started Hyperparameter Optimization", "abstract": "We study the general framework of warm-started hyperparameter optimization (HPO) where we have some source datasets (tasks) where we have already performed HPO, and we wish to leverage the results of these HPO to warm-start the HPO on an unseen target dataset (and perform few-shot HPO). Various meta-learning schemes have been proposed over the last decade (and more) for this problem. In this paper, we theoretically analyse the optimality gap of the hyperparameter obtained via such warm-started few-shot HPO, and provide novel results for multiple existing meta-learning schemes. We show how these results allow us identify situations where certain schemes have advantage over others."}}
