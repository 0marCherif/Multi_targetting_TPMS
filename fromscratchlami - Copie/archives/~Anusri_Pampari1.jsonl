{"id": "9P-FOvJtDvI", "cdate": 1621406272408, "mdate": null, "content": {"title": "Help Me Search: Leveraging User-System Collaboration for Query Construction to Improve Accuracy for Difficult Queries", "abstract": "In this paper, we address the problem of difficult queries by using a novel strategy of collaborative query construction where the search engine would actively engage users in an iterative process to continuously revise a query. This approach can be implemented in any search engine to provide search support for users via a \"Help Me Search\" button, which a user can click on as needed. We focus on studying a specific collaboration strategy where the search engine and the user work together to iteratively expand a query. We propose a possible implementation for this strategy in which the system generates candidate terms by utilizing the history of interactions of the user with the system. Evaluation using a simulated user study shows the great promise of the proposed approach. We also perform a case study with three real users which further illustrates the potential effectiveness of the approach."}}
{"id": "VMVtCImY3ml", "cdate": 1621406141244, "mdate": null, "content": {"title": "Unsupervised Calibration under Covariate Shift", "abstract": "A probabilistic model is said to be calibrated if its predicted probabilities match the corresponding empirical frequencies. Calibration is important for uncertainty quantification and decision making in safety-critical applications. While calibration of classifiers has been widely studied, we find that calibration is brittle and can be easily lost under minimal covariate shifts. Existing techniques, including domain adaptation ones, primarily focus on prediction accuracy and do not guarantee calibration neither in theory nor in practice. In this work, we formally introduce the problem of calibration under domain shift, and propose an importance sampling based approach to address it. We evaluate and discuss the efficacy of our method on both real-world datasets and synthetic datasets."}}
{"id": "oNlyZj8Q71", "cdate": 1609459200000, "mdate": 1682317922421, "content": {"title": "Patient Experience Surveys Reveal Gender-Biased Descriptions of Their Care Providers", "abstract": "Patient experience surveys (PES) are collected by healthcare systems as a surrogate marker of quality and published unedited online for the purpose of transparency, but these surveys may reflect gender biases directed toward healthcare providers. This retrospective study evaluated PES at a single university hospital between July 2016 and June 2018. Surveys were stratified by overall provider rating and self-identified provider gender. Adjectives from free-text survey comments were extracted using natural language processing techniques and applied to a statistical machine learning model to identify descriptors predictive of provider gender. 109,994 surveys were collected, 17,395 contained free-text comments describing 687 unique providers. The mean overall rating between male (8.84, n\u2009=\u20098558) and female (8.80, n\u2009=\u20098837) providers did not differ (p\u2009=\u20090.149). However, highly-rated male providers were more often described for their agentic qualities using adjectives such as \u201cinformative,\u201d \u201cforthright,\u201d \u201csuperior,\u201d and \u201cutmost\u201d (OR 1.48, p\u2009<\u20090.01)\u2014whereas highly-rated female providers were more often described by their communal qualities through adjectives such as \u201cempathetic,\u201d \u201csweet,\u201d \u201cwarm,\u201d \u201cattentive,\u201d and \u201capproachable\u201d (OR 2.11, p\u2009<\u20090.0001). PES may contain gender stereotypes, raising questions about their impact on physicians and their validity as a quality metric which must be balanced with the need for unedited transparency. Future prospective studies are needed to further characterize this trend across geographically and racially diverse healthcare providers."}}
{"id": "lpTm8vDUfmy", "cdate": 1577836800000, "mdate": 1631211374544, "content": {"title": "Unsupervised Calibration under Covariate Shift", "abstract": "A probabilistic model is said to be calibrated if its predicted probabilities match the corresponding empirical frequencies. Calibration is important for uncertainty quantification and decision making in safety-critical applications. While calibration of classifiers has been widely studied, we find that calibration is brittle and can be easily lost under minimal covariate shifts. Existing techniques, including domain adaptation ones, primarily focus on prediction accuracy and do not guarantee calibration neither in theory nor in practice. In this work, we formally introduce the problem of calibration under domain shift, and propose an importance sampling based approach to address it. We evaluate and discuss the efficacy of our method on both real-world datasets and synthetic datasets."}}
{"id": "T9zcCKJkZFY", "cdate": 1546300800000, "mdate": 1682317922311, "content": {"title": "Help Me Search: Leveraging User-System Collaboration for Query Construction to Improve Accuracy for Difficult Queries", "abstract": "In this paper, we address the problem of difficult queries by using a novel strategy of collaborative query construction where the search engine would actively engage users in an iterative process to continuously revise a query. This approach can be implemented in any search engine to provide search support for users via a \"Help Me Search\" button, which a user can click on as needed. We focus on studying a specific collaboration strategy where the search engine and the user work together to iteratively expand a query. We propose a possible implementation for this strategy in which the system generates candidate terms by utilizing the history of interactions of the user with the system. Evaluation using a simulated user study shows the great promise of the proposed approach. We also perform a case study with three real users which further illustrates the potential effectiveness of the approach."}}
{"id": "rJVBwMGu-B", "cdate": 1514764800000, "mdate": null, "content": {"title": "emrQA: A Large Corpus for Question Answering on Electronic Medical Records", "abstract": "We propose a novel methodology to generate domain-specific large-scale question answering (QA) datasets by re-purposing existing annotations for other NLP tasks. We demonstrate an instance of this methodology in generating a large-scale QA dataset for electronic medical records by leveraging existing expert annotations on clinical notes for various NLP tasks from the community shared i2b2 datasets. The resulting corpus (emrQA) has 1 million question-logical form and 400,000+ question-answer evidence pairs. We characterize the dataset and explore its learning potential by training baseline models for question to logical form and question to answer mapping."}}
{"id": "Ce1DoWN6Zu7", "cdate": 1514764800000, "mdate": 1682317922309, "content": {"title": "emrQA: A Large Corpus for Question Answering on Electronic Medical Records", "abstract": "We propose a novel methodology to generate domain-specific large-scale question answering (QA) datasets by re-purposing existing annotations for other NLP tasks. We demonstrate an instance of this methodology in generating a large-scale QA dataset for electronic medical records by leveraging existing expert annotations on clinical notes for various NLP tasks from the community shared i2b2 datasets. The resulting corpus (emrQA) has 1 million question-logical form and 400,000+ question-answer evidence pairs. We characterize the dataset and explore its learning potential by training baseline models for question to logical form and question to answer mapping."}}
