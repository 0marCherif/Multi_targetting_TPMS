{"id": "fdyxLGHE6bU", "cdate": 1652737765046, "mdate": null, "content": {"title": "Active Learning with Safety Constraints", "abstract": "Active learning methods have shown great promise in reducing the number of samples necessary for learning. As automated learning systems are adopted into real-time, real-world decision-making pipelines, it is increasingly important that such algorithms are designed with safety in mind. In this work we investigate the complexity of learning the best safe decision in interactive environments. We reduce this problem to a safe linear bandits problem, where our goal is to find the best arm satisfying certain (unknown) safety constraints. We propose an adaptive experimental design-based algorithm, which we show efficiently trades off between the difficulty of showing an arm is unsafe vs suboptimal. To our knowledge, our results are the first on best-arm identification in linear bandits with safety constraints. In  practice, we demonstrate that this approach performs well on synthetic and real world datasets."}}
{"id": "-76EsjcHnbj", "cdate": 1652737623404, "mdate": null, "content": {"title": "Instance-Dependent Near-Optimal Policy Identification in Linear MDPs via Online Experiment Design", "abstract": "While much progress has been made in understanding the minimax sample complexity of reinforcement learning (RL)---the complexity of learning on the ``worst-case'' instance---such measures of complexity often do not capture the true difficulty of learning. In practice, on an ``easy'' instance, we might hope to achieve a complexity far better than that achievable on the worst-case instance. In this work we seek to understand this ``instance-dependent'' complexity of learning in the setting of RL with linear function approximation. We propose an algorithm, PEDEL, which achieves a fine-grained instance-dependent measure of complexity, the first of its kind in the RL with function approximation setting, thereby capturing the difficulty of learning on each particular problem instance. Through an explicit example, we show that PEDEL yields provable gains over low-regret, minimax-optimal algorithms and that such algorithms are unable to hit the instance-optimal rate. Our approach relies on a novel online experiment design-based procedure which focuses the exploration budget on the ``directions'' most relevant to learning a near-optimal policy, and may be of independent interest."}}
{"id": "RjH5qnLANe", "cdate": 1640995200000, "mdate": 1681830630267, "content": {"title": "Active Learning with Safety Constraints", "abstract": "Active learning methods have shown great promise in reducing the number of samples necessary for learning. As automated learning systems are adopted into real-time, real-world decision-making pipelines, it is increasingly important that such algorithms are designed with safety in mind. In this work we investigate the complexity of learning the best safe decision in interactive environments. We reduce this problem to a constrained linear bandits problem, where our goal is to find the best arm satisfying certain (unknown) safety constraints. We propose an adaptive experimental design-based algorithm, which we show efficiently trades off between the difficulty of showing an arm is unsafe vs suboptimal. To our knowledge, our results are the first on best-arm identification in linear bandits with safety constraints. In practice, we demonstrate that this approach performs well on synthetic and real world datasets."}}
{"id": "HaHWu30Go3", "cdate": 1640995200000, "mdate": 1681830630223, "content": {"title": "Instance-Dependent Near-Optimal Policy Identification in Linear MDPs via Online Experiment Design", "abstract": "While much progress has been made in understanding the minimax sample complexity of reinforcement learning (RL) -- the complexity of learning on the \"worst-case\" instance -- such measures of complexity often do not capture the true difficulty of learning. In practice, on an \"easy\" instance, we might hope to achieve a complexity far better than that achievable on the worst-case instance. In this work we seek to understand the \"instance-dependent\" complexity of learning near-optimal policies (PAC RL) in the setting of RL with linear function approximation. We propose an algorithm, \\textsc{Pedel}, which achieves a fine-grained instance-dependent measure of complexity, the first of its kind in the RL with function approximation setting, thereby capturing the difficulty of learning on each particular problem instance. Through an explicit example, we show that \\textsc{Pedel} yields provable gains over low-regret, minimax-optimal algorithms and that such algorithms are unable to hit the instance-optimal rate. Our approach relies on a novel online experiment design-based procedure which focuses the exploration budget on the \"directions\" most relevant to learning a near-optimal policy, and may be of independent interest."}}
{"id": "B9Z1dhuaeq", "cdate": 1640995200000, "mdate": 1646263398756, "content": {"title": "Reward-Free RL is No Harder Than Reward-Aware RL in Linear Markov Decision Processes", "abstract": "Reward-free reinforcement learning (RL) considers the setting where the agent does not have access to a reward function during exploration, but must propose a near-optimal policy for an arbitrary reward function revealed only after exploring. In the the tabular setting, it is well known that this is a more difficult problem than reward-aware (PAC) RL -- where the agent has access to the reward function during exploration -- with optimal sample complexities in the two settings differing by a factor of $|\\mathcal{S}|$, the size of the state space. We show that this separation does not exist in the setting of linear MDPs. We first develop a computationally efficient algorithm for reward-free RL in a $d$-dimensional linear MDP with sample complexity scaling as $\\widetilde{\\mathcal{O}}(d^2 H^5/\\epsilon^2)$. We then show a lower bound with matching dimension-dependence of $\\Omega(d^2 H^2/\\epsilon^2)$, which holds for the reward-aware RL setting. To our knowledge, our approach is the first computationally efficient algorithm to achieve optimal $d$ dependence in linear MDPs, even in the single-reward PAC setting. Our algorithm relies on a novel procedure which efficiently traverses a linear MDP, collecting samples in any given ``feature direction'', and enjoys a sample complexity scaling optimally in the (linear MDP equivalent of the) maximal state visitation probability. We show that this exploration procedure can also be applied to solve the problem of obtaining ``well-conditioned'' covariates in linear MDPs."}}
{"id": "53ZSgDS64w", "cdate": 1640995200000, "mdate": 1682184600057, "content": {"title": "Leveraging Offline Data in Online Reinforcement Learning", "abstract": "Two central paradigms have emerged in the reinforcement learning (RL) community: online RL and offline RL. In the online RL setting, the agent has no prior knowledge of the environment, and must interact with it in order to find an $\\epsilon$-optimal policy. In the offline RL setting, the learner instead has access to a fixed dataset to learn from, but is unable to otherwise interact with the environment, and must obtain the best policy it can from this offline data. Practical scenarios often motivate an intermediate setting: if we have some set of offline data and, in addition, may also interact with the environment, how can we best use the offline data to minimize the number of online interactions necessary to learn an $\\epsilon$-optimal policy? In this work, we consider this setting, which we call the \\textsf{FineTuneRL} setting, for MDPs with linear structure. We characterize the necessary number of online samples needed in this setting given access to some offline dataset, and develop an algorithm, \\textsc{FTPedel}, which is provably optimal, up to $H$ factors. We show through an explicit example that combining offline data with online interactions can lead to a provable improvement over either purely offline or purely online RL. Finally, our results illustrate the distinction between \\emph{verifiable} learning, the typical setting considered in online RL, and \\emph{unverifiable} learning, the setting often considered in offline RL, and show that there is a formal separation between these regimes."}}
{"id": "rhNZJ_2dag9", "cdate": 1609459200000, "mdate": 1646263398757, "content": {"title": "Experimental Design for Regret Minimization in Linear Bandits", "abstract": "In this paper we propose a novel experimental design-based algorithm to minimize regret in online stochastic linear and combinatorial bandits. While existing literature tends to focus on optimism-based algorithms\u2013which have been shown to be suboptimal in many cases\u2013our approach carefully plans which action to take by balancing the tradeoff between information gain and reward, overcoming the failures of optimism. In addition, we leverage tools from the theory of suprema of empirical processes to obtain regret guarantees that scale with the Gaussian width of the action set, avoiding wasteful union bounds. We provide state-of-the-art finite time regret guarantees and show that our algorithm can be applied in both the bandit and semi-bandit feedback regime. In the combinatorial semi-bandit setting, we show that our algorithm is computationally efficient and relies only on calls to a linear maximization oracle. In addition, we show that with slight modification our algorithm can be used for pure exploration, obtaining state-of-the-art pure exploration guarantees in the semi-bandit setting. Finally, we provide, to the best of our knowledge, the first example where optimism fails in the semi-bandit regime, and show that in this setting our algorithm succeeds."}}
{"id": "rY8Zyd2_Tl5", "cdate": 1609459200000, "mdate": 1646263398757, "content": {"title": "Best Arm Identification with Safety Constraints", "abstract": "The best arm identification problem in the multi-armed bandit setting is an excellent model of many real-world decision-making problems, yet it fails to capture the fact that in the real-world, safety constraints often must be met while learning. In this work we study the question of best-arm identification in safety-critical settings, where the goal of the agent is to find the best safe option out of many, while exploring in a way that guarantees certain, initially unknown safety constraints are met. We first analyze this problem in the setting where the reward and safety constraint takes a linear structure, and show nearly matching upper and lower bounds. We then analyze a much more general version of the problem where we only assume the reward and safety constraint can be modeled by monotonic functions, and propose an algorithm in this setting which is guaranteed to learn safely. We conclude with experimental results demonstrating the effectiveness of our approaches in scenarios such as safely identifying the best drug out of many in order to treat an illness."}}
{"id": "r-Z1Ohdax5", "cdate": 1609459200000, "mdate": 1646263398761, "content": {"title": "Task-Optimal Exploration in Linear Dynamical Systems", "abstract": "Exploration in unknown environments is a fundamental problem in reinforcement learning and control. In this work, we study task-guided exploration and determine what precisely an agent must learn a..."}}
{"id": "HxWku3uTx9", "cdate": 1609459200000, "mdate": 1646263398760, "content": {"title": "First-Order Regret in Reinforcement Learning with Linear Function Approximation: A Robust Estimation Approach", "abstract": "Obtaining first-order regret bounds -- regret bounds scaling not as the worst-case but with some measure of the performance of the optimal policy on a given instance -- is a core question in sequential decision-making. While such bounds exist in many settings, they have proven elusive in reinforcement learning with large state spaces. In this work we address this gap, and show that it is possible to obtain regret scaling as $\\widetilde{\\mathcal{O}}(\\sqrt{d^3 H^3 \\cdot V_1^\\star \\cdot K} + d^{3.5}H^3\\log K )$ in reinforcement learning with large state spaces, namely the linear MDP setting. Here $V_1^\\star$ is the value of the optimal policy and $K$ is the number of episodes. We demonstrate that existing techniques based on least squares estimation are insufficient to obtain this result, and instead develop a novel robust self-normalized concentration bound based on the robust Catoni mean estimator, which may be of independent interest."}}
