{"id": "jpWa2RnZpIK", "cdate": 1663850220629, "mdate": null, "content": {"title": "MaskNeRF: Masked Neural Radiance Fields for Sparse View Synthesis", "abstract": "Although Neural Radiance Fields (NeRF) has achieved impressive 3D reconstruction with dense view images, its performance degrades significantly when the training views are sparse. We observe that under the sparse view setting, it is important to learn the correspondence of pixels among different views, i.e., the 3D consistency, to improve the reconstruction quality. To achieve this, we first propose the Hard-Mask that utilizes the depth information to locate pixels with correspondence relationship and then assigns higher loss weights on these pixels. The key idea is to achieve pixel-wise differentiated optimization of NeRF based on the 3D consistency among target views and source views instead of treating each pixel equally. This optimization strategy helps NeRF-based algorithms to learn fine-grained object details with limited data. To deal with the absence of accurate depth information, the Soft-Mask is proposed to estimate the correspondence relationship based on the trend of training losses. Our proposed method can serve as a plug-in component for existing NeRF-based view-synthesis models. Extensive experiments on recent representative works, including NeRF, IBRNet and MVSNeRF, show that our method can significantly improve the model performance under sparse view conditions (e.g., up to 70\\% improvement in PSNR on DTU dataset). "}}
{"id": "r1HT1AW8PSf", "cdate": 1650547879238, "mdate": null, "content": {"title": "DHA: End-to-End Joint Optimization of Data Augmentation Policy, Hyper-parameter and Architecture", "abstract": "Automated machine learning (AutoML) usually involves several crucial components, such as Data Augmentation (DA) policy, Hyper-Parameter Optimization (HPO), and Neural Architecture Search (NAS). However joint optimization of these components remains challenging due to the largely increased search dimension and the variant input types of each component. In parallel to this, the common practice of searching for the optimal architecture first and then retraining it before deployment in NAS often suffers from the low-performance correlation between the search and retraining stages. An end-to-end solution that integrates the AutoML components and returns a ready-to-use model at the end of the search is desirable. In view of these, we propose DHA, which achieves joint optimization of Data augmentation policy, Hyper-parameter, and Architecture. Specifically, end-to-end NAS is achieved in a differentiable manner by optimizing a compressed lower-dimensional feature space, while DA policy and HPO are updated dynamically at the same time."}}
