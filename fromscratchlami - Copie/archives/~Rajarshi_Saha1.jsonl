{"id": "_JzH9C_Am9", "cdate": 1695552845171, "mdate": 1695552845171, "content": {"title": "Matrix Compression via Randomized Low Rank and Low Precision Factorization", "abstract": "Matrices are exceptionally useful in various fields of study as they provide a\nconvenient framework to organize and manipulate data in a structured manner.\nHowever, modern matrices can involve billions of elements, making their storage\nand processing quite demanding in terms of computational resources and memory\nusage. Although prohibitively large, such matrices are often approximately low\nrank. We propose an algorithm that exploits this structure to obtain a low rank\ndecomposition of any matrix A as A = LR, where L and R are the low rank\nfactors. The total number of elements in L and R can be significantly less than that\nin A. Furthermore, the entries of L and R are quantized to low precision formats\n\u2013 compressing A by giving us a low rank and low precision factorization. Our\nalgorithm first computes an approximate basis of the range space of A by randomly\nsketching its columns, followed by a quantization of the vectors constituting\nthis basis. It then computes approximate projections of the columns of A onto\nthis quantized basis. We derive upper bounds on the approximation error of our\nalgorithm, and analyze the impact of target rank and quantization bit-budget. The\ntradeoff between compression ratio and the approximation accuracy allows for\nflexibility in choosing these parameters based on specific application requirements.\nWe empirically demonstrate the efficacy of our algorithm in nearest neighbor\nclassification of image and text embeddings. Our results illustrate that we can\nachieve compression ratios as aggressive as one bit per matrix coordinate, all while\nsurpassing or maintaining the performance of traditional compression techniques"}}
{"id": "IwTtmEe2I1", "cdate": 1672531200000, "mdate": 1696268992506, "content": {"title": "Collaborative Mean Estimation over Intermittently Connected Networks with Peer-To-Peer Privacy", "abstract": "This work considers the problem of Distributed Mean Estimation (DME) over networks with intermittent connectivity, where the goal is to learn a global statistic over the data samples localized across distributed nodes with the help of a central server. To mitigate the impact of intermittent links, nodes can collaborate with their neighbors to compute local consensus which they forward to the central server. In such a setup, the communications between any pair of nodes must satisfy local differential privacy constraints. We study the tradeoff between collaborative relaying and privacy leakage due to the additional data sharing among nodes and, subsequently, propose a novel differentially private collaborative algorithm for DME to achieve the optimal tradeoff. Finally, we present numerical simulations to substantiate our theoretical findings."}}
{"id": "8b0RHdh2Xd0", "cdate": 1663939408127, "mdate": null, "content": {"title": "ColRel: Collaborative Relaying for Federated Learning over Intermittently Connected Networks", "abstract": "Intermittent connectivity of clients to the parameter server (PS) is a major bottleneck in federated edge learning. It induces a large generalization gap, especially when the local data distribution amongst clients exhibits heterogeneity. To overcome communication blockages between clients and the central PS, we have introduced the concept of collaborative relaying (ColRel), wherein the participating clients relay their neighbors' local updates to the PS in order to boost the participation of clients with poor connectivity to the PS. For every communication round, each client initially computes a local consensus of a subset of its neighboring clients' updates and subsequently transmits to the PS, a weighted average of its own update and those of its neighbors'. In this work, we view ColRel as a variance reduction technique that helps in improving the convergence rate for different optimization setups. Consequently, our ColRel approach can be readily integrated as a black box with existing federated learning systems. We provide analytical upper bounds on the resulting convergence rate, which we reduce by optimizing the weights subject to an unbiasedness condition for the global update. Numerical evaluations on the CIFAR-10 dataset demonstrate that our ColRel-based approach achieves a higher test accuracy over Federated Averaging based benchmarks for learning over intermittently-connected networks."}}
{"id": "MlXuVE2K7hO", "cdate": 1640995200000, "mdate": 1682318143719, "content": {"title": "Minimax Optimal Quantization of Linear Models: Information-Theoretic Limits and Efficient Algorithms", "abstract": "High-dimensional models often have a large memory footprint and must be quantized after training before being deployed on resource-constrained edge devices for inference tasks. In this work, we develop an information-theoretic framework for the problem of quantizing a linear regressor learned from training data $(\\mathbf{X}, \\mathbf{y})$, for some underlying statistical relationship $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\theta} + \\mathbf{v}$. The learned model, which is an estimate of the latent parameter $\\boldsymbol{\\theta} \\in \\mathbb{R}^d$, is constrained to be representable using only $Bd$ bits, where $B \\in (0, \\infty)$ is a pre-specified budget and $d$ is the dimension. We derive an information-theoretic lower bound for the minimax risk under this setting and propose a matching upper bound using randomized embedding-based algorithms which is tight up to constant factors. The lower and upper bounds together characterize the minimum threshold bit-budget required to achieve a performance risk comparable to the unquantized setting. We also propose randomized Hadamard embeddings that are computationally efficient and are optimal up to a mild logarithmic factor of the lower bound. Our model quantization strategy can be generalized and we show its efficacy by extending the method and upper-bounds to two-layer ReLU neural networks for non-linear regression. Numerical simulations show the improved performance of our proposed scheme as well as its closeness to the lower bound."}}
{"id": "GMJDFBohE4", "cdate": 1640995200000, "mdate": 1684216405954, "content": {"title": "Semi-Decentralized Federated Learning with Collaborative Relaying", "abstract": "We present a semi-decentralized federated learning algorithm wherein clients collaborate by relaying their neighbors\u2019 local updates to a central parameter server (PS). At every communication round to the PS, each client computes a local consensus of the updates from its neighboring clients and eventually transmits a weighted average of its own update and those of its neighbors to the PS. We appropriately optimize these averaging weights to ensure that the global update at the PS is unbiased and to reduce the variance of the global update at the PS, consequently improving the rate of convergence. Numerical simulations substantiate our theoretical claims and demonstrate settings with intermittent connectivity between the clients and the PS, where our proposed algorithm shows an improved convergence rate and accuracy in comparison with the federated averaging algorithm."}}
{"id": "Cd6peTodNRC", "cdate": 1640995200000, "mdate": 1696268992517, "content": {"title": "Decentralized Optimization Over Noisy, Rate-Constrained Networks: Achieving Consensus by Communicating Differences", "abstract": "In decentralized optimization, multiple nodes in a network collaborate to minimize the sum of their local loss functions. The information exchange between nodes required for this task, is often limited by network connectivity. We consider a setting in which communication between nodes is hindered by both (i) a finite rate-constraint on the signal transmitted by any node, and (ii) additive noise corrupting the signal received by any node. We propose a novel algorithm for this scenario: Decentralized Lazy Mirror Descent with Differential Exchanges (DLMD-DiffEx), which guarantees convergence of the local estimates to the optimal solution under the given communication constraints. A salient feature of DLMD-DiffEx is the introduction of additional proxy variables that are maintained by the nodes to account for the disagreement in their estimates due to channel noise and rate-constraints. Convergence to the optimal solution is attained by having nodes iteratively exchange these disagreement terms until consensus is achieved. In order to prevent noise accumulation during this exchange, DLMD-DiffEx relies on two sequences: one controlling the power of the transmitted signal, and the other determining the consensus rate. We provide insights on the design of these two sequences which highlights the interplay between consensus rate and noise amplification. We investigate the performance of DLMD-DiffEx both from a theoretical perspective as well as through numerical evaluations on synthetic data and MNIST. MATLAB and Python implementations can be found at <uri>https://github.com/rajarshisaha95/DLMD-DiffEx</uri>."}}
{"id": "6JIHrtLrqH", "cdate": 1640995200000, "mdate": 1682318143903, "content": {"title": "Efficient Randomized Subspace Embeddings for Distributed Optimization Under a Communication Budget", "abstract": "We study first-order optimization algorithms under the constraint that the descent direction is quantized using a pre-specified budget of <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$R$ </tex-math></inline-formula> -bits per dimension, where <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$R \\in (0,\\infty)$ </tex-math></inline-formula> . We propose computationally efficient optimization algorithms with convergence rates matching the information-theoretic performance lower bounds for: (i) Smooth and Strongly-Convex objectives with access to an Exact Gradient oracle, as well as (ii) General Convex and Non-Smooth objectives with access to a Noisy Subgradient oracle. The crux of these algorithms is a polynomial complexity source coding scheme that embeds a vector into a random subspace before quantizing it. These embeddings are such that with high probability, their projection along any of the canonical directions of the transform space is small. As a consequence, quantizing these embeddings followed by an inverse transform to the original space yields a source coding method with optimal covering efficiency while utilizing just <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$R$ </tex-math></inline-formula> -bits per dimension. Our algorithms guarantee optimality for arbitrary values of the bit-budget <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$R$ </tex-math></inline-formula> , which includes both the sub-linear budget regime ( <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$R &lt; 1$ </tex-math></inline-formula> ), as well as the high-budget regime ( <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$R \\geq 1$ </tex-math></inline-formula> ), while requiring <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$O(n^{2})$ </tex-math></inline-formula> multiplications, where <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula> is the dimension. We also propose an efficient relaxation of this coding scheme using Hadamard subspaces that requires a near-linear time, i.e., <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$O(n \\log n)$ </tex-math></inline-formula> additions. Furthermore, we show that the utility of our proposed embeddings can be extended to significantly improve the performance of gradient sparsification schemes. Numerical simulations validate our theoretical claims. Our implementations are available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/rajarshisaha95/DistOptConstrComm</uri> ."}}
{"id": "7_oVgC7TP_i", "cdate": 1609459200000, "mdate": 1637058319876, "content": {"title": "Decentralized Optimization Over Noisy, Rate-Constrained Networks: How We Agree By Talking About How We Disagree", "abstract": "In decentralized optimization, multiple nodes in a network collaborate to minimize the sum of their local loss functions. The information exchange between nodes required for this task is often limited by network connectivity. We consider a generalization of this setting, in which communication is further hindered by (i) a finite data-rate constraint on the signal transmitted by any node, and (ii) an additive noise corrupting the signal received by any node. We develop a novel algorithm for this scenario: Decentralized Lazy Mirror Descent with Differential Exchanges (DLMD-DiffEx), which guarantees convergence of the local estimates to the optimal solution. A salient feature of DLMD-DiffEx is the introduction of additional proxy variables that are maintained by the nodes to account for the disagreement in their estimates due to channel noise and data-rate constraints. We investigate the performance of DLMD-DiffEx both from a theoretical perspective as well as through numerical evaluations."}}
