{"id": "tTHfzpIEByv", "cdate": 1672531200000, "mdate": 1684232754675, "content": {"title": "One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era", "abstract": "OpenAI has recently released GPT-4 (a.k.a. ChatGPT plus), which is demonstrated to be one small step for generative AI (GAI), but one giant leap for artificial general intelligence (AGI). Since its official release in November 2022, ChatGPT has quickly attracted numerous users with extensive media coverage. Such unprecedented attention has also motivated numerous researchers to investigate ChatGPT from various aspects. According to Google scholar, there are more than 500 articles with ChatGPT in their titles or mentioning it in their abstracts. Considering this, a review is urgently needed, and our work fills this gap. Overall, this work is the first to survey ChatGPT with a comprehensive review of its underlying technology, applications, and challenges. Moreover, we present an outlook on how ChatGPT might evolve to realize general-purpose AIGC (a.k.a. AI-generated content), which will be a significant milestone for the development of AGI."}}
{"id": "sAtg9gHPzQ", "cdate": 1672531200000, "mdate": 1699167446659, "content": {"title": "A Hybrid Multimodal Emotion Recognition Framework for UX Evaluation Using Generalized Mixture Functions", "abstract": "Multimodal emotion recognition has gained much traction in the field of affective computing, human\u2013computer interaction (HCI), artificial intelligence (AI), and user experience (UX). There is growing demand to automate analysis of user emotion towards HCI, AI, and UX evaluation applications for providing affective services. Emotions are increasingly being used, obtained through the videos, audio, text or physiological signals. This has led to process emotions from multiple modalities, usually combined through ensemble-based systems with static weights. Due to numerous limitations like missing modality data, inter-class variations, and intra-class similarities, an effective weighting scheme is thus required to improve the aforementioned discrimination between modalities. This article takes into account the importance of difference between multiple modalities and assigns dynamic weights to them by adapting a more efficient combination process with the application of generalized mixture (GM) functions. Therefore, we present a hybrid multimodal emotion recognition (H-MMER) framework using multi-view learning approach for unimodal emotion recognition and introducing multimodal feature fusion level, and decision level fusion using GM functions. In an experimental study, we evaluated the ability of our proposed framework to model a set of four different emotional states (Happiness, Neutral, Sadness, and Anger) and found that most of them can be modeled well with significantly high accuracy using GM functions. The experiment shows that the proposed framework can model emotional states with an average accuracy of 98.19% and indicates significant gain in terms of performance in contrast to traditional approaches. The overall evaluation results indicate that we can identify emotional states with high accuracy and increase the robustness of an emotion classification system required for UX measurement."}}
{"id": "qtc1RacfWX", "cdate": 1672531200000, "mdate": 1699167446593, "content": {"title": "Time Series Anomaly Detection Using Transformer-Based GAN With Two-Step Masking", "abstract": "Time series anomaly detection is a task that determines whether an unseen signal is normal or abnormal, and it is a crucial function in various real-world applications. Typical approach is to learn normal data representation using generative models, like Generative Adversarial Network (GAN), to discriminate between normal and abnormal signals. Recently, a few studies actively adopt Transformer to model time series data, but there is no pure Transformer-based GAN framework for time series anomaly detection. As a pioneer work, we propose a new pure Transformer-based GAN framework, called AnoFormer, and its effective training strategy for better representation learning. Specifically, we improve the detection ability of our model by introducing two-step masking strategies. The first step is Random masking: we design a random mask pool to hide parts of the signal randomly. This allows our model to learn the representation of normal data. The second step is Exclusive and Entropy-based Re-masking: we propose a novel refinement step to provide feedback to accurately model the exclusive and uncertain parts in the first step. We empirically demonstrate the effectiveness of re-masking step that generates more normal-like signals robustly. Extensive experiments on various datasets show that AnoFormer significantly outperforms the state-of-the-art methods in time series anomaly detection."}}
{"id": "m4zY4UIJuV", "cdate": 1672531200000, "mdate": 1699167446633, "content": {"title": "Improved Abdominal Multi-Organ Segmentation via 3D Boundary-Constrained Deep Neural Networks", "abstract": "Quantitative assessment of the abdominal region from CT scans requires the accurate delineation of abdominal organs. Therefore, automatic abdominal image segmentation has been the subject of intensive research for the past two decades. Recently, deep learning-based methods have resulted in state-of-the-art performance for the 3D abdominal CT segmentation. However, the complex characterization of abdominal organs with weak boundaries prevents the deep learning methods from accurate segmentation. Specifically, the voxels on the boundary of organs are more vulnerable to misprediction due to the highly-varying intensities. This paper proposes a method for improved abdominal image segmentation by leveraging organ-boundary prediction as a complementary task. We train 3D encoder-decoder networks to simultaneously segment the abdominal organs and their boundaries via multi-task learning. We explore two network topologies based on the extent of weights shared between the two tasks within a unified multi-task framework. In the first topology, the whole-organ prediction task and the boundary detection task share all the layers in the network except for the last task-specific layers. The second topology employs a single shared encoder but two separate task-specific decoders. The effectiveness of utilizing the organs\u2019 boundary information for abdominal multi-organ segmentation is evaluated on two publically available abdominal CT datasets: Pancreas-CT and the BTCV dataset. The improvements shown in segmentation results reveal the advantage of the multi-task training that forces the network to pay attention to ambiguous boundaries of organs. A maximum relative improvement of 3.5% and 3.6% is observed in Mean Dice Score for Pancreas-CT and BTCV datasets, respectively."}}
{"id": "aD8iUgLz-bL", "cdate": 1672531200000, "mdate": 1699167446372, "content": {"title": "LINe: Out-of-Distribution Detection by Leveraging Important Neurons", "abstract": "It is important to quantify the uncertainty of input samples, especially in mission-critical domains such as autonomous driving and healthcare, where failure predictions on out-of-distribution (OOD) data are likely to cause big problems. OOD detection problem fundamentally begins in that the model cannot express what it is not aware of. Post-hoc OOD detection approaches are widely explored because they do not require an additional re-training process which might degrade the model's performance and increase the training cost. In this study, from the perspective of neurons in the deep layer of the model representing high-level features, we introduce a new aspect for analyzing the difference in model outputs between in-distribution data and OOD data. We propose a novel method, Leveraging Important Neurons (LINe), for post-hoc Out of distribution detection. Shapley value-based pruning reduces the effects of noisy outputs by selecting only high-contribution neurons for predicting specific classes of input data and masking the rest. Activation clipping fixes all values above a certain threshold into the same value, allowing LINe to treat all the class-specific features equally and just consider the difference between the number of activated feature differences between in-distribution and OOD data. Comprehensive experiments verify the effectiveness of the proposed method by outperforming state-of-the-art post-hoc OOD detection methods on CIFAR-10, CIFAR-100, and ImageNet datasets."}}
{"id": "OxguMvIaNR", "cdate": 1672531200000, "mdate": 1699167446369, "content": {"title": "Toward Label-Efficient Neural Network Training: Diversity-Based Sampling in Semi-Supervised Active Learning", "abstract": "Collecting large-labeled data is an expensive and challenging issue for training deep neural networks. To address this issue, active learning is recently studied where the active learner provides informative samples for labeling. Diversity-based sampling algorithms are commonly used for representation-based active learning. In this paper, a new diversity-based sampling is introduced for semi-supervised active learning. To select more informative data at the initial stage, we devise a diversity-based initial dataset selection method by using self-supervised representation. We further propose a new active learning query strategy, which exploits both consistency and diversity. Comparative experiments show that the proposed method can outperform other active learning approaches on two public datasets."}}
{"id": "6FIA21bRHH", "cdate": 1672531200000, "mdate": 1697735240853, "content": {"title": "Interactive Segmentation for COVID-19 Infection Quantification on Longitudinal CT Scans", "abstract": "Consistent segmentation of CT scans in COVID-19 patients across multiple time points is important to accurately evaluate disease progression and therapeutic response. In medical domains, previous interactive segmentation studies have been mainly conducted on data from a single time point. However, the valuable segmentation information from previous time points is often underutilized in assisting the segmentation of a patient\u2019s follow-up scans. Moreover, fully automatic segmentation techniques frequently produce results that would need further refinement for clinical applicability. In this study, we propose a novel single-network model for interactive segmentation that fully leverages all available past information to refine the segmentation of follow-up scans. In the first segmentation round, our model takes concatenated slices of 3D volumes from two-time points (target and reference), employing the segmentation results from the reference time point as a guide for segmenting the target scan. Subsequent refinement rounds incorporate user feedback in the form of scribbles that rectify the segmentation, in addition to incorporating the previous segmentation results of the target scan. This iterative process ensures the preservation of segmentation information from prior refinement rounds. Experimental results obtained from our in-house multiclass longitudinal COVID-19 dataset demonstrate the effectiveness of the proposed method compared to its static counterpart, thus providing valuable assistance in localizing COVID-19 infections in patients\u2019 follow-up scans."}}
{"id": "ZNBDO9N76--", "cdate": 1668040656931, "mdate": 1668040656931, "content": {"title": "Multimodal facial biometrics recognition: Dual-stream convolutional neural networks with multi-feature fusion layers", "abstract": "Facial recognition for surveillance applications still remains challenging in uncontrolled environments, especially with the appearances of masks/veils and different ethnicities effects. Multimodal facial biometrics recognition becomes one of the major studies to overcome such scenarios. However, to cooperate with multimodal facial biometrics, many existing deep learning networks rely on feature concatenation or weight combination to construct a representation layer to perform its desired recognition task. This concatenation is often inefficient, as it does not effectively cooperate with the multimodal data to improve on recognition performance. Therefore, this paper proposes using multi-feature fusion layers for multimodal facial biometrics, thereby leading to significant and informative data learning in dual-stream convolutional neural networks. Specifically, this network consists of two progressive parts with distinct fusion strategies to aggregate RGB data and texture descriptors for multimodal facial biometrics. We demonstrate that the proposed network offers a discriminative feature representation and benefits from the multi-feature fusion layers for an accuracy-performance gain. We also introduce and share a new dataset for multimodal facial biometric data, namely the Ethnic-facial dataset for benchmarking. In addition, four publicly accessible datasets, namely AR, FaceScrub, IMDB_WIKI, and YouTube Face datasets are used to evaluate the proposed network. Through our experimental analysis, the proposed network outperformed several competing networks on these datasets for both recognition and verification tasks."}}
{"id": "iAWNOXfLz0", "cdate": 1652737690014, "mdate": null, "content": {"title": "AnoFormer: Time Series Anomaly Detection using Transformer-based GAN with Two-Step Masking", "abstract": "Time series anomaly detection is a task that determines whether an unseen signal is normal or abnormal, and it is a crucial function in various real-world applications. Typical approach is to learn normal data representation using generative models, like Generative Adversarial Network (GAN), to discriminate between normal and abnormal signals. Recently, a few studies actively adopt transformer to model time series data, but there is no transformer-based GAN framework for time series anomaly detection. As a pioneer work, we propose a new transformer-based GAN framework, called AnoFormer, and its effective training strategy for better representation learning. Specifically, we improve the detection ability of our model by introducing two-step masking strategies. The first step is \\textit{Random masking}: we design a random mask pool to hide parts of the signal randomly. This allows our model to learn the representation of normal data. The second step is \\textit{Exclusive and Entropy-based Re-masking}: we propose a novel refinement step to provide feedback to accurately model the exclusive and uncertain parts in the first step. We empirically demonstrate the effectiveness of re-masking step that our model generates more normal-like signals robustly. Extensive experiments on various datasets show that AnoFormer significantly outperforms the state-of-the-art methods in time series anomaly detection."}}
{"id": "c34Y2FlETL", "cdate": 1640995200000, "mdate": 1699167446466, "content": {"title": "Exploiting Diversity of Unlabeled Data for Label-Efficient Semi-Supervised Active Learning", "abstract": "The availability of large labeled datasets is the key component for the success of deep learning. However, annotating labels on large datasets is generally time-consuming and expensive. Active learning is a research area that addresses the issues of expensive labeling by selecting the most important samples for labeling. Diversity-based sampling algorithms are known as integral components of representation-based approaches for active learning. In this paper, we introduce a new diversity-based initial dataset selection algorithm to select the most informative set of samples for initial labeling in the active learning setting. Self-supervised representation learning is used to consider the diversity of samples in the initial dataset selection algorithm. Also, we propose a novel active learning query strategy, which uses diversity-based sampling on consistency-based embeddings. By considering the consistency information with the diversity in the consistency-based embedding scheme, the proposed method could select more informative samples for labeling in the semi-supervised learning setting. Comparative experiments show that the proposed method achieves compelling results on CIFAR-10 and Caltech-101 datasets compared with previous active learning approaches by utilizing the diversity of unlabeled data."}}
