{"id": "AV_bv4Ydcr9", "cdate": 1663850107888, "mdate": null, "content": {"title": "Attention Enables Zero Approximation Error", "abstract": "Attention-based architectures become the core backbone of many state-of-the-art models for various tasks, including language translation and image classification. However, theoretical properties of attention-based models are seldom considered. In this work, we show that with suitable adaptations, the single-head self-attention transformer with a fixed number of transformer encoder blocks and free parameters is able to generate any desired polynomial of the input with no error. The number of transformer encoder blocks is the same as the degree of the target polynomial. Even more exciting, we find that these transformer encoder blocks in this model do not need to be trained. As a direct consequence, we show that the single-head self-attention transformer with increasing numbers of free parameters is universal. Also, we show that our proposed model can avoid the classical trade-off between approximation error and sample error in the mean squared error analysis for the regression task if the target function is a polynomial. We conduct various experiments and ablation studies to verify our theoretical results."}}
{"id": "P7TayMSBhnV", "cdate": 1652737541446, "mdate": null, "content": {"title": "Stability and Generalization for Markov Chain Stochastic Gradient Methods", "abstract": "Recently there is a large amount of work devoted to the study of Markov chain stochastic gradient methods (MC-SGMs)  which mainly focus on their convergence analysis for solving minimization problems. In this paper, we provide a comprehensive generalization analysis of MC-SGMs for both minimization and minimax problems through the lens of algorithmic stability in the framework of statistical learning theory. For empirical risk minimization (ERM) problems, we establish the optimal excess population risk bounds for both smooth and non-smooth cases by introducing on-average argument stability. For minimax problems, we develop a quantitative connection between on-average argument stability and generalization error which extends the existing results for uniform stability (Lei et al., 2021). We further develop the first nearly optimal convergence rates for convex-concave problems both in expectation and with high probability, which, combined with our stability results, show that the optimal generalization bounds can be attained for both smooth and non-smooth cases. To the best of our knowledge, this is the first generalization analysis of SGMs when the gradients are sampled from a Markov process.   \n "}}
{"id": "aBF8l_jsZP_", "cdate": 1620330661501, "mdate": null, "content": {"title": "Theory of deep convolutional neural networks II: Spherical analysis", "abstract": "powerful in many practical applications, but it lacks enough theoretical verifications. In this paper,\nwe consider a family of deep convolutional neural networks applied to approximate functions on the\nunit sphere Sd\u22121 of Rd. Our analysis presents rates of uniform approximation when the approximated\nfunction lies in the Sobolev space Wr\u221e\n(Sd\u22121) with r > 0 or takes an additive ridge form. Our work\nverifies theoretically the modelling and approximation ability of deep convolutional neural networks\nfollowed by downsampling and one fully connected layer or two. The key idea of our spherical analysis\nis to use the inner product form of the reproducing kernels of the spaces of spherical harmonics and\nthen to apply convolutional factorizations of filters to realize the generated linear features."}}
{"id": "t5p5MmsqAQL", "cdate": 1620330567337, "mdate": null, "content": {"title": "Theory of deep convolutional neural networks: Downsampling", "abstract": "Establishing a solid theoretical foundation for structured deep neural networks is greatly desired\ndue to the successful applications of deep learning in various practical domains. This paper aims\nat an approximation theory of deep convolutional neural networks whose structures are induced by\nconvolutions. To overcome the difficulty in theoretical analysis of the networks with linearly increasing\nwidths arising from convolutions, we introduce a downsampling operator to reduce the widths. We\nprove that the downsampled deep convolutional neural networks can be used to approximate ridge\nfunctions nicely, which hints some advantages of these structured networks in terms of approximation\nor modeling. We also prove that the output of any multi-layer fully-connected neural network can\nbe realized by that of a downsampled deep convolutional neural network with free parameters of\nthe same order, which shows that in general, the approximation ability of deep convolutional neural\nnetworks is at least as good as that of fully-connected networks. Finally, a theorem for approximating\nfunctions on Riemannian manifolds is presented, which demonstrates that deep convolutional neural\nnetworks can be used to learn manifold features of data."}}
{"id": "6ZJCJJuwyp3", "cdate": 1620330443944, "mdate": null, "content": {"title": "Universality of deep convolutional neural networks", "abstract": "Deep learning has been widely applied and brought breakthroughs in speech recognition, computer vision, and many other domains.\nDeep neural network architectures and computational issues have been well studied in machine learning.\nBut there lacks a theoretical foundation for understanding the approximation or generalization ability of deep learning methods generated by the network  architectures such as\ndeep convolutional neural networks. Here we show that a deep convolutional neural network (CNN) is universal, meaning that\nit can be used to approximate any continuous function to an arbitrary accuracy\nwhen the depth of the neural network is large enough. This answers an open question in learning theory.\nOur quantitative estimate, given tightly in terms of the number of free parameters to be computed, verifies the efficiency of deep CNNs in dealing with large dimensional data.\nOur study also demonstrates the role of convolutions in deep CNNs."}}
{"id": "PZOJIzK0XY", "cdate": 1609899662263, "mdate": null, "content": {"title": "Sparsity and Error Analysis of Empirical Feature-Based Regularization Schemes", "abstract": "We consider a learning algorithm generated by a regularization scheme with a concave regularizer for the purpose of achieving sparsity and good learning rates in a least squares regression setting. The regularization is induced for linear combinations of empirical features, constructed in the literatures of kernel principal component analysis and kernel projection machines, based on kernels and samples. In addition to the separability of the involved optimization problem caused by the empirical features, we carry out sparsity and error analysis, giving bounds in the norm of the reproducing kernel Hilbert space, based on a priori conditions which do not require assumptions on sparsity in terms of any basis or system. In particular, we show that as the concave exponent q of the concave regularizer increases to 1, the learning ability of the algorithm improves. Some numerical simulations for both artificial and real MHC-peptide binding data involving the \u2113q regularizer and the SCAD penalty are presented to demonstrate the sparsity and error analysis."}}
{"id": "ZkWLNqv7jHq", "cdate": 1609899398589, "mdate": null, "content": {"title": "Distributed Learning with Regularized Least Squares", "abstract": "We study distributed learning with the least squares regularization scheme in a reproducing kernel Hilbert space (RKHS). By a divide-and-conquer approach, the algorithm partitions a data set into disjoint data subsets, applies the least squares regularization scheme to each data subset to produce an output function, and then takes an average of the individual output functions as a final global estimator or predictor. We show with error bounds and learning rates in expectation in both the L2-metric and RKHS-metric that the global output function of this distributed learning is a good approximation to the algorithm processing the whole data in one single machine. Our derived learning rates in expectation are optimal and stated in a general setting without any eigenfunction assumption. The analysis is achieved by a novel second order decomposition of operator differences in our integral operator approach. Even for the classical least squares regularization scheme in the RKHS associated with a general kernel, we give the best learning rate in expectation in the literature."}}
{"id": "BJg15lrKvS", "cdate": 1569439879384, "mdate": null, "content": {"title": "Towards Understanding the Spectral Bias of Deep Learning", "abstract": "An intriguing phenomenon observed during training neural networks is the spectral bias, where neural networks are biased towards learning less complex functions. The priority of learning functions with low complexity might be at the core of explaining generalization ability of neural network, and certain efforts have been made to provide theoretical explanation for spectral bias. However, there is still no satisfying theoretical results justifying the existence of spectral bias. In this work, we give a comprehensive and rigorous explanation for spectral bias and relate it with the neural tangent kernel function proposed in recent work. We prove that the training process of neural networks can be decomposed along different directions defined by the eigenfunctions of the neural tangent kernel, where each direction has its own convergence rate and the rate is determined by the corresponding eigenvalue. We then provide a case study when the input data is uniformly distributed over the unit shpere, and show that lower degree spherical harmonics are easier to be learned by over-parameterized neural networks."}}
