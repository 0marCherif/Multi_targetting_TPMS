{"id": "UR7x0ZYXMrO", "cdate": 1681751591200, "mdate": 1681751591200, "content": {"title": "Bilevel Entropy based Mechanism Design for Balancing Meta in Video Games", "abstract": "We address a mechanism design problem where the goal of the\ndesigner is to maximize the entropy of a player\u2019s mixed strategy at\na Nash equilibrium. This objective is of special relevance to video\ngames where game designers wish to diversify the players\u2019 inter-\naction with the game. To solve this design problem, we propose a\nbi-level alternating optimization technique that (1) approximates\nthe mixed strategy Nash equilibrium using a Nash Monte-Carlo\nreinforcement learning approach and (2) applies a gradient-free op-\ntimization technique (Covariance-Matrix Adaptation Evolutionary\nStrategy) to maximize the entropy of the mixed strategy obtained in\nlevel (1). The experimental results show that our approach achieves\ncomparable results to the state-of-the-art approach on three bench-\nmark domains \u201cRock-Paper-Scissors-Fire-Water\u201d, \u201cWorkshop War-\nfare\u201d and \u201cPokemon Video Game Championship\u201d. Next, we show\nthat, unlike previous state-of-the-art approaches, the computational\ncomplexity of our proposed approach scales significantly better in\nlarger combinatorial strategy spaces."}}
{"id": "33wyZ4xTIx", "cdate": 1664725483239, "mdate": null, "content": {"title": "The (Un)Scalability of Heuristic Approximators for NP-Hard Search Problems", "abstract": "The A* algorithm is commonly used to solve \\cNP-hard combinatorial optimization problems. When provided with a completely informed heuristic function, A* solves many \\cNP-hard minimum-cost path problems in time polynomial in the branching factor and the number of edges in a minimum-cost path. Thus, approximating their completely informed heuristic functions with high precision is \\cNP-hard. We, therefore, examine recent publications that propose the use of neural networks for this purpose. We support our claim that these approaches do not scale to large instance sizes both theoretically and experimentally. Our first experimental results for three representative \\cNP-hard minimum-cost path problems suggest that using neural networks to approximate completely informed heuristic functions with high precision might result in network sizes that scale exponentially in the instance sizes. The research community might thus benefit from investigating other ways of integrating heuristic search with machine learning."}}
{"id": "zMnkDSQ9PI", "cdate": 1640995200000, "mdate": 1681751020866, "content": {"title": "A Joint Imitation-Reinforcement Learning Framework for Reduced Baseline Regret", "abstract": "In various control task domains, existing controllers provide a baseline level of performance that -- though possibly suboptimal -- should be maintained. Reinforcement learning (RL) algorithms that rely on extensive exploration of the state and action space can be used to optimize a control policy. However, fully exploratory RL algorithms may decrease performance below a baseline level during training. In this paper, we address the issue of online optimization of a control policy while minimizing regret w.r.t a baseline policy performance. We present a joint imitation-reinforcement learning framework, denoted JIRL. The learning process in JIRL assumes the availability of a baseline policy and is designed with two objectives in mind \\textbf{(a)} leveraging the baseline's online demonstrations to minimize the regret w.r.t the baseline policy during training, and \\textbf{(b)} eventually surpassing the baseline performance. JIRL addresses these objectives by initially learning to imitate the baseline policy and gradually shifting control from the baseline to an RL agent. Experimental results show that JIRL effectively accomplishes the aforementioned objectives in several, continuous action-space domains. The results demonstrate that JIRL is comparable to a state-of-the-art algorithm in its final performance while incurring significantly lower baseline regret during training in all of the presented domains. Moreover, the results show a reduction factor of up to $21$ in baseline regret over a state-of-the-art baseline regret minimization approach."}}
{"id": "gFFH3SyTpJ", "cdate": 1640995200000, "mdate": 1681751020867, "content": {"title": "The (Un)Scalability of Heuristic Approximators for NP-Hard Search Problems", "abstract": "The A* algorithm is commonly used to solve NP-hard combinatorial optimization problems. When provided with a completely informed heuristic function, A* solves many NP-hard minimum-cost path problems in time polynomial in the branching factor and the number of edges in a minimum-cost path. Thus, approximating their completely informed heuristic functions with high precision is NP-hard. We therefore examine recent publications that propose the use of neural networks for this purpose. We support our claim that these approaches do not scale to large instance sizes both theoretically and experimentally. Our first experimental results for three representative NP-hard minimum-cost path problems suggest that using neural networks to approximate completely informed heuristic functions with high precision might result in network sizes that scale exponentially in the instance sizes. The research community might thus benefit from investigating other ways of integrating heuristic search with machine learning."}}
{"id": "1mCJe-Bu4J", "cdate": 1640995200000, "mdate": 1681751020875, "content": {"title": "A Discussion on the Scalability of Heuristic Approximators (Extended Abstract)", "abstract": "In this work, we examine a line of recent publications that propose to use deep neural networks to approximate the goal distances of states for heuristic search. We present a first step toward showing that this work suffers from inherent scalability limitations since --- under the assumption that P\u2260NP --- such approaches require network sizes that scale exponentially in the number of states to achieve the necessary (high) approximation accuracy."}}
{"id": "HIag61ATTRA", "cdate": 1609459200000, "mdate": 1681751020873, "content": {"title": "A Joint Imitation-Reinforcement Learning Framework for Reduced Baseline Regret", "abstract": "In various control task domains, existing controllers provide a baseline level of performance that\u2014though possibly suboptimal\u2014should be maintained. Reinforcement learning (RL) algorithms that rely on extensive exploration of the state and action space can be used to optimize a control policy. However, fully exploratory RL algorithms may decrease performance below a baseline level during training. In this paper, we address the issue of online optimization of a control policy while minimizing regret with respect to a baseline policy performance. We present a joint imitation-reinforcement learning framework, denoted JIRL. The learning process in JIRL assumes the availability of a baseline policy and is designed with two objectives in mind (a) training while leveraging demonstrations from the baseline policy to minimize regret with respect to the baseline policy, and (b) eventually surpassing the baseline performance. JIRL addresses these objectives by initially learning to imitate the baseline policy and gradually shifting control from the baseline to an RL agent. Experimental results show that JIRL effectively accomplishes the aforementioned objectives in several, continuous action-space domains. The results demonstrate that JIRL is comparable to a state-of-the-art algorithm in its final performance while incurring significantly lower baseline regret during training. Moreover, the results show a reduction factor of up to 21 in baseline regret over a trust-region-based approach that guarantees monotonic policy improvement."}}
