{"id": "1g1zg2dgIH", "cdate": 1676827079458, "mdate": null, "content": {"title": "Simple Transferability Estimation for Regression Tasks", "abstract": "We consider transferability estimation, the problem of estimating how well deep learning models transfer from a source to a target task. We focus on regression tasks, which received little previous attention, and propose two simple and computationally efficient approaches that estimate transferability based on the negative regularized mean squared error of a linear regression model. We prove novel theoretical results connecting our approaches to the actual transferability of the optimal target models obtained from the transfer learning process. Despite their simplicity, our approaches significantly outperform existing state-of-the-art regression transferability estimators in both accuracy and efficiency. On two large-scale keypoint regression benchmarks, our approaches yield 12% to 36% better results on average while being at least 27% faster than previous state-of-the-art methods."}}
{"id": "2dCAMPKgCo", "cdate": 1664928783128, "mdate": null, "content": {"title": "Transferability Between Regression Tasks", "abstract": "Transfer learning has been a widely used technique to adapt a deep learning model trained for one task to another when there is a data distribution shift between these tasks. To improve the effectiveness of transfer learning and to understand relationships between tasks, we consider the problem of transferability estimation between regression tasks and propose two novel transferability estimators that are simple, computationally efficient, yet effective and theoretically grounded. We test our proposed methods extensively in various challenging, practical scenarios and show they significantly outperform existing state-of-the-art regression task transferability estimators in both accuracy and efficiency."}}
{"id": "LB6KMRUqng2", "cdate": 1663850022205, "mdate": null, "content": {"title": "Transferability Between Regression Tasks", "abstract": "We consider the problem of estimating how well deep neural network regression models would transfer from source to target tasks. We focus on regression tasks, which received little previous attention, and develop novel transferability estimation methods that are simple, computationally efficient, yet effective and theoretically grounded. We propose two families of transferability estimators, both of which utilize the mean squared error of a regularized linear regression model to estimate the transferability. We prove novel theoretical bounds connecting our methods with the expected risk of the optimal target models obtained from the actual transfer learning process. We test our methods extensively in various challenging, practical scenarios and show they significantly outperform existing state-of-the-art regression task transferability estimators, in both accuracy and efficiency."}}
{"id": "RWRcZpVNLuH", "cdate": 1648702250052, "mdate": 1648702250052, "content": {"title": "Probablistic Path Hamiltonian Monte Carlo", "abstract": "Hamiltonian Monte Carlo (HMC) is an efficient and effective means of sampling posterior distributions on Euclidean space, which has been extended to manifolds with boundary. However,some applications require an extension to more general spaces. For example, phylogenetic (evolutionary) trees are defined in terms of both a discrete graph and associated continuous parameters; although one can represent these aspects using a single connected space, this rather complex space is not suitable for existing HMC algorithms. In this paper, we develop Probabilistic Path HMC (PPHMC) as a first step to sampling distributions on spaces with intricate combinatorial structure. We define PPHMC on orthant complexes, show that the resulting Markov chain is ergodic, and provide a promising implementation for the case of phylogenetic trees in opensource software. We also show that a surrogate function to ease the transition across a boundary on which the log-posterior has discontinuous derivatives can greatly improve efficiency."}}
{"id": "S6fb8bi7rgq", "cdate": 1640995200000, "mdate": 1645718270229, "content": {"title": "Bayesian active learning with abstention feedbacks", "abstract": "We study pool-based active learning with abstention feedbacks where a labeler can abstain from labeling a queried example with some unknown abstention rate. This is an important problem with many useful applications. We take a Bayesian approach to the problem and develop two new greedy algorithms that learn both the classification problem and the unknown abstention rate at the same time. These are achieved by simply incorporating the estimated average abstention rate into the greedy criteria. We prove that both algorithms have near-optimality guarantees: they respectively achieve a ( 1 - 1 e ) constant factor approximation of the optimal expected or worst-case value of a useful utility function. Our experiments show the algorithms perform well in various practical scenarios."}}
{"id": "SfWIZomHe9", "cdate": 1609459200000, "mdate": 1645718270230, "content": {"title": "Searching for Minimal Optimal Neural Networks", "abstract": "Large neural network models have high predictive power but may suffer from overfitting if the training set is not large enough. Therefore, it is desirable to select an appropriate size for neural networks. The destructive approach, which starts with a large architecture and then reduces the size using a Lasso-type penalty, has been used extensively for this task. Despite its popularity, there is no theoretical guarantee for this technique. Based on the notion of minimal neural networks, we posit a rigorous mathematical framework for studying the asymptotic theory of the destructive technique. We prove that Adaptive group Lasso is consistent and can reconstruct the correct number of hidden nodes of one-hidden-layer feedforward networks with high probability. To the best of our knowledge, this is the first theoretical result establishing for the destructive technique."}}
{"id": "O2DVmaf8Qeg", "cdate": 1577836800000, "mdate": null, "content": {"title": "Posterior concentration and fast convergence rates for generalized Bayesian learning", "abstract": "In this paper, we study the learning rate of generalized Bayes estimators in a general setting where the hypothesis class can be uncountable and have an irregular shape, the loss function can have heavy tails, and the optimal hypothesis may not be unique. We prove that under the multi-scale Bernstein\u2019s condition, the generalized posterior distribution concentrates around the set of optimal hypotheses and the generalized Bayes estimator can achieve fast learning rate. Our results are applied to show that the standard Bayesian linear regression is robust to heavy-tailed distributions."}}
{"id": "LA5QoZKpcX_", "cdate": 1577836800000, "mdate": null, "content": {"title": "Consistent feature selection for analytic deep neural networks", "abstract": "One of the most important steps toward interpretability and explainability of neural network models is feature selection, which aims to identify the subset of relevant features. Theoretical results in the field have mostly focused on the prediction aspect of the problem with virtually no work on feature selection consistency for deep neural networks due to the model's severe nonlinearity and unidentifiability. This lack of theoretical foundation casts doubt on the applicability of deep learning to contexts where correct interpretations of the features play a central role. In this work, we investigate the problem of feature selection for analytic deep networks. We prove that for a wide class of networks, including deep feed-forward neural networks, convolutional neural networks and a major sub-class of residual neural networks, the Adaptive Group Lasso selection procedure with Group Lasso as the base estimator is selection-consistent. The work provides further evidence that Group Lasso might be inefficient for feature selection with neural networks and advocates the use of Adaptive Group Lasso over the popular Group Lasso."}}
{"id": "Iip-l4WL-D", "cdate": 1577836800000, "mdate": null, "content": {"title": "Consistent feature selection for neural networks via Adaptive Group Lasso", "abstract": "One main obstacle for the wide use of deep learning in medical and engineering sciences is its interpretability. While neural network models are strong tools for making predictions, they often provide little information about which features play significant roles in influencing the prediction accuracy. To overcome this issue, many regularization procedures for learning with neural networks have been proposed for dropping non-significant features. Unfortunately, the lack of theoretical results casts doubt on the applicability of such pipelines. In this work, we propose and establish a theoretical guarantee for the use of the adaptive group lasso for selecting important features of neural networks. Specifically, we show that our feature selection method is consistent for single-output feed-forward neural networks with one hidden layer and hyperbolic tangent activation function. We demonstrate its applicability using both simulation and data analysis."}}
{"id": "5JcqIbmHZm", "cdate": 1546300800000, "mdate": null, "content": {"title": "An active learning framework for set inversion", "abstract": "Set inversion is a classical problem in control theory that has many important applications in various fields of science and engineering. The state-of-the-art method for solving this problem, Set Inverter Via Interval Analysis (SIVIA), usually does not work well in high dimensions and often fails to recover sets with complicated structures. In this work, we propose a new approach to the problem of set inversion, which employs techniques from machine learning to resolve these issues. Our algorithm can handle problems in high dimensions and achieve the same level of accuracy with fewer data points compared to SIVIA. We illustrate the performance of our method in various simulation studies and apply it to investigate the dynamics of the 17th-century plague in Eyam village, England."}}
