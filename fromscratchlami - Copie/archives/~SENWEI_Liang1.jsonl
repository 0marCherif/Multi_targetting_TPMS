{"id": "ivJdk85zKBi", "cdate": 1683560229985, "mdate": null, "content": {"title": "Quantifying the spatial homogeneity of urban road networks via graph neural networks", "abstract": "Quantifying the topological similarities of different parts of urban road networks enables us to understand urban growth patterns. Although conventional statistics provide useful information about the characteristics of either a single node\u2019s direct neighbours or the entire network, such metrics fail to measure the similarities of subnetworks or capture local, indirect neighbourhood relationships. Here we propose a graph-based machine learning method to quantify the spatial homogeneity of subnetworks. We apply the method to 11,790 urban road networks across 30 cities worldwide to measure the spatial homogeneity of road networks within each city and across different cities. We find that intracity spatial homogeneity is highly associated with socioeconomic status indicators such as gross domestic product and population growth. Moreover, intercity spatial homogeneity values obtained by transferring the model across different cities reveal the intercity similarity of urban network structures originating in Europe, passed on to cities in the United States and Asia. The socioeconomic development and intercity similarity revealed using our method can be leveraged to understand and transfer insights between cities. It also enables us to address urban policy challenges including network planning in rapidly urbanizing areas and regional inequality."}}
{"id": "6RBgOrJZSA", "cdate": 1680549447907, "mdate": 1680549447907, "content": {"title": "AlterSGD: Finding Flat Minima for Continual Learning by Alternative Training", "abstract": "Deep neural networks suffer from catastrophic forgetting when learning multiple knowledge sequentially, and a growing number of approaches have been proposed to mitigate this problem. Some of these methods achieved considerable performance by associating the flat local minima with forgetting mitigation in continual learning. However, they inevitably need (1) tedious hyperparameters tuning, and (2) additional computational cost. To alleviate these problems, in this paper, we propose a simple yet effective optimization method, called AlterSGD, to search for a flat minima in the loss landscape. In AlterSGD, we conduct gradient descent and ascent alternatively when the network tends to converge at each session of learning new knowledge. Moreover, we theoretically prove that such a strategy can encourage the optimization to converge to a flat minima. We verify AlterSGD on continual learning benchmark for semantic segmentation and the empirical results show that we can significantly mitigate the forgetting and outperform the state-of-the-art methods with a large margin under challenging continual learning protocols."}}
{"id": "svPXnSJmOg", "cdate": 1667334247786, "mdate": 1667334247786, "content": {"title": "AlterSGD: Finding Flat Minima for Continual Learning by Alternative Training", "abstract": "Deep neural networks suffer from catastrophic forgetting when learning multiple knowledge sequentially, and a growing number of approaches have been proposed to mitigate this problem. Some of these methods achieved considerable performance by associating the flat local minima with forgetting mitigation in continual learning. However, they inevitably need (1) tedious hyperparameters tuning, and (2) additional computational cost. To alleviate these problems, in this paper, we propose a simple yet effective optimization method, called AlterSGD, to search for a flat minima in the loss landscape. In AlterSGD, we conduct gradient descent and ascent alternatively when the network tends to converge at each session of learning new knowledge. Moreover, we theoretically prove that such a strategy can encourage the optimization to converge to a flat minima. We verify AlterSGD on continual learning benchmark for semantic segmentation and the empirical results show that we can significantly mitigate the forgetting and outperform the state-of-the-art methods with a large margin under\nchallenging continual learning protocols."}}
{"id": "fmxPu4gfu_W", "cdate": 1640995200000, "mdate": 1667337387577, "content": {"title": "Finite Expression Method for Solving High-Dimensional Partial Differential Equations", "abstract": "Designing efficient and accurate numerical solvers for high-dimensional partial differential equations (PDEs) remains a challenging and important topic in computational science and engineering, mainly due to the \"curse of dimensionality\" in designing numerical schemes that scale in dimension. This paper introduces a new methodology that seeks an approximate PDE solution in the space of functions with finitely many analytic expressions and, hence, this methodology is named the finite expression method (FEX). It is proved in approximation theory that FEX can avoid the curse of dimensionality. As a proof of concept, a deep reinforcement learning method is proposed to implement FEX for various high-dimensional PDEs in different dimensions, achieving high and even machine accuracy with a memory complexity polynomial in dimension and an amenable time complexity. An approximate solution with finite analytic expressions also provides interpretable insights into the ground truth PDE solution, which can further help to advance the understanding of physical systems and design postprocessing techniques for a refined solution."}}
{"id": "Tk9RThQpo7", "cdate": 1640995200000, "mdate": 1667337387619, "content": {"title": "Accelerating Numerical Solvers for Large-Scale Simulation of Dynamical System via NeurVec", "abstract": "Ensemble-based large-scale simulation of dynamical systems is essential to a wide range of science and engineering problems. Conventional numerical solvers used in the simulation are significantly limited by the step size for time integration, which hampers efficiency and feasibility especially when high accuracy is desired. To overcome this limitation, we propose a data-driven corrector method that allows using large step sizes while compensating for the integration error for high accuracy. This corrector is represented in the form of a vector-valued function and is modeled by a neural network to regress the error in the phase space. Hence we name the corrector neural vector (NeurVec). We show that NeurVec can achieve the same accuracy as traditional solvers with much larger step sizes. We empirically demonstrate that NeurVec can accelerate a variety of numerical solvers significantly and overcome the stability restriction of these solvers. Our results on benchmark problems, ranging from high-dimensional problems to chaotic systems, suggest that NeurVec is capable of capturing the leading error term and maintaining the statistics of ensemble forecasts."}}
{"id": "MwOLKJfKRg", "cdate": 1640995200000, "mdate": 1667337387575, "content": {"title": "Stiffness-aware neural network for learning Hamiltonian systems", "abstract": "We propose stiffness-aware neural network (SANN), a new method for learning Hamiltonian dynamical systems from data. SANN identifies and splits the training data into stiff and nonstiff portions based on a stiffness-aware index, a simple, yet effective metric we introduce to quantify the stiffness of the dynamical system. This classification along with a resampling technique allows us to apply different time integration strategies such as step size adaptation to better capture the dynamical characteristics of the Hamiltonian vector fields. We evaluate SANN on complex physical systems including a three-body problem and billiard model. We show that SANN is more stable and can better preserve energy when compared with the state-of-the-art methods, leading to significant improvement in accuracy."}}
{"id": "75oDZ9LUC5", "cdate": 1640995200000, "mdate": 1667337387582, "content": {"title": "Quantifying the spatial homogeneity of urban road networks via graph neural networks", "abstract": "The spatial homogeneity of urban road networks can be quantified in a fine-grained manner with graph neural networks. This method is studied across 11,790 inner-city road networks around the world and can be used to study socioeconomic development and help with urban planning."}}
{"id": "-3fK2fORqL", "cdate": 1640995200000, "mdate": 1667337387573, "content": {"title": "The Lottery Ticket Hypothesis for Self-attention in Convolutional Neural Network", "abstract": "Recently many plug-and-play self-attention modules (SAMs) are proposed to enhance the model generalization by exploiting the internal information of deep convolutional neural networks (CNNs). In general, previous works ignore where to plug in the SAMs since they connect the SAMs individually with each block of the entire CNN backbone for granted, leading to incremental computational cost and the number of parameters with the growth of network depth. However, we empirically find and verify some counterintuitive phenomena that: (a) Connecting the SAMs to all the blocks may not always bring the largest performance boost, and connecting to partial blocks would be even better; (b) Adding the SAMs to a CNN may not always bring a performance boost, and instead it may even harm the performance of the original CNN backbone. Therefore, we articulate and demonstrate the Lottery Ticket Hypothesis for Self-attention Networks: a full self-attention network contains a subnetwork with sparse self-attention connections that can (1) accelerate inference, (2) reduce extra parameter increment, and (3) maintain accuracy. In addition to the empirical evidence, this hypothesis is also supported by our theoretical evidence. Furthermore, we propose a simple yet effective reinforcement-learning-based method to search the ticket, i.e., the connection scheme that satisfies the three above-mentioned conditions. Extensive experiments on widely-used benchmark datasets and popular self-attention networks show the effectiveness of our method. Besides, our experiments illustrate that our searched ticket has the capacity of transferring to some vision tasks, e.g., crowd counting and segmentation."}}
{"id": "uVXEKeqJbNa", "cdate": 1632875527853, "mdate": null, "content": {"title": "Stiffness-aware neural network for learning Hamiltonian systems", "abstract": "We propose stiffness-aware neural network (SANN), a new method for learning Hamiltonian dynamical systems from data. SANN identifies and splits the training data into stiff and nonstiff portions based on a stiffness-aware index, a simple, yet effective metric we introduce to quantify the stiffness of the dynamical system. This classification along with a resampling technique allows us to apply different time integration strategies such as step size adaptation to better capture the dynamical characteristics of the Hamiltonian vector fields. We evaluate SANN on complex physical systems including a three-body problem and  billiard model. We show that SANN is more stable and can better preserve energy when compared with the state-of-the-art methods, leading to significant improvement in accuracy."}}
{"id": "nkZALRB8np", "cdate": 1609459200000, "mdate": 1667337387580, "content": {"title": "AlterSGD: Finding Flat Minima for Continual Learning by Alternative Training", "abstract": "Deep neural networks suffer from catastrophic forgetting when learning multiple knowledge sequentially, and a growing number of approaches have been proposed to mitigate this problem. Some of these methods achieved considerable performance by associating the flat local minima with forgetting mitigation in continual learning. However, they inevitably need (1) tedious hyperparameters tuning, and (2) additional computational cost. To alleviate these problems, in this paper, we propose a simple yet effective optimization method, called AlterSGD, to search for a flat minima in the loss landscape. In AlterSGD, we conduct gradient descent and ascent alternatively when the network tends to converge at each session of learning new knowledge. Moreover, we theoretically prove that such a strategy can encourage the optimization to converge to a flat minima. We verify AlterSGD on continual learning benchmark for semantic segmentation and the empirical results show that we can significantly mitigate the forgetting and outperform the state-of-the-art methods with a large margin under challenging continual learning protocols."}}
