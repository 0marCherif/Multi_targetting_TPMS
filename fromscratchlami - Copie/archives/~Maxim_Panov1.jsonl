{"id": "uBqcJ8QEe1", "cdate": 1676827090782, "mdate": null, "content": {"title": "Learning from Low Rank Tensor Data: A Random Tensor Theory Perspective", "abstract": "Under a simplified data model, this paper provides a theoretical analysis of learning from data that have an underlying low-rank tensor structure in both supervised and unsupervised settings. For the supervised setting, we provide an analysis of a Ridge classifier (with high regularization parameter) with and without knowledge of the low-rank structure of the data. Our results quantify analytically the gain in misclassification errors achieved by exploiting the low-rank structure for denoising purposes, as opposed to treating data as mere vectors. We further provide a similar analysis in the context of clustering, thereby quantifying the exact performance gap between tensor methods and standard approaches which treat data as simple vectors."}}
{"id": "qALZADB_R0Q", "cdate": 1672531200000, "mdate": 1681652388353, "content": {"title": "Scalable Batch Acquisition for Deep Bayesian Active Learning", "abstract": ""}}
{"id": "UqiiZ9Xdlrw", "cdate": 1665251223265, "mdate": null, "content": {"title": "Distributional deep Q-learning with CVaR regression", "abstract": "Reinforcement learning (RL) allows an agent interacting sequentially with an environment to maximize its long-term return, in expectation. In distributional RL (DRL), the agent is also interested in the probability distribution of the return, not just its expected value. This so-called distributional perspective of RL has led to new algorithms with improved empirical performance. In this paper, we recall the atomic DRL (ADRL) framework based on atomic distributions projected via the Wasserstein-2 metric. Then, we derive two new deep ADRL algorithms, namely SAD-Q-learning and MAD-Q-learning (both for the control task). Numerical experiments on various environments compare our approach against existing deep (distributional) RL methods."}}
{"id": "v6NNlubbSQ", "cdate": 1652737787850, "mdate": null, "content": {"title": "Nonparametric Uncertainty Quantification for Single Deterministic Neural Network", "abstract": "  This paper proposes a fast and scalable method for uncertainty quantification of machine learning models' predictions. First, we show the principled way to measure the uncertainty of predictions for a classifier based on Nadaraya-Watson's nonparametric estimate of the conditional label distribution. Importantly, the approach allows to disentangle explicitly \\textit{aleatoric} and \\textit{epistemic} uncertainties. The resulting method works directly in the feature space. However, one can apply it to any neural network by considering an embedding of the data induced by the network. We demonstrate the strong performance of the method in uncertainty estimation tasks on text classification problems and a variety of real-world image datasets, such as MNIST, SVHN, CIFAR-100 and several versions of ImageNet."}}
{"id": "yKwZTnqbEK6W", "cdate": 1640995200000, "mdate": 1668715842284, "content": {"title": "Towards OOD Detection in Graph Classification from Uncertainty Estimation Perspective", "abstract": "The problem of out-of-distribution detection for graph classification is far from being solved. The existing models tend to be overconfident about OOD examples or completely ignore the detection task. In this work, we consider this problem from the uncertainty estimation perspective and perform the comparison of several recently proposed methods. In our experiment, we find that there is no universal approach for OOD detection, and it is important to consider both graph representations and predictive categorical distribution."}}
{"id": "tnSHU5EWPy", "cdate": 1640995200000, "mdate": 1681652389301, "content": {"title": "Scalable computation of prediction intervals for neural networks via matrix sketching", "abstract": ""}}
{"id": "jdHR2JPcyW", "cdate": 1640995200000, "mdate": 1681652388164, "content": {"title": "Active Learning for Abstractive Text Summarization", "abstract": ""}}
{"id": "a_ZL4LJCObS", "cdate": 1640995200000, "mdate": 1667893452309, "content": {"title": "ScaleFace: Uncertainty-aware Deep Metric Learning", "abstract": "The performance of modern deep learning-based systems dramatically depends on the quality of input objects. For example, face recognition quality would be lower for blurry or corrupted inputs. However, it is hard to predict the influence of input quality on the resulting accuracy in more complex scenarios. We propose an approach for deep metric learning that allows direct estimation of the uncertainty with almost no additional computational cost. The developed \\textit{ScaleFace} algorithm uses trainable scale values that modify similarities in the space of embeddings. These input-dependent scale values represent a measure of confidence in the recognition result, thus allowing uncertainty estimation. We provide comprehensive experiments on face recognition tasks that show the superior performance of ScaleFace compared to other uncertainty-aware face recognition approaches. We also extend the results to the task of text-to-image retrieval showing that the proposed approach beats the competitors with significant margin."}}
{"id": "_qhSc7CTjN", "cdate": 1640995200000, "mdate": 1664551095591, "content": {"title": "Embedded Ensembles: infinite width limit and operating regimes", "abstract": "A memory efficient approach to ensembling neural networks is to share most weights among the ensembled models by means of a single reference network. We refer to this strategy as Embedded Ensembling (EE); its particular examples are BatchEnsembles and Monte-Carlo dropout ensembles. In this paper we perform a systematic theoretical and empirical analysis of embedded ensembles with different number of models. Theoretically, we use a Neural-Tangent-Kernel-based approach to derive the wide network limit of the gradient descent dynamics. In this limit, we identify two ensemble regimes - independent and collective - depending on the architecture and initialization strategy of ensemble models. We prove that in the independent regime the embedded ensemble behaves as an ensemble of independent models. We confirm our theoretical prediction with a wide range of experiments with finite networks, and further study empirically various effects such as transition between the two regimes, scaling of ensemble performance with the network width and number of models, and dependence of performance on a number of architecture and hyperparameter choices."}}
{"id": "ZTpAgTZvNIc", "cdate": 1640995200000, "mdate": 1668592014841, "content": {"title": "Uncertainty Estimation of Transformer Predictions for Misclassification Detection", "abstract": "Artem Vazhentsev, Gleb Kuzmin, Artem Shelmanov, Akim Tsvigun, Evgenii Tsymbalov, Kirill Fedyanin, Maxim Panov, Alexander Panchenko, Gleb Gusev, Mikhail Burtsev, Manvel Avetisian, Leonid Zhukov. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022."}}
