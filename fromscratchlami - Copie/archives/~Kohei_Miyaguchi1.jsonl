{"id": "vjAbOGWqNU", "cdate": 1696380072485, "mdate": 1696380072485, "content": {"title": "Adaptive Minimax Regret against Smooth Logarithmic Losses over High-Dimensional l1-Balls via Envelope Complexity", "abstract": "We develop a new theoretical framework, the envelope complexity, to analyze the minimax regret with logarithmic loss functions. Within the framework, we derive a Bayesian predictor that adaptively achieves the minimax regret over high-dimensional l1-balls within a factor of two. The prior is newly derived for achieving the minimax regret and called the spike-and-tails (ST) prior as it looks like. The resulting regret bound is so simple that it is completely determined with the smoothness of the loss function and the radius of the balls except with logarithmic factors, and it has a generalized form of existing regret/risk bounds."}}
{"id": "Sh97TNO5YY_", "cdate": 1663850136905, "mdate": null, "content": {"title": "Biases in Evaluation of Molecular Optimization Methods and Bias Reduction Strategies", "abstract": "We are interested in in silico evaluation methodology for molecular optimization methods. Given a sample of molecules and their properties of our interest, we wish not only to train a generator of molecules that can find those optimized with respect to a target property but also to evaluate its performance accurately. A common practice is to train a predictor of the target property on the sample and use it for both training and evaluating the generator. We theoretically investigate this evaluation methodology and show that it potentially suffers from two biases; one is due to misspecification of the predictor and the other to reusing the same sample for training and evaluation. We discuss bias reduction methods for each of the biases, and empirically investigate their effectiveness.\n"}}
{"id": "cwf7nnoK5o", "cdate": 1663850083456, "mdate": null, "content": {"title": "Interval-based Offline Policy Evaluation without Sufficient Exploration or Realizability", "abstract": "We study the problem of offline policy evaluation (OPE),\nwhere the goal is to estimate the value of given decision-making policy without interacting with the actual environment.\nIn particular, we consider the interval-based OPE, where the output is an interval rather than a point, indicating the uncertainty of the evaluation.\nThe interval-based estimation is especially important in OPE since, \nwhen the data coverage is insufficient relative to the complexity of the environmental model,\nany OPE method can be biased even with infinite sample size.\nIn this paper, we characterize the worst case of such irreducible bias, called the *minimax bias*, in terms of the discrepancy between the target policy and the data-sampling distribution,\nand show that the marginal-importance-sampling (MIS) estimator achieves the minimax bias with an appropriate importance-weight function.\nMotivated with this result, we then propose a new interval-based MIS estimator that asymptotically achieves the minimax bias."}}
{"id": "zAuiZpZ478l", "cdate": 1652737651396, "mdate": null, "content": {"title": "Hierarchical Lattice Layer for Partially Monotone Neural Networks", "abstract": "Partially monotone regression is a regression analysis in which the target values are monotonically increasing with respect to a subset of input features.   The TensorFlow Lattice library is one of the standard machine learning libraries for partially monotone regression.  It consists of several neural network layers, and its core component is the lattice layer.  One of the problems of the lattice layer is that it requires the projected gradient descent algorithm with many constraints to train it.  Another problem is that it cannot receive a high-dimensional input vector due to the memory consumption.   We propose a novel neural network layer, the hierarchical lattice layer (HLL), as an extension of the lattice layer so that we can use a standard stochastic gradient descent algorithm to train HLL while satisfying monotonicity constraints and so that it can receive a high-dimensional input vector.  Our experiments demonstrate that HLL did not sacrifice its prediction performance on real datasets compared with the lattice layer."}}
{"id": "qnQN4yr6FJz", "cdate": 1632875615131, "mdate": null, "content": {"title": "Variational Inference for Discriminative Learning with Generative Modeling of Feature Incompletion", "abstract": "We are concerned with the problem of distributional prediction with incomplete features: The goal is to estimate the distribution of target variables given feature vectors with some of the elements missing. A typical approach to this problem is to perform missing-value imputation and regression, simultaneously or sequentially, which we call the generative approach. Another approach is to perform regression after appropriately encoding missing values into the feature, which we call the discriminative approach. In comparison, the generative approach is more robust to the feature corruption while the discriminative approach is more favorable to maximize the performance of prediction. \nIn this study, we propose a hybrid method to take the best of both worlds. Our method utilizes the black-box variational inference framework so that it can be applied to a wide variety of modern machine learning models, including the variational autoencoders. We also confirmed the effectiveness of the proposed method empirically.\n"}}
{"id": "_y2G1-i7L8", "cdate": 1621630127733, "mdate": null, "content": {"title": "Asymptotically Exact Error Characterization of Offline Policy Evaluation with Misspecified Linear Models", "abstract": "We consider the problem of offline policy evaluation~(OPE) with Markov decision processes~(MDPs), where the goal is to estimate the utility of given decision-making policies based on static datasets. Recently, theoretical understanding of OPE has been rapidly advanced under (approximate) realizability assumptions, i.e., where the environments of interest are well approximated with the given hypothetical models. On the other hand, the OPE under unrealizability has not been well understood as much as in the realizable setting despite its importance in real-world applications.\nTo address this issue, we study the behavior of a simple existing OPE method called the linear direct method~(DM) under the unrealizability. Consequently, we obtain an asymptotically exact characterization of the OPE error in a doubly robust form. Leveraging this result, we also establish the nonparametric consistency of the tile-coding estimators under quite mild assumptions.\n"}}
{"id": "SXVqIT-lOTr", "cdate": 1546300800000, "mdate": null, "content": {"title": "Cogra: Concept-Drift-Aware Stochastic Gradient Descent for Time-Series Forecasting.", "abstract": "We approach the time-series forecasting problem in the presence of concept drift by automatic learning rate tuning of stochastic gradient descent (SGD). The SGD-based approach is preferable to other concept drift algorithms in that it can be applied to any model and it can keep learning efficiently whilst predicting online. Among a number of SGD algorithms, the variance-based SGD (vSGD) can successfully handle concept drift by automatic learning rate tuning, which is reduced to an adaptive mean estimation problem. However, its performance is still limited because of its heuristic mean estimator. In this paper, we present a concept-drift-aware stochastic gradient descent (Cogra), equipped with more theoretically-sound mean estimator called sequential mean tracker (SMT). Our key contribution is that we define a goodness criterion for the mean estimators; SMT is designed to be optimal according to this criterion. As a result of comprehensive experiments, we find that (i) our SMT can estimate the mean better than vSGD\u2019s estimator in the presence of concept drift, and (ii) in terms of predictive performance, Cogra reduces the predictive loss by 16\u201367% for real-world datasets, indicating that SMT improves the prediction accuracy significantly."}}
