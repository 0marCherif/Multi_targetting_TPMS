{"id": "CPjbm9sKge", "cdate": 1668669605936, "mdate": null, "content": {"title": "Temporal Segmentation of Fine-gained Semantic Action: A Motion-Centered Figure Skating Dataset", "abstract": "Temporal Action Segmentation (TAS) has achieved great success in many fields such as exercise rehabilitation, movie editing, etc. Currently, task-driven TAS is a central topic in human action analysis. However, motion-centered TAS, as an important topic, is little researched due to unavailable datasets. In order to explore more models and practical applications of motion-centered TAS, we introduce a Motion-Centered Figure Skating (MCFS) dataset in this paper. Compared with existing temporal action segmentation datasets, the MCFS dataset is fine-grained semantic, specialized and motion-centered. Besides, RGB-based and Skeleton-based features are provided in the MCFS dataset. Experimental results show that existing state-of-the-art methods are difficult to achieve excellent segmentation results (including accuracy, edit and F1 score) in the MCFS dataset. This indicates that MCFS is a challenging dataset for motion-centered TAS. "}}
{"id": "zE6eY2X7qGg", "cdate": 1640995200000, "mdate": 1683164879800, "content": {"title": "Double Attention Network Based on Sparse Sampling", "abstract": "Locating action segments in long untrimmed videos is a sub-task of video understanding, which more and more scholars pay attention to. Boundary ambiguity and over-segmentation errors are two difficult problems. To handle them, we propose a network called Double Attention Network based on Sparse Sampling (DASS) on the basis of MS-TCN series. First, we design a Seq2Seq Convolution Sampling Network (SCSN) to reduce feature redundancy, which also works on over-fitting. Second, we devise a Global Temporal Attention Module (GTAM) to help predict action boundaries and improve the effect of post-processing from a global perspective. Third, we propose Local Temporal Attention Module (LTAM), which both casts attention to local frames and complements details lost in high dilated layers. We perform experiments on three challenging datasets: 50Salads, GTEA and Breakfast and prove our model is state-of-the-art."}}
{"id": "u3oogx8JGo", "cdate": 1640995200000, "mdate": 1683164879722, "content": {"title": "Spatial Focus Attention for Fine-Grained Skeleton-Based Action Tasks", "abstract": "Dynamic skeletal data has been widely studied for human action tasks due to its high-level semantic information and less data than RGB features. However, attention-based previous methods fail to focus on the local grouped joint dependence of the human body, which is vital to distinguishing various actions in fine-grained tasks, such as skeletal action segmentation and recognition. This work proposes spatial focus attention for the fine-grained skeleton-based action tasks. Specifically, we decouple the attention map to enhance the grouped joint dependence adaptively by the decouple probability. To further focus on local grouped dependence, the tree structural attention maps can be built by hierarchical decoupling and guide the model to focus on complementary local dependence in the different leaf nodes. Our proposed approach achieves state-of-the-art performance on fine-grained skeleton-based human action segmentation tasks (MCFS-22) and recognition tasks (FSD-10). Besides, on the coarse-grained dataset (NTU-60), the proposed spatial focus attention also achieves outstanding performance."}}
{"id": "pq-axf5gJ1", "cdate": 1640995200000, "mdate": 1683164879799, "content": {"title": "Spatial Temporal Graph Attention Network for Skeleton-Based Action Recognition", "abstract": "It's common for current methods in skeleton-based action recognition to mainly consider capturing long-term temporal dependencies as skeleton sequences are typically long (>128 frames), which forms a challenging problem for previous approaches. In such conditions, short-term dependencies are few formally considered, which are critical for classifying similar actions. Most current approaches are consisted of interleaving spatial-only modules and temporal-only modules, where direct information flow among joints in adjacent frames are hindered, thus inferior to capture short-term motion and distinguish similar action pairs. To handle this limitation, we propose a general framework, coined as STGAT, to model cross-spacetime information flow. It equips the spatial-only modules with spatial-temporal modeling for regional perception. While STGAT is theoretically effective for spatial-temporal modeling, we propose three simple modules to reduce local spatial-temporal feature redundancy and further release the potential of STGAT, which (1) narrow the scope of self-attention mechanism, (2) dynamically weight joints along temporal dimension, and (3) separate subtle motion from static features, respectively. As a robust feature extractor, STGAT generalizes better upon classifying similar actions than previous methods, witnessed by both qualitative and quantitative results. STGAT achieves state-of-the-art performance on three large-scale datasets: NTU RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400. Code is released."}}
{"id": "U_NfEfS4kf1", "cdate": 1640995200000, "mdate": 1683164879735, "content": {"title": "Background Suppressed and Motion Enhanced Network for Weakly Supervised Video Anomaly Detection", "abstract": "Weakly supervised video anomaly detection (VAD) is a significant and challenging task in the surveillance video analysis field, locating anomalous motion frames using only video-level label information. In general, weakly supervised VAD mainly faces two obstacles. Firstly, the greatly changed background always causes false detection. Secondly, the motion changes are implicit in real-world surveillance videos caused by unsuitable focal distance, illumination, etc. This paper proposes a background suppressed and motion enhanced network (BSMEN) for Weakly Supervised VAD to solve the problems. The BSMEN utilizes the multi-head self-attention mechanism to construct a triple branch framework to suppress the influence from the background and enhance the motion significance in the VAD process. Moreover, a motion discrimination sequence extraction (MDSE) module is devised to locate the anomaly motion frames more accurately in the BSMEN. Extensive experiments on two mainstream VAD evaluation datasets validate that BSMEN outperforms state-of-the-art weakly supervised VAD methods."}}
{"id": "qXwqwfgunZf", "cdate": 1609459200000, "mdate": 1648812368105, "content": {"title": "Efficient Two-Step Networks for Temporal Action Segmentation", "abstract": "Due to boundary ambiguity and over-segmentation issues, identifying all the frames in long untrimmed videos is still challenging. To address these problems, we present the Efficient Two-Step Network (ETSN) with two components. The first step of ETSN is Efficient Temporal Series Pyramid Networks (ETSPNet) that capture both local and global frame-level features and provide accurate predictions of segmentation boundaries. The second step is a novel unsupervised approach called Local Burr Suppression (LBS), which significantly reduces the over-segmentation errors. Our empirical evaluations on the benchmarks including 50Salads, GTEA and Breakfast dataset demonstrate that ETSN outperforms the current state-of-the-art methods by a large margin."}}
{"id": "G8PFIYClOJ", "cdate": 1609459200000, "mdate": 1683164879741, "content": {"title": "Adaptive Graph Convolutional Network with Prior Knowledge for Action Recognition", "abstract": "Skeleton-based action recognition has been paid more and more attention in recent years. Previous researches mainly depend on CNNs or RNNs to capture dependencies among sequences. Recently, graph convolution networks are widely used due to its extraordinary ability to exploit node relationships. We propose a new GCN-based model named PK-GCN which utilizes prior knowledge to design learnable node connections. It can be proved that models can learn adaptive connections by itself, because the node connections can be learned with random initialization. The prior knowledge can be used to design node connections by selecting prominent pairs of joints in actions. By combining the proposed methods above, PK-GCN achieves the best performance in ablation study. Compared with other single-stream GCN-based models, PK-GCN on two large-scale datasets NTU-RGB+D and Kinetics achieves state-of-the-art results."}}
{"id": "D5rWOAGvQvL", "cdate": 1609459200000, "mdate": 1648812368100, "content": {"title": "Spatial-Temporal Attention Network with Multi-similarity Loss for Fine-Grained Skeleton-Based Action Recognition", "abstract": "In skeleton-based action recognition, the Graph Convolutional Networks (GCNs) have achieved remarkable results. However, in fine-grained skeleton-based action recognition, existing methods can\u2019t distinguish highly similar sample pairs well. This is because that the current methods do not pay attention to highly similar sample pairs and can\u2019t model complex actions through the spatial-temporal separation framework. In order to solve the above problems, we combine the multi-similarity loss function to weight the difficult sample pairs, and propose a novel ST-Attention module to construct the nodes connection between spatial and temporal. Finally, we used the Spatial-Temporal Attention Network with Multi-Similarity Loss (STATT-MS) to conduct experiments on NTU-RGBD-60, FSD-10 and UAV-human datasets and achieved state-of-the-art performance."}}
{"id": "4VG8_tCS7E", "cdate": 1609459200000, "mdate": 1683164879780, "content": {"title": "A Lightweight Multidimensional Self-attention Network for Fine-Grained Action Recognition", "abstract": "Fine-grained action recognition is a challengeable task due to its background independence and complex semantics over time and space. Multidimensional attention is essential for this task to capture discriminative spatial details, temporal and channel features. However, multidimensional attention has challenge in keeping the balance between adaptive feature perception and computational overhead. To address this issue, this paper proposes a Lightweight Multidimensional Self-Attention Network (LMSA-Net) which can adaptively capture the discriminative features over multiple dimensions in an efficient manner. It is worth remarking that the contextual relationship between time and channel is established in temporal stream, which is complementary for spatial attention in spatial stream. Compared with the RGB based models, LMSA-Net achieves state-of-the-art performance in two fine-grained action recognition datasets, i.e. FSD-10 and Diving48-V2. In addition, it can be found that the streams of LMSA-Net are end-to-end trainable to reduce the overhead of computation and storage, and the recognition accuracy can reach the level of two-stage two-stream models."}}
{"id": "15S95pjdUNo", "cdate": 1609459200000, "mdate": 1648812368089, "content": {"title": "Temporal Segmentation of Fine-gained Semantic Action: A Motion-Centered Figure Skating Dataset", "abstract": "Temporal Action Segmentation (TAS) has achieved great success in many fields such as exercise rehabilitation, movie editing, etc. Currently, task-driven TAS is a central topic in human action analysis. However, motion-centered TAS, as an important topic, is little researched due to unavailable datasets. In order to explore more models and practical applications of motion-centered TAS, we introduce a Motion-Centered Figure Skating (MCFS) dataset in this paper. Compared with existing temporal action segmentation datasets, the MCFS dataset is fine-grained semantic, specialized and motion-centered. Besides, RGB-based and Skeleton-based features are provided in the MCFS dataset. Experimental results show that existing state-of-the-art methods are difficult to achieve excellent segmentation results (including accuracy, edit and F1 score) in the MCFS dataset. This indicates that MCFS is a challenging dataset for motion-centered TAS. The latest dataset can be downloaded at https://shenglanliu.github.io/mcfs-dataset/."}}
