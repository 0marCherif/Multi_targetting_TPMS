{"id": "PKaQP7ywWtS", "cdate": 1682958567204, "mdate": null, "content": {"title": "Adaptive Split Learning", "abstract": "Federated learning (FL) is a popular distributed deep learning framework which enables personalized experiences across a wide range of web clients and mobile/IoT devices. However, FL-based methods are challenged by the compute resources on client devices given the exploding growth in size of state-of-the-art models (eg. billion parameter models). Split Learning (SL), a recent framework, reduces client compute load by splitting model training between client and server. This flexibility is useful for low-compute setups but is achieved at the cost of massive increase in bandwidth consumption. This split also results in sub-optimal performance, especially when data across clients is heterogeneous. The goal of this paper is to make SL a viable alternative to FL. Specifically, we introduce adaptive split learning (AdaSplit) which enables efficiently scaling SL to low-resource scenarios by reducing bandwidth consumption and improving performance across heterogenous clients. We validate the effectiveness of AdaSplit under limited resources through extensive experimental comparison with strong federated and split learning baselines. Finally, we also present sensitivity analyses of key design choices in AdaSplit which highlight the ability of AdaSplit to adapt to variable resource budgets."}}
{"id": "gRCWdltNQq", "cdate": 1665081442665, "mdate": null, "content": {"title": "Differentially Private CutMix for Split Learning with Vision Transformer", "abstract": "Recently, vision transformer (ViT) has started to outpace the conventional CNN in computer vision tasks. Considering privacy-preserving distributed learning with ViT, federated learning (FL) communicates models, which becomes ill-suited due to ViT's large model size and computing costs. Split learning (SL) detours this by communicating smashed data at a cut-layer, yet suffers from data privacy leakage and large communication costs caused by high similarity between ViT's smashed data and input data. Motivated by this problem, we propose \\textit{DP-CutMixSL}, a differentially private (DP) SL framework by developing \\textit{DP patch-level randomized CutMix (DP-CutMix)}, a novel privacy-preserving inter-client interpolation scheme that replaces randomly selected patches in smashed data. By experiment, we show that DP-CutMixSL not only boosts privacy guarantees and communication efficiency, but also achieves higher accuracy than its Vanilla SL counterpart. Theoretically, we analyze that DP-CutMix amplifies R\\'enyi DP (RDP), which is upper-bounded by its Vanilla Mixup counterpart."}}
{"id": "Jw5ivmKS2C", "cdate": 1663850177440, "mdate": null, "content": {"title": "Posthoc Privacy guarantees for neural network queries", "abstract": "Cloud based machine learning inference is an emerging paradigm where users share their data with a service provider. Due to increased concerns over data privacy, recent works have proposed using Adversarial Representation Learning (ARL) to learn a privacy-preserving encoding of sensitive user data before it is shared with an untrusted service provider. Traditionally, the privacy of these encodings is evaluated empirically as they lack formal guarantees. In this work, we develop a new framework that provides formal privacy guarantees for an arbitrarily trained neural network by linking its local Lipschitz constant with its local sensitivity. To utilize local sensitivity for guaranteeing privacy, we extend the Propose-Test-Release(PTR) framework to make it tractable for neural network based queries. We verify the efficacy of our framework experimentally on real-world datasets and elucidate the role of ARL in improving the privacy-utility tradeoff."}}
{"id": "iOWs0vrURYT", "cdate": 1640995200000, "mdate": 1652532905637, "content": {"title": "The Privacy-Welfare Trade-off: Effects of Differential Privacy on Influence & Welfare in Social Choice", "abstract": "This work studies a fundamental trade-off between privacy and welfare in aggregation of privatized preferences of individuals. It presents the precise rate at which welfare decreases with increasing levels of privacy. Trade-offs in achieving privacy while maintaining accuracy or more recently in maintaining fairness have been studied so far in prior works. Social choice functions help aggregate individual preferences while differentially private mechanisms provide formal privacy guarantees to release answers of queries operating on data. However, once differential privacy inducing noise is introduced into a voting system, the deterministic social choice function used to release an aggregated choice may not be ideal anymore. It might change the power balances between the voters. It could be the case that an alternate social choice function becomes more ideal to aggregate the preferences. There could be a constraint to operate the voting system at a specific level of either privacy or influence or welfare, and one would like to know the effects on rest of the unconstrained choices. In this paper, we introduce notions of welfare and probabilistic influence of privatization mechanisms to help precisely answer such questions through the proposed results. Throughout the paper, we restrict our work to social choice functions where several voters vote for either of two candidates. We present two different ways of proving each of our results that connects privacy with welfare & probabilistic influence: i.) by using first principles from combinatorics/probability, ii.) by using Fourier analysis of Boolean functions. Finally, we analyze the accuracy of the private mechanism on various social choice functions. The results in this paper thereby help bridge two prominent fields of social choice theory and differential privacy."}}
{"id": "bWbMvLUD07k", "cdate": 1640995200000, "mdate": 1652532905602, "content": {"title": "LocFedMix-SL: Localize, Federate, and Mix for Improved Scalability, Convergence, and Latency in Split Learning", "abstract": "Split learning (SL) is a promising distributed learning framework that enables to utilize the huge data and parallel computing resources of mobile devices. SL is built upon a model-split architecture, wherein a server stores an upper model segment that is shared by different mobile clients storing its lower model segments. Without exchanging raw data, SL achieves high accuracy and fast convergence by only uploading smashed data from clients and downloading global gradients from the server. Nonetheless, the original implementation of SL sequentially serves multiple clients, incurring high latency with many clients. A parallel implementation of SL has great potential in reducing latency, yet existing parallel SL algorithms resort to compromising scalability and/or convergence speed. Motivated by this, the goal of this article is to develop a scalable parallel SL algorithm with fast convergence and low latency. As a first step, we identify that the fundamental bottleneck of existing parallel SL comes from the model-split and parallel computing architectures, under which the server-client model updates are often imbalanced, and the client models are prone to detach from the server\u2019s model. To fix this problem, by carefully integrating local parallelism, federated learning, and mixup augmentation techniques, we propose a novel parallel SL framework, coined LocFedMix-SL. Simulation results corroborate that LocFedMix-SL achieves improved scalability, convergence speed, and latency, compared to sequential SL as well as the state-of-the-art parallel SL algorithms such as SplitFed and LocSplitFed."}}
{"id": "Uc-9mRzJfJM", "cdate": 1640995200000, "mdate": 1652532905602, "content": {"title": "Decouple-and-Sample: Protecting sensitive information in task agnostic data release", "abstract": "We propose sanitizer, a framework for secure and task-agnostic data release. While releasing datasets continues to make a big impact in various applications of computer vision, its impact is mostly realized when data sharing is not inhibited by privacy concerns. We alleviate these concerns by sanitizing datasets in a two-stage process. First, we introduce a global decoupling stage for decomposing raw data into sensitive and non-sensitive latent representations. Secondly, we design a local sampling stage to synthetically generate sensitive information with differential privacy and merge it with non-sensitive latent features to create a useful representation while preserving the privacy. This newly formed latent information is a task-agnostic representation of the original dataset with anonymized sensitive information. While most algorithms sanitize data in a task-dependent manner, a few task-agnostic sanitization techniques sanitize data by censoring sensitive information. In this work, we show that a better privacy-utility trade-off is achieved if sensitive information can be synthesized privately. We validate the effectiveness of the sanitizer by outperforming state-of-the-art baselines on the existing benchmark tasks and demonstrating tasks that are not possible using existing techniques."}}
{"id": "3ZuLmU7zBpy", "cdate": 1632875719483, "mdate": null, "content": {"title": "Sanitizer: Sanitizing data for anonymizing sensitive information", "abstract": "We propose a framework that protects against sensitive information leakage to facilitate data release with untrusted parties. Sanitization concerns with transforming a data sample to remove sensitive attribute information while retaining every other information with a goal of keeping its utility high for unknown downstream tasks. This is done in a two-step process: first, we develop a method that encodes unstructured image-like modality into a structured representation bifurcated by sensitive and non-sensitive representation. Second, we design mechanisms that transform the sensitive features such that the data obtained from projecting features back to the image protects from the leakage of sensitive information. Instead of removing sensitive information from the unstructured data, we replace the sensitive features by sampling synthetic features from the joint distribution of the sensitive features in its structured representation. Hence, using this method one can share a sanitized dataset that preserves distribution with the original dataset resulting in a good utility-privacy trade-off. We compare our technique against state-of-the-art baselines and demonstrate competitive empirical results quantitatively and qualitatively."}}
{"id": "uF-HvlFaPOz", "cdate": 1609459200000, "mdate": 1652532905634, "content": {"title": "Differentially Private Supervised Manifold Learning with Applications like Private Image Retrieval", "abstract": "Differential Privacy offers strong guarantees such as immutable privacy under post processing. Thus it is often looked to as a solution to learning on scattered and isolated data. This work focuses on supervised manifold learning, a paradigm that can generate fine-tuned manifolds for a target use case. Our contributions are two fold. 1) We present a novel differentially private method \\textit{PrivateMail} for supervised manifold learning, the first of its kind to our knowledge. 2) We provide a novel private geometric embedding scheme for our experimental use case. We experiment on private \"content based image retrieval\" - embedding and querying the nearest neighbors of images in a private manner - and show extensive privacy-utility tradeoff results, as well as the computational efficiency and practicality of our methods."}}
{"id": "nRtOPmFbB0r", "cdate": 1609459200000, "mdate": 1652532905604, "content": {"title": "NoPeek-Infer: Preventing face reconstruction attacks in distributed inference after on-premise training", "abstract": "For models trained on-premise but deployed in a distributed fashion across multiple entities, we demonstrate that minimizing distance correlation between sensitive data such as faces and intermediary representations enables prediction while preventing reconstruction attacks. Leakage (measured using distance correlation between input and intermediate representations) is the risk associated with the reconstruction of raw face data from intermediary representations that are communicated in a distributed setting. We demonstrate on face datasets that our method is resilient to reconstruction attacks during distributed inference while maintaining information required to sustain good classification accuracy. We share modular code for performing NoPeek-Infer at http://tiny.cc/nopeek along with corresponding trained models for benchmarking attack techniques."}}
{"id": "l_B7Ich8jx", "cdate": 1609459200000, "mdate": 1652532905608, "content": {"title": "Private measurement of nonlinear correlations between data hosted across multiple parties", "abstract": "We introduce a differentially private method to measure nonlinear correlations between sensitive data hosted across two entities. We provide utility guarantees of our private estimator. Ours is the first such private estimator of nonlinear correlations, to the best of our knowledge within a multi-party setup. The important measure of nonlinear correlation we consider is distance correlation. This work has direct applications to private feature screening, private independence testing, private k-sample tests, private multi-party causal inference and private data synthesis in addition to exploratory data analysis. Code access: A link to publicly access the code is provided in the supplementary file."}}
