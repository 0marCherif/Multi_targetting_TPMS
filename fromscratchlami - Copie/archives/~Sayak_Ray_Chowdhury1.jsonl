{"id": "jNF96XWF-pg", "cdate": 1676827088253, "mdate": null, "content": {"title": "Combinatorial Categorized Bandits with Expert Rankings", "abstract": " Many real-world systems such as e-commerce websites and content-serving platforms employ two-stage recommendation --- in the first stage, multiple nominators (experts) provide ranked lists of items (one nominator per category, e.g., sports and political news articles), and in the second stage, an aggregator filters across the lists and outputs a single (short) list of $K$ items to the users. The aggregation stage can be posed as a combinatorial multi-armed bandit problem, with the additional structure that the arms are grouped into categories (disjoint sets of items) and the ranking of arms within each category is known. We propose algorithms for selecting top $K$ items in this setting under two learning objectives, namely minimizing regret over rounds and identifying the top $K$ items within a fixed number of rounds. For each of the objectives, we provide sharp regret/error analysis using carefully defined notion of ``gap'' that exploits our problem structure. The resulting regret/error bounds strictly improve over prior work in combinatorial bandits literature. We also provide supporting evidence from simulations on synthetic and semi-synthetic problems."}}
{"id": "aEOd6wh5GA", "cdate": 1665069637541, "mdate": null, "content": {"title": "Distributed Differential Privacy in Multi-Armed Bandits", "abstract": "We consider the standard $K$-armed bandit problem under a distributed trust model of differential privacy (DP), which enables to guarantee privacy without a trustworthy server. Under this trust model, previous work largely focus on achieving privacy using a shuffle protocol, where a batch of users data are randomly permuted before sending to a central server. This protocol achieves ($\\epsilon,\\delta$) or approximate-DP guarantee by sacrificing an additive $O\\!\\left(\\!\\frac{K\\log T\\sqrt{\\log(1/\\delta)}}{\\epsilon}\\!\\right)\\!$ factor in $T$-step cumulative regret. In contrast, the optimal privacy cost to achieve a stronger ($\\epsilon,0$) or pure-DP guarantee under the widely used central trust model is only $\\Theta\\!\\left(\\!\\frac{K\\log T}{\\epsilon}\\!\\right)\\!$, where, however, a trusted server is required. In this work, we aim to obtain a pure-DP guarantee under distributed trust model while sacrificing no more regret than that under central trust model. We achieve this by designing a generic bandit algorithm based on successive arm elimination, where privacy is guaranteed by corrupting rewards with an equivalent discrete Laplace noise ensured by a secure computation protocol. We numerically simulate regret performance of our algorithm, which corroborate our theoretical findings."}}
{"id": "cw8FeirkIfU", "cdate": 1663850196278, "mdate": null, "content": {"title": "Distributed Differential Privacy in Multi-Armed Bandits", "abstract": "We consider the standard $K$-armed bandit problem under a distributed trust model of differential privacy (DP), which enables to guarantee privacy without a trustworthy server.  Under this trust model, previous work largely focus on achieving privacy using a shuffle protocol, where a batch of users data are randomly permuted before sending to a central server. This protocol achieves ($\\epsilon,\\delta$) or approximate-DP guarantee by sacrificing an additive $O\\!\\left(\\!\\frac{K\\log T\\sqrt{\\log(1/\\delta)}}{\\epsilon}\\!\\right)\\!$ factor in $T$-step cumulative regret. In contrast, the optimal privacy cost to achieve a stronger ($\\epsilon,0$) or pure-DP guarantee under the widely used central trust model is only $\\Theta\\!\\left(\\!\\frac{K\\log T}{\\epsilon}\\!\\right)\\!$, where, however, a trusted server is required. In this work, we aim to obtain a pure-DP guarantee under distributed trust model while sacrificing no more regret than that under central trust model. We achieve this by designing a generic bandit algorithm based on successive arm elimination, where privacy is guaranteed by corrupting rewards with an equivalent discrete Laplace noise ensured by a secure computation protocol. We also show that our algorithm, when instantiated with Skellam noise and the secure protocol, ensures \\emph{R\\'{e}nyi differential privacy} -- a stronger notion than approximate DP -- under distributed trust model with a privacy cost of $O\\!\\left(\\!\\frac{K\\sqrt{\\log T}}{\\epsilon}\\!\\right)\\!$. Finally, as a by-product of our techniques, we also recover the best-known regret bounds for bandits under central and local models while using only \\emph{discrete privacy noise}, which can avoid the privacy leakage due to floating point arithmetic of continuous noise on finite computers."}}
{"id": "ZyMpCZuU3Sj", "cdate": 1609459200000, "mdate": null, "content": {"title": "No-regret Algorithms for Multi-task Bayesian Optimization", "abstract": "We consider multi-objective optimization (MOO) of an unknown vector-valued function in the non-parametric Bayesian optimization (BO) setting. Our aim is to maximize the expected cumulative utility of all objectives, as expressed by a given prior over a set of scalarization functions. Most existing BO algorithms do not model the fact that the multiple objectives, or equivalently, tasks can share similarities, and even the few that do lack rigorous, finite-time regret guarantees that capture explicitly inter-task structure. In this work, we address this problem by modelling inter-task dependencies using a multi-task kernel and develop two novel BO algorithms based on random scalarization of the objectives. Our algorithms employ vector-valued kernel regression as a stepping stone and belong to the upper confidence bound class of algorithms. Under a smoothness assumption that the unknown vector-valued function is an element of the reproducing kernel Hilbert space associated with the multi-task kernel, we derive worst-case regret bounds for our algorithms that explicitly capture the similarities between tasks. We numerically benchmark our algorithms on both synthetic and real-life MOO problems, and show the advantages offered by learning with multi-task kernels."}}
{"id": "0LMeUJiMB-C", "cdate": 1609459200000, "mdate": null, "content": {"title": "Reinforcement Learning in Parametric MDPs with Exponential Families", "abstract": "Extending model-based regret minimization strategies for Markov decision processes (MDPs) beyond discrete state-action spaces requires structural assumptions on the reward and transition models. Existing parametric approaches establish regret guarantees by making strong assumptions about either the state transition distribution or the value function as a function of state-action features, and often do not satisfactorily capture classical problems like linear dynamical systems or factored MDPs. This paper introduces a new MDP transition model defined by a collection of linearly parameterized exponential families with $d$ unknown parameters. For finite-horizon episodic RL with horizon $H$ in this MDP model, we propose a model-based upper confidence RL algorithm (Exp-UCRL) that solves a penalized maximum likelihood estimation problem to learn the $d$-dimensional representation of the transition distribution, balancing the exploitation-exploration tradeoff using confidence sets in the exponential family space. We demonstrate the efficiency of our algorithm by proving a frequentist (worst-case) regret bound that is of order $\\tilde O(d\\sqrt{H^3 N})$, sub-linear in total time $N$, linear in dimension $d$, and polynomial in the planning horizon $H$. This is achieved by deriving a novel concentration inequality for conditional exponential families that might be of independent interest. The exponential family MDP model also admits an efficient posterior sampling-style algorithm for which a similar guarantee on the Bayesian regret is shown."}}
{"id": "xOMTlOMYdkF", "cdate": 1577836800000, "mdate": null, "content": {"title": "No-regret Algorithms for Multi-task Bayesian Optimization", "abstract": "We consider multi-objective optimization (MOO) of an unknown vector-valued function in the non-parametric Bayesian optimization (BO) setting, with the aim being to learn points on the Pareto front of the objectives. Most existing BO algorithms do not model the fact that the multiple objectives, or equivalently, tasks can share similarities, and even the few that do lack rigorous, finite-time regret guarantees that capture explicitly inter-task structure. In this work, we address this problem by modelling inter-task dependencies using a multi-task kernel and develop two novel BO algorithms based on random scalarizations of the objectives. Our algorithms employ vector-valued kernel regression as a stepping stone and belong to the upper confidence bound class of algorithms. Under a smoothness assumption that the unknown vector-valued function is an element of the reproducing kernel Hilbert space associated with the multi-task kernel, we derive worst-case regret bounds for our algorithms that explicitly capture the similarities between tasks. We numerically benchmark our algorithms on both synthetic and real-life MOO problems, and show the advantages offered by learning with multi-task kernels."}}
{"id": "fD0VNb1NLei", "cdate": 1577836800000, "mdate": null, "content": {"title": "No-Regret Reinforcement Learning with Value Function Approximation: a Kernel Embedding Approach", "abstract": "We consider the regret minimization problem in reinforcement learning (RL) in the episodic setting. In many real-world RL environments, the state and action spaces are continuous or very large. Existing approaches establish regret guarantees by either a low-dimensional representation of the stochastic transition model or an approximation of the $Q$-functions. However, the understanding of function approximation schemes for state-value functions largely remains missing. In this paper, we propose an online model-based RL algorithm, namely the CME-RL, that learns representations of transition distributions as embeddings in a reproducing kernel Hilbert space while carefully balancing the exploitation-exploration tradeoff. We demonstrate the efficiency of our algorithm by proving a frequentist (worst-case) regret bound that is of order $\\tilde{O}\\big(H\\gamma_N\\sqrt{N}\\big)$\\footnote{ $\\tilde{O}(\\cdot)$ hides only absolute constant and poly-logarithmic factors.}, where $H$ is the episode length, $N$ is the total number of time steps and $\\gamma_N$ is an information theoretic quantity relating the effective dimension of the state-action feature space. Our method bypasses the need for estimating transition probabilities and applies to any domain on which kernels can be defined. It also brings new insights into the general theory of kernel methods for approximate inference and RL regret minimization."}}
{"id": "TzzjlwN7RG3", "cdate": 1577836800000, "mdate": null, "content": {"title": "Active Learning of Conditional Mean Embeddings via Bayesian Optimisation", "abstract": "We consider the problem of sequentially optimising the conditional expectation of an objective function, with both the conditional distribution and the objective function assumed to be fixed but un..."}}
{"id": "HJlCiBSxUS", "cdate": 1567802790095, "mdate": null, "content": {"title": "Bayesian Optimization under Heavy-tailed Payoffs", "abstract": "We consider black box optimization of an unknown function in the nonparametric Gaussian process setting when the noise in the observed function values can be heavy tailed. This is in contrast to existing literature that typically assumes sub-Gaussian noise distributions for queries. Under the assumption that the unknown function belongs to the Reproducing Kernel Hilbert Space (RKHS) induced by a kernel, we first show that an adaptation of the well-known GP-UCB algorithm with reward truncation enjoys sublinear $\\tilde{O}(T^{\\frac{2 + \\alpha}{2(1+\\alpha)}})$ regret even with only the $(1+\\alpha)$-th moments, $\\alpha \\in (0,1]$, of the reward distribution being bounded ($\\tilde{O}$ hides logarithmic factors). However, for the common squared exponential (SE) and Mat\\'{e}rn kernels, this is seen to be significantly larger than a fundamental $\\Omega(T^{\\frac{1}{1+\\alpha}})$ lower bound on regret. We resolve this gap by developing novel Bayesian optimization algorithms, based on kernel approximation techniques, with regret bounds matching the lower bound in order for the SE kernel. We numerically benchmark the algorithms on environments based on both synthetic models and real-world data sets."}}
{"id": "sT7UQSW7Mb", "cdate": 1546300800000, "mdate": null, "content": {"title": "On Online Learning in Kernelized Markov Decision Processes", "abstract": "We develop algorithms with low regret for learning episodic Markov decision processes based on kernel approximation techniques. The algorithms are based on both the Upper Confidence Bound (UCB) as well as Posterior or Thompson Sampling (PSRL) philosophies, and work in the general setting of continuous state and action spaces when the true unknown transition dynamics are assumed to have smoothness induced by an appropriate Reproducing Kernel Hilbert Space (RKHS)."}}
