{"id": "E1JQwHbFNq", "cdate": 1668536780876, "mdate": 1668536780876, "content": {"title": "Learning to Segment Actions from Visual and Language Instructions via Differentiable Weak Sequence Alignment", "abstract": "We address the problem of unsupervised localization\nof task-relevant actions (key-steps) and feature learning\nin instructional videos using both visual and language\ninstructions. Our key observation is that the sequences of\nvisual and linguistic key-steps are weakly aligned: there\nis an ordered one-to-one correspondence between most\nvisual and language key-steps, while some key-steps in\none modality are absent in the other. To recover the\ntwo sequences, we develop an ordered prototype learning\nmodule, which extracts visual and linguistic prototypes\nrepresenting key-steps. To find weak alignment and perform\nfeature learning, we develop a differentiable weak sequence\nalignment (DWSA) method that finds ordered one-to-one\nmatching between sequences while allowing some items in\na sequence to stay unmatched. We develop an efficient forward and backward algorithm for computing the alignment\nand the loss derivative with respect to parameters of visual\nand language feature learning modules. By experiments on\ntwo instructional video datasets, we show that our method\nsignificantly improves the state of the art."}}
{"id": "uAboJCXHqtN", "cdate": 1668536712530, "mdate": 1668536712530, "content": {"title": "Weakly-Supervised Action Segmentation and Alignment via Transcript-Aware Union-of-Subspaces Learning", "abstract": "We address the problem of learning to segment actions\nfrom weakly-annotated videos, i.e., videos accompanied by\ntranscripts (ordered list of actions). We propose a framework in which we model actions with a union of lowdimensional subspaces, learn the subspaces using transcripts and refine video features that lend themselves to action subspaces. To do so, we design an architecture consisting of a Union-of-Subspaces Network, which is an ensemble\nof autoencoders, each modeling a low-dimensional action\nsubspace and can capture variations of an action within\nand across videos. For learning, at each iteration, we generate positive and negative soft alignment matrices using\nthe segmentations from the previous iteration, which we use\nfor discriminative training of our model. To regularize the\nlearning, we introduce a constraint loss that prevents imbalanced segmentations and enforces relatively similar duration of each action across videos. To have a real-time inference, we develop a hierarchical segmentation framework\nthat uses subset selection to find representative transcripts\nand hierarchically align a test video with increasingly refined representative transcripts. Our experiments on three\ndatasets show that our method improves the state-of-the-art\naction segmentation and alignment, while speeding up the\ninference time by a factor of 4 to 13."}}
{"id": "lG0ySBGy-Om", "cdate": 1668536573737, "mdate": 1668536573737, "content": {"title": "Zero-Shot Attribute Attacks on Fine-Grained Recognition Models", "abstract": "Zero-shot fine-grained recognition is an important classification task, whose goal is to recognize visually very similar classes, including the ones without training images. Despite recent advances on the development of zero-shot fine-grained recognition methods, the robustness of such models to adversarial attacks is not well understood. On the other hand, adversarial attacks have been widely studied for conventional classification with visually distinct classes. Such attacks, in particular, universal perturbations that are class-agnostic and ideally should generalize to unseen classes, however, cannot leverage or capture small distinctions among fine-grained classes. Therefore, we propose a compositional attribute-based framework for generating adversarial attacks on zero-shot fine-grained recognition models. To generate attacks that capture small differences between fine-grained classes, generalize well to previously unseen classes and can be applied in real-time, we propose to learn and compose multiple attribute-based universal perturbations (AUPs). Each AUP corresponds to an image-agnostic perturbation on a specific attribute. To build our attack, we compose AUPs with weights obtained by learning a class-attribute compatibility function. To learn the AUPs and the parameters of our model, we minimize a loss, consisting of a ranking loss and a novel utility loss, which ensures AUPs are effectively learned and utilized. By extensive experiments on three datasets for zero-shot fine-grained recognition, we show that our attacks outperform conventional universal classification attacks and transfer well between different recognition architectures."}}
{"id": "hu6U1ncYwH", "cdate": 1668030224867, "mdate": 1668030224867, "content": {"title": "Set-Supervised Action Learning in Procedural Task Videos via Pairwise Order Consistency", "abstract": "We address the problem of set-supervised action learning, whose goal is to learn an action segmentation model\nusing weak supervision in the form of sets of actions occurring in training videos. Our key observation is that videos\nwithin the same task have similar ordering of actions, which\ncan be leveraged for effective learning. Therefore, we propose an attention-based method with a new Pairwise Ordering Consistency (POC) loss that encourages that for each\ncommon action pair in two videos of the same task, the attentions of actions follow a similar ordering. Unlike existing sequence alignment methods, which misalign actions\nin videos with different orderings or cannot reliably separate more from less consistent orderings, our POC loss\nefficiently aligns videos with different action orders and is\ndifferentiable, which enables end-to-end training. In addition, it avoids the time-consuming pseudo-label generation\nof prior works. Our method efficiently learns the actions\nand their temporal locations, therefore, extends the existing\nattention-based action localization methods from learning\none action per video to multiple actions using our POC loss\nalong with video-level and frame-level losses. By experiments on three datasets, we demonstrate that our method\nsignificantly improves the state of the art. We also show that\nour method, with a small modification, can effectively address the transcript-supervised action learning task, where\nactions and their ordering are available during training."}}
{"id": "spngLDxSX8", "cdate": 1668030141040, "mdate": 1668030141040, "content": {"title": "Semi-Weakly-Supervised Learning of Complex Actions from Instructional Task Videos", "abstract": "We address the problem of action segmentation in\ninstructional task videos with a small number of weakly-labeled training videos and a large number of unlabeled\nvideos, which we refer to as Semi-Weakly-Supervised\nLearning (SWSL) of actions. We propose a general\nSWSL framework that can efficiently learn from both\ntypes of videos and can leverage any of the existing\nweakly-supervised action segmentation methods. Our key\nobservation is that the distance between the transcript of\nan unlabeled video and those of the weakly-labeled videos\nfrom the same task is small yet often nonzero. Therefore, we\ndevelop a Soft Restricted Edit (SRE) loss to encourage small\nvariations between the predicted transcripts of unlabeled\nvideos and ground-truth transcripts of the weakly-labeled\nvideos of the same task. To compute the SRE loss, we\ndevelop a flexible transcript prediction (FTP) method that\nuses the output of the action classifier to find both the length\nof the transcript and the sequence of actions occurring\nin an unlabeled video. We propose an efficient learning\nscheme in which we alternate between minimizing our proposed loss and generating pseudo-transcripts for unlabeled\nvideos. By experiments on two benchmark datasets, we\ndemonstrate that our approach can significantly improve\nthe performance by using unlabeled videos, especially when\nthe number of weakly-labeled videos is small."}}
{"id": "vrY9Aw-DxL", "cdate": 1640995200000, "mdate": 1677627135177, "content": {"title": "Zero-Shot Attribute Attacks on Fine-Grained Recognition Models", "abstract": ""}}
{"id": "X25U6rurV8Z", "cdate": 1640995200000, "mdate": 1677627135289, "content": {"title": "Set-Supervised Action Learning in Procedural Task Videos via Pairwise Order Consistency", "abstract": ""}}
{"id": "ExVRneBk61k", "cdate": 1640995200000, "mdate": 1677627135262, "content": {"title": "Towards Effective Multi-Label Recognition Attacks via Knowledge Graph Consistency", "abstract": ""}}
{"id": "9W8NQu01c-", "cdate": 1640995200000, "mdate": 1668221006316, "content": {"title": "Semi-Weakly-Supervised Learning of Complex Actions from Instructional Task Videos", "abstract": "We address the problem of action segmentation in instructional task videos with a small number of weakly-labeled training videos and a large number of unlabeled videos, which we refer to as Semi-Weakly-Supervised Learning (SWSL) of actions. We propose a general SWSL framework that can efficiently learn from both types of videos and can leverage any of the existing weakly-supervised action segmentation methods. Our key observation is that the distance between the transcript of an unlabeled video and those of the weakly-labeled videos from the same task is small yet often nonzero. Therefore, we develop a Soft Restricted Edit (SRE) loss to encourage small variations between the predicted transcripts of unlabeled videos and ground-truth transcripts of the weakly-labeled videos of the same task. To compute the SRE loss, we develop a flexible transcript prediction (FTP) method that uses the output of the action classifier to find both the length of the transcript and the sequence of actions occurring in an unlabeled video. We propose an efficient learning scheme in which we alternate between minimizing our proposed loss and generating pseudo-transcripts for unlabeled videos. By experiments on two benchmark datasets, we demonstrate that our approach can significantly improve the performance by using unlabeled videos, especially when the number of weakly-labeled videos is small. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> Code available at https://github.com/Yuhan-Shen/SWSL.."}}
{"id": "IACULnkPwYO", "cdate": 1617820299575, "mdate": null, "content": {"title": "Compositional Zero-Shot Learning via Fine-Grained Dense Feature Composition", "abstract": "We develop a novel generative model for zero-shot learning to recognize fine\u0002grained unseen classes without training samples. Our observation is that generating\nholistic features of unseen classes fails to capture every attribute needed to dis\u0002tinguish small differences among classes. We propose a feature composition\nframework that learns to extract attribute-based features from training samples\nand combines them to construct fine-grained features for unseen classes. Feature\ncomposition allows us to not only selectively compose features of unseen classes\nfrom only relevant training samples, but also obtain diversity among composed\nfeatures via changing samples used for composition. In addition, instead of build\u0002ing a global feature of an unseen class, we use all attribute-based features to form\na dense representation consisting of fine-grained attribute details. To recognize\nunseen classes, we propose a novel training scheme that uses a discriminative\nmodel to construct features that are subsequently used to train itself. Therefore,\nwe directly train the discriminative model on composed features without learning\nseparate generative models. We conduct experiments on four popular datasets\nof DeepFashion, AWA2, CUB, and SUN, showing that our method significantly\nimproves the state of the art."}}
