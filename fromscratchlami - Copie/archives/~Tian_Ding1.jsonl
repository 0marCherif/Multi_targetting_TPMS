{"id": "rHHm-S5bwlq", "cdate": 1577836800000, "mdate": 1645840952938, "content": {"title": "The Global Landscape of Neural Networks: An Overview", "abstract": "One of the major concerns for neural network training is that the nonconvexity of the associated loss functions may cause a bad landscape. The recent success of neural networks suggests that their loss landscape is not too bad, but what specific results do we know about the landscape? In this article, we review recent findings and results on the global landscape of neural networks."}}
{"id": "Bpv_-B9WPgq", "cdate": 1577836800000, "mdate": 1645840953175, "content": {"title": "The Global Landscape of Neural Networks: An Overview", "abstract": "One of the major concerns for neural network training is that the non-convexity of the associated loss functions may cause bad landscape. The recent success of neural networks suggests that their loss landscape is not too bad, but what specific results do we know about the landscape? In this article, we review recent findings and results on the global landscape of neural networks. First, we point out that wide neural nets may have sub-optimal local minima under certain assumptions. Second, we discuss a few rigorous results on the geometric properties of wide networks such as \"no bad basin\", and some modifications that eliminate sub-optimal local minima and/or decreasing paths to infinity. Third, we discuss visualization and empirical explorations of the landscape for practical neural nets. Finally, we briefly discuss some convergence results and their relation to landscape results."}}
{"id": "BkltK0NtPr", "cdate": 1569439360974, "mdate": null, "content": {"title": "WHAT ILLNESS OF LANDSCAPE CAN OVER-PARAMETERIZATION ALONE CURE?", "abstract": "Over-parameterized networks are widely believed to have nice landscape, but what rigorous results can we prove? In this work, we prove that: (i) from under-parameterized to over-parameterized networks, there is a phase transition from having sub-optimal basins to no sub-optimal basins; (ii) over-parameterization alone cannot eliminate bad non-strict local minima. Specifically, we prove that for any continuous activation functions, the loss surface of a class of over-parameterized networks has no sub-optimal basin, where \u201cbasin\u201d is defined as the setwise strict local minimum. Furthermore, for under-parameterized network, we construct loss landscape with strict local minimum that is not global. We then show that it is impossible to prove \u201call over-parameterized networks have no sub-optimal local minima\u201d, by giving counter-examples for 1-hidden-layer networks with a class of neurons.\nViewing various bad patterns of landscape as illnesses (bad basins, flat regions, etc.), our results indicate that over-parameterization is not a panacea for every \u201cillness\u201d of the landscape, but it can cure one practically annoying illness (bad basins)."}}
{"id": "Y6EN8iCqu-S", "cdate": 1546300800000, "mdate": null, "content": {"title": "Sub-Optimal Local Minima Exist for Almost All Over-parameterized Neural Networks", "abstract": "Does over-parameterization eliminate sub-optimal local minima for neural networks? An affirmative answer was given by a classical result in [59] for 1-hidden-layer wide neural networks. A few recent works have extended the setting to multi-layer neural networks, but none of them has proved every local minimum is global. Why is this result never extended to deep networks? In this paper, we show that the task is impossible because the original result for 1-hidden-layer network in [59] can not hold. More specifically, we prove that for any multi-layer network with generic input data and non-linear activation functions, sub-optimal local minima can exist, no matter how wide the network is (as long as the last hidden layer has at least two neurons). While the result of [59] assumes sigmoid activation, our counter-example covers a large set of activation functions (dense in the set of continuous functions), indicating that the limitation is not due to the specific activation. Our result indicates that \"no bad local-min\" may be unable to explain the benefit of over-parameterization for training neural nets."}}
{"id": "LASb1nsxx1", "cdate": 1546300800000, "mdate": 1683172262583, "content": {"title": "Sparsity Learning-Based Multiuser Detection in Grant-Free Massive-Device Multiple Access", "abstract": "In this paper, we study the multiuser detection (MUD) problem for a grant-free massive-device multiple access (MaDMA) system, where a large number of single-antenna user devices transmit sporadic data to a multi-antenna base station (BS). Specifically, we put forth two MUD schemes, termed random sparsity learning multiuser detection (RSL-MUD) and structured sparsity learning multiuser detection (SSL-MUD) for the time-slotted and non-time-slotted grant-free MaDMA systems, respectively. In RSL-MUD, active users generate and transmit data packets with random sparsity. In SSL-MUD, we introduce a sliding-window-based detection framework, and the user signals in each observation window naturally exhibit structured sparsity. We show that by exploiting the sparsity embedded in the user signals, we can recover the user activity state, the channel, and the user data in a single phase, without using pilot signals for channel estimation and/or active user identification. To this end, we develop a message-passing-based statistical inference framework for the BS to blindly detect the user data without any prior knowledge of the identities and the channel state information (CSI) of active users. The simulation results show that our RSL-MUD and SSL-MUD schemes significantly outperform their counterpart schemes in both reducing the transmission overhead and improving the error behavior of the system."}}
{"id": "xt2NrgSvZV", "cdate": 1514764800000, "mdate": null, "content": {"title": "Over-Parameterized Deep Neural Networks Have No Strict Local Minima For Any Continuous Activations", "abstract": "Wide networks are often believed to have a nice optimization landscape, but what rigorous results can we prove? To understand the benefit of width, it is important to identify the difference between wide and narrow networks. In this work, we prove that from narrow to wide networks, there is a phase transition from having sub-optimal basins to no sub-optimal basins. Specifically, we prove two results: on the positive side, for any continuous activation functions, the loss surface of a class of wide networks has no sub-optimal basins, where \"basin\" is defined as the set-wise strict local minimum; on the negative side, for a large class of networks with width below a threshold, we construct strict local minima that are not global. These two results together show the phase transition from narrow to wide networks."}}
{"id": "LmPgTIMSO9", "cdate": 1514764800000, "mdate": 1683172262633, "content": {"title": "Structured Sparsity Learning Based Multiuser Detection in Massive-Device Multiple Access", "abstract": "In this work, we study the non-time-slotted massive-device multiple access (MaDMA) problem where massive user devices transmit sporadic data to a multi-antenna base station (BS). We develop a structured sparsity learning based multiuser detection (SSL-MUD) scheme. By exploiting the structured sparsity naturally embedded in user signals, our SSL-MUD scheme is able to blindly detect the user packets without any prior knowledge of the user activity state (UAS) and the channel state information (CSI), and hence significantly reduces the transmission overhead. For the blind signal detection at the BS, we put forth the turbo bilinear generalized approximate message passing (Turbo-BiG-AMP) algorithm. Simulation results demonstrate that the Turbo-BiG- AMP algorithm significantly outperforms the existing compressed sensing based approach and achieves a performance comparable to that of the oracle linear minimum mean-square error (Oracle- LMMSE) algorithm (which assumes perfect knowledge of UAS and CSI at the BS)."}}
{"id": "G40fzIltZy", "cdate": 1514764800000, "mdate": 1683172262547, "content": {"title": "Algorithmic Beamforming Design for MIMO Multiway Relay Channel With Clustered Full Data Exchange", "abstract": "This paper studies the beamforming design for the multiple-input multiple-output (MIMO) multiway relay channel with clustered full data exchange. We formulate the linear signal alignment in the model under a rank-constrained rank-minimization (RCRM) framework. Our contribution is twofold. We decompose the RCRM problem into independent rank-minimization subproblems and put forth an iterative algorithm for the beamforming design. For symmetric antenna setups, our approach advances the state of the art by expanding the achievable degree of freedom (DoF) region, and for asymmetric antenna setups, our approach can be directly applied while prior approaches are not readily applicable."}}
{"id": "kDfwemQcly9", "cdate": 1483228800000, "mdate": 1683172262615, "content": {"title": "Network-coded fronthaul transmission for cache-aided C-RAN", "abstract": "In this paper, we study the cache-aided cloud radio access network (C-RAN) with wireless fronthaul, where multiple cache-enabled users are served by multiple cache-enabled transmitters that are connected to a cloud processor through a wireless fronthaul link. We put forth a caching-and-delivery scheme that combines network-coded fronthaul transmission with cache-aided interference management. By broadcasting network-coded messages, the cloud processor provides additional information of the requested files to the transmitters, so as to reduce the edge delivery time. Based on our scheme, an achievable normalized delivery time (NDT) is derived with respect to the cache sizes and the fronthaul capacity."}}
{"id": "VGn8QsegeHJ", "cdate": 1483228800000, "mdate": 1683172262561, "content": {"title": "On the Degrees of Freedom of the Symmetric Multi-Relay MIMO Y Channel", "abstract": "In this paper, we study the degrees of freedom (DoF) of the symmetric multi-relay multiple-input multiple-output Y channel, where three user nodes, each with M antennas, communicate via K geographically separated relay nodes, each with N antennas. For this model, we establish a general DoF achievability framework based on linear precoding and post-processing methods. The framework poses a nonlinear problem with respect to user precoders, user post-processors, and relay precoders. To solve this problem, we adopt an uplink-downlink asymmetric strategy, where the user precoders are designed for signal alignment and the user post-processors are used for interference neutralization. With the user precoder and post-processor designs fixed as such, the original problem then reduces to a problem of relay precoder design. To address the solvability of the system, we propose a general method for solving matrix equations. Together with the techniques of antenna disablement and symbol extension, an achievable DoF of the considered model is derived for an arbitrary setup of (K, M, N). We show that for K \u2265 2, the optimal DoF is achieved for (M/N) \u2208 [0, max{(\u221a(3K/3)), 1}) \u222a [((3K + (9K <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup> - 12K) <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1/2</sup> )/6), \u221e). We also show that the uplink-downlink asymmetric design proposed in this paper considerably outperforms the conventional approach based on uplink-downlink symmetry."}}
