{"id": "akBpa1evT12", "cdate": 1684161419763, "mdate": 1684161419763, "content": {"title": "Edge: Explaining deep reinforcement learning policies", "abstract": "With the rapid development of deep reinforcement learning (DRL) techniques, there is an increasing need to understand and interpret DRL policies. While recent research has developed explanation methods to interpret how an agent determines its moves, they cannot capture the importance of actions/states to a game's final result. In this work, we propose a novel self-explainable model that augments a Gaussian process with a customized kernel function and an interpretable predictor. Together with the proposed model, we also develop a parameter learning procedure that leverages inducing points and variational inference to improve learning efficiency. Using our proposed model, we can predict an agent's final rewards from its game episodes and extract time step importance within episodes as strategy-level explanations for that agent. Through experiments on Atari and MuJoCo games, we verify the explanation fidelity of our method and demonstrate how to employ interpretation to understand agent behavior, discover policy vulnerabilities, remediate policy errors, and even defend against adversarial attacks."}}
{"id": "VzwJ2GctKJ", "cdate": 1665724005749, "mdate": 1665724005749, "content": {"title": "Are Shortest Rationales the Best Explanations For Human Understanding?", "abstract": "Existing self-explaining models typically favor extracting the shortest rationales possible (\u201cshortest yet coherent subset of input to predict the same label\u201d), with the assumption that short rationales are more intuitive to humans, even though short rationales lead to lower accuracy.   However, there is a lack of human studies on validating the effect of rationale length on human understanding. Is the shortest rationale indeed the most understandable for humans? To answer this question, we design a self-explaining model that can take control on rationale length.  Our model incorporates contextual information and supports flexibly extracting rationales at any target length. Through quantitative evaluation on model performance, we further verify that our method  LIMITEDINK outperforms existing self-explaining baselines on both end-task prediction and human-annotated rationale agreement.  We use it to generate rationales at 5 length levels, and conduct user studies to understand how much rationale would be sufficient for humans to confidently make predictions. We show that while most prior work extracts 10%-30% of the text to be the rationale, human accuracy tends to stabilize after seeing 40% of the full text.  Our result suggests the need for more careful design of the best human rationales."}}
{"id": "Wp3we5kv6P", "cdate": 1621630297290, "mdate": null, "content": {"title": "EDGE: Explaining Deep Reinforcement Learning Policies", "abstract": "With the rapid development of deep reinforcement learning (DRL) techniques, there is an increasing need to understand and interpret DRL policies. While recent research has developed explanation methods to interpret how an agent determines its moves, they cannot capture the importance of actions/states to a game's final result. In this work, we propose a novel self-explainable model that augments a Gaussian process with a customized kernel function and an interpretable predictor. Together with the proposed model, we also develop a parameter learning procedure that leverages inducing points and variational inference to improve learning efficiency. Using our proposed model, we can predict an agent's final rewards from its game episodes and extract time step importance within episodes as strategy-level explanations for that agent. Through experiments on Atari and MuJoCo games, we verify the explanation fidelity of our method and demonstrate how to employ interpretation to understand agent behavior, discover policy vulnerabilities, remediate policy errors, and even defend against adversarial attacks."}}
{"id": "ugb3fwiMpGT", "cdate": 1609459200000, "mdate": 1632856315301, "content": {"title": "BACKDOORL: Backdoor Attack against Competitive Reinforcement Learning", "abstract": "Recent research has confirmed the feasibility of backdoor attacks in deep reinforcement learning (RL) systems. However, the existing attacks require the ability to arbitrarily modify an agent's observation, constraining the application scope to simple RL systems such as Atari games. In this paper, we migrate backdoor attacks to more complex RL systems involving multiple agents and explore the possibility of triggering the backdoor without directly manipulating the agent's observation. As a proof of concept, we demonstrate that an adversary agent can trigger the backdoor of the victim agent with its own action in two-player competitive RL systems. We prototype and evaluate BackdooRL in four competitive environments. The results show that when the backdoor is activated, the winning rate of the victim drops by 17% to 37% compared to when not activated. The videos are hosted at https://github.com/wanglun1996/multi_agent_rl_backdoor_videos."}}
{"id": "dZqA5g6Lcxm", "cdate": 1609459200000, "mdate": 1632856315150, "content": {"title": "Adversarial Policy Learning in Two-player Competitive Games", "abstract": "In a two-player deep reinforcement learning task, recent work shows an attacker could learn an adversarial policy that triggers a target agent to perform poorly and even react in an undesired way. ..."}}
{"id": "QG3lNdgdzw", "cdate": 1609459200000, "mdate": 1632856315201, "content": {"title": "Adversarial Policy Training against Deep Reinforcement Learning", "abstract": ""}}
{"id": "MLL11Ium_70a", "cdate": 1609459200000, "mdate": 1632856315167, "content": {"title": "DANCE: Enhancing saliency maps using decoys", "abstract": "Saliency methods can make deep neural network predictions more interpretable by identifying a set of critical features in an input sample, such as pixels that contribute most strongly to a predicti..."}}
{"id": "BI7gRx-Noj_1", "cdate": 1609459200000, "mdate": 1632856315365, "content": {"title": "FARE: Enabling Fine-grained Attack Categorization under Low-quality Labeled Data", "abstract": ""}}
{"id": "A4lfN6cmrZO", "cdate": 1609459200000, "mdate": 1632856315205, "content": {"title": "CADE: Detecting and Explaining Concept Drift Samples for Security Applications", "abstract": ""}}
{"id": "6Vi7Vrg05O", "cdate": 1609459200000, "mdate": 1632856315195, "content": {"title": "RNNRepair: Automatic RNN Repair via Model-based Analysis", "abstract": "Deep neural networks are vulnerable to adversarial attacks. Due to their black-box nature, it is rather challenging to interpret and properly repair these incorrect behaviors. This paper focuses on..."}}
