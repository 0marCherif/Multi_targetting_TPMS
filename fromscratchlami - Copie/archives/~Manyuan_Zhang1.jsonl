{"id": "cYblvG2gBRI", "cdate": 1609459200000, "mdate": 1649148730444, "content": {"title": "Switchable K-class Hyperplanes for Noise-Robust Representation Learning", "abstract": "Optimizing the K-class hyperplanes in the latent space has become the standard paradigm for efficient representation learning. However, it\u2019s almost impossible to find an optimal K-class hyperplane to accurately describe the latent space of massive noisy data. For this potential problem, we constructively propose a new method, named Switchable K-class Hyperplanes (SKH), to sufficiently describe the latent space by the mixture of K-class hyperplanes. It can directly replace the conventional single K-class hyperplane optimization as the new paradigm for noise-robust representation learning. When collaborated with the popular ArcFace on million-level data representation learning, we found that the switchable manner in SKH can effectively eliminate the gradient conflict generated by real-world label noise on a single K-class hyperplane. Moreover, combined with the margin-based loss functions (e.g. ArcFace), we propose a simple Posterior Data Clean strategy to reduce the model optimization deviation on clean dataset caused by the reduction of valid categories in each K-class hyperplane. Extensive experiments demonstrate that the proposed SKH easily achieves new state-of-the-art on IJB-B and IJB-C by encouraging noise-robust representation learning. Our code will be available at https://github.com/liubx07/SKH.git."}}
{"id": "fs3yAltpNYb", "cdate": 1577836800000, "mdate": 1649148730273, "content": {"title": "Discriminability Distillation in Group Representation Learning", "abstract": "Learning group representation is a commonly concerned issue in tasks where the basic unit is a group, set, or sequence. Previously, the research community tries to tackle it by aggregating the elements in a group based on an indicator either defined by humans such as the quality and saliency, or generated by a black box such as the attention score. This article provides a more essential and explicable view. We claim the most significant indicator to show whether the group representation can be benefited from one of its element is not the quality or an inexplicable score, but the discriminability w.r.t. the model. We explicitly design the discrimiability using embedded class centroids on a proxy set. We show the discrimiability knowledge has good properties that can be distilled by a light-weight distillation network and can be generalized on the unseen target set. The whole procedure is denoted as discriminability distillation learning (DDL). The proposed DDL can be flexibly plugged into many group-based recognition tasks without influencing the original training procedures. Comprehensive experiments on various tasks have proven the effectiveness of DDL for both accuracy and efficiency. Moreover, it pushes forward the state-of-the-art results on these tasks by an impressive margin."}}
{"id": "cYJ1aElGt80", "cdate": 1577836800000, "mdate": 1649148730443, "content": {"title": "Complementary Boundary Generator with Scale-Invariant Relation Modeling for Temporal Action Localization: Submission to ActivityNet Challenge 2020", "abstract": "This technical report presents an overview of our solution used in the submission to ActivityNet Challenge 2020 Task 1 (\\textbf{temporal action localization/detection}). Temporal action localization requires to not only precisely locate the temporal boundaries of action instances, but also accurately classify the untrimmed videos into specific categories. In this paper, we decouple the temporal action localization task into two stages (i.e. proposal generation and classification) and enrich the proposal diversity through exhaustively exploring the influences of multiple components from different but complementary perspectives. Specifically, in order to generate high-quality proposals, we consider several factors including the video feature encoder, the proposal generator, the proposal-proposal relations, the scale imbalance, and ensemble strategy. Finally, in order to obtain accurate detections, we need to further train an optimal video classifier to recognize the generated proposals. Our proposed scheme achieves the state-of-the-art performance on the temporal action localization task with \\textbf{42.26} average mAP on the challenge testing set."}}
{"id": "M-J9jfJlFxn", "cdate": 1577836800000, "mdate": 1649148730442, "content": {"title": "1st place solution for AVA-Kinetics Crossover in AcitivityNet Challenge 2020", "abstract": "This technical report introduces our winning solution to the spatio-temporal action localization track, AVA-Kinetics Crossover, in ActivityNet Challenge 2020. Our entry is mainly based on Actor-Context-Actor Relation Network. We describe technical details for the new AVA-Kinetics dataset, together with some experimental results. Without any bells and whistles, we achieved 39.62 mAP on the test set of AVA-Kinetics, which outperforms other entries by a large margin. Code will be available at: https://github.com/Siyu-C/ACAR-Net."}}
{"id": "Bfhy5fZsADr", "cdate": 1577836800000, "mdate": 1649148730434, "content": {"title": "Top-1 Solution of Multi-Moments in Time Challenge 2019", "abstract": "In this technical report, we briefly introduce the solutions of our team 'Efficient' for the Multi-Moments in Time challenge in ICCV 2019. We first conduct several experiments with popular Image-Based action recognition methods TRN, TSN, and TSM. Then a novel temporal interlacing network is proposed towards fast and accurate recognition. Besides, the SlowFast network and its variants are explored. Finally, we ensemble all the above models and achieve 67.22\\% on the validation set and 60.77\\% on the test set, which ranks 1st on the final leaderboard. In addition, we release a new code repository for video understanding which unifies state-of-the-art 2D and 3D methods based on PyTorch. The solution of the challenge is also included in the repository, which is available at https://github.com/Sense-X/X-Temporal."}}
{"id": "9jPQK9OExvf", "cdate": 1577836800000, "mdate": 1649148730259, "content": {"title": "Discriminability Distillation in Group Representation Learning", "abstract": "Learning group representation is a commonly concerned issue in tasks where the basic unit is a group, set, or sequence. Previously, the research community tries to tackle it by aggregating the elements in a group based on an indicator either defined by humans such as the quality and saliency, or generated by a black box such as the attention score. This article provides a more essential and explicable view. We claim the most significant indicator to show whether the group representation can be benefited from one of its element is not the quality or an inexplicable score, but the discriminability w.r.t. the model. We explicitly design the discrimiability using embedded class centroids on a proxy set. We show the discrimiability knowledge has good properties that can be distilled by a light-weight distillation network and can be generalized on the unseen target set. The whole procedure is denoted as discriminability distillation learning (DDL). The proposed DDL can be flexibly plugged into many group-based recognition tasks without influencing the original training procedures. Comprehensive experiments on various tasks have proven the effectiveness of DDL for both accuracy and efficiency. Moreover, it pushes forward the state-of-the-art results on these tasks by an impressive margin."}}
{"id": "rJgE9CEYPS", "cdate": 1569439371597, "mdate": null, "content": {"title": "Discriminability Distillation in Group Representation Learning", "abstract": "Learning group representation is a commonly concerned issue in tasks where the basic unit is a group, set or sequence.\nThe computer vision community tries to tackle it by aggregating the elements in a group based on an indicator either defined by human such as the quality or saliency of an element, or generated by a black box such as the attention score or output of a RNN. \n\nThis article provides a more essential and explicable view. \nWe claim the most significant indicator to show whether the group representation can be benefited from an element is not the quality, or an inexplicable score, but the \\textit{discrimiability}. \nOur key insight is to explicitly design the \\textit{discrimiability} using embedded class centroids on a proxy set, \nand show the discrimiability distribution \\textit{w.r.t.} the element space can be distilled by a light-weight auxiliary distillation network. \nThis processing is called \\textit{discriminability distillation learning} (DDL).\nWe show the proposed DDL can be flexibly plugged into many group based recognition tasks without influencing the training procedure of the original tasks. Comprehensive experiments on set-to-set face recognition and action recognition valid the advantage of DDL on both accuracy and efficiency, and it pushes forward the state-of-the-art results on these tasks by an impressive margin."}}
{"id": "WViu1axmGLr", "cdate": 1546300800000, "mdate": 1649148730377, "content": {"title": "Towards Flops-constrained Face Recognition", "abstract": "Large scale face recognition is challenging especially when the computational budget is limited. Given a \\textit{flops} upper bound, the key is to find the optimal neural network architecture and optimization method. In this article, we briefly introduce the solutions of team 'trojans' for the ICCV19 - Lightweight Face Recognition Challenge~\\cite{lfr}. The challenge requires each submission to be one single model with the computational budget no higher than 30 GFlops. We introduce a searched network architecture `Efficient PolyFace' based on the Flops constraint, a novel loss function `ArcNegFace', a novel frame aggregation method `QAN++', together with a bag of useful tricks in our implementation (augmentations, regular face, label smoothing, anchor finetuning, etc.). Our basic model, `Efficient PolyFace', takes 28.25 Gflops for the `deepglint-large' image-based track, and the `PolyFace+QAN++' solution takes 24.12 Gflops for the `iQiyi-large' video-based track. These two solutions achieve 94.198\\% @ 1e-8 and 72.981\\% @ 1e-4 in the two tracks respectively, which are the state-of-the-art results."}}
{"id": "RqgbTERyqgD", "cdate": 1546300800000, "mdate": 1649148730243, "content": {"title": "Towards Flops-Constrained Face Recognition", "abstract": "Large scale face recognition is challenging especially when the computational budget is limited. Given a flops upper bound, the key is to find the optimal neural network architecture and optimization method. In this article, we introduce the solutions of team 'trojans' for the ICCV19 - Lightweight Face Recognition Challenge. Our team mainly focuses on the two 'large' tracks, image-based and video-based, respectively. The submissions of these two tracks are required to be one single model with computational budget no higher than 30 GFlops. We introduce a network architecture 'Efficient PolyFace', a novel loss function 'ArcNegFace', a novel frame aggregation method 'QAN++', together with a bag of useful tricks in our implementation (augmentations, regular face, label smoothing, anchor finetuning, etc.). Our basic model, 'Efficient PolyFace', takes 28.25 Gflops for the 'deepglint-large' image-based track, and the 'PolyFace+QAN++' solution takes 24.12 Gflops for the 'iQiyi-large' video-based track. These two solutions achieve 94.198% @ 1e-8 and 72.981% @ 1e-4 in the two tracks respectively, which are the state-of-the-art results."}}
{"id": "v2mEdRmRoPG", "cdate": 1514764800000, "mdate": 1649148730275, "content": {"title": "Privacy-preserving Sensory Data Recovery", "abstract": "In recent years, a large scale of various wireless sensor networks have been deployed for basic scientific works. Massive data loss is so common that there is a great demand for data recovery. While data recovery methods fulfil the requirement of accuracy, the potential privacy leakage caused by them concerns us a lot. Thus the major challenge of sensory data recovery is the issue of effective privacy preservation. Existing algorithms can either accomplish accurate data recovery or solve privacy issue, yet no single design is able to address these two problems simultaneously. Therefore in this paper, we propose a novel approach Privacy-Preserving Compressive Sensing with Multi-Attribute Assistance (PPCS-MAA). It applies PPCS scheme to sensory data recovery, which can effectively encrypts sensory data without decreasing accuracy, because it maintains the homomorphic obfuscation property for compressive sensing. In addition, multiple environmental attributes from sensory datasets usually have strong correlation so that we design a MultiAttribute Assistance (MAA) component to leverage this feature for better recovery accuracy. Combining PPCS with MAA, the novel recovery scheme can provide reliable privacy with high accuracy. Firstly, based on two real datasets, IntelLab and GreenOrbs, we reveal the inherited low-rank features as the ground truth and find such multi-attribute correlation. Secondly, we develop a PPCS-MAA algorithm to preserve privacy and optimize the recovery accuracy. Thirdly, the results of real data-driven simulations show that the algorithm outperforms the existing solutions."}}
