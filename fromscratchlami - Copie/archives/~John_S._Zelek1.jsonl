{"id": "uAjexCokEn", "cdate": 1672531200000, "mdate": 1699987840465, "content": {"title": "Mitigating Motion Blur for Robust 3D Baseball Player Pose Modeling for Pitch Analysis", "abstract": "Using videos to analyze pitchers in baseball can play a vital role in strategizing and injury prevention. Computer vision-based pose analysis offers a time-eficient and cost-effective approach. However, the use of accessible broadcast videos, with a 30fps framerate, often results in partial body motion blur during fast actions, limiting the performance of existing pose keypoint estimation models. Previous works have primarily relied on fixed backgrounds, assuming minimal motion differences between frames, or utilized multiview data to address this problem. To this end, we propose a synthetic data augmentation pipeline to enhance the model's capability to deal with the pitcher's blurry actions. In addition, we leverage in-the-wild videos to make our model robust under different real-world conditions and camera positions. By carefully optimizing the augmentation parameters, we observed a notable reduction in the loss by 54.2% and 36.2% on the test dataset for 2D and 3D pose estimation respectively. By applying our approach to existing state-of-the-art pose estimators, we demonstrate an average improvement of 29.2%. The findings highlight the effectiveness of our method in mitigating the challenges posed by motion blur, thereby enhancing the overall quality of pose estimation."}}
{"id": "t78h0mvFIpB", "cdate": 1672531200000, "mdate": 1699987840533, "content": {"title": "Motion Segmentation from a Moving Monocular Camera", "abstract": "Identifying and segmenting moving objects from a moving monocular camera is difficult when there is unknown camera motion, different types of object motions and complex scene structures. To tackle these challenges, we take advantage of two popular branches of monocular motion segmentation approaches: point trajectory based and optical flow based methods, by synergistically fusing these two highly complementary motion cues at object level. By doing this, we are able to model various complex object motions in different scene structures at once, which has not been achieved by existing methods. We first obtain object-specific point trajectories and optical flow mask for each common object in the video, by leveraging the recent foundational models in object recognition, segmentation and tracking. We then construct two robust affinity matrices representing the pairwise object motion affinities throughout the whole video using epipolar geometry and the motion information provided by optical flow. Finally, co-regularized multi-view spectral clustering is used to fuse the two affinity matrices and obtain the final clustering. Our method shows state-of-the-art performance on the KT3DMoSeg dataset, which contains complex motions and scene structures. Being able to identify moving objects allows us to remove them for map building when using visual SLAM or SFM."}}
{"id": "rOT6AJjx6ba", "cdate": 1672531200000, "mdate": 1699987840532, "content": {"title": "Naive Scene Graphs: How Visual is Modern Visual Relationship Detection?", "abstract": "Modern approaches to scene graph generation still struggle with their performance, with even state of the art approaches hovering under a 15% mean recall on certain evaluation modes. This poor performance is partially a result of networks heavily relying and fixating on non-visual data, such as class statistics, instead of the pixel-level signals present in the images. We demonstrate this by examining the 'visual-ness' of visual relationship detection approaches. We first describe and implement a new Naive Bayes-based statistical baseline for scene graph generation. Most notably, this basic classifier does not utilize the image pixels, but relies on the properties of the bounding boxes (class labels, topological configuration, \u2026 etc.) to predict the relationship labels. We demonstrate that our classical machine learning approach, one as simple as a categorical Naive Bayes classifier, can perform relationship detection in a manner that achieves relatively competitive performance to that of modern scene graph generators. This is an alarming finding regarding scene graph generation that implies that visual data in images may not be utilized in modern visual relationship detection past the point of object detection. We finally discuss how more visual modern approaches to scene graph generation appear to remedy some of these shortcomings."}}
{"id": "q_k63wVQr9y", "cdate": 1672531200000, "mdate": 1699150422025, "content": {"title": "Mitigating Motion Blur for Robust 3D Baseball Player Pose Modeling for Pitch Analysis", "abstract": "Using videos to analyze pitchers in baseball can play a vital role in strategizing and injury prevention. Computer vision-based pose analysis offers a time-efficient and cost-effective approach. However, the use of accessible broadcast videos, with a 30fps framerate, often results in partial body motion blur during fast actions, limiting the performance of existing pose keypoint estimation models. Previous works have primarily relied on fixed backgrounds, assuming minimal motion differences between frames, or utilized multiview data to address this problem. To this end, we propose a synthetic data augmentation pipeline to enhance the model's capability to deal with the pitcher's blurry actions. In addition, we leverage in-the-wild videos to make our model robust under different real-world conditions and camera positions. By carefully optimizing the augmentation parameters, we observed a notable reduction in the loss by 54.2% and 36.2% on the test dataset for 2D and 3D pose estimation respectively. By applying our approach to existing state-of-the-art pose estimators, we demonstrate an average improvement of 29.2%. The findings highlight the effectiveness of our method in mitigating the challenges posed by motion blur, thereby enhancing the overall quality of pose estimation."}}
{"id": "apD6bRK-54", "cdate": 1672531200000, "mdate": 1699987840534, "content": {"title": "Player tracking and identification in ice hockey", "abstract": ""}}
{"id": "UIs16e4gXAk", "cdate": 1672531200000, "mdate": 1699987840457, "content": {"title": "An Overlap-Free Calibration Method for LiDAR-Camera Platforms Based on Environmental Perception", "abstract": "Indoor environments are challenging for multisensor calibrations. Traditional calibration methods use the target structure for camera and LiDAR calibration. Those approaches not only require pre-processed data and offline calculations, but also face challenges in low-light and object-occluded indoor environments. We proposed an automatic calibration method using trajectory constraints on the LiDAR-Camera. The proposed method first obtains the accurate LiDAR trajectory by the LiDAR-SLAM (LIO-SAM) algorithm. At the same time, the problem of visual SLAM trajectory drift in the indoor environment is improved by graphical optimization using the rigid relative position invariance between sensors during displacement. Thus, extrinsic calibration is achieved by using the relative relationship between sensor trajectories. This method has higher robustness than the target-based calibration methods. The experimental results show that our algorithm has higher accuracy than the target-based calibration in the underground environment. The rotation root-mean-square error (RMSE) improves from 6.637\u00b0 to 0.564\u00b0, and the translation RMSE improves from 0.197 to 0.082 m."}}
{"id": "TQcvLsQ92s", "cdate": 1672531200000, "mdate": 1699987840457, "content": {"title": "Jersey Number Recognition using Keyframe Identification from Low-Resolution Broadcast Videos", "abstract": "Player identification is a crucial component in vision-driven soccer analytics, enabling various downstream tasks such as player assess- ment, in-game analysis, and broadcast production. However, auto- matically detecting jersey numbers from player tracklets in videos presents challenges due to motion blur, low resolution, distortions, and occlusions. Existing methods, utilizing Spatial Transformer Networks, CNNs, and Vision Transformers, have shown success in image data but struggle with real-world video data, where jersey numbers are not visible in most of the frames. Hence, identifying frames that contain the jersey number is a key sub-problem to tackle. To address these issues, we propose a robust keyframe identification module that extracts frames containing essential high-level infor- mation about the jersey number. A spatio-temporal network is then employed to model spatial and temporal context and predict the probabilities of jersey numbers in the video. Additionally, we adopt a multi-task loss function to predict the probability distribution of each digit separately. Extensive evaluations on the SoccerNet dataset demonstrate that incorporating our proposed keyframe identification module results in a significant 37.81% and 37.70% increase in the accuracies of 2 different test sets with domain gaps. These results highlight the effectiveness and importance of our approach in tackling the challenges of automatic jersey number detection in sports videos."}}
{"id": "RBwhW4iMUi", "cdate": 1672531200000, "mdate": 1699987840522, "content": {"title": "H-SLAM: Hybrid Direct-Indirect Visual SLAM", "abstract": "The recent success of hybrid methods in monocular odometry has led to many attempts to generalize the performance gains to hybrid monocular SLAM. However, most attempts fall short in several respects, with the most prominent issue being the need for two different map representations (local and global maps), with each requiring different, computationally expensive, and often redundant processes to maintain. Moreover, these maps tend to drift with respect to each other, resulting in contradicting pose and scene estimates, and leading to catastrophic failure. In this paper, we propose a novel approach that makes use of descriptor sharing to generate a single inverse depth scene representation. This representation can be used locally, queried globally to perform loop closure, and has the ability to re-activate previously observed map points after redundant points are marginalized from the local map, eliminating the need for separate and redundant map maintenance processes. The maps generated by our method exhibit no drift between each other, and can be computed at a fraction of the computational cost and memory footprint required by other monocular SLAM systems. Despite the reduced resource requirements, the proposed approach maintains its robustness and accuracy, delivering performance comparable to state-of-the-art SLAM methods (e.g., LDSO, ORB-SLAM3) on the majority of sequences from well-known datasets like EuRoC, KITTI, and TUM VI. The source code is available at: https://github.com/AUBVRL/fslam_ros_docker."}}
{"id": "IKmluVMThFz", "cdate": 1672531200000, "mdate": 1699150422005, "content": {"title": "Jersey Number Recognition using Keyframe Identification from Low-Resolution Broadcast Videos", "abstract": "Player identification is a crucial component in vision-driven soccer analytics, enabling various downstream tasks such as player assessment, in-game analysis, and broadcast production. However, automatically detecting jersey numbers from player tracklets in videos presents challenges due to motion blur, low resolution, distortions, and occlusions. Existing methods, utilizing Spatial Transformer Networks, CNNs, and Vision Transformers, have shown success in image data but struggle with real-world video data, where jersey numbers are not visible in most of the frames. Hence, identifying frames that contain the jersey number is a key sub-problem to tackle. To address these issues, we propose a robust keyframe identification module that extracts frames containing essential high-level information about the jersey number. A spatio-temporal network is then employed to model spatial and temporal context and predict the probabilities of jersey numbers in the video. Additionally, we adopt a multi-task loss function to predict the probability distribution of each digit separately. Extensive evaluations on the SoccerNet dataset demonstrate that incorporating our proposed keyframe identification module results in a significant 37.81% and 37.70% increase in the accuracies of 2 different test sets with domain gaps. These results highlight the effectiveness and importance of our approach in tackling the challenges of automatic jersey number detection in sports videos."}}
{"id": "risQg1QlOTH", "cdate": 1546300800000, "mdate": null, "content": {"title": "Temporal Hockey Action Recognition via Pose and Optical Flows.", "abstract": "In this paper, a novel two-stream architecture has been designed to improve action recognition accuracy for hockey using three main components. First, pose is estimated via the Part Affinity Fields model to extract meaningful cues from the player. Second, optical flow (using LiteFlownet) is used to extract temporal features. Third, pose and optical flow streams are fused and passed to fully-connected layers to estimate the hockey player's action. A novel publicly available dataset named HARPET (Hockey Action Recognition Pose Estimation, Temporal) was created, composed of sequences of annotated actions and pose of hockey players including their hockey sticks as an extension of human body pose. Three contributions are recognized. (1) The novel two-stream architecture achieves 85% action recognition accuracy, with the inclusion of optical flows increasing accuracy by about 10%. Thus, demonstrating the complementary nature of pose estimation and optical flow. (2) The unique localization of hand-held objects (e.g., hockey sticks) as part of pose increases accuracy by about 13%. (3) For pose estimation, a bigger and more general dataset, MSCOCO, is successfully used for transfer learning to a smaller and more specific dataset, HARPET, achieving a PCKh of 87%."}}
