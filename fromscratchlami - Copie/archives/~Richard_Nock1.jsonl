{"id": "2o5PxBE1bhk", "cdate": 1672531200000, "mdate": 1681751818897, "content": {"title": "Smoothly Giving up: Robustness for Simple Models", "abstract": "There is a growing need for models that are interpretable and have reduced energy and computational cost (e.g., in health care analytics and federated learning). Examples of algorithms to train such models include logistic regression and boosting. However, one challenge facing these algorithms is that they provably suffer from label noise; this has been attributed to the joint interaction between oft-used convex loss functions and simpler hypothesis classes, resulting in too much emphasis being placed on outliers. In this work, we use the margin-based $\\alpha$-loss, which continuously tunes between canonical convex and quasi-convex losses, to robustly train simple models. We show that the $\\alpha$ hyperparameter smoothly introduces non-convexity and offers the benefit of \"giving up\" on noisy training examples. We also provide results on the Long-Servedio dataset for boosting and a COVID-19 survey dataset for logistic regression, highlighting the efficacy of our approach across multiple relevant domains."}}
{"id": "22iiqQC4wmn", "cdate": 1672531200000, "mdate": 1681751818821, "content": {"title": "LegendreTron: Uprising Proper Multiclass Loss Learning", "abstract": "Loss functions serve as the foundation of supervised learning and are often chosen prior to model development. To avoid potentially ad hoc choices of losses, statistical decision theory describes a desirable property for losses known as \\emph{properness}, which asserts that Bayes' rule is optimal. Recent works have sought to \\emph{learn losses} and models jointly. Existing methods do this by fitting an inverse canonical link function which monotonically maps $\\mathbb{R}$ to $[0,1]$ to estimate probabilities for binary problems. In this paper, we extend monotonicity to maps between $\\mathbb{R}^{C-1}$ and the projected probability simplex $\\tilde{\\Delta}^{C-1}$ by using monotonicity of gradients of convex functions. We present {\\sc LegendreTron} as a novel and practical method that jointly learns \\emph{proper canonical losses} and probabilities for multiclass problems. Tested on a benchmark of domains with up to 1,000 classes, our experimental results show that our method consistently outperforms the natural multiclass baseline under a $t$-test at 99% significance on all datasets with greater than 10 classes."}}
{"id": "nBmPm1_rL7z", "cdate": 1668776691528, "mdate": 1668776691528, "content": {"title": "Siamese Networks: The Tale of Two Manifolds", "abstract": "Siamese networks are non-linear deep models that have found their ways into a broad set of problems in learning theory, thanks to their embedding capabilities. In this paper, we study Siamese networks from a new perspective and question the validity of their training procedure. We show that in the majority of cases, the objective of a Siamese network is endowed with an invariance property. Neglecting the invariance property leads to a hindrance in training the Siamese networks. To alleviate this issue, we propose two Riemannian structures and generalize a well-established accelerated stochastic gradient descent method to take into account the proposed Riemannian structures. Our empirical evaluations suggest that by making use of the Riemannian geometry, we achieve state-of-the-art results against several algorithms for the challenging problem of fine-grained image classification."}}
{"id": "OtUNyRYAbL", "cdate": 1667348470124, "mdate": 1667348470124, "content": {"title": "Manifold Learning Benefits GANs", "abstract": "In this paper, we improve Generative Adversarial Net- works by incorporating a manifold learning step into the discriminator. We consider locality-constrained linear and subspace-based manifolds, and locality-constrained non-linear manifolds. In our design, the manifold learning and coding steps are intertwined with layers of the discriminator, with the goal of attracting intermediate feature representations onto manifolds. We adaptively balance the discrepancy between feature representations and their manifold view, which is a trade-off between denoising on the manifold and refining the manifold. We find that locality-constrained non-linear manifolds outperform linear manifolds due to their non-uniform density and smoothness. We also substantially outperform state-of-the-art baselines."}}
{"id": "rxrLt7rTlAr", "cdate": 1652737495520, "mdate": null, "content": {"title": "Fair Wrapping for Black-box Predictions", "abstract": "We introduce a new family of techniques to post-process (``wrap\") a black-box classifier in order to reduce its bias. Our technique builds on the recent analysis of improper loss functions whose optimization can correct any twist in prediction, unfairness being treated as a twist. In the post-processing, we learn a wrapper function which we define as an $\\alpha$-tree, which modifies the prediction. We provide two generic boosting algorithms to learn $\\alpha$-trees. We show that our modification has appealing properties in terms of composition of $\\alpha$-trees, generalization, interpretability, and KL divergence between modified and original predictions. We exemplify the use of our technique in three fairness notions: conditional value-at-risk, equality of opportunity, and statistical parity; and provide experiments on several readily available datasets."}}
{"id": "VePo1Lidm5", "cdate": 1640995200000, "mdate": 1681491092344, "content": {"title": "Clustering above Exponential Families with Tempered Exponential Measures", "abstract": ""}}
{"id": "TLqXM2y82-Y", "cdate": 1640995200000, "mdate": 1681751818955, "content": {"title": "Manifold Learning Benefits GANs", "abstract": "In this paper <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> Code: https://qithub.com/MaxwellYaoNi/LCSAGAN., we improve Generative Adversarial Net-works by incorporating a manifold learning step into the discriminator. We consider locality-constrained linear and subspace-based manifolds <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup> The coding spaces considered in this paper are loosely termed man-ifolds. In most cases they are not manifolds in the strict mathematical sense, but rather topological spaces such as varieties, or simplicial com-plexes. The word will be used only in an informal sense., and locality-constrained non-linear manifolds. In our design, the manifold learning and coding steps are intertwined with layers of the discrimina-tor, with the goal of attracting intermediate feature repre-sentations onto manifolds. We adaptively balance the dis-crepancy between feature representations and their mani-fold view, which is a trade-off between denoising on the manifold and refining the manifold. We find that locality-constrained non-linear manifolds outperform linear mani-folds due to their non-uniform density and smoothness. We also substantially outperform state-of-the-art baselines."}}
{"id": "M9rteKWX7Yy", "cdate": 1640995200000, "mdate": 1667589760864, "content": {"title": "Fair Wrapping for Black-box Predictions", "abstract": "We introduce a new family of techniques to post-process (\"wrap\") a black-box classifier in order to reduce its bias. Our technique builds on the recent analysis of improper loss functions whose optimization can correct any twist in prediction, unfairness being treated as a twist. In the post-processing, we learn a wrapper function which we define as an $\\alpha$-tree, which modifies the prediction. We provide two generic boosting algorithms to learn $\\alpha$-trees. We show that our modification has appealing properties in terms of composition of $\\alpha$-trees, generalization, interpretability, and KL divergence between modified and original predictions. We exemplify the use of our technique in three fairness notions: conditional value-at-risk, equality of opportunity, and statistical parity; and provide experiments on several readily available datasets."}}
{"id": "7qMc0AXqJU_", "cdate": 1640995200000, "mdate": 1681751818988, "content": {"title": "What killed the Convex Booster ?", "abstract": "A landmark negative result of Long and Servedio established a worst-case spectacular failure of a supervised learning trio (loss, algorithm, model) otherwise praised for its high precision machinery. Hundreds of papers followed up on the two suspected culprits: the loss (for being convex) and/or the algorithm (for fitting a classical boosting blueprint). Here, we call to the half-century+ founding theory of losses for class probability estimation (properness), an extension of Long and Servedio's results and a new general boosting algorithm to demonstrate that the real culprit in their specific context was in fact the (linear) model class. We advocate for a more general stanpoint on the problem as we argue that the source of the negative result lies in the dark side of a pervasive -- and otherwise prized -- aspect of ML: \\textit{parameterisation}."}}
{"id": "56m72faLE1", "cdate": 1640995200000, "mdate": 1681751818913, "content": {"title": "Generative Trees: Adversarial and Copycat", "abstract": "While Generative Adversarial Networks (GANs) achieve spectacular results on unstructured data like images, there is still a gap on <em>tabular data</em>, data for which state of the art <em>supervi..."}}
