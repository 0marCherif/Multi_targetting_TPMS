{"id": "tldrgRvVQSR", "cdate": 1672531200000, "mdate": 1695771160248, "content": {"title": "Assumption Generation for the Verification of Learning-Enabled Autonomous Systems", "abstract": "Providing safety guarantees for autonomous systems is difficult as these systems operate in complex environments that require the use of learning-enabled components, such as deep neural networks (DNNs) for visual perception. DNNs are hard to analyze due to their size (they can have thousands or millions of parameters), lack of formal specifications (DNNs are typically learnt from labeled data, in the absence of any formal requirements), and sensitivity to small changes in the environment. We present an assume-guarantee style compositional approach for the formal verification of system-level safety properties of such autonomous systems. Our insight is that we can analyze the system in the absence of the DNN perception components by automatically synthesizing assumptions on the DNN behaviour that guarantee the satisfaction of the required safety properties. The synthesized assumptions are the weakest in the sense that they characterize the output sequences of all the possible DNNs that, plugged into the autonomous system, guarantee the required safety properties. The assumptions can be leveraged as run-time monitors over a deployed DNN to guarantee the safety of the overall system; they can also be mined to extract local specifications for use during training and testing of DNNs. We illustrate our approach on a case study taken from the autonomous airplanes domain that uses a complex DNN for perception."}}
{"id": "nCfVPODtvB", "cdate": 1672531200000, "mdate": 1695771160241, "content": {"title": "Closed-Loop Analysis of Vision-Based Autonomous Systems: A Case Study", "abstract": "Deep neural networks (DNNs) are increasingly used in safety-critical autonomous systems as perception components processing high-dimensional image data. Formal analysis of these systems is particularly challenging due to the complexity of the perception DNNs, the sensors (cameras), and the environment conditions. We present a case study applying formal probabilistic analysis techniques to an experimental autonomous system that guides airplanes on taxiways using a perception DNN. We address the above challenges by replacing the camera and the network with a compact abstraction whose transition probabilities are computed from the confusion matrices measuring the performance of the DNN on a representative image data set. As the probabilities are estimated based on empirical data, and thus are subject to error, we also compute confidence intervals in addition to point estimates for these probabilities and thereby strengthen the soundness of the analysis. We also show how to leverage local, DNN-specific analyses as run-time guards to filter out mis-behaving inputs and increase the safety of the overall system. Our findings are applicable to other autonomous systems that use complex DNNs for perception."}}
{"id": "dClpHbsjgcb", "cdate": 1672531200000, "mdate": 1695771160242, "content": {"title": "On the Perils of Cascading Robust Classifiers", "abstract": ""}}
{"id": "1QgVdbWbpZ2", "cdate": 1672531200000, "mdate": 1682341545605, "content": {"title": "Feature-Guided Analysis of Neural Networks", "abstract": "Applying standard software engineering practices to neural networks is challenging due to the lack of high-level abstractions describing a neural network\u2019s behavior. To address this challenge, we propose to extract high-level task-specific features from the neural network internal representation, based on monitoring the neural network activations. The extracted feature representations can serve as a link to high-level requirements and can be leveraged to enable fundamental software engineering activities, such as automated testing, debugging, requirements analysis, and formal verification, leading to better engineering of neural networks. Using two case studies, we present initial empirical evidence demonstrating the feasibility of our ideas."}}
{"id": "-rErKuNRIuj", "cdate": 1672531200000, "mdate": 1680025254415, "content": {"title": "Closed-loop Analysis of Vision-based Autonomous Systems: A Case Study", "abstract": ""}}
{"id": "tQG-o3SeipT", "cdate": 1663850045659, "mdate": null, "content": {"title": "On the Perils of Cascading Robust Classifiers", "abstract": "Ensembling certifiably robust neural networks is a promising approach for improving the \\emph{certified robust accuracy} of neural models. \nBlack-box ensembles that assume only query-access to the constituent models (and their robustness certifiers) during prediction are particularly attractive due to their modular structure. Cascading ensembles are a popular instance of black-box ensembles that appear to improve certified robust accuracies in practice. However, we show that the robustness certifier used by a cascading ensemble is unsound. That is, when a cascading ensemble is certified as locally robust at an input $x$ (with respect to $\\epsilon$), there can be inputs $x'$ in the $\\epsilon$-ball centered at $x$, such that the cascade's prediction at $x'$ is different from $x$ and thus the ensemble is not locally robust. Our theoretical findings are accompanied by empirical results that further demonstrate this unsoundness. We present a new attack against cascading ensembles and show that: (1) there exists an adversarial input for up to 88\\% of the samples where the ensemble claims to be certifiably robust and accurate; and (2) the accuracy of a cascading ensemble under our attack is as low as 11\\% when it claims to be certifiably robust and accurate on 97\\% of the test set. Our work reveals a critical pitfall of cascading certifiably robust models by showing that the seemingly beneficial strategy of cascading can actually hurt the robustness of the resulting ensemble. Our code is available at https://github.com/TristaChi/ensembleKW."}}
{"id": "uhfyggDgUj", "cdate": 1640995200000, "mdate": 1680025254413, "content": {"title": "Self-correcting Neural Networks for Safe Classification", "abstract": ""}}
{"id": "rh4zoxjmGmc", "cdate": 1640995200000, "mdate": 1648667378757, "content": {"title": "Discrete-Event Controller Synthesis for Autonomous Systems with Deep-Learning Perception Components", "abstract": "We present DEEPDECS, a new method for the synthesis of correct-by-construction discrete-event controllers for autonomous systems that use deep neural network (DNN) classifiers for the perception step of their decision-making processes. Despite major advances in deep learning in recent years, providing safety guarantees for these systems remains very challenging. Our controller synthesis method addresses this challenge by integrating DNN verification with the synthesis of verified Markov models. The synthesised models correspond to discrete-event controllers guaranteed to satisfy the safety, dependability and performance requirements of the autonomous system, and to be Pareto optimal with respect to a set of optimisation criteria. We use the method in simulation to synthesise controllers for mobile-robot collision avoidance, and for maintaining driver attentiveness in shared-control autonomous driving."}}
{"id": "D5uRpq2Fqm", "cdate": 1640995200000, "mdate": 1680025254414, "content": {"title": "On the Perils of Cascading Robust Classifiers", "abstract": ""}}
{"id": "BO8R-RyInPo", "cdate": 1640995200000, "mdate": 1680025254414, "content": {"title": "A Cascade of Checkers for Run-time Certification of Local Robustness", "abstract": ""}}
