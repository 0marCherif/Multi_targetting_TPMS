{"id": "9d6IOxpHnk", "cdate": 1702045181640, "mdate": 1702045181640, "content": {"title": "MEDITRON-70B: Scaling Medical Pretraining for Large Language Models", "abstract": "Large language models (LLMs) can potentially democratize access to medical knowledge. While many efforts have been made to harness and improve LLMs' medical knowledge and reasoning capacities, the resulting models are either closed-source (e.g., PaLM, GPT-4) or limited in scale (<= 13B parameters), which restricts their abilities. In this work, we improve access to large-scale medical LLMs by releasing MEDITRON: a suite of open-source LLMs with 7B and 70B parameters adapted to the medical domain. MEDITRON builds on Llama-2 (through our adaptation of Nvidia's Megatron-LM distributed trainer), and extends pretraining on a comprehensively curated medical corpus, including selected PubMed articles, abstracts, and internationally-recognized medical guidelines. Evaluations using four major medical benchmarks show significant performance gains over several state-of-the-art baselines before and after task-specific finetuning. Overall, MEDITRON achieves a 6% absolute performance gain over the best public baseline in its parameter class and 3% over the strongest baseline we finetuned from Llama-2. Compared to closed-source LLMs, MEDITRON-70B outperforms GPT-3.5 and Med-PaLM and is within 5% of GPT-4 and 10% of Med-PaLM-2. We release our code for curating the medical pretraining corpus and the MEDITRON model weights to drive open-source development of more capable medical LLMs."}}
{"id": "srBw1BRNKem", "cdate": 1672531200000, "mdate": 1681639530500, "content": {"title": "Learning Translation Quality Evaluation on Low Resource Languages from Large Language Models", "abstract": ""}}
{"id": "JsrvkgM8gO2", "cdate": 1663850184673, "mdate": null, "content": {"title": "Large Learning Rate Matters for Non-Convex Optimization", "abstract": "When training neural networks, it has been widely observed that a large step size is essential in stochastic gradient descent (SGD) for obtaining superior models. However, the effect of large step sizes on the success of SGD is not well understood theoretically. \nSeveral previous works have attributed this success to the stochastic noise present in SGD.  However, we show through a novel set of experiments that the stochastic noise is not sufficient to explain good non-convex training, and that instead the effect of a large learning rate itself is essential for obtaining best performance.\nWe demonstrate the same effects also in the noise-less case, i.e. for full-batch GD. We formally prove that GD with large step size---on certain non-convex function classes---follows a different trajectory than GD with a small step size, which can lead to convergence to a global minimum instead of a local one. \nFinally, we also demonstrate the difference in trajectories for small and large learning rates for real neural networks, again observing that large learning rates allow escaping from a local minimum, confirming this behavior is indeed relevant in practice."}}
{"id": "xv2TdpdjQC", "cdate": 1640995200000, "mdate": 1681639530504, "content": {"title": "Masked Training of Neural Networks with Partial Gradients", "abstract": ""}}
{"id": "dhkGrPKuULo", "cdate": 1640995200000, "mdate": 1652728960603, "content": {"title": "Characterizing & Finding Good Data Orderings for Fast Convergence of Sequential Gradient Methods", "abstract": "While SGD, which samples from the data with replacement is widely studied in theory, a variant called Random Reshuffling (RR) is more common in practice. RR iterates through random permutations of the dataset and has been shown to converge faster than SGD. When the order is chosen deterministically, a variant called incremental gradient descent (IG), the existing convergence bounds show improvement over SGD but are worse than RR. However, these bounds do not differentiate between a good and a bad ordering and hold for the worst choice of order. Meanwhile, in some cases, choosing the right order when using IG can lead to convergence faster than RR. In this work, we quantify the effect of order on convergence speed, obtaining convergence bounds based on the chosen sequence of permutations while also recovering previous results for RR. In addition, we show benefits of using structured shuffling when various levels of abstractions (e.g. tasks, classes, augmentations, etc.) exists in the dataset in theory and in practice. Finally, relying on our measure, we develop a greedy algorithm for choosing good orders during training, achieving superior performance (by more than 14 percent in accuracy) over RR."}}
{"id": "2uddIYIueXv", "cdate": 1640995200000, "mdate": 1681639530468, "content": {"title": "On Avoiding Local Minima Using Gradient Descent With Large Learning Rates", "abstract": ""}}
{"id": "N43DVxrjCw", "cdate": 1612102771386, "mdate": null, "content": {"title": "Reproducibility Report for \"On Warm-Starting Neural Network Training\"", "abstract": "Scope of Reproducibility\nWe reproduce the results of the paper \"On Warm-Starting Neural Network Training.\" In many real-world applications, the training data is not readily available and is accumulated over time. As training models from scratch is a time- consuming task, it is preferred to use warm-starting, i.e., using the already existing models as the starting point to obtain faster convergence. This paper investigates the effect of warm-starting on the final model\u2019s performance. It identifies a noticeable gap between warm-started and randomly-initialized models, hereafter referenced as the warm-starting gap. Furthermore, they provide a solution to mitigate this side-effect. In addition to reproducing the original paper\u2019s results, we propose an alternative solution and assess its effectiveness.\n\nMethodology\nWe reproduced almost every figure and table in the main text and some of those in the appendix. We used our implementation to produce these results. In case of a mismatch of the results, we also investigated the cause and proposed possible explanations. We mainly used GPUs to train our models using infrastructure offered by public clouds and those that were available to us privately.\n\nResults\nMost of our results closely match the reported results in the original paper. Therefore, we confirm that the warm-starting gap exists in certain settings and that the Shrink-Perturb method successfully reduces or eliminates this gap. However, in some cases, we were not able to completely reproduce their results. By investigating the root of such mismatches, we provide another solution to avoid this gap. In particular, we show that data augmentation also helps to reduce the warm-starting gap.\n\nWhat was easy\nThe experiments described in the paper were based on regular training of neural networks on a portion of widely-used datasets, possibly from a pre-trained model. Therefore implementing each experiment was relatively easy to do. Furthermore, since many of the parameters were reported in the original paper, we did not need much tuning in most experiments. Finally, it is straightforward to implement and use the proposed solution.\n\nWhat was difficult\nThough implementing each experiment is relatively simple, the numerosity of experiments proved to be slightly challenging. In particular, each of the online experiments in the original setting requires training a deep network to convergence more than 30 times. In these cases, we sometimes changed the settings, sacrificing granularity to reduce computation time. However, these changes did not affect the interpretability of the final results.\n\nCommunication with original authors\nWe briefly communicated with the authors to clarify the experiments\u2019 details, such as the convergence conditions."}}
{"id": "7XUisT2trUP", "cdate": 1609459200000, "mdate": 1652728960595, "content": {"title": "Critical Parameters for Scalable Distributed Learning with Large Batches and Asynchronous Updates", "abstract": "It has been experimentally observed that the efficiency of distributed training with stochastic gradient (SGD) depends decisively on the batch size and\u2014in asynchronous implementations\u2014on the gradient staleness. Especially, it has been observed that the speedup saturates beyond a certain batch size and/or when the delays grow too large. We identify a data-dependent parameter that explains the speedup saturation in both these settings. Our comprehensive theoretical analysis, for strongly convex, convex and non-convex settings, unifies and generalized prior work directions that often focused on only one of these two aspects. In particular, our approach allows us to derive improved speedup results under frequently considered sparsity assumptions. Our insights give rise to theoretically based guidelines on how the learning rates can be adjusted in practice. We show that our results are tight and illustrate key findings in numerical experiments."}}
{"id": "-dtM5npv_cL", "cdate": 1577836800000, "mdate": null, "content": {"title": "The Splay-List: A Distribution-Adaptive Concurrent Skip-List", "abstract": "The design and implementation of efficient concurrent data structures has seen significant attention. However, most of this work has focused on concurrent data structures providing good worst-case guarantees. In real workloads, objects are often accessed at different rates, since access distributions may be non-uniform. Efficient distribution-adaptive data structures are known in the sequential case, e.g. the splay-trees; however, they often are hard to translate efficiently in the concurrent case. In this paper, we investigate distribution-adaptive concurrent data structures, and propose a new design called the splay-list. At a high level, the splay-list is similar to a standard skip-list, with the key distinction that the height of each element adapts dynamically to its access rate: popular elements \"move up,\" whereas rarely-accessed elements decrease in height. We show that the splay-list provides order-optimal amortized complexity bounds for a subset of operations, while being amenable to efficient concurrent implementation. Experimental results show that the splay-list can leverage distribution-adaptivity to improve on the performance of classic concurrent designs, and can outperform the only previously-known distribution-adaptive design in certain settings."}}
{"id": "Bye-sxHFwB", "cdate": 1569439897085, "mdate": null, "content": {"title": "A Gradient-Based Approach to Neural Networks Structure Learning", "abstract": "Designing the architecture of deep neural networks (DNNs) requires human expertise and is a cumbersome task. One approach to automatize this task has been considering DNN architecture parameters such as the number of layers, the number of neurons per layer, or the activation function of each layer as  hyper-parameters, and using an external method for optimizing it. Here we propose a novel neural network model, called Farfalle Neural Network, in which important architecture features such as the number of neurons in each layer and the wiring among the neurons are automatically learned during the training process. We show that the proposed model can replace a stack of dense layers, which is used as a part of many DNN architectures. It can achieve higher accuracy using significantly fewer parameters.  "}}
