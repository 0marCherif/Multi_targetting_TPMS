{"id": "QbYS4dXH0dD", "cdate": 1621629766478, "mdate": null, "content": {"title": "Privately Learning Mixtures of Axis-Aligned Gaussians", "abstract": "We consider the problem of learning multivariate Gaussians under the constraint of approximate differential privacy. We prove that $\\widetilde{O}(k^2 d \\log^{3/2}(1/\\delta) / \\alpha^2 \\varepsilon)$ samples are sufficient to learn a mixture of $k$ axis-aligned Gaussians in $\\mathbb{R}^d$ to within total variation distance $\\alpha$ while satisfying $(\\varepsilon, \\delta)$-differential privacy. This is the first result for privately learning mixtures of unbounded axis-aligned (or even unbounded univariate) Gaussians. If the covariance matrices of each of the Gaussians is the identity matrix, we show that $\\widetilde{O}(kd/\\alpha^2 + kd \\log(1/\\delta) / \\alpha \\varepsilon)$ samples are sufficient.\nTo prove our results, we design a new technique for privately learning mixture distributions.  A class of distributions $\\mathcal{F}$ is said to be list-decodable if there is an algorithm that, given \"heavily corrupted\" samples from $f \\in \\mathcal{F}$, outputs a list of distributions one of which approximates $f$. We show that if $\\mathcal{F}$ is privately list-decodable then we can learn mixtures of distributions in $\\mathcal{F}$. Finally, we show axis-aligned Gaussian distributions are privately list-decodable, thereby proving mixtures of such distributions are privately learnable.\n\n"}}
{"id": "rkluJ2R9KQ", "cdate": 1538087903798, "mdate": null, "content": {"title": "A new dog learns old tricks:  RL finds classic optimization algorithms", "abstract": "This paper introduces a novel framework for learning algorithms to solve online combinatorial optimization problems. Towards this goal, we introduce a number of key ideas from traditional algorithms and complexity theory. First, we draw a new connection between primal-dual methods and reinforcement learning. Next, we introduce the concept of adversarial distributions (universal and high-entropy training sets), which are distributions that encourage the learner to find algorithms that work well in the worst case. We test our new ideas on a number of optimization problem such as the AdWords problem, the online knapsack problem, and the secretary problem. Our results indicate that the models have learned behaviours that are consistent with the traditional optimal algorithms for these problems."}}
{"id": "Skb-1DZOWB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Nearly tight sample complexity bounds for learning mixtures of Gaussians via sample compression schemes", "abstract": "We prove that \u03f4(k d^2 / \u03b5^2) samples are necessary and sufficient for learning a mixture of k Gaussians in R^d, up to error \u03b5 in total variation distance. This improves both the known upper bounds and lower bounds for this problem. For mixtures of axis-aligned Gaussians, we show that O(k d / \u03b5^2) samples suffice, matching a known lower bound. The upper bound is based on a novel technique for distribution learning based on a notion of sample compression. Any class of distributions that allows such a sample compression scheme can also be learned with few samples. Moreover, if a class of distributions has such a compression scheme, then so do the classes of products and mixtures of those distributions. The core of our main result is showing that the class of Gaussians in R^d has an efficient sample compression."}}
