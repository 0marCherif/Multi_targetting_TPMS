{"id": "KCoSUUk3XT", "cdate": 1684271140687, "mdate": 1684271140687, "content": {"title": "SWEM: Towards Real-Time Video Object Segmentation with Sequential Weighted Expectation-Maximization", "abstract": "Matching-based methods, especially those based on space-time memory, are significantly ahead of other solutions in semi-supervised video object segmentation (VOS). However, continuously growing and redundant template features lead to an inefficient inference. To alleviate this, we propose a novel Sequential Weighted Expectation-Maximization (SWEM) network to greatly reduce the redundancy of memory features. Different from the previous methods which only detect feature redundancy between frames, SWEM merges both intra-frame and inter-frame similar features by leveraging the sequential weighted EM algorithm. Further, adaptive weights for frame features endow SWEM with the flexibility to represent hard samples, improving the discrimination of templates. Besides, the proposed method maintains a fixed number of template features in memory, which ensures the stable inference complexity of the VOS system. Extensive experiments on commonly used DAVIS and YouTube-VOS datasets verify the high efficiency (36 FPS) and high performance (84.3\\% J&F on DAVIS 2017 validation dataset) of SWEM. Code is available at: https://github.com/lmm077/SWEM.\n"}}
{"id": "TXI8ysdW9Us", "cdate": 1684270681362, "mdate": 1684270681362, "content": {"title": "Dynamixer: a vision MLP architecture with dynamic mixing", "abstract": "Recently, MLP-like vision models have achieved promising performances on mainstream visual recognition tasks. In contrast with vision transformers and CNNs, the success of MLP-like models shows that simple information fusion operations among tokens and channels can yield a good representation power for deep recognition models. However, existing MLP-like models fuse tokens through static fusion operations, lacking adaptability to the contents of the tokens to be mixed. Thus, customary information fusion procedures are not effective enough. To this end, this paper presents an efficient MLP-like network architecture, dubbed DynaMixer, resorting to dynamic information fusion. Critically, we propose a procedure, on which the DynaMixer model relies, to dynamically generate mixing matrices by leveraging the contents of all the tokens to be mixed. To reduce the time complexity and improve the robustness, a dimensionality reduction technique and a multi-segment fusion mechanism are adopted. Our proposed DynaMixer model (97M parameters) achieves 84.3% top-1 accuracy on the ImageNet-1K dataset without extra training data, performing favorably against the state-of-the-art vision MLP models. When the number of parameters is reduced to 26M, it still achieves 82.7% top-1 accuracy, surpassing the existing MLP-like models with a similar capacity. The code is available at https: //github.com/ziyuwwang/DynaMixer."}}
{"id": "Esihq8901n", "cdate": 1677628800000, "mdate": 1681532043097, "content": {"title": "EDFace-Celeb-1 M: Benchmarking Face Hallucination With a Million-Scale Dataset", "abstract": ""}}
{"id": "eCYsoBzVLZ", "cdate": 1672531200000, "mdate": 1681532043455, "content": {"title": "Enhanced Spatio-Temporal Interaction Learning for Video Deraining: Faster and Better", "abstract": ""}}
{"id": "MqmSwof7yt", "cdate": 1672531200000, "mdate": 1681532043468, "content": {"title": "Robust Pose Transfer With Dynamic Details Using Neural Video Rendering", "abstract": ""}}
{"id": "CF8nth9UkqU", "cdate": 1672531200000, "mdate": 1681532043928, "content": {"title": "CrossFormer++: A Versatile Vision Transformer Hinging on Cross-scale Attention", "abstract": ""}}
{"id": "dPWR2Prbj13", "cdate": 1668741999890, "mdate": 1668741999890, "content": {"title": "LARNet: Lie Algebra Residual Network for Face Recognition", "abstract": "Face recognition is an important yet challenging problem in computer vision. A major challenge in practical face recognition applications lies in significant variations between profile and frontal faces. Traditional techniques address this challenge either by synthesizing frontal faces or by pose invariant learning. In this paper, we propose a novel method with Lie algebra theory to explore how face rotation in the 3D space affects the deep feature generation process of convolutional neural networks (CNNs). We prove that face rotation in the image space is equivalent to an additive residual component in the feature space of CNNs, which is determined solely by the rotation. Based on this theoretical finding, we further design a Lie Algebraic Residual Network (LARNet) for tackling pose robust face recognition. Our LARNet consists of a residual subnet for decoding rotation information from input face images, and a gating subnet to learn rotation magnitude for controlling the strength of the residual component contributing to the feature learning process. Comprehensive experimental evaluations on both frontal-profile face datasets and general face recognition datasets convincingly demonstrate that our method consistently outperforms the state-of-the-art ones."}}
{"id": "KJsKK8uvpX", "cdate": 1668511052174, "mdate": 1668511052174, "content": {"title": "Towards Photo-Realistic Virtual Try-On by Adaptively Generating-Preserving Image Content", "abstract": "Image visual try-on aims at transferring a target clothes image onto a reference person, and has become a hot topic in recent years. Prior arts usually focus on preserving the character of a clothes image (e.g. texture, logo, embroidery) when warping it to arbitrary human pose. However, it remains a big challenge to generate photo-realistic try-on images when large occlusions and human poses are presented in the reference person. To address this issue, we propose a novel visual try-on network, namely Adaptive Content Generating and Preserving Network (ACGPN). In particular, ACGPN first predicts semantic layout of the reference image that will be changed after try-on (e.g.long sleeve shirt-arm, arm-jacket), and then determines whether its image content needs to be generated or preserved according to the predicted semantic layout, leading to photo-realistic try-on and rich clothes details. ACGPN generally involves three major modules. First, a semantic layout generation module utilizes semantic segmentation of the reference image to progressively predict the desired semantic layout after try-on. Second, a clothes warping module warps clothes image according to the generated semantic layout, where a second-order difference constraint is introduced to stabilize the warping process during training.Third, an inpainting module for content fusion integrates all information (e.g. reference image, semantic layout, warped clothes) to adaptively produce each semantic part of human body. In comparison to the state-of-the-art methods, ACGPN can generate photo-realistic images with much better perceptual quality and richer fine-details."}}
{"id": "1jXXkkynjw", "cdate": 1668478672275, "mdate": 1668478672275, "content": {"title": "ArtFlow: Unbiased Image Style Transfer via Reversible Neural Flows", "abstract": "Universal style transfer retains styles from reference images in content images. While existing methods have achieved state-of-the-art style transfer performance, they are not aware of the content leak phenomenon that the im- age content may corrupt after several rounds of stylization process. In this paper, we propose ArtFlow to prevent content leak during universal style transfer. ArtFlow consists of reversible neural flows and an unbiased feature transfer module. It supports both forward and backward inferences and operates in a projection-transfer-reversion scheme. The forward inference projects input images into deep features, while the backward inference remaps deep features back to input images in a lossless and unbiased way. Extensive experiments demonstrate that ArtFlow achieves comparable performance to state-of-the-art style transfer methods while avoiding content leak."}}
{"id": "HHcZrIrcUIe", "cdate": 1667368355680, "mdate": 1667368355680, "content": {"title": "Generalizing Face Forgery Detection with High-frequency Features", "abstract": "Current face forgery detection methods achieve high accuracy under the within-database scenario where training and testing forgeries are synthesized by the same algorithm. However, few of them gain satisfying performance under the cross-database scenario where training and testing forgeries are synthesized by different algorithms. In this paper, we find that current CNN-based detectors tend to overfit\nto method-specific color textures and thus fail to generalize. Observing that image noises remove color textures and expose discrepancies between authentic and tampered regions, we propose to utilize the high-frequency noises for face forgery detection. We carefully devise three functional modules to take full advantage of the high-frequency features. The first is the multi-scale high-frequency feature extraction module that extracts high-frequency noises at multiple scales and composes a novel modality. The second is the residual-guided spatial attention module that guides the low-level RGB feature extractor to concentrate more on forgery traces from a new perspective. The last is the crossmodality attention module that leverages the correlation between the two complementary modalities to promote feature learning for each other. Comprehensive evaluations on several benchmark databases corroborate the superior generalization performance of our proposed method."}}
