{"id": "Q5y4gonHTK", "cdate": 1640995200000, "mdate": 1681661710842, "content": {"title": "Federated Continual Learning for Text Classification via Selective Inter-client Transfer", "abstract": ""}}
{"id": "VGkqySFfD8i", "cdate": 1609459200000, "mdate": null, "content": {"title": "Multi-source Neural Topic Modeling in Multi-view Embedding Spaces", "abstract": "Though word embeddings and topics are complementary representations, several past works have only used pretrained word embeddings in (neural) topic modeling to address data sparsity in short-text or small collection of documents. This work presents a novel neural topic modeling framework using multi-view embedding spaces: (1) pretrained topic-embeddings, and (2) pretrained word-embeddings (context insensitive from Glove and context-sensitive from BERT models) jointly from one or many sources to improve topic quality and better deal with polysemy. In doing so, we first build respective pools of pretrained topic (i.e., TopicPool) and word embeddings (i.e., WordPool). We then identify one or more relevant source domain(s) and transfer knowledge to guide meaningful learning in the sparse target domain. Within neural topic modeling, we quantify the quality of topics and document representations via generalization (perplexity), interpretability (topic coherence) and information retrieval (IR) using short-text, long-text, small and large document collections from news and medical domains. Introducing the multi-source multi-view embedding spaces, we have shown state-of-the-art neural topic modeling using 6 source (high-resource) and 5 target (low-resource) corpora."}}
{"id": "MVOOAQ07usr", "cdate": 1609459200000, "mdate": 1642685262758, "content": {"title": "Multi-source Neural Topic Modeling in Multi-view Embedding Spaces", "abstract": "Pankaj Gupta, Yatin Chaudhary, Hinrich Sch\u00fctze. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021."}}
{"id": "e74tb5qyrTG", "cdate": 1577836800000, "mdate": null, "content": {"title": "TopicBERT for Energy Efficient Document Classification", "abstract": "Prior research notes that BERT\u2019s computational cost grows quadratically with sequence length thus leading to longer training times, higher GPU memory constraints and carbon emissions. While recent work seeks to address these scalability issues at pre-training, these issues are also prominent in fine-tuning especially for long sequence tasks like document classification. Our work thus focuses on optimizing the computational cost of fine-tuning for document classification. We achieve this by complementary learning of both topic and language models in a unified framework, named TopicBERT. This significantly reduces the number of self-attention operations \u2013 a main performance bottleneck. Consequently, our model achieves a 1.4x ( 40%) speedup with 40% reduction in CO2 emission while retaining 99.9% performance over 5 datasets."}}
{"id": "YCIu6zHaenP", "cdate": 1577836800000, "mdate": null, "content": {"title": "Neural Topic Modeling with Continual Lifelong Learning", "abstract": "Lifelong learning has recently attracted attention in building machine learning systems that continually accumulate and transfer knowledge to help future learning. Unsupervised topic modeling has b..."}}
{"id": "Nmf0NsEt-jf", "cdate": 1577836800000, "mdate": null, "content": {"title": "Explainable and Discourse Topic-aware Neural Language Understanding", "abstract": "Marrying topic models and language models exposes language understanding to a broader source of document-level context beyond sentences via topics. While introducing topical semantics in language m..."}}
{"id": "ByxODxHYwB", "cdate": 1569439840075, "mdate": null, "content": {"title": "Multi-source Multi-view Transfer Learning in Neural Topic Modeling with Pretrained Topic and Word Embeddings", "abstract": "Though word embeddings and topics are complementary representations, several\npast works have only used pretrained word embeddings in (neural) topic modeling\nto address data sparsity problem in short text or small collection of documents.\nHowever, no prior work has employed (pretrained latent) topics in transfer learning\nparadigm. In this paper, we propose a framework to perform transfer learning\nin neural topic modeling using (1) pretrained (latent) topics obtained from a large\nsource corpus, and (2) pretrained word and topic embeddings jointly (i.e., multiview)\nin order to improve topic quality, better deal with polysemy and data sparsity\nissues in a target corpus. In doing so, we first accumulate topics and word representations\nfrom one or many source corpora to build respective pools of pretrained\ntopic (i.e., TopicPool) and word embeddings (i.e., WordPool). Then, we identify\none or multiple relevant source domain(s) and take advantage of corresponding\ntopics and word features via the respective pools to guide meaningful learning\nin the sparse target domain. We quantify the quality of topic and document representations\nvia generalization (perplexity), interpretability (topic coherence) and\ninformation retrieval (IR) using short-text, long-text, small and large document\ncollections from news and medical domains. We have demonstrated the state-ofthe-\nart results on topic modeling with the proposed transfer learning approaches."}}
{"id": "sZ04vatMTG", "cdate": 1546300800000, "mdate": 1642685263027, "content": {"title": "Texttovec: Deep Contextualized Neural autoregressive Topic Models of Language with Distributed Compositional Prior", "abstract": "We address two challenges of probabilistic topic modelling in order to better estimate the probability of a word in a given context, i.e., P(wordjcontext) : (1) No Language Structure in Context: Probabilistic topic models ignore word order by summarizing a given context as a \u201cbag-of-word\u201d and consequently the semantics of words in the context is lost. In this work, we incorporate language structure by combining a neural autoregressive topic model (TM) with a LSTM based language model (LSTM-LM) in a single probabilistic framework. The LSTM-LM learns a vector-space representation of each word by accounting for word order in local collocation patterns, while the TM simultaneously learns a latent representation from the entire document. In addition, the LSTM-LM models complex characteristics of language (e.g., syntax and semantics), while the TM discovers the underlying thematic structure in a collection of documents. We unite two complementary paradigms of learning the meaning of word occurrences by combining a topic model and a language model in a unified probabilistic framework, named as ctx-DocNADE. (2) Limited Context and/or Smaller training corpus of documents: In settings with a small number of word occurrences (i.e., lack of context) in short text or data sparsity in a corpus of few documents, the application of TMs is challenging. We address this challenge by incorporating external knowledge into neural autoregressive topic models via a language modelling approach: we use word embeddings as input of a LSTM-LM with the aim to improve the wordtopic mapping on a smaller and/or short-text corpus. The proposed DocNADE extension is named as ctx-DocNADEe. We present novel neural autoregressive topic model variants coupled with neural language models and embeddings priors that consistently outperform state-of-theart generative topic models in terms of generalization (perplexity), interpretability (topic coherence) and applicability (retrieval and classification) over 6 long-text and 8 short-text datasets from diverse domains."}}
{"id": "jRVSdOQRmkg", "cdate": 1546300800000, "mdate": null, "content": {"title": "Document Informed Neural Autoregressive Topic Models with Distributional Prior", "abstract": "We address two challenges in topic models: (1) Context information around words helps in determining their actual meaning, e.g., \u201cnetworks\u201d used in the contexts artificial neural networks vs. biological neuron networks. Generative topic models infer topic-word distributions, taking no or only little context into account. Here, we extend a neural autoregressive topic model to exploit the full context information around words in a document in a language modeling fashion. The proposed model is named as iDocNADE. (2) Due to the small number of word occurrences (i.e., lack of context) in short text and data sparsity in a corpus of few documents, the application of topic models is challenging on such texts. Therefore, we propose a simple and efficient way of incorporating external knowledge into neural autoregressive topic models: we use embeddings as a distributional prior. The proposed variants are named as DocNADEe and iDocNADEe. We present novel neural autoregressive topic model variants that consistently outperform state-of-the-art generative topic models in terms of generalization, interpretability (topic coherence) and applicability (retrieval and classification) over 7 long-text and 8 short-text datasets from diverse domains."}}
{"id": "7jUSObcWer3", "cdate": 1546300800000, "mdate": null, "content": {"title": "BioNLP-OST 2019 RDoC Tasks: Multi-grain Neural Relevance Ranking Using Topics and Attention Based Query-Document-Sentence Interactions", "abstract": "This paper presents our system details and results of participation in the RDoC Tasks of BioNLP-OST 2019. Research Domain Criteria (RDoC) construct is a multi-dimensional and broad framework to describe mental health disorders by combining knowledge from genomics to behaviour. Non-availability of RDoC labelled dataset and tedious labelling process hinders the use of RDoC framework to reach its full potential in Biomedical research community and Healthcare industry. Therefore, Task-1 aims at retrieval and ranking of PubMed abstracts relevant to a given RDoC construct and Task-2 aims at extraction of the most relevant sentence from a given PubMed abstract. We investigate (1) attention based supervised neural topic model and SVM for retrieval and ranking of PubMed abstracts and, further utilize BM25 and other relevance measures for re-ranking, (2) supervised and unsupervised sentence ranking models utilizing multi-view representations comprising of query-aware attention-based sentence representation (QAR), bag-of-words (BoW) and TF-IDF. Our best systems achieved 1st rank and scored 0.86 mAP and 0.58 macro average accuracy in Task-1 and Task-2 respectively."}}
