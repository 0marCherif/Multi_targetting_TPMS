{"id": "BnM5Aaac6iZ", "cdate": 1596126271623, "mdate": null, "content": {"title": "A Random Algorithm for Semidefinite Programming Problems", "abstract": "We introduce a first-order method for solving semidefinite programming problems. This method has low computational complexity per iteration and is easy to implement. In each iteration, it alternates in two steps: gradient-descent to optimize the objective function, and random projection to reduce the infeasibility of the constraints. Due to its low computational complexity per iteration, it can be scaled to\nlarge problems. We also prove the algorithm\u2019s convergence and demonstrate its performance in numerical examples."}}
{"id": "rm07K2m5DXr", "cdate": 1596126193608, "mdate": null, "content": {"title": "Online Control Basis Selection by a Regularized Actor Critic Algorithm", "abstract": "Policy gradient algorithms are useful reinforcement learning methods which optimize a control policy by performing stochastic gradient descent with respect to controller parameters. In this paper, we extend actor-critic algorithms by adding an $\\ell_1$ norm regularization on the actor part, which makes our algorithm automatically select and optimize the useful controller basis functions. Our method is closely related to existing approaches to sparse controller design and actuator selection, but in contrast to these, our approach runs online\nand does not require a plant model. In order to utilize $\\ell_1$ regularization online, the actor updates are extended to include an\niterative soft-thresholding step. Convergence of the algorithm is proved using methods from stochastic approximation. The\neffectiveness of our algorithm for control basis and actuator selection is demonstrated on numerical examples."}}
{"id": "_ScVFRQvq5s", "cdate": 1596125884288, "mdate": null, "content": {"title": "Trading-Off Static and Dynamic Regret in Online Least-Squares and Beyond", "abstract": "Recursive least-squares algorithms often use forgetting factors as a heuristic to adapt to nonstationary data streams. The first contribution of this paper rigorously characterizes the effect of forgetting factors for a class of online Newton algorithms. For exp-concave and strongly convex objectives, the algorithms achieve the dynamic regret of $\\max\\{O(\\log T), O(\\sqrt{T V})\\}$, where V is a\nbound on the path length of the comparison sequence. In particular, we show how classic recursive\nleast-squares with a forgetting factor achieves this dynamic regret bound. By varying V, we obtain a trade-off between static and dynamic regret. In order to obtain more computationally efficient algorithms, our second contribution is a novel gradient descent step size rule for strongly convex functions. Our gradient descent rule recovers the order optimal dynamic regret bounds described above. For smooth problems, we can also obtain static regret of $O(T^{1\u2212\\beta})$ and dynamic regret of $O(T^{\\beta}V^*)$, where $\\beta \\in (0, 1)$ and $V^*$ is the path length of the sequence of minimizers. By varying $\\beta$, we obtain a trade-off between static and dynamic regret."}}
{"id": "HyV0R9W_-r", "cdate": 1546300800000, "mdate": null, "content": {"title": "Online Adaptive Principal Component Analysis and Its extensions", "abstract": "We propose algorithms for online principal component analysis (PCA) and variance minimization for adaptive settings. Previous literature has focused on upper bounding the static adversarial regret,..."}}
{"id": "HkZesdb_bH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Online convex optimization for cumulative constraints", "abstract": "We propose the algorithms for online convex optimization which lead to cumulative squared constraint violations of the form $\\sum\\limits_{t=1}^T\\big([g(x_t)]_+\\big)^2=O(T^{1-\\beta})$, where $\\beta\\in(0,1)$. Previous literature has focused on long-term constraints of the form $\\sum\\limits_{t=1}^Tg(x_t)$. There, strictly feasible solutions can cancel out the effects of violated constraints. In contrast, the new form heavily penalizes large constraint violations and cancellation effects cannot occur. Furthermore, useful bounds on the single step constraint violation $[g(x_t)]_+$ are derived. For convex objectives, our regret bounds generalize existing bounds, and for strongly convex objectives we give improved regret bounds. In numerical experiments, we show that our algorithm closely follows the constraint boundary leading to low cumulative violation."}}
{"id": "SJ-im3-O-B", "cdate": 1388534400000, "mdate": null, "content": {"title": "Optimal Mean Robust Principal Component Analysis", "abstract": "Dimensionality reduction techniques extract low-dimensional structure from high-dimensional data and are widespread in machine learning research. In practice, due to lacking labeled data, the unsup..."}}
