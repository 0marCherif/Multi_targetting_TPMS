{"id": "mCjfJYBGnTT", "cdate": 1672531200000, "mdate": 1681691956565, "content": {"title": "A0.81 mm2 740\u03bcW Real-Time Speech Enhancement Processor Using Multiplier-Less PE Arrays for Hearing Aids in 28nm CMOS", "abstract": "Speech enhancement (SE) is a task to improve voice quality and intelligibility by removing noise from the audio, which is widely adopted in hearing assistive devices. Hearing aids are generally worn in or behind the ear, requiring real-time processing with a limited power budget. Deep learning-based algorithms provide excellent SE quality, but their large model size and high computational complexity make them unsuitable for wearable hearing assistive devices. Recent hardware-oriented works mitigate these issues through algorithm and hardware optimization [1\u20133]. Nonetheless, they exhibit inferior SE performance relative to state-of-the-art models or rely on large neural network models, limiting overall processing efficiency. This paper presents an end-to-end SE system that delivers high-quality SE with low power consumption and small area, while meeting real-time processing constraints. Our main contributions are: 1) an importance-aware neural network optimization method that significantly reduces computational costs, while maintaining enhancement quality, 2) a reconfigurable processing element (PE) for efficiently processing both the coordinate rotation digital computer (CORDIC) algorithm and neural network layers, and 3) PE input routing and weight mapping schemes to minimize processing latency by enhancing PE utilization. Based on these design techniques, our processor fabricated in a <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$28\\mathsf{nm}$</tex> CMOS process fulfills real-time speech enhancement by processing each frame within <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$8\\mathsf{ms}$</tex> while consuming only <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$740\\mu \\mathsf{W}$</tex> . Also, the design achieves the highest objective evaluation score compared to previous SE processors."}}
{"id": "YgC62m4CY3r", "cdate": 1663850102027, "mdate": null, "content": {"title": "Learning with Auxiliary Activation for Memory-Efficient Training", "abstract": "While deep learning has achieved great success in various fields, a large amount of memory is necessary to train deep neural networks, which hinders the development of massive state-of-the-art models. The reason is the conventional learning rule, backpropagation, should temporarily store input activations of all the layers in the network. To overcome this, recent studies suggested various memory-efficient implementations of backpropagation. However, those approaches incur computational overhead due to the recomputation of activations, slowing down neural network training. In this work, we propose a new learning rule which significantly reduces memory requirements while closely matching the performance of backpropagation. The algorithm combines auxiliary activation with output activation during forward propagation, while only auxiliary activation is used during backward propagation instead of actual input activation to reduce the amount of data to be temporarily stored. We mathematically show that our learning rule can reliably train the networks whose loss landscape is convex if the auxiliary activation satisfies certain conditions. Based on this observation, we suggest candidates of auxiliary activation that satisfy those conditions. Experimental results confirm that the proposed learning rule achieves competitive performance compared to backpropagation in various models such as ResNet, Transformer, BERT, ViT, and MLP-Mixer."}}
{"id": "zTvLO9lWrf", "cdate": 1640995200000, "mdate": 1681691956579, "content": {"title": "A 28nm 1.644TFLOPS/W Floating-Point Computation SRAM Macro with Variable Precision for Deep Neural Network Inference and Training", "abstract": "This paper presents a digital compute-in-memory (CIM) macro for accelerating deep neural networks. The macro provides high-precision computation required for training deep neural networks and running state-of-the-art models by supporting floating-point MAC operations. Additionally, the design supports variable computation precision, enabling optimized processing for different models and tasks. The design achieves 1.644TFLOPS/W energy efficiency and 57.9GFLOPS/mm <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup> computation density while supporting a wide range of floating-point data formats and computation precisions."}}
{"id": "lQpNMcRwCt", "cdate": 1640995200000, "mdate": 1681691956511, "content": {"title": "An In-Memory Computing SRAM Macro for Memory-Augmented Neural Network", "abstract": "In-Memory Computing (IMC) has been widely studied to mitigate data transfer bottlenecks in von Neumann architectures. Recently proposed IMC circuit topologies dramatically reduce data transfer requirements by performing various operations such as Multiply-Accumulate (MAC) inside the memory. In this brief, we present an SRAM macro designed for accelerating Memory-Augmented Neural Network (MANN). We first propose algorithmic optimizations for a few-shot learning algorithm employing MANN for efficient hardware implementation. Then, we present an SRAM macro that efficiently accelerates the algorithm by realizing key operations such as L1 distance calculation and Winner-Take-All (WTA) operation through mixed-signal computation circuits. Fabricated in 40nm LP CMOS technology, the design demonstrates 27.7 TOPS/W maximum energy efficiency, while achieving 93.40% and 98.28% classification accuracy for 5-way 1-shot and 5-way 5-shot learning on the Omniglot dataset, which closely matches the accuracy of the baseline algorithm."}}
{"id": "SyJb-jaunzP", "cdate": 1640995200000, "mdate": 1681691956490, "content": {"title": "A 270-mA Self-Calibrating-Clocked Output-Capacitor-Free LDO With 0.15-1.15V Output Range and 0.183-fs FoM", "abstract": "This article proposes a fully integrated output-capacitor-free low-dropout regulator (LDO) for mobile applications. To overcome the limited output voltage range of typical analog LDOs, our design uses a rail-to-rail voltage-difference-to-time-converter (VDTC) and a charge pump (CP) to achieve a wide output range. Using a self-calibrating clock generator (SCCG) removes the need for an external clock source and adaptively tunes the clock frequency, enabling fast transient responses while minimizing quiescent current. A tunable undershoot compensator (TUC) mitigates voltage droop by detecting the drop in the output voltage due to a sharp increase in load current and compensating the output voltage immediately. The proposed LDO is fabricated in a 65-nm low power (LP) CMOS process and demonstrates a maximum load current capacity of 270 mA. The input and output voltage ranges of the LDO are 0.5\u20131.2 and 0.15\u20131.15 V, respectively, with 12.7- <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\mu \\text{A}$ </tex-math></inline-formula> quiescent current and 99.99% peak current efficiency. The measured undershoot and settling time are 150 mV and 100 ns at a slew rate of 200 mA/3 ns, respectively, achieving a figure of merit (FoM) of 0.183 fs."}}
{"id": "SEIU_kDSCdT", "cdate": 1640995200000, "mdate": 1681691956544, "content": {"title": "A Neural Network Training Processor With 8-Bit Shared Exponent Bias Floating Point and Multiple-Way Fused Multiply-Add Trees", "abstract": "Recent advances in deep neural networks (DNNs) and machine learning algorithms have induced the demand for services based on machine learning algorithms that require a large number of computations, and specialized hardware ranging from accelerators for data centers to on-device computing systems have been introduced. Low-precision math such as 8-bit integers have been used in neural networks for energy-efficient neural network inference, but training with low-precision numbers without performance degradation have remained to be a challenge. To overcome this challenge, this article presents an 8-bit floating-point neural network training processor for state-of-the-art non-sparse neural networks. As na\u00efve 8-bit floating-point numbers are insufficient for training DNNs robustly, two additional methods are introduced to ensure high-performance DNN training. First, a novel numeric system which we dub as 8-bit floating point with shared exponent bias (FP8-SEB) is introduced. Moreover, multiple-way fused multiply-add (FMA) trees are used in FP8-SEB\u2019s hardware implementation to ensure higher numerical precision and reduced energy. FP8-SEB format combined with multiple-way FMA trees is evaluated under various scenarios to show a trained-from-scratch performance that is close to or even surpasses that of current networks trained with full-precision (FP32). Our silicon-verified DNN training processor utilizes 24-way FMA trees implemented with FP8-SEB math and flexible 2-D routing schemes to show <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$2.48\\times $ </tex-math></inline-formula> higher energy efficiency than prior low-power neural network training processors and <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$78.1\\times $ </tex-math></inline-formula> lower energy than standard GPUs."}}
{"id": "RPorVkHRAk", "cdate": 1640995200000, "mdate": 1681691956550, "content": {"title": "A low power neural network training processor with 8-bit floating point with a shared exponent bias and fused multiply add trees", "abstract": "This demonstration showcases a neural network training processor implemented in silicon through 40nm LPCMOS technology. Based on custom 8-bit floating point and efficient tree-based processing schemes and dataflow, we achieve 2.48\u00d7 higher energy efficiency than a prior low-power neural network training processor."}}
{"id": "Lu8IkCpi6f", "cdate": 1640995200000, "mdate": 1681691956506, "content": {"title": "An Automatic Circuit Design Framework for Level Shifter Circuits", "abstract": "Although design automation is a key enabler of modern large-scale digital systems, automating the transistor-level circuit design process still remains a challenge. Some recent works suggest that deep learning algorithms could be adopted to find optimal transistor dimensions in relatively small circuitry such as analog amplifiers. However, those approaches are not capable of exploring different circuit structures to meet the given design constraints. In this work, we propose an automatic circuit design framework that can generate practical circuit structures from scratch as well as optimize the size of each transistor, considering performance and reliability. We employ the framework to design level shifter circuits, and the experimental results show that the framework produces novel level shifter circuit topologies and the automatically optimized designs achieve <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$2.8\\times $ </tex-math></inline-formula> \u2013 <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$5.3\\times $ </tex-math></inline-formula> lower power-delay product (PDP) than prior arts designed by human experts."}}
{"id": "-WJN1HtDWgc", "cdate": 1640995200000, "mdate": 1681691956528, "content": {"title": "Toward Efficient Low-Precision Training: Data Format Optimization and Hysteresis Quantization", "abstract": "As the complexity and size of deep neural networks continue to increase, low-precision training has been extensively studied in the last few years to reduce hardware overhead. Training performance is largely affected by the numeric formats representing different values in low-precision training, but finding an optimal format typically requires numerous training runs, which is a very time-consuming process. In this paper, we propose a method to efficiently find an optimal format for activations and errors without actual training. We employ this method to determine an 8-bit format suitable for training various models. In addition, we propose hysteresis quantization to suppress undesired fluctuation in quantized weights during training. This scheme enables deeply quantized training using 4-bit weights, exhibiting only 0.2% degradation for ResNet-18 trained on ImageNet."}}
{"id": "3HJOA-1hb0e", "cdate": 1632875663496, "mdate": null, "content": {"title": "Toward Efficient Low-Precision Training: Data Format Optimization and Hysteresis Quantization", "abstract": "As the complexity and size of deep neural networks continue to increase, low-precision training has been extensively studied in the last few years to reduce hardware overhead. Training performance is largely affected by the numeric formats representing different values in low-precision training, but finding an optimal format typically requires numerous training runs, which is a very time-consuming process. In this paper, we propose a method to efficiently find an optimal format for activations and errors without actual training. We employ this method to determine an 8-bit format suitable for training various models. In addition, we propose hysteresis quantization to suppress undesired fluctuation in quantized weights during training. This scheme enables deeply quantized training using 4-bit weights, exhibiting only 0.2% degradation for ResNet-18 trained on ImageNet."}}
