{"id": "zulSGANU1C", "cdate": 1672531200000, "mdate": 1693554261406, "content": {"title": "A Hybrid Wireless Image Transmission Scheme with Diffusion", "abstract": "We propose a hybrid joint source-channel coding (JSCC) scheme, in which the conventional digital communication scheme is complemented with a generative refinement component to improve the perceptual quality of the reconstruction. The input image is decomposed into two components: the first is a coarse compressed version, and is transmitted following the conventional separation based approach. An additional component is obtained through the diffusion process by adding independent Gaussian noise to the input image, and is transmitted using DeepJSCC. The decoder combines the two signals to produce a high quality reconstruction of the source. Experimental results show that the hybrid design provides bandwidth savings and enables graceful performance improvement as the channel quality improves."}}
{"id": "N7tfN2MyeK", "cdate": 1672531200000, "mdate": 1693554261965, "content": {"title": "Computation of Rate-Distortion-Perception Functions With Wasserstein Barycenter", "abstract": "The nascent field of Rate-Distortion-Perception (RDP) theory is seeing a surge of research interest due to the application of machine learning techniques in the area of lossy compression. The information RDP function characterizes the three-way trade-off between description rate, average distortion, and perceptual quality measured by discrepancy between probability distributions. However, computing RDP functions has been a challenge due to the introduction of the perceptual constraint, and existing research often resorts to data-driven methods. In this paper, we show that the information RDP function can be transformed into a Wasserstein Barycenter problem. The non-strictly convexity brought by the perceptual constraint can be regularized by an entropy regularization term. We prove that the entropy regularized model converges to the original problem. Furthermore, we propose an alternating iteration method based on the Sinkhorn algorithm to numerically solve the regularized optimization problem. Experimental results demonstrate the efficiency and accuracy of the proposed algorithm."}}
{"id": "1Gh1qtdAsSd", "cdate": 1672531200000, "mdate": 1693554261967, "content": {"title": "Conditional Rate-Distortion-Perception Trade-Off", "abstract": "Recent advances in machine learning-aided lossy compression are incorporating perceptual fidelity into the rate-distortion theory. In this paper, we study the rate-distortion-perception trade-off when the perceptual quality is measured by the total variation distance between the empirical and product distributions of the discrete memoryless source and its reconstruction. We consider the general setting, where two types of resources are available at both the encoder and decoder: a common side information sequence, correlated with the source sequence, and common randomness. We consider both the strong perceptual constraint and the weaker empirical perceptual constraint. The required communication rate for achieving the distortion and empirical perceptual constraint is the minimum conditional mutual information, and similar result holds for strong perceptual constraint when sufficient common randomness is provided and the output along with the side information is constraint to an independent and identically distributed sequence."}}
{"id": "leLCff0vyU", "cdate": 1640995200000, "mdate": 1681662242174, "content": {"title": "Generalizing K\u00f6rner's graph entropy to graphons", "abstract": "K\\\"orner introduced the notion of graph entropy in 1973 as the minimal code rate of a natural coding problem where not all pairs of letters can be distinguished in the alphabet. Later it turned out that it can be expressed as the solution of a minimization problem over the so-called vertex-packing polytope. In this paper we generalize this notion to graphons. We show that the analogous minimization problem provides an upper bound for graphon entropy. We also give a lower bound in the shape of a maximization problem. The main result of the paper is that for most graphons these two bounds actually coincide and hence precisely determine the entropy in question. Furthermore, graphon entropy has a nice connection to the fractional chromatic number and the fractional clique number."}}
{"id": "_FfdC1zXPmn", "cdate": 1640995200000, "mdate": 1681662242198, "content": {"title": "Learning Cluster Causal Diagrams: An Information-Theoretic Approach", "abstract": "Many real-world phenomena arise from causal relationships among a set of variables. As a powerful tool, Bayesian Network (BN) has been successful in describing high-dimensional distributions. However, the faithfulness condition, enforced in most BN learning algorithms, is violated in the settings where multiple variables synergistically affect the outcome (i.e., with polyadic dependencies). Building upon recent development in cluster causal diagrams (C-DAGs), we initiate the formal study of learning C-DAGs from observational data to relax the faithfulness condition. We propose a new scoring function, the Clustering Information Criterion (CIC), based on information-theoretic measures that represent various complex interactions among variables. The CIC score also contains a penalization of the model complexity under the minimum description length principle. We further provide a searching strategy to learn structures of high scores. Experiments on both synthetic and real data support the effectiveness of the proposed method."}}
{"id": "B63zXHX4nR", "cdate": 1640995200000, "mdate": 1681662242201, "content": {"title": "Conditional graph entropy as an alternating minimization problem", "abstract": "In this paper we show that conditional graph entropy can be formulated as an alternating minimization problem, which gives rise to a simple iterative algorithm for numerically computing (conditional) graph entropy. The systematic study of alternating minimization problems was initiated by Csisz\\'ar and Tusn\\'ady. We apply this theory to conditional graph entropy, which was shown to be the minimal rate for a natural functional compression problem with side information at the receiver. This also leads to a new formula which shows that conditional graph entropy is part of a more general framework: the solution of an optimization problem over a convex corner. In the special case of graph entropy (i.e., unconditioned version) this was known due to Csisz\\'ar, K\\\"orner, Lov\\'asz, Marton, and Simonyi. In that case the role of the convex corner was played by the so-called vertex packing polytope. In the conditional version it is a more intricate convex body but the function to minimize is the same."}}
{"id": "4MEaXPAROiv", "cdate": 1609459200000, "mdate": 1648733810651, "content": {"title": "Information Flow in Markov Chains", "abstract": "We consider the problem of characterizing the flow of information in stochastic systems. Recently, several measures of partial information decomposition (PID) have been proposed which, for a fixed target variable, can distinguish unique, redundant, and synergistic contributions from the predictor variables. We study how each of those partial informations travel in a Markov chain, entering at one variable, passing through several variables, and eventually exiting downstream. Our work is agnostic to specific partial information decomposition (PID) measures. We investigate partial information flow among variables relating to overflow events in a river system."}}
{"id": "top5jMlgvtt", "cdate": 1577836800000, "mdate": 1648733810657, "content": {"title": "Synergy and Redundancy Duality Between Gaussian Multiple Access and Broadcast Channels", "abstract": "We investigate a novel duality for scalar Gaussian multiple access channels and broadcast channels. The duality we explore is based on shared partial information quantities (e.g. synergy and redundancy). Using lattice theory, we establish a crossover correspondence of the synergistic and redundant components between these two channels. The dual channels are similar to the traditional pairs based on capacity regions, though the pairs we identify have equal transmission powers instead of a sum constraint relating transmission powers."}}
{"id": "8pdsbW34J6w", "cdate": 1546300800000, "mdate": 1648733810694, "content": {"title": "A Measure of Synergy, Redundancy, and Unique Information using Information Geometry", "abstract": "It is well known that joint interactions between agents can be described qualitatively as having synergistic, unique, and redundant components. In recent years, there have been renewed efforts to decompose mutual information, a general, non-parametric measure of joint interactions, into constituent parts. We propose a novel, non-negative decomposition of mutual information between two sources and a target variable. The decomposition is for the exponential family, and thus can be applied to a broad range of distributions. We also show that values from our decomposition arise naturally from testing hypotheses of conditional dependence. We demonstrate the method numerically using standard binary logic gates and Gaussian channels, as well as apply the method to investigate redundancy between brain regions using an fMRI-based image classification data-set."}}
