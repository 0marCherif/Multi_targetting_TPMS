{"id": "FiyUTAy4sB8", "cdate": 1664203150232, "mdate": null, "content": {"title": "SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training", "abstract": "Tabular data underpins numerous high-impact applications of machine learning from fraud detection to genomics and healthcare.  Classical approaches to solving tabular problems, such as gradient boosting and random forests, are widely used by practitioners.  However, recent deep learning methods have achieved a degree of performance competitive with popular techniques. We devise a hybrid deep learning approach to solving tabular data problems.  Our method, SAINT, performs attention over both rows and columns, and it includes an enhanced embedding method.  We also study a new contrastive self-supervised pre-training method for use when labels are scarce.  SAINT consistently improves performance over previous deep learning methods, and it even performs competitively with gradient boosting methods, including XGBoost, CatBoost, and LightGBM, on average over $30$ benchmark datasets in regression, binary classification, and multi-class classification tasks."}}
{"id": "FUMxFwyLQZ", "cdate": 1664184434832, "mdate": null, "content": {"title": "Transfer Learning with Deep Tabular Models", "abstract": "Recent work on deep learning for tabular data demonstrates the strong performance of deep tabular models, often bridging the gap between gradient boosted decision trees and neural networks. Accuracy aside, a major advantage of neural models is that they are easily fine-tuned in new domains and learn reusable features. This property is often exploited in computer vision and natural language applications, where transfer learning is indispensable when task-specific training data is scarce. In this work, we explore the benefits that representation learning provides for knowledge transfer in the tabular domain. We conduct experiments in a realistic medical diagnosis test bed with limited amounts of downstream data and find that transfer learning with deep tabular models provides a definitive advantage over gradient boosted decision tree methods. We further compare the supervised and self-supervised pretraining strategies and provide practical advice on transfer learning with tabular models. Finally, we propose a pseudo-feature method for cases where the upstream and downstream feature sets differ, a tabular-specific problem widespread in real-world applications."}}
{"id": "b5RD94lXu2j", "cdate": 1663850468838, "mdate": null, "content": {"title": "Protecting Bidder Information in Neural Auctions", "abstract": "Single-shot auctions take place all the time, for example when selling ad space or allocating radio frequencies. Devising mechanisms for auctions with many bidders and multiple items can be complicated. It has been shown that neural networks can be used to approximate these mechanisms by satisfying the constraints that an auction be strategyproof and revenue maximizing. We show that despite such auctions maximizing revenue, they do so at the cost of revealing private bidder information. While randomness is often used to build in privacy, in this context it comes with complications if done without care. Specifically, it can violate rationality and feasibility constraints and can fundamentally change the incentive structure of the mechanism, and/or harm top-level metrics such as revenue or social welfare. We propose a method based on stochasticity that ensures privacy and meets the requirements for auction mechanisms. Furthermore, we analyze the cost to the auction house in expected revenue that comes with introducing privacy of various degrees."}}
{"id": "b0RuGUYo8pA", "cdate": 1663850372846, "mdate": null, "content": {"title": "Transfer Learning with Deep Tabular Models", "abstract": "Recent work on deep learning for tabular data demonstrates the strong performance of deep tabular models, often bridging the gap between gradient boosted decision trees and neural networks. Accuracy aside, a major advantage of neural models is that they are easily fine-tuned in new domains and learn reusable features. This property is often exploited in computer vision and natural language applications, where transfer learning is indispensable when task-specific training data is scarce. In this work, we explore the benefits that representation learning provides for knowledge transfer in the tabular domain. We conduct experiments in a realistic medical diagnosis test bed with limited amounts of downstream data and find that transfer learning with deep tabular models provides a definitive advantage over gradient boosted decision tree methods. We further compare the supervised and self-supervised pretraining strategies and provide practical advice on transfer learning with tabular models. Finally, we propose a pseudo-feature method for cases where the upstream and downstream feature sets differ, a tabular-specific problem widespread in real-world applications."}}
{"id": "PPjSKy40XUB", "cdate": 1652737605168, "mdate": null, "content": {"title": "End-to-end Algorithm Synthesis with Recurrent Networks: Extrapolation without Overthinking", "abstract": "Machine learning systems perform well on pattern matching tasks, but their ability to perform algorithmic or logical reasoning is not well understood. One important reasoning capability is algorithmic extrapolation, in which models trained only on small/simple reasoning problems can synthesize complex strategies for large/complex problems at test time. Algorithmic extrapolation can be achieved through recurrent systems, which can be iterated many times to solve difficult reasoning problems. We observe that this approach fails to scale to highly complex problems because behavior degenerates when many iterations are applied -- an issue we refer to as \"overthinking.\" We propose a recall architecture that keeps an explicit copy of the problem instance in memory so that it cannot be forgotten. We also employ a progressive training routine that prevents the model from learning behaviors that are specific to iteration number and instead pushes it to learn behaviors that can be repeated indefinitely. These innovations prevent the overthinking problem, and enable recurrent systems to solve extremely hard extrapolation tasks."}}
{"id": "M-9bPO0M2K5", "cdate": 1632875654431, "mdate": null, "content": {"title": "MetaBalance: High-Performance Neural Networks for Class-Imbalanced Data", "abstract": "Class-imbalanced data, in which some classes contain far more samples than others, is ubiquitous in real-world applications. Standard techniques for handling class-imbalance usually work by training on a re-weighted loss or on re-balanced data.  Unfortunately, training overparameterized neural networks on such objectives causes rapid memorization of minority class data. To avoid this trap, we harness meta-learning, which uses both an \"outer-loop'' and an \"inner-loop'' loss, each of which may be balanced using different strategies. We evaluate our method, MetaBalance, on image classification, credit-card fraud detection, loan default prediction, and facial recognition tasks with severely imbalanced data. We find that MetaBalance outperforms a wide array of popular strategies designed to handle class-imbalance, especially in scenarios with very few samples in minority classes."}}
{"id": "kDF4Owotj5j", "cdate": 1632875647949, "mdate": null, "content": {"title": "Thinking Deeper With Recurrent Networks: Logical Extrapolation Without Overthinking", "abstract": "Classical machine learning systems perform best when they are trained and tested on the same distribution, and they lack a mechanism to increase model power after training is complete. In contrast, recent work has observed that recurrent networks can exhibit logical extrapolation; models trained only on small/simple problem instances can extend their abilities to solve large/complex instances at test time simply by performing more recurrent iterations.  While preliminary results on these ``thinking systems'' are promising, existing recurrent systems, when iterated many times, often collapse rather than improve their performance.  This ``overthinking'' phenomenon has prevented thinking systems from scaling to particularly large and complex problems. In this paper, we design a recall architecture that keeps an explicit copy of the problem instance in memory so that it cannot be forgotten.  We also propose an incremental training routine that prevents the model from learning behaviors that are specific to iteration number and instead pushes it to learn behaviors that can be repeated indefinitely. Together, these design choices encourage models to converge to a steady state solution rather than deteriorate when many iterations are used. These innovations help to tackle the overthinking problem and boost deep thinking behavior on each of the benchmark tasks proposed by  Schwarzschild et al. (2021a)."}}
{"id": "3wNcr5nq56", "cdate": 1632875529749, "mdate": null, "content": {"title": "The Uncanny Similarity of Recurrence and Depth", "abstract": "It is widely believed that deep neural networks contain layer specialization, wherein networks extract hierarchical features representing edges and patterns in shallow layers and complete objects in deeper layers. Unlike common feed-forward models that have distinct filters at each layer, recurrent networks reuse the same parameters at various depths. In this work, we observe that recurrent models exhibit the same hierarchical behaviors and the same performance benefits as depth despite reusing the same filters at every recurrence. By training models of various feed-forward and recurrent architectures on several datasets for image classification as well as maze solving, we show that recurrent networks have the ability to closely emulate the behavior of non-recurrent deep models, often doing so with far fewer parameters."}}
{"id": "nL2lDlsrZU", "cdate": 1632875461808, "mdate": null, "content": {"title": "SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training", "abstract": "Tabular data underpins numerous high-impact applications of machine learning from fraud detection to genomics and healthcare.  Classical approaches to solving tabular problems, such as gradient boosting and random forests, are widely used by practitioners.  However, recent deep learning methods have achieved a degree of performance competitive with popular techniques.  We devise a hybrid deep learning approach to solving tabular data problems.  Our method, SAINT, performs attention over both rows and columns, and it includes an enhanced embedding method.  We also study a new contrastive self-supervised pre-training method for use when labels are scarce.  SAINT consistently improves performance over previous deep learning methods, and it even performs competitively with gradient boosting methods, including XGBoost, CatBoost, and LightGBM, on average over $30$ benchmark datasets in regression, binary classification, and multi-class classification tasks."}}
{"id": "Tsp2PL7-GQ", "cdate": 1621630082853, "mdate": null, "content": {"title": "Can You Learn an Algorithm?  Generalizing from Easy to Hard Problems with Recurrent Networks", "abstract": "Deep neural networks are powerful machines for visual pattern recognition, but reasoning tasks that are easy for humans may still be difficult for neural models. Humans possess the ability to extrapolate reasoning strategies learned on simple problems to solve harder examples, often by thinking for longer. For example, a person who has learned to solve small mazes can easily extend the very same search techniques to solve much larger mazes by spending more time.  In computers, this behavior is often achieved through the use of algorithms, which scale to arbitrarily hard problem instances at the cost of more computation. In contrast, the sequential computing budget of feed-forward neural networks is limited by their depth, and networks trained on simple problems have no way of extending their reasoning to accommodate harder problems. In this work, we show that recurrent networks trained to solve simple problems with few recurrent steps can indeed solve much more complex problems simply by performing additional recurrences during inference. We demonstrate this algorithmic behavior of recurrent networks on prefix sum computation, mazes, and chess.  In all three domains, networks trained on simple problem instances are able to extend their reasoning abilities at test time simply by \"thinking for longer.\""}}
