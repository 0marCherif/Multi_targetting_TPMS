{"id": "Lvlxq_H96lI", "cdate": 1652737479009, "mdate": null, "content": {"title": "Learning Manifold Dimensions with Conditional Variational Autoencoders", "abstract": "Although the variational autoencoder (VAE) and its conditional extension (CVAE) are capable of state-of-the-art results across multiple domains, their precise behavior is still not fully understood, particularly in the context of data (like images) that lie on or near a low-dimensional manifold. For example, while prior work has suggested that the globally optimal VAE solution can learn the correct manifold dimension, a necessary (but not sufficient) condition for producing samples from the true data distribution, this has never been rigorously proven.  Moreover, it remains unclear how such considerations would change when various types of conditioning variables are introduced, or when the data support is extended to a union of manifolds (e.g., as is likely the case for MNIST digits and related).  In this work, we address these points by first proving that VAE global minima are indeed capable of recovering the correct manifold dimension.  We then extend this result to more general CVAEs, demonstrating practical scenarios whereby the conditioning variables allow the model to adaptively learn manifolds of varying dimension across samples.  Our analyses, which have practical implications for various CVAE design choices, are also supported by numerical results on both synthetic and real-world datasets."}}
{"id": "Zwy3usE9RxT", "cdate": 1632875680993, "mdate": null, "content": {"title": "Training Deep Generative Models via Auxiliary Supervised Learning", "abstract": "Deep generative modeling has long been viewed as a challenging unsupervised learning problem, partly due to the lack of labels and the high dimension of the data. Although various latent variable models have been proposed to tackle these difficulties, the latent variable only serves as a device to model the observed data, and is typically averaged out during training. In this article, we show that by introducing a properly pre-trained encoder, the latent variable can play a more important role, which decomposes a deep generative model into a supervised learning problem and a much simpler unsupervised learning task. With this new training method, which we call the auxiliary supervised learning (ASL) framework, deep generative models can benefit from the enormous success of deep supervised learning and representation learning techniques. By evaluating on various synthetic and real data sets, we demonstrate that ASL is a stable, efficient, and accurate training framework for deep generative models."}}
