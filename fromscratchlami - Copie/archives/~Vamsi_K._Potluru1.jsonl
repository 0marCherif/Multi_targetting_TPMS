{"id": "PGZfRCueOJ", "cdate": 1683902473059, "mdate": 1683902473059, "content": {"title": "Automatic Differentiation of Sketched Regression", "abstract": "Sketching for speeding up regression problems involves using a sketching matrix S to\nquickly find the approximate solution to a\nlinear least squares regression (LLS) problem: given A of size n \u00d7 d, with n \u001d d,\nalong with b of size n \u00d7 1, we seek a vector\ny with minimal regression error kAy \u2212 bk2.\nThis approximation technique is now standard in data science, and many software systems use sketched regression internally, as a\ncomponent. It is often useful to calculate\nderivatives (gradients for the purpose of optimization, for example) of such large systems, where sketched LLS is merely a component of a larger system whose derivatives\nare needed. To support Automatic Differentiation (AD) of systems containing sketched\nLLS, we consider propagating derivatives\nthrough LLS: both propagating perturbations (forward AD) and gradients (reverse\nAD). AD performs accurate differentiation\nand is efficient for problems with a huge number of independent variables. Since we use\nLLSS (sketched LLS) instead of LLS for reasons of efficiency, propagation of derivatives\nalso needs to trade accuracy for efficiency,\npresumably by sketching. There are two approaches for this: (a) use AD to transform\nthe code that defines LLSS, or (b) approximate exact derivative propagation through\nLLS using sketching methods. We provide\nstrong bounds on the errors produced due to\nthese two natural forms of sketching in the\ncontext of AD, giving the first dimensionality reduction analysis for calculating the\nderivatives of a sketched computation. Our\nresults crucially depend on a novel analysis of\nthe operator norm of a sketched inverse matrix product in this context. Extensive experiments on both synthetic and real-world\nexperiments demonstrate the efficacy of our\nsketched gradients."}}
{"id": "5ar8cSEiFFE", "cdate": 1676827088846, "mdate": null, "content": {"title": "Differentially Private Synthetic Data Using KD-Trees", "abstract": "Creation of a synthetic dataset that faithfully represents the data distribution and simultaneously preserves privacy is a major research challenge. Many space partitioning based approaches have emerged in recent years for answering statistical queries in a differentially private manner. However, for synthetic data generation problem, recent research has been mainly focused on deep generative models. In contrast, we exploit space partitioning techniques together with noise perturbation and thus achieve intuitive and transparent algorithms. We propose both data independent and data dependent algorithms for $\\epsilon$-differentially private synthetic data generation whose kernel density resembles that of the real dataset. Additionally, we provide theoretical results on the utility-privacy trade-offs and show how our data dependent approach overcomes the curse of dimensionality and leads to a scalable algorithm. We show empirical utility improvements over the prior work, and discuss performance of our algorithm on a downstream classification task on a real dataset. "}}
{"id": "nbw3zjN3fu", "cdate": 1664816296580, "mdate": null, "content": {"title": "Fast Learning of Multidimensional Hawkes Processes via Frank-Wolfe", "abstract": "Hawkes processes have recently risen to the forefront of tools when it comes to modeling and generating sequential events data. Multidimensional Hawkes processes model both the self and cross-excitation between different types of events and have been applied successfully in various domain such as finance, epidemiology and personalized recommendations, among others. In this work we present an adaptation of the Frank-Wolfe algorithm for learning multidimensional Hawkes processes. Experimental results show that our approach has better or on par accuracy in terms of parameter estimation than other first order methods, while enjoying a significantly faster runtime."}}
{"id": "Ur3-yOdj3hC", "cdate": 1577836800000, "mdate": null, "content": {"title": "Goal recognition via model-based and model-free techniques", "abstract": "Goal recognition aims at predicting human intentions from a trace of observations. This ability allows people or organizations to anticipate future actions and intervene in a positive (collaborative) or negative (adversarial) way. Goal recognition has been successfully used in many domains, but it has been seldom been used by financial institutions. We claim the techniques are ripe for its wide use in finance-related tasks. The main two approaches to perform goal recognition are model-based (planning-based) and model-free (learning-based). In this paper, we adapt state-of-the-art learning techniques to goal recognition, and compare model-based and model-free approaches in different domains. We analyze the experimental data to understand the trade-offs of using both types of methods. The experiments show that planning-based approaches are ready for some goal-recognition finance tasks."}}
{"id": "UYxPyF459U8", "cdate": 1577836800000, "mdate": null, "content": {"title": "Heuristics for Link Prediction in Multiplex Networks", "abstract": "Link prediction, or the inference of future or missing connections between entities, is a well-studied problem in network analysis. A multitude of heuristics exist for link prediction in ordinary networks with a single type of connection. However, link prediction in multiplex networks, or networks with multiple types of connections, is not a well understood problem. We propose a novel general framework and three families of heuristics for multiplex network link prediction that are simple, interpretable, and take advantage of the rich connection type correlation structure that exists in many real world networks. We further derive a theoretical threshold for determining when to use a different connection type based on the number of links that overlap with an Erdos-Renyi random graph. Through experiments with simulated and real world scientific collaboration, transportation and global trade networks, we demonstrate that the proposed heuristics show increased performance with the richness of connection type correlation structure and significantly outperform their baseline heuristics for ordinary networks with a single connection type."}}
{"id": "-A2UwH3mxg8", "cdate": 1577836800000, "mdate": null, "content": {"title": "Automatic Differentiation of Sketched Regression", "abstract": "Sketching for speeding up regression problems involves using a sketching matrix $S$ to quickly find the approximate solution to a linear least squares regression (LLS) problem: given $A$ of size $n..."}}
{"id": "qrCUTpAxZ0P", "cdate": 1546300800000, "mdate": null, "content": {"title": "Conservative Exploration using Interleaving", "abstract": "In many practical problems, a learning agent may want to learn the best action in hindsight without ever taking a bad action, which is much worse than a default production action. In general, this ..."}}
{"id": "gU9zwzX2Lg-", "cdate": 1546300800000, "mdate": null, "content": {"title": "Grouped sparse projection", "abstract": "We design a new sparse projection method for a set of vectors that guarantees a desired average sparsity level measured leveraging the popular Hoyer measure (an affine function of the ratio of the $\\ell_1$ and $\\ell_2$ norms). Existing approaches either project each vector individually or require the use of a regularization parameter which implicitly maps to the average $\\ell_0$-measure of sparsity. Instead, in our approach we set the sparsity level for the whole set explicitly and simultaneously project a group of vectors with the sparsity level of each vector tuned automatically. We show that the computational complexity of our projection operator is linear in the size of the problem. Additionally, we propose a generalization of this projection by replacing the $\\ell_1$ norm by its weighted version. We showcase the efficacy of our approach in both supervised and unsupervised learning tasks on image datasets including CIFAR10 and ImageNet. In deep neural network pruning, the sparse models produced by our method on ResNet50 have significantly higher accuracies at corresponding sparsity values compared to existing competitors. In nonnegative matrix factorization, our approach yields competitive reconstruction errors against state-of-the-art algorithms."}}
{"id": "XxtSZb8rB-q", "cdate": 1514764800000, "mdate": null, "content": {"title": "Conservative Exploration using Interleaving", "abstract": "In many practical problems, a learning agent may want to learn the best action in hindsight without ever taking a bad action, which is significantly worse than the default production action. In general, this is impossible because the agent has to explore unknown actions, some of which can be bad, to learn better actions. However, when the actions are combinatorial, this may be possible if the unknown action can be evaluated by interleaving it with the production action. We formalize this concept as learning in stochastic combinatorial semi-bandits with exchangeable actions. We design efficient learning algorithms for this problem, bound their n-step regret, and evaluate them on both synthetic and real-world problems. Our real-world experiments show that our algorithms can learn to recommend K most attractive movies without ever violating a strict production constraint, both overall and subject to a diversity constraint."}}
{"id": "HJVCun-_-B", "cdate": 1451606400000, "mdate": null, "content": {"title": "How to Fake Multiply by a Gaussian Matrix", "abstract": "Have you ever wanted to multiply an n \\times d matrix X, with n \u226bd, on the left by an m \\times n matrix \\tilde G of i.i.d. Gaussian random variables, but could not afford to do it because it was to..."}}
