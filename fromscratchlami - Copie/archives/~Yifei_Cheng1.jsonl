{"id": "Kn6i2BZW69w", "cdate": 1663849820546, "mdate": null, "content": {"title": "DropIT: Dropping Intermediate Tensors for Memory-Efficient DNN Training", "abstract": "A standard hardware bottleneck when training deep neural networks is GPU memory. The bulk of memory is occupied by caching intermediate tensors for gradient computation in the backward pass. We propose a novel method to reduce this footprint - Dropping Intermediate Tensors (DropIT).  DropIT drops min-k elements of the intermediate tensors and approximates gradients from the sparsified tensors in the backward pass. Theoretically, DropIT reduces noise on estimated gradients and therefore has a higher rate of convergence than vanilla-SGD. Experiments show that we can drop up to 90\\% of the intermediate tensor elements in fully-connected and convolutional layers while achieving higher testing accuracy for Visual Transformers and Convolutional Neural Networks on various tasks (e.g., classification, object detection, instance segmentation). Our code and models are available at https://github.com/chenjoya/dropit."}}
{"id": "9sJgHnvaiF", "cdate": 1640995200000, "mdate": 1681651030611, "content": {"title": "Accelerating local SGD for non-IID data using variance reduction", "abstract": ""}}
{"id": "71nKwsVWLdy", "cdate": 1640995200000, "mdate": 1675690835011, "content": {"title": "DropIT: Dropping Intermediate Tensors for Memory-Efficient DNN Training", "abstract": "A standard hardware bottleneck when training deep neural networks is GPU memory. The bulk of memory is occupied by caching intermediate tensors for gradient computation in the backward pass. We propose a novel method to reduce this footprint - Dropping Intermediate Tensors (DropIT). DropIT drops min-k elements of the intermediate tensors and approximates gradients from the sparsified tensors in the backward pass. Theoretically, DropIT reduces noise on estimated gradients and therefore has a higher rate of convergence than vanilla-SGD. Experiments show that we can drop up to 90\\% of the intermediate tensor elements in fully-connected and convolutional layers while achieving higher testing accuracy for Visual Transformers and Convolutional Neural Networks on various tasks (e.g., classification, object detection, instance segmentation). Our code and models are available at https://github.com/chenjoya/dropit."}}
{"id": "HiXt7EH31vC", "cdate": 1609459200000, "mdate": 1667918888838, "content": {"title": "Is Heuristic Sampling Necessary in Training Deep Object Detectors?", "abstract": "To train accurate deep object detectors under the extreme foreground-background imbalance, heuristic sampling methods are always necessary, which either re-sample a subset of all training samples (hard sampling methods, <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">e.g.</i> biased sampling, OHEM), or use all training samples but re-weight them discriminatively (soft sampling methods, <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">e.g.</i> Focal Loss, GHM). In this paper, we challenge the necessity of such hard/soft sampling methods for training accurate deep object detectors. While previous studies have shown that training detectors without heuristic sampling methods would significantly degrade accuracy, we reveal that this degradation comes from an unreasonable classification gradient magnitude caused by the imbalance, rather than a lack of re-sampling/re-weighting. Motivated by our discovery, we propose a simple yet effective <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Sampling-Free</i> mechanism to achieve a reasonable classification gradient magnitude by initialization and loss scaling. Unlike heuristic sampling methods with multiple hyperparameters, our Sampling-Free mechanism is fully data diagnostic, without laborious hyperparameters searching. We verify the effectiveness of our method in training anchor-based and anchor-free object detectors, where our method always achieves higher detection accuracy than heuristic sampling methods on COCO and PASCAL VOC datasets. Our Sampling-Free mechanism provides a new perspective to address the foreground-background imbalance. Our code is released at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/ChenJoya/sampling-free</uri> ."}}
{"id": "GWEpvd2CvS", "cdate": 1609459200000, "mdate": 1681651030730, "content": {"title": "STL-SGD: Speeding Up Local SGD with Stagewise Communication Period", "abstract": ""}}
{"id": "EHCNvy-aF0k", "cdate": 1577836800000, "mdate": 1681651030739, "content": {"title": "STL-SGD: Speeding Up Local SGD with Stagewise Communication Period", "abstract": ""}}
{"id": "S1lXnhVKPr", "cdate": 1569438890524, "mdate": null, "content": {"title": "Variance Reduced Local SGD with Lower Communication Complexity", "abstract": "To accelerate the training of machine learning models, distributed stochastic gradient descent (SGD) and its variants have been widely adopted, which apply multiple workers in parallel to speed up training. Among them, Local SGD has gained much attention due to its lower communication cost. Nevertheless, when the data distribution on workers is non-identical, Local SGD requires $O(T^{\\frac{3}{4}} N^{\\frac{3}{4}})$ communications to maintain its \\emph{linear iteration speedup} property, where $T$ is the total number of iterations and $N$ is the number of workers. In this paper, we propose Variance Reduced Local SGD (VRL-SGD) to further reduce the communication complexity. Benefiting from eliminating the dependency on the gradient variance among workers, we theoretically prove that VRL-SGD achieves a \\emph{linear iteration speedup} with a lower communication complexity $O(T^{\\frac{1}{2}} N^{\\frac{3}{2}})$ even if workers access non-identical datasets. We conduct experiments on three machine learning tasks, and the experimental results demonstrate that VRL-SGD performs impressively better than Local SGD when the data among workers are quite diverse."}}
{"id": "u4Qkt5HJho", "cdate": 1546300800000, "mdate": 1681651030576, "content": {"title": "Faster Distributed Deep Net Training: Computation and Communication Decoupled Stochastic Gradient Descent", "abstract": ""}}
{"id": "sF-kbtSeqe", "cdate": 1546300800000, "mdate": 1681651030579, "content": {"title": "Faster Distributed Deep Net Training: Computation and Communication Decoupled Stochastic Gradient Descent", "abstract": ""}}
{"id": "U56dUY-ph8", "cdate": 1546300800000, "mdate": 1681651030730, "content": {"title": "Variance Reduced Local SGD with Lower Communication Complexity", "abstract": ""}}
