{"id": "h9O0wsmL-cT", "cdate": 1663850538453, "mdate": null, "content": {"title": "Regression with Label Differential Privacy", "abstract": "We study the task of training regression models with the guarantee of _label_ differential privacy (DP). Based on a global prior distribution of label values, which could be obtained privately, we derive a label DP randomization mechanism that is optimal under a given regression loss function. We prove that the optimal mechanism takes the form of a \"randomized response on bins\", and propose an efficient algorithm for finding the optimal bin values. We carry out a thorough experimental evaluation on several datasets demonstrating the efficacy of our algorithm."}}
{"id": "dD3pwu4g8Fh", "cdate": 1652737725291, "mdate": null, "content": {"title": "Anonymized Histograms in Intermediate Privacy Models", "abstract": "We study the problem of  privately computing the $\\mbox{\\it anonymized histogram}$ (a.k.a. $\\mbox{\\it unattributed histogram}$), which is defined as the histogram without item labels. Previous works have provided algorithms with $\\ell_1$- and $\\ell_2^2$-errors of $O_\\varepsilon(\\sqrt{n})$ in the central model of differential privacy (DP).\n\nIn this work, we provide an algorithm with a nearly matching error guarantee of $\\widetilde{O}_\\varepsilon(\\sqrt{n})$ in the shuffle DP and pan-private models. Our algorithm is very simple: it just post-processes the discrete Laplace-noised histogram!  Using this algorithm as a subroutine, we show applications in privately estimating symmetric properties of distributions such as entropy, support coverage, and support size.\n"}}
{"id": "jOYdlD4oYrn", "cdate": 1652737724180, "mdate": null, "content": {"title": "Private Isotonic Regression", "abstract": "In this paper, we consider the problem of differentially private (DP) algorithms for isotonic regression.  For the most general problem of isotonic regression over a partially ordered set (poset)  $\\mathcal{X}$ and for any Lipschitz loss function, we obtain a pure-DP algorithm that, given $n$ input points, has an expected excess empirical risk of roughly $\\mathrm{width}(\\mathcal{X}) \\cdot \\log|\\mathcal{X}| / n$, where $\\mathrm{width}(\\mathcal{X})$ is the width of the poset.  In contrast, we also obtain a near-matching lower bound of roughly $(\\mathrm{width}(\\mathcal{X}) + \\log |\\mathcal{X}|) / n$, that holds even for approximate-DP algorithms. Moreover, we show that the above bounds are essentially the best that can be obtained without utilizing any further structure of the poset.\nIn the special case of a totally ordered set and for $\\ell_1$ and $\\ell_2^2$ losses, our algorithm can be implemented in near-linear running time; we also provide extensions of this algorithm to the problem of private isotonic regression with additional structural constraints on the output function."}}
{"id": "jHIn0U9U6RO", "cdate": 1652737388005, "mdate": null, "content": {"title": "Understanding the Eluder Dimension", "abstract": "We provide new insights on eluder dimension, a complexity measure that has been extensively used to bound the regret of algorithms for online bandits and reinforcement learning with function approximation. First, we study the relationship between the eluder dimension for a function class and a generalized notion of \\emph{rank}, defined for any monotone ``activation'' $\\sigma : \\mathbb{R}\\to \\mathbb{R}$, which corresponds to the minimal dimension required to represent the class as a generalized linear model. It is known that when $\\sigma$ has derivatives bounded away from $0$, $\\sigma$-rank gives rise to an upper bound on eluder dimension for any function class; we show however that eluder dimension can be exponentially smaller than $\\sigma$-rank. We also show that the condition on the derivative is necessary; namely, when $\\sigma$ is the $\\mathsf{relu}$ activation, the eluder dimension can be exponentially larger than $\\sigma$-rank. For Boolean-valued function classes, we obtain a characterization of the eluder dimension in terms of star number and threshold dimension, quantities which are relevant in active learning and online learning respectively."}}
{"id": "WYrC0Aentah", "cdate": 1621630133175, "mdate": null, "content": {"title": "On the Power of Differentiable Learning versus PAC and SQ Learning", "abstract": "We study the power of learning via mini-batch stochastic gradient descent (SGD) on the loss of a differentiable model or neural network, and ask what learning problems can be learnt using this paradigm. We show that SGD can always simulate\u00a0learning with statistical queries (SQ), but its ability to go beyond that depends on the precision $\\rho$ of the gradients and the minibatch size $b$. With fine enough precision relative to minibatch size, namely when $b \\rho$ is small enough, SGD can go beyond SQ learning and simulate any sample-based learning algorithm and thus its learning power is equivalent to that of PAC learning;\u00a0this extends prior work that achieved this result for $b=1$.\u00a0Moreover,\u00a0with polynomially many bits of precision (i.e. when $\\rho$ is exponentially small), SGD can simulate PAC learning regardless of the batch size. On the other hand, when $b \\rho^2$ is large enough, the power of SGD is equivalent to that of SQ learning."}}
{"id": "TZPidZS3r_z", "cdate": 1621630133175, "mdate": null, "content": {"title": "On the Power of Differentiable Learning versus PAC and SQ Learning", "abstract": "We study the power of learning via mini-batch stochastic gradient descent (SGD) on the loss of a differentiable model or neural network, and ask what learning problems can be learnt using this paradigm. We show that SGD can always simulate\u00a0learning with statistical queries (SQ), but its ability to go beyond that depends on the precision $\\rho$ of the gradients and the minibatch size $b$. With fine enough precision relative to minibatch size, namely when $b \\rho$ is small enough, SGD can go beyond SQ learning and simulate any sample-based learning algorithm and thus its learning power is equivalent to that of PAC learning;\u00a0this extends prior work that achieved this result for $b=1$.\u00a0Moreover,\u00a0with polynomially many bits of precision (i.e. when $\\rho$ is exponentially small), SGD can simulate PAC learning regardless of the batch size. On the other hand, when $b \\rho^2$ is large enough, the power of SGD is equivalent to that of SQ learning."}}
{"id": "niwg09-v_0", "cdate": 1621003120804, "mdate": null, "content": {"title": "Bayesian Inference of Temporal Task Specifications from Demonstrations", "abstract": "When observing task demonstrations, human apprentices are able to identify whether a given task is executed correctly long before they gain expertise in actually performing that task. Prior research into learning from demonstrations (LfD) has failed to capture this notion of the acceptability of an execution; meanwhile, temporal logics provide a flexible language for expressing task specifications. Inspired by this, we present Bayesian specification inference, a probabilistic model for inferring task specification as a temporal logic formula. We incorporate methods from probabilistic programming to define our priors, along with a domain-independent likelihood function to enable sampling-based inference. We demonstrate the efficacy of our model for inferring true specifications with over 90% similarity between the inferred specification and the ground truth, both within a synthetic domain and a real-world table setting task.\n\n"}}
{"id": "YWuzZ-kE1nW", "cdate": 1596134524418, "mdate": null, "content": {"title": "On the Complexity of Modulo-q Arguments and the Chevalley-Warning Theorem", "abstract": "We study the search problem class PPAq defined as a modulo-q analog of the well-known polynomial parity argument class PPA introduced by Papadimitriou '94. Our first result shows that this class can be characterized in terms of PPAp for prime p.\nOur main result is to establish that an explicit version of a search problem associated to the Chevalley--Warning theorem is complete for PPAp for prime p. This problem is natural in that it does not explicitly involve circuits as part of the input. It is the first such complete problem for PPAp when p\u22653.\nFinally we discuss connections between Chevalley-Warning theorem and the well-studied short integer solution problem and survey the structural properties of PPAq. "}}
{"id": "ryVjgYW_bH", "cdate": 1514764800000, "mdate": null, "content": {"title": "Bayesian Inference of Temporal Task Specifications from Demonstrations", "abstract": "When observing task demonstrations, human apprentices are able to identify whether a given task is executed correctly long before they gain expertise in actually performing that task. Prior research into learning from demonstrations (LfD) has failed to capture this notion of the acceptability of an execution; meanwhile, temporal logics provide a flexible language for expressing task specifications. Inspired by this, we present Bayesian specification inference, a probabilistic model for inferring task specification as a temporal logic formula. We incorporate methods from probabilistic programming to define our priors, along with a domain-independent likelihood function to enable sampling-based inference. We demonstrate the efficacy of our model for inferring true specifications with over 90% similarity between the inferred specification and the ground truth, both within a synthetic domain and a real-world table setting task."}}
