{"id": "ncPGMQtS8F", "cdate": 1672531200000, "mdate": 1682343943649, "content": {"title": "On the Possibilities of AI-Generated Text Detection", "abstract": "Our work focuses on the challenge of detecting outputs generated by Large Language Models (LLMs) to distinguish them from those generated by humans. This ability is of the utmost importance in numerous applications. However, the possibility of such discernment has been the subject of debate within the community. Therefore, a central question is whether we can detect AI-generated text and, if so, when. In this work, we provide evidence that it should almost always be possible to detect AI-generated text unless the distributions of human and machine-generated texts are exactly the same over the entire support. This observation follows from the standard results in information theory and relies on the fact that if the machine text becomes more human-like, we need more samples to detect it. We derive a precise sample complexity bound of AI-generated text detection, which tells how many samples are needed to detect AI-generated text. This gives rise to additional challenges of designing more complicated detectors that take in $n$ samples for detection (rather than just one), which is the scope of future research on this topic. Our empirical evaluations on various real and synthetic datasets support our claim about the existence of better detectors, demonstrating that AI-generated text detection should be achievable in the majority of scenarios. Our theory and results align with OpenAI's empirical findings, (in relation to sequence length), and we are the first to provide a solid theoretical justification for these outcomes."}}
{"id": "TH8GUX0TMwP", "cdate": 1672531200000, "mdate": 1681490236590, "content": {"title": "STEERING: Stein Information Directed Exploration for Model-Based Reinforcement Learning", "abstract": ""}}
{"id": "1Qg8B5UbWD", "cdate": 1672531200000, "mdate": 1682343943684, "content": {"title": "RE-MOVE: An Adaptive Policy Design Approach for Dynamic Environments via Language-Based Feedback", "abstract": "Reinforcement learning-based policies for continuous control robotic navigation tasks often fail to adapt to changes in the environment during real-time deployment, which may result in catastrophic failures. To address this limitation, we propose a novel approach called RE-MOVE (\\textbf{RE}quest help and \\textbf{MOVE} on), which uses language-based feedback to adjust trained policies to real-time changes in the environment. In this work, we enable the trained policy to decide \\emph{when to ask for feedback} and \\emph{how to incorporate feedback into trained policies}. RE-MOVE incorporates epistemic uncertainty to determine the optimal time to request feedback from humans and uses language-based feedback for real-time adaptation. We perform extensive synthetic and real-world evaluations to demonstrate the benefits of our proposed approach in several test-time dynamic navigation scenarios. Our approach enable robots to learn from human feedback and adapt to previously unseen adversarial situations."}}
{"id": "25Fc-EEUSt", "cdate": 1665069639228, "mdate": null, "content": {"title": "Controllable Attack and Improved Adversarial Training in Multi-Agent Reinforcement Learning", "abstract": "Deep reinforcement learning policies have been shown vulnerable to adversarial attacks due to the inherit frangibility of neural networks. \nCurrent attack methods mainly focus on the adversarial state or action perturbations, where such direct manipulations to a reinforcement learning system may not always be feasible or realizable in the real-world. In this paper, we consider the more practical adversarial attacks realized through actions by an adversarial agent in the same environment.It has been shown, in prior work, that an victim agent is vulnerable to behaviors of an adversarial agent who targets to attack the victim, at the cost of introducing perceivable abnormal behaviors for the adversarial agent itself. To address this, we propose to constrain the state distribution shift caused by the adversarial policy and offer a more controllable attack scheme by building connections among policy space variations, state distribution shift, and the value function difference. To provide provable defense, we revisit the cycling behavior of common adversarial training methods in Markov game, which has been a well-known issue in general differential games including Generative Adversarial Networks (GANs) and adversarial training in supervised learning. We propose to fix the non-converging behavior through a simple timescale separation mechanism. In sharp contrast to general differential games, where timescale separation may only converge to stationary points, a two-timescale training methods in Markov games can converge to the Nash Equilibrium (NE). Using the Robosumo competition experiments, we demonstrate the controllable attack is much more efficient in the sense that it can introduce much less state distribution shift while achieving the same winning rate with unconstrained attack. Furthermore, in both Kuhn Poker and Robosumo competition, we verify that the rule of timescale separation leads to stable learning dynamics and less exploitable victim policies."}}
{"id": "GdGpM3VWWXD", "cdate": 1664310940589, "mdate": null, "content": {"title": "Posterior Coreset Construction with Kernelized Stein Discrepancy for Model-Based Reinforcement Learning", "abstract": "Model-based reinforcement learning (MBRL) exhibits favorable performance in practice, but its theoretical guarantees are mostly restricted to the setting when the transition model is Gaussian or Lipschitz and demands a posterior estimate whose representational complexity grows unbounded with time. In this work, we develop a novel MBRL method (i) which relaxes the assumptions on the target transition model to belong to a generic family of mixture models; (ii) is applicable to large-scale training by incorporating a compression step such that the posterior estimate consists of a \\emph{Bayesian coreset} of only statistically significant past state-action pairs; and (iii) {exhibits a Bayesian regret of $\\mathcal{O}(dH^{1+({\\alpha}/{2})}T^{1-({\\alpha}/{2})})$ with coreset size of $\\Omega(\\sqrt{T^{1+\\alpha}})$, where $d$ is the aggregate dimension of state action space, $H$ is the episode length, $T$ is the total number of time steps experienced, and $\\alpha\\in (0,1]$ is the tuning parameter which is a novel introduction into the analysis of MBRL in this work}. To achieve these results, we adopt an approach based upon Stein's method, which allows distributional distance to be evaluated in closed form as the kernelized Stein discrepancy (KSD). Experimentally, we observe that this approach is competitive with several state-of-the-art RL methodologies, and can achieve up to $50\\%$ reduction in wall clock time in some continuous control environments."}}
{"id": "uPWhEXjyvoo", "cdate": 1655376341012, "mdate": null, "content": {"title": "HTRON: Efficient Outdoor Navigation with Sparse Rewards via  Heavy Tailed Adaptive Reinforce Algorithm", "abstract": "We present a novel approach to improve the performance of deep reinforcement learning (DRL) based outdoor robot navigation systems. Most, existing DRL methods are based on carefully designed dense reward functions that learn the efficient behavior in an environment. \u00a0We circumvent this issue by working only with sparse rewards (which are easy to design) and propose a novel adaptive Heavy-Tailed Reinforce algorithm for Outdoor Navigation called HTRON. Our main idea is to utilize heavy-tailed policy parametrizations which implicitly induce exploration in sparse reward settings. We evaluate the performance of HTRON against Reinforce, PPO, and TRPO algorithms in three different outdoor scenarios: goal-reaching, obstacle avoidance, and uneven terrain navigation. We observe average an increase of 34.41% in terms of success rate, a 15.15% decrease in the average time steps taken to reach the goal, and a 24.9% decrease in the elevation cost compared to the navigation policies obtained by the other methods. Further, we demonstrate that our algorithm can be transferred directly into a Clearpath Husky robot to perform outdoor terrain navigation in real-world scenarios."}}
{"id": "_i-gM_42ChF", "cdate": 1640995200000, "mdate": 1682343943803, "content": {"title": "HTRON: Efficient Outdoor Navigation with Sparse Rewards via Heavy Tailed Adaptive Reinforce Algorithm", "abstract": "We present a novel approach to improve the performance of deep reinforcement learning (DRL) based outdoor robot navigation systems. Most, existing DRL methods are based on carefully designed dense ..."}}
{"id": "MraL1LFEoG", "cdate": 1640995200000, "mdate": 1681490236648, "content": {"title": "On the Hidden Biases of Policy Mirror Ascent in Continuous Action Spaces", "abstract": ""}}
{"id": "M6klGqh6ih", "cdate": 1640995200000, "mdate": 1681490236650, "content": {"title": "Dealing with Sparse Rewards in Continuous Control Robotics via Heavy-Tailed Policies", "abstract": ""}}
{"id": "GZyC5tr-jq4", "cdate": 1640995200000, "mdate": 1682343943646, "content": {"title": "HTRON: Efficient Outdoor Navigation with Sparse Rewards via Heavy Tailed Adaptive Reinforce Algorithm", "abstract": "We present a novel approach to improve the performance of deep reinforcement learning (DRL) based outdoor robot navigation systems. Most, existing DRL methods are based on carefully designed dense reward functions that learn the efficient behavior in an environment. We circumvent this issue by working only with sparse rewards (which are easy to design), and propose a novel adaptive Heavy-Tailed Reinforce algorithm for Outdoor Navigation called HTRON. Our main idea is to utilize heavy-tailed policy parametrizations which implicitly induce exploration in sparse reward settings. We evaluate the performance of HTRON against Reinforce, PPO and TRPO algorithms in three different outdoor scenarios: goal-reaching, obstacle avoidance, and uneven terrain navigation. We observe in average an increase of 34.41% in terms of success rate, a 15.15% decrease in the average time steps taken to reach the goal, and a 24.9% decrease in the elevation cost compared to the navigation policies obtained by the other methods. Further, we demonstrate that our algorithm can be transferred directly into a Clearpath Husky robot to perform outdoor terrain navigation in real-world scenarios."}}
