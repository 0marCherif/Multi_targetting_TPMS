{"id": "gT6XC3hJEPv", "cdate": 1640995200000, "mdate": 1681835746420, "content": {"title": "Inverse Imaging with Generative Priors Via Langevin Dynamics", "abstract": "Deep generative models have emerged as a powerful class of priors for signals in various inverse problems such as compressed sensing, phase retrieval and super-resolution. Here, we assume an unknown signal to lie in the range of some pre-trained generative model. A popular approach for signal recovery is via gradient descent in the low-dimensional latent space. While gradient descent has achieved good empirical performance, its theoretical behavior is not well understood. In this paper, we introduce the use of stochastic gradient Langevin dynamics (SGLD) for compressed sensing with a generative prior. Under mild assumptions on the generative model, we prove the convergence of SGLD to the true signal. We also demonstrate competitive empirical performance to standard gradient descent."}}
{"id": "TStIsvxwp9", "cdate": 1640995200000, "mdate": 1681835746330, "content": {"title": "Provable Compressed Sensing With Generative Priors via Langevin Dynamics", "abstract": "Deep generative models have emerged as a powerful class of priors for signals in various inverse problems such as compressed sensing, phase retrieval and super-resolution. In this work, we consider the compressed sensing problem and assume the unknown signal to lie in the range of some pre-trained generative model. A popular approach for signal recovery is via gradient descent in the low-dimensional latent space. While gradient descent has achieved good empirical performance, its theoretical behavior is not well understood. We introduce the use of stochastic gradient Langevin dynamics (SGLD) for compressed sensing with a generative prior. Under mild assumptions on the generative model, we prove the convergence of SGLD to the true signal. We also demonstrate competitive empirical performance to standard gradient descent."}}
{"id": "LxVUG_w2DZ", "cdate": 1624022585742, "mdate": null, "content": {"title": "Adversarially Robust Learning via Entropic Regularization", "abstract": "In this paper we propose a new family of algorithms, ATENT, for training adversarially robust deep neural networks. We formulate a new loss function that is equipped with an entropic regularization. Our loss considers the contribution of adversarial samples that are drawn from a specially designed distribution that assigns high probability to points with high loss and in the immediate neighborhood of training samples. ATENT achieves competitive (or better) performance in terms of robust classification accuracy as compared to several state-of-the-art robust learning approaches on benchmark datasets such as MNIST and CIFAR-10."}}
{"id": "Y7XVs024Gl", "cdate": 1609459200000, "mdate": 1681835746332, "content": {"title": "Adversarial Token Attacks on Vision Transformers", "abstract": "Vision transformers rely on a patch token based self attention mechanism, in contrast to convolutional networks. We investigate fundamental differences between these two families of models, by designing a block sparsity based adversarial token attack. We probe and analyze transformer as well as convolutional models with token attacks of varying patch sizes. We infer that transformer models are more sensitive to token attacks than convolutional models, with ResNets outperforming Transformer models by up to $\\sim30\\%$ in robust accuracy for single token attacks."}}
{"id": "BpjJ_6tjH2u", "cdate": 1609459200000, "mdate": 1681835746309, "content": {"title": "Provable Compressed Sensing with Generative Priors via Langevin Dynamics", "abstract": "Deep generative models have emerged as a powerful class of priors for signals in various inverse problems such as compressed sensing, phase retrieval and super-resolution. Here, we assume an unknown signal to lie in the range of some pre-trained generative model. A popular approach for signal recovery is via gradient descent in the low-dimensional latent space. While gradient descent has achieved good empirical performance, its theoretical behavior is not well understood. In this paper, we introduce the use of stochastic gradient Langevin dynamics (SGLD) for compressed sensing with a generative prior. Under mild assumptions on the generative model, we prove the convergence of SGLD to the true signal. We also demonstrate competitive empirical performance to standard gradient descent."}}
{"id": "5snl-5hns6d", "cdate": 1609459200000, "mdate": 1681835746422, "content": {"title": "Adversarially Robust Learning via Entropic Regularization", "abstract": "In this paper we propose a new family of algorithms, ATENT, for training adversarially robust deep neural networks. We formulate a new loss function that is equipped with an additional entropic regularization. Our loss function considers the contribution of adversarial samples that are drawn from a specially designed distribution in the data space that assigns high probability to points with high loss and in the immediate neighborhood of training samples. Our proposed algorithms optimize this loss to seek adversarially robust valleys of the loss landscape. Our approach achieves competitive (or better) performance in terms of robust classification accuracy as compared to several state-of-the-art robust learning approaches on benchmark datasets such as MNIST and CIFAR-10."}}
{"id": "_FlqLxBNzP", "cdate": 1577836800000, "mdate": null, "content": {"title": "Sample Efficient Fourier Ptychography for Structured Data", "abstract": "We study the problem of recovering structured data from Fourier ptychography measurements. Fourier ptychography is an image acquisition scheme that uses an array of images to produce high-resolution images in microscopy as well as long-distance imaging, to mitigate the effects of diffraction blurring. The number of measurements is typically much larger than the size of the signal (image or video) to be reconstructed, which translates to high storage and computational requirements. The issue of high sample complexity can be alleviated by utilizing structural properties of the image (or video). In this article, we first discuss a range of subsampling schemes which can reduce the amount of measurements in Fourier ptychography setups; however, this makes the problem ill-posed. Correspondingly, we impose structural constraints on the signals to be recovered, to regularize the problem. Through our novel framework of recovery algorithms, we show that one can reconstruct high-resolution images (or video) from fewer samples, via simple and natural assumptions on the structure of the images (or video). We demonstrate the validity of our claims through a series of experiments, both on simulated and real data."}}
{"id": "AAu9jca9xpu", "cdate": 1577836800000, "mdate": null, "content": {"title": "Adversarially Robust Learning via Entropic Regularization", "abstract": "In this paper we propose a new family of algorithms, ATENT, for training adversarially robust deep neural networks. We formulate a new loss function that is equipped with an additional entropic regularization. Our loss function considers the contribution of adversarial samples that are drawn from a specially designed distribution in the data space that assigns high probability to points with high loss and in the immediate neighborhood of training samples. Our proposed algorithms optimize this loss to seek adversarially robust valleys of the loss landscape. Our approach achieves competitive (or better) performance in terms of robust classification accuracy as compared to several state-of-the-art robust learning approaches on benchmark datasets such as MNIST and CIFAR-10."}}
{"id": "A9k8246m0JS", "cdate": 1577836800000, "mdate": null, "content": {"title": "High Dynamic Range Imaging Using Deep Image Priors", "abstract": "Traditionally, dynamic range enhancement for images has involved a combination of contrast improvement (via gamma correction or histogram equalization) and a denoising operation to reduce the effects of photon noise. More recently, modulo-imaging methods have been introduced for high dynamic range photography to significantly expand dynamic range at the sensing stage itself. The transformation function for both of these problems is highly non-linear, and the image reconstruction procedure is typically non-convex and ill-posed. A popular recent approach is to regularize the above inverse problem via a neural network prior (such as a trained autoencoder), but this requires extensive training over a dataset with thousands of paired regular/HDR image data samples.In this paper, we introduce a new approach for HDR image reconstruction using neural priors that require no training data. Specifically, we employ deep image priors, which have been successfully used for imaging problems such as denoising, super-resolution, inpainting and compressive sensing with promising performance gains over conventional regularization techniques. In this paper, we consider two different approaches to high dynamic range (HDR) imaging - gamma encoding and modulo encoding - and propose a combination of deep image prior and total variation (TV) regularization for reconstructing low-light images. We demonstrate the significant improvement achieved by both of these approaches as compared to traditional dynamic range enhancement techniques."}}
{"id": "r1l9n725IH", "cdate": 1568486322052, "mdate": null, "content": {"title": "Phase Retrieval using Untrained Neural Network Priors", "abstract": "Untrained deep neural networks as image priors have been recently introduced for linear inverse imaging problems such as denoising, super-resolution,  inpainting and compressive sensing with promising performance gains over hand-crafted image priors such as sparsity. Moreover, unlike learned generative priors they do not require any training over large datasets.  In this paper, we consider the problem of solving the non-linear inverse problem of compressive phase retrieval; this involves reconstructing a $d$-dimensional image signal from $n$ magnitude-only measurements, and $n<d$. We model images to lie in the range of an untrained deep generative network with a fixed seed. We further present two approaches for solving this problem: vanilla gradient descent and a projected gradient descent scheme and show superior empirical performance when compared to algorithms that use hand crafted priors."}}
