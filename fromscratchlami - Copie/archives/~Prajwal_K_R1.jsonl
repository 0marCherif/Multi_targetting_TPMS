{"id": "zNXar-TrXu", "cdate": 1640995200000, "mdate": 1667647499536, "content": {"title": "Lip-to-Speech Synthesis for Arbitrary Speakers in the Wild", "abstract": "In this work, we address the problem of generating speech from silent lip videos for any speaker in the wild. In stark contrast to previous works, our method (i) is not restricted to a fixed number of speakers, (ii) does not explicitly impose constraints on the domain or the vocabulary and (iii) deals with videos that are recorded in the wild as opposed to within laboratory settings. The task presents a host of challenges, with the key one being that many features of the desired target speech, like voice, pitch and linguistic content, cannot be entirely inferred from the silent face video. In order to handle these stochastic variations, we propose a new VAE-GAN architecture that learns to associate the lip and speech sequences amidst the variations. With the help of multiple powerful discriminators that guide the training process, our generator learns to synthesize speech sequences in any voice for the lip movements of any person. Extensive experiments on multiple datasets show that we outperform all baselines by a large margin. Further, our network can be fine-tuned on videos of specific identities to achieve a performance comparable to single-speaker models that are trained on $4\\times$ more data. We conduct numerous ablation studies to analyze the effect of different modules of our architecture. We also provide a demo video that demonstrates several qualitative results along with the code and trained models on our website: \\url{http://cvit.iiit.ac.in/research/projects/cvit-projects/lip-to-speech-synthesis}}"}}
{"id": "tAhTwdvRti", "cdate": 1640995200000, "mdate": 1667647499535, "content": {"title": "Sub-word Level Lip Reading With Visual Attention", "abstract": "The goal of this paper is to learn strong lip reading models that can recognise speech in silent videos. Most prior works deal with the open-set visual speech recognition problem by adapting existing automatic speech recognition techniques on top of trivially pooled visual features. Instead, in this paper, we focus on the unique challenges encountered in lip reading and propose tailored solutions. To this end, we make the following contributions: (1) we propose an attention-based pooling mechanism to aggregate visual speech representations; (2) we use sub-word units for lip reading for the first time and show that this allows us to better model the ambiguities of the task; (3) we propose a model for Visual Speech Detection (VSD), trained on top of the lip reading network. Following the above, we obtain state-of-the-art results on the challenging LRS2 and LRS3 benchmarks when training on public datasets, and even surpass models trained on large-scale industrial datasets by using an order of magnitude less data. Our best model achieves 22.6% word error rate on the LRS2 dataset, a performance unprecedented for lip reading models, significantly reducing the performance gap between lip reading and automatic speech recognition. Moreover, on the AVA-ActiveSpeaker benchmark, our VSD model surpasses all visual-only baselines and even outperforms several recent audio-visual methods."}}
{"id": "dl-YOlgobh", "cdate": 1640995200000, "mdate": 1667647499542, "content": {"title": "Lip-to-Speech Synthesis for Arbitrary Speakers in the Wild", "abstract": "In this work, we address the problem of generating speech from silent lip videos for any speaker in the wild. In stark contrast to previous works, our method (i) is not restricted to a fixed number of speakers, (ii) does not explicitly impose constraints on the domain or the vocabulary and (iii) deals with videos that are recorded in the wild as opposed to within laboratory settings. The task presents a host of challenges, with the key one being that many features of the desired target speech, like voice, pitch and linguistic content, cannot be entirely inferred from the silent face video. In order to handle these stochastic variations, we propose a new VAE-GAN architecture that learns to associate the lip and speech sequences amidst the variations. With the help of multiple powerful discriminators that guide the training process, our generator learns to synthesize speech sequences in any voice for the lip movements of any person. Extensive experiments on multiple datasets show that we outperform all baselines by a large margin. Further, our network can be fine-tuned on videos of specific identities to achieve a performance comparable to single-speaker models that are trained on $4\\times$ more data. We conduct numerous ablation studies to analyze the effect of different modules of our architecture. We also provide a demo video that demonstrates several qualitative results along with the code and trained models on our website http://cvit.iiit.ac.in/research/projects/cvit-projects/lip-to-speech-synthesis."}}
{"id": "d82iktN9Tjr", "cdate": 1640995200000, "mdate": 1699141890546, "content": {"title": "Weakly-supervised Fingerspelling Recognition in British Sign Language Videos", "abstract": ""}}
{"id": "9SkEbNSuHMZ", "cdate": 1640995200000, "mdate": 1668336656580, "content": {"title": "Automatic Dense Annotation of Large-Vocabulary Sign Language Videos", "abstract": "Recently, sign language researchers have turned to sign language interpreted TV broadcasts, comprising (i) a video of continuous signing and (ii) subtitles corresponding to the audio content, as a readily available and large-scale source of training data. One key challenge in the usability of such data is the lack of sign annotations. Previous work exploiting such weakly-aligned data only found sparse correspondences between keywords in the subtitle and individual signs. In this work, we propose a simple, scalable framework to vastly increase the density of automatic annotations. Our contributions are the following: (1)\u00a0we significantly improve previous annotation methods by making use of synonyms and subtitle-signing alignment; (2)\u00a0we show the value of pseudo-labelling from a sign recognition model as a way of sign spotting; (3)\u00a0we propose a novel approach for increasing our annotations of known and unknown classes based on in-domain exemplars; (4)\u00a0on the BOBSL BSL sign language corpus, we increase the number of confident automatic annotations from 670K to 5M. We make these annotations publicly available to support the sign language research community."}}
{"id": "yoYIizGFq1z", "cdate": 1609459200000, "mdate": 1667647499533, "content": {"title": "Visual Keyword Spotting with Attention", "abstract": ""}}
{"id": "xHAe8en8ZyC", "cdate": 1609459200000, "mdate": 1667647499547, "content": {"title": "Visual Keyword Spotting with Attention", "abstract": ""}}
{"id": "wUa4d0SB4Zh", "cdate": 1609459200000, "mdate": null, "content": {"title": "Data-Efficient Training Strategies for Neural TTS Systems", "abstract": "India is a country with thousands of languages and dialects spoken across a billion-strong population. For multi-lingual content creation and accessibility, text-to-speech systems will play a crucial role. However, the current neural TTS systems are data-hungry and need about 20 hours of clean single-speaker speech data for each language and speaker. This is not scalable for the large number of Indian languages and dialects. In this work, we demonstrate three simple, yet effective pre-training strategies that allow us to train neural TTS systems with just about one-tenth of the data needs while also achieving better accuracy and naturalness. We show that such pre-trained neural TTS systems can be quickly adapted to different speakers across languages and genders with less than 2 hours of data, thus significantly reducing the effort for future expansions to the thousands of rare Indian languages. We specifically highlight the benefits of multi-lingual pre-training and its consistent impact across our neural TTS systems for 8 Indian languages."}}
{"id": "YK4VZIo382", "cdate": 1609459200000, "mdate": 1667647499541, "content": {"title": "Visual Speech Enhancement Without A Real Visual Stream", "abstract": "In this work, we re-think the task of speech enhancement in unconstrained real-world environments. Current state- of-the-art methods use only the audio stream and are limited in their performance in a wide range of real-world noises. Recent works using lip movements as additional cues improve the quality of generated speech over \"audio-only \" methods. But, these methods cannot be used for several applications where the visual stream is unreliable or completely absent. We propose a new paradigm for speech enhancement by exploiting recent breakthroughs in speech- driven lip synthesis. Using one such model as a teacher network, we train a robust student network to produce accurate lip movements that mask away the noise, thus acting as a \"visual noise filter\". The intelligibility of the speech enhanced by our pseudo-lip approach is comparable ( <; 3% difference) to the case of using real lips. This implies that we can exploit the advantages of using lip movements even in the absence of a real video stream. We rigorously evaluate our model using quantitative metrics as well as human evaluations. Additional ablation studies and a demo video on our website containing qualitative comparisons and results clearly illustrate the effectiveness of our approach."}}
{"id": "rjGzUFLWI4g", "cdate": 1577836800000, "mdate": 1667647499534, "content": {"title": "A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild", "abstract": ""}}
