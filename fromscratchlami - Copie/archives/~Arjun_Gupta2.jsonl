{"id": "9W3Dnyfy8Fl", "cdate": 1668734800616, "mdate": null, "content": {"title": "DP-InstaHide: Data Augmentations Provably Enhance Guarantees Against Dataset Manipulations", "abstract": "Data poisoning and backdoor attacks manipulate training data to induce security breaches in a victim model. These attacks can be provably deflected using differentially private (DP) training methods, although this comes with a sharp decrease in model performance. The InstaHide method has recently been proposed as an alternative to DP training that leverages supposed privacy properties of the mixup augmentation, although without rigorous guarantees. In this paper, we rigorously show that $k$-way mixup provably yields at least $k$ times stronger DP guarantees than a naive DP mechanism, and we observe that this enhanced privacy guarantee is a strong foundation for building defenses against poisoning."}}
{"id": "3i_Bzt7Hmcm", "cdate": 1663850313681, "mdate": null, "content": {"title": "DP-InstaHide: Data Augmentations Provably Enhance Guarantees Against Dataset Manipulations", "abstract": "Data poisoning and backdoor attacks manipulate training data to induce security breaches in a victim model. These attacks can be provably deflected using differentially private (DP) training methods, although this comes with a sharp decrease in model performance. The InstaHide method has recently been proposed as an alternative to DP training that leverages supposed privacy properties of the mixup augmentation, although without rigorous guarantees. In this paper, we rigorously show that $k$-way mixup provably yields at least $k$ times stronger DP guarantees than a naive DP mechanism, and we observe that this enhanced privacy guarantee is a strong foundation for building defenses against poisoning."}}
{"id": "0WIM9dHzQBh", "cdate": 1632875748799, "mdate": null, "content": {"title": "DP-InstaHide: Data Augmentations Provably Enhance Guarantees Against Dataset Manipulations", "abstract": "Data poisoning and backdoor attacks manipulate training data to induce security breaches in a victim model. These attacks can be provably deflected using differentially private (DP) training methods, although this comes with a sharp decrease in model performance.   The InstaHide method has recently been proposed as an alternative to DP training that leverages supposed privacy properties of the mixup augmentation, although without rigorous guarantees. In this paper, we rigorously show that k-way mixup provably yields at least k times stronger DP guarantees than a naive DP mechanism, and we observe that this enhanced privacy guarantee is a strong foundation for building defenses against poisoning."}}
{"id": "1RqyBxJU_Wy", "cdate": 1632875594842, "mdate": null, "content": {"title": "A Flexible Measurement of Diversity in Datasets with Random Network Distillation", "abstract": "Generative models are increasingly able to produce remarkably high quality images and text.  The community has developed numerous evaluation metrics for comparing generative models.  However, these metrics do not always effectively quantify data diversity.  We develop a new, more flexible diversity metric that can readily be applied to data, both synthetic and natural, of any type.  Our method employs random network distillation, a technique introduced in reinforcement learning.  We validate and deploy this metric on both images and text.  We further explore diversity in few-shot image generation, a setting which was previously difficult to evaluate.\n"}}
{"id": "3wNcr5nq56", "cdate": 1632875529749, "mdate": null, "content": {"title": "The Uncanny Similarity of Recurrence and Depth", "abstract": "It is widely believed that deep neural networks contain layer specialization, wherein networks extract hierarchical features representing edges and patterns in shallow layers and complete objects in deeper layers. Unlike common feed-forward models that have distinct filters at each layer, recurrent networks reuse the same parameters at various depths. In this work, we observe that recurrent models exhibit the same hierarchical behaviors and the same performance benefits as depth despite reusing the same filters at every recurrence. By training models of various feed-forward and recurrent architectures on several datasets for image classification as well as maze solving, we show that recurrent networks have the ability to closely emulate the behavior of non-recurrent deep models, often doing so with far fewer parameters."}}
{"id": "Tsp2PL7-GQ", "cdate": 1621630082853, "mdate": null, "content": {"title": "Can You Learn an Algorithm?  Generalizing from Easy to Hard Problems with Recurrent Networks", "abstract": "Deep neural networks are powerful machines for visual pattern recognition, but reasoning tasks that are easy for humans may still be difficult for neural models. Humans possess the ability to extrapolate reasoning strategies learned on simple problems to solve harder examples, often by thinking for longer. For example, a person who has learned to solve small mazes can easily extend the very same search techniques to solve much larger mazes by spending more time.  In computers, this behavior is often achieved through the use of algorithms, which scale to arbitrarily hard problem instances at the cost of more computation. In contrast, the sequential computing budget of feed-forward neural networks is limited by their depth, and networks trained on simple problems have no way of extending their reasoning to accommodate harder problems. In this work, we show that recurrent networks trained to solve simple problems with few recurrent steps can indeed solve much more complex problems simply by performing additional recurrences during inference. We demonstrate this algorithmic behavior of recurrent networks on prefix sum computation, mazes, and chess.  In all three domains, networks trained on simple problem instances are able to extend their reasoning abilities at test time simply by \"thinking for longer.\""}}
{"id": "hBxSksqPuOg", "cdate": 1601308386334, "mdate": null, "content": {"title": "Random Network Distillation as a Diversity Metric for Both Image and Text Generation", "abstract": "Generative models are increasingly able to produce remarkably high quality images and text.  The community has developed numerous evaluation metrics for comparing generative models.  However, these metrics do not effectively quantify data diversity.  We develop a new diversity metric that can readily be applied to data, both synthetic and natural, of any type.  Our method employs random network distillation, a technique introduced in reinforcement learning.  We validate and deploy this metric on both images and text.  We further explore diversity in few-shot image generation, a setting which was previously difficult to evaluate."}}
{"id": "c77KhoLYSwF", "cdate": 1601308096448, "mdate": null, "content": {"title": "Just How Toxic is Data Poisoning?  A Benchmark for Backdoor and Data Poisoning Attacks", "abstract": "Data poisoning and backdoor attacks manipulate training data in order to cause models to fail during inference.  A recent survey of industry practitioners found that data poisoning is the number one concern among threats ranging from model stealing to adversarial attacks. However, we find that the impressive performance evaluations from data poisoning attacks are, in large part, artifacts of inconsistent experimental design. Moreover, we find that existing poisoning methods have been tested in contrived scenarios, and many fail in more realistic settings. In order to promote fair comparison in future work, we develop standardized benchmarks for data poisoning and backdoor attacks."}}
