{"id": "eAvvjCxCJ8C", "cdate": 1682369398933, "mdate": 1682369398933, "content": {"title": "Finite-Time Convergence in Continuous-Time Optimization", "abstract": "In this paper, we investigate a Lyapunov-like differential inequality that allows us to establish finite-time stability of a continuous-time state-space dynamical system represented via a multivariate ordinary differential equation or differential inclusion. Equipped with this condition, we successfully synthesize first and second-order dynamical systems that achieve finite-time convergence to the minima of a given sufficiently regular cost function. As a byproduct, we show that the p-rescaled gradient flow (p-RGF) proposed by Wibisono et al.(2016) is indeed finite-time convergent, provided the cost function is gradient dominated of order q in (1, p). Thus, we effectively bridge a gap between the p-RGF and the normalized gradient flow (NGF)(p=\\infty) proposed by Cortes (2006) in his seminal paper in the context of multi-agent systems. We discuss strategies to discretize our proposed flows and conclude by conducting some numerical experiments to illustrate our results."}}
{"id": "jWaLuyg6OEw", "cdate": 1632875466923, "mdate": null, "content": {"title": "First-Order Optimization Inspired from Finite-Time Convergent Flows", "abstract": "In this paper, we investigate the performance of two first-order optimization algorithms, obtained from forward Euler discretization of finite-time optimization flows. These flows are the rescaled-gradient flow (RGF) and the signed-gradient flow (SGF), and consist of non-Lipscthiz or discontinuous dynamical systems that converge locally in finite time to the minima of gradient-dominated functions. We propose an Euler discretization for these first-order finite-time flows, and provide convergence guarantees, in the deterministic and the stochastic setting. We then apply the proposed algorithms to academic examples, as well as deep neural networks training, where we empirically test their performances on the SVHN dataset. Our results show that our schemes demonstrate faster convergences against standard optimization alternatives."}}
