{"id": "nchvKfvNeX0", "cdate": 1663850273001, "mdate": null, "content": {"title": "Learning ReLU networks to high uniform accuracy is intractable", "abstract": "Statistical learning theory provides bounds on the necessary number of training samples needed to reach a prescribed accuracy in a learning problem formulated over a given target class. This accuracy is typically measured in terms of a generalization error, that is, an expected value of a given loss function. However, for several applications --- for example in a security-critical context or for problems in the computational sciences --- accuracy in this sense is not sufficient. In such cases, one would like to have guarantees for high accuracy on every input value, that is, with respect to the uniform norm. In this paper we precisely quantify the number of training samples needed for any conceivable training algorithm to guarantee a given uniform accuracy on any learning problem formulated over target classes containing (or consisting of) ReLU neural networks of a prescribed architecture. We prove that, under very general assumptions, the minimal number of training samples for this task scales exponentially both in the depth and the input dimension of the network architecture."}}
{"id": "nX-gReQ0OT", "cdate": 1652737384649, "mdate": null, "content": {"title": "Gold-standard solutions to the Schr\u00f6dinger equation using deep learning: How much physics do we need?", "abstract": "Finding accurate solutions to the Schr\u00f6dinger equation is the key unsolved challenge of computational chemistry. Given its importance for the development of new chemical compounds, decades of research have been dedicated to this problem, but due to the large dimensionality even the best available methods do not yet reach the desired accuracy.\nRecently the combination of deep learning with Monte Carlo methods has emerged as a promising way to obtain highly accurate energies and moderate scaling of computational cost. In this paper we significantly contribute towards this goal by introducing a novel deep-learning architecture that achieves 40-70% lower energy error at 6x lower computational cost compared to previous approaches. Using our method we establish a new benchmark by calculating the most accurate variational ground state energies ever published for a number of different atoms and molecules.\nWe systematically break down and measure our improvements, focusing in particular on the effect of increasing physical prior knowledge.\nWe surprisingly find that increasing the prior knowledge given to the architecture can actually decrease accuracy."}}
{"id": "r1VPNiA5Fm", "cdate": 1538087727353, "mdate": null, "content": {"title": "The Universal Approximation Power of Finite-Width Deep ReLU Networks", "abstract": "We show that finite-width deep ReLU neural networks yield rate-distortion optimal approximation (B\u00f6lcskei et al., 2018) of a wide class of functions, including polynomials, windowed sinusoidal functions, one-dimensional oscillatory textures, and the Weierstrass function, a fractal function which is continuous but nowhere differentiable. Together with the recently established universal approximation result for affine function systems (B\u00f6lcskei et al., 2018), this demonstrates that deep neural networks approximate vastly different signal structures generated by the affine group, the Weyl-Heisenberg group, or through warping, and even certain fractals, all with approximation error decaying exponentially in the number of neurons. We also prove that in the approximation of sufficiently smooth functions finite-width deep networks require strictly fewer neurons than finite-depth wide networks."}}
{"id": "Skbjv2Z_WB", "cdate": 1451606400000, "mdate": null, "content": {"title": "Discrete Deep Feature Extraction: A Theory and New Architectures", "abstract": "First steps towards a mathematical theory of deep convolutional neural networks for feature extraction were made\u2014for the continuous-time case\u2014in Mallat, 2012, and Wiatowski and B\u00f6lcskei, 2015. This..."}}
