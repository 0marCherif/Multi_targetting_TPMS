{"id": "vYwVtetWhF7", "cdate": 1688169600000, "mdate": 1682316123451, "content": {"title": "Relation-mining self-attention network for skeleton-based human action recognition", "abstract": ""}}
{"id": "jN6cUx3Sbx", "cdate": 1672531200000, "mdate": 1703299228240, "content": {"title": "Fine-Grained Spatio-Temporal Parsing Network for Action Quality Assessment", "abstract": ""}}
{"id": "EXNipU1UbfY", "cdate": 1672531200000, "mdate": 1703299228236, "content": {"title": "Self-Supervised Scene-Debiasing for Video Representation Learning via Background Patching", "abstract": "Self-supervised learning has considerably improved video representation learning by discovering supervisory signals automatically from unlabeled videos. However, due to the scene-biased nature of existing video datasets, the current methods are biased to the dominant scene context during action inference. Hence, this paper proposes Background Patching (BP), a scene-debiasing augmentation strategy to alleviate the model reliance on the video background in a self-supervised contrastive manner. The BP reduces the negative influence of the video background by mixing a randomly patched frame to the video background. BP randomly crops four frames from four different videos and patches them to construct a new frame for each video separately. The patched frame is mixed with all frames of the target video to produce a spatially distorted video sample. Then, we use existing self-supervised contrastive frameworks to pull representations of the distorted and original videos closer together. Moreover, BP mixes the semantic labels of patches with the target video's label, resulting in the regularization of the contrastive model to soften the decision boundaries in the embedding space. Therefore, the model is explicitly constrained to suppress the background influence by emphasizing more on the motion changes. The extensive experimental results show that our BP significantly improved the performance of various video understanding downstream tasks including action recognition, action detection, and video retrieval."}}
{"id": "tlmEZa_i8H", "cdate": 1640995200000, "mdate": 1682316123453, "content": {"title": "View-Invariant Human Action Recognition Via View Transformation Network (VTN)", "abstract": "Since the human body is non-rigid, actions captured in different views always involve action occlusion and information loss. Recently, view-variation-related human action recognition is still a challenging problem. To address the problem, we propose a View Transformation Network (VTN) that realizes the view normalization by transforming arbitrary-view action samples to a base view to seek for a view-invariant representation. an attention learning module is designed to learn a co-attention for action samples of different views, that contributes to output a similar feature representation to erase the view diversity in different views. Extensive and fair evaluations are performed on the UESTC varying-view RGB-D dataset, the NTU RGB-D 60 dataset, and the NTU RGB-D 120 dataset, where three evaluation types,  <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">i.e.</i>  X-subject, X-view, and A-view recognition, are performed. Experiments illustrate that our VTN model achieves outstanding performance."}}
{"id": "FJC_5K5HKi", "cdate": 1640995200000, "mdate": 1682316123454, "content": {"title": "Multi-level Multi-modal Feature Fusion for Action Recognition in Videos", "abstract": "Several multi-modal feature fusion approaches have been proposed in recent years in order to improve action recognition in videos. These approaches do not take full advantage of the multi-modal information in the videos, since they are biased towards a single modality or treat modalities separately. To address the multi-modal problem, we propose a Multi-Level Multi-modal feature Fusion (MLMF) for action recognition in videos. The MLMF projects each modality to shared and specific feature spaces. According to the similarity between the two modal shared features space, we augment the features in the specific feature space. As a result, the fused features not only incorporate the unique characteristics of the two modalities, but also explicitly emphasize their similarities. Moreover, the video's action segments differ in length, so the model needs to consider different-level feature ensembling for fine-grained action recognition. The optimal multi-level unified action feature representation is achieved by aggregating features at different levels. Our approach is evaluated in the EPIC-KITCHEN 100 dataset, and achieved encouraging results of action recognition in videos."}}
{"id": "E1T5NUe9Rtj", "cdate": 1640995200000, "mdate": 1703299228235, "content": {"title": "Actor-Aware Contrastive Learning for Semi-Supervised Action Recognition", "abstract": "The unique features of existing video datasets have led self-supervised contrastive learning to scene correlations and background biases, resulting in poor generalization in scene-invariant action recognition. Therefore, we propose Actor-aware Contrastive Learning for semi-supervised action recognition (ActorCLR). We employ localized actors to encourage the model to learn discriminative regions and mitigate the model's reliance on the video background during contrastive training. Furthermore, we introduce Inter-video Background Mixing (iBM) augmentation strategy to inject scene-invariance into the model. For iBM, we patch inter-video crops of four randomly selected frames to create a distinct frame for each video individually. The patched frame is mixed with the target video frames to produce a spatially distorted sample. Then, we jointly optimize contrastive loss and consistency regularization with localized actors and corresponding iBM-augmented videos in a semi-supervised manner. iBM also mixes the one-hot-encoded labels of patches with the target video's label, which softens the decision boundaries of the semi-supervised model. Our experimental results show that ActorCLR significantly improved action recognition on Kinetics-400, UCF101, and HMDB51 datasets under a low-label regime."}}
{"id": "C7o3gfgMv3", "cdate": 1609459200000, "mdate": 1682316149460, "content": {"title": "Attention augmented residual network for tomato disease detection and classification", "abstract": "Deep learning techniques help agronomists efficiently identify, analyze, and monitor tomato health. CNN (convolutional neural network) locality constraint and existing small train sample adversely influenced disease recognition performance. To alleviate these challenges, we proposed a discriminative feature learning attention augmented residual (AAR) network. The AAR network contains a stacked pre-activated residual block that learns deep coarse level features with locality context, whereas the attention block captures salient feature sets while maintaining the global relationship in data points, attention features augment the learning of the residual block. We used conditional variational generative adversarial network (CVGAN) image reconstruction network and augmentation techniques to enlarge the training sample size and improve feature distribution. We conducted several experiments to demonstrate the AAR network performance.The AAR network performed 97.04% accuracy without data generation and augmentation, 98.91% with data generation and augmentation, and 99.03% trained with data augmentation, which consistently improved tomato disease recognition and visualization effectiveness in both cases by learning salient features than deep and wide CNN baseline networks and other related works. Therefore, the AAR network can be a good candidate for improved tomato disease detection and classification task."}}
{"id": "4DSwvA9VWe", "cdate": 1609459200000, "mdate": 1668741455984, "content": {"title": "Arbitrary-view human action recognition via novel-view action generation", "abstract": ""}}
