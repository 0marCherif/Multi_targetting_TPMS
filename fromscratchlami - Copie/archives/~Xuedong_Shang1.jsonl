{"id": "PS6uMD9l3bz", "cdate": 1609459200000, "mdate": null, "content": {"title": "UCB Momentum Q-learning: Correcting the bias without forgetting", "abstract": "We propose UCBMQ, Upper Confidence Bound Momentum Q-learning, a new algorithm for reinforcement learning in tabular and possibly stage-dependent, episodic Markov decision process. UCBMQ is based on Q-learning where we add a momentum term and rely on the principle of optimism in face of uncertainty to deal with exploration. Our new technical ingredient of UCBMQ is the use of momentum to correct the bias that Q-learning suffers while, at the same time, limiting the impact it has on the second-order term of the regret. For UCBMQ, we are able to guarantee a regret of at most $O(\\sqrt{H^3SAT}+ H^4 S A )$ where $H$ is the length of an episode, $S$ the number of states, $A$ the number of actions, $T$ the number of episodes and ignoring terms in poly-$\\log(SAHT)$. Notably, UCBMQ is the first algorithm that simultaneously matches the lower bound of $\\Omega(\\sqrt{H^3SAT})$ for large enough $T$ and has a second-order term (with respect to the horizon $T$) that scales only linearly with the number of states $S$."}}
{"id": "_ppiuyhaeME", "cdate": 1596126855173, "mdate": null, "content": {"title": "Gamification of pure exploration for linear bandits", "abstract": "We investigate an active pure-exploration setting, that includes best-arm identification, in the context of linear stochastic bandits. While asymptotically optimal algorithms exist for standard multi-arm bandits, the existence of such algorithms for the best-arm identification in linear bandits has been elusive despite several attempts to address it. First, we provide a thorough comparison and new insight over different notions of optimality in the linear case, including G-optimality, transductive optimality from optimal experimental design and asymptotic optimality. Second, we design the first asymptotically optimal algorithm for fixed-confidence pure exploration in linear bandits. As a consequence, our algorithm naturally bypasses the pitfall caused by a simple but difficult instance, that most prior algorithms had to be engineered to deal with explicitly. Finally, we avoid the need to fully solve an optimal design problem by providing an approach that entails an efficient implementation."}}
{"id": "Tb36QtW4hb7", "cdate": 1596126647831, "mdate": null, "content": {"title": "A simple dynamic bandit algorithm for hyper-parameter optimization", "abstract": "Hyper-parameter tuning is a major part of modern machine learning systems. The tuning itself can be seen as a sequential resource allocation problem. As such, methods for multi-armed bandits have been already applied. In this paper, we view hyper-parameter optimization as an instance of best-arm identification in infinitely many-armed bandits. We propose D-TTTS, a new adaptive algorithm inspired by Thompson sampling, which dynamically balances between refining the estimate of the quality of hyper-parameter configurations previously explored and adding new hyper-parameter configurations to the pool of candidates. The algorithm is easy to implement and shows competitive performance compared to state-of-the-art algorithms for hyper-parameter tuning."}}
{"id": "6Hzh5GLo7B", "cdate": 1577836800000, "mdate": null, "content": {"title": "Stochastic Bandits with Vector Losses: Minimizing \ud835\udcc1\u221e-Norm of Relative Losses", "abstract": "Multi-armed bandits are widely applied in scenarios like recommender systems, for which the goal is to maximize the click rate. However, more factors should be considered, e.g., user stickiness, user growth rate, user experience assessment, etc. In this paper, we model this situation as a problem of $K$-armed bandit with multiple losses. We define relative loss vector of an arm where the $i$-th entry compares the arm and the optimal arm with respect to the $i$-th loss. We study two goals: (a) finding the arm with the minimum $\\ell^\\infty$-norm of relative losses with a given confidence level (which refers to fixed-confidence best-arm identification); (b) minimizing the $\\ell^\\infty$-norm of cumulative relative losses (which refers to regret minimization). For goal (a), we derive a problem-dependent sample complexity lower bound and discuss how to achieve matching algorithms. For goal (b), we provide a regret lower bound of $\\Omega(T^{2/3})$ and provide a matching algorithm."}}
{"id": "yMDSmMW6_Z", "cdate": 1546300800000, "mdate": null, "content": {"title": "Fixed-Confidence Guarantees for Bayesian Best-Arm Identification.", "abstract": "We investigate and provide new insights on the sampling rule called Top-Two Thompson Sampling (TTTS). In particular, we justify its use for fixed-confidence best-arm identification. We further propose a variant of TTTS called Top-Two Transportation Cost (T3C), which disposes of the computational burden of TTTS. As our main contribution, we provide the first sample complexity analysis of TTTS and T3C when coupled with a very natural Bayesian stopping rule, for bandits with Gaussian rewards, solving one of the open questions raised by Russo (2016). We also provide new posterior convergence results for TTTS under two models that are commonly used in practice: bandits with Gaussian and Bernoulli rewards and conjugate priors."}}
{"id": "jf1rSW96xlT", "cdate": 1546300800000, "mdate": null, "content": {"title": "General parallel optimization a without metric.", "abstract": "Hierarchical bandits are an approach for global optimization of \\emph{extremely} irregular functions. This paper provides new elements regarding POO, an adaptive meta-algorithm that does not requir..."}}
