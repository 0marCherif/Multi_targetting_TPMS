{"id": "7eIIHUIBj6n", "cdate": 1667615333522, "mdate": 1667615333522, "content": {"title": "Pixel2ISDF: Implicit Signed Distance Fields based Human Body Model from Multi-view and Multi-pose Images", "abstract": "In this report, we focus on reconstructing clothed humans in the canonical space given multiple views and poses of a human as the input. To achieve this, we utilize the geometric prior of the SMPLX model in the canonical space to learn the implicit representation for geometry reconstruction. Based on the observation that the topology between the posed mesh and the mesh in the canonical space are consistent, we propose to learn latent codes on the posed mesh by leveraging multiple input images and then assign the latent codes to the mesh in the canonical space. Specifically, we first leverage normal and geometry networks to extract the feature vector for each vertex on the SMPLX mesh. Normal maps are adopted for better generalization to unseen images compared to 2D images. Then, features for each vertex on the posed mesh from multiple images are integrated by MLPs. The integrated features acting as the latent code are anchored to the SMPLX mesh in the canonical space. Finally, latent code for each 3D point is extracted and utilized to calculate the SDF. Our work for reconstructing the human shape on canonical pose achieves 3rd performance on WCPA MVP-Human Body Challenge."}}
{"id": "sncEfPOcsc", "cdate": 1581734288285, "mdate": null, "content": {"title": "Deep Learning for Light Field Saliency Detection", "abstract": "Recent research in 4D saliency detection is limited by the deficiency of a large-scale 4D light field dataset. To address this, we introduce a new dataset to assist the subsequent research in 4D light field saliency detection. To the best of our knowledge, this is to date the largest light field dataset in which the dataset provides 1465 all-focus images with human-labeled ground truth masks and the corresponding focal stacks for every light field image. To verify the effectiveness of the light field data, we first introduce a fusion framework which includes two CNN streams where the focal stacks and all-focus images serve as the input. The focal stack stream utilizes a recurrent attention mechanism to adaptively learn to integrate every slice in the focal stack, which benefits from the extracted features of the good slices. Then it is incorporated with the output map generated by the all-focus stream to make the saliency prediction. In addition, we introduce adversarial examples by adding noise intentionally into images to help train the deep network, which can improve the robustness of the proposed network. The noise is designed by users, which is imperceptible but can fool the CNNs to make the wrong prediction. Extensive experiments show the effectiveness and superiority of the proposed model on the popular evaluation metrics. The proposed method performs favorably compared with the existing 2D, 3D and 4D saliency detection methods on the proposed dataset and existing LFSD light field dataset. The code and results can be found at https://github.com/OIPLab-DUT/ICCV2019_Deeplightfield_Saliency. Moreover, to facilitate research in this field, all images we collected are shared in a ready-to-use manner. "}}
{"id": "lKaW9efe_w", "cdate": 1577836800000, "mdate": 1633332160745, "content": {"title": "Visual Saliency Detection via Kernelized Subspace Ranking With Active Learning", "abstract": "Saliency detection task has witnessed a booming interest for years, due to the growth of the computer vision community. In this paper, we introduce a new saliency model that performs active learning with kernelized subspace ranker (KSR) referred to as KSR-AL. This pool-based active learning algorithm ranks the informativeness of unlabeled data by considering both uncertainty sampling and information density, thereby minimizing the cost of labeling. The informative images are selected to train the KSR iteratively and incrementally. The learning model of this algorithm is designed on object-level proposals and region-based convolutional neural network (R-CNN) features, by jointly learning a Rank-SVM classifier and a subspace projection. When the active learning process meets its stopping criteria, the saliency map of each image is generated by a weight fusion of its top-ranked proposals, whose ranking scores are graded by the learned ranker. We show that the KSR-AL achieves a reduction in annotation, as well as improvement in performance, compared with the supervised learning scheme. Besides, the proposed algorithm also outperforms the state-of-the-art methods. These improvements are demonstrated by extensive experiments on six publicly available benchmark datasets."}}
{"id": "_QZz2aBPk1w", "cdate": 1577836800000, "mdate": 1633332160732, "content": {"title": "A Multistage Refinement Network for Salient Object Detection", "abstract": "Deep convolutional neural networks (CNNs) have been successfully applied to a wide variety of problems in computer vision, including salient object detection. To accurately detect and segment salient objects, it is necessary to extract and combine high-level semantic features with low-level fine details simultaneously. This is challenging for CNNs because repeated subsampling operations such as pooling and convolution lead to a significant decrease in the feature resolution, which results in the loss of spatial details and finer structures. Therefore, we propose augmenting feedforward neural networks by using the multistage refinement mechanism. In the first stage, a master net is built to generate a coarse prediction map in which most detailed structures are missing. In the following stages, the refinement net with layerwise recurrent connections to the master net is equipped to progressively combine local context information across stages to refine the preceding saliency maps in a stagewise manner. Furthermore, the pyramid pooling module and channel attention module are applied to aggregate different-region-based global contexts. Extensive evaluations over six benchmark datasets show that the proposed method performs favorably against the state-of-the-art approaches."}}
{"id": "Md10ljtPAnc", "cdate": 1577836800000, "mdate": 1633332160736, "content": {"title": "Dynamically-Passed Contextual Information Network for Saliency Detection", "abstract": "Nowadays, deep convolutional neural networks (CNNs) have made significant improvement in detecting salient objects by integrating multi-level convolutional features or exploiting the advantage of dilated convolution. However, how to construct finer structure to produce effective features for saliency detection is still a challenging task. In this paper, we propose a novel deep learning based network by dynamically incorporating multi-level feature maps. The proposed Dynamically-passed Contextual Information Network (DCI-Net) can effectively control the information passage process, incorporate multi-scale context information and alleviate the distraction of background noise to improve the performance of saliency detection. Specifically, we first integrate an effective Passage Unit (PU) that progressively incorporates the low-level information with the high-level cues, in order to preserve the boundary details of salient objects. Second, a Spatial Pyramid Dilation (SPD) module is used to enhance the multi-scale feature representation by using multiple convolution dilations to handle the varied size of salient objects. Finally, we apply a Residual Attention module (RA) to further reinforce the saliency detection. Quantitative and qualitative experiments demonstrate the effectiveness of the proposed framework. Our method can significantly improve the performance based on five popular benchmark datasets."}}
{"id": "udbHDjlkfvr", "cdate": 1546300800000, "mdate": 1633332160738, "content": {"title": "Edge-Aware Convolution Neural Network Based Salient Object Detection", "abstract": "Salient object detection has received great amount of attention in recent years. In this letter, we propose a novel salient object detection algorithm, which combines the global contextual information along with the low-level edge features. First, we train an edge detection stream based on the state-of-the-art holistically-nested edge detection (HED) model and extract hierarchical boundary information from each VGG block. Then, the edge contours are served as the complementary edge-aware information and integrated with the saliency detection stream to depict continuous boundary for salient objects. Finally, we combine pyramid pooling modules with auxiliary side output supervision to form the multi-scale pyramid-based supervision module, providing multi-scale global contextual information for the saliency detection network. Compared with the previous methods, the proposed network contains more explicit edge-aware features and exploit the multi-scale global information more effectively. Experiments demonstrate the effectiveness of the proposed method, which achieves the state-of-the-art performance on five popular benchmarks."}}
{"id": "pY8pnnr28g7", "cdate": 1546300800000, "mdate": 1633332160731, "content": {"title": "A hybrid-backward refinement model for salient object detection", "abstract": "The deep Convolutional Neural Networks (CNNs) have been investigated in many salient object detection works and have achieved state-of-the-art performance compared to the classic methods. However, most of the existing CNN-based methods still struggle in addressing the problem of incomplete contours of salient objects. To overcome this problem, this paper focuses on accurately capturing the fine details of salient objects by proposing a novel Hybrid-Backward Refinement Network (HBRNet), which combines the high-level and low-level features extracted from two different CNNs. Taking advantage of the access to the visual cues and semantic information of CNNs, our hybrid deep network helps in modeling the object\u2019s context and preserving its boundaries as well. Specifically, we integrate effective hybrid refinement modules by merging feature maps of two consecutive layers from two deep networks. Also, our refinement model uses the residual convolutional unit in order to provide an effective end-to-end training. Furthermore, we apply the feature fusion technique to enable full exploitation of multi-scale features and progressively recover the resolution of the coarse prediction map. Through the experimental results, we demonstrate that the proposed framework achieves state-of-the-art performance on several popular benchmark datasets. The proposed hybrid refinement module, the residual convolutional unit as well as the fusion method, can appreciably improve the quality of the prediction maps."}}
{"id": "h9n-t3KvTJH", "cdate": 1546300800000, "mdate": 1633332160744, "content": {"title": "Multi-scale Pyramid Pooling Network for salient object detection", "abstract": "In recent years, visual saliency has witnessed tremendous progress through using deep convolutional neural networks (CNNs). For effective salient object detection, contextual information has been widely employed since the global context can tell different objects apart while the local context can distinguish salient ones from the background. Inspired by this, in this paper we propose a novel Multi-scale Pyramid Pooling Network (MPPNet) by exploiting global and local context in a unified way. This is achieved by incorporating hierarchical local information and global pyramid pooling representation. Particularly, the integration of multi-scale pyramid pooling proves its capacity to produce high-quality prediction map through the use of multiple pooling variables. Quantitative and qualitative experiments demonstrate the effectiveness of the proposed framework. Our method can significantly improve the performance based on four popular benchmark datasets."}}
{"id": "Yiry4-k-Ttd", "cdate": 1546300800000, "mdate": 1633332160742, "content": {"title": "Deep Learning for Light Field Saliency Detection", "abstract": "Recent research in 4D saliency detection is limited by the deficiency of a large-scale 4D light field dataset. To address this, we introduce a new dataset to assist the subsequent research in 4D light field saliency detection. To the best of our knowledge, this is to date the largest light field dataset in which the dataset provides 1465 all-focus images with human-labeled ground truth masks and the corresponding focal stacks for every light field image. To verify the effectiveness of the light field data, we first introduce a fusion framework which includes two CNN streams where the focal stacks and all-focus images serve as the input. The focal stack stream utilizes a recurrent attention mechanism to adaptively learn to integrate every slice in the focal stack, which benefits from the extracted features of the good slices. Then it is incorporated with the output map generated by the all-focus stream to make the saliency prediction. In addition, we introduce adversarial examples by adding noise intentionally into images to help train the deep network, which can improve the robustness of the proposed network. The noise is designed by users, which is imperceptible but can fool the CNNs to make the wrong prediction. Extensive experiments show the effectiveness and superiority of the proposed model on the popular evaluation metrics. The proposed method performs favorably compared with the existing 2D, 3D and 4D saliency detection methods on the proposed dataset and existing LFSD light field dataset. The code and results can be found at https://github.com/OIPLab-DUT/ ICCV2019_Deeplightfield_Saliency. Moreover, to facilitate research in this field, all images we collected are shared in a ready-to-use manner."}}
{"id": "ZjEp06JuL7", "cdate": 1514764800000, "mdate": 1633332160738, "content": {"title": "Deep multi-level networks with multi-task learning for saliency detection", "abstract": "Category-independent region proposals have been utilized for salient objects detection in recent works. However, these works may fail when the extracted proposals have poor overlap with salient objects. In this paper, we demonstrate segment-level saliency prediction can provide these methods with complementary information to improve detection results. In addition, classification loss (i.e., softmax) can distinguish positive samples from negative ones and similarity loss (i.e., triplet) can enlarge the contrast difference between samples with different class labels. We propose a joint optimization of the two losses to further promote the performance. Finally, a multi-layer cellular automata model is incorporated to generate the final saliency map with fine shape boundary and object-level highlighting. The proposed method has achieved state-of-the-art results on four benchmark datasets."}}
