{"id": "VbQI9Yv2P8f", "cdate": 1675209600000, "mdate": 1681734817637, "content": {"title": "Learning a Coordinated Network for Detail-Refinement Multiexposure Image Fusion", "abstract": "Nowadays, deep learning has made rapid progress in the field of multi-exposure image fusion. However, it is still challenging to extract available features while retaining texture details and color. To address this difficult issue, in this paper, we propose a coordinated learning network for detail-refinement in an end-to-end manner. Firstly, we obtain shallow feature maps from extreme over/under-exposed source images by a collaborative extraction module. Secondly, smooth attention weight maps are generated under the guidance of a self-attention module, which can draw a global connection to correlate patches in different locations. With the cooperation of the two aforementioned used modules, our proposed network can obtain a coarse fused image. Moreover, by assisting with an edge revision module, edge details of fused results are refined and noise is suppressed effectively. We conduct subjective qualitative and objective quantitative comparisons between the proposed method and twelve state-of-the-art methods on two available public datasets, respectively. The results show that our fused images significantly outperform others in visual effects and evaluation metrics. In addition, we also perform ablation experiments to verify the function and effectiveness of each module in our proposed method. The source code can be achieved at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/lok-18/LCNDR</uri> ."}}
{"id": "1yP3MMIVG2", "cdate": 1663770050977, "mdate": 1663770050977, "content": {"title": "Target-aware Dual Adversarial Learning and a Multi-scenario Multi-Modality Benchmark to Fuse Infrared and Visible for Object Detection.", "abstract": "This study addresses the issue of fusing infrared and visible images that appear differently for object detection. Aiming at generating an image of high visual quality, previous approaches discover commons underlying the two modalities and fuse upon the common space either by iterative optimization or deep networks. These approaches neglect that modality differences implying the complementary information are extremely important for both fusion and subsequent detection task. This paper proposes a bilevel optimization formulation for the joint problem of fusion and detection, and then unrolls to a target-aware Dual Adversarial Learning (TarDAL) network for fusion and a commonly used detection network. The fusion network with one generator and dual discriminators seeks commons while learning from differences, which preserves structural information of targets from the infrared and textural details from the visible. Furthermore, we build a synchronized imaging system with calibrated infrared and optical sensors, and collect currently the most comprehensive benchmark covering a wide range of scenarios. Extensive experiments on several public datasets and our benchmark demonstrate that our method outputs not only visually appealing fusion but also averagely 10:9% higher detection mAP than the stateof- the-art approaches on various challenging scenarios."}}
{"id": "tIc58KUrOxv", "cdate": 1640995200000, "mdate": 1668011899202, "content": {"title": "Learning a Deep Multi-Scale Feature Ensemble and an Edge-Attention Guidance for Image Fusion", "abstract": "Image fusion integrates a series of images acquired from different sensors, <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">e.g.</i> , infrared and visible, outputting an image with richer information than either one. Traditional and recent deep-based methods have difficulties in preserving prominent structures and recovering vital textural details for practical applications. In this article, we propose a deep network for infrared and visible image fusion cascading a feature learning module with a fusion learning mechanism. Firstly, we apply a coarse-to-fine deep architecture to learn multi-scale features for multi-modal images, which enables discovering prominent common structures for later fusion operations. The proposed feature learning module requires no well-aligned image pairs for training. Compared with the existing learning-based methods, the proposed feature learning module can ensemble numerous examples from respective modals for training, increasing the ability of feature representation. Secondly, we design an edge-guided attention mechanism upon the multi-scale features to guide the fusion focusing on common structures, thus recovering details while attenuating noise. Moreover, we provide a new aligned infrared and visible image fusion dataset, RealStreet, collected in various practical scenarios for comprehensive evaluation. Extensive experiments on two benchmarks, TNO and RealStreet, demonstrate the superiority of the proposed method over the state-of-the-art in terms of both visual inspection and objective analysis on six evaluation metrics. We also conduct the experiments on the FLIR and NIR datasets, containing foggy weather and poor light conditions, to verify the generalization and robustness of the proposed method."}}
{"id": "s-9InB8j3AP", "cdate": 1640995200000, "mdate": 1668011899208, "content": {"title": "Unsupervised Misaligned Infrared and Visible Image Fusion via Cross-Modality Image Generation and Registration", "abstract": "Recent learning-based image fusion methods have marked numerous progress in pre-registered multi-modality data, but suffered serious ghosts dealing with misaligned multi-modality data, due to the spatial deformation and the difficulty narrowing cross-modality discrepancy. To overcome the obstacles, in this paper, we present a robust cross-modality generation-registration paradigm for unsupervised misaligned infrared and visible image fusion (IVIF). Specifically, we propose a Cross-modality Perceptual Style Transfer Network (CPSTN) to generate a pseudo infrared image taking a visible image as input. Benefiting from the favorable geometry preservation ability of the CPSTN, the generated pseudo infrared image embraces a sharp structure, which is more conducive to transforming cross-modality image alignment into mono-modality registration coupled with the structure-sensitive of the infrared image. In this case, we introduce a Multi-level Refinement Registration Network (MRRN) to predict the displacement vector field between distorted and pseudo infrared images and reconstruct registered infrared image under the mono-modality setting. Moreover, to better fuse the registered infrared images and visible images, we present a feature Interaction Fusion Module (IFM) to adaptively select more meaningful features for fusion in the Dual-path Interaction Fusion Network (DIFN). Extensive experimental results suggest that the proposed method performs superior capability on misaligned cross-modality image fusion."}}
{"id": "_UHIynlPjXK", "cdate": 1640995200000, "mdate": 1668011899144, "content": {"title": "Target-aware Dual Adversarial Learning and a Multi-scenario Multi-Modality Benchmark to Fuse Infrared and Visible for Object Detection", "abstract": "This study addresses the issue of fusing infrared and visible images that appear differently for object detection. Aiming at generating an image of high visual quality, previous approaches discover commons underlying the two modalities and fuse upon the common space either by iterative optimization or deep networks. These approaches neglect that modality differences implying the complementary information are extremely important for both fusion and subsequent detection task. This paper proposes a bilevel optimization formulation for the joint problem of fusion and detection, and then unrolls to a target-aware Dual Adversarial Learning (TarDAL) network for fusion and a commonly used detection network. The fusion network with one generator and dual discriminators seeks commons while learning from differences, which preserves structural information of targets from the infrared and textural details from the visible. Furthermore, we build a synchronized imaging system with calibrated infrared and optical sensors, and collect currently the most comprehensive benchmark covering a wide range of scenarios. Extensive experiments on several public datasets and our benchmark demonstrate that our method outputs not only visually appealing fusion but also higher detection mAP than the state-of-the-art approaches. The source code and benchmark are available at https://github.com/dlut-dimt/TarDAL."}}
{"id": "U6h9L0m5YzE", "cdate": 1640995200000, "mdate": 1681626433140, "content": {"title": "Learn to Search a Lightweight Architecture for Target-Aware Infrared and Visible Image Fusion", "abstract": ""}}
{"id": "SgK6Dt0U7xC", "cdate": 1640995200000, "mdate": 1668011899064, "content": {"title": "Attention-Guided Global-Local Adversarial Learning for Detail-Preserving Multi-Exposure Image Fusion", "abstract": "Deep learning networks have recently demonstrated yielded impressive progress for multi-exposure image fusion. However, how to restore realistic texture details while correcting color distortion is still a challenging problem to be solved. To alleviate the aforementioned issues, in this paper, we propose an attention-guided global-local adversarial learning network for fusing extreme exposure images in a coarse-to-fine manner. Firstly, the coarse fusion result is generated under the guidance of attention weight maps, which acquires the essential region of interest from both sides. Secondly, we formulate an edge loss function, along with a spatial feature transform layer, for refining the fusion process. So that it can take full use of the edge information to deal with blurry edges. Moreover, by incorporating global-local learning, our method can balance pixel intensity distribution and correct the color distortion on spatially varying source images from both image/patch perspectives. Such a global-local discriminator ensures all the local patches of the fused images align with realistic normal-exposure ones. Extensive experimental results on two publicly available datasets show that our method drastically outperforms state-of-the-art methods in visual inspection and objective analysis. Furthermore, sufficient ablation experiments prove that our method has significant advantages in generating high-quality fused results with appealing details, clear targets, and faithful color. Source code will be available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/JinyuanLiu-CV/AGAL</uri> ."}}
{"id": "3Le5pOMBzY", "cdate": 1640995200000, "mdate": 1681626435068, "content": {"title": "ReCoNet: Recurrent Correction Network for Fast and Efficient Multi-modality Image Fusion", "abstract": ""}}
{"id": "v-bGjUSkHw", "cdate": 1609459200000, "mdate": 1668011899160, "content": {"title": "SMoA: Searching a Modality-Oriented Architecture for Infrared and Visible Image Fusion", "abstract": "Nowadays, driven by the high demand for autonomous driving and surveillance, infrared and visible image fusion\u00a0(IVIF)\u00a0has attracted significant attention from both the industry and research community. Existing learning-based IVIF methods tried to design various architectures to extract features. Still, these hand-crafted designed architectures cannot adequately represent the typical features of different modalities, resulting in undesirable artifacts on their fused results. To alleviate this issue, we propose a Neural Architecture Search (NAS)-based deep learning network to realize the IVIF task, which can automatically discover the modality-oriented feature representation. Our network is accomplished through two modality-oriented encoders and a unified decoder, in addition to a self-visual saliency weight module\u00a0(SvSW). The two modality-oriented encoders target to learn different intrinsic feature representations automatically from infrared-/visible- modality images. Subsequently, these intermediate features are merged via the SvSW module. Finally, the fused image is recovered by a unified decoder. Extensive experiments demonstrate that our method outperforms the state-of-the-art approaches by a large margin, especially in generating distinct targets and abundant details."}}
{"id": "Wac_97tfAPt", "cdate": 1609459200000, "mdate": 1668011899207, "content": {"title": "Halder: Hierarchical Attention-Guided Learning with Detail-Refinement for Multi-Exposure Image Fusion", "abstract": "Deep learning techniques have yielded impressive progress in the field of computational imaging. Existing approaches ignore designing specific constrain on illumination or edges, making them limited in handling asymmetric halos and more likely to generate a fusion result with color discrepancy or blurred edges. To alleviate these issues, we propose a hierarchical attention-guided learning with detail-refinement, termed as HALDeR, to tackle the multi-exposure fusion (MEF) task in a coarse-to-fine manner. Firstly, a hierarchical attention network is designed to produce a fusion result by calculating well-exposed areas under different illumination. Secondly, we develop a collaborative-refine module for preventing the missing details and correcting distorted color simultaneously. Moreover, adversarial learning is employed at end of our network, which can effectively alleviate other remaining artifacts (e.g., ringing effect and noises). Extensive quantitative and qualitative results on two publicly available datasets demonstrate that our HALDeR performs favorably against the state-of-the-art methods in generating vivid color and faithful detail. Source code will be available at https://github.com/JinyuanLiu-CV/HALDeR."}}
