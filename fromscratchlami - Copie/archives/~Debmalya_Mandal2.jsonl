{"id": "6IEMvyVGJ6", "cdate": 1684232361764, "mdate": 1684232361764, "content": {"title": "Markov Decision Processes with Time-Varying Geometric Discounting", "abstract": "Canonical models of Markov decision processes (MDPs) usually consider geometric discounting based on a constant discount factor. While this standard modeling approach has led to many elegant results, some recent studies indicate the necessity of modeling time-varying discounting in certain applications. This paper studies a model of infinite-horizon MDPs with time-varying discount factors. We take a game-theoretic perspective\u2014whereby each time step is treated as an independent decision maker with their own (fixed) discount factor\u2014and we study the subgame perfect equilibrium (SPE) of the resulting game as well as the related algorithmic problems.\nWe present a constructive proof of the existence of an SPE and demonstrate the EXPTIME-hardness of computing an SPE. We also turn to the approximate notion of \u03f5-SPE and show that an \u03f5-SPE exists under milder assumptions. An algorithm is presented to compute an \u03f5-SPE, of which an upper bound of the time complexity, as a function of the convergence property of the time-varying discount factor, is provided."}}
{"id": "BwhzMDO80BR", "cdate": 1672531200000, "mdate": 1682371226034, "content": {"title": "Implicit Poisoning Attacks in Two-Agent Reinforcement Learning: Adversarial Policies for Training-Time Attacks", "abstract": "In targeted poisoning attacks, an attacker manipulates an agent-environment interaction to force the agent into adopting a policy of interest, called target policy. Prior work has primarily focused on attacks that modify standard MDP primitives, such as rewards or transitions. In this paper, we study targeted poisoning attacks in a two-agent setting where an attacker implicitly poisons the effective environment of one of the agents by modifying the policy of its peer. We develop an optimization framework for designing optimal attacks, where the cost of the attack measures how much the solution deviates from the assumed default policy of the peer agent. We further study the computational properties of this optimization framework. Focusing on a tabular setting, we show that in contrast to poisoning attacks based on MDP primitives (transitions and (unbounded) rewards), which are always feasible, it is NP-hard to determine the feasibility of implicit poisoning attacks. We provide characterization results that establish sufficient conditions for the feasibility of the attack problem, as well as an upper and a lower bound on the optimal cost of the attack. We propose two algorithmic approaches for finding an optimal adversarial policy: a model-based approach with tabular policies and a model-free approach with parametric/neural policies. We showcase the efficacy of the proposed algorithms through experiments."}}
{"id": "5Jb-DDbIMB", "cdate": 1672531200000, "mdate": 1682371226182, "content": {"title": "Online Reinforcement Learning with Uncertain Episode Lengths", "abstract": "Existing episodic reinforcement algorithms assume that the length of an episode is fixed across time and known a priori. In this paper, we consider a general framework of episodic reinforcement learning when the length of each episode is drawn from a distribution. We first establish that this problem is equivalent to online reinforcement learning with general discounting where the learner is trying to optimize the expected discounted sum of rewards over an infinite horizon, but where the discounting function is not necessarily geometric. We show that minimizing regret with this new general discounting is equivalent to minimizing regret with uncertain episode lengths. We then design a reinforcement learning algorithm that minimizes regret with general discounting but acts for the setting with uncertain episode lengths. We instantiate our general bound for different types of discounting, including geometric and polynomial discounting. We also show that we can obtain similar regret bounds even when the uncertainty over the episode lengths is unknown, by estimating the unknown distribution over time. Finally, we compare our learning algorithms with existing value-iteration based episodic RL algorithms in a grid-world environment."}}
{"id": "pTH1UkWXK_", "cdate": 1640995200000, "mdate": 1684337817219, "content": {"title": "Sequential Blocked Matching", "abstract": "We consider a sequential blocked matching (SBM) model where strategic agents repeatedly report ordinal preferences over a set of services to a central planner. The planner's goal is to elicit agents' true preferences and design a policy that matches services to agents in order to maximize the expected social welfare with the added constraint that each matched service can be blocked or unavailable for a number of time periods. Naturally, SBM models the repeated allocation of reusable services to a set of agents where each allocated service becomes unavailable for a fixed duration. We first consider the offline SBM setting, where the strategic agents are aware of their true preferences. We measure the performance of any policy by distortion, the worst-case multiplicative approximation guaranteed by any policy. For the setting with s services, we establish lower bounds of \u03a9(s) and \u03a9(\u221as) on the distortions of any deterministic and randomised mechanisms, respectively. We complement these results by providing approximately truthful, measured by incentive ratio, deterministic and randomised policies based on random serial dictatorship which match our lower bounds. Our results show that there is a significant improvement if one considers the class of randomised policies. Finally, we consider the online SBM setting with bandit feedback where each agent is initially unaware of her true preferences, and the planner must facilitate each agent in the learning of their preferences through the matching of services over time. We design an approximately truthful mechanism based on the explore-then-commit paradigm, which achieves logarithmic dynamic approximate regret."}}
{"id": "kxq0-VOeqH", "cdate": 1640995200000, "mdate": 1684337817225, "content": {"title": "Learning Tensor Representations for Meta-Learning", "abstract": "We introduce a tensor-based model of shared representation for meta-learning from a diverse set of tasks. Prior works on learning linear representations for meta-learning assume that there is a common shared representation across different tasks, and do not consider the additional task-specific observable side information. In this work, we model the meta-parameter through an order-$3$ tensor, which can adapt to the observed task features of the task. We propose two methods to estimate the underlying tensor. The first method solves a tensor regression problem and works under natural assumptions on the data generating process. The second method uses the method of moments under additional distributional assumptions and has an improved sample complexity in terms of the number of tasks. We also focus on the meta-test phase, and consider estimating task-specific parameters on a new task. Substituting the estimated tensor from the first step allows us estimating the task-specific parameters with very few samples of the new task, thereby showing the benefits of learning tensor representations for meta-learning. Finally, through simulation and several real-world datasets, we evaluate our methods and show that it improves over previous linear models of shared representations for meta-learning."}}
{"id": "DKlLVDLQzNw", "cdate": 1640995200000, "mdate": 1684337817224, "content": {"title": "Socially Fair Reinforcement Learning", "abstract": "We consider the problem of episodic reinforcement learning where there are multiple stakeholders with different reward functions. Our goal is to output a policy that is socially fair with respect to different reward functions. Prior works have proposed different objectives that a fair policy must optimize including minimum welfare, and generalized Gini welfare. We first take an axiomatic view of the problem, and propose four axioms that any such fair objective must satisfy. We show that the Nash social welfare is the unique objective that uniquely satisfies all four objectives, whereas prior objectives fail to satisfy all four axioms. We then consider the learning version of the problem where the underlying model i.e. Markov decision process is unknown. We consider the problem of minimizing regret with respect to the fair policies maximizing three different fair objectives -- minimum welfare, generalized Gini welfare, and Nash social welfare. Based on optimistic planning, we propose a generic learning algorithm and derive its regret bound with respect to the three different policies. For the objective of Nash social welfare, we also derive a lower bound in regret that grows exponentially with $n$, the number of agents. Finally, we show that for the objective of minimum welfare, one can improve regret by a factor of $O(H)$ for a weaker notion of regret."}}
{"id": "87H44VK9gs", "cdate": 1640995200000, "mdate": 1684337817224, "content": {"title": "Learning Tensor Representations for Meta-Learning", "abstract": "We introduce a tensor-based model of shared representation for meta-learning from a diverse set of tasks. Prior works on learning linear representations for meta-learning assume that there is a common shared representation across different tasks, and do not consider the additional task-specific observable side information. In this work, we model the meta-parameter through an order-$3$ tensor, which can adapt to the observed task features of the task. We propose two methods to estimate the underlying tensor. The first method solves a tensor regression problem and works under natural assumptions on the data generating process. The second method uses the method of moments under additional distributional assumptions and has an improved sample complexity in terms of the number of tasks. We also focus on the meta-test phase, and consider estimating task-specific parameters on a new task. Substituting the estimated tensor from the first step allows us estimating the task-specific parameters with very few samples of the new task, thereby showing the benefits of learning tensor representations for meta-learning. Finally, through simulation and several real-world datasets, we evaluate our methods and show that it improves over previous linear models of shared representations for meta-learning."}}
{"id": "4_9VYnQonN0", "cdate": 1640995200000, "mdate": 1684337817225, "content": {"title": "Performative Reinforcement Learning", "abstract": "We introduce the framework of performative reinforcement learning where the policy chosen by the learner affects the underlying reward and transition dynamics of the environment. Following the recent literature on performative prediction~\\cite{Perdomo et. al., 2020}, we introduce the concept of performatively stable policy. We then consider a regularized version of the reinforcement learning problem and show that repeatedly optimizing this objective converges to a performatively stable policy under reasonable assumptions on the transition dynamics. Our proof utilizes the dual perspective of the reinforcement learning problem and may be of independent interest in analyzing the convergence of other algorithms with decision-dependent environments. We then extend our results for the setting where the learner just performs gradient ascent steps instead of fully optimizing the objective, and for the setting where the learner has access to a finite number of trajectories from the changed environment. For both settings, we leverage the dual formulation of performative reinforcement learning and establish convergence to a stable solution. Finally, through extensive experiments on a grid-world environment, we demonstrate the dependence of convergence on various parameters e.g. regularization, smoothness, and the number of samples."}}
{"id": "vtwTCFW75Wr", "cdate": 1609459200000, "mdate": 1684337817349, "content": {"title": "Improving Teacher-Student Interactions in Online Educational Forums using a Markov Chain based Stackelberg Game Model", "abstract": "With the rapid proliferation of the Internet, the area of education has undergone a massive transformation in terms of how students and instructors interact in a classroom. Online learning now takes more than one form, including the use of technology to enhance a face-to-face class, a hybrid class that combines both face-to-face meetings and online work, and fully online courses. Further, online classrooms are usually composed of an online education forum (OEF) where students and instructor discuss open-ended questions for gaining better understanding of the subject. However, empirical studies have repeatedly shown that the dropout rates in these online courses are very high partly due to the lack of motivation among the enrolled students. We undertake an empirical comparison of student behavior in OEFs associated with a graduate-level course during two terms. We identify key parameters dictating the dynamics of OEFs like effective incentive design, student heterogeneity, and super-posters phenomenon. Motivated by empirical observations, we propose an analytical model based on continuous time Markov chains (CTMCs) to capture instructor-student interactions in an OEF. Using concepts from lumpability of CTMCs, we compute steady state and transient probabilities along with expected net-rewards for the instructor and the students. We formulate a mixed-integer linear program which views an OEF as a single-leader-multiple-followers Stackelberg game. Through simulations, we observe that students exhibit varied degree of non-monotonicity in their participation (with increasing instructor involvement). We also study the effect of instructor bias and budget on the student participation levels. Our model exhibits the empirically observed super-poster phenomenon under certain parameter configurations and recommends an optimal plan to the instructor for maximizing student participation in OEFs."}}
{"id": "oMV9qfKrV0z", "cdate": 1609459200000, "mdate": 1684337817228, "content": {"title": "Surprisingly Popular Voting Recovers Rankings, Surprisingly!", "abstract": "The wisdom of the crowd has long become the de facto approach for eliciting information from individuals or experts in order to predict the ground truth. However, classical democratic approaches for aggregating individual \\emph{votes} only work when the opinion of the majority of the crowd is relatively accurate. A clever recent approach, \\emph{surprisingly popular voting}, elicits additional information from the individuals, namely their \\emph{prediction} of other individuals' votes, and provably recovers the ground truth even when experts are in minority. This approach works well when the goal is to pick the correct option from a small list, but when the goal is to recover a true ranking of the alternatives, a direct application of the approach requires eliciting too much information. We explore practical techniques for extending the surprisingly popular algorithm to ranked voting by partial votes and predictions and designing robust aggregation rules. We experimentally demonstrate that even a little prediction information helps surprisingly popular voting outperform classical approaches."}}
