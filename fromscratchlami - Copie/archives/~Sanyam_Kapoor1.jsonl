{"id": "ao30zaT3YL", "cdate": 1653595784952, "mdate": null, "content": {"title": "Pre-Train Your Loss: Easy Bayesian Transfer Learning with Informative Prior", "abstract": "Deep learning is increasingly moving towards a transfer learning paradigm whereby large ``foundation models'' are fine-tuned on downstream tasks, starting from an initialization learned on the source task. But an initialization contains relatively little information about the source task.  %, and would not affect the final solution at all if we do a good job of optimization. \nInstead, we show that we can learn highly informative posteriors from the source task, which serves as the basis for priors that modify the whole loss surface on the downstream task. This simple modular approach enables significant performance gains and more data-efficient learning on various downstream classification and segmentation tasks, serving as a drop-in replacement for standard pre-training strategies."}}
{"id": "pBJe5yu41Pq", "cdate": 1652737831185, "mdate": null, "content": {"title": "On Uncertainty, Tempering, and Data Augmentation in Bayesian Classification", "abstract": "Aleatoric uncertainty captures the inherent randomness of the data, such as measurement noise. In Bayesian regression, we often use a Gaussian observation model, where we control the level of aleatoric uncertainty with a noise variance parameter. By contrast, for Bayesian classification we use a categorical distribution with no mechanism to represent our beliefs about aleatoric uncertainty. Our work shows that explicitly accounting for aleatoric uncertainty significantly improves the performance of Bayesian neural networks. We note that many standard benchmarks, such as CIFAR-10, have essentially no aleatoric uncertainty. Moreover, we show that data augmentation in approximate inference softens the likelihood, leading to underconfidence and misrepresenting our beliefs about aleatoric uncertainty. Accordingly, we find that a cold posterior, tempered by a power greater than one, often more honestly reflects our beliefs about aleatoric uncertainty than no tempering --- providing an explicit link between data augmentation and cold posteriors. We further show that we can match or exceed the performance of posterior tempering by using a Dirichlet observation model, where we explicitly control the level of aleatoric uncertainty, without any need for tempering."}}
{"id": "YCniF6_3Jb", "cdate": 1652737802657, "mdate": null, "content": {"title": "Pre-Train Your Loss: Easy Bayesian Transfer Learning with Informative Priors", "abstract": "Deep learning is increasingly moving towards a transfer learning paradigm whereby large foundation models are fine-tuned on downstream tasks, starting from an initialization learned on the source task. But an initialization contains relatively little information about the source task, and does not reflect the belief that our knowledge of the source task should affect the locations and shape of optima on the downstream task.\nInstead, we show that we can learn highly informative posteriors from the source task, through supervised or self-supervised approaches, which then serve as the basis for priors that modify the whole loss surface on the downstream task. This simple modular approach enables significant performance gains and more data-efficient learning on a variety of downstream classification and segmentation tasks, serving as a drop-in replacement for standard pre-training strategies. These highly informative priors also can be saved for future use, similar to pre-trained weights, and stand in contrast to the zero-mean isotropic uninformative priors that are typically used in Bayesian deep learning. "}}
{"id": "o8nYuR8ekFm", "cdate": 1652737643134, "mdate": null, "content": {"title": "PAC-Bayes Compression Bounds So Tight That They Can Explain Generalization", "abstract": "While there has been progress in developing non-vacuous generalization bounds for deep neural networks, these bounds tend to be uninformative about why deep learning works. In this paper, we develop a compression approach based on quantizing neural network parameters in a linear subspace, profoundly improving on previous results to provide state-of-the-art generalization bounds on a variety of tasks, including transfer learning. We use these tight bounds to better understand the role of model size, equivariance, and the implicit biases of optimization, for generalization in deep learning. Notably, we find large models can be compressed to a much greater extent than previously known, encapsulating Occam\u2019s razor."}}
{"id": "mWBS19VsOOk", "cdate": 1640995200000, "mdate": 1680040894402, "content": {"title": "PAC-Bayes Compression Bounds So Tight That They Can Explain Generalization", "abstract": ""}}
{"id": "m8sXwA05N1N", "cdate": 1640995200000, "mdate": 1680040894421, "content": {"title": "On Uncertainty, Tempering, and Data Augmentation in Bayesian Classification", "abstract": ""}}
{"id": "ViCHN8YteC", "cdate": 1640995200000, "mdate": 1667401030897, "content": {"title": "Pre-Train Your Loss: Easy Bayesian Transfer Learning with Informative Priors", "abstract": "Deep learning is increasingly moving towards a transfer learning paradigm whereby large foundation models are fine-tuned on downstream tasks, starting from an initialization learned on the source task. But an initialization contains relatively little information about the source task. Instead, we show that we can learn highly informative posteriors from the source task, through supervised or self-supervised approaches, which then serve as the basis for priors that modify the whole loss surface on the downstream task. This simple modular approach enables significant performance gains and more data-efficient learning on a variety of downstream classification and segmentation tasks, serving as a drop-in replacement for standard pre-training strategies. These highly informative priors also can be saved for future use, similar to pre-trained weights, and stand in contrast to the zero-mean isotropic uninformative priors that are typically used in Bayesian deep learning."}}
{"id": "jlMIR16KzpX", "cdate": 1609459200000, "mdate": 1632856451928, "content": {"title": "SKIing on Simplices: Kernel Interpolation on the Permutohedral Lattice for Scalable Gaussian Processes", "abstract": "State-of-the-art methods for scalable Gaussian processes use iterative algorithms, requiring fast matrix vector multiplies (MVMs) with the co-variance kernel. The Structured Kernel Interpolation (S..."}}
{"id": "h5Y485L9J--G", "cdate": 1609459200000, "mdate": 1632856451930, "content": {"title": "Variational Auto-Regressive Gaussian Processes for Continual Learning", "abstract": "Through sequential construction of posteriors on observing data online, Bayes\u2019 theorem provides a natural framework for continual learning. We develop Variational Auto-Regressive Gaussian Processes..."}}
{"id": "VE9HQWtxu-", "cdate": 1609459200000, "mdate": 1680040894432, "content": {"title": "When are Iterative Gaussian Processes Reliably Accurate?", "abstract": ""}}
