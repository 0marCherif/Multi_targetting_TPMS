{"id": "iTFqu71HZV", "cdate": 1640995200000, "mdate": 1682379544157, "content": {"title": "Using Interactive Feedback to Improve the Accuracy and Explainability of Question Answering Systems Post-Deployment", "abstract": "Most research on question answering focuses on the pre-deployment stage; i.e., building an accurate model for deployment. In this paper, we ask the question: Can we improve QA systems further \\emph{post-}deployment based on user interactions? We focus on two kinds of improvements: 1) improving the QA system's performance itself, and 2) providing the model with the ability to explain the correctness or incorrectness of an answer. We collect a retrieval-based QA dataset, FeedbackQA, which contains interactive feedback from users. We collect this dataset by deploying a base QA system to crowdworkers who then engage with the system and provide feedback on the quality of its answers. The feedback contains both structured ratings and unstructured natural language explanations. We train a neural model with this feedback data that can generate explanations and re-score answer candidates. We show that feedback data not only improves the accuracy of the deployed QA system but also other stronger non-deployed systems. The generated explanations also help users make informed decisions about the correctness of answers. Project page: https://mcgill-nlp.github.io/feedbackqa/"}}
{"id": "d6Oq6krZfQD", "cdate": 1640995200000, "mdate": 1652156579702, "content": {"title": "Text Revision by On-the-Fly Representation Optimization", "abstract": "Text revision refers to a family of natural language generation tasks, where the source and target sequences share moderate resemblance in surface form but differentiate in attributes, such as text formality and simplicity. Current state-of-the-art methods formulate these tasks as sequence-to-sequence learning problems, which rely on large-scale parallel training corpus. In this paper, we present an iterative in-place editing approach for text revision, which requires no parallel data. In this approach, we simply fine-tune a pre-trained Transformer with masked language modeling and attribute classification. During inference, the editing at each iteration is realized by two-step span replacement. At the first step, the distributed representation of the text optimizes on the fly towards an attribute function. At the second step, a text span is masked and another new one is proposed conditioned on the optimized representation. The empirical experiments on two typical and important text revision tasks, text formalization and text simplification, show the effectiveness of our approach. It achieves competitive and even better performance than state-of-the-art supervised methods on text simplification, and gains better performance than strong unsupervised methods on text formalization \\footnote{Code and model are available at \\url{https://github.com/jingjingli01/OREO}}."}}
{"id": "aMhW9UGUIR", "cdate": 1640995200000, "mdate": 1682379544192, "content": {"title": "Using Interactive Feedback to Improve the Accuracy and Explainability of Question Answering Systems Post-Deployment", "abstract": ""}}
{"id": "8ByV0geiNWe", "cdate": 1640995200000, "mdate": 1663743090284, "content": {"title": "Text Revision By On-the-Fly Representation Optimization", "abstract": "Text revision refers to a family of natural language generation tasks, where the source and target sequences share moderate resemblance in surface form but differentiate in attributes, such as text formality and simplicity. Current state-of-the-art methods formulate these tasks as sequence-to-sequence learning problems, which rely on large-scale parallel training corpus. In this paper, we present an iterative in-place editing approach for text revision, which requires no parallel data. In this approach, we simply fine-tune a pre-trained Transformer with masked language modeling and attribute classification. During inference, the editing at each iteration is realized by two-step span replacement. At the first step, the distributed representation of the text optimizes on the fly towards an attribute function. At the second step, a text span is masked and another new one is proposed conditioned on the optimized representation. The empirical experiments on two typical and important text revision tasks, text formalization and text simplification, show the effectiveness of our approach. It achieves competitive and even better performance than state-of-the-art supervised methods on text simplification, and gains better performance than strong unsupervised methods on text formalization. Our code and model are released at https://github.com/jingjingli01/OREO."}}
{"id": "r-ZTBJWVZ0n", "cdate": 1546300800000, "mdate": 1634590535069, "content": {"title": "EditNTS: An Neural Programmer-Interpreter Model for Sentence Simplification through Explicit Editing", "abstract": "Yue Dong, Zichao Li, Mehdi Rezagholizadeh, Jackie Chi Kit Cheung. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019."}}
{"id": "UgtLifmFII", "cdate": 1546300800000, "mdate": 1634590535106, "content": {"title": "EditNTS: An Neural Programmer-Interpreter Model for Sentence Simplification through Explicit Editing", "abstract": "We present the first sentence simplification model that learns explicit edit operations (ADD, DELETE, and KEEP) via a neural programmer-interpreter approach. Most current neural sentence simplification systems are variants of sequence-to-sequence models adopted from machine translation. These methods learn to simplify sentences as a byproduct of the fact that they are trained on complex-simple sentence pairs. By contrast, our neural programmer-interpreter is directly trained to predict explicit edit operations on targeted parts of the input sentence, resembling the way that humans might perform simplification and revision. Our model outperforms previous state-of-the-art neural sentence simplification models (without external knowledge) by large margins on three benchmark text simplification corpora in terms of SARI (+0.95 WikiLarge, +1.89 WikiSmall, +1.41 Newsela), and is judged by humans to produce overall better and simpler output sentences."}}
