{"id": "wRPGd9Mt-ac", "cdate": 1672531200000, "mdate": 1681760635736, "content": {"title": "VNE: An Effective Method for Improving Deep Representation by Manipulating Eigenvalue Distribution", "abstract": "Since the introduction of deep learning, a wide scope of representation properties, such as decorrelation, whitening, disentanglement, rank, isotropy, and mutual information, have been studied to improve the quality of representation. However, manipulating such properties can be challenging in terms of implementational effectiveness and general applicability. To address these limitations, we propose to regularize von Neumann entropy~(VNE) of representation. First, we demonstrate that the mathematical formulation of VNE is superior in effectively manipulating the eigenvalues of the representation autocorrelation matrix. Then, we demonstrate that it is widely applicable in improving state-of-the-art algorithms or popular benchmark algorithms by investigating domain-generalization, meta-learning, self-supervised learning, and generative models. In addition, we formally establish theoretical connections with rank, disentanglement, and isotropy of representation. Finally, we provide discussions on the dimension control of VNE and the relationship with Shannon entropy. Code is available at: https://github.com/jaeill/CVPR23-VNE."}}
{"id": "oarh34UfVZ", "cdate": 1672531200000, "mdate": 1681760635638, "content": {"title": "Meta-Learning with a Geometry-Adaptive Preconditioner", "abstract": "Model-agnostic meta-learning (MAML) is one of the most successful meta-learning algorithms. It has a bi-level optimization structure where the outer-loop process learns a shared initialization and the inner-loop process optimizes task-specific weights. Although MAML relies on the standard gradient descent in the inner-loop, recent studies have shown that controlling the inner-loop's gradient descent with a meta-learned preconditioner can be beneficial. Existing preconditioners, however, cannot simultaneously adapt in a task-specific and path-dependent way. Additionally, they do not satisfy the Riemannian metric condition, which can enable the steepest descent learning with preconditioned gradient. In this study, we propose Geometry-Adaptive Preconditioned gradient descent (GAP) that can overcome the limitations in MAML; GAP can efficiently meta-learn a preconditioner that is dependent on task-specific parameters, and its preconditioner can be shown to be a Riemannian metric. Thanks to the two properties, the geometry-adaptive preconditioner is effective for improving the inner-loop optimization. Experiment results show that GAP outperforms the state-of-the-art MAML family and preconditioned gradient descent-MAML (PGD-MAML) family in a variety of few-shot learning tasks. Code is available at: https://github.com/Suhyun777/CVPR23-GAP."}}
{"id": "Q5vGMRmQ6PP", "cdate": 1672531200000, "mdate": 1681760635892, "content": {"title": "DR.CPO: Diversified and Realistic 3D Augmentation via Iterative Construction, Random Placement, and HPR Occlusion", "abstract": "In autonomous driving, data augmentation is commonly used for improving 3D object detection. The most basic methods include insertion of copied objects and rotation and scaling of the entire training frame. Numerous variants have been developed as well. The existing methods, however, are considerably limited when compared to the variety of the real world possibilities. In this work, we develop a diversified and realistic augmentation method that can flexibly construct a whole-body object, freely locate and rotate the object, and apply self-occlusion and external-occlusion accordingly. To improve the diversity of the whole-body object construction, we develop an iterative method that stochastically combines multiple objects observed from the real world into a single object. Unlike the existing augmentation methods, the constructed objects can be randomly located and rotated in the training frame because proper occlusions can be reflected to the whole-body objects in the final step. Finally, proper self-occlusion at each local object level and external-occlusion at the global frame level are applied using the Hidden Point Removal (HPR) algorithm that is computationally efficient. HPR is also used for adaptively controlling the point density of each object according to the object's distance from the LiDAR. Experiment results show that the proposed DR.CPO algorithm is data-efficient and model-agnostic without incurring any computational overhead. Also, DR.CPO can improve mAP performance by 2.08% when compared to the best 3D detection result known for KITTI dataset. The code is available at https://github.com/SNU-DRL/DRCPO.git"}}
{"id": "KKLtjZV0kTS", "cdate": 1672531200000, "mdate": 1681760811849, "content": {"title": "Deep Learning Framework With Essential Pre-Processing Techniques for Improving Mixed-Gas Concentration Prediction", "abstract": "Multiple gas detection in mixed-gas environments is a challenging issue in many engineering industries because some of the gases can raise defect rates and reduce production efficiency. For chemo-resistive gas sensors, a precise estimation can be challenging because of the measurement variance and non-linear nature of the gas sensors, especially in a low concentration environment. A simple application of the deep learning models, however, does not yield sufficiently accurate predictions of the concentrations of multiple gases in gas mixtures; thus, it is essential to develop basic strategies for enhancing the accuracy in all possible ways. In this study, we develop a deep learning framework for achieving high accuracy of gas concentration prediction by studying the essential pre-processing techniques, learning task design, and architecture design. For the pre-processing, we study several aspects of processing time-series sensor data and identify the key techniques for complementing deep learning models\u2019 limitations. We utilize the mixed-gas nature for the learning task design and show that multi-task learning can generate a synergistic effect. Additionally, we show that a further improvement is possible by considering on-off classification as a part of the hybrid learning task. Concerning architecture design, we investigate Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN) models after applying the identified pre-processing techniques. CNN outperformed other models in a joint analysis with the learning task. The effectiveness of our framework is confirmed with the UCI gas mixture dataset acquired using a chemical detection platform where 16 chemical sensors are exposed to ethylene, CO, and methane gases. Using the dataset, we study the basic techniques that can be effective to mixed-gas prediction. For the UCI dataset, our deep learning framework achieves a significant improvement in estimation accuracy when compared to the previous studies."}}
{"id": "mencEcrobES", "cdate": 1663850272882, "mdate": null, "content": {"title": "GAML: geometry-aware meta-learning via a fully adaptive preconditioner", "abstract": "Model-Agnostic Meta-Learning (MAML) is one of the most successful meta-learning algorithms. It has a bi-level optimization structure, where the outer-loop process learns the shared initialization and the inner-loop process optimizes the task-specific weights. Although MAML relies on the standard gradient descent in the inner-loop, recent works have shown that it can be beneficial to control the inner loop's gradient descent with a meta-learned preconditioner. The existing preconditioners, however, cannot adapt in a task-specific and path-dependent way at the same time. Also, most of them do not consider the geometry of the loss surface. In this work, we propose Geometry-Aware Meta-Learning (GAML) that can overcome the limitations. GAML can efficiently meta-learn a preconditioner that is dependent on the task-specific parameters and its preconditioner can be shown to be a Riemannian metric that defines the geometry of the loss surface. Therefore, we can perform a fully-adaptive and geometry-aware optimization in the inner-loop. Experiment results show that GAML outperforms the state-of-the-art MAML family and PGD-MAML family for a variety of few-shot learning tasks."}}
{"id": "6FAWzRMRk7A", "cdate": 1663849947519, "mdate": null, "content": {"title": "Correcting Three Existing Beliefs on Mutual Information in Contrastive Learning", "abstract": "Contrastive learning has played a pivotal role in the recent success of unsupervised representation learning. It has been commonly explained with instance discrimination and a mutual information loss, and some of the fundamental explanations are based on mutual information analysis. In this work, we develop new methods that enable rigorous analysis of mutual information in contrastive learning. Using the methods, we investigate three existing beliefs and show that they are incorrect. Based on the investigation results, we address two issues in the discussion section. In particular, we question if contrastive learning is indeed an unsupervised representation learning method because the current framework of contrastive learning relies on validation performance for tuning the augmentation design."}}
{"id": "tP87Wy3Pdh", "cdate": 1640995200000, "mdate": 1681760811656, "content": {"title": "AID-Purifier: A Light Auxiliary Network for Boosting Adversarial Defense", "abstract": "In this study, we propose AID-Purifier that can boost the robustness of adversarially-trained networks by purifying their inputs. AID-Purifier is an auxiliary network that works as an add-on to an already trained main classifier. To keep it computationally light, it is trained as a discriminator with a binary cross-entropy loss. To obtain additionally useful information from the adversarial examples, the architecture design is closely related to the information maximization principle where two layers of the main classification network are piped into the auxiliary network. To assist the iterative optimization procedure of purification, the auxiliary network is trained with AVmixup. AID-Purifier can be also used together with other purifiers such as PixelDefend for an extra enhancement. Because input purification has been studied relative less when compared to adversarial training or gradient masking, we conduct extensive attack experiments to validate AID-Purifier\u2019s robustness. The overall results indicate that the best performing adversarially-trained networks can be enhanced further with AID-Purifier. The code is available in https://github.com/yelobean/AIDPurifier."}}
{"id": "qwMvuax3FYu", "cdate": 1640995200000, "mdate": 1681760635609, "content": {"title": "Semi-Siamese Bi-encoder Neural Ranking Model Using Lightweight Fine-Tuning", "abstract": "A BERT-based Neural Ranking Model (NRM) can be either a cross-encoder or a bi-encoder. Between the two, bi-encoder is highly efficient because all the documents can be pre-processed before the actual query time. In this work, we show two approaches for improving the performance of BERT-based bi-encoders. The first approach is to replace the full fine-tuning step with a lightweight fine-tuning. We examine lightweight fine-tuning methods that are adapter-based, prompt-based, and hybrid of the two. The second approach is to develop semi-Siamese models where queries and documents are handled with a limited amount of difference. The limited difference is realized by learning two lightweight fine-tuning modules, where the main language model of BERT is kept common for both query and document. We provide extensive experiment results for monoBERT, TwinBERT, and ColBERT where three performance metrics are evaluated over Robust04, ClueWeb09b, and MS-MARCO datasets. The results confirm that both lightweight fine-tuning and semi-Siamese are considerably helpful for improving BERT-based bi-encoders. In fact, lightweight fine-tuning is helpful for cross-encoder, too.1"}}
{"id": "iXoKQ6-thDq", "cdate": 1640995200000, "mdate": 1681760635758, "content": {"title": "B2EA: An Evolutionary Algorithm Assisted by Two Bayesian Optimization Modules for Neural Architecture Search", "abstract": "The early pioneering Neural Architecture Search (NAS) works were multi-trial methods applicable to any general search space. The subsequent works took advantage of the early findings and developed weight-sharing methods that assume a structured search space typically with pre-fixed hyperparameters. Despite the amazing computational efficiency of the weight-sharing NAS algorithms, it is becoming apparent that multi-trial NAS algorithms are also needed for identifying very high-performance architectures, especially when exploring a general search space. In this work, we carefully review the latest multi-trial NAS algorithms and identify the key strategies including Evolutionary Algorithm (EA), Bayesian Optimization (BO), diversification, input and output transformations, and lower fidelity estimation. To accommodate the key strategies into a single framework, we develop B2EA that is a surrogate assisted EA with two BO surrogate models and a mutation step in between. To show that B2EA is robust and efficient, we evaluate three performance metrics over 14 benchmarks with general and cell-based search spaces. Comparisons with state-of-the-art multi-trial algorithms reveal that B2EA is robust and efficient over the 14 benchmarks for three difficulty levels of target performance. The B2EA code is publicly available at \\url{https://github.com/snu-adsl/BBEA}."}}
{"id": "iQ0llmAyxJs", "cdate": 1640995200000, "mdate": 1681760811746, "content": {"title": "Finding Inverse Document Frequency Information in BERT", "abstract": "For many decades, BM25 and its variants have been the dominant document retrieval approach, where their two underlying features are Term Frequency (TF) and Inverse Document Frequency (IDF). The traditional approach, however, is being rapidly replaced by Neural Ranking Models (NRMs) that can exploit semantic features. In this work, we consider BERT-based NRMs and study if IDF information is present in the NRMs. This simple question is interesting because IDF has been indispensable for the traditional lexical matching, but global features like IDF are not explicitly learned by neural language models including BERT. We adopt linear probing as the main analysis tool because typical BERT based NRMs utilize linear or inner-product based score aggregators. We analyze input embeddings, representations of all BERT layers, and the self-attention weights of CLS. By studying MS-MARCO dataset with three BERT-based models, we show that all of them contain information that is strongly dependent on IDF."}}
