{"id": "JKrxk5a9vE", "cdate": 1672531200000, "mdate": 1681735579072, "content": {"title": "A survey of evolutionary algorithms for supervised ensemble learning", "abstract": "This paper presents a comprehensive review of evolutionary algorithms that learn an ensemble of predictive models for supervised machine learning (classification and regression). We propose a detailed four-level taxonomy of studies in this area. The first level of the taxonomy categorizes studies based on which stage of the ensemble learning process is addressed by the evolutionary algorithm: the generation of base models, model selection, or the integration of outputs. The next three levels of the taxonomy further categorize studies based on methods used to address each stage. In addition, we categorize studies according to the main types of objectives optimized by the evolutionary algorithm, the type of base learner used and the type of evolutionary algorithm used. We also discuss controversial topics, like the pros and cons of the selection stage of ensemble learning, and the need for using a diversity measure for the ensemble\u2019s members in the fitness function. Finally, as conclusions, we summarize our findings about patterns in the frequency of use of different methods and suggest several new research directions for evolutionary ensemble learning."}}
{"id": "xRcg-cdmL8", "cdate": 1640995200000, "mdate": 1681735578759, "content": {"title": "How Resilient Are Imitation Learning Methods to Sub-optimal Experts?", "abstract": "Imitation Learning (IL) algorithms try to mimic expert behavior in order to be capable of performing specific tasks, but it remains unclear what those strategies could achieve when learning from sub-optimal data (faulty experts). Studying how Imitation Learning approaches learn when dealing with different degrees of quality from the observations can benefit tasks such as optimizing data collection, producing interpretable models, reducing bias when using sub-optimal experts, and more. Therefore, in this work we provide extensive experiments to verify how different Imitation Learning agents perform under various degrees of expert optimality. We experiment with four IL algorithms, three of them that learn self-supervisedly and one that uses the ground-truth labels (BC) in four different environments (tasks), and we compare them using optimal and sub-optimal experts. For assessing the performance of each agent, we compute two metrics: Performance and Average Episodic Reward. Our experiments show that IL approaches that learn self-supervisedly are relatively resilient to sub-optimal experts, which is not the case of the supervised approach. We also observe that sub-optimal experts are sometimes beneficial since they seem to act as a kind of regularization method, preventing models from data overfitting. You can replicate our experiments by using the code in our GitHub ( https://github.com/NathanGavenski/How-resilient-IL-methods-are )."}}
{"id": "VNv4vkT_xo", "cdate": 1640995200000, "mdate": 1681735579079, "content": {"title": "Leveraging Textual Descriptions for House Price Valuation", "abstract": "Real estate valuation has been vastly studied by the research community, with several articles proposing Automated Valuation Models (AVM). However, most of those models base their estimates only on geographic location and structural characteristics of the property, disregarding several factors that influence prices, such as the need for repairs and sun exposure. To support decision making, an AVM needs to \u201clook\u201d for the same type of information a person would when valuating a property, including photos and textual descriptions. In this work, we show that the usage of textual data can significantly increase the performance of house price-prediction models. Our experiments explore different combinations of learning algorithms and methods to extract relevant information from textual descriptions, with some surprising conclusions regarding the best combination of approaches. Overall, we shed some light on how textual features can be leveraged by the models, explaining the paths that lead to predictions that end up resulting in performance gains."}}
{"id": "K8W69EF9Blv", "cdate": 1640995200000, "mdate": 1681735578722, "content": {"title": "Debiasing Methods for Fairer Neural Models in Vision and Language Research: A Survey", "abstract": "Despite being responsible for state-of-the-art results in several computer vision and natural language processing tasks, neural networks have faced harsh criticism due to some of their current shortcomings. One of them is that neural networks are correlation machines prone to model biases within the data instead of focusing on actual useful causal relationships. This problem is particularly serious in application domains affected by aspects such as race, gender, and age. To prevent models from incurring on unfair decision-making, the AI community has concentrated efforts in correcting algorithmic biases, giving rise to the research area now widely known as fairness in AI. In this survey paper, we provide an in-depth overview of the main debiasing methods for fairness-aware neural networks in the context of vision and language research. We propose a novel taxonomy to better organize the literature on debiasing methods for fairness, and we discuss the current challenges, trends, and important future work directions for the interested researcher and practitioner."}}
{"id": "A0ms-EvV7X", "cdate": 1640995200000, "mdate": 1668021831108, "content": {"title": "COBE: A Natural Language Code Search Robustness Benchmark", "abstract": "Benchmark frameworks and datasets allow us to analyze and understand the knowledge that NLP models capture about the world they were trained on. Several Transformer models have recently been adapted to code-related tasks such as code search, in which the goal is to find the most semantically relevant code given a query written in natural language. To achieve satisfactory performance, the retrieval models heavily rely on the quality of the query. In this paper, we introduce the Natural Language Code Search Robustness Benchmark (COBE), which provides a more holistic evaluation of the state-of-the-art models considering several aspects of the retrieval models, such as: (i) retrieval capabilities measured in multiple ranking metrics; (ii) robustness to a plethora of input perturbations; (iii) efficiency in terms of training and retrieval times; and (iv) stability across fine-tuning runs. We shed a light over important questions showing that simply computing performance-based retrieval metrics does not suffice to evaluate this kind of model. The proposed benchmark introduces novel metrics and measurement strategies that allow a rigorous quantitative analysis of input-query robustness while providing an understanding of model generalization behavior. We perform an extensive set of experiments using state-of-the-art models such as CodeBert, GraphCodeBert, and CodeT5. Those models are fine-tuned over many different scenarios in six programming languages. Several models trained in this study outperform their state-of-the-art counterparts, which provides evidence that the standard fine-tuning approach used in code search related work is sub-optimal. The proposed benchmark is a powerful tool to evaluate code search models, providing insights on how they behave during fine-tuning and how they are interpreting the input queries."}}
{"id": "8wsBunjGXij", "cdate": 1640995200000, "mdate": 1668021831092, "content": {"title": "Efficient Counterfactual Debiasing for Visual Question Answering", "abstract": "Despite the success of neural architectures for Visual Question Answering (VQA), several recent studies have shown that VQA models are mostly driven by superficial correlations that are learned by exploiting undesired priors within training datasets. They often lack sufficient image grounding or tend to overly-rely on textual information, failing to capture knowledge from the images. This affects their generalization to test sets with slight changes in the distribution of facts. To address such an issue, some bias mitigation methods have relied on new training procedures that are capable of synthesizing counterfactual samples by masking critical objects within the images, and words within the questions, while also changing the corresponding ground truth. We propose a novel model-agnostic counterfactual training procedure, namely Efficient Counterfactual Debiasing (ECD), in which we introduce a new negative answer-assignment mechanism that exploits the probability distribution of the answers based on their frequencies, as well as an improved counterfactual sample synthesizer. Our experiments demonstrate that ECD is a simple, computationally-efficient counterfactual sample-synthesizer training procedure that establishes itself as the new state of the art for unbiased VQA."}}
{"id": "wsWsruHa0AA", "cdate": 1620667705409, "mdate": null, "content": {"title": "Language-agnostic visual-semantic embeddings", "abstract": "This paper proposes a framework for training language-invariant cross-modal retrieval models. We also introduce a novel character-based word-embedding approach, allowing the model to project similar words across languages into the same word-embedding space. In addition, by performing cross-modal retrieval at the character level, the storage requirements for a text encoder decrease substantially, allowing for lighter and more scalable retrieval architectures. The proposed language-invariant textual encoder based on characters is virtually unaffected in terms of storage requirements when novel languages are added to the system. Our contributions include new methods for building character-level-based word-embeddings, an improved loss function, and a novel cross-language alignment module that not only makes the architecture language-invariant, but also presents better predictive performance. We show that our models outperform the current state-of-the-art in both single and multi-language scenarios. This work can be seen as the basis of a new path on retrieval research, now allowing for the effective use of captions in multiple-language scenarios. Code is available at https://github. com/jwehrmann/lavse."}}
{"id": "GoEXeq6T8_", "cdate": 1620667459197, "mdate": null, "content": {"title": "Adaptive cross-modal embeddings for image-text alignment", "abstract": "In this paper, we introduce a novel approach for training image-text alignment models, namely ADAPT. Image-text\nalignment methods are often used for cross-modal retrieval, i.e., to retrieve an image given a query text, or captions that\nsuccessfully label an image. ADAPT is designed to adjust an intermediate representation of instances from a modality\na using an embedding vector of an instance from modality b. Such an adaptation is designed to filter and enhance important information across internal features, allowing for guided vector representations \u2013 which resembles the working of attention modules, though far more computationally efficient. Experimental results on two large-scale Image-Text alignment datasets show that ADAPT-models outperform all the baseline approaches by large margins. Particularly, for Image Retrieval, ADAPT, with a single model, outperforms the state-of-the-art approach by a relative improvement of R@1 \u2248 24% and for Image Annotation, R@1 \u2248 8% on Flickr30k dataset. On MS COCO it provides an improvement of R@1 \u2248 12% for Image Retrieval, and \u2248 7% R@1 for Image Annotation. Code is available at https://github.com/jwehrmann/retrieval.pytorch."}}
{"id": "wgposwO2pyL", "cdate": 1609459200000, "mdate": 1668021831165, "content": {"title": "An extensive experimental evaluation of automated machine learning methods for recommending classification algorithms", "abstract": "This paper presents an experimental comparison among four automated machine learning (AutoML) methods for recommending the best classification algorithm for a given input dataset. Three of these methods are based on evolutionary algorithms (EAs), and the other is Auto-WEKA, a well-known AutoML method based on the combined algorithm selection and hyper-parameter optimisation (CASH) approach. The EA-based methods build classification algorithms from a single machine learning paradigm: either decision-tree induction, rule induction, or Bayesian network classification. Auto-WEKA combines algorithm selection and hyper-parameter optimisation to recommend classification algorithms from multiple paradigms. We performed controlled experiments where these four AutoML methods were given the same runtime limit for different values of this limit. In general, the difference in predictive accuracy of the three best AutoML methods was not statistically significant. However, the EA evolving decision-tree induction algorithms has the advantage of producing algorithms that generate interpretable classification models and that are more scalable to large datasets, by comparison with many algorithms from other learning paradigms that can be recommended by Auto-WEKA. We also observed that Auto-WEKA has shown meta-overfitting, a form of overfitting at the meta-learning level, rather than at the base-learning level."}}
{"id": "Ar-OjDAaXBs", "cdate": 1609459200000, "mdate": 1668021831057, "content": {"title": "Model Compression in Object Detection", "abstract": "Compressed neural-network models have a growing relevance in the Deep Learning literature, since they allow the deployment of AI on devices with computational constraints for many automation purposes. Despite the amount and diversity of work for this purpose, there is no standard benchmark in the literature, and it is often difficult to choose the proper approach due to the large difference between models, datasets, and training details that are tested. Therefore, this paper proposes a standard experimental benchmark for different model compression approaches for the object detection task, using a fixed model (the well-known YOLOv3) and training scheme. Between Pruning, Knowledge Distillation, and Neural Architecture Search, our experiments reveal that the best trade-off is by using pruning, which enables the creation of a model with 80.67% mAP of the original model but removing 98.8% of the parameters, 96.53% of the Multiply-Accumulate Operations, and reducing the storage size from 235.44MB to 11.61MB."}}
