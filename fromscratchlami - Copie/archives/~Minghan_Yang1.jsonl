{"id": "kfARqAvUuVt", "cdate": 1682899200000, "mdate": 1682333578214, "content": {"title": "An Efficient Fisher Matrix Approximation Method for Large-Scale Neural Network Optimization", "abstract": "Although the shapes of the parameters are not crucial for designing first-order optimization methods in large scale empirical risk minimization problems, they have important impact on the size of the matrix to be inverted when developing second-order type methods. In this article, we propose an efficient and novel second-order method based on the parameters in the real matrix space <inline-formula><tex-math notation=\"LaTeX\">$\\mathbb {R}^{m\\times n}$</tex-math></inline-formula> and a matrix-product approximate Fisher matrix (MatFisher) by using the products of gradients. The size of the matrix to be inverted is much smaller than that of the Fisher information matrix in the real vector space <inline-formula><tex-math notation=\"LaTeX\">$\\mathbb {R}^{d}$</tex-math></inline-formula> . Moreover, by utilizing the matrix delayed update and the block diagonal approximation techniques, the computational cost can be controlled and is comparable with first-order methods. A global convergence and a superlinear local convergence analysis are established under mild conditions. Numerical results on image classification with ResNet50, quantum chemistry modeling with SchNet, and data-driven partial differential equations solution with PINN illustrate that our method is quite competitive to the state-of-the-art methods."}}
{"id": "r2xahpITLbz", "cdate": 1640995200000, "mdate": 1673795046071, "content": {"title": "A stochastic extra-step quasi-Newton method for nonsmooth nonconvex optimization", "abstract": ""}}
{"id": "Q4eyIxl-JLL", "cdate": 1640995200000, "mdate": 1673795046076, "content": {"title": "Sketch-Based Empirical Natural Gradient Methods for Deep Learning", "abstract": ""}}
{"id": "LI5VSIp8SkH", "cdate": 1640995200000, "mdate": 1671847091526, "content": {"title": "Riemannian Natural Gradient Methods", "abstract": "This paper studies large-scale optimization problems on Riemannian manifolds whose objective function is a finite sum of negative log-probability losses. Such problems arise in various machine learning and signal processing applications. By introducing the notion of Fisher information matrix in the manifold setting, we propose a novel Riemannian natural gradient method, which can be viewed as a natural extension of the natural gradient method from the Euclidean setting to the manifold setting. We establish the almost-sure global convergence of our proposed method under standard assumptions. Moreover, we show that if the loss function satisfies certain convexity and smoothness conditions and the input-output map satisfies a Riemannian Jacobian stability condition, then our proposed method enjoys a local linear -- or, under the Lipschitz continuity of the Riemannian Jacobian of the input-output map, even quadratic -- rate of convergence. We then prove that the Riemannian Jacobian stability condition will be satisfied by a two-layer fully connected neural network with batch normalization with high probability, provided that the width of the network is sufficiently large. This demonstrates the practical relevance of our convergence rate result. Numerical experiments on applications arising from machine learning demonstrate the advantages of the proposed method over state-of-the-art ones."}}
{"id": "xMVSYFf1b2J", "cdate": 1609459200000, "mdate": 1648698140581, "content": {"title": "NG+ : A Multi-Step Matrix-Product Natural Gradient Method for Deep Learning", "abstract": "In this paper, a novel second-order method called NG+ is proposed. By following the rule ``the shape of the gradient equals the shape of the parameter\", we define a generalized fisher information matrix (GFIM) using the products of gradients in the matrix form rather than the traditional vectorization. Then, our generalized natural gradient direction is simply the inverse of the GFIM multiplies the gradient in the matrix form. Moreover, the GFIM and its inverse keeps the same for multiple steps so that the computational cost can be controlled and is comparable with the first-order methods. A global convergence is established under some mild conditions and a regret bound is also given for the online learning setting. Numerical results on image classification with ResNet50, quantum chemistry modeling with Schnet, neural machine translation with Transformer and recommendation system with DLRM illustrate that GN+ is competitive with the state-of-the-art methods."}}
{"id": "2ZVVV0IT-3m", "cdate": 1609459200000, "mdate": 1648698140581, "content": {"title": "Enhance Curvature Information by Structured Stochastic Quasi-Newton Methods", "abstract": "In this paper, we consider stochastic second-order methods for minimizing a finite summation of nonconvex functions. One important key is to find an ingenious but cheap scheme to incorporate local curvature information. Since the true Hessian matrix is often a combination of a cheap part and an expensive part, we propose a structured stochastic quasi-Newton method by using partial Hessian information as much as possible. By further exploiting either the low-rank structure or the Kronecker-product properties of the quasi-Newton approximations, the computation of the quasi-Newton direction is affordable. Global convergence to stationary point and local superlinear convergence rate are established under some mild assumptions. Numerical results on logistic regression, deep autoencoder networks and deep convolutional neural networks show that our proposed method is quite competitive to the state-of-the-art methods."}}
