{"id": "ZyexGgrswiB", "cdate": 1672531200000, "mdate": 1683770062669, "content": {"title": "On Finite Blocklength Lossy Source Coding", "abstract": "In this monograph, we review recent advances in second-order asymptotics for lossy source coding, which provides approximations to the finite blocklength performance of optimal codes. The monograph is divided into three parts. In part I, we motivate the monograph, present basic definitions, introduce mathematical tools and illustrate the motivation of non-asymptotic and second-order asymptotics via the example of lossless source coding. In part II, we first present existing results for the rate-distortion problem with proof sketches. Subsequently, we present five generations of the rate-distortion problem to tackle various aspects of practical quantization tasks: noisy source, noisy channel, mismatched code, Gauss-Markov source and fixed-to-variable length compression. By presenting theoretical bounds for these settings, we illustrate the effect of noisy observation of the source, the influence of noisy transmission of the compressed information, the effect of using a fixed coding scheme for an arbitrary source and the roles of source memory and variable rate. In part III, we present four multiterminal generalizations of the rate-distortion problem to consider multiple encoders, decoders or source sequences: the Kaspi problem, the successive refinement problem, the Fu-Yeung problem and the Gray-Wyner problem. By presenting theoretical bounds for these multiterminal problems, we illustrate the role of side information, the optimality of stop and transmit, the effect of simultaneous lossless and lossy compression, and the tradeoff between encoders' rates in compressing correlated sources. Finally, we conclude the monograph, mention related results and discuss future directions."}}
{"id": "VmJJUxhVB7l", "cdate": 1672531200000, "mdate": 1683770062773, "content": {"title": "Learning, Fast and Slow: A Goal-Directed Memory-Based Approach for Dynamic Environments", "abstract": "Model-based next state prediction and state value prediction are slow to converge. To address these challenges, we do the following: i) Instead of a neural network, we do model-based planning using a parallel memory retrieval system (which we term the slow mechanism); ii) Instead of learning state values, we guide the agent's actions using goal-directed exploration, by using a neural network to choose the next action given the current state and the goal state (which we term the fast mechanism). The goal-directed exploration is trained online using hippocampal replay of visited states and future imagined states every single time step, leading to fast and efficient training. Empirical studies show that our proposed method has a 92% solve rate across 100 episodes in a dynamically changing grid world, significantly outperforming state-of-the-art actor critic mechanisms such as PPO (54%), TRPO (50%) and A2C (24%). Ablation studies demonstrate that both mechanisms are crucial. We posit that the future of Reinforcement Learning (RL) will be to model goals and sub-goals for various tasks, and plan it out in a goal-directed memory-based approach."}}
{"id": "8Hs63VheHy4", "cdate": 1672531200000, "mdate": 1683770062675, "content": {"title": "Local Intrinsic Dimensional Entropy", "abstract": "Most entropy measures depend on the spread of the probability distribution over the sample space $\\mathcal{X}$, and the maximum entropy achievable scales proportionately with the sample space cardinality $|\\mathcal{X}|$. For a finite $|\\mathcal{X}|$, this yields robust entropy measures which satisfy many important properties, such as invariance to bijections, while the same is not true for continuous spaces (where $|\\mathcal{X}|=\\infty$). Furthermore, since $\\mathbb{R}$ and $\\mathbb{R}^d$ ($d\\in \\mathbb{Z}^+$) have the same cardinality (from Cantor's correspondence argument), cardinality-dependent entropy measures cannot encode the data dimensionality. In this work, we question the role of cardinality and distribution spread in defining entropy measures for continuous spaces, which can undergo multiple rounds of transformations and distortions, e.g., in neural networks. We find that the average value of the local intrinsic dimension of a distribution, denoted as ID-Entropy, can serve as a robust entropy measure for continuous spaces, while capturing the data dimensionality. We find that ID-Entropy satisfies many desirable properties and can be extended to conditional entropy, joint entropy and mutual-information variants. ID-Entropy also yields new information bottleneck principles and also links to causality. In the context of deep learning, for feedforward architectures, we show, theoretically and empirically, that the ID-Entropy of a hidden layer directly controls the generalization gap for both classifiers and auto-encoders, when the target function is Lipschitz continuous. Our work primarily shows that, for continuous spaces, taking a structural rather than a statistical approach yields entropy measures which preserve intrinsic data dimensionality, while being relevant for studying various architectures."}}
{"id": "0qowBhUMmc-", "cdate": 1672531200000, "mdate": 1683770062669, "content": {"title": "Multiple Task Resource Allocation Considering QoS in Energy Harvesting Systems", "abstract": "Most approaches to resource allocation in energy harvesting systems for Internet-of-Things (IoT) networks do not consider real-time periodic task allocation with Quality of Service (QoS). This article studies the offline 2-D optimization problem for allocating randomly harvested energy flowing causally between slots into different real-time periodic tasks on an IoT device. We formulate an energy allocation problem, for tasks with different energy costs and requested QoS, which aims to maximize a convex reward function subject to energy causality (EC), energy saturation (ES) and task executable (TE) constraints within a certain length of time. We decouple the optimization problem into two subproblems after analysis. First, we propose a novel method to allocate energy to slots based on the Karush\u2013Kuhn\u2013Tucher conditions only considering the EC & ES constraints. The proposed method outperforms the state-of-the-art \u201cTunnel Policy,\u201d based on geometric programming. Next, an adaption is made to satisfy the TE constraints by allocating in the original constant power slots directly without iteratively checking the wasted energy. Finally, the energy already allocated to slots is put into tasks to complete the 2-D allocation. The effectiveness of the proposed methods and task framework are validated by extensive experiments."}}
{"id": "OPGy07PojsZ", "cdate": 1663850523427, "mdate": null, "content": {"title": "Rethinking Symbolic Regression: Morphology and Adaptability in the Context of Evolutionary Algorithms", "abstract": "Symbolic Regression (SR) is the well-studied problem of finding closed-form analytical expressions that describe the relationship between variables in a measurement dataset. In this paper, we rethink SR from two perspectives: morphology and adaptability. Morphology: Current SR algorithms typically use several man-made heuristics to influence the morphology (or structure) of the expressions in the search space. These man-made heuristics may introduce unintentional bias and data leakage, especially with the relatively few equation-recovery benchmark problems available for evaluating SR approaches. To address this, we formulate a novel minimalistic approach, based on constructing a depth-aware mathematical language model trained on terminal walks of expression trees, as a replacement to these heuristics. Adaptability: Current SR algorithms tend to select expressions based on only a single fitness function (e.g., MSE on the training set). We promote the use of an adaptability framework in evolutionary SR which uses fitness functions that alternate across generations. This leads to robust expressions that perform well on the training set and are close to the true functional form. We demonstrate this by alternating fitness functions that quantify faithfulness to values (via MSE) and empirical derivatives (via a novel theoretically justified fitness metric coined MSEDI). Proof-of-concept: We combine these ideas into a minimalistic evolutionary SR algorithm that outperforms all benchmark and state of-the-art SR algorithms in problems with unknown constants added, which we claim are more reflective of SR performance for real-world applications. Our claim is then strengthened by reproducing the superior performance on real-world regression datasets from SRBench. For researchers interested in equation-recovery problems, we also propose a set of conventions that can be used to promote fairness in comparison across SR methods and to reduce unintentional bias."}}
{"id": "P1MaSJlwdT4", "cdate": 1663849909068, "mdate": null, "content": {"title": "Go-Explore with a guide: Speeding up search in sparse reward settings with goal-directed intrinsic rewards", "abstract": "Reinforcement Learning (RL) agents have traditionally been very sample-intensive to train, especially in environments with sparse rewards. Seeking inspiration from neuroscience experiments of rats learning the structure of a maze without needing extrinsic rewards, we seek to incorporate additional intrinsic rewards to guide behavior. We propose a potential-based goal-directed intrinsic reward (GDIR), which provides a reward signal regardless of whether the task is achieved, and ensures that learning can always take place. While GDIR may be similar to approaches such as reward shaping in incorporating goal-based rewards, we highlight that GDIR is innate to the agent and hence applicable across a wide range of environments without needing to rely on a properly shaped environment reward. We also note that GDIR is different from curiosity-based intrinsic motivation, which can diminish over time and lead to inefficient exploration. Go-Explore is a well-known state-of-the-art algorithm for sparse reward domains, and we demonstrate that by incorporating GDIR in the ``Go\" and ``Explore\" phases, we can improve Go-Explore's performance and enable it to learn faster across multiple environments, for both discrete (2D grid maze environments, Towers of Hanoi, Game of Nim) and continuous (Cart Pole and Mountain Car) state spaces. Furthermore, to consolidate learnt trajectories better, our method also incorporates a novel approach of hippocampal replay to update the values of GDIR and reset state visit and selection counts of states along the successful trajectory. As a benchmark, we also show that our proposed approaches learn significantly faster than traditional extrinsic-reward-based RL algorithms such as Proximal Policy Optimization, TD-learning, and Q-learning."}}
{"id": "yzm3GkhSKdD", "cdate": 1640995200000, "mdate": 1683770062656, "content": {"title": "Unified Analysis of Coordinated Multipoint Transmissions in mmWave Cellular Networks", "abstract": "This article performs a unified analysis of three coordinated multipoint (CoMP) transmission strategies in the downlink of mmWave cellular networks, including the fixed-number base station (BS) cooperation (FNC), the fixed-region BS cooperation (FRC), and the interference-aware BS cooperation (IAC). We first develop a comprehensive framework for CoMP operation in cellular networks, and investigate the network performance under a Poisson point process (PPP) model together with mmWave spectrum. To show what fraction of users in the network achieve target reliability for a given signal to interference-plus-noise ratio (SINR)/signal-to-interference ratio (SIR), we derive the SINR/SIR meta distributions, and further obtain the coverage probability as well as mean local delay for the three cooperation strategies. A pivotal intermediate step to compute the performance metrics is the derivation of joint distributions of distances between a typical user and cooperative BSs. Our analysis demonstrates that parameters of blockage have a significant influence on the network performance for the three CoMP schemes. We find that the FRC scheme makes more users achieve the given link reliability for the scenario with a low density of BSs, while the IAC scheme provides better performance for the network with a high density of BSs. Moreover, the optimal CoMP scheme can be approximately selected by considering the nearest distance from the serving BS to user and the radius of the approximate line-of-sight (LoS) region in the cellular networks."}}
{"id": "ym4-hoti-Y", "cdate": 1640995200000, "mdate": 1683770062615, "content": {"title": "Brick Tic-Tac-Toe: Exploring the Generalizability of AlphaZero to Novel Test Environments", "abstract": "Traditional reinforcement learning (RL) environments typically are the same for both the training and testing phases. Hence, current RL methods are largely not generalizable to a test environment which is conceptually similar but different from what the method has been trained on, which we term the novel test environment. As an effort to push RL research towards algorithms which can generalize to novel test environments, we introduce the Brick Tic-Tac-Toe (BTTT) test bed, where the brick position in the test environment is different from that in the training environment. Using a round-robin tournament on the BTTT environment, we show that traditional RL state-search approaches such as Monte Carlo Tree Search (MCTS) and Minimax are more generalizable to novel test environments than AlphaZero is. This is surprising because AlphaZero has been shown to achieve superhuman performance in environments such as Go, Chess and Shogi, which may lead one to think that it performs well in novel test environments. Our results show that BTTT, though simple, is rich enough to explore the generalizability of AlphaZero. We find that merely increasing MCTS lookahead iterations was insufficient for AlphaZero to generalize to some novel test environments. Rather, increasing the variety of training environments helps to progressively improve generalizability across all possible starting brick configurations."}}
{"id": "xZAuwBmh0kX", "cdate": 1640995200000, "mdate": 1683770062772, "content": {"title": "Long-Term Incentives for Contributor-Initiated Proactive Sensing in Mobile Crowdsensing", "abstract": "Mobile crowdsensing (MCS) is an emerging human-powered service for large-scale sensing and data collection. Most existing frameworks for controlling MCS services focus on a system-initiated setting where the crowdsourcer selects a subset of contributors and incentivizes them to collect the sensing data after the queries from the consumers arrive at the system. Such a system-initiated setting may cause large delays for the system to answer the consumers\u2019 queries, which is not suitable for many real-time sensing applications. In this article, we propose contributor-initiated proactive sensing (CIPS) frameworks for MCS where the sensing data are collected in a proactive manner before the consumers\u2019 queries arrive. In CIPS, the consumers can get answers about their queries with virtually no delay, which opens the door for MCS to many real-time applications. We first propose a centralized algorithm called C-CIPS as a benchmark for sensing scheduling by assuming the contributors are truthful and the consumer queries are known <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">a priori</i> . Next, we propose a distributed algorithm called D-CIPS to deal with strategic contributors and unknown consumer queries. Through rigorous theoretical analysis, we prove that both C-CIPS and D-CIPS can achieve near-optimal solutions. Furthermore, D-CIPS is proved to be truthful. Through comprehensive simulations with both synthetic and real-world data sets, we demonstrate the effectiveness of the proposed algorithms."}}
{"id": "utQ6_6Lnbp", "cdate": 1640995200000, "mdate": 1683770062618, "content": {"title": "AP: Selective Activation for De-sparsifying Pruned Neural Networks", "abstract": "The rectified linear unit (ReLU) is a highly successful activation function in neural networks as it allows networks to easily obtain sparse representations, which reduces overfitting in overparameterized networks. However, in network pruning, we find that the sparsity introduced by ReLU, which we quantify by a term called dynamic dead neuron rate (DNR), is not beneficial for the pruned network. Interestingly, the more the network is pruned, the smaller the dynamic DNR becomes during optimization. This motivates us to propose a method to explicitly reduce the dynamic DNR for the pruned network, i.e., de-sparsify the network. We refer to our method as Activating-while-Pruning (AP). We note that AP does not function as a stand-alone method, as it does not evaluate the importance of weights. Instead, it works in tandem with existing pruning methods and aims to improve their performance by selective activation of nodes to reduce the dynamic DNR. We conduct extensive experiments using popular networks (e.g., ResNet, VGG) via two classical and three state-of-the-art pruning methods. The experimental results on public datasets (e.g., CIFAR-10/100) suggest that AP works well with existing pruning methods and improves the performance by 3% - 4%. For larger scale datasets (e.g., ImageNet) and state-of-the-art networks (e.g., vision transformer), we observe an improvement of 2% - 3% with AP as opposed to without. Lastly, we conduct an ablation study to examine the effectiveness of the components comprising AP."}}
