{"id": "7QDPaL-Yl8U", "cdate": 1632875732502, "mdate": null, "content": {"title": "LPRules: Rule Induction in Knowledge Graphs Using Linear Programming", "abstract": "Knowledge graph (KG) completion is a well-studied problem in AI. Rule-based methods and embedding-based methods form two of the solution techniques. Rule-based methods learn first-order logic rules that capture existing facts in an input graph and then use these rules for reasoning about missing facts. A major drawback of such methods is the lack of scalability to large datasets. In this paper, we present a simple linear programming (LP) model to choose rules from a list of candidate rules and assign weights to them. For smaller KGs, we use simple heuristics to create the candidate list. For larger KGs, we start with a small initial candidate list, and then use standard column generation ideas to add more rules in order to improve the LP model objective value. To foster interpretability and generalizability, we limit the complexity of the set of chosen rules via explicit constraints, and tune the complexity hyperparameter for individual datasets. We show that our method can obtain state-of-the-art results for three out of four widely used KG datasets, while taking significantly less computing time than other popular rule learners including some based on neuro-symbolic methods. The improved scalability of our method allows us to tackle large datasets such as YAGO3-10."}}
{"id": "u_rt3_-OpD", "cdate": 1621572637064, "mdate": null, "content": {"title": "Integer Programming for Causal Structure Learning in the Presence of Latent Variables", "abstract": "The problem of finding an ancestral acyclic directed mixed graph (ADMG) that represents the\ncausal relationships between a set of variables is an important area of research for causal inference.\nHowever, most of existing score-based structure learning methods focus on learning the directed\nacyclic graph (DAG) without latent variables. A number of score-based methods have recently\nbeen proposed for the ADMG learning, yet they are heuristic in nature and do not guarantee an\noptimal solution. We propose a novel exact score-based method that solves an integer programming\n(IP) formulation and returns a score-maximizing ancestral ADMG for a set of continuous variables.\nIn particular, we generalize the state-of-the-art IP model for DAG learning problems and derive\nnew classes of valid inequalities to formalize the IP-based ADMG learning model. Empirically\nour model can be solved efficiently for medium-sized problems and achieves better accuracy than\nstate-of-the-art score-based methods as well as benchmark constraint-based methods."}}
{"id": "By4DUo-_-B", "cdate": 1546300800000, "mdate": null, "content": {"title": "Generalized Linear Rule Models", "abstract": "This paper considers generalized linear models using rule-based features, also referred to as rule ensembles, for regression and probabilistic classification. Rules facilitate model interpretation ..."}}
{"id": "BkVmRd-dWS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Boolean Decision Rules via Column Generation", "abstract": "This paper considers the learning of Boolean rules in either disjunctive normal form (DNF, OR-of-ANDs, equivalent to decision rule sets) or conjunctive normal form (CNF, AND-of-ORs) as an interpretable model for classification. An integer program is formulated to optimally trade classification accuracy for rule simplicity. Column generation (CG) is used to efficiently search over an exponential number of candidate clauses (conjunctions or disjunctions) without the need for heuristic rule mining. This approach also bounds the gap between the selected rule set and the best possible rule set on the training data. To handle large datasets, we propose an approximate CG algorithm using randomization. Compared to three recently proposed alternatives, the CG algorithm dominates the accuracy-simplicity trade-off in 8 out of 16 datasets. When maximized for accuracy, CG is competitive with rule learners designed for this purpose, sometimes finding significantly simpler solutions that are no less accurate."}}
