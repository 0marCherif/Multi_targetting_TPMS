{"id": "B1VoVwj30a", "cdate": 1665561091913, "mdate": 1665561091913, "content": {"title": "Graph Neural Network Policies and Imitation Learning for Multi-Domain Task-Oriented Dialogues", "abstract": "Task-oriented dialogue systems are designed to achieve specific goals while conversing with humans. In practice, they may have to handle simultaneously several domains and tasks. The dialogue manager must therefore be able to take into account domain changes and plan over different domains/tasks in order to deal with multidomain dialogues. However, learning with reinforcement in such context becomes difficult because the state-action dimension is larger while the reward signal remains scarce. Our experimental results suggest that structured policies based on graph neural networks combined with different degrees of imitation learning can effectively handle multi-domain dialogues. The reported experiments underline the benefit of structured policies over standard policies. "}}
{"id": "6GuaG8Vcby", "cdate": 1665478252756, "mdate": 1665478252756, "content": {"title": "Diluted Near-Optimal Expert Demonstrations for Guiding Dialogue Stochastic Policy Optimisation", "abstract": "A learning dialogue agent can infer its behaviour from interactions with the users. These interactions can be taken from either human-to-human or human-machine conversations. However, human interactions are scarce and costly, making learning from few interactions essential. One solution to speed-up the learning process is to guide the agent\u2019s exploration with the help of an expert. We present in this\npaper several imitation learning strategies for dialogue policy where the guiding expert is a near-optimal handcrafted policy. We incorporate these strategies with state-of-the-art reinforcement learning methods based on Q-learning and actor-critic. We notably propose a randomised exploration policy which allows for a seamless hybridisation of the learned policy and the expert, which can be seen\nas a dilution of the expert\u2019s demonstration into the resulting policy. Our experiments show that our hybridisation strategy outperforms several baselines, and that it could accelerate the learning when facing real humans."}}
{"id": "tTQzJ6TJGVi", "cdate": 1663349228287, "mdate": null, "content": {"title": "Tabular Data Generation: Can We Fool XGBoost ?", "abstract": "If by 'realistic' we mean indistinguishable from (fresh) real data, generating realistic synthetic tabular data is far from being a trivial task. We present here a series of experiments showing that strong classifiers like XGBoost are able to distinguish state-of-the-art synthetic data from fresh real data almost perfectly on several tabular datasets. By studying  the important features of these classifiers, we remark that mixed-type (continuous/discrete) and ill-distributed numerical columns are the ones which are the less faithfully reconstituted. We hence propose and experiment a series of automated reversible column-wise encoders which improve the realism of the generators."}}
{"id": "vjea2ynL7MW", "cdate": 1602926478998, "mdate": null, "content": {"title": "Neural-Driven Multi-criteria Tree Search for Paraphrase Generation", "abstract": "A good paraphrase is semantically similar to the original sentence but it must be also well formed, and syntactically different to ensure diversity. To deal with this trade-off, we propose to cast the paraphrase generation task as a multi-objectives search problem on the lattice of text transformations. We use BERT and GPT2 to measure respectively the semantic distance and the correctness of the candidates. We study two search algorithms: Monte-Carlo Tree Search For Paraphrase Generation (MCPG) and Pareto Tree Search (PTS) that we use to explore the huge sets of candidates generated by applying the PPDB-2.0 edition rules.  We evaluate this approach on 5 datasets and show that it performs reasonably well and that it outperforms a state-of-the-art edition-based text generation method."}}
{"id": "_Ivf_BNABxJ", "cdate": 1594626676924, "mdate": null, "content": {"title": "Corrupt Bandits for Preserving Local Privacy", "abstract": "We study a variant of the stochastic multi-armed bandit (MAB) problem in which the rewards are corrupted. In this framework, motivated by privacy preservation in online recommender systems, the goal is to maximize the sum of the (unobserved) rewards, based on the observation of transformation of these rewards through a stochastic corruption process with known parameters. We provide a lower bound on the expected regret of any bandit algorithm in this corrupted setting. We devise a frequentist algorithm, KLUCB-CF, and a Bayesian algorithm, TS-CF and give upper bounds on their regret. We also provide the appropriate corruption parameters to guarantee a desired level of local privacy and analyze how this impacts the regret. Finally, we present some experimental results that confirm our analysis. "}}
{"id": "v-7QG3yX5fJ", "cdate": 1594626392357, "mdate": null, "content": {"title": "Budgeted Reinforcement Learning in Continuous State Space", "abstract": "A Budgeted Markov Decision Process (BMDP) is an extension of a Markov Decision Process to critical applications requiring safety constraints. It relies on a notion of risk implemented in the shape of an upper bound on a constrains violation signal that -- importantly -- can be modified in real-time. So far, BMDPs could only be solved in the case of finite state spaces with known dynamics. This work extends the state-of-the-art to continuous spaces environments and unknown dynamics. We show that the solution to a BMDP is the fixed point of a novel Budgeted Bellman Optimality operator. This observation allows us to introduce natural extensions of Deep Reinforcement Learning algorithms to address large-scale BMDPs. We validate our approach on two simulated applications: spoken dialogue and autonomous driving."}}
{"id": "B1EzsoWOWr", "cdate": 1420070400000, "mdate": null, "content": {"title": "A Relative Exponential Weighing Algorithm for Adversarial Utility-based Dueling Bandits", "abstract": "We study the K-armed dueling bandit problem which is a variation of the classical Multi-Armed Bandit (MAB) problem in which the learner receives only relative feedback about the selected pairs of a..."}}
{"id": "SJZCKiZubr", "cdate": 1356998400000, "mdate": null, "content": {"title": "Generic Exploration and K-armed Voting Bandits", "abstract": "We study a stochastic online learning scheme with partial feedback where the utility of decisions is only observable through an estimation of the environment parameters. We propose a generic pure-e..."}}
