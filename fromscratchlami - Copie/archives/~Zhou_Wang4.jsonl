{"id": "muxzo22ptQ", "cdate": 1690848000000, "mdate": 1699157047150, "content": {"title": "Perceptual Quality Assessment of Colored 3D Point Clouds", "abstract": "3D point clouds have found a wide variety of applications in multimedia processing, remote sensing, and scientific computing. Although most point cloud processing systems are developed to improve viewer experiences, little work has been dedicated to perceptual quality assessment of 3D point clouds. In this work, we build a new 3D point cloud database, namely the Waterloo Point Cloud (WPC) database. In contrast to existing datasets consisting of small-scale and low-quality source content of constrained viewing angles, the WPC database contains 20 high quality, realistic, and omni-directional source point clouds and 740 diversely distorted point clouds. We carry out a subjective quality assessment experiment over the database in a controlled lab environment. Our statistical analysis suggests that existing objective point cloud quality assessment (PCQA) models only achieve limited success in predicting subjective quality ratings. We propose a novel objective PCQA model based on an attention mechanism and a variant of information content-weighted structural similarity, which significantly outperforms existing PCQA models. The database has been made publicly available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/qdushl/Waterloo-Point-Cloud-Database</uri> ."}}
{"id": "WM0_jpc8-4", "cdate": 1672531200000, "mdate": 1684172857169, "content": {"title": "Blind Omnidirectional Image Quality Assessment: Integrating Local Statistics and Global Semantics", "abstract": "Omnidirectional image quality assessment (OIQA) aims to predict the perceptual quality of omnidirectional images that cover the whole 180$\\times$360$^{\\circ}$ viewing range of the visual environment. Here we propose a blind/no-reference OIQA method named S$^2$ that bridges the gap between low-level statistics and high-level semantics of omnidirectional images. Specifically, statistic and semantic features are extracted in separate paths from multiple local viewports and the hallucinated global omnidirectional image, respectively. A quality regression along with a weighting process is then followed that maps the extracted quality-aware features to a perceptual quality prediction. Experimental results demonstrate that the proposed S$^2$ method offers highly competitive performance against state-of-the-art methods."}}
{"id": "WAyjt0KUfYx", "cdate": 1672531200000, "mdate": 1683708808905, "content": {"title": "Bitstream-Based Perceptual Quality Assessment of Compressed 3D Point Clouds", "abstract": "With the increasing demand of compressing and streaming 3D point clouds under constrained bandwidth, it has become ever more important to accurately and efficiently determine the quality of compressed point clouds, so as to assess and optimize the quality-of-experience (QoE) of end users. Here we make one of the first attempts developing a bitstream-based no-reference (NR) model for perceptual quality assessment of point clouds without resorting to full decoding of the compressed data stream. Specifically, we first establish a relationship between texture complexity and the bitrate and texture quantization parameters based on an empirical rate-distortion model. We then construct a texture distortion assessment model upon texture complexity and quantization parameters. By combining this texture distortion model with a geometric distortion model derived from Trisoup geometry encoding parameters, we obtain an overall bitstream-based NR point cloud quality model named streamPCQ. Experimental results show that the proposed streamPCQ model demonstrates highly competitive performance when compared with existing classic full-reference (FR) and reduced-reference (RR) point cloud quality assessment methods with a fraction of computational cost."}}
{"id": "9xWHrztAYdW", "cdate": 1672531200000, "mdate": 1699157045004, "content": {"title": "Degraded Reference Image Quality Assessment", "abstract": "In practical media distribution systems, visual content usually undergoes multiple stages of quality degradation along the delivery chain, but the pristine source content is rarely available at most quality monitoring points along the chain to serve as a reference for quality assessment. As a result, full-reference (FR) and reduced-reference (RR) image quality assessment (IQA) methods are generally infeasible. Although no-reference (NR) methods are readily applicable, their performance is often not reliable. On the other hand, intermediate references of degraded quality are often available, e.g., at the input of video transcoders, but how to make the best use of them in proper ways has not been deeply investigated. Here we make one of the first attempts to establish a new paradigm named degraded-reference IQA (DR IQA). Specifically, by using a two-stage distortion pipeline we lay out the architectures of DR IQA and introduce a 6-bit code to denote the choices of configurations. We construct the first large-scale databases dedicated to DR IQA and have made them publicly available. We make novel observations on distortion behavior in multi-stage distortion pipelines by comprehensively analyzing five multiple distortion combinations. Based on these observations, we develop novel DR IQA models and make extensive comparisons with a series of baseline models derived from top-performing FR and NR models. The results suggest that DR IQA may offer significant performance improvement in multiple distortion environments, thereby establishing DR IQA as a valid IQA paradigm that is worth further exploration."}}
{"id": "7V5HhvjemV", "cdate": 1672531200000, "mdate": 1699157046504, "content": {"title": "Perceptual Quality Assessment of 360\u00b0 Images Based on Generative Scanpath Representation", "abstract": "Despite substantial efforts dedicated to the design of heuristic models for omnidirectional (i.e., 360$^\\circ$) image quality assessment (OIQA), a conspicuous gap remains due to the lack of consideration for the diversity of viewing behaviors that leads to the varying perceptual quality of 360$^\\circ$ images. Two critical aspects underline this oversight: the neglect of viewing conditions that significantly sway user gaze patterns and the overreliance on a single viewport sequence from the 360$^\\circ$ image for quality inference. To address these issues, we introduce a unique generative scanpath representation (GSR) for effective quality inference of 360$^\\circ$ images, which aggregates varied perceptual experiences of multi-hypothesis users under a predefined viewing condition. More specifically, given a viewing condition characterized by the starting point of viewing and exploration time, a set of scanpaths consisting of dynamic visual fixations can be produced using an apt scanpath generator. Following this vein, we use the scanpaths to convert the 360$^\\circ$ image into the unique GSR, which provides a global overview of gazed-focused contents derived from scanpaths. As such, the quality inference of the 360$^\\circ$ image is swiftly transformed to that of GSR. We then propose an efficient OIQA computational framework by learning the quality maps of GSR. Comprehensive experimental results validate that the predictions of the proposed framework are highly consistent with human perception in the spatiotemporal domain, especially in the challenging context of locally distorted 360$^\\circ$ images under varied viewing conditions. The code will be released at https://github.com/xiangjieSui/GSR"}}
{"id": "S_ZeQIab_RY", "cdate": 1643810122848, "mdate": 1643810122848, "content": {"title": "Blind image quality assessment using a deep bilinear convolutional neural network", "abstract": "We propose a deep bilinear model for blind image quality assessment that works for both synthetically and authentically distorted images. Our model constitutes two streams of deep convolutional neural networks (CNNs), specializing in two distortion scenarios separately. For synthetic distortions, we first pre-train a CNN to classify the distortion type and the level of an input image, whose ground truth label is readily available at a large scale. For authentic distortions, we make use of a pre-train CNN (VGG-16) for the image classification task. The two feature sets are bilinearly pooled into one representation for a final quality prediction. We fine-tune the whole network on the target databases using a variant of stochastic gradient descent. The extensive experimental results show that the proposed model achieves state-of-the-art performance on both synthetic and authentic IQA databases. Furthermore, we verify the generalizability of our method on the large-scale Waterloo Exploration Database, and demonstrate its competitiveness using the group maximum differentiation competition methodology."}}
{"id": "z-kEIdhnnG", "cdate": 1640995200000, "mdate": 1684172857163, "content": {"title": "Quality Assessment of Image Super-Resolution: Balancing Deterministic and Statistical Fidelity", "abstract": "There has been a growing interest in developing image super-resolution (SR) algorithms that convert low-resolution (LR) to higher resolution images, but automatically evaluating the visual quality of super-resolved images remains a challenging problem. Here we look at the problem of SR image quality assessment (SR IQA) in a two-dimensional (2D) space of deterministic fidelity (DF) versus statistical fidelity (SF). This allows us to better understand the advantages and disadvantages of existing SR algorithms, which produce images at different clusters in the 2D space of (DF, SF). Specifically, we observe an interesting trend from more traditional SR algorithms that are typically inclined to optimize for DF while losing SF, to more recent generative adversarial network (GAN) based approaches that by contrast exhibit strong advantages in achieving high SF but sometimes appear weak at maintaining DF. Furthermore, we propose an uncertainty weighting scheme based on content-dependent sharpness and texture assessment that merges the two fidelity measures into an overall quality prediction named the Super Resolution Image Fidelity (SRIF) index, which demonstrates superior performance against state-of-the-art IQA models when tested on subject-rated datasets."}}
{"id": "pmWBDix3Do", "cdate": 1640995200000, "mdate": 1699157046084, "content": {"title": "Quality assessment of video with film grain", "abstract": "Film grain noise originally arises from small metallic silver particles on processed photographic celluloid. Although modern digital video acquisition systems are capable of largely reducing noise, sometimes to nearly invisible levels, the look of cinematic film grain has not gone away. Instead, content creators often purposely introduce simulated film grain in post-production to emulate dust in the environment, enrich texture details, and develop a certain visual tone. Despite the artistic benefits, film grain has posed significant challenges to video delivery systems. Compressing and transmitting videos containing film grain noise is extremely costly due to the large number of bits required to encode the noisy pixels of much higher entropy than the typical visual content of the scene. Heavy compression may remove film grain, but meanwhile, remove meaningful texture content in the visual scene or deteriorate the artistic effect of the creator's intent. It also casts major challenges to quality control of video delivery systems, for which film grain-susceptible fidelity measures are highly desirable for measurement and optimization purposes. Here after describing the characteristics of film grain and its impact to video quality, we present a novel framework that unifies natural video quality assessment and creative intent friendly video quality assessment. We also demonstrate an instantiation of the framework in the context of film-grained content in terms of predicting the perception of different groups of subjects."}}
{"id": "gnfOr_nwX9N", "cdate": 1640995200000, "mdate": 1699157047592, "content": {"title": "A Bayesian Quality-of-Experience Model for Adaptive Streaming Videos", "abstract": "The fundamental conflict between the enormous space of adaptive streaming videos and the limited capacity for subjective experiment casts significant challenges to objective Quality-of-Experience (QoE) prediction. Existing objective QoE models either employ pre-defined parametrization or exhibit complex functional form, achieving limited generalization capability in diverse streaming environments. In this study, we propose an objective QoE model, namely, the Bayesian streaming quality index (BSQI), to integrate prior knowledge on the human visual system and human annotated data in a principled way. By analyzing the subjective characteristics towards streaming videos from a corpus of subjective studies, we show that a family of QoE functions lies in a convex set. Using a variant of projected gradient descent, we optimize the objective QoE model over a database of training videos. The proposed BSQI demonstrates strong prediction accuracy in a broad range of streaming conditions, evident by state-of-the-art performance on four publicly available benchmark datasets and a novel analysis-by-synthesis visual experiment."}}
{"id": "dfScgZvD3o3", "cdate": 1640995200000, "mdate": 1684172857162, "content": {"title": "Quality Assessment of Image Super-Resolution: Balancing Deterministic and Statistical Fidelity", "abstract": ""}}
