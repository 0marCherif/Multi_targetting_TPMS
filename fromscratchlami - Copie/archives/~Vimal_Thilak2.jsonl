{"id": "lY1e0PNkSJ", "cdate": 1664872119027, "mdate": null, "content": {"title": "The Slingshot Mechanism: An Empirical Study of Adaptive Optimizers and the \\emph{Grokking Phenomenon}", "abstract": "The \\emph{grokking phenomenon} reported by Power et al.~\\cite{power2021grokking} refers to a regime where a long period of overfitting is followed by a seemingly sudden transition to perfect generalization. In this paper, we attempt to reveal the underpinnings of Grokking via  empirical studies. Specifically, we uncover an optimization anomaly plaguing adaptive optimizers at extremely late stages of training, referred to as the \\emph{Slingshot Mechanism}. A prominent artifact of the Slingshot Mechanism can be measured by the cyclic phase transitions between stable and unstable training regimes, and can be easily monitored by the cyclic behavior of the norm of the last layers weights. We empirically observe that without explicit regularization, Grokking as reported in \\cite{power2021grokking} almost exclusively happens at the onset of \\emph{Slingshots}, and is absent without it. \n    While common and easily reproduced in more general settings, the Slingshot Mechanism does not follow from any known optimization theories that we are aware of, and can be easily overlooked without an in depth examination. Our work points to a surprising and useful inductive bias of adaptive gradient optimizers at late stages of training, calling for a revised theoretical analysis of their origin."}}
{"id": "dJgYhYKvr1", "cdate": 1652737859292, "mdate": null, "content": {"title": "The Slingshot Mechanism: An Empirical Study of Adaptive Optimizers and the \\emph{Grokking Phenomenon}", "abstract": "The \\emph{grokking phenomenon} as reported by Power et al.~\\cite{power2021grokking} refers to a regime where a long period of overfitting is followed by a seemingly sudden transition to perfect generalization. In this paper, we attempt to reveal the underpinnings of Grokking via a series of empirical studies. Specifically, we uncover an optimization anomaly plaguing adaptive optimizers at extremely late stages of training, referred to as the \\emph{Slingshot Mechanism}. A prominent artifact of the Slingshot Mechanism can be measured by the cyclic phase transitions between stable and unstable training regimes, and can be easily monitored by the cyclic behavior of the norm of the last layers weights. We empirically observe that without explicit regularization, Grokking as reported in \\cite{power2021grokking} almost exclusively happens at the onset of \\emph{Slingshots}, and is absent without it. \n    While common and easily reproduced in more general settings, the Slingshot Mechanism does not follow from any known optimization theories that we are aware of, and can be easily overlooked without an in depth examination. Our work points to a surprising and useful inductive bias of adaptive gradient optimizers at late stages of training, calling for a revised theoretical analysis of their origin."}}
{"id": "wevz6KB56w", "cdate": 1640995200000, "mdate": 1683396924845, "content": {"title": "The Slingshot Mechanism: An Empirical Study of Adaptive Optimizers and the Grokking Phenomenon", "abstract": "The grokking phenomenon as reported by Power et al. ( arXiv:2201.02177 ) refers to a regime where a long period of overfitting is followed by a seemingly sudden transition to perfect generalization. In this paper, we attempt to reveal the underpinnings of Grokking via a series of empirical studies. Specifically, we uncover an optimization anomaly plaguing adaptive optimizers at extremely late stages of training, referred to as the Slingshot Mechanism. A prominent artifact of the Slingshot Mechanism can be measured by the cyclic phase transitions between stable and unstable training regimes, and can be easily monitored by the cyclic behavior of the norm of the last layers weights. We empirically observe that without explicit regularization, Grokking as reported in ( arXiv:2201.02177 ) almost exclusively happens at the onset of Slingshots, and is absent without it. While common and easily reproduced in more general settings, the Slingshot Mechanism does not follow from any known optimization theories that we are aware of, and can be easily overlooked without an in depth examination. Our work points to a surprising and useful inductive bias of adaptive gradient optimizers at late stages of training, calling for a revised theoretical analysis of their origin."}}
{"id": "v-Z1Ig35Zh_", "cdate": 1609459200000, "mdate": 1683396924877, "content": {"title": "Implicit Greedy Rank Learning in Autoencoders via Overparameterized Linear Networks", "abstract": "Deep linear networks trained with gradient descent yield low rank solutions, as is typically studied in matrix factorization. In this paper, we take a step further and analyze implicit rank regularization in autoencoders. We show greedy learning of low-rank latent codes induced by a linear sub-network at the autoencoder bottleneck. We further propose orthogonal initialization and principled learning rate adjustment to mitigate sensitivity of training dynamics to spectral prior and linear depth. With linear autoencoders on synthetic data, our method converges stably to ground-truth latent code rank. With nonlinear autoencoders, our method converges to latent ranks optimal for downstream classification and image sampling."}}
{"id": "Fv6_eYn--Pn", "cdate": 1609459200000, "mdate": 1681756325848, "content": {"title": "Implicit Acceleration and Feature Learning in Infinitely Wide Neural Networks with Bottlenecks", "abstract": "We analyze the learning dynamics of infinitely wide neural networks with a finite sized bottle-neck. Unlike the neural tangent kernel limit, a bottleneck in an otherwise infinite width network al-lows data dependent feature learning in its bottle-neck representation. We empirically show that a single bottleneck in infinite networks dramatically accelerates training when compared to purely in-finite networks, with an improved overall performance. We discuss the acceleration phenomena by drawing similarities to infinitely wide deep linear models, where the acceleration effect of a bottleneck can be understood theoretically."}}
{"id": "FmFpw2qYVmG", "cdate": 1293840000000, "mdate": 1683396924837, "content": {"title": "Passive Polarimetric Imagery-Based Material Classification Robust to Illumination Source Position and Viewpoint", "abstract": "Polarization, a property of light that conveys information about the transverse electric field orientation, complements other attributes of electromagnetic radiation such as intensity and frequency. Using multiple passive polarimetric images, we develop an iterative, model-based approach to estimate the complex index of refraction and apply it to target classification."}}
{"id": "-hok_fkHeo", "cdate": 1167609600000, "mdate": 1683396924877, "content": {"title": "Material Classification using Passive Polarimetric Imagery", "abstract": "Passive imaging polarimetry has emerged as an useful tool in many remote sensing applications including material classification, target detection and shape extraction. In this paper we present a method to classify specular objects based on their material composition from passive polarimetric imagery. The proposed algorithm is built on an iterative model-based method to recover the complex index of refraction of a specular target from multiple polarization measurements. The recovered parameters are then used to discriminate between objects by employing the nearest neighbor rule. The effectiveness of the proposed method is validated with data collected in laboratory conditions. Experimental results indicate that the classification approach is highly effective for distinguishing between various targets of interest. Most significantly, the proposed classification method is robust to a wide range of observational geometry."}}
{"id": "0Za2c3Yf_4", "cdate": 1072915200000, "mdate": 1683396924829, "content": {"title": "Tracking of extended size targets in H.264 compressed video using the probabilistic data association filter", "abstract": "Object detection and tracking play a significant role in critical applications such as video monitoring and remote surveillance. These systems employ compression to efficiently utilize the available bandwidth. An example of an efficient compression solution to low bit rate video applications is the recently proposed H.264/AVC video coding standard. In particular, H.264/AVC has been optimized for transmission over wireless channels making it an attractive candidate for use in remote surveillance systems. In this paper, we propose an algorithm that exploits motion vectors generated by the H.264 encoder for object detection and tracking. Experimental results demonstrate the effectiveness of the proposed method to detect and track objects in real video sequences."}}
{"id": "5RP_vYkcQH7", "cdate": 1009843200000, "mdate": 1683396924845, "content": {"title": "Robust bandlimited watermarking with trellis coded modulation", "abstract": "Digital watermarking is equivalent to bandlimited, power-limited digital communication. The power limit is due to the requirement of imperceptible watermarks, and has long been recognized. The bandwidth limit is due to the lowpass spectrum of images and possible lowpass attacks (malicious or unintentional). This is related to the concept of channel capacity, and has only recently been addressed in the context of watermarking. To achieve high reliability without loss of watermark data rates, we propose an adaptation of trellis coded modulation (TCM). Our method is applicable to a wide class of watermarking algorithms. The objective is to maintain the data rate while improving reliability of watermark detection, in terms of bit error rates (BER). Alternatively, higher data rates can be achieved while maintaining watermark BER. We conduct experiments for blind as well as. informed detection. Simulation results show significant improvement compared to conventional methods over a wide-range of channel (attack) signal-to-noise ratios."}}
