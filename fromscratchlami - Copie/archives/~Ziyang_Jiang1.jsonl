{"id": "7XBkGedeIZ", "cdate": 1672954698454, "mdate": null, "content": {"title": "Incorporating Prior Knowledge into Neural Networks through an Implicit Composite Kernel", "abstract": "It is challenging to guide neural network (NN) learning with prior knowledge. In contrast, many known properties, such as spatial smoothness or seasonality, are straightforward to model by choosing an appropriate kernel in a Gaussian process (GP). Many deep learning applications could be enhanced by modeling such known properties. For example, convolutional neural networks (CNNs) are frequently used in remote sensing, which is subject to strong seasonal effects. We propose to blend the strengths of deep learning and the clear modeling capabilities of GPs by using a composite kernel that combines a kernel implicitly defined by a neural network with a second kernel function chosen to model known properties (e.g., seasonality). We implement this idea by combining a deep network and an efficient mapping based on the Nystrom approximation, which we call Implicit Composite Kernel (ICK). We then adopt a sample-then-optimize approach to approximate the full GP posterior distribution. We demonstrate that ICK has superior performance and flexibility on both synthetic and real-world data sets. We believe that ICK framework can be used to include prior information into neural networks in many applications."}}
{"id": "SLfZvB_gKt", "cdate": 1672531200000, "mdate": 1681690582754, "content": {"title": "Estimating Causal Effects using a Multi-task Deep Ensemble", "abstract": "A number of methods have been proposed for causal effect estimation, yet few have demonstrated efficacy in handling data with complex structures, such as images. To fill this gap, we propose Causal Multi-task Deep Ensemble (CMDE), a novel framework that learns both shared and group-specific information from the study population. We provide proofs demonstrating equivalency of CDME to a multi-task Gaussian process (GP) with a coregionalization kernel a priori. Compared to multi-task GP, CMDE efficiently handles high-dimensional and multi-modal covariates and provides pointwise uncertainty estimates of causal effects. We evaluate our method across various types of datasets and tasks and find that CMDE outperforms state-of-the-art methods on a majority of these tasks."}}
{"id": "9X6EUblA3d", "cdate": 1672531200000, "mdate": 1681690582676, "content": {"title": "Domain Adaptation via Rebalanced Sub-domain Alignment", "abstract": "Unsupervised domain adaptation (UDA) is a technique used to transfer knowledge from a labeled source domain to a different but related unlabeled target domain. While many UDA methods have shown success in the past, they often assume that the source and target domains must have identical class label distributions, which can limit their effectiveness in real-world scenarios. To address this limitation, we propose a novel generalization bound that reweights source classification error by aligning source and target sub-domains. We prove that our proposed generalization bound is at least as strong as existing bounds under realistic assumptions, and we empirically show that it is much stronger on real-world data. We then propose an algorithm to minimize this novel generalization bound. We demonstrate by numerical experiments that this approach improves performance in shifted class distribution scenarios compared to state-of-the-art methods."}}
{"id": "thmtrq5exDC", "cdate": 1672365906254, "mdate": 1672365906254, "content": {"title": "Improving Spatial Variation of Ground-level PM2.5 Prediction with Contrastive Learning from Satellite Imagery", "abstract": "Convolutional Neural Networks (CNNs) are a promising technique to predict highly localized fine particulate\nmatter (i.e., PM2.5 levels) based on high-resolution satellite imagery. Unfortunately, CNNs typically require large\namounts of supervised data to perform well, whereas this application generally has lots of unsupervised data (all\nsatellite imagery) and relatively sparse supervised data (measurements from ground sensors). Previous work used\ntransfer learning from another visual task to initialize the CNN weights; however, we hypothesize that standard\ntransfer learning strategies would bias the CNN to focus on irrelevant details of the image for our applications.\nInstead, we develop a novel framework called Spatiotemporal Contrastive Learning (SCL) to pre-train the CNN.\nWe test both regular contrastive learning and SCL on predicting PM2.5 levels from satellite images in two\ndifferent cities, Delhi and Beijing, and compare to CNNs with parameters initialized randomly and by transfer\nlearning. Our results show that regular contrastive learning and our SCL frameworks both manage to better\ncapture spatial variation of ground-level PM2.5 concentrations compared to traditional initialization schemes,\nand that this performance gap increases as the number of ground sensors decreases, implying that the approach\nwill be even more valuable in cities with fewer ground sensors. Our work demonstrates that contrastive learning\nis a powerful pre-training technique to build better spatial maps of PM2.5, and can be broadly applied in related\nsituations"}}
{"id": "pAq8iDy00Oa", "cdate": 1652737425560, "mdate": null, "content": {"title": "Incorporating Prior Knowledge into Neural Networks through an Implicit Composite Kernel", "abstract": "It is challenging to guide neural network (NN) learning with prior knowledge. In contrast, many known properties, such as spatial smoothness or seasonality, are straightforward to model by choosing an appropriate kernel in a Gaussian process (GP). Many deep learning applications could be enhanced by modeling such known properties. For example, convolutional neural networks (CNNs) are frequently used in remote sensing, which is subject to strong seasonal effects. We propose to blend the strengths of deep learning and the clear modeling capabilities of GPs by using a composite kernel that combines a kernel implicitly defined by a neural network with a second kernel function chosen to model known properties (e.g., seasonality). Then, we approximate the resultant GP by combining a deep network and an efficient mapping based on the Nystrom approximation, which we call Implicit Composite Kernel (ICK). ICK is flexible and can be used to include prior information in neural networks in many applications. We demonstrate the strength of our framework by showing its superior performance and flexibility on both synthetic and real-world data sets. The code is available at: https://anonymous.4open.science/r/ICK_NNGP-17C5/. "}}
