{"id": "4HKjezHU-w8", "cdate": 1653752159876, "mdate": null, "content": {"title": "Adaptive Intrinsic Motivation with Decision Awareness", "abstract": "Intrinsic motivation is a simple but powerful method to encourage exploration, which is one of the fundamental challenges of reinforcement learning. However, we demonstrate that widely used intrinsic motivation methods are highly dependent on the ratio between the extrinsic and intrinsic rewards through extensive experiments on sparse reward MiniGrid tasks. To overcome the problem, we propose an intrinsic reward coefficient adaptation scheme that is equipped with intrinsic motivation awareness and adjusts the intrinsic reward coefficient online to maximize the extrinsic return. We demonstrate that our method, named Adaptive Intrinsic Motivation with Decision Awareness (AIMDA), operates stably in various challenging MiniGrid environments without algorithm-task-specific hyperparameter tuning."}}
{"id": "PnpS7_SlNZi", "cdate": 1621629739248, "mdate": null, "content": {"title": "Improving Generalization in Meta-RL with Imaginary Tasks from Latent Dynamics Mixture", "abstract": "The generalization ability of most meta-reinforcement learning (meta-RL) methods is largely limited to test tasks that are sampled from the same distribution used to sample training tasks. To overcome the limitation, we propose Latent Dynamics Mixture (LDM) that trains a reinforcement learning agent with imaginary tasks generated from mixtures of learned latent dynamics. By training a policy on mixture tasks along with original training tasks, LDM allows the agent to prepare for unseen test tasks during training and prevents the agent from overfitting the training tasks. LDM significantly outperforms standard meta-RL methods in test returns on the gridworld navigation and MuJoCo tasks where we strictly separate the training task distribution and the test task distribution."}}
{"id": "Byg5D65GTS", "cdate": 1575296354250, "mdate": null, "content": {"title": "Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update", "abstract": "We propose Episodic Backward Update (EBU) \u2013 a novel deep reinforcement learning algorithm with a direct value propagation. In contrast to the conventional\nuse of the experience replay with uniform random sampling, our agent samples\na whole episode and successively propagates the value of a state to its previous\nstates. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate directly through all transitions of the sampled episode.\nWe theoretically prove the convergence of the EBU method and experimentally\ndemonstrate its performance in both deterministic and stochastic environments.\nEspecially in 49 games of Atari 2600 domain, EBU achieves the same mean and\nmedian human normalized performance of DQN by using only 5% and 10% of\nsamples, respectively.\n"}}
{"id": "HyleUNBeIr", "cdate": 1567802439909, "mdate": null, "content": {"title": "Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update", "abstract": "We propose Episodic Backward Update (EBU) \u2013 a novel deep reinforcement learning algorithm with a direct value propagation. In contrast to the conventional use of the experience replay with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state to its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate directly through all transitions of the sampled episode. We theoretically prove the convergence of the EBU method and experimentally demonstrate its performance in both deterministic and stochastic environments. Especially in 49 games of Atari 2600 domain, EBU achieves the same mean and median human normalized performance of DQN by using only 5% and 10% of samples, respectively."}}
{"id": "BJvWjcgAZ", "cdate": 1518730180650, "mdate": null, "content": {"title": "Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update", "abstract": "We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost."}}
