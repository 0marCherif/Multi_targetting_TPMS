{"id": "bwGy1ZeqvCX", "cdate": 1653595781543, "mdate": null, "content": {"title": "Generative Self-training Improves Pre-training for Visual Dialog", "abstract": "Visual dialog (VisDial) is a task of answering a series of questions grounded in an image, using the dialog history as context. Prior work has trained the dialog models solely on VisDial data via supervised learning or leveraged pre-training on related vision-and-language datasets. This paper presents a semi-supervised learning approach for VisDial, called Generative Self-Training (GST), to enhance the pre-training. Specifically, GST generates synthetic dialog data for unlabeled images via multimodal conditional text generation and trains the dialog model on the synthetic and the original VisDial data. Moreover, we also propose perplexity-based data selection and multimodal consistency regularization for robust training of the synthetic data. Evaluation on VisDial v1.0 dataset shows that GST improves the pre-training and achieves new state-of-the-art results.  "}}
{"id": "UoZkKle_VdX", "cdate": 1640995200000, "mdate": 1667782462781, "content": {"title": "Improving Robustness to Texture Bias via Shape-focused Augmentation", "abstract": "Despite significant progress of deep neural networks in image classification, it has been reported that CNNs trained on ImageNet have heavily focused on local texture information, rather than capturing complex visual concepts of the objects. To delve into this phenomenon, recent studies proposed to generate images with modified texture information for training the model. However, these methods largely sacrifice the classification accuracy on the in-domain dataset while achieving improved performance on the out-of-distribution dataset. Motivated by the fact that human tends to focus on shape information, we aim to resolve this issue by proposing a shape-focused augmentation where the texture in the object\u2019s foreground and background are separately changed. Key idea is that by applying different modifications to the inside and outside of an object, not only the bias toward texture is reduced but also the model is induced to focus on shape. Experiments show that the proposed method successfully reduces texture bias and also improves the classification performance on the original dataset."}}
{"id": "8f50FZPuCt", "cdate": 1640995200000, "mdate": 1681171971040, "content": {"title": "The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training", "abstract": ""}}
{"id": "YBKMqSadMSm", "cdate": 1609459200000, "mdate": 1667782462774, "content": {"title": "Attend What You Need: Motion-Appearance Synergistic Networks for Video Question Answering", "abstract": "Ahjeong Seo, Gi-Cheon Kang, Joonhan Park, Byoung-Tak Zhang. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021."}}
{"id": "8NI-Noq59tm", "cdate": 1609459200000, "mdate": 1667782462782, "content": {"title": "Reasoning Visual Dialog with Sparse Graph Learning and Knowledge Transfer", "abstract": ""}}
{"id": "3_GrVKQvx_v", "cdate": 1577836800000, "mdate": 1667782462773, "content": {"title": "Label Propagation Adaptive Resonance Theory for Semi-Supervised Continuous Learning", "abstract": "Semi-supervised learning and continuous learning are fundamental paradigms for human-level intelligence. To deal with real-world problems where labels are rarely given and the opportunity to access the same data is limited, it is necessary to apply these two paradigms in a joined fashion. In this paper, we propose Label Propagation Adaptive Resonance Theory (LPART) for semi-supervised continuous learning. LPART uses an online label propagation mechanism to perform classification and gradually improves its accuracy as the observed data accumulates. We evaluated the proposed model on visual (MNIST, SVHN, CIFAR-10) and audio (NSynth) datasets by adjusting the ratio of the labeled and unlabeled data. The accuracies are much higher when both labeled and unlabeled data are used, demonstrating the significant advantage of LPART in environments where the data labels are scarce."}}
{"id": "3vDhLm7yes", "cdate": 1546300800000, "mdate": 1667782462720, "content": {"title": "Dual Attention Networks for Visual Reference Resolution in Visual Dialog", "abstract": "Gi-Cheon Kang, Jaeseo Lim, Byoung-Tak Zhang. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019."}}
