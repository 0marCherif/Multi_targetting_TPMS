{"id": "1zaoVA_z8Q", "cdate": 1663850162058, "mdate": null, "content": {"title": "SemSup-XC: Semantic Supervision for Extreme Classification", "abstract": "Extreme classification (XC) considers the scenario of predicting over a very large number of classes (thousands to millions), with real-world applications including serving search engine results, e-commerce product tagging, and news article  classification. The zero-shot version of this task involves the addition of new categories at test time, requiring models to generalize to novel classes without\nadditional training data (e.g. one may add a new class \u201cfidget spinner\u201d for ecommerce product tagging). In this paper, we develop SEMSUP-XC, a model that achieves state-of-the-art zero-shot (ZS) and few-shot (FS) performance on three extreme classification benchmarks spanning the domains of law, e-commerce, and Wikipedia. SEMSUP-XC builds upon the recently proposed framework of semantic supervision that uses semantic label descriptions to represent and generalize to classes (e.g., \u201cfidget spinner\u201d described as \u201cA popular spinning toy intended as a stress reliever\u201d). Specifically, we use a combination of contrastive learning, a hybrid lexico-semantic similarity module and automated description collection to train SEMSUP-XC efficiently over extremely large class spaces. SEMSUP-XC\nsignificantly outperforms baselines and state-of-the-art models on all three datasets, by up to 6-10 precision@1 points on zero-shot classification and >10 precision points on few-shot classification, with similar gains for recall@10 (3 for zero-shot and 2 for few-shot). Our ablation studies and qualitative analyses demonstrate the relative importance of our various improvements and show that SEMSUP-XC\u2019s\nautomated pipeline offers a consistently efficient method for extreme classification."}}
{"id": "BeGnZwhsZ9", "cdate": 1647195908038, "mdate": null, "content": {"title": "Semantic Supervision: Enabling Generalization over Output Spaces", "abstract": "In this paper, we propose semantic supervision (SemSup) - a unified paradigm for training classifiers that generalize over output spaces. In contrast to standard classification, which treats classes as discrete symbols, SemSup represents them as dense vector features obtained from descriptions of classes (e.g., \"The cat is a small carnivorous mammal\"). This allows the output space to be unbounded (in the space of descriptions) and enables models to generalize both over unseen inputs and unseen outputs. Specifically, SemSup enables four types of generalization, to - (1) unseen class descriptions, (2) unseen classes, (3) unseen super-classes, and (4) unseen tasks. Through experiments on four classification datasets, two input modalities (text and images), and two output description modalities (text and JSON), we show that our SemSup models significantly outperform baselines. For instance, our model outperforms baselines by $40\\%$ and $15\\%$ precision points on unseen descriptions and classes, respectively, on a news categorization dataset (RCV1). SemSup can serve as a pathway for scaling neural models to large unbounded output spaces and enabling better generalization and model reuse for unseen tasks and domains."}}
{"id": "0rhamyuqRf", "cdate": 1577836800000, "mdate": 1629682936236, "content": {"title": "Guiding Attention for Self-Supervised Learning with Transformers", "abstract": "Ameet Deshpande, Karthik Narasimhan. Findings of the Association for Computational Linguistics: EMNLP 2020. 2020."}}
{"id": "-DgcQSmYSvH", "cdate": 1577836800000, "mdate": 1629682936227, "content": {"title": "CLEVR Parser: A Graph Parser Library for Geometric Learning on Language Grounded Image Scenes", "abstract": "The CLEVR dataset has been used extensively in language grounded visual reasoning in Machine Learning (ML) and Natural Language Processing (NLP) domains. We present a graph parser library for CLEVR, that provides functionalities for object-centric attributes and relationships extraction, and construction of structural graph representations for dual modalities. Structural order-invariant representations enable geometric learning and can aid in downstream tasks like language grounding to vision, robotics, compositionality, interpretability, and computational grammar construction. We provide three extensible main components - parser, embedder, and visualizer that can be tailored to suit specific learning setups. We also provide out-of-the-box functionality for seamless integration with popular deep graph neural network (GNN) libraries. Additionally, we discuss downstream usage and applications of the library, and how it accelerates research for the NLP research community."}}
{"id": "PEBwm5BFU4s", "cdate": 1546300800000, "mdate": 1629682936247, "content": {"title": "FigureNet : A Deep Learning model for Question-Answering on Scientific Plots", "abstract": "Deep Learning has managed to push boundaries in a wide variety of tasks. One area of interest is to tackle problems in reasoning and understanding, with an aim to emulate human intelligence. In this work, we describe a deep learning model that addresses the reasoning task of question-answering on categorical plots. We introduce a novel architecture FigureNet, that learns to identify various plot elements, quantify the represented values and determine a relative ordering of these statistical values. We test our model on the FigureQA dataset which provides images and accompanying questions for scientific plots like bar graphs and pie charts, augmented with rich annotations. Our approach outperforms the state-of-the-art Relation Networks baseline by approximately 7% on this dataset, with a training time that is over an order of magnitude lesser."}}
{"id": "EXbcj-AV10_", "cdate": 1546300800000, "mdate": 1629682936229, "content": {"title": "Leveraging Ontological Knowledge for Neural Language Models", "abstract": "Neural Language Models such as Word2Vec and GloVe have been shown to encode semantic relatedness between words. Improvements in unearthing these embeddings can ameliorate performance in numerous downstream applications such as sentiment analysis, question answering, and dialogue generation. Lexical ontologies such as WordNet are known to supply information about semantic similarity rather than relatedness. Further, extracting word em-beddings from small corpora is daunting for data-hungry neural networks. This work shows how methods that conflate Word2Vec and Ontologies can achieve better performance, reduce training time and help adapt to domains with a minimum amount of data."}}
{"id": "Syez3j0cKX", "cdate": 1538087849543, "mdate": null, "content": {"title": "Dissecting an Adversarial framework for Information Retrieval", "abstract": "Recent advances in Generative Adversarial Networks facilitated by improvements to the framework and successful application to various problems has resulted in extensions to multiple domains. IRGAN attempts to leverage the framework for Information-Retrieval (IR), a task that can be described as modeling the correct conditional probability distribution p(d|q) over the documents (d), given the query (q). The work that proposes IRGAN claims that optimizing their minimax loss function will result in a generator which can learn the distribution, but their setup and baseline term steer the model away from an exact adversarial formulation, and this work attempts to point out certain inaccuracies in their formulation. Analyzing their loss curves gives insight into possible mistakes in the loss functions and better performance can be obtained by using the co-training like setup we propose, where two models are trained in a co-operative rather than an adversarial fashion."}}
