{"id": "KmoOwyzCX_", "cdate": 1664725482310, "mdate": null, "content": {"title": "When Does Re-initialization Work?", "abstract": "Re-initializing a neural network during training has been observed to improve generalization in recent works. Yet it is neither widely adopted in deep learning practice nor is it often used in state-of-the-art training protocols. This raises the question of when re-initialization works, and whether it should be used together with regularization techniques such as data augmentation, weight decay and learning rate schedules. In this work, we conduct an extensive empirical comparison of standard training with a selection of re-initialization methods to answer this question, training over 15,000 models on a variety of image classification benchmarks. We first establish that such methods are consistently beneficial for generalization in the absence of any other regularization. However, when deployed alongside other carefully tuned regularization techniques, re-initialization methods offer little to no added benefit for generalization, although optimal generalization performance becomes less sensitive to the choice of learning rate and weight decay hyperparameters. To investigate the impact of re-initialization methods on noisy data, we also consider learning under label noise. Surprisingly, in this case, re-initialization significantly improves upon standard training, even in the presence of other carefully tuned regularization techniques."}}
{"id": "va3b82vZPa", "cdate": 1640995200000, "mdate": 1681651557787, "content": {"title": "When Does Re-initialization Work?", "abstract": ""}}
{"id": "oQ-R9YxpP-7", "cdate": 1609459200000, "mdate": 1681728661600, "content": {"title": "Spectral Normalisation for Deep Reinforcement Learning: An Optimisation Perspective", "abstract": "Most of the recent deep reinforcement learning advances take an RL-centric perspective and focus on refinements of the training objective. We diverge from this view and show we can recover the perf..."}}
{"id": "HcbQBjXBlq", "cdate": 1609459200000, "mdate": 1645718331229, "content": {"title": "A study on the plasticity of neural networks", "abstract": "One aim shared by multiple settings, such as continual learning or transfer learning, is to leverage previously acquired knowledge to converge faster on the current task. Usually this is done through fine-tuning, where an implicit assumption is that the network maintains its plasticity, meaning that the performance it can reach on any given task is not affected negatively by previously seen tasks. It has been observed recently that a pretrained model on data from the same distribution as the one it is fine-tuned on might not reach the same generalisation as a freshly initialised one. We build and extend this observation, providing a hypothesis for the mechanics behind it. We discuss the implication of losing plasticity for continual learning which heavily relies on optimising pretrained models."}}
{"id": "0dYLQkR9NU", "cdate": 1609459200000, "mdate": 1681728662134, "content": {"title": "Spectral Normalisation for Deep Reinforcement Learning: an Optimisation Perspective", "abstract": "Most of the recent deep reinforcement learning advances take an RL-centric perspective and focus on refinements of the training objective. We diverge from this view and show we can recover the performance of these developments not by changing the objective, but by regularising the value-function estimator. Constraining the Lipschitz constant of a single layer using spectral normalisation is sufficient to elevate the performance of a Categorical-DQN agent to that of a more elaborated \\rainbow{} agent on the challenging Atari domain. We conduct ablation studies to disentangle the various effects normalisation has on the learning dynamics and show that is sufficient to modulate the parameter updates to recover most of the performance of spectral normalisation. These findings hint towards the need to also focus on the neural component and its learning dynamics to tackle the peculiarities of Deep Reinforcement Learning."}}
{"id": "wJbgg5ZLEaW", "cdate": 1483228800000, "mdate": null, "content": {"title": "Learning to Maximize Return in a Stag Hunt Collaborative Scenario through Deep Reinforcement Learning", "abstract": "In this paper we present a deep reinforcement learning approach for learning to play a time extended social dilemma game in a simulated environment. Agents face different types of adversaries with different levels of commitment to a collaborative strategy. Our method builds on recent advances in policy gradient training using deep neural networks. We investigate multiple stochastic gradient algorithms such as Reinforce or Actor Critic with auxiliary tasks for faster convergence."}}
{"id": "9ILk_sQYuqG", "cdate": 1483228800000, "mdate": null, "content": {"title": "Random Projection Based Representations for Learning Policies in Deterministic Atari Games", "abstract": "Recent advances in sample efficient reinforcement learning algorithms in quasi-deterministic environments highlight the requirement for computationally inexpensive visual representations. Here we investigate non-parametric dimensionality reduction techniques based on random linear transformations and we provide empirical evidence on the importance of high-variance projections using sparse random matrices in the context of episodic controllers learning deterministic policies. We also propose a novel Maximum-Variance Random Projection and improve on the performance of the original Model-Free Episodic Control results with respect to both sample efficiency and final average score."}}
{"id": "Y4ZE-SH4o6B", "cdate": 1420070400000, "mdate": null, "content": {"title": "Learning Collaboration in Reactive Agents Ensembles", "abstract": "In this paper we present an empirical study on using reinforcement learning techniques in reactive multi-agent systems where agents have local perception of the environment and limited communication capabilities. Agents have no a priori information about the task to be solved in the environment and no interpreted representation of the sensory input. We investigate a scenario in which agents receive a higher reward if they coordinate to solve the proposed task. We show that using a variant of Q-Learning agents can learn to value collaboration and self-organize to get higher rewards. The results are promising, but better techniques are suggested to solve the problems that arise from state space explosion."}}
{"id": "Yw0oLfbieh", "cdate": 1356998400000, "mdate": null, "content": {"title": "An Argumentation Framework for BDI Agents", "abstract": "This article presents a practical approach to building argumentative BDI agents. As in the last years the domain of argumentation reached maturity and offers now a very rich and well structured abstract theory, the challenge now is to put this work into practice and prove its usefulness in real applications. There is a high interest from the multi-agent systems community in applying argumentation for agents\u2019 defeasible reasoning. The main goal of the work presented in this paper was to provide the means to enable argumentative capabilities in BDI agents. For this reason, Jason, a platform for the development of multi-agent systems using the BDI model of agency, was extended with a module for argumentation. The proposed argumentation module is decoupled from the BDI reasoning cycle as it operates only on the belief base of the agents and does not interfere in the execution of plans, creation of goals, or agent\u2019s commitments. Although no protocol for argumentation-based dialogues is proposed here, agents can engage in any such dialogues as the argumentation module makes suggestions of attacks to put forward in conversation or gives structured justifications for different beliefs. An instantiation of Dung\u2019s abstract framework is used with state of the art structure of arguments and ways of attack and defeat between arguments."}}
