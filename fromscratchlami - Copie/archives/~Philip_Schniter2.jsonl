{"id": "fRVW1d7y0abR", "cdate": 1640995200000, "mdate": 1668446377243, "content": {"title": "A Regularized Conditional GAN for Posterior Sampling in Inverse Problems", "abstract": "In image recovery problems, one seeks to reconstruct an image from distorted, incomplete, and/or noise-corrupted measurements. Such problems arise in magnetic resonance imaging (MRI), computed tomography, deblurring, super-resolution, inpainting, phase retrieval, image-to-image translation, and other applications. Given a training set of signal/measurement pairs, we design a method to generate posterior samples rapidly and accurately. In particular, we propose a regularized conditional Wasserstein GAN that generates dozens of high-quality posterior samples per second. Our regularization comprises an $\\ell_1$ penalty and an adaptively weighted standard-deviation reward. Using quantitative evaluation metrics like conditional Fr\\'{e}chet inception distance, we demonstrate that our method produces state-of-the-art posterior samples in both multicoil MRI and large-scale inpainting applications."}}
{"id": "dO1tBRPeDx4M", "cdate": 1640995200000, "mdate": 1668446377377, "content": {"title": "Denoising Generalized Expectation-Consistent Approximation for MRI Image Recovery", "abstract": "To solve inverse problems, plug-and-play (PnP) methods replace the proximal step in a convex optimization algorithm with a call to an application-specific denoiser, often implemented using a deep neural network (DNN). Although such methods yield accurate solutions, they can be improved. For example, denoisers are usually designed/trained to remove white Gaussian noise, but the denoiser input error in PnP algorithms is usually far from white or Gaussian. Approximate message passing (AMP) methods provide white and Gaussian denoiser input error, but only when the forward operator is sufficiently random. In this work, for Fourier-based forward operators, we propose a PnP algorithm based on generalized expectation-consistent (GEC) approximation -- a close cousin of AMP -- that offers predictable error statistics at each iteration, as well as a new DNN denoiser that leverages those statistics. We apply our approach to magnetic resonance (MR) image recovery and demonstrate its advantages over existing PnP and AMP methods."}}
{"id": "aX6d3wma7al", "cdate": 1640995200000, "mdate": 1668446377122, "content": {"title": "MRI Recovery with a Self-Calibrated Denoiser", "abstract": "Plug-and-play (PnP) methods that employ application-specific denoisers have been proposed to solve inverse problems, including MRI reconstruction. However, training application-specific denoisers is not feasible for many applications due to the lack of training data. In this work, we propose a PnP-inspired recovery method that does not require data beyond the single, incomplete set of measurements. The proposed self-supervised method, called recovery with a self-calibrated denoiser (ReSiDe), trains the denoiser from the patches of the image being recovered. The denoiser training and a call to the denoising subroutine are performed in each iteration of a PnP algorithm, leading to a progressive refinement of the reconstructed image. For validation, we compare ReSiDe with a compressed sensing-based method and a PnP method with BM3D denoising using single-coil MRI brain data."}}
{"id": "DsdCuIfieQ7C", "cdate": 1640995200000, "mdate": 1668446377737, "content": {"title": "Expectation Consistent Plug-and-Play for MRI", "abstract": "For image recovery problems, plug-and-play (PnP) methods have been developed that replace the proximal step in an optimization algorithm with a call to an application-specific denoiser, often implemented using a deep neural network. Although such methods have been successful, they can be improved. For example, the denoiser is often trained using white Gaussian noise, while PnP\u2019s denoiser input error is often far from white and Gaussian, with statistics that are difficult to predict from iteration to iteration. PnP methods based on approximate message passing (AMP) are an exception, but only when the forward operator behaves like a large random matrix. In this work, we design a PnP method using the expectation consistent (EC) approximation algorithm, a generalization of AMP, that offers predictable error statistics at each iteration, from which a deep-net denoiser can be effectively trained."}}
{"id": "9Q-ato7bl5M", "cdate": 1640995200000, "mdate": 1668446378142, "content": {"title": "Expectation Consistent Plug-and-Play for MRI", "abstract": "For image recovery problems, plug-and-play (PnP) methods have been developed that replace the proximal step in an optimization algorithm with a call to an application-specific denoiser, often implemented using a deep neural network. Although such methods have been successful, they can be improved. For example, the denoiser is often trained using white Gaussian noise, while PnP's denoiser input error is often far from white and Gaussian, with statistics that are difficult to predict from iteration to iteration. PnP methods based on approximate message passing (AMP) are an exception, but only when the forward operator behaves like a large random matrix. In this work, we design a PnP method using the expectation consistent (EC) approximation algorithm, a generalization of AMP, that offers predictable error statistics at each iteration, from which a deep-net denoiser can be effectively trained."}}
{"id": "oRF0-zGYAei", "cdate": 1634622668961, "mdate": null, "content": {"title": "Matching Plug-and-Play Algorithms to the Denoiser", "abstract": "To solve inverse problems, plug-and-play (PnP) methods have been developed that replace the proximal step in a convex optimization algorithm with a call to an application-specific denoiser, often implemented using a deep neural network (DNN). Although such methods have been successful, they can be improved. For example, denoisers are usually designed/trained to remove white Gaussian noise, but the denoiser input error in PnP algorithms is usually far from white or Gaussian. Approximate message passing (AMP) methods provide white and Gaussian denoiser input error, but only when the forward operator is a large random matrix. In this work, we propose a PnP algorithm based on generalized expectation consistent (GEC) approximation that offers predictable error statistics at each iteration, as well as a new DNN denoiser that leverages those statistics.\n"}}
{"id": "gDsyADeesL2", "cdate": 1609459200000, "mdate": 1668446377180, "content": {"title": "Deep Neural Networks for Radar Waveform Classification", "abstract": "We consider the problem of classifying radar pulses given raw I/Q waveforms in the presence of noise and absence of synchronization. We also consider the problem of classifying multiple superimposed radar pulses. For both, we design deep neural networks (DNNs) that are robust to synchronization, pulse width, and SNR. Our designs yield more than 100x reduction in error-rate over the current state-of-the-art."}}
{"id": "YvX_RXJKt3dE", "cdate": 1609459200000, "mdate": 1668446377739, "content": {"title": "Phase-Modulated Radar Waveform Classification Using Deep Networks", "abstract": "We consider the problem of classifying radar pulses given raw I/Q waveforms in the presence of noise and absence of synchronization. We also consider the problem of classifying multiple superimposed radar pulses. For both, we design deep neural networks (DNNs) that are robust to synchronization, pulse width, and SNR. Our designs yield more than 100x reduction in error-rate over the previous state-of-the-art."}}
{"id": "NIKBkgmev_P", "cdate": 1609459200000, "mdate": 1668446377223, "content": {"title": "MRI Image Recovery using Damped Denoising Vector AMP", "abstract": "Motivated by image recovery in magnetic resonance imaging (MRI), we propose a new approach to solving linear inverse problems based on iteratively calling a deep neural-network, sometimes referred to as plug-and-play recovery. Our approach is based on the vector approximate message passing (VAMP) algorithm, which is known for mean-squared error (MSE)-optimal recovery under certain conditions. The forward operator in MRI, however, does not satisfy these conditions, and thus we design new damping and initialization schemes to help VAMP. The resulting DD-VAMP++ algorithm is shown to outperform existing algorithms in convergence speed and accuracy when recovering images from the fastMRI database for the practical case of Cartesian sampling."}}
{"id": "18jdW4S9exa", "cdate": 1609459200000, "mdate": 1668446377772, "content": {"title": "Sketching Data Sets for Large-Scale Learning: Keeping only what you need", "abstract": "Big data can be a blessing: with very large training data sets it becomes possible to perform complex learning tasks with unprecedented accuracy. Yet, this improved performance comes at the price of enormous computational challenges. Thus, one may wonder: Is it possible to leverage the information content of huge data sets while keeping computational resources under control? Can this also help solve some of the privacy issues raised by large-scale learning? This is the ambition of compressive learning, where the data set is massively compressed before learning. Here, a \"sketch\" is first constructed by computing carefully chosen nonlinear random features [e.g., random Fourier (RF) features] and averaging them over the whole data set. Parameters are then learned from the sketch, without access to the original data set. This article surveys the current state of the art in compressive learning, including the main concepts and algorithms, their connections with established signal processing methods, existing theoretical guarantees on both information preservation and privacy preservation, and important open problems. For an extended version of this article that contains additional references and more in-depth discussions on a variety of topics, see [1]."}}
