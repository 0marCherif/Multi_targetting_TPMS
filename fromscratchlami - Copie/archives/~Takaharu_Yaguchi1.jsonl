{"id": "rTu7fkxAsC", "cdate": 1672531200000, "mdate": 1696180691768, "content": {"title": "FINDE: Neural Differential Equations for Finding and Preserving Invariant Quantities", "abstract": ""}}
{"id": "cK4rwl3Q867", "cdate": 1672531200000, "mdate": 1696180691768, "content": {"title": "Good Lattice Training: Physics-Informed Neural Networks Accelerated by Number Theory", "abstract": "Physics-informed neural networks (PINNs) offer a novel and efficient approach to solving partial differential equations (PDEs). Their success lies in the physics-informed loss, which trains a neural network to satisfy a given PDE at specific points and to approximate the solution. However, the solutions to PDEs are inherently infinite-dimensional, and the distance between the output and the solution is defined by an integral over the domain. Therefore, the physics-informed loss only provides a finite approximation, and selecting appropriate collocation points becomes crucial to suppress the discretization errors, although this aspect has often been overlooked. In this paper, we propose a new technique called good lattice training (GLT) for PINNs, inspired by number theoretic methods for numerical analysis. GLT offers a set of collocation points that are effective even with a small number of points and for multi-dimensional spaces. Our experiments demonstrate that GLT requires 2--20 times fewer collocation points (resulting in lower computational cost) than uniformly random sampling or Latin hypercube sampling, while achieving competitive performance."}}
{"id": "tLScKVhcCR", "cdate": 1663849929821, "mdate": null, "content": {"title": "FINDE: Neural Differential Equations for Finding and Preserving Invariant Quantities", "abstract": "Many real-world dynamical systems are associated with first integrals (a.k.a. invariant quantities), which are quantities that remain unchanged over time. The discovery and understanding of first integrals are fundamental and important topics both in the natural sciences and in industrial applications. First integrals arise from the conservation laws of system energy, momentum, and mass, and from constraints on states; these are typically related to specific geometric structures of the governing equations. Existing neural networks designed to ensure such first integrals have shown excellent accuracy in modeling from data. However, these models incorporate the underlying structures, and in most situations where neural networks learn unknown systems, these structures are also unknown. This limitation needs to be overcome for scientific discovery and modeling of unknown systems. To this end, we propose first integral-preserving neural differential equation (FINDE). By leveraging the projection method and the discrete gradient method, FINDE finds and preserves first integrals from data, even in the absence of prior knowledge about underlying structures. Experimental results demonstrate that FINDE can predict future states of target systems much longer and find various quantities consistent with well-known first integrals in a unified manner."}}
{"id": "xDaoT2zlJ0r", "cdate": 1652737492459, "mdate": null, "content": {"title": "FINDE: Neural Differential Equations for Finding and Preserving Invariant Quantities", "abstract": "Neural networks have shown promise for modeling dynamical systems from data. Recent models, such as Hamiltonian neural networks, have been designed to ensure known geometric structures of target systems and have shown excellent modeling accuracy. However, in most situations where neural networks learn unknown systems, their underlying structures are also unknown. Even in such cases, one can expect that target systems are associated with first integrals (a.k.a. invariant quantities), which are quantities remaining unchanged over time. First integrals come from the conservation laws of system energy, momentum, and mass, from constraints on states, and from other features of governing equations. By leveraging projection methods and discrete gradient methods, we propose first integral-preserving neural differential equations (FINDE). The proposed FINDE finds and preserves first integrals from data, even in the absence of prior knowledge about the underlying structures. Experimental results demonstrate that the proposed FINDE is able to predict future states of given systems much longer and find various quantities consistent with well-known first integrals of the systems in a unified manner."}}
{"id": "qFFHr1ZLgy5", "cdate": 1640995200000, "mdate": 1682326482238, "content": {"title": "KAM Theory Meets Statistical Learning Theory: Hamiltonian Neural Networks with Non-zero Training Loss", "abstract": "Many physical phenomena are described by Hamiltonian mechanics using an energy function (Hamiltonian). Recently, the Hamiltonian neural network, which approximates the Hamiltonian by a neural network, and its extensions have attracted much attention. This is a very powerful method, but theoretical studies are limited. In this study, by combining the statistical learning theory and KAM theory, we provide a theoretical analysis of the behavior of Hamiltonian neural networks when the learning error is not completely zero. A Hamiltonian neural network with non-zero errors can be considered as a perturbation from the true dynamics, and the perturbation theory of the Hamilton equation is widely known as KAM theory. To apply KAM theory, we provide a generalization error bound for Hamiltonian neural networks by deriving an estimate of the covering number of the gradient of the multi-layer perceptron, which is the key ingredient of the model. This error bound gives a sup-norm bound on the Hamiltonian that is required in the application of KAM theory."}}
{"id": "oFoWt1exztE", "cdate": 1640995200000, "mdate": 1682326482228, "content": {"title": "FINDE: Neural Differential Equations for Finding and Preserving Invariant Quantities", "abstract": "Many real-world dynamical systems are associated with first integrals (a.k.a. invariant quantities), which are quantities that remain unchanged over time. The discovery and understanding of first integrals are fundamental and important topics both in the natural sciences and in industrial applications. First integrals arise from the conservation laws of system energy, momentum, and mass, and from constraints on states; these are typically related to specific geometric structures of the governing equations. Existing neural networks designed to ensure such first integrals have shown excellent accuracy in modeling from data. However, these models incorporate the underlying structures, and in most situations where neural networks learn unknown systems, these structures are also unknown. This limitation needs to be overcome for scientific discovery and modeling of unknown systems. To this end, we propose first integral-preserving neural differential equation (FINDE). By leveraging the projection method and the discrete gradient method, FINDE finds and preserves first integrals from data, even in the absence of prior knowledge about underlying structures. Experimental results demonstrate that FINDE can predict future states of target systems much longer and find various quantities consistent with well-known first integrals in a unified manner."}}
{"id": "4h4oqp-ATxb", "cdate": 1621630138537, "mdate": null, "content": {"title": "Neural Symplectic Form: Learning Hamiltonian Equations on General Coordinate Systems", "abstract": "In recent years, substantial research on the methods for learning Hamiltonian equations has been conducted. Although these approaches are very promising, the commonly used representation of the Hamilton equation uses the generalized momenta, which are generally unknown. Therefore, the training data must be represented in this unknown coordinate system, and this causes difficulty in applying the model to real data. Meanwhile, Hamiltonian equations also have a coordinate-free expression that is expressed by using the symplectic 2-form. In this study, we propose a model that learns the symplectic form from data using neural networks, thereby providing a method for learning Hamiltonian equations from data represented in general coordinate systems, which are not limited to the generalized coordinates and the generalized momenta. Consequently, the proposed method is capable not only of modeling target equations of both Hamiltonian and Lagrangian formalisms but also of extracting unknown Hamiltonian structures hidden in the data. For example, many polynomial ordinary differential equations such as the Lotka-Volterra equation are known to admit non-trivial Hamiltonian structures, and our numerical experiments show that such structures can be certainly learned from data. Technically, each symplectic 2-form is associated with a skew-symmetric matrix, but not all skew-symmetric matrices define the symplectic 2-form. In the proposed method, using the fact that symplectic 2-forms are derived as the exterior derivative of certain differential 1-forms, we model the differential 1-form by neural networks, thereby improving the efficiency of learning."}}
{"id": "46J_l-cpc1W", "cdate": 1621629891507, "mdate": null, "content": {"title": "Symplectic Adjoint Method for Exact Gradient of Neural ODE with Minimal Memory", "abstract": "A neural network model of a differential equation, namely neural ODE, has enabled the learning of continuous-time dynamical systems and probabilistic distributions with high accuracy. The neural ODE uses the same network repeatedly during a numerical integration. The memory consumption of the backpropagation algorithm is proportional to the number of uses times the network size. This is true even if a checkpointing scheme divides the computation graph into sub-graphs. Otherwise, the adjoint method obtains a gradient by a numerical integration backward in time. Although this method consumes memory only for a single network use, it requires high computational cost to suppress numerical errors. This study proposes the symplectic adjoint method, which is an adjoint method solved by a symplectic integrator. The symplectic adjoint method obtains the exact gradient (up to rounding error) with memory proportional to the number of uses plus the network size. The experimental results demonstrate that the symplectic adjoint method consumes much less memory than the naive backpropagation algorithm and checkpointing schemes, performs faster than the adjoint method, and is more robust to rounding errors."}}
{"id": "wDT5cb7LY5", "cdate": 1609459200000, "mdate": 1682326482129, "content": {"title": "Error Factor Analysis of DNN-based Fingerprinting Localization through Virtual Space", "abstract": "Localization using low-power wireless devices is one of the promising methods even in outdoor environments because the Global Navigation Satellite System (GNSS) devices generally consume much power to result in frequent battery replacements. Received Signal Strength Indicator (RSSI) is often utilized in such methods due to availability in most wireless devices. Even in outdoor open space, however, various factors such as path-loss characteristics and antenna directivity possibly cause difficulty in RSSI-based localization. This paper investigates the error factors of Deep Neural Network (DNN)-based fingerprinting localization in terms of antenna directivity. Most existing studies have analyzed the error factor in localization algorithms or methods to improve accuracy. On the other hand, the error factors on the environments had not been paid attention yet at all. In this paper, we investigate distance errors of the DNN-based fingerprinting method in terms of antenna directivity as one of the environmental factors using virtual space reproduced in a computer. Through experiments where an actual Bluetooth Low Energy (BLE) tag system was emulated, we found that the tag antenna directivity could be the factor to worsen the localization accuracy while receiver antenna directivity to better."}}
{"id": "lr-HplVzwAT", "cdate": 1609459200000, "mdate": 1682326482241, "content": {"title": "Symplectic Adjoint Method for Exact Gradient of Neural ODE with Minimal Memory", "abstract": "A neural network model of a differential equation, namely neural ODE, has enabled the learning of continuous-time dynamical systems and probabilistic distributions with high accuracy. The neural ODE uses the same network repeatedly during a numerical integration. The memory consumption of the backpropagation algorithm is proportional to the number of uses times the network size. This is true even if a checkpointing scheme divides the computation graph into sub-graphs. Otherwise, the adjoint method obtains a gradient by a numerical integration backward in time. Although this method consumes memory only for a single network use, it requires high computational cost to suppress numerical errors. This study proposes the symplectic adjoint method, which is an adjoint method solved by a symplectic integrator. The symplectic adjoint method obtains the exact gradient (up to rounding error) with memory proportional to the number of uses plus the network size. The experimental results demonstrate that the symplectic adjoint method consumes much less memory than the naive backpropagation algorithm and checkpointing schemes, performs faster than the adjoint method, and is more robust to rounding errors."}}
