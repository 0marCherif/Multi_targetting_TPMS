{"id": "XfpmehHGo2", "cdate": 1676827100184, "mdate": null, "content": {"title": "Optimistic Thompson Sampling-based Algorithms for Episodic Reinforcement Learning", "abstract": "We propose two  Thompson Sampling-like, model-based learning algorithms for episodic Markov decision processes (MDPs) with a finite time horizon. Our proposed algorithms are inspired by Optimistic Thompson Sampling (O-TS), empirically studied in Chapelle and Li [2011], May et al. [2012] for stochastic multi-armed bandits. The key idea for the original O-TS is to clip the posterior distribution in an optimistic way to ensure that the sampled models are always better than the empirical models. Both of our proposed algorithms are easy to implement and only need one posterior sample to construct an episode-dependent model. Our first algorithm, Optimistic Thompson Sampling for MDPs (O-TS-MDP), achieves a $\\widetilde{O} \\left(\\sqrt{AS^2H^4T} \\right)$ regret bound, where $S$ is the size of the state space, $A$ is the size of the action space, $H$ is the number of time-steps per episode and $T$ is the number of episodes. Our second algorithm, Optimistic Thompson Sampling plus for MDPs (O-TS-MDP$^+$),  achieves the (near)-optimal $\\widetilde{O} \\left(\\sqrt{ASH^3T} \\right)$ regret bound by taking a more aggressive clipping strategy.  Since O-TS was only empirically studied previously, we derive regret bounds of O-TS for stochastic bandits. In addition, we propose,  O-TS-Bandit$^+$, a randomized version of UCB1 [Auer et al., 2002], for stochastic bandits. Both O-TS and O-TS-Bandit$^+$ achieve the optimal $O\\left(\\frac{A\\ln(T)}{\\Delta} \\right)$ problem-dependent regret bound, where $\\Delta$ denotes the sub-optimality gap."}}
{"id": "22tWlk_NycL", "cdate": 1664731452397, "mdate": null, "content": {"title": "Target-based Surrogates for Stochastic Optimization", "abstract": "We consider minimizing functions for which it is expensive to compute the gradient. Such functions are prevalent in reinforcement learning, imitation learning and bilevel optimization. Our target optimization framework uses the (expensive) gradient computation to construct surrogate functions in a \\emph{target space} (e.g. the logits output by a linear model for classification) that can be minimized efficiently. This allows for multiple parameter updates to the model, amortizing the cost of gradient computation. In the full-batch setting, we prove that our surrogate is a global upper-bound on the loss, and can be (locally) minimized using a black-box optimization algorithm. We prove that the resulting majorization-minimization algorithm ensures convergence to a stationary point of the loss. Next, we instantiate our framework in the stochastic setting and propose the $SSO$ algorithm that can be viewed as projected stochastic gradient descent in the target space. This connection enables us to use standard stochastic optimization algorithms to construct surrogates which can be minimized using deterministic optimization. Our experiments on supervised learning and imitation learning exhibit the benefits of target optimization, even in stochastic settings. "}}
{"id": "v0vaaGQC3GR", "cdate": 1664731451689, "mdate": null, "content": {"title": "Fast Convergence of Greedy 2-Coordinate Updates for Optimizing with an Equality Constraint", "abstract": "We consider minimizing a smooth function subject to an equality constraint. We analyze a greedy 2-coordinate update algorithm, and prove that greedy coordinate selection leads to faster convergence than random selection (under the Polyak-\\L{}ojasiewicz assumption). Our simple analysis exploits am equivalence between the greedy 2-coordinate update and equality-constrained steepest descent in the L1-norm. Unlike previous 2-coordinate analyses, our convergence rate is dimension independent."}}
{"id": "CzLyJCo-a7", "cdate": 1664731446764, "mdate": null, "content": {"title": "Fast Convergence of Random Reshuffling under Interpolation and the Polyak-\\L ojasiewicz Condition", "abstract": "Modern machine learning models are often over-parameterized and as a result they can interpolate\nthe training data. Under such a scenario, we study the convergence properties of a sampling-\nwithout-replacement variant of Stochastic Gradient Descent (SGD), known as Random Reshuffling\n(RR). Unlike SGD that samples data with replacement at every iteration, RR chooses a random\npermutation of data at the beginning of each epoch. For under-parameterized models, it has been\nrecently shown that RR converges faster than SGD when the number of epochs is larger than the\ncondition number (\u03ba) of the problem under standard assumptions like strong convexity. However,\nprevious works do not show that RR outperforms SGD under interpolation for strongly convex\nobjectives. Here, we show that for the class of Polyak-\u0141ojasiewicz (PL) functions that generalizes\nstrong convexity, RR can outperform SGD as long as the number of samples (n) is less than the\nparameter (\u03c1) of a strong growth condition (SGC)."}}
{"id": "1aybhSfabqh", "cdate": 1664194170041, "mdate": null, "content": {"title": "Practical Structured Riemannian Optimization with Momentum by using Generalized Normal Coordinates", "abstract": "Adding momentum into Riemannian optimization is computationally challenging due to the intractable ODEs needed to define the exponential and parallel transport maps.\nWe address these issues for Gaussian Fisher-Rao manifolds by proposing new local coordinates to exploit sparse structures and efficiently approximate the ODEs, which results in a numerically stable update scheme.\nOur approach extends the structured natural-gradient descent method of Lin et al. (2021a) by incorporating momentum into it and scaling the method for large-scale applications arising in numerical optimization and deep learning."}}
{"id": "a65YK0cqH8g", "cdate": 1663850460519, "mdate": null, "content": {"title": "Noise Is Not the Main Factor Behind the Gap Between Sgd and Adam on Transformers, But Sign Descent Might Be", "abstract": "The success of the Adam optimizer on a wide array of architectures has made it the default in settings where stochastic gradient descent (SGD) performs poorly. However, our theoretical understanding of this discrepancy is lagging, preventing the development of significant improvements on either algorithm. Recent work advances the hypothesis that Adam and other heuristics like gradient clipping outperform SGD on language tasks because the distribution of the error induced by sampling has heavy tails. This suggests that Adam outperform SGD because it uses a more robust gradient estimate. We evaluate this hypothesis by varying the batch size, up to the entire dataset, to control for stochasticity. We present evidence that stochasticity and heavy-tailed noise  are not major factors in the performance gap between SGD and Adam. Rather, Adam performs better as the batch size increases,  while SGD is less effective at taking advantage of the reduction in noise. This raises the question as to why Adam outperforms SGD in the full-batch setting. Through further investigation of simpler variants of SGD, we find that  the behavior of Adam with large batches is similar to sign descent with momentum."}}
{"id": "bR0K-nz1-6p", "cdate": 1634067449530, "mdate": null, "content": {"title": "A Closer Look at Gradient Estimators with Reinforcement Learning as Inference", "abstract": "The concept of reinforcement learning as inference (RLAI) has led to the creation of a variety of popular algorithms in deep reinforcement learning. Unfortunately, most research in this area relies on wider algorithmic innovations not necessarily relevant to such frameworks. Additionally, many seemingly unimportant modifications made to these algorithms, actually produce inconsistencies with the original inference problem posed by RLAI. Taking a divergence minimization perspective, this work considers some of the practical merits and theoretical issues created by the choice of loss function minimized in the policy update for off-policy reinforcement learning. Our results show that while the choice of divergence rarely has a major affect on the sample efficiency of the algorithm, it can have important practical repercussions on ease of implementation, computational efficiency, and restrictions to the distribution over actions."}}
{"id": "KvDedKtOX7B", "cdate": 1634067443099, "mdate": null, "content": {"title": "An Empirical Study of Non-Uniform Sampling in Off-Policy Reinforcement Learning for Continuous Control", "abstract": "Off-policy reinforcement learning (RL) algorithms can take advantage of samples generated from all previous interactions with the environment through \"experience replay\". Such methods outperform almost all on-policy and model-based alternatives in complex tasks where a structured or well parameterized model of the world does not exist. This makes them desirable for practitioners who lack domain specific knowledge, but who still require high sample efficiency. However this high performance can come at a cost. Because of additional hyperparameters introduced to efficiently learn function approximators, off-policy RL can perform poorly on new problems. To address parameter sensitivity, we show how the correct choice of non-uniform sampling for experience replay can stabilize model performance under varying environmental conditions and hyper-parameters."}}
{"id": "E5V98ZfaFi", "cdate": 1620639266897, "mdate": null, "content": {"title": "Robust Asymmetric Learning in POMDPs", "abstract": "Policies for partially observed Markov decision processes can be efficiently learned by imitating expert policies generated using asymmetric information. Unfortunately, existing approaches for this kind of imitation learning have a serious flaw: the expert does not know what the trainee cannot see, and as a result, may encourage actions that are suboptimal or unsafe under partial information. To address this issue, we derive an update which, when applied iteratively to an expert, maximizes the expected reward of the trainee\u2019s policy. Using this update, we construct a computationally efficient algorithm, adaptive asymmetric DAgger (A2D), that jointly trains the expert and trainee policies. We then show that A2D allows the trainee to safely imitate the modified expert, and outperforms policies learned either by imitating a fixed expert or through direct reinforcement learning."}}
{"id": "bXDVOdoxvH", "cdate": 1619311422152, "mdate": null, "content": {"title": "AutoRetouch: Automatic Professional Face Retouching", "abstract": "Face retouching is one of the most time-consuming steps in professional photography pipelines. The existing automated approaches blindly apply smoothing on the skin, destroying the delicate texture of the face. We present the first automatic face retouching approach that produces high-quality professional-grade results in less than two seconds. Unlike previous work, we show that our method preserves textures and distinctive features while retouching the skin. We demonstrate that our trained models generalize across datasets and are suitable for low-resolution cellphone images. Finally, we release the first large-scale, professionally retouched dataset with our baseline to encourage further work on the presented problem. "}}
