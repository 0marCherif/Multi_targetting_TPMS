{"id": "AZup6dwugb", "cdate": 1601061940096, "mdate": null, "content": {"title": "Local Policy Optimization for Trajectory-Centric Reinforcement Learning", "abstract": "The goal of this paper is to present a method for simultaneous trajectory and local stabilizing policy optimization to generate local policies for trajectory-centric model-based reinforcement learning (MBRL). This is motivated by the fact that global policy optimization for non-linear systems could be a very challenging problem both algorithmically and numerically. However, a lot of robotic manipulation tasks are trajectory-centric, and thus do not require a global model or policy. Due to inaccuracies in the learned model estimates, an open-loop trajectory optimization process mostly results in very poor performance when used on the real system. Motivated by these problems, we try to formulate the problem of trajectory optimization and local policy synthesis as a single optimization problem. It is then solved simultaneously as an instance of nonlinear programming. We provide some results for analysis as well as achieved performance of the proposed technique under some simplifying assumptions."}}
{"id": "HyxB7qwtDS", "cdate": 1569450525501, "mdate": null, "content": {"title": "Value-Aware Loss Function for Model-based Reinforcement Learning", "abstract": "We consider the problem of estimating the transition probability kernel to be used by a model-based reinforcement learning (RL) algorithm. We argue that estimating a generative model that minimizes a probabilistic loss, such as the log-loss, is an overkill because it does not take into account the underlying structure of decision problem and the RL algorithm that intends to solve it. We introduce a loss function that takes the structure of the value function into account. We provide a finite-sample upper bound for the loss function showing the dependence of the error on model approximation error, number of samples, and the complexity of the model space. We also empirically compare the method with the maximum likelihood estimator on a simple problem."}}
{"id": "SkW_ErfObS", "cdate": 1514764800000, "mdate": null, "content": {"title": "Time Series Chains: A Novel Tool for Time Series Data Mining", "abstract": "Since their introduction over a decade ago, time se-ries motifs have become a fundamental tool for time series analytics, finding diverse uses in dozens of domains. In this work we introduce Time Series Chains, which are related to, but distinct from, time series motifs. Informally, time series chains are a temporally ordered set of subsequence patterns, such that each pattern is similar to the pattern that preceded it, but the first and last patterns are arbi-trarily dissimilar. In the discrete space, this is simi-lar to extracting the text chain \u201chit, hot, dot, dog\u201d from a paragraph. The first and last words have nothing in common, yet they are connected by a chain of words with a small mutual difference. Time Series Chains can capture the evolution of systems, and help predict the future. As such, they potentially have implications for prognostics. In this work, we introduce a robust definition of time series chains, and a scalable algorithm that allows us to discover them in massive datasets."}}
{"id": "ByWZ_oWd-r", "cdate": 1514764800000, "mdate": null, "content": {"title": "Reinforcement Learning with Function-Valued Action Spaces for Partial Differential Equation Control", "abstract": "Recent work has shown that reinforcement learning (RL) is a promising approach to control dynamical systems described by partial differential equations (PDE). This paper shows how to use RL to tack..."}}
{"id": "vpI7uxcQO0C", "cdate": 1483228800000, "mdate": 1696013884556, "content": {"title": "Deep reinforcement learning for partial differential equation control", "abstract": "This paper develops a data-driven method for control of partial differential equations (PDE) based on deep reinforcement learning (RL) techniques. We design a Deep Fitted Q-Iteration (DFQI) algorithm that works directly with a high-dimensional representation of the state of PDE, thus allowing us to avoid the model order reduction step common in the conventional PDE control design approaches. We apply the DFQI algorithm to the problem of flow control for time-varying 2D convection-diffusion PDE, as a simplified model for heating, ventilating, air conditioning (HVAC) control design in a room. We also study the transfer learning of a policy learned for a PDE to another one."}}
{"id": "B1Wvxd-OZB", "cdate": 1483228800000, "mdate": null, "content": {"title": "Random Projection Filter Bank for Time Series Data", "abstract": "We propose Random Projection Filter Bank (RPFB) as a generic and simple approach to extract features from time series data. RPFB is a set of randomly generated stable autoregressive filters that are convolved with the input time series to generate the features. These features can be used by any conventional machine learning algorithm for solving tasks such as time series prediction, classification with time series data, etc. Different filters in RPFB extract different aspects of the time series, and together they provide a reasonably good summary of the time series. RPFB is easy to implement, fast to compute, and parallelizable. We provide an error upper bound indicating that RPFB provides a reasonable approximation to a class of dynamical systems. The empirical results in a series of synthetic and real-world problems show that RPFB is an effective method to extract features from time series."}}
{"id": "SJbB_l-u-r", "cdate": 1451606400000, "mdate": null, "content": {"title": "Truncated Approximate Dynamic Programming with Task-Dependent Terminal Value", "abstract": "We propose a new class of computationally fast algorithms to find close to optimal policy for Markov Decision Processes (MDP) with large finite horizon T. The main idea is that instead of planning until the time horizon T, we plan only up to a truncated horizon H \u226a T and use an estimate of the true optimal value function as the terminal value. Our approach of finding the terminal value function is to learn a mapping from an MDP to its value function by solving many similar MDPs during a training phase and fit a regression estimator. We analyze the method by providing an error propagation theorem that shows the effect of various sources of errors to the quality of the solution. We also empirically validate this approach in a real-world application of designing an energy management system for Hybrid Electric Vehicles with promising results."}}
{"id": "TkeCtVBIk9", "cdate": 1420070400000, "mdate": 1696013884556, "content": {"title": "Barycentric quantization for planning in continuous domains", "abstract": "We consider the class of planning and sequential decision making problems where the state space has continuous components, but the available actions come from a discrete set, and argue that a suitable approach for solving them could involve an appropriate quantization scheme for the continuous state variables, followed by approximate dynamic programming. We propose one such scheme based on barycentric approximations that effectively converts the continuous dynamics into a Markov decision process, and demonstrate that it can be viewed both as an approximation to the continuous dynamics, as well as a value function approximator over the continuous domain. We describe the application of this method to several hard industrial problems, and point out additional candidate problems that could be amenable to it."}}
{"id": "c95Ab_iaAm", "cdate": 1356998400000, "mdate": 1696013884557, "content": {"title": "Smart Meter Data Analysis for Power Theft Detection", "abstract": "We propose a method for power theft detection based on predictive models for technical losses in electrical distribution networks estimated entirely from data collected by smart meters in smart grids. Although the data sampling rate of smart meters is not sufficiently high to detect power theft with complete certainty, detection is still possible in a statistical decision theory sense, based on statistical models estimated from collected data sets. Even without detailed knowledge of the exact topology of the distribution network, it is possible to estimate a statistical model of the technical losses that allows indirect estimation of the non-technical losses (power theft) with high accuracy."}}
{"id": "HkEKtsbObr", "cdate": 946684800000, "mdate": null, "content": {"title": "Learning Probabilistic Models for Decision-Theoretic Navigation of Mobile Robots", "abstract": ""}}
