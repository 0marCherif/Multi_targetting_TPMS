{"id": "cYPja_wj9d", "cdate": 1652737780907, "mdate": null, "content": {"title": "Mesoscopic modeling of hidden spiking neurons", "abstract": "Can we use spiking neural networks (SNN) as generative models of multi-neuronal recordings, while taking into account that most neurons are unobserved? Modeling the unobserved neurons with large pools of hidden spiking neurons leads to severely underconstrained problems that are hard to tackle with maximum likelihood estimation. In this work, we use coarse-graining and mean-field approximations to derive a bottom-up, neuronally-grounded latent variable model (neuLVM), where the activity of the unobserved neurons is reduced to a low-dimensional mesoscopic description. In contrast to previous latent variable models, neuLVM can be explicitly mapped to a recurrent, multi-population SNN, giving it a transparent biological interpretation. We show, on synthetic spike trains, that a few observed neurons are sufficient for neuLVM to perform efficient model inversion of large SNNs, in the sense that it can recover connectivity parameters, infer single-trial latent population activity, reproduce ongoing metastable dynamics, and generalize when subjected to perturbations mimicking optogenetic stimulation."}}
{"id": "4Sh7-92GCrT", "cdate": 1623671945109, "mdate": 1623671945109, "content": {"title": "Slow processes of neurons enable a biologically plausible approximation to policy gradient", "abstract": "Recurrent neural networks underlie the astounding information processing capabilities of the brain, and play a key role in many state-of-the-art algorithms in deep reinforcement learning.  But it has remained an open question how such networks could learn from rewards in a biologically plausible manner, with synaptic plasticity that is both local and online.  We describe such an algorithm that approximates actor-critic policy gradient in recurrent neural networks. Building on an approximation of backpropagation through time (BPTT):e-prop, and using the equivalence between forward and backward view in reinforcement learning (RL), we formulate a novel learning rule for RL that is both online and local, called reward-based e-prop. This learning rule uses neuroscience inspired slow processes and top-down signals, while still being rigorously derived as an approximation to actor-critic policy gradient.  To empirically evaluate this algorithm, we consider a delayed reaching task, where an arm is controlled using a recurrent network of spiking neurons. In this task, we show that reward-based e-prop performs as well as an agent trained with actor-critic policy gradient with biologically implausible BPTT"}}
{"id": "JFQjzB1Y3J", "cdate": 1623671560460, "mdate": 1623671560460, "content": {"title": "Spike frequency adaptation supports network computations on temporally dispersed information", "abstract": "For solving tasks such as recognizing a song, answering a question, or inverting a sequence of symbols, cortical microcircuits need to integrate and manipulate information that was dispersed over time during the preceding seconds. Creating biologically realistic models for the underlying computations, especially with spiking neurons and for behaviorally relevant integration time spans, is notoriously difficult. We examine the role of spike frequency adaptation in such computations and find that it has a surprisingly large impact. The inclusion of this well known property of a substantial fraction of neurons in the neocortex \u2014 especially in higher areas of the human neocortex \u2014 moves the performance of spiking neural network models for computations on network inputs that are temporally dispersed from a fairly low level up to the performance level of the human brain."}}
{"id": "eF6JUvOkbd", "cdate": 1623671396535, "mdate": 1623671396535, "content": {"title": "Revisiting the role of synaptic plasticity and network dynamics for fast learning in spiking neural networks", "abstract": "Spike-based neural network models have so far not been able to reproduce the capability of the brain to learn from very few, often even from just a single example. We show that this deficiency of models disappears if one allows synaptic weights to store priors and other information that optimize the learning process, while using the network state to quickly absorb information from new examples. For that, it suffices to include biologically realistic neurons with spike frequency adaptation in the neural network model, and to optimize the learning process through meta-learning. We demonstrate this on a variety of tasks, including fast learning and deletion of attractors, adaptation of motor control to changes in the body, and solving the Morris water maze task \u2013 a paradigm for fast learning of navigation to a new goal."}}
{"id": "Yu8Q6341U7W", "cdate": 1621629912384, "mdate": null, "content": {"title": "Local plasticity rules can learn deep representations using self-supervised contrastive predictions", "abstract": "Learning in the brain is poorly understood and learning rules that respect biological constraints, yet yield deep hierarchical representations, are still unknown. Here, we propose a learning rule that takes inspiration from neuroscience and recent advances in self-supervised deep learning. Learning minimizes a simple layer-specific loss function and does not need to back-propagate error signals within or between layers. Instead, weight updates follow a local, Hebbian, learning rule that only depends on pre- and post-synaptic neuronal activity, predictive dendritic input and widely broadcasted modulation factors which are identical for large groups of neurons. The learning rule applies contrastive predictive learning to a causal, biological setting using saccades (i.e. rapid shifts in gaze direction). We find that networks trained with this self-supervised and local rule build deep hierarchical representations of images, speech and video. "}}
{"id": "9DEAT9pDiN", "cdate": 1621629890331, "mdate": null, "content": {"title": "Fitting summary statistics of neural data with a differentiable spiking network simulator", "abstract": "Fitting network models to neural activity is an important tool in neuroscience. A popular approach is to model a brain area with a probabilistic recurrent spiking network whose parameters maximize the likelihood of the recorded activity. Although this is widely used, we show that the resulting model does not produce realistic neural activity. To correct for this, we suggest to augment the log-likelihood with terms that measure the dissimilarity between simulated and recorded activity. This dissimilarity is defined via summary statistics commonly used in neuroscience and the optimization is efficient because it relies on back-propagation through the stochastically simulated spike trains. We analyze this method theoretically and show empirically that it generates more realistic activity statistics. We find that it improves upon other fitting algorithms for spiking network models like GLMs (Generalized Linear Models) which do not usually rely on back-propagation. This new fitting algorithm also enables the consideration of hidden neurons which is otherwise notoriously hard, and we show that it can be crucial when trying to infer the network connectivity from spike recordings."}}
{"id": "hJVZL5v9pEN", "cdate": 1620892090663, "mdate": null, "content": {"title": "A solution to the learning dilemma for recurrent networks of spiking neurons", "abstract": "Recurrently connected networks of spiking neurons underlie the astounding information\nprocessing capabilities of the brain. Yet in spite of extensive research, how they can learn\nthrough synaptic plasticity to carry out complex network computations remains unclear. We\nargue that two pieces of this puzzle were provided by experimental data from neuroscience.\nA mathematical result tells us how these pieces need to be combined to enable biologically\nplausible online network learning through gradient descent, in particular deep reinforcement\nlearning. This learning method\u2013called e-prop\u2013approaches the performance of backpropagation\nthrough time (BPTT), the best-known method for training recurrent neural\nnetworks in machine learning. In addition, it suggests a method for powerful on-chip learning\nin energy-efficient spike-based hardware for artificial intelligence."}}
{"id": "SD64JD8fBi", "cdate": 1598867997128, "mdate": null, "content": {"title": "Long short-term memory and learning-to-learn in networks of spiking neurons", "abstract": "Recurrent networks of spiking neurons (RSNNs) underlie the astounding comput-\ning and learning capabilities of the brain. But computing and learning capabilities\nof RSNN models have remained poor, at least in comparison with ANNs. We\naddress two possible reasons for that. One is that RSNNs in the brain are not\nrandomly connected or designed according to simple rules, and they do not start\nlearning as a tabula rasa network. Rather, RSNNs in the brain were optimized for\ntheir tasks through evolution, development, and prior experience. Details of these\noptimization processes are largely unknown. But their functional contribution can\nbe approximated through powerful optimization methods, such as backpropaga-\ntion through time (BPTT).\nA second major mismatch between RSNNs in the brain and models is that the\nlatter only show a small fraction of the dynamics of neurons and synapses in\nthe brain. We include neurons in our RSNN model that reproduce one promi-\nnent dynamical process of biological neurons that takes place at the behaviourally\nrelevant time scale of seconds: neuronal adaptation. We denote these networks\nas LSNNs because of their Long short-term memory. The inclusion of adapting\nneurons drastically increases the computing and learning capability of RSNNs if\nthey are trained and configured by deep learning (BPTT combined with a rewiring\nalgorithm that optimizes the network architecture). In fact, the computational per-\nformance of these RSNNs approaches for the first time that of LSTM networks.\nIn addition RSNNs with adapting neurons can acquire abstract knowledge from\nprior learning in a Learning-to-Learn (L2L) scheme, and transfer that knowledge\nin order to learn new but related tasks from very few examples. We demonstrate\nthis for supervised learning and reinforcement learning."}}
{"id": "0GZ7LgQcD85", "cdate": 1598686607896, "mdate": null, "content": {"title": "Biologically inspired alternatives to backpropagation through time for learning in recurrent neural nets", "abstract": "he way how recurrently connected networks of spiking neurons in the brain acquire powerful information processing capabilities through learning has remained a mystery. This lack of understanding is linked to a lack of learning algorithms for recurrent networks of spiking neurons (RSNNs) that are both functionally powerful and can be implemented by known biological mechanisms. Since RSNNs are simultaneously a primary target for implementations of brain-inspired circuits in neuromorphic hardware, this lack of algorithmic insight also hinders technological progress in that area. The gold standard for learning in recurrent neural networks in machine learning is back-propagation through time (BPTT), which implements stochastic gradient descent with regard to a given loss function. But BPTT is unrealistic from a biological perspective, since it requires a transmission of error signals backwards in time and in space, i.e., from post- to presynaptic neurons. We show that an online merging of locally available information during a computation with suitable top-down learning signals in real-time provides highly capable approximations to BPTT. For tasks where information on errors arises only late during a network computation, we enrich locally available information through feedforward eligibility traces of synapses that can easily be computed in an online manner. The resulting new generation of learning algorithms for recurrent neural networks provides a new understanding of network learning in the brain that can be tested experimentally. In addition, these algorithms provide efficient methods for on-chip training of RSNNs in neuromorphic hardware. "}}
{"id": "SN3cACKJfx", "cdate": 1598686418013, "mdate": null, "content": {"title": "A solution to the learning dilemma for recurrentnetworks of spiking neurons", "abstract": "Recurrently connected networks of spiking neurons underlie the astounding informationprocessing capabilities of the brain. Yet in spite of extensive research, how they can learnthrough synaptic plasticity to carry out complex network computations remains unclear. Weargue that two pieces of this puzzle were provided by experimental data from neuroscience.A mathematical result tells us how these pieces need to be combined to enable biologicallyplausible online network learning through gradient descent, in particular deep reinforcementlearning.  This  learning  method\u2013called  e-prop\u2013approaches  the  performance  of  back-propagation through time (BPTT), the best-known method for training recurrent neuralnetworks in machine learning. In addition, it suggests a method for powerful on-chip learningin energy-efficient spike-based hardware for artificial intelligence."}}
