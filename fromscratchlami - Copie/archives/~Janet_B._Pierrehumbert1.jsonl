{"id": "4cHdXgIOxu", "cdate": 1706776362007, "mdate": 1706776362007, "content": {"title": "Forecasting COVID-19 Caseloads Using Unsupervised Embedding Clusters of Social Media Posts", "abstract": "We present a novel approach incorporating transformer-based language models into infectious disease modelling. Text-derived features are quantified by tracking high-density clusters of sentence-level representations of Reddit posts within specific US states\u2019 COVID-19 subreddits. We benchmark these clustered embedding features against features extracted from other high-quality datasets. In a threshold-classification task, we show that they outperform all other feature types at predicting upward trend signals, a significant result for infectious disease modelling in areas where epidemiological data is unreliable. Subsequently, in a time-series forecasting task, we fully utilise the predictive power of the caseload and compare the relative strengths of using different supplementary datasets as covariate feature sets in a transformer-based time-series model."}}
{"id": "zQvwARsz-h", "cdate": 1640995200000, "mdate": 1681661710845, "content": {"title": "Geographic Adaptation of Pretrained Language Models", "abstract": "Geographic features are commonly used to improve the performance of pretrained language models (PLMs) on NLP tasks where they are intuitively beneficial (e.g., geolocation prediction, dialect feature prediction). Existing methods, however, leverage geographic information in task-specific fine-tuning and fail to integrate it into the geo-linguistic knowledge encoded by PLMs, which would make it transferable across different tasks. In this paper, we introduce an approach to task-agnostic geoadaptation of PLMs that forces them to learn associations between linguistic phenomena and geographic locations. Geoadaptation is an intermediate training step that couples language modeling and geolocation prediction in a multi-task learning setup. In our main set of experiments, we geoadapt BERTi\\'{c}, a PLM for Bosnian-Croatian-Montenegrin-Serbian (BCMS), using a corpus of geotagged BCMS tweets. Evaluation on three tasks, namely fine-tuned as well as zero-shot geolocation prediction and zero-shot prediction of dialect features, shows that geoadaptation is very effective: e.g., we obtain state-of-the-art performance in supervised geolocation prediction and report massive gains over geographically uninformed PLMs on zero-shot geolocation prediction. Moreover, in follow-up experiments we successfully geoadapt two other PLMs, specifically ScandiBERT on Norwegian, Swedish, and Danish tweets and GermanBERT on Jodel posts in German from Austria, Germany, and Switzerland, proving that the benefits of geoadaptation are not limited to a particular language area and PLM."}}
{"id": "XbYrL83L7H", "cdate": 1640995200000, "mdate": 1681661710972, "content": {"title": "Unsupervised Detection of Contextualized Embedding Bias with Application to Ideology", "abstract": "We propose a fully unsupervised method to detect bias in contextualized embeddings. The method leverages the assortative information latently encoded by social networks and combines orthogonality regularization, structured sparsity learning, and graph neural networks to find the embedding subspace capturing this information. As a concrete example, we focus on the phenomenon of ideological bias: we introduce the concept of an ideological subspace, show how it can be found by applying our method to online discussion forums, and present techniques to probe it. Our experiments suggest that the ideological subspace encodes abstract evaluative semantics and reflects changes in the political left-right spectrum during the presidency of Donald Trump."}}
{"id": "XKU54PZdD8", "cdate": 1640995200000, "mdate": 1683553842840, "content": {"title": "Forecasting COVID-19 Caseloads Using Unsupervised Embedding Clusters of Social Media Posts", "abstract": "We present a novel approach incorporating transformer-based language models into infectious disease modelling. Text-derived features are quantified by tracking high-density clusters of sentence-level representations of Reddit posts within specific US states' COVID-19 subreddits. We benchmark these clustered embedding features against features extracted from other high-quality datasets. In a threshold-classification task, we show that they outperform all other feature types at predicting upward trend signals, a significant result for infectious disease modelling in areas where epidemiological data is unreliable. Subsequently, in a time-series forecasting task we fully utilise the predictive power of the caseload and compare the relative strengths of using different supplementary datasets as covariate feature sets in a transformer-based time-series model."}}
{"id": "SOfJ86pgUMx", "cdate": 1640995200000, "mdate": 1681661710978, "content": {"title": "Unsupervised Detection of Contextualized Embedding Bias with Application to Ideology", "abstract": "We propose a fully unsupervised method to detect bias in contextualized embeddings. The method leverages the assortative information latently encoded by social networks and combines orthogonality r..."}}
{"id": "Pef2nCmBwPd", "cdate": 1640995200000, "mdate": 1683553842838, "content": {"title": "Forecasting COVID-19 Caseloads Using Unsupervised Embedding Clusters of Social Media Posts", "abstract": "Felix Drinkall, Stefan Zohren, Janet Pierrehumbert. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022."}}
{"id": "MiN9EYsco2t", "cdate": 1640995200000, "mdate": 1683553842833, "content": {"title": "Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks", "abstract": "Paul Rottger, Bertie Vidgen, Dirk Hovy, Janet Pierrehumbert. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022."}}
{"id": "GZSXHo1Yhtv", "cdate": 1640995200000, "mdate": 1681661710845, "content": {"title": "Modeling Ideological Salience and Framing in Polarized Online Groups with Graph Neural Networks and Structured Sparsity", "abstract": ""}}
{"id": "ArLjgwwSO5x", "cdate": 1640995200000, "mdate": 1681661710839, "content": {"title": "The Reddit Politosphere: A Large-Scale Text and Network Resource of Online Political Discourse", "abstract": "We introduce the Reddit Politosphere, a large-scale resource of online political discourse covering more than 600 political discussion groups over a period of 12 years. It is to the best of our knowledge the largest and ideologically most comprehensive dataset of its type now available. One key feature of the Reddit Politosphere is that it consists of both text and network data, allowing for methodologically-diverse analyses. We describe in detail how we create the Reddit Politosphere, present descriptive statistics, and sketch potential directions for future research based on the resource."}}
{"id": "2HAqOjxCago", "cdate": 1640995200000, "mdate": 1681661710948, "content": {"title": "An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers", "abstract": ""}}
