{"id": "PGBxJ_A7Q3", "cdate": 1640995200000, "mdate": 1682667451451, "content": {"title": "Listwise Learning to Rank Based on Approximate Rank Indicators", "abstract": "We study here a way to approximate information retrieval metrics through a softmax-based approximation of the rank indicator function. Indeed, this latter function is a key component in the design of information retrieval metrics, as well as in the design of the ranking and sorting functions. Obtaining a good approximation for it thus opens the door to differentiable approximations of many evaluation measures that can in turn be used in neural end-to-end approaches. We first prove theoretically that the approximations proposed are of good quality, prior to validate them experimentally on both learning to rank and text-based information retrieval tasks."}}
{"id": "Lz97LAw0iE6", "cdate": 1609459200000, "mdate": 1682667451454, "content": {"title": "Modelling document-query interaction in a hierarchical neural model for IR", "abstract": "Recent deep approaches to information retrieval are either representation-oriented or interaction-oriented, depending on how they view the modelling of document and query representations and their interactions. We explore a hierarchical approach to document encoding that enables modelling the query-document interaction at different levels of granularity. The proposed model splits the input documents into blocks that are individually matched to a given query through a series of self-attention modules, along with pooling and projection layers. We test our method on the MQ2007 standard IR collection. The approach shows promising preliminary results, albeit a more in-depth exploration of the modelling choices could provide further gains."}}
{"id": "CxanALUlH-", "cdate": 1609459200000, "mdate": 1682667451455, "content": {"title": "The Power of Selecting Key Blocks with Local Pre-ranking for Long Document Information Retrieval", "abstract": "On a wide range of natural language processing and information retrieval tasks, transformer-based models, particularly pre-trained language models like BERT, have demonstrated tremendous effectiveness. Due to the quadratic complexity of the self-attention mechanism, however, such models have difficulties processing long documents. Recent works dealing with this issue include truncating long documents, in which case one loses potential relevant information, segmenting them into several passages, which may lead to miss some information and high computational complexity when the number of passages is large, or modifying the self-attention mechanism to make it sparser as in sparse-attention models, at the risk again of missing some information. We follow here a slightly different approach in which one first selects key blocks of a long document by local query-block pre-ranking, and then few blocks are aggregated to form a short document that can be processed by a model such as BERT. Experiments conducted on standard Information Retrieval datasets demonstrate the effectiveness of the proposed approach."}}
{"id": "2fjXAyHvbO", "cdate": 1609459200000, "mdate": 1682667451454, "content": {"title": "SmoothI: Smooth Rank Indicators for Differentiable IR Metrics", "abstract": "Information retrieval (IR) systems traditionally aim to maximize metrics built on rankings, such as precision or NDCG. However, the non-differentiability of the ranking operation prevents direct optimization of such metrics in state-of-the-art neural IR models, which rely entirely on the ability to compute meaningful gradients. To address this shortcoming, we propose SmoothI, a smooth approximation of rank indicators that serves as a basic building block to devise differentiable approximations of IR metrics. We further provide theoretical guarantees on SmoothI and derived approximations, showing in particular that the approximation errors decrease exponentially with an inverse temperature-like hyperparameter that controls the quality of the approximations. Extensive experiments conducted on four standard learning-to-rank datasets validate the efficacy of the listwise losses based on SmoothI, in comparison to previously proposed ones. Additional experiments with a vanilla BERT ranking model on a text-based IR task also confirm the benefits of our listwise approach."}}
{"id": "ePBs5L3g247", "cdate": 1577836800000, "mdate": null, "content": {"title": "Adaptive Pointwise-Pairwise Learning-to-Rank for Content-based Personalized Recommendation", "abstract": "This paper extends the standard pointwise and pairwise paradigms for learning-to-rank in the context of personalized recommendation, by considering these two approaches as two extremes of a continuum of possible strategies. It basically consists of a surrogate loss that models how to select and combine these two approaches adaptively, depending on the context (query or user, pair of items, etc.). In other words, given a training instance, which is typically a triplet (a query/user and two items with different preferences or relevance grades), the strategy adaptively determines whether it is better to focus on the \u201cmost preferred\u201d item (pointwise - positive instance), on the \u201cless preferred\u201d one (pointwise - negative instance) or on the pair (pairwise), or on anything else in between these 3 extreme alternatives. We formulate this adaptive strategy as minimizing a particular loss function that generalizes simultaneously the traditional pointwise and pairwise loss functions (negative log-likelihood) through a mixture coefficient. This coefficient is formulated as a learnable function of the features associated to the triplet. Experimental results on several real-world news recommendation datasets show clear improvements over several pointwise, pairwise, and listwise approaches."}}
{"id": "1TSvl2yHpXk", "cdate": 1577836800000, "mdate": null, "content": {"title": "Word Representations Concentrate and This is Good News!", "abstract": "This article establishes that, unlike the legacy tf*idf representation, recent natural language representations (word embedding vectors) tend to exhibit a so-called concentration of measure phenomenon, in the sense that, as the representation size p and database size n are both large, their behavior is similar to that of large dimensional Gaussian random vectors. This phenomenon may have important consequences as machine learning algorithms for natural language data could be amenable to improvement, thereby providing new theoretical insights into the field of natural language processing."}}
{"id": "eIkVt70u6aD", "cdate": 1514764800000, "mdate": null, "content": {"title": "Period-aware content attention RNNs for time series forecasting with missing values", "abstract": "Recurrent neural networks (RNNs) recently received considerable attention for sequence modeling and time series analysis. Many time series contain periods, e.g. seasonal changes in weather time series or electricity usage at day and night time. Here, we first analyze the behavior of RNNs with an attention mechanism with respect to periods in time series and illustrate that they fail to model periods. Then, we propose an extended attention model for sequence-to-sequence RNNs designed to capture periods in time series with or without missing values. This extended attention model can be deployed on top of any RNN, and is shown to yield state-of-the-art performance for time series forecasting on several univariate and multivariate time series."}}
{"id": "RkXzI0ZM9vn", "cdate": 1514764800000, "mdate": null, "content": {"title": "Online metrics prediction in monitoring systems", "abstract": "Monitoring thousands of machines and services in a datacenter produces a lot of time series points, giving a general idea of the health of a cluster. However, there is a lack of tools to further exploit this data, for instance for prediction purposes. We propose to apply linear regression algorithms to predict the future behavior of monitored systems and anticipate downtimes, giving system administrators the information they need ahead of the problems arising. This problem is quite challenging when dealing with a high number of monitoring metrics, given our three main constraints: a low number of false positives (thus blacklisting volatile metrics), a high availability (due to the nature of monitoring systems), and a good scalability. We implemented and evaluated such a system using production metrics from Coservit, a company specialized in infrastructure monitoring. The results we obtained are promising: sub-second latency per predicted metric per CPU core, for the entire end-to-end process. This latency is constant when scaling the system up to 125 cores on 4 machines dedicated for monitoring predictions, and the performances don't decrease with time: during 15 minutes, it is able to handle more than 100 000 monitoring metrics."}}
{"id": "aW_NW8AWYS", "cdate": 1483228800000, "mdate": null, "content": {"title": "Time Series Forecasting using RNNs: an Extended Attention Mechanism to Model Periods and Handle Missing Values", "abstract": "We propose here an extended attention model for sequence-to-sequence recurrent neural networks (RNNs) designed to capture (pseudo-)periods in time series. This extended attention model can be deployed on top of any RNN and is shown to yield state-of-the-art performance for time series forecasting on several univariate and multivariate time series."}}
{"id": "C3hZdPbPieg", "cdate": 1483228800000, "mdate": null, "content": {"title": "Position-Based Content Attention for Time Series Forecasting with Sequence-to-Sequence RNNs", "abstract": "We propose here an extended attention model for sequence-to-sequence recurrent neural networks (RNNs) designed to capture (pseudo-)periods in time series. This extended attention model can be deployed on top of any RNN and is shown to yield state-of-the-art performance for time series forecasting on several univariate and multivariate time series."}}
