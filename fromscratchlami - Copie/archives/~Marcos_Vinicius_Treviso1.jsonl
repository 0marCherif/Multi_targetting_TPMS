{"id": "7ZJId64r40", "cdate": 1706782711674, "mdate": 1706782711674, "content": {"title": "Unbabel's Participation in the WMT19 Translation Quality Estimation Shared Task", "abstract": "We present the contribution of the Unbabel team to the WMT 2019 Shared Task on Quality Estimation. We participated on the word, sentence, and document-level tracks, encompassing 3 language pairs: English-German, English-Russian, and English-French. Our submissions build upon the recent OpenKiwi framework: we combine linear, neural, and predictor-estimator systems with new transfer learning approaches using BERT and XLM pre-trained models. We compare systems individually and propose new ensemble techniques for word and sentence-level predictions. We also propose a simple technique for converting word labels into document-level predictions. Overall, our submitted systems achieve the best results on all tracks and language pairs by a considerable margin."}}
{"id": "51PgDMOsMEm", "cdate": 1706782362337, "mdate": 1706782362337, "content": {"title": "OpenKiwi: An Open Source Framework for Quality Estimation", "abstract": "We introduce OpenKiwi, a PyTorch-based\nopen source framework for translation quality estimation. OpenKiwi supports training\nand testing of word-level and sentence-level\nquality estimation systems, implementing the\nwinning systems of the WMT 2015\u201318 quality estimation campaigns. We benchmark\nOpenKiwi on two datasets from WMT 2018\n(English-German SMT and NMT), yielding\nstate-of-the-art performance on the word-level\ntasks and near state-of-the-art in the sentencelevel tasks."}}
{"id": "V5rlSPsHpkf", "cdate": 1652737731876, "mdate": null, "content": {"title": "Learning to Scaffold: Optimizing Model Explanations for Teaching", "abstract": "Modern machine learning models are opaque, and as a result there is a burgeoning academic subfield on methods that explain these models' behavior.  However, what is the precise goal of providing such explanations, and how can we demonstrate that explanations achieve this goal? Some research argues that explanations should help teach a student (either human or machine) to simulate the model being explained, and that the quality of explanations can be measured by the simulation accuracy of students on unexplained examples. In this work, leveraging meta-learning techniques, we extend this idea to improve the quality of the explanations themselves, specifically by optimizing explanations such that student models more effectively learn to simulate the original model. We train models on three natural language processing and computer vision tasks, and find that students trained with explanations extracted with our framework are able to simulate the teacher significantly more effectively than ones produced with previous methods. Through human annotations and a user study, we further find that these learned explanations more closely align with how humans would explain the required decisions in these tasks. Our code is available at https://github.com/coderpat/learning-scaffold."}}
{"id": "q6-YhxkbsVh", "cdate": 1609459200000, "mdate": 1636113587827, "content": {"title": "Sparse Continuous Distributions and Fenchel-Young Losses", "abstract": "Exponential families are widely used in machine learning, including many distributions in continuous and discrete domains (e.g., Gaussian, Dirichlet, Poisson, and categorical distributions via the softmax transformation). Distributions in each of these families have fixed support. In contrast, for finite domains, recent work on sparse alternatives to softmax (e.g., sparsemax, $\\alpha$-entmax, and fusedmax), has led to distributions with varying support. This paper develops sparse alternatives to continuous distributions, based on several technical contributions: First, we define $\\Omega$-regularized prediction maps and Fenchel-Young losses for arbitrary domains (possibly countably infinite or continuous). For linearly parametrized families, we show that minimization of Fenchel-Young losses is equivalent to moment matching of the statistics, generalizing a fundamental property of exponential families. When $\\Omega$ is a Tsallis negentropy with parameter $\\alpha$, we obtain ``deformed exponential families,'' which include $\\alpha$-entmax and sparsemax ($\\alpha=2$) as particular cases. For quadratic energy functions, the resulting densities are $\\beta$-Gaussians, an instance of elliptical distributions that contain as particular cases the Gaussian, biweight, triweight, and Epanechnikov densities, and for which we derive closed-form expressions for the variance, Tsallis entropy, and Fenchel-Young loss. When $\\Omega$ is a total variation or Sobolev regularizer, we obtain a continuous version of the fusedmax. Finally, we introduce continuous-domain attention mechanisms, deriving efficient gradient backpropagation algorithms for $\\alpha \\in \\{1, 4/3, 3/2, 2\\}$. Using these algorithms, we demonstrate our sparse continuous distributions for attention-based audio classification and visual question answering, showing that they allow attending to time intervals and compact regions."}}
{"id": "NeS7a67o2ac", "cdate": 1609459200000, "mdate": 1636113588029, "content": {"title": "Predicting Attention Sparsity in Transformers", "abstract": "Transformers' quadratic complexity with respect to the input sequence length has motivated a body of work on efficient sparse approximations to softmax. An alternative path, used by entmax transformers, consists of having built-in exact sparse attention; however this approach still requires quadratic computation. In this paper, we propose Sparsefinder, a simple model trained to identify the sparsity pattern of entmax attention before computing it. We experiment with three variants of our method, based on distances, quantization, and clustering, on two tasks: machine translation (attention in the decoder) and masked language modeling (encoder-only). Our work provides a new angle to study model efficiency by doing extensive analysis of the tradeoff between the sparsity and recall of the predicted attention graph. This allows for detailed comparison between different models along their Pareto curves, important to guide future benchmarks for sparse attention models."}}
{"id": "BquuXPTv4Lv", "cdate": 1577836800000, "mdate": 1636113587829, "content": {"title": "Sparse and Continuous Attention Mechanisms", "abstract": "Exponential families are widely used in machine learning; they include many distributions in continuous and discrete domains (e.g., Gaussian, Dirichlet, Poisson, and categorical distributions via the softmax transformation). Distributions in each of these families have fixed support. In contrast, for finite domains, there has been recent work on sparse alternatives to softmax (e.g., sparsemax and alpha-entmax), which have varying support, being able to assign zero probability to irrelevant categories. These discrete sparse mappings have been used for improving interpretability of neural attention mechanisms. This paper expands that work in two directions: first, we extend alpha-entmax to continuous domains, revealing a link with Tsallis statistics and deformed exponential families. Second, we introduce continuous-domain attention mechanisms, deriving efficient gradient backpropagation algorithms for alpha in {1,2}. Experiments on attention-based text classification, machine translation, and visual question answering illustrate the use of continuous attention in 1D and 2D, showing that it allows attending to time intervals and compact regions."}}
{"id": "-Q0LDKpFhVT", "cdate": 1577836800000, "mdate": 1636113587827, "content": {"title": "The Explanation Game: Towards Prediction Explainability through Sparse Communication", "abstract": "Marcos Treviso, Andr\u00e9 F. T. Martins. Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. 2020."}}
{"id": "mzYTMQ6SLBq", "cdate": 1546300800000, "mdate": 1636113587833, "content": {"title": "Unbabel's Participation in the WMT19 Translation Quality Estimation Shared Task", "abstract": "Fabio Kepler, Jonay Tr\u00e9nous, Marcos Treviso, Miguel Vera, Ant\u00f3nio G\u00f3is, M. Amin Farajian, Ant\u00f3nio V. Lopes, Andr\u00e9 F. T. Martins. Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2). 2019."}}
{"id": "AjcrjptBuvu", "cdate": 1546300800000, "mdate": 1636113587832, "content": {"title": "OpenKiwi: An Open Source Framework for Quality Estimation", "abstract": "Fabio Kepler, Jonay Tr\u00e9nous, Marcos Treviso, Miguel Vera, Andr\u00e9 F. T. Martins. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. 2019."}}
