{"id": "saJSL-zsIHQ", "cdate": 1693526400000, "mdate": 1699199193184, "content": {"title": "Performance Analysis of Automotive SAR With Radar Based Motion Estimation", "abstract": "Automotive synthetic aperture radar (SAR) can achieve a significant angular resolution enhancement for detecting static objects, which is essential for automated driving. Obtaining high-resolution SAR images requires precise ego vehicle velocity estimation. A small velocity estimation error can result in a focused SAR image with objects at offset angles. In this paper, we consider an automotive SAR system that produces SAR images of static objects based on ego vehicle velocity estimation from the radar return signal without the overhead in complexity and cost of using an auxiliary global navigation satellite system (GNSS) and inertial measurement unit (IMU). We derive a novel analytical approximation for the automotive SAR angle estimation error variance when the radar estimates the velocity. The developed analytical analysis closely predicts the actual SAR angle estimation variance. It provides insights into the effects of the radar parameters and the environmental condition on the automotive SAR angle estimation error. We evaluate via the analytical analysis and simulation tests the radar settings and environmental conditions in which the automotive SAR attains a significant performance gain over the angular resolution of the short aperture physical antenna array. We show that, perhaps surprisingly, when the velocity is estimated by the radar, the performance advantage of automotive SAR is realized only in limited conditions. Hence since its implementation comes with an increase in computation and system complexity as well as an increase in the detection delay, it should be used carefully and selectively."}}
{"id": "RHU0ft5bntu", "cdate": 1672531200000, "mdate": 1699199193356, "content": {"title": "A Neural Collapse Perspective on Feature Evolution in Graph Neural Networks", "abstract": "Graph neural networks (GNNs) have become increasingly popular for classification tasks on graph-structured data. Yet, the interplay between graph topology and feature evolution in GNNs is not well understood. In this paper, we focus on node-wise classification, illustrated with community detection on stochastic block model graphs, and explore the feature evolution through the lens of the \"Neural Collapse\" (NC) phenomenon. When training instance-wise deep classifiers (e.g. for image classification) beyond the zero training error point, NC demonstrates a reduction in the deepest features' within-class variability and an increased alignment of their class means to certain symmetric structures. We start with an empirical study that shows that a decrease in within-class variability is also prevalent in the node-wise classification setting, however, not to the extent observed in the instance-wise case. Then, we theoretically study this distinction. Specifically, we show that even an \"optimistic\" mathematical model requires that the graphs obey a strict structural condition in order to possess a minimizer with exact collapse. Interestingly, this condition is viable also for heterophilic graphs and relates to recent empirical studies on settings with improved GNNs' generalization. Furthermore, by studying the gradient dynamics of the theoretical model, we provide reasoning for the partial collapse observed empirically. Finally, we present a study on the evolution of within- and between-class feature variability across layers of a well-trained GNN and contrast the behavior with spectral methods."}}
{"id": "NACEVzRggPe", "cdate": 1672531200000, "mdate": 1699199193355, "content": {"title": "Perturbation Analysis of Neural Collapse", "abstract": "Training deep neural networks for classification often includes minimizing the training loss beyond the zero training error point. In this phase of training, a \"neural collapse\" behavior has been observed: the variability of features (outputs of the penultimate layer) of within-class samples decreases and the mean features of different classes approach a certain tight frame structure. Recent works analyze this behavior via idealized unconstrained features models where all the minimizers exhibit exact collapse. However, with practical networks and datasets, the features typically do not reach exact collapse, e.g., because deep layers cannot arbitrarily modify intermediate features that are far from being collapsed. In this paper, we propose a richer model that can capture this phenomenon by forcing the features to stay in the vicinity of a predefined features matrix (e.g., intermediate features). We explore the model in the small vicinity case via perturbation analysis and establish results that cannot be obtained by the previously studied models. For example, we prove reduction in the within-class variability of the optimized features compared to the predefined input features (via analyzing gradient flow on the \"central-path\" with minimal assumptions), analyze the minimizers in the near-collapse regime, and provide insights on the effect of regularization hyperparameters on the closeness to collapse. We support our theory with experiments in practical deep learning settings."}}
{"id": "uAmv2zRAWn", "cdate": 1663850148307, "mdate": null, "content": {"title": "Perturbation Analysis of Neural Collapse", "abstract": "Training deep neural networks for classification often includes minimizing the training loss beyond the zero training error point. In this phase of training, a \u201cneural collapse\u201d behavior has been observed: the variability of features (outputs of the penultimate layer) of within-class samples decreases and the mean features of different classes approach a certain tight frame structure. Recent works analyze this behavior via idealized unconstrained features models where all the minimizers exhibit exact collapse. However, with practical networks and datasets, the features typically do not reach exact collapse, e.g., because deep layers cannot arbitrarily modify intermediate features that are far from being collapsed. In this paper, we propose a richer model that can capture this phenomenon by forcing the features to stay in the vicinity of a predefined features matrix (e.g., intermediate features). We explore the model in the small vicinity case via perturbation analysis and establish results that cannot be obtained by the previously studied models. For example, we prove reduction in the within-class variability of the optimized features compared to the predefined input features (via analyzing gradient flow on the \u201ccentral-path\u201d with minimal assumptions), analyze the minimizers in the near-collapse regime, and provide insights on the effect of regularization hyperparameters on the closeness to collapse. We support our theory with experiments in practical deep learning settings."}}
{"id": "xv7DtueUN4", "cdate": 1640995200000, "mdate": 1683623165162, "content": {"title": "Extended Unconstrained Features Model for Exploring Deep Neural Collapse", "abstract": "The modern strategy for training deep neural networks for classification tasks includes optimizing the network's weights even after the training error vanishes to further push the training loss toward zero. Recently, a phenomenon termed \"neural collapse\" (NC) has been empirically observed in this training procedure. Specifically, it has been shown that the learned features (the output of the penultimate layer) of within-class samples converge to their mean, and the means of different classes exhibit a certain tight frame structure, which is also aligned with the last layer's weights. Recent papers have shown that minimizers with this structure emerge when optimizing a simplified \"unconstrained features model\" (UFM) with a regularized cross-entropy loss. In this paper, we further analyze and extend the UFM. First, we study the UFM for the regularized MSE loss, and show that the minimizers' features can have a more delicate structure than in the cross-entropy case. This affects also the structure of the weights. Then, we extend the UFM by adding another layer of weights as well as ReLU nonlinearity to the model and generalize our previous results. Finally, we empirically demonstrate the usefulness of our nonlinear extended UFM in modeling the NC phenomenon that occurs with practical networks."}}
{"id": "ZZj5UeFacC", "cdate": 1640995200000, "mdate": 1683623165137, "content": {"title": "Extended Unconstrained Features Model for Exploring Deep Neural Collapse", "abstract": "The modern strategy for training deep neural networks for classification tasks includes optimizing the network\u2019s weights even after the training error vanishes to further push the training loss tow..."}}
{"id": "T-suoOddLs", "cdate": 1640995200000, "mdate": 1683623165103, "content": {"title": "Performance Analysis of Automotive SAR With Radar Based Motion Estimation", "abstract": "Automotive synthetic aperture radar (SAR) can achieve a significant angular resolution enhancement for detecting static objects, which is essential for automated driving. Obtaining high resolution SAR images requires precise ego vehicle velocity estimation. A small velocity estimation error can result in a focused SAR image with objects at offset angles. In this paper, we consider an automotive SAR system that produces SAR images of static objects based on ego vehicle velocity estimation from the radar return signal without the overhead in complexity and cost of using an auxiliary global navigation satellite system (GNSS) and inertial measurement unit (IMU). We derive a novel analytical approximation for the automotive SAR angle estimation error variance when the velocity is estimated by the radar. The developed analytical analysis closely predicts the true SAR angle estimation variance, and also provides insights on the effects of the radar parameters and the environment condition on the automotive SAR angle estimation error. We evaluate via the analytical analysis and simulation tests the radar settings and environment condition in which the automotive SAR attains a significant performance gain over the angular resolution of the short aperture physical antenna array. We show that, perhaps surprisingly, when the velocity is estimated by the radar the performance advantage of automotive SAR is realized only in limited conditions. Hence since its implementation comes with an increase in computation and system complexity as well as an increase in the detection delay it should be used carefully and selectively."}}
{"id": "Qjeulu7mTA", "cdate": 1640995200000, "mdate": 1668680921490, "content": {"title": "Image Restoration by Deep Projected GSURE", "abstract": "Ill-posed inverse problems appear in many image processing applications, such as deblurring and super-resolution. In recent years, solutions that are based on deep Convolutional Neural Networks (CNNs) have shown great promise. Yet, most of these techniques, which train CNNs using external data, are restricted to the observation models that have been used in the training phase. A recent alternative that does not have this drawback relies on learning the target image using internal learning. One such prominent example is the Deep Image Prior (DIP) technique that trains a network directly on the input image with the least-squares loss. In this paper, we propose a new image restoration framework that is based on minimizing a loss function that includes a \"projected-version\" of the Generalized Stein Unbiased Risk Estimator (GSURE) and parameterization of the latent image by a CNN. We propose two ways to use our framework. In the first one, where no explicit prior is used, we show that the proposed approach outperforms other internal learning methods, such as DIP. In the second one, we show that our GSURE-based loss leads to improved performance when used within a plug-and-play priors scheme."}}
{"id": "KB1-wNdjOp", "cdate": 1640995200000, "mdate": 1682139417604, "content": {"title": "ADIR: Adaptive Diffusion for Image Reconstruction", "abstract": "In recent years, denoising diffusion models have demonstrated outstanding image generation performance. The information on natural images captured by these models is useful for many image reconstruction applications, where the task is to restore a clean image from its degraded observations. In this work, we propose a conditional sampling scheme that exploits the prior learned by diffusion models while retaining agreement with the observations. We then combine it with a novel approach for adapting pretrained diffusion denoising networks to their input. We examine two adaption strategies: the first uses only the degraded image, while the second, which we advocate, is performed using images that are ``nearest neighbors'' of the degraded image, retrieved from a diverse dataset using an off-the-shelf visual-language model. To evaluate our method, we test it on two state-of-the-art publicly available diffusion models, Stable Diffusion and Guided Diffusion. We show that our proposed `adaptive diffusion for image reconstruction' (ADIR) approach achieves a significant improvement in the super-resolution, deblurring, and text-based editing tasks."}}
{"id": "Ijs-kpbSZgB", "cdate": 1640995200000, "mdate": 1683623165152, "content": {"title": "Direction of Arrival Estimation and Phase-Correction for Noncoherent Subarrays: A Convex Optimization Approach", "abstract": "Estimating the direction of arrival (DOA) of sources is an important problem in aerospace and vehicular communication, localization, and radar. In this article, we consider a challenging multisource DOA estimation task, where the receiving antenna array is composed of noncoherent subarrays, i.e., subarrays that observe different unknown phase shifts at every snapshot (e.g., due to waiving the demanding synchronization of local oscillators across the entire array). We formulate this problem as the reconstruction of joint sparse and low-rank matrices and solve the problem\u2019s convex relaxation. To scale the optimization complexity with the number of snapshots better than general-purpose solvers, we design an optimization scheme, based on integrating the alternating direction method of multipliers and the accelerated proximal gradient techniques, which exploits the structure of the problem. While the DOAs can be estimated from the solution of the aforementioned convex problem, we further show how an improvement is obtained if, instead, one estimates from this solution only the subarrays\u2019 phase shifts. This is done using another, computationally light, convex relaxation that is practically tight. Using the estimated phase shifts, \u201cphase-corrected\u201d observations are created and a final plain (\u201ccoherent\u201d) DOA estimation method can be applied. Numerical experiments show the performance advantages of the proposed strategies over existing methods."}}
