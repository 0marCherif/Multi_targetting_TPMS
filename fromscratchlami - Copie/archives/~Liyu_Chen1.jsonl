{"id": "l1gmW9YBcWb", "cdate": 1676827079956, "mdate": null, "content": {"title": "Posterior Sampling-based Online Learning for the Stochastic Shortest Path Model", "abstract": "We consider the problem of online reinforcement learning for the Stochastic Shortest Path (SSP) problem modeled as an unknown MDP with an absorbing state. We propose \\ssp, a simple posterior sampling-based reinforcement learning algorithm for the SSP problem. The algorithm operates in epochs. At the beginning of each epoch, a sample is drawn from the posterior distribution on the unknown model dynamics, and the optimal policy with respect to the drawn sample is followed during that epoch. An epoch completes if either the  number of visits to the goal state in the current epoch exceeds that of the previous epoch, or the number of visits to any of the state-action pairs is doubled. We establish a Bayesian regret bound of $\\tilde{O}(B S\\sqrt{AK})$, where $B$ is an upper bound on the expected cost of the optimal policy, $S$ is the size of the state space, $A$ is the size of the action space, and $K$ is the number of episodes. The algorithm only requires the knowledge of the prior distribution, and has no hyper-parameters to tune. It is the first such posterior sampling algorithm and outperforms numerically previously proposed optimism-based algorithms."}}
{"id": "VgST3vrBAo8", "cdate": 1652737673115, "mdate": null, "content": {"title": "Near-Optimal Goal-Oriented Reinforcement Learning in Non-Stationary Environments", "abstract": "We initiate the study of dynamic regret minimization for goal-oriented reinforcement learning modeled by a non-stationary stochastic shortest path problem with changing cost and transition functions.\nWe start by establishing a lower bound $\\Omega((B_{\\star} SAT_{\\star}(\\Delta_c + B_{\\star}^2\\Delta_P))^{1/3}K^{2/3})$, where $B_{\\star}$ is the maximum expected cost of the optimal policy of any episode starting from any state, $T_{\\star}$ is the maximum hitting time of the optimal policy of any episode starting from the initial state, $SA$ is the number of state-action pairs, $\\Delta_c$ and $\\Delta_P$ are the amount of changes of the cost and transition functions respectively, and $K$ is the number of episodes.\nThe different roles of $\\Delta_c$ and $\\Delta_P$ in this lower bound inspire us to design algorithms that estimate costs and transitions separately.\nSpecifically, assuming the knowledge of $\\Delta_c$ and $\\Delta_P$, we develop a simple but sub-optimal algorithm and another more involved minimax optimal algorithm (up to logarithmic terms).\nThese algorithms combine the ideas of finite-horizon approximation [Chen et al., 2021b], special Bernstein-style bonuses of the MVP algorithm [Zhang et al., 2020], adaptive confidence widening [Wei and Luo, 2021], as well as some new techniques such as properly penalizing long-horizon policies.\n\tFinally, when $\\Delta_c$ and $\\Delta_P$ are unknown, we develop a variant of the MASTER algorithm [Wei and Luo, 2021] and integrate the aforementioned ideas into it to achieve $\\widetilde{O}(\\min\\{B_{\\star} S\\sqrt{ALK}, (B_{\\star}^2S^2AT_{\\star}(\\Delta_c+B_{\\star}\\Delta_P))^{1/3}K^{2/3}\\})$ regret, where $L$ is the unknown number of changes of the environment."}}
{"id": "25XwID3wKsi", "cdate": 1652737489160, "mdate": null, "content": {"title": "Follow-the-Perturbed-Leader for Adversarial Markov Decision Processes with Bandit Feedback", "abstract": "We consider regret minimization for Adversarial Markov Decision Processes (AMDPs), where the loss functions are changing over time and adversarially chosen, and the learner only observes the losses for the visited state-action pairs (i.e., bandit feedback). While there has been a surge of studies on this problem using Online-Mirror-Descent (OMD) methods, very little is known about the Follow-the-Perturbed-Leader (FTPL) methods, which are usually computationally more efficient and also easier to implement since it only requires solving an offline planning problem. Motivated by this, we take a closer look at FTPL for learning AMDPs, starting from the standard episodic finite-horizon setting. We find some unique and intriguing difficulties in the analysis and propose a workaround to eventually show that FTPL is also able to achieve near-optimal regret bounds in this case. More importantly, we then find two significant applications: First, the analysis of FTPL turns out to be readily generalizable to delayed bandit feedback with order-optimal regret, while OMD methods exhibit extra difficulties (Jin et al., 2022). Second, using FTPL, we also develop the first no-regret algorithm for learning communicating AMDPs in the infinite-horizon setting with bandit feedback and stochastic transitions. Our algorithm is efficient assuming access to an offline planning oracle, while even for the easier full-information setting, the only existing algorithm (Chandrasekaran and Tewari, 2021) is computationally inefficient."}}
{"id": "mo-EcqhZU2R", "cdate": 1621629964641, "mdate": null, "content": {"title": "Implicit Finite-Horizon Approximation and Efficient Optimal Algorithms for Stochastic Shortest Path", "abstract": "We introduce a generic template for developing regret minimization algorithms in the Stochastic Shortest Path (SSP) model, which achieves minimax optimal regret as long as certain properties are ensured. The key of our analysis is a new technique called implicit finite-horizon approximation, which approximates the SSP model by a finite-horizon counterpart only in the analysis without explicit implementation. Using this template, we develop two new algorithms: the first one is model-free (the first in the literature to our knowledge) and minimax optimal under strictly positive costs; the second one is model-based and minimax optimal even with zero-cost state-action pairs, matching the best existing result from [Tarbouriech et al., 2021b]. Importantly, both algorithms admit highly sparse updates, making them  computationally more efficient than all existing algorithms. Moreover, both can be made completely parameter-free."}}
{"id": "ry-E7ub_ZB", "cdate": 1514764800000, "mdate": null, "content": {"title": "Synthesize Policies for Transfer and Adaptation across Tasks and Environments", "abstract": "The ability to transfer in reinforcement learning is key towards building an agent of general artificial intelligence. In this paper, we consider the problem of learning to simultaneously transfer across both environments and tasks, probably more importantly, by learning from only sparse (environment, task) pairs out of all the possible combinations. We propose a novel compositional neural network architecture which depicts a meta rule for composing policies from environment and task embeddings. Notably, one of the main challenges is to learn the embeddings jointly with the meta rule. We further propose new training methods to disentangle the embeddings, making them both distinctive signatures of the environments and tasks and effective building blocks for composing the policies. Experiments on GridWorld and THOR, of which the agent takes as input an egocentric view, show that our approach gives rise to high success rates on all the (environment, task) pairs after learning from only 40% of them."}}
