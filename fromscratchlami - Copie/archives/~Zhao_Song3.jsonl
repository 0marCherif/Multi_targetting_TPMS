{"id": "hJg4ji3FBT-", "cdate": 1683944601716, "mdate": 1683944601716, "content": {"title": "A Faster $k$-means++ Algorithm", "abstract": "K-means++ is an important algorithm to choose initial cluster centers for the k-means clustering algorithm.\nIn this work, we present a new algorithm that can solve the $k$-means++ problem with near optimal running time. Given $n$ data points in $\\mathbb{R}^d$, the current state-of-the-art algorithm runs in $\\widetilde{O}(k )$ iterations, and each iteration takes $\\widetilde{O}(nd k)$ time. The overall running time is thus $\\widetilde{O}(n d k^2)$. We propose a new algorithm \\textsc{FastKmeans++} that only takes in $\\widetilde{O}(nd + nk^2)$ time, in total."}}
{"id": "ac_IInuo2et", "cdate": 1681135283455, "mdate": 1681135283455, "content": {"title": "Fast sketching of polynomial kernels of polynomial degree", "abstract": "Kernel methods are fundamental in machine learning, and faster algorithms for kernel approximation provide direct speedups for many core tasks in machine learning. The polynomial kernel is especially important as other kernels can often be approximated by the polynomial kernel via a Taylor series expansion. Recent techniques in oblivious sketching reduce the dependence in the running time on the degree q of the polynomial kernel from exponential to polynomial, which is useful for the Gaussian kernel, for which q can be chosen to be polylogarithmic. However, for more slowly growing kernels, such as the neural tangent and arc-cosine kernels, q needs to be polynomial, and previous work incurs a polynomial factor slowdown in the running time. We give a new oblivious sketch which greatly improves upon this running time, by removing the dependence on q in the leading order term. Combined with a novel sampling scheme, we give the fastest algorithms for approximating a large family of slow-growing kernels."}}
{"id": "tOINxqez17", "cdate": 1672531200000, "mdate": 1683235214875, "content": {"title": "Randomized and Deterministic Attention Sparsification Algorithms for Over-parameterized Feature Dimension", "abstract": "Large language models (LLMs) have shown their power in different areas. Attention computation, as an important subroutine of LLMs, has also attracted interests in theory. Recently the static computation and dynamic maintenance of attention matrix has been studied by [Alman and Song 2023] and [Brand, Song and Zhou 2023] from both algorithmic perspective and hardness perspective. In this work, we consider the sparsification of the attention problem. We make one simplification which is the logit matrix is symmetric. Let $n$ denote the length of sentence, let $d$ denote the embedding dimension. Given a matrix $X \\in \\mathbb{R}^{n \\times d}$, suppose $d \\gg n$ and $\\| X X^\\top \\|_{\\infty} < r$ with $r \\in (0,0.1)$, then we aim for finding $Y \\in \\mathbb{R}^{n \\times m}$ (where $m\\ll d$) such that \\begin{align*} \\| D(Y)^{-1} \\exp( Y Y^\\top ) - D(X)^{-1} \\exp( X X^\\top) \\|_{\\infty} \\leq O(r) \\end{align*} We provide two results for this problem. $\\bullet$ Our first result is a randomized algorithm. It runs in $\\widetilde{O}(\\mathrm{nnz}(X) + n^{\\omega} ) $ time, has $1-\\delta$ succeed probability, and chooses $m = O(n \\log(n/\\delta))$. Here $\\mathrm{nnz}(X)$ denotes the number of non-zero entries in $X$. We use $\\omega$ to denote the exponent of matrix multiplication. Currently $\\omega \\approx 2.373$. $\\bullet$ Our second result is a deterministic algorithm. It runs in $\\widetilde{O}(\\min\\{\\sum_{i\\in[d]}\\mathrm{nnz}(X_i)^2, dn^{\\omega-1}\\} + n^{\\omega+1})$ time and chooses $m = O(n)$. Here $X_i$ denote the $i$-th column of matrix $X$. Our main findings have the following implication for applied LLMs task: for any super large feature dimension, we can reduce it down to the size nearly linear in length of sentence."}}
{"id": "tGFwYrlu-b", "cdate": 1672531200000, "mdate": 1683235214158, "content": {"title": "Fast Attention Requires Bounded Entries", "abstract": "In modern machine learning, inner product attention computation is a fundamental task for training large language models such as Transformer, GPT-1, BERT, GPT-2, GPT-3 and ChatGPT. Formally, in this problem, one is given as input three matrices $Q, K, V \\in [-B,B]^{n \\times d}$, and the goal is to construct the matrix $\\mathrm{Att}(Q,K,V) := \\mathrm{diag}(A {\\bf 1}_n)^{-1} A V \\in \\mathbb{R}^{n \\times d}$, where $A = \\exp(QK^\\top/d)$ is the `attention matrix', and $\\exp$ is applied entry-wise. Straightforward methods for this problem explicitly compute the $n \\times n$ attention matrix $A$, and hence require time $\\Omega(n^2)$ even when $d = n^{o(1)}$ is small. In this paper, we investigate whether faster algorithms are possible by implicitly making use of the matrix $A$. We present two results, showing that there is a sharp transition at $B = \\Theta(\\sqrt{\\log n})$. $\\bullet$ If $d = O(\\log n)$ and $B = o(\\sqrt{\\log n})$, there is an $n^{1+o(1)}$ time algorithm to approximate $\\mathrm{Att}(Q,K,V)$ up to $1/\\mathrm{poly}(n)$ additive error. $\\bullet$ If $d = O(\\log n)$ and $B = \\Theta (\\sqrt{\\log n})$, assuming the Strong Exponential Time Hypothesis from fine-grained complexity theory, it is impossible to approximate $\\mathrm{Att}(Q,K,V)$ up to $1/\\mathrm{poly}(n)$ additive error in truly subquadratic time $n^{2 - \\Omega(1)}$. This gives a theoretical explanation for the phenomenon observed in practice that attention computation is much more efficient when the input matrices have smaller entries."}}
{"id": "qVXaYcOPaH", "cdate": 1672531200000, "mdate": 1682318410299, "content": {"title": "A General Algorithm for Solving Rank-one Matrix Sensing", "abstract": "Matrix sensing has many real-world applications in science and engineering, such as system control, distance embedding, and computer vision. The goal of matrix sensing is to recover a matrix $A_\\star \\in \\mathbb{R}^{n \\times n}$, based on a sequence of measurements $(u_i,b_i) \\in \\mathbb{R}^{n} \\times \\mathbb{R}$ such that $u_i^\\top A_\\star u_i = b_i$. Previous work [ZJD15] focused on the scenario where matrix $A_{\\star}$ has a small rank, e.g. rank-$k$. Their analysis heavily relies on the RIP assumption, making it unclear how to generalize to high-rank matrices. In this paper, we relax that rank-$k$ assumption and solve a much more general matrix sensing problem. Given an accuracy parameter $\\delta \\in (0,1)$, we can compute $A \\in \\mathbb{R}^{n \\times n}$ in $\\widetilde{O}(m^{3/2} n^2 \\delta^{-1} )$, such that $ |u_i^\\top A u_i - b_i| \\leq \\delta$ for all $i \\in [m]$. We design an efficient algorithm with provable convergence guarantees using stochastic gradient descent for this problem."}}
{"id": "qAGevA9-p21", "cdate": 1672531200000, "mdate": 1683235214393, "content": {"title": "A Theoretical Analysis Of Nearest Neighbor Search On Approximate Near Neighbor Graph", "abstract": "Graph-based algorithms have demonstrated state-of-the-art performance in the nearest neighbor search (NN-Search) problem. These empirical successes urge the need for theoretical results that guarantee the search quality and efficiency of these algorithms. However, there exists a practice-to-theory gap in the graph-based NN-Search algorithms. Current theoretical literature focuses on greedy search on exact near neighbor graph while practitioners use approximate near neighbor graph (ANN-Graph) to reduce the preprocessing time. This work bridges this gap by presenting the theoretical guarantees of solving NN-Search via greedy search on ANN-Graph for low dimensional and dense vectors. To build this bridge, we leverage several novel tools from computational geometry. Our results provide quantification of the trade-offs associated with the approximation while building a near neighbor graph. We hope our results will open the door for more provable efficient graph-based NN-Search algorithms."}}
{"id": "myJURQZ9nU", "cdate": 1672531200000, "mdate": 1682331381565, "content": {"title": "Super-resolution and Robust Sparse Continuous Fourier Transform in Any Constant Dimension: Nearly Linear Time and Sample Complexity", "abstract": "The ability to resolve detail in the object that is being imaged, named by resolution, is the core parameter of an imaging system. Super-resolution is a class of techniques that can enhance the resolution of an imaging system and even transcend the diffraction limit of systems. Despite huge success in the application, super-resolution is not well understood on the theoretical side, especially for any dimension d \u2265 2. In particular, in order to recover a k-sparse signal, all previous results suffer from either/both poly(k) samples or running time. We design robust algorithms for any (constant) dimension under a strong noise model based on developing some new techniques in Sparse Fourier transform (Sparse FT), such as inverting a robust linear system, \u201ceggshell\u201d sampling schemes, and partition and voting methods in high dimension. These algorithms are the first to achieve running time and sample complexity (nearly) linear in the number of source points and logarithmic in bandwidth for any constant dimension, and we believe the techniques developed in the work can find their further applications on the Super-resolution and Sparse FT problem. * The full version of the paper can be accessed at https://arxiv.org/abs/2005.06156"}}
{"id": "kdu6cTWzvN", "cdate": 1672531200000, "mdate": 1683235214456, "content": {"title": "Solving Regularized Exp, Cosh and Sinh Regression Problems", "abstract": "In modern machine learning, attention computation is a fundamental task for training large language models such as Transformer, GPT-4 and ChatGPT. In this work, we study exponential regression problem which is inspired by the softmax/exp unit in the attention mechanism in large language models. The standard exponential regression is non-convex. We study the regularization version of exponential regression problem which is a convex problem. We use approximate newton method to solve in input sparsity time. Formally, in this problem, one is given matrix $A \\in \\mathbb{R}^{n \\times d}$, $b \\in \\mathbb{R}^n$, $w \\in \\mathbb{R}^n$ and any of functions $\\exp, \\cosh$ and $\\sinh$ denoted as $f$. The goal is to find the optimal $x$ that minimize $ 0.5 \\| f(Ax) - b \\|_2^2 + 0.5 \\| \\mathrm{diag}(w) A x \\|_2^2$. The straightforward method is to use the naive Newton's method. Let $\\mathrm{nnz}(A)$ denote the number of non-zeros entries in matrix $A$. Let $\\omega$ denote the exponent of matrix multiplication. Currently, $\\omega \\approx 2.373$. Let $\\epsilon$ denote the accuracy error. In this paper, we make use of the input sparsity and purpose an algorithm that use $\\log ( \\|x_0 - x^*\\|_2 / \\epsilon)$ iterations and $\\widetilde{O}(\\mathrm{nnz}(A) + d^{\\omega} )$ per iteration time to solve the problem."}}
{"id": "jtJwR0GLUP8", "cdate": 1672531200000, "mdate": 1683235214475, "content": {"title": "Convex Minimization with Integer Minima in \u00d5(n4) Time", "abstract": "Given a convex function $f$ on $\\mathbb{R}^n$ with an integer minimizer, we show how to find an exact minimizer of $f$ using $O(n^2 \\log n)$ calls to a separation oracle and $O(n^4 \\log n)$ time. The previous best polynomial time algorithm for this problem given in [Jiang, SODA 2021, JACM 2022] achieves $\\widetilde{O}(n^2)$ oracle complexity. However, the overall runtime of Jiang's algorithm is at least $\\widetilde{\\Omega}(n^8)$, due to expensive sub-routines such as the Lenstra-Lenstra-Lov\\'asz (LLL) algorithm [Lenstra, Lenstra, Lov\\'asz, Math. Ann. 1982] and random walk based cutting plane method [Bertsimas, Vempala, JACM 2004]. Our significant speedup is obtained by a nontrivial combination of a faster version of the LLL algorithm due to [Neumaier, Stehl\\'e, ISSAC 2016] that gives similar guarantees, the volumetric center cutting plane method (CPM) by [Vaidya, FOCS 1989] and its fast implementation given in [Jiang, Lee, Song, Wong, STOC 2020].   For the special case of submodular function minimization (SFM), our result implies a strongly polynomial time algorithm for this problem using $O(n^3 \\log n)$ calls to an evaluation oracle and $O(n^4 \\log n)$ additional arithmetic operations. Both the oracle complexity and the number of arithmetic operations of our more general algorithm are better than the previous best-known runtime algorithms for this specific problem given in [Lee, Sidford, Wong, FOCS 2015] and [Dadush, V\\'egh, Zambelli, SODA 2018, MOR 2021]."}}
{"id": "bnFyNXelNo", "cdate": 1672531200000, "mdate": 1683235214470, "content": {"title": "An Improved Sample Complexity for Rank-1 Matrix Sensing", "abstract": "Matrix sensing is a problem in signal processing and machine learning that involves recovering a low-rank matrix from a set of linear measurements. The goal is to reconstruct the original matrix as accurately as possible, given only a set of linear measurements obtained by sensing the matrix [Jain, Netrapalli and Shanghavi, 2013]. In this work, we focus on a particular direction of matrix sensing, which is called rank-$1$ matrix sensing [Zhong, Jain and Dhillon, 2015]. We present an improvement over the original algorithm in [Zhong, Jain and Dhillon, 2015]. It is based on a novel analysis and sketching technique that enables faster convergence rates and better accuracy in recovering low-rank matrices. The algorithm focuses on developing a theoretical understanding of the matrix sensing problem and establishing its advantages over previous methods. The proposed sketching technique allows for efficiently extracting relevant information from the linear measurements, making the algorithm computationally efficient and scalable. Our novel matrix sensing algorithm improves former result [Zhong, Jain and Dhillon, 2015] on in two senses: $\\bullet$ We improve the sample complexity from $\\widetilde{O}(\\epsilon^{-2} dk^2)$ to $\\widetilde{O}(\\epsilon^{-2} (d+k^2))$. $\\bullet$ We improve the running time from $\\widetilde{O}(md^2 k^2)$ to $\\widetilde{O}(m d^2 k)$. The proposed algorithm has theoretical guarantees and is analyzed to provide insights into the underlying structure of low-rank matrices and the nature of the linear measurements used in the recovery process. It advances the theoretical understanding of matrix sensing and provides a new approach for solving this important problem."}}
