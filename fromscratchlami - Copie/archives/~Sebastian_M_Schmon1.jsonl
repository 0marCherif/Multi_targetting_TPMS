{"id": "qTyeGCKdeN", "cdate": 1681462862629, "mdate": 1681462862629, "content": {"title": "Approximate Bayesian Computation with Path Signatures", "abstract": "Simulation models often lack tractable likelihood functions, making likelihood-free inference methods indispensable. Approximate Bayesian computation generates likelihood-free posterior samples by comparing simulated and observed data through some distance measure, but existing approaches are often poorly suited to time series simulators, for example due to an independent and identically distributed data assumption. In this paper, we propose to use path signatures in approximate Bayesian computation to handle the sequential nature of time series. We provide theoretical guarantees on the resultant posteriors and demonstrate competitive Bayesian parameter inference for simulators generating univariate, multivariate, irregularly spaced, and even non-Euclidean sequences"}}
{"id": "pREEF8_kWNT", "cdate": 1664806781264, "mdate": null, "content": {"title": "SurviVAEl: Variational Autoencoders for Clustering Time Series", "abstract": "Multi-state models are generalizations of time-to-event models, where individuals progress through discrete states in continuous time. As opposed to classical approaches to survival analysis which include only alive-dead transitions, states can be competing in nature and transient, enabling richer modelling of complex clinical event series. Classical multi-state models, such as the Cox-Markov model, struggle to capture idiosyncratic, non-linear, time dependent, or high-dimensional covariates for which more sophisticated machine learning models are needed. Recently proposed extensions can overcome these limitations, however, they do not allow for uncertainty quantification of the model prediction, and typically have limited interpretability at the individual or population level. Here, we introduce SurviVAEl, a multi-state survival framework based on a VAE architecture, enabling uncertainty quantification and interpretable patient trajectory clustering."}}
{"id": "Bol45H5FAc", "cdate": 1664028936736, "mdate": null, "content": {"title": "Approximate Bayesian Computation for Panel Data with Signature Maximum Mean Discrepancies", "abstract": "Simulation models are becoming a staple tool across application domains from economics to biology. When such models are stochastic, evaluating their likelihood functions in a reasonable time is typically infeasible or even impossible. In these settings, simulation-based inference procedures are a convenient means to approximating conventional parameter calibration procedures. A popular example is approximate Bayesian computation, in which the observed data is compared to the simulation output at different parameter values through some distance function. While many such methods exist, few are compatible with panel data of various kinds, as might appear in medical settings, for example; many methods instead assume iid observations in both the simulated and observed data. We seek to address this gap through the use of signature maximum mean discrepancies as distance measures in approximate Bayesian computation. Through experiments with a dynamical model of functional brain networks, we demonstrate that such an approach can flexibly operate on panel data of various kinds, for example dynamic graph data arising from multiple patients/subjects in fMRI settings."}}
{"id": "hUVR8esZH9t", "cdate": 1653772128273, "mdate": null, "content": {"title": "High Performance Simulation for Scalable Multi-Agent Reinforcement Learning", "abstract": "Multi-agent reinforcement learning experiments and open-source training environments are typically limited in scale, supporting tens or sometimes up to hundreds of interacting agents. In this paper we demonstrate the use of Vogue, a high performance agent based modelling framework. Vogue serves as a multi-agent training environment, supporting thousands to tens of thousands of interacting agents while maintaining high training throughput by running both the environment and reinforcement learning agents on the GPU. \nHigh performance multi-agent environments at this scale have the potential to enable the learning of robust and flexible policies for use in agent based models and simulations of complex systems. We demonstrate training performance with two newly developed, large scale multi-agent training environments. Moreover, we show that these environments can train shared reinforcement learning policies on time-scales of minutes and hours."}}
{"id": "ZWyHGTUcgJD", "cdate": 1653772128139, "mdate": null, "content": {"title": "Calibrating Agent-based Models to Microdata with Graph Neural Networks", "abstract": "Calibrating agent-based models to data is among the most fundamental requirements to ensure the model fulfils its desired purpose. In recent years, simulation-based inference methods have emerged as powerful tools for performing this task when the model likelihood function is intractable, as is often the case for agent-based models. In some real-world use cases of agent-based models, both the observed data and the agent-based model output consist of the agents' states and their interactions over time. In such cases, there is a tension between the desire to make full use of the rich information content of such granular data on the one hand, and the need to reduce the dimensionality of the data to prevent difficulties associated with high-dimensional learning tasks on the other. A possible resolution is to construct lower-dimensional time-series through the use of summary statistics describing the macrostate of the system at each time point. However, a poor choice of summary statistics can result in an unacceptable loss of information from the original dataset, dramatically reducing the quality of the resulting calibration. In this work, we instead propose to learn parameter posteriors associated with granular microdata directly using temporal graph neural networks. We will demonstrate that such an approach offers highly compelling inductive biases for Bayesian inference using the raw agent-based model microstates as output."}}
{"id": "MHE27tjD8m3", "cdate": 1652737826881, "mdate": null, "content": {"title": "Robust Neural Posterior Estimation and Statistical Model Criticism", "abstract": "Computer simulations have proven a valuable tool for understanding complex phenomena across the sciences. However, the utility of simulators for modelling and forecasting purposes is often restricted by low data quality, as well as practical limits to model fidelity. In order to circumvent these difficulties, we argue that modellers must treat simulators as idealistic representations of the true data generating process, and consequently should thoughtfully consider the risk of model misspecification. In this work we revisit neural posterior estimation (NPE), a class of algorithms that enable black-box parameter inference in simulation models, and consider the implication of a simulation-to-reality gap. While recent works have demonstrated reliable performance of these methods, the analyses have been performed using synthetic data generated by the simulator model itself, and have therefore only addressed the well-specified case. In this paper, we find that the presence of misspecification, in contrast, leads to unreliable inference when NPE is used na\u00efvely. As a remedy we argue that principled scientific inquiry with simulators should incorporate a model criticism component, to facilitate interpretable identification of misspecification and a robust inference component, to fit \u2018wrong but useful\u2019 models. We propose robust neural posterior estimation (RNPE), an extension of NPE to simultaneously achieve both these aims, through explicitly modelling the discrepancies between simulations and the observed data. We assess the approach on a range of artificially misspecified examples, and find RNPE performs well across the tasks, whereas na\u00efvely using NPE leads to misleading and erratic posteriors."}}
{"id": "BY88eBbkpe5", "cdate": 1646223672198, "mdate": null, "content": {"title": "Denoising Diffusion Probabilistic Models on SO(3) for Rotational Alignment", "abstract": "Probabilistic diffusion models are capable of modeling complex data distributions on high-dimensional Euclidean spaces for a range applications. However, many real world tasks involve more complex structures such as data distributions defined on manifolds which cannot be easily represented by diffusions on $\\mathbb{R}^n$. This paper proposes denoising diffusion models for tasks involving 3D rotations leveraging diffusion processes on the Lie group $SO(3)$ in order to generate candidate solutions to rotational alignment tasks. The experimental results show the proposed $SO(3)$ diffusion process outperforms na\u00efve approaches such as Euler angle diffusion in synthetic rotational distribution sampling and in a 3D object alignment task. "}}
{"id": "1xXvPrAshao", "cdate": 1632875631362, "mdate": null, "content": {"title": "Learning Multimodal VAEs through Mutual Supervision", "abstract": "Multimodal VAEs seek to model the joint distribution over heterogeneous data (e.g.\\ vision, language), whilst also capturing a shared representation across such modalities. Prior work has typically combined information from the modalities by reconciling idiosyncratic representations directly in the recognition model through explicit products, mixtures, or other such factorisations. Here we introduce a novel alternative, the MEME, that avoids such explicit combinations by repurposing semi-supervised VAEs to combine information between modalities implicitly through mutual supervision. This formulation naturally allows learning from partially-observed data where some modalities can be entirely missing---something that most existing approaches either cannot handle, or do so to a limited extent. We demonstrate that MEME outperforms baselines on standard metrics across both partial and complete observation schemes on the MNIST-SVHN (image--image) and CUB (image--text) datasets. We also contrast the quality of the representations learnt by mutual supervision against standard approaches and observe interesting trends in its ability to capture relatedness between data."}}
{"id": "OOlxsoRPyFL", "cdate": 1622637627900, "mdate": null, "content": {"title": "Deep Signature Statistics for Likelihood-free Time-series Models", "abstract": "Simulation-based inference (SBI) has emerged as a family of methods for performing inference on complex simulation models with intractable likelihood functions. A common bottleneck in SBI is the construction of low-dimensional summary statistics of the data. In this respect, time-series data, often being high-dimensional, multivariate, and complex in structure, present a particular challenge. To address this we introduce deep signature statistics, a principled and automated method for combining summary statistic selection for time-series data with neural SBI methods. Our approach leverages deep signature transforms, trained concurrently with a neural density estimator, to produce informative statistics for multivariate sequential data that encode important geometric properties of the underlying path. We obtain competitive results across benchmark models."}}
{"id": "rJwdiB_Z7Fz", "cdate": 1618592702831, "mdate": null, "content": {"title": "Optimal scaling of random walk Metropolis algorithms using Bayesian large-sample asymptotics", "abstract": "High-dimensional asymptotics have been shown to be useful to derive tuning rules for finding the optimal\nscaling in random walk Metropolis algorithms. The assumptions under which weak convergence results\nare proved are however restrictive; the target density is typically assumed to be of a product form. Users\nmay thus doubt the validity of such tuning rules in practical applications. In this paper, we shed some\nlight on optimal scaling problems from a different perspective, namely a large-sample one. This allows to\nprove weak convergence results under realistic assumptions and to propose novel parameter dimension\ndependent tuning guidelines. The proposed guidelines are consistent with previous ones when the target\ndensity is close to having a product form, but significantly different when this is not the case.\n"}}
