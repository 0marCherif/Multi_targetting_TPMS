{"id": "3usGD1nUYTf", "cdate": 1700046714573, "mdate": 1700046714573, "content": {"title": "Bags of affine subspaces for robust object tracking", "abstract": "We propose an adaptive tracking algorithm where the object is modelled as a continuously updated bag of affine subspaces, with each subspace constructed from the object's appearance over several consecutive frames. In contrast to linear subspaces, affine subspaces explicitly model the origin of subspaces. Furthermore, instead of using a brittle point-to-subspace distance during the search for the object in a new frame, we propose to use a subspace-to-subspace distance by representing candidate image areas also as affine subspaces. Distances between subspaces are then obtained by exploiting the non-Euclidean geometry of Grassmann manifolds. Experiments on challenging videos (containing object occlusions, deformations, as well as variations in pose and illumination) indicate that the proposed method achieves higher tracking accuracy than several recent discriminative trackers."}}
{"id": "jb7gX8s7av", "cdate": 1683882788860, "mdate": 1683882788860, "content": {"title": "Block Mean Approximation for Efficient Second Order Optimization", "abstract": "Advanced optimization algorithms such as Newton method and AdaGrad benefit from second order derivative or second order statistics to achieve better descent directions and faster convergence rates. At their heart, such algorithms need to compute the inverse or inverse square root of a matrix whose size is quadratic of the dimensionality of the search space. For high dimensional search spaces, the matrix inversion or inversion of square root becomes overwhelming which in turn demands for approximate methods. In this work, we propose a new matrix approximation method which divides a matrix into blocks and represents each block by one or two numbers. The method allows efficient computation of matrix inverse and inverse square root. We apply our method to AdaGrad in training deep neural networks. Experiments show encouraging results compared to the diagonal approximation.\n"}}
{"id": "j9yxfHTsfc", "cdate": 1683882508809, "mdate": 1683882508809, "content": {"title": "Devon: Deformable Volume Network for Learning Optical Flow", "abstract": "State-of-the-art neural network models estimate large displacement optical flow in multi-resolution and use warping to propagate the estimation between two resolutions. Despite their impressive results, it is known that there are two problems with the approach. First, the multi-resolution estimation of optical flow fails in situations where small objects move fast. Second, warping creates artifacts when occlusion or dis-occlusion happens. In this paper, we propose a new neural network module, Deformable Cost Volume, which alleviates the two problems. Based on this module, we designed the Deformable Volume Network (Devon) which can estimate multi-scale optical flow in a single high resolution. Experiments show Devon is more suitable in handling small objects moving fast and achieves comparable results to the state-of-the-art methods in public benchmarks."}}
{"id": "p1KfZ1M9Pcb", "cdate": 1672531200000, "mdate": 1681557075412, "content": {"title": "Curvature-Adaptive Meta-Learning for Fast Adaptation to Manifold Data", "abstract": ""}}
{"id": "BuTJ-ag-pA", "cdate": 1672531200000, "mdate": 1681557075428, "content": {"title": "LAVA:Label-efficient Visual Learning and Adaptation", "abstract": ""}}
{"id": "TD8yNmqaTcA", "cdate": 1668777818291, "mdate": 1668777818291, "content": {"title": "Discrepant collaborative training by Sinkhorn divergences", "abstract": "Deep Co-Training algorithms are typically comprised of two distinct and diverse feature extractors that simultaneously attempt to learn task-specific features from the same inputs. Achieving such an objective is, however, not trivial, despite its innocent look. This is because homogeneous networks tend to mimic each other under the collaborative training setup. Keeping this difficulty in mind, we make use of the newly proposed S\u2208 divergence to encourage diversity between homogeneous networks. The S\u2208 divergence encapsulates popular measures such as maximum mean discrepancy and the Wasserstein distance under the same umbrella and provides us with a principled, yet simple and straightforward mechanism. Our empirical results in two domains, classification in the presence of noisy labels and semi-supervised image classification, clearly demonstrate the benefits of the proposed framework in learning distinct and diverse features. We show that in these respective settings, we achieve impressive results by a notable margin."}}
{"id": "gdP4R-n-qeR", "cdate": 1668777712248, "mdate": 1668777712248, "content": {"title": "Learning Deep Optimal Embeddings with Sinkhorn Divergences", "abstract": "Deep Metric Learning algorithms aim to learn an efficient embedding space to preserve the similarity relationships among the input data. Whilst these algorithms have achieved significant performance gains across a wide plethora of tasks, they have also failed to consider and increase comprehensive similarity constraints; thus learning a sub-optimal metric in the embedding space. Moreover, up until now; there have been few studies with respect to their performance in the presence of noisy labels. Here, we address the concern of learning a discriminative deep embedding space by designing a novel, yet effective Deep Class-wise Discrepancy Loss (DCDL) function that segregates the underlying similarity distributions (thus introducing class-wise discrepancy) of the embedding points between each and every class. Our empirical results across three standard image classification datasets and two fine-grained image recognition datasets in the presence and absence of noise clearly demonstrate the need for incorporating such class-wise similarity relationships along with traditional algorithms while learning a discriminative embedding space."}}
{"id": "VYNhEzpJol", "cdate": 1668777193477, "mdate": 1668777193477, "content": {"title": "Constrained Stochastic Gradient Descent: The Good Practice", "abstract": "Stochastic Gradient Descent (SGD) is the method of choice for large scale problems, most notably in deep learning. Recent studies target improving convergence and speed of the SGD algorithm. In this paper, we equip the SGD algorithm and its advanced versions with an intriguing feature, namely handling constrained problems. Constraints such as orthogonality are pervasive in learning theory. Nevertheless and to some extent surprising, constrained SGD algorithms are rarely studied. Our proposal makes use of Riemannian geometry and accelerated optimization techniques to deliver efficient and constrained-aware SGD methods.We will assess and contrast our proposed approaches in a wide range of problems including incremental dimensionality reduction, karcher mean and deep metric learning."}}
{"id": "c8Z9AeOomy6", "cdate": 1668777137069, "mdate": 1668777137069, "content": {"title": "Attention in Attention Networks for Person Retrieval", "abstract": "This paper generalizes the Attention in Attention (AiA) mechanism, in P. Fang et al., 2019 by employing explicit mapping in reproducing kernel Hilbert spaces to generate attention values of the input feature map. The AiA mechanism models the capacity of building inter-dependencies among the local and global features by the interaction of inner and outer attention modules. Besides a vanilla AiA module, termed linear attention with AiA, two non-linear counterparts, namely, second-order polynomial attention and Gaussian attention, are also proposed to utilize the non-linear properties of the input features explicitly, via the second-order polynomial kernel and Gaussian kernel approximation. The deep convolutional neural network, equipped with the proposed AiA blocks, is referred to as Attention in Attention Network (AiA-Net). The AiA-Net learns to extract a discriminative pedestrian representation, which combines complementary person appearance and corresponding part features. Extensive ablation studies verify the effectiveness of the AiA mechanism and the use of non-linear features hidden in the feature map for attention design. Furthermore, our approach outperforms current state-of-the-art by a considerable margin across a number of benchmarks. In addition, state-of-the-art performance is also achieved in the video person retrieval task with the assistance of the proposed AiA blocks"}}
{"id": "5mbGbPkciI", "cdate": 1668776949015, "mdate": 1668776949015, "content": {"title": "Learning from Noisy Labels via Discrepant Collaborative Training", "abstract": "Noise is ubiquitous in the world around us. Difficulty in estimating the noise within a dataset makes learning from such a dataset a difficult and challenging task. In this pa-per, we propose a novel and effective learning framework in order to alleviate the adverse effects of noise within a dataset. Towards this aim, we modify a collaborative train-ing framework to utilize discrepancy constraints between respective feature extractors enabling the learning of dis-tinct, yet discriminative features, pacifying the adverse effects of noise. Empirical results of our proposed algorithm, Discrepant Collaborative Training (DCT), achieve competitive results against several current state-of-the-art algorithms across MNIST, CIFAR10 and CIFAR100, as well as large fine-grained image classification datasets such asCUBS-200-2011 and CARS196 for different levels of noise."}}
