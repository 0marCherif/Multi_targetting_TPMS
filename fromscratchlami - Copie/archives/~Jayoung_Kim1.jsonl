{"id": "1mNssCWt_v", "cdate": 1663850146611, "mdate": null, "content": {"title": "STaSy: Score-based Tabular data Synthesis", "abstract": "Tabular data synthesis is a long-standing research topic in machine learning. Many different methods have been proposed over the past decades, ranging from statistical methods to deep generative methods. However, it has not always been successful due to the complicated nature of real-world tabular data. In this paper, we present a new model named $\\textbf{S}$core-based $\\textbf{Ta}$bular data $\\textbf{Sy}$nthesis ($\\texttt{STaSy}$) and its training strategy based on the paradigm of score-based generative modeling. Despite the fact that score-based generative models have resolved many issues in generative models, there still exists room for improvement in tabular data synthesis. Our proposed training strategy includes a self-paced learning technique and a fine-tuning strategy, which further increases the sampling quality and diversity by stabilizing the denoising score matching training. Furthermore, we also conduct rigorous experimental studies in terms of the generative task trilemma: sampling quality, diversity, and time. In our experiments with 15 benchmark tabular datasets and 7 baselines, our method outperforms existing methods in terms of task-dependant evaluations and diversity.\n"}}
{"id": "ynb2LSF6xR", "cdate": 1640995200000, "mdate": 1681649693224, "content": {"title": "STaSy: Score-based Tabular data Synthesis", "abstract": ""}}
{"id": "onuXJcSxeqs", "cdate": 1640995200000, "mdate": 1668213325660, "content": {"title": "LORD: Lower-Dimensional Embedding of Log-Signature in Neural Rough Differential Equations", "abstract": "The problem of processing very long time-series data (e.g., a length of more than 10,000) is a long-standing research problem in machine learning. Recently, one breakthrough, called neural rough differential equations (NRDEs), has been proposed and has shown that it is able to process such data. Their main concept is to use the log-signature transform, which is known to be more efficient than the Fourier transform for irregular long time-series, to convert a very long time-series sample into a relatively shorter series of feature vectors. However, the log-signature transform causes non-trivial spatial overheads. To this end, we present the method of LOweR-Dimensional embedding of log-signature (LORD), where we define an NRDE-based autoencoder to implant the higher-depth log-signature knowledge into the lower-depth log-signature. We show that the encoder successfully combines the higher-depth and the lower-depth log-signature knowledge, which greatly stabilizes the training process and increases the model accuracy. In our experiments with benchmark datasets, the improvement ratio by our method is up to 75\\% in terms of various classification and forecasting evaluation metrics."}}
{"id": "jbey_iHEyER", "cdate": 1640995200000, "mdate": 1681649693250, "content": {"title": "An Empirical Study on the Membership Inference Attack against Tabular Data Synthesis Models", "abstract": ""}}
{"id": "bREl2cWPKzn", "cdate": 1640995200000, "mdate": 1668213325639, "content": {"title": "EXIT: Extrapolation and Interpolation-based Neural Controlled Differential Equations for Time-series Classification and Forecasting", "abstract": "Deep learning inspired by differential equations is a recent research trend and has marked the state of the art performance for many machine learning tasks. Among them, time-series modeling with neural controlled differential equations (NCDEs) is considered as a breakthrough. In many cases, NCDE-based models not only provide better accuracy than recurrent neural networks (RNNs) but also make it possible to process irregular time-series. In this work, we enhance NCDEs by redesigning their core part, i.e., generating a continuous path from a discrete time-series input. NCDEs typically use interpolation algorithms to convert discrete time-series samples to continuous paths. However, we propose to i) generate another latent continuous path using an encoder-decoder architecture, which corresponds to the interpolation process of NCDEs, i.e., our neural network-based interpolation vs. the existing explicit interpolation, and ii) exploit the generative characteristic of the decoder, i.e., extrapolation beyond the time domain of original data if needed. Therefore, our NCDE design can use both the interpolated and the extrapolated information for downstream machine learning tasks. In our experiments with 5 real-world datasets and 12 baselines, our extrapolation and interpolation-based NCDEs outperform existing baselines by non-trivial margins."}}
{"id": "YdGkQaTC55t", "cdate": 1640995200000, "mdate": 1681649693440, "content": {"title": "An Empirical Study on the Membership Inference Attack against Tabular Data Synthesis Models", "abstract": ""}}
{"id": "QNx6VfbqN9e", "cdate": 1640995200000, "mdate": 1652638507470, "content": {"title": "EXIT: Extrapolation and Interpolation-based Neural Controlled Differential Equations for Time-series Classification and Forecasting", "abstract": "Deep learning inspired by differential equations is a recent research trend and has marked the state of the art performance for many machine learning tasks. Among them, time-series modeling with neural controlled differential equations (NCDEs) is considered as a breakthrough. In many cases, NCDE-based models not only provide better accuracy than recurrent neural networks (RNNs) but also make it possible to process irregular time-series. In this work, we enhance NCDEs by redesigning their core part, i.e., generating a continuous path from a discrete time-series input. NCDEs typically use interpolation algorithms to convert discrete time-series samples to continuous paths. However, we propose to i) generate another latent continuous path using an encoder-decoder architecture, which corresponds to the interpolation process of NCDEs, i.e., our neural network-based interpolation vs. the existing explicit interpolation, and ii) exploit the generative characteristic of the decoder, i.e., extrapolation beyond the time domain of original data if needed. Therefore, our NCDE design can use both the interpolated and the extrapolated information for downstream machine learning tasks. In our experiments with 5 real-world datasets and 12 baselines, our extrapolation and interpolation-based NCDEs outperform existing baselines by non-trivial margins."}}
{"id": "PMwQCl_IQUg", "cdate": 1640995200000, "mdate": 1681649693405, "content": {"title": "SOS: Score-based Oversampling for Tabular Data", "abstract": ""}}
{"id": "HCgjhJrDCmh", "cdate": 1640995200000, "mdate": 1681649693421, "content": {"title": "SOS: Score-based Oversampling for Tabular Data", "abstract": ""}}
{"id": "4_fvsaJE747", "cdate": 1640995200000, "mdate": 1652638507307, "content": {"title": "LORD: Lower-Dimensional Embedding of Log-Signature in Neural Rough Differential Equations", "abstract": "The problem of processing very long time-series data (e.g., a length of more than 10,000) is a long-standing research problem in machine learning. Recently, one breakthrough, called neural rough differential equations (NRDEs), has been proposed and has shown that it is able to process such data. Their main concept is to use the log-signature transform, which is known to be more efficient than the Fourier transform for irregular long time-series, to convert a very long time-series sample into a relatively shorter series of feature vectors. However, the log-signature transform causes non-trivial spatial overheads. To this end, we present the method of LOweR-Dimensional embedding of log-signature (LORD), where we define an NRDE-based autoencoder to implant the higher-depth log-signature knowledge into the lower-depth log-signature. We show that the encoder successfully combines the higher-depth and the lower-depth log-signature knowledge, which greatly stabilizes the training process and increases the model accuracy. In our experiments with benchmark datasets, the improvement ratio by our method is up to 75\\% in terms of various classification and forecasting evaluation metrics."}}
