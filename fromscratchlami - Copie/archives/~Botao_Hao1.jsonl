{"id": "3pBfxQNP_xf", "cdate": 1674761391750, "mdate": 1674761391750, "content": {"title": "Confident Least Square Value Iteration with Local Access to a Simulator", "abstract": "Learning with simulators is ubiquitous in\nmodern reinforcement learning (RL). The simulator can either correspond to a simplified\nversion of the real environment (such as a\nphysics simulation of a robot arm) or to the\nenvironment itself (such as in games like Atari\nand Go). Among algorithms that are provably\nsample-efficient in this setting, most make the\nunrealistic assumption that all possible environment states are known before learning\nbegins, or perform global optimistic planning\nwhich is computationally inefficient. In this\nwork, we focus on simulation-based RL under\na more realistic local access protocol, where\nthe state space is unknown and the simulator can only be queried at states that have\npreviously been observed (initial states and\nthose returned by previous queries). We propose an algorithm named Confident-LSVI\nbased on the template of least-square value\niteration. Confident-LSVI incrementally\nbuilds a coreset of important states and uses\nthe simulator to revisit them. Assuming that\nthe linear function class has low approximation error under the Bellman optimality operator (a.k.a. low inherent Bellman error), we\nbound the algorithm performance in terms\nof this error, and show that it is query- and\ncomputationally-efficient."}}
{"id": "XHHWmPtjH8S", "cdate": 1674761298148, "mdate": 1674761298148, "content": {"title": "Efficient Local Planning with Linear Function Approximation", "abstract": "We study query and computationally efficient planning algorithms for discounted Markov decision processes (MDPs) with linear function approximation and a simulator. The agent is assumed\nto have local access to the simulator, meaning that the simulator can be queried only at states that\nhave been encountered in previous steps. We propose two new algorithms for this setting, which we\ncall confident Monte Carlo least-squares policy iteration (CONFIDENT MC-LSPI), and confident\nMonte Carlo Politex (CONFIDENT MC-POLITEX), respectively. The main novelty in our algorithms is that they gradually build a set of state-action pairs (\u201ccore set\u201d) with which it can control\nthe extrapolation errors. We show that our algorithms have polynomial query and computational\ncost in the dimension of the features, the effective planning horizon and the targeted sub-optimality,\nwhile the cost remains independent of the size of the state space. An interesting technical contribution of our work is the introduction of a novel proof technique that makes use of a virtual policy\niteration algorithm. We use this method to leverage existing results on approximate policy iteration\nwith `\u221e-bounded error to show that our algorithm can learn the optimal policy for the given initial\nstate even only with local access to the simulator. We believe that this technique can be extended\nto broader settings beyond this work."}}
{"id": "O1eTmqXe5C4", "cdate": 1672531200000, "mdate": 1682991900590, "content": {"title": "Leveraging Demonstrations to Improve Online Learning: Quality Matters", "abstract": "We investigate the extent to which offline demonstration data can improve online learning. It is natural to expect some improvement, but the question is how, and by how much? We show that the degree of improvement must depend on the quality of the demonstration data. To generate portable insights, we focus on Thompson sampling (TS) applied to a multi-armed bandit as a prototypical online learning algorithm and model. The demonstration data is generated by an expert with a given competence level, a notion we introduce. We propose an informed TS algorithm that utilizes the demonstration data in a coherent way through Bayes' rule and derive a prior-dependent Bayesian regret bound. This offers insight into how pretraining can greatly improve online performance and how the degree of improvement increases with the expert's competence level. We also develop a practical, approximate informed TS algorithm through Bayesian bootstrapping and show substantial empirical regret reduction through experiments."}}
{"id": "EgnEwwhNU-w", "cdate": 1672531200000, "mdate": 1683907561782, "content": {"title": "Sample Efficient Deep Reinforcement Learning via Local Planning", "abstract": "The focus of this work is sample-efficient deep reinforcement learning (RL) with a simulator. One useful property of simulators is that it is typically easy to reset the environment to a previously observed state. We propose an algorithmic framework, named uncertainty-first local planning (UFLP), that takes advantage of this property. Concretely, in each data collection iteration, with some probability, our meta-algorithm resets the environment to an observed state which has high uncertainty, instead of sampling according to the initial-state distribution. The agent-environment interaction then proceeds as in the standard online RL setting. We demonstrate that this simple procedure can dramatically improve the sample cost of several baseline RL algorithms on difficult exploration tasks. Notably, with our framework, we can achieve super-human performance on the notoriously hard Atari game, Montezuma's Revenge, with a simple (distributional) double DQN. Our work can be seen as an efficient approximate implementation of an existing algorithm with theoretical guarantees, which offers an interpretation of the positive empirical results."}}
{"id": "1fftu999o0z", "cdate": 1672531200000, "mdate": 1682991900587, "content": {"title": "Bridging Imitation and Online Reinforcement Learning: An Optimistic Tale", "abstract": "In this paper, we address the following problem: Given an offline demonstration dataset from an imperfect expert, what is the best way to leverage it to bootstrap online learning performance in MDPs. We first propose an Informed Posterior Sampling-based RL (iPSRL) algorithm that uses the offline dataset, and information about the expert's behavioral policy used to generate the offline dataset. Its cumulative Bayesian regret goes down to zero exponentially fast in N, the offline dataset size if the expert is competent enough. Since this algorithm is computationally impractical, we then propose the iRLSVI algorithm that can be seen as a combination of the RLSVI algorithm for online RL, and imitation learning. Our empirical results show that the proposed iRLSVI algorithm is able to achieve significant reduction in regret as compared to two baselines: no offline data, and offline dataset but used without information about the generative policy. Our algorithm bridges online RL and imitation learning for the first time."}}
{"id": "JyTT03dqCFD", "cdate": 1652737585404, "mdate": null, "content": {"title": "The Neural Testbed: Evaluating Joint Predictions", "abstract": "\nPredictive distributions quantify uncertainties ignored by point estimates. This paper introduces The Neural Testbed: an open source benchmark for controlled and principled evaluation of agents that generate such predictions. Crucially, the testbed assesses agents not only on the quality of their marginal predictions per input, but also on their joint predictions across many inputs. We evaluate a range of agents using a simple neural network data generating process.\n\nOur results indicate that some popular Bayesian deep learning agents do not fare well with joint predictions, even when they can produce accurate marginal predictions. We also show that the quality of joint predictions drives performance in downstream decision tasks. We find these results are robust across choice a wide range of generative models, and highlight the practical importance of joint predictions to the community."}}
{"id": "1pHC-yZfaTK", "cdate": 1652737311530, "mdate": null, "content": {"title": "Regret Bounds for Information-Directed Reinforcement Learning", "abstract": "Information-directed sampling (IDS) has revealed its potential as a data-efficient algorithm for reinforcement learning (RL). However, theoretical understanding of IDS for Markov Decision Processes (MDPs) is still limited. We develop novel information-theoretic tools to bound the information ratio and cumulative information gain about the learning target. Our theoretical results shed light on the importance of choosing the learning target such that the practitioners can balance the computation and regret bounds. As a consequence, we derive prior-free Bayesian regret bounds for vanilla-IDS which learns the whole environment under tabular finite-horizon MDPs. In addition, we propose a computationally-efficient regularized-IDS that maximizes an additive form rather than the ratio form and show that it enjoys the same regret bound as vanilla-IDS. With the aid of rate-distortion theory, we improve the regret bound by learning a surrogate, less informative environment. Furthermore, we extend our analysis to linear MDPs and prove similar regret bounds for Thompson sampling as a by-product."}}
{"id": "d4C03gOQur", "cdate": 1640995200000, "mdate": 1683892181627, "content": {"title": "Interacting Contour Stochastic Gradient Langevin Dynamics", "abstract": "We propose an interacting contour stochastic gradient Langevin dynamics (ICSGLD) sampler, an embarrassingly parallel multiple-chain contour stochastic gradient Langevin dynamics (CSGLD) sampler with efficient interactions. We show that ICSGLD can be theoretically more efficient than a single-chain CSGLD with an equivalent computational budget. We also present a novel random-field function, which facilitates the estimation of self-adapting parameters in big data and obtains free mode explorations. Empirically, we compare the proposed algorithm with popular benchmark methods for posterior sampling. The numerical results show a great potential of ICSGLD for large-scale uncertainty estimation tasks."}}
{"id": "aSV2gi3IyYX", "cdate": 1640995200000, "mdate": 1683907560139, "content": {"title": "Regret Bounds for Information-Directed Reinforcement Learning", "abstract": "Information-directed sampling (IDS) has revealed its potential as a data-efficient algorithm for reinforcement learning (RL). However, theoretical understanding of IDS for Markov Decision Processes (MDPs) is still limited. We develop novel information-theoretic tools to bound the information ratio and cumulative information gain about the learning target. Our theoretical results shed light on the importance of choosing the learning target such that the practitioners can balance the computation and regret bounds. As a consequence, we derive prior-free Bayesian regret bounds for vanilla-IDS which learns the whole environment under tabular finite-horizon MDPs. In addition, we propose a computationally-efficient regularized-IDS that maximizes an additive form rather than the ratio form and show that it enjoys the same regret bound as vanilla-IDS. With the aid of rate-distortion theory, we improve the regret bound by learning a surrogate, less informative environment. Furthermore, we extend our analysis to linear MDPs and prove similar regret bounds for Thompson sampling as a by-product."}}
{"id": "Z1bN6Bbsn3Q", "cdate": 1640995200000, "mdate": 1664580035721, "content": {"title": "Contextual Information-Directed Sampling", "abstract": "Information-directed sampling (IDS) has recently demonstrated its potential as a data-efficient reinforcement learning algorithm. However, it is still unclear what is the right form of information ratio to optimize when contextual information is available. We investigate the IDS design through two contextual bandit problems: contextual bandits with graph feedback and sparse linear contextual bandits. We provably demonstrate the advantage of contextual IDS over conditional IDS and emphasize the importance of considering the context distribution. The main message is that an intelligent agent should invest more on the actions that are beneficial for the future unseen contexts while the conditional IDS can be myopic. We further propose a computationally-efficient version of contextual IDS based on Actor-Critic and evaluate it empirically on a neural network contextual bandit."}}
