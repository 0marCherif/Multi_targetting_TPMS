{"id": "XMQgwiJ7KSX", "cdate": 1686324863293, "mdate": null, "content": {"title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control", "abstract": "We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink)."}}
{"id": "a5wxtH3zbp", "cdate": 1640995200000, "mdate": 1679934380622, "content": {"title": "Entity-Focused Dense Passage Retrieval for Outside-Knowledge Visual Question Answering", "abstract": ""}}
{"id": "1ymr-P9ja8", "cdate": 1640995200000, "mdate": 1679934380664, "content": {"title": "Multi-Modal Answer Validation for Knowledge-Based VQA", "abstract": ""}}
{"id": "ILYX-vQnwe_", "cdate": 1632875725599, "mdate": null, "content": {"title": "Breaking Down Questions for Outside-Knowledge VQA", "abstract": "While general Visual Question Answering (VQA) focuses on querying visual content within an image, there is a recent trend towards Knowledge-Based VQA (KB-VQA) where a system needs to link some aspects of the question to different types of knowledge beyond the image, such as commonsense concepts and factual information. To address this issue, we propose a novel approach that passes knowledge from various sources between different pieces of semantic content in the question. Questions are first segmented into several chunks, and each segment is used as a key to retrieve knowledge from ConceptNet and Wikipedia. Then, a graph neural network, taking advantage of the question's syntactic structure, integrates the knowledge for different segments to jointly predict the answer. Our experiments on the OK-VQA dataset show that our approach achieves new state-of-the-art results. "}}
{"id": "64kvr7EpDV", "cdate": 1609459200000, "mdate": 1679934380502, "content": {"title": "Visual question answering based on local-scene-aware referring expression generation", "abstract": ""}}
{"id": "l3s0YFtN32G", "cdate": 1577836800000, "mdate": 1679934380615, "content": {"title": "CoNAN: A Complementary Neighboring-based Attention Network for Referring Expression Generation", "abstract": ""}}
{"id": "aNC9V_2E602", "cdate": 1577836800000, "mdate": 1679934380613, "content": {"title": "Improving VQA and its Explanations by Comparing Competing Explanations", "abstract": ""}}
{"id": "SyepTNrxLB", "cdate": 1567802564571, "mdate": null, "content": {"title": "Self-Critical Reasoning for Robust Visual Question Answering", "abstract": "Visual Question Answering (VQA) deep-learning systems tend to capture superficial statistical correlations in the training data because of strong language priors and fail to generalize to test data with a significantly different question-answer (QA) distribution. To address this issue, we introduce a self-critical training objective that ensures that visual explanations of correct answers match the most influential image regions more than other competitive answer candidates. The influential regions are either determined from human visual/textual explanations or automatically from just significant words in the question and answer. We evaluate our approach on the VQA generalization task using the VQA-CP dataset, achieving a new state-of-the-art i.e. 49.5\\% using textual explanations and 48.5\\% using automatically"}}
{"id": "YXrAsmjYxFG", "cdate": 1546300800000, "mdate": 1679934380769, "content": {"title": "Generating Question Relevant Captions to Aid Visual Question Answering", "abstract": ""}}
{"id": "JhU1ck4snn", "cdate": 1546300800000, "mdate": 1679934380559, "content": {"title": "Hidden State Guidance: Improving Image Captioning Using an Image Conditioned Autoencoder", "abstract": ""}}
