{"id": "AlgbeSuE1lx", "cdate": 1652737267691, "mdate": null, "content": {"title": "Coded Residual Transform for Generalizable Deep Metric Learning", "abstract": "A fundamental challenge in deep metric learning is the generalization capability of the  feature embedding network model since the embedding network learned on training classes need to be evaluated on new test classes. To address this challenge, in this paper, we introduce a new method called coded residual transform (CRT) for deep metric learning to significantly improve its generalization capability. Specifically, we learn a set of diversified prototype features, project the feature map onto each prototype, and then encode its features using their projection residuals weighted by their correlation coefficients with each prototype. The proposed CRT method has the following two unique characteristics. First, it represents and encodes the feature map from a set of complimentary perspectives based on projections onto diversified prototypes. Second, unlike existing transformer-based feature representation approaches which encode the original values of features based on global correlation analysis, the proposed coded residual transform encodes the relative differences between the original features and their projected prototypes. Embedding space density and spectral decay analysis show that this multi perspective projection onto diversified prototypes and coded residual representation  are able to achieve significantly improved generalization capability in metric learning. Finally, to further enhance the generalization performance, we propose to enforce the consistency on their feature similarity matrices between  coded residual transforms with different sizes of projection prototypes and embedding dimensions. Our extensive experimental results and ablation studies demonstrate that the proposed CRT method outperform the state-of-the-art deep metric learning methods by large margins and improving upon the current best method by up to 4.28% on the CUB dataset."}}
{"id": "kLD4USiy2sF", "cdate": 1640995200000, "mdate": 1651664182898, "content": {"title": "Local Semantic Correlation Modeling Over Graph Neural Networks for Deep Feature Embedding and Image Retrieval", "abstract": "Deep feature embedding aims to learn discriminative features or feature embeddings for image samples which can minimize their intra-class distance while maximizing their inter-class distance. Recent state-of-the-art methods have been focusing on learning deep neural networks with carefully designed loss functions. In this work, we propose to explore a new approach to deep feature embedding. We learn a graph neural network to characterize and predict the local correlation structure of images in the feature space. Based on this correlation structure, neighboring images collaborate with each other to generate and refine their embedded features based on local linear combination. Graph edges learn a correlation prediction network to predict the correlation scores between neighboring images. Graph nodes learn a feature embedding network to generate the embedded feature for a given image based on a weighted summation of neighboring image features with the correlation scores as weights. Our extensive experimental results under the image retrieval settings demonstrate that our proposed method outperforms the state-of-the-art methods by a large margin, especially for top-1 recalls."}}
{"id": "ddXMok1dtqH", "cdate": 1640995200000, "mdate": 1668429252637, "content": {"title": "Contrastive Bayesian Analysis for Deep Metric Learning", "abstract": "Recent methods for deep metric learning have been focusing on designing different contrastive loss functions between positive and negative pairs of samples so that the learned feature embedding is able to pull positive samples of the same class closer and push negative samples from different classes away from each other. In this work, we recognize that there is a significant semantic gap between features at the intermediate feature layer and class labels at the final output layer. To bridge this gap, we develop a contrastive Bayesian analysis to characterize and model the posterior probabilities of image labels conditioned by their features similarity in a contrastive learning setting. This contrastive Bayesian analysis leads to a new loss function for deep metric learning. To improve the generalization capability of the proposed method onto new classes, we further extend the contrastive Bayesian loss with a metric variance constraint. Our experimental results and ablation studies demonstrate that the proposed contrastive Bayesian metric learning method significantly improves the performance of deep metric learning in both supervised and pseudo-supervised scenarios, outperforming existing methods by a large margin."}}
{"id": "Ih83kM7g4Ez", "cdate": 1640995200000, "mdate": 1649323114675, "content": {"title": "A GAN-based input-size flexibility model for single image dehazing", "abstract": "Highlights \u2022 First, an end-to-end input-size flexibility cGAN model is proposed for single image dehazing. \u2022 Second, a simple and effective UR-Net structure is designed based on the popular U-Net structure and residual learning. \u2022 Third, a consistency loss is proposed to keep the transformation consistency between dehazing image and real image. Abstract Image-to-image translation based on generative adversarial network (GAN) has achieved state-of-the-art performance in various image restoration applications. Single image dehazing is a typical example, which aims to obtain the haze-free image of a haze one. This paper concentrates on the challenging task of single image dehazing. Based on the atmospheric scattering model, a novel model is designed to directly generate the haze-free image. The main challenge of image dehazing is that the atmospheric scattering model has two parameters, i.e., transmission map and atmospheric light. When they are estimated respectively, the errors will be accumulated to compromise the dehazing quality. Considering this reason and various image sizes, a novel input-size flexibility conditional generative adversarial network (cGAN) is proposed for single image dehazing, which is input-size flexibility at both training and test stages for image-to-image translation with cGAN framework. A simple and effective U-connection residual network (UR-Net) is proposed to combine the generator and adopt the spatial pyramid pooling (SPP) to design the discriminator. Moreover, the model is trained with multi-loss function, in which the consistency loss is a novel designed loss in this paper. Finally, a multi-scale cGAN fusion model is built to realize state-of-the-art single image dehazing performance. The proposed models receive a haze image as input and directly output a haze-free one. Experimental results demonstrate the effectiveness and efficiency of the proposed models."}}
{"id": "yepHu0KGPvo", "cdate": 1609459200000, "mdate": 1649323114730, "content": {"title": "Learned Model Composition With Critical Sample Look-Ahead for Semi-Supervised Learning on Small Sets of Labeled Samples", "abstract": "In this work, we propose to push the performance limit of semi-supervised learning on very small sets of labeled samples by developing a new method called <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">learned model composition with critical sample look-ahead</i> (LMCS). Training efficient deep neural networks on much smaller sets of labeled samples is a challenging problem. With a small labeled set, the initial network suffers from low accuracy. Based on this error-prone network, the subsequent semi-supervised learning process will be fragile and unstable. To address this issue, we propose to introduce a look-ahead master model to identify the correct direction of model evolution to effectively guide the semi-supervised learning process of the student model. Specifically, our proposed LMCS method explores two major ideas. First, it introduces a new learned model composition structure so that we can compose a more efficient master network from student models of past iterations through a network learning process. Second, we develop a new method, called <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">confined maximum entropy search</i> , to discover new critical samples near the model decision boundary and provide the master model with look-ahead access to these samples to enhance its guidance capability. Our extensive experimental results demonstrate that the proposed LMCS method outperforms the state-of-the-art semi-supervised learning methods, especially on small sets of labeled samples. For example, on the CIFAR-10 dataset, with a very small set of 80 labeled samples, our method outperforms Google\u2019s MixMatch method, reducing the error rate by more than 10%."}}
{"id": "mxEzHiVEGR", "cdate": 1609459200000, "mdate": 1649323114675, "content": {"title": "Spatial Assembly Networks for Image Representation Learning", "abstract": "It has been long recognized that deep neural networks are sensitive to changes in spatial configurations or scene structures. Image augmentations, such as random translation, cropping, and resizing, can be used to improve the robustness of deep neural networks under spatial transforms. However, changes in object part configurations, spatial layout of object, and scene structures of the images may still result in major changes in the their feature representations generated by the network, creating significant challenges for various visual learning tasks, including representation or metric learning, image classification and retrieval. In this work, we introduce a new learnable module, called spatial assembly network (SAN), to address this important issue. This SAN module examines the input image and performs a learned re-organization and assembly of feature points from different spatial locations conditioned by feature maps from previous network layers so as to maximize the discriminative power of the final feature representation. This differentiable module can be flexibly incorporated into existing network architectures, improving their capabilities in handling spatial variations and structural changes of the image scene. We demonstrate that the proposed SAN module is able to significantly improve the performance of various metric / representation learning, image retrieval and classification tasks, in both supervised and unsupervised learning scenarios."}}
{"id": "gYKssMLW8oK", "cdate": 1609459200000, "mdate": 1649323114762, "content": {"title": "Relative Order Analysis and Optimization for Unsupervised Deep Metric Learning", "abstract": "In unsupervised learning of image features without labels, especially on datasets with fine-grained object classes, it is often very difficult to tell if a given image belongs to one specific object class or another, even for human eyes. However, we can reliably tell if image C is more similar to image A than image B. In this work, we propose to explore how this relative order can be used to learn discriminative features with an unsupervised metric learning method. Instead of resorting to clustering or self-supervision to create pseudo labels for an absolute decision, which often suffers from high label error rates, we construct reliable relative orders for groups of image samples and learn a deep neural network to predict these relative orders. During training, this relative order prediction network and the feature embedding network are tightly coupled, providing mutual constraints to each other to improve overall metric learning performance in a cooperative manner. During testing, the predicted relative orders are used as constraints to optimize the generated features and refine their feature distance-based image retrieval results using a constrained optimization procedure. Our experimental results demonstrate that the proposed relative orders for unsupervised learning (ROUL) method is able to significantly improve the performance of unsupervised deep metric learning."}}
{"id": "VBCiAykmnVf", "cdate": 1609459200000, "mdate": 1649323114710, "content": {"title": "Distorted Vehicle Detection and Distance Estimation by Metric Learning-Based SSD", "abstract": "Object detection and distance estimation based on videos are important issues in advanced driver-sssistant system (ADAS). In practice, fisheye cameras are widely used to capture images with a large field of view, which will produce distorted image frames. But most of the object detection algorithms were designed for the nonfisheye camera videos without..."}}
{"id": "NsAi9D89LTb", "cdate": 1609459200000, "mdate": 1649323114760, "content": {"title": "Cross-domain Person Re-identification Based on the Sample Relation Guidance", "abstract": "The existing mainstream cross-domain person re-identification (Re-ID) methods mainly focus on reducing the deviation of the generated pseudo labels, and they did not introduce veracious label information for algorithm training on the unlabeled target domain. In this paper, we propose a new sample relation guidance (SRG) method. Specifically, the sample relation is a real label, which represents a definite positive sample pairs\u2019 relation or negative sample pairs\u2019 relation. Here, we construct a triple-branch network to form sample relation labels to improve the expressive power of features. In addition, the potential relationship of target domain label loss and source domain label loss is explored, and an adaptive adjustment label loss (ADLL) method is proposed, which effectively improves the generalization performance of the model. Extensive experiments over three benchmarks proved that our method outperforms the state-of-the-art methods."}}
{"id": "N-IM0UgPE6k", "cdate": 1609459200000, "mdate": 1649323114675, "content": {"title": "Block-based image matching for image retrieval", "abstract": "Due to the lighting, translation, scaling and rotation, image matching is a challenge task in computer vision area. In the past decades, local descriptors (e.g. SIFT, SURF and HOG, etc.) and global features (e.g. HSV, CNN, etc.) play a vital role for this task. However, most image matching methods are based on the whole image, i.e., matching the entire image directly base on some image representation methods (e.g. BoW, VLAD and deep learning, etc.). In most situations, this idea is simple and effective, but we recognize that a robust image matching can be realized based on sub-images. Thus, a block-based image matching algorithm is proposed in this paper. First, a new local composite descriptor is proposed, which combines the advantages of local gradient and color features with spatial information. Then, VLAD method is used to encode the proposed composite descriptors in one block, and block-CNN feature is extracted at the same time. Second, a block-based similarity metric is proposed for similarity calculation of two images. Finally, the proposed methods are verified on several benchmark datasets. Compared with other methods, experimental results show that our method achieves better performance."}}
