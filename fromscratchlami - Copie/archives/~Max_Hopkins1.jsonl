{"id": "joqYlksqfX2", "cdate": 1672531200000, "mdate": 1685045563452, "content": {"title": "Do PAC-Learners Learn the Marginal Distribution?", "abstract": "We study a foundational variant of Valiant and Vapnik and Chervonenkis' Probably Approximately Correct (PAC)-Learning in which the adversary is restricted to a known family of marginal distributions $\\mathscr{P}$. In particular, we study how the PAC-learnability of a triple $(\\mathscr{P},X,H)$ relates to the learners ability to infer \\emph{distributional} information about the adversary's choice of $D \\in \\mathscr{P}$. To this end, we introduce the `unsupervised' notion of \\emph{TV-Learning}, which, given a class $(\\mathscr{P},X,H)$, asks the learner to approximate $D$ from unlabeled samples with respect to a natural class-conditional total variation metric. In the classical distribution-free setting, we show that TV-learning is \\emph{equivalent} to PAC-Learning: in other words, any learner must infer near-maximal information about $D$. On the other hand, we show this characterization breaks down for general $\\mathscr{P}$, where PAC-Learning is strictly sandwiched between two approximate variants we call `Strong' and `Weak' TV-learning, roughly corresponding to unsupervised learners that estimate most relevant distances in $D$ with respect to $H$, but differ in whether the learner \\emph{knows} the set of well-estimated events. Finally, we observe that TV-learning is in fact equivalent to the classical notion of \\emph{uniform estimation}, and thereby give a strong refutation of the uniform convergence paradigm in supervised learning."}}
{"id": "aXgiMkN-8yj", "cdate": 1672531200000, "mdate": 1682614553811, "content": {"title": "Robust Empirical Risk Minimization with Tolerance", "abstract": "Developing simple, sample-efficient learning algorithms for robust classification is a pressing issue in today\u2019s tech-dominated world, and current theoretical techniques requiring exponential sampl..."}}
{"id": "Fia72hM3gEI", "cdate": 1672531200000, "mdate": 1685045563350, "content": {"title": "Stability Is Stable: Connections between Replicability, Privacy, and Adaptive Generalization", "abstract": ""}}
{"id": "-f8NON8veD", "cdate": 1672531200000, "mdate": 1685045563345, "content": {"title": "Sampling Equilibria: Fast No-Regret Learning in Structured Games", "abstract": ""}}
{"id": "-uezmSLXVoE", "cdate": 1652737602987, "mdate": null, "content": {"title": "Active Learning Polynomial Threshold Functions", "abstract": "We initiate the study of active learning polynomial threshold functions (PTFs). While traditional lower bounds imply that even univariate quadratics cannot be non-trivially actively learned, we show that allowing the learner basic access to the derivatives of the underlying classifier circumvents this issue and leads to a computationally efficient algorithm for active learning degree-$d$ univariate PTFs in $\\tilde{O}(d^3\\log(1/\\varepsilon\\delta))$ queries. We extend this result to the batch active setting, providing a smooth transition between query complexity and rounds of adaptivity, and also provide near-optimal algorithms for active learning PTFs in several average case settings. Finally, we prove that access to derivatives is insufficient for active learning multivariate PTFs, even those of just two variables."}}
{"id": "r5UWrSaEHe5", "cdate": 1640995200000, "mdate": 1645722940814, "content": {"title": "Active Learning Polynomial Threshold Functions", "abstract": "We initiate the study of active learning polynomial threshold functions (PTFs). While traditional lower bounds imply that even univariate quadratics cannot be non-trivially actively learned, we show that allowing the learner basic access to the derivatives of the underlying classifier circumvents this issue and leads to a computationally efficient algorithm for active learning degree-$d$ univariate PTFs in $\\tilde{O}(d^3\\log(1/\\varepsilon\\delta))$ queries. We also provide near-optimal algorithms and analyses for active learning PTFs in several average case settings. Finally, we prove that access to derivatives is insufficient for active learning multivariate PTFs, even those of just two variables."}}
{"id": "jAetp2WQHX6", "cdate": 1640995200000, "mdate": 1653668273151, "content": {"title": "Eigenstripping, Spectral Decay, and Edge-Expansion on Posets", "abstract": "Fast mixing of random walks on hypergraphs has led to myriad breakthroughs in theoretical computer science in the last five years. On the other hand, many important applications (e.g. to locally testable codes, 2-2 games) rely on a more general class of underlying structures called posets, and crucially take advantage of non-simplicial structure. These works make it clear the global expansion properties of posets depend strongly on their underlying architecture (e.g. simplicial, cubical, linear algebraic), but the overall phenomenon remains poorly understood. In this work, we quantify the advantage of different poset architectures in both a spectral and combinatorial sense, highlighting how regularity controls the spectral decay and edge-expansion of corresponding random walks. We show that the spectra of walks on expanding posets (Dikstein, Dinur, Filmus, Harsha APPROX-RANDOM 2018) concentrate in strips around a small number of approximate eigenvalues controlled by the regularity of the underlying poset. This gives a simple condition to identify poset architectures (e.g. the Grassmann) that exhibit exponential decay of eigenvalues, versus architectures like hypergraphs whose eigenvalues decay linearly -- a crucial distinction in applications to hardness of approximation such as the recent proof of the 2-2 Games Conjecture (Khot, Minzer, Safra FOCS 2018). We show these results lead to a tight characterization of edge-expansion on posets in the $\\ell_2$-regime (generalizing recent work of Bafna, Hopkins, Kaufman, and Lovett (SODA 2022)), and pay special attention to the case of the Grassmann where we show our results are tight for a natural set of sparsifications of the Grassmann graphs. We note for clarity that our results do not recover the characterization of expansion used in the proof of the 2-2 Games Conjecture which relies on $\\ell_\\infty$ rather than $\\ell_2$-structure."}}
{"id": "E1hYblp_zKn", "cdate": 1640995200000, "mdate": 1653668273100, "content": {"title": "Explicit Lower Bounds Against \u03a9(n)-Rounds of Sum-of-Squares", "abstract": "We construct an explicit family of 3-XOR instances hard for $\\Omega(n)$-levels of the Sum-of-Squares (SoS) semi-definite programming hierarchy. Not only is this the first explicit construction to beat brute force search (beyond low-order improvements (Tulsiani 2021, Pratt 2021)), combined with standard gap amplification techniques it also matches the (optimal) hardness of random instances up to imperfect completeness (Grigoriev TCS 2001, Schoenebeck FOCS 2008).   Our result is based on a new form of small-set high dimensional expansion (SS-HDX) inspired by recent breakthroughs in locally testable and quantum LDPC codes. Adapting the recent framework of Dinur, Filmus, Harsha, and Tulsiani (ITCS 2021) for SoS lower bounds from the Ramanujan complex to this setting, we show any (bounded-degree) SS-HDX can be transformed into a highly unsatisfiable 3-XOR instance that cannot be refuted by $\\Omega(n)$-levels of SoS. We then show Leverrier and Z\\'emor's (Arxiv 2022) recent qLDPC construction gives the desired explicit family of bounded-degree SS-HDX. Incidentally, this gives the strongest known form of bi-directional high dimensional expansion to date."}}
{"id": "rV3bSH64Slq", "cdate": 1609459200000, "mdate": 1645722940869, "content": {"title": "Hypercontractivity on High Dimensional Expanders: a Local-to-Global Approach for Higher Moments", "abstract": "Hypercontractivity is one of the most powerful tools in Boolean function analysis. Originally studied over the discrete hypercube, recent years have seen increasing interest in extensions to settings like the $p$-biased cube, slice, or Grassmannian, where variants of hypercontractivity have found a number of breakthrough applications including the resolution of Khot's 2-2 Games Conjecture (Khot, Minzer, Safra FOCS 2018). In this work, we develop a new theory of hypercontractivity on high dimensional expanders (HDX), an important class of expanding complexes that has recently seen similarly impressive applications in both coding theory and approximate sampling. Our results lead to a new understanding of the structure of Boolean functions on HDX, including a tight analog of the KKL Theorem and a new characterization of non-expanding sets. Unlike previous settings satisfying hypercontractivity, HDX can be asymmetric, sparse, and very far from products, which makes the application of traditional proof techniques challenging. We handle these barriers with the introduction of two new tools of independent interest: a new explicit combinatorial Fourier basis for HDX that behaves well under restriction, and a new local-to-global method for analyzing higher moments. Interestingly, unlike analogous second moment methods that apply equally across all types of expanding complexes, our tools rely inherently on simplicial structure. This suggests a new distinction among high dimensional expanders based upon their behavior beyond the second moment."}}
{"id": "Wfy8s_u3Hz2", "cdate": 1609459200000, "mdate": null, "content": {"title": "Bounded Memory Active Learning through Enriched Queries", "abstract": "The explosive growth of easily-accessible unlabeled data has lead to growing interest in active learning, a paradigm in which data-hungry learning algorithms adaptively select informative examples in order to lower prohibitively expensive labeling costs. Unfortunately, in standard worst-case models of learning, the active setting often provides no improvement over non-adaptive algorithms. To combat this, a series of recent works have considered a model in which the learner may ask enriched queries beyond labels. While such models have seen success in drastically lowering label costs, they tend to come at the expense of requiring large amounts of memory. In this work, we study what families of classifiers can be learned in bounded memory. To this end, we introduce a novel streaming-variant of enriched-query active learning along with a natural combinatorial parameter called lossless sample compression that is sufficient for learning not only with bounded memory, but in a query-optimal and computationally efficient manner as well. Finally, we give three fundamental examples of classifier families with small, easy to compute lossless compression schemes when given access to basic enriched queries: axis-aligned rectangles, decision trees, and halfspaces in two dimensions."}}
