{"id": "WMXSpZBeYm", "cdate": 1699837400829, "mdate": 1699837400829, "content": {"title": "Sound Source Localization is All about Cross-Modal Alignment", "abstract": "Humans can easily perceive the direction of sound sources in a visual scene, termed sound source localization.\nRecent studies on learning-based sound source localization have mainly explored the problem from a localization perspective. However, prior arts and existing benchmarks do not account for a more important aspect of the problem, cross-modal semantic understanding, which is essential for genuine sound source localization. Cross-modal semantic understanding is important in understanding semantically mismatched audio-visual events, e.g., silent objects, or offscreen sounds. To account for this, we propose a crossmodal alignment task as a joint task with sound source localization to better learn the interaction between audio and visual modalities. Thereby, we achieve high localization performance with strong cross-modal semantic understanding. Our method outperforms the state-of-the-art approaches in both sound source localization and cross-modal retrieval. Our work suggests that jointly tackling both tasks is necessary to conquer genuine sound source localization."}}
{"id": "A89KIvRYooT", "cdate": 1632875442432, "mdate": null, "content": {"title": "FoxInst: A Frustratingly Simple Baseline for Weakly Few-shot Instance Segmentation", "abstract": "We propose the first weakly-supervised few-shot instance segmentation task and a frustratingly simple but strong baseline model, FoxInst. Our work is distinguished from other approaches in that our method is trained with weak annotations, i.e., class and box annotations, during all phases, which leads to further data efficiency and practicality. Considering the challenging regime of our problem, we design the network to be an anchor-free architecture to avoid anchor box restriction, and train the network in a simple and stable way that first trains the whole network on the base classes, and then only fine-tunes the heads partially with few novel class data. To establish the foundation as a strong baseline, we carefully design evaluation setups by correcting the existing problems in the evaluation metric and test set, so that the effects of each component are well revealed. We show that FoxInst achieves comparable or even higher performance with the prior fully-supervised FSIS networks on COCO and PASCAL VOC datasets. We will release the code if accepted for reproduction."}}
{"id": "qfABnNv_3HP", "cdate": 1632765014473, "mdate": null, "content": {"title": "Normality-Calibrated Autoencoder for Unsupervised Anomaly Detection on Data Contamination", "abstract": "In this paper, we propose Normality-Calibrated Autoencoder (NCAE), which can boost anomaly detection performance on the contaminated datasets without any prior information or explicit abnormal samples in the training phase. The NCAE adversarially generates high confident normal samples from a latent space having low entropy and leverages them to predict abnormal samples in a training dataset. NCAE is trained to minimise reconstruction errors in uncontaminated samples and maximise reconstruction errors in contaminated samples. The experimental results demonstrate that our method outperforms shallow, hybrid, and deep methods for unsupervised anomaly detection and achieves comparable performance compared with semi-supervised methods using labelled anomaly samples in the training phase. The source code is publicly available on 'https://github.com/andreYoo/NCAE_UAD.git'."}}
{"id": "T1Za4BYofnN", "cdate": 1577836800000, "mdate": null, "content": {"title": "DeepPTZ: Deep Self-Calibration for PTZ Cameras", "abstract": "Rotating and zooming cameras, also called PTZ (Pan-Tilt-Zoom) cameras, are widely used in modern surveillance systems. While their zooming ability allows acquiring detailed images of the scene, it also makes their calibration more challenging since any zooming action results in a modification of their intrinsic parameters. Therefore, such camera calibration has to be computed online; this process is called self-calibration. In this paper, given an image pair captured by a PTZ camera, we propose a deep learning based approach to automatically estimate the focal length and distortion parameters of both images as well as the rotation angles between them. The proposed approach relies on a dual-Siamese structure, imposing bidirectional constraints. The proposed network is trained on a large-scale dataset automatically generated from a set of panoramas. Empirically, we demonstrate that our proposed approach achieves competitive performance with respect to both deep learning based and traditional state-of-the art methods. Our code and model will be publicly available at https://github.com/ChaoningZhang/DeepPTZ."}}
{"id": "HJe4ipEKDB", "cdate": 1569439132386, "mdate": null, "content": {"title": "Restoration of Video Frames from a Single Blurred Image with Motion Understanding", "abstract": "We propose a novel framework to generate clean video frames from a single motion-blurred image.\nWhile a broad range of literature focuses on recovering a single image from a blurred image, in this work, we tackle a more challenging task i.e.  video restoration from a blurred image. We formulate video restoration from a single blurred image as an inverse problem by setting clean image sequence and their respective motion as latent factors, and the blurred image as an observation. Our framework is based on an encoder-decoder structure with spatial transformer network modules to restore a video sequence and its underlying motion in an end-to-end manner. We design a loss function and regularizers with complementary properties to stabilize the training and analyze variant models of the proposed network. The effectiveness and transferability of our network are highlighted through a large set of experiments on two different types of datasets: camera rotation blurs generated from panorama scenes and dynamic motion blurs in high speed videos. Our code and models will be publicly available. "}}
{"id": "rwmdv_nj9r2", "cdate": 1546300800000, "mdate": null, "content": {"title": "Revisiting Residual Networks with Nonlinear Shortcuts", "abstract": ""}}
{"id": "5fbTbjkqUOm", "cdate": 1546300800000, "mdate": null, "content": {"title": "Visuomotor Understanding for Representation Learning of Driving Scenes", "abstract": ""}}
{"id": "WqD6XQw64aX", "cdate": 1451606400000, "mdate": null, "content": {"title": "Human body part classification from optical flow", "abstract": "Body part (BP) classification is important in human image analysis. In this paper, we propose an optical flow based pixel-wise BP classifier using random forest (RF) in a monocular video. Running the BP classifier on each optical frame generates noisy mis classifications. We integrate the classifier with a human detector and temporal voting to reduce most of misclassified pixels. The proposed method is evaluated quantitatively on a real world dataset and showed better performance."}}
{"id": "iu5AY0QWlEw", "cdate": 1388534400000, "mdate": null, "content": {"title": "A fusion approach for robust visual object tracking in crowd scenes", "abstract": "The visual object tracking problem in a crowd scene has many challenges such as occlusion, similar objects and complex motion. This study presents a system of which modules are composed of feature tracking and detection methods. The proposed system fuses the two modules by converting the incomparable responses into a same metric domain. According to an explicit combining rule, the results of the modules are combined and learned only when the two modules produce consistent results. The performance of the proposed algorithm was quantitatively validated and was compared with other modern visual trackers on i-Lids dataset."}}
