{"id": "Iu7hGmFqlJ", "cdate": 1672531200000, "mdate": 1681693023378, "content": {"title": "mSAM: Micro-Batch-Averaged Sharpness-Aware Minimization", "abstract": "Modern deep learning models are over-parameterized, where different optima can result in widely varying generalization performance. To account for this, Sharpness-Aware Minimization (SAM) modifies the underlying loss function to guide descent methods towards flatter minima, which arguably have better generalization abilities. In this paper, we focus on a variant of SAM known as micro-batch SAM (mSAM), which, during training, averages the updates generated by adversarial perturbations across several disjoint shards (micro batches) of a mini-batch. We extend a recently developed and well-studied general framework for flatness analysis to show that distributed gradient computation for sharpness-aware minimization theoretically achieves even flatter minima. In order to support this theoretical superiority, we provide a thorough empirical evaluation on a variety of image classification and natural language processing tasks. We also show that contrary to previous work, mSAM can be implemented in a flexible and parallelizable manner without significantly increasing computational costs. Our practical implementation of mSAM yields superior generalization performance across a wide range of tasks compared to SAM, further supporting our theoretical framework."}}
{"id": "Irp2ThUR4y", "cdate": 1672531200000, "mdate": 1681693023335, "content": {"title": "Sharpness-Aware Minimization: An Implicit Regularization Perspective", "abstract": "Sharpness-Aware Minimization (SAM) is a recent optimization framework aiming to improve the deep neural network generalization, through obtaining flatter (i.e. less sharp) solutions. As SAM has been numerically successful, recent papers have studied the theoretical aspects of the framework and have shown SAM solutions are indeed flat. However, there has been limited theoretical exploration regarding statistical properties of SAM. In this work, we directly study the statistical performance of SAM, and present a new theoretical explanation of why SAM generalizes well. To this end, we study two statistical problems, neural networks with a hidden layer and kernel regression, and prove under certain conditions, SAM has smaller prediction error over Gradient Descent (GD). Our results concern both convex and non-convex settings, and show that SAM is particularly well-suited for non-convex problems. Additionally, we prove that in our setup, SAM solutions are less sharp as well, showing our results are in agreement with the previous work. Our theoretical findings are validated using numerical experiments on numerous scenarios, including deep neural networks."}}
{"id": "cbUI6a9Sw4X", "cdate": 1664731447131, "mdate": null, "content": {"title": "Improved Deep Neural Network Generalization Using m-Sharpness-Aware Minimization", "abstract": "Modern deep learning models are over-parameterized, where the optimization setup strongly affects the generalization performance. A key element of reliable optimization for these systems is the modification of the loss function. Sharpness-Aware Minimization (SAM) modifies the underlying loss function to guide descent methods towards flatter minima, which arguably have better generalization abilities. In this paper, we focus on a variant of SAM known as mSAM, which, during training, averages the updates generated by adversarial perturbations across several disjoint shards of a mini-batch. Recent work suggests that mSAM can outperform SAM in terms of test accuracy. However, a comprehensive empirical study of mSAM is missing from the literature---previous results have mostly been limited to specific architectures and datasets. To that end,  this paper presents a thorough empirical evaluation of mSAM on various tasks and datasets. We provide a flexible implementation of mSAM and compare the generalization performance of mSAM to the performance of SAM and vanilla training on different image classification and natural language processing tasks. We also conduct careful experiments to understand the computational cost of training with mSAM, its sensitivity to hyperparameters and its correlation with the flatness of the loss landscape. Our analysis reveals that mSAM yields superior generalization performance and flatter minima, compared to SAM, across a wide range of tasks without significantly increasing computational costs."}}
{"id": "e14LHuwKzO", "cdate": 1640995200000, "mdate": 1681693023342, "content": {"title": "Improved Deep Neural Network Generalization Using m-Sharpness-Aware Minimization", "abstract": "Modern deep learning models are over-parameterized, where the optimization setup strongly affects the generalization performance. A key element of reliable optimization for these systems is the modification of the loss function. Sharpness-Aware Minimization (SAM) modifies the underlying loss function to guide descent methods towards flatter minima, which arguably have better generalization abilities. In this paper, we focus on a variant of SAM known as mSAM, which, during training, averages the updates generated by adversarial perturbations across several disjoint shards of a mini-batch. Recent work suggests that mSAM can outperform SAM in terms of test accuracy. However, a comprehensive empirical study of mSAM is missing from the literature -- previous results have mostly been limited to specific architectures and datasets. To that end, this paper presents a thorough empirical evaluation of mSAM on various tasks and datasets. We provide a flexible implementation of mSAM and compare the generalization performance of mSAM to the performance of SAM and vanilla training on different image classification and natural language processing tasks. We also conduct careful experiments to understand the computational cost of training with mSAM, its sensitivity to hyperparameters and its correlation with the flatness of the loss landscape. Our analysis reveals that mSAM yields superior generalization performance and flatter minima, compared to SAM, across a wide range of tasks without significantly increasing computational costs."}}
{"id": "3CXfVmUlIt", "cdate": 1609459200000, "mdate": 1681693023344, "content": {"title": "Archetypal Analysis for Sparse Nonnegative Matrix Factorization: Robustness Under Misspecification", "abstract": "We consider the problem of sparse nonnegative matrix factorization (NMF) with archetypal regularization. The goal is to represent a collection of data points as nonnegative linear combinations of a few nonnegative sparse factors with appealing geometric properties, arising from the use of archetypal regularization. We generalize the notion of robustness studied in Javadi and Montanari (2019) (without sparsity) to the notions of (a) strong robustness that implies each estimated archetype is close to the underlying archetypes and (b) weak robustness that implies there exists at least one recovered archetype that is close to the underlying archetypes. Our theoretical results on robustness guarantees hold under minimal assumptions on the underlying data, and applies to settings where the underlying archetypes need not be sparse. We propose new algorithms for our optimization problem; and present numerical experiments on synthetic and real datasets that shed further insights into our proposed framework and theoretical developments."}}
{"id": "RUx9t4p-OiD", "cdate": 1577836800000, "mdate": 1681693023358, "content": {"title": "Missing Low-Rank and Sparse Decomposition Based on Smoothed Nuclear Norm", "abstract": "Recovering low-rank and sparse components from missing observations is an essential problem in various fields. In this paper, we have proposed a method to address the missing low-rank and sparse decomposition problem. We have used the smoothed nuclear norm and the L <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sub> norm to impose the low-rankness and sparsity constraints on the components, respectively. Furthermore, we have suggested a linear modeling for the corrupted observations. The problem has been solved with the aid of alternating minimization. Moreover, some simplifications have been applied to the relations to reduce the computational complexity, which makes the algorithm suitable for large-scale problems. To evaluate the proposed method, different simulation scenarios have been devised. The superiority of the suggested scheme over its counterparts has been confirmed on both the recovery accuracy and the convergence speed in various applications."}}
{"id": "FFaEPOObe9e", "cdate": 1577836800000, "mdate": 1681693023354, "content": {"title": "Transductive multi-label learning from missing data using smoothed rank function", "abstract": ""}}
{"id": "UP-bxYGxelK", "cdate": 1514764800000, "mdate": 1681693023326, "content": {"title": "Recovering Quantized Data with Missing Information Using Bilinear Factorization and Augmented Lagrangian Method", "abstract": "In this paper, we propose a novel approach in order to recover a quantized matrix with missing information. We propose a regularized convex cost function composed of a log-likelihood term and a Trace norm term. The Bi-factorization approach and the Augmented Lagrangian Method (ALM) are applied to find the global minimizer of the cost function in order to recover the genuine data. We provide mathematical convergence analysis for our proposed algorithm. In the Numerical Experiments Section, we show the superiority of our method in accuracy and also its robustness in computational complexity compared to the state-of-the-art literature methods."}}
{"id": "IQMYGs_4AW", "cdate": 1514764800000, "mdate": 1681693023339, "content": {"title": "Transduction with Matrix Completion Using Smoothed Rank Function", "abstract": "In this paper, we propose two new algorithms for transduction with Matrix Completion (MC) problem. The joint MC and prediction tasks are addressed simultaneously to enhance the accuracy, i.e., the label matrix is concatenated to the data matrix forming a stacked matrix. Assuming the data matrix is of low rank, we propose new recommendation methods by posing the problem as a constrained minimization of the Smoothed Rank Function (SRF). We provide convergence analysis for the proposed algorithms. The simulations are conducted on real datasets in two different scenarios of randomly missing pattern with and without block loss. The results confirm that the accuracy of our proposed methods outperforms those of state-of-the-art methods even up to 10% in low observation rates for the scenario without block loss. Our accuracy in the latter scenario, is comparable to state-of-the-art methods while the complexity of the proposed algorithms are reduced up to 4 times."}}
