{"id": "2JXIgoMDwxx", "cdate": 1664194165683, "mdate": null, "content": {"title": "Hyperbolic and Mixed Geometry Graph Neural Networks", "abstract": "Hyperbolic Graph Neural Networks (GNNs) have shown great promise for modeling hierarchical and graph-structured data in the hyperbolic space, which reduces embedding distortion comparing to Euclidean space. However, existing hyperbolic GNNs implement most operations through differential and exponential maps in the tangent space, which is a Euclidean subspace. To avoid such complex transformations between the hyperbolic and Euclidean spaces, recent advances in hyperbolic learning have formalized hyperbolic neural networks based on the Lorentz model that realize their operations entirely in the hyperbolic space via Lorentz transformations \\cite{chen-etal-2022-fully}. Here, we adopt the hyperbolic framework from \\cite{chen-etal-2022-fully} and propose a family of hyperbolic GNNs with greater modeling capabilities as opposed to existing hyperbolic GNNs. We also show that this framework allows us to have neural networks with both hyperbolic layers and Euclidean layers that can be trained jointly. Our experiments demonstrate that our fully hyperbolic GNNs lead to substantial improvement in comparison with their Euclidean counterparts."}}
{"id": "BlWfnEZJTl9", "cdate": 1646223668394, "mdate": null, "content": {"title": "CubeRep: Learning Relations Between Different Views of Data", "abstract": "Multi-view learning tasks typically seek an aggregate synthesis of multiple views or perspectives of a single data set. The current approach assumes that there is an ambient space $X$ in which the views are images of $X$ under certain functions and attempts to learn these functions via a neural network. Unfortunately, such an approach neglects to consider the geometry of the ambient space. Hierarchically hyperbolic spaces (HHSes) do, however, provide a natural multi-view arrangement of data; they provide geometric tools for the assembly of different views of a single data set into a coherent global space, a \\emph{CAT(0) cube complex}. In this work, we provide the first step toward theoretically justifiable methods for learning embeddings of multi-view data sets into CAT(0) cube complexes. We present an algorithm which, given a finite set of finite metric spaces (views) on a finite set of points (the objects), produces the key components of an HHS structure. From this structure, we can produce a \\emph{CAT(0) cube complex} that encodes the hyperbolic geometry in the data while simultaneously allowing for Euclidean features given by the detected relations among the views."}}
{"id": "Wv518ns9-8", "cdate": 1640995200000, "mdate": 1683896907765, "content": {"title": "Knowledge Graphs of the QAnon Twitter Network", "abstract": "Using Knowledge Graphs to understand noisy naturalistic data has gained significant prominence in recent years. In this paper, we apply Knowledge Graphs to a new dataset of tweets of an ideologically far-right Twitter network by sourcing tweet histories of users who discussed QAnon in the summer of 2018 [1]. We further develop a new method that arms topic models with relational information from Knowledge Graphs and apply the new technique to study this dataset. Our analysis shows that users do not form a monolithic belief or social network, but rather comprise many smaller interlinking communities which discuss unique key political events (e.g., the January 6 <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">th</sup> Capitol riots)."}}
{"id": "T_Z-jTIKgC", "cdate": 1640995200000, "mdate": 1682097915216, "content": {"title": "Predicting the Future of AI with AI: High-quality link prediction in an exponentially growing knowledge network", "abstract": "A tool that could suggest new personalized research directions and ideas by taking insights from the scientific literature could significantly accelerate the progress of science. A field that might benefit from such an approach is artificial intelligence (AI) research, where the number of scientific publications has been growing exponentially over the last years, making it challenging for human researchers to keep track of the progress. Here, we use AI techniques to predict the future research directions of AI itself. We develop a new graph-based benchmark based on real-world data -- the Science4Cast benchmark, which aims to predict the future state of an evolving semantic network of AI. For that, we use more than 100,000 research papers and build up a knowledge network with more than 64,000 concept nodes. We then present ten diverse methods to tackle this task, ranging from pure statistical to pure learning methods. Surprisingly, the most powerful methods use a carefully curated set of network features, rather than an end-to-end AI approach. It indicates a great potential that can be unleashed for purely ML approaches without human knowledge. Ultimately, better predictions of new future research directions will be a crucial component of more advanced research suggestion tools."}}
{"id": "LqYlZYutiM", "cdate": 1640995200000, "mdate": 1658939231704, "content": {"title": "ICLR 2022 Challenge for Computational Geometry and Topology: Design and Results", "abstract": "This paper presents the computational challenge on differential geometry and topology that was hosted within the ICLR 2022 workshop ``Geometric and Topological Representation Learning\". The competition asked participants to provide implementations of machine learning algorithms on manifolds that would respect the API of the open-source software Geomstats (manifold part) and Scikit-Learn (machine learning part) or PyTorch. The challenge attracted seven teams in its two month duration. This paper describes the design of the challenge and summarizes its main findings."}}
{"id": "5ALGcXpmFyC", "cdate": 1632875549706, "mdate": null, "content": {"title": "Training Data Size Induced Double Descent For Denoising Neural Networks and the Role of Training Noise Level", "abstract": "When training a denoising neural network, we show that more data isn\u2019t more beneficial. In fact the generalization error versus number of of training data points is a double descent curve.\nTraining a network to denoise noisy inputs is the most widely used technique for pre-training deep neural networks. Hence one important question is the effect of scaling the number of training data points. We formalize the question of how many data points should be used by looking at the generalization error for denoising noisy test data. Prior work on computing the generalization error focus on adding noise to target outputs. However, adding noise to the input is more in line with current pre-training practices. In the linear (in the inputs) regime, we provide an asymptotically exact formula for the generalization error for rank 1 data and an approximation for the generalization error for rank r data. We show using our formulas, that the generalization error versus number of data points follows a double descent curve. From this, we derive a formula for the amount of noise that needs to be added to the training data to minimize the denoising error and see that this follows a double descent curve as well."}}
{"id": "0jHeZ7-ehGr", "cdate": 1621629858838, "mdate": null, "content": {"title": "How can classical multidimensional scaling go wrong?", "abstract": "Given a matrix $D$ describing the pairwise dissimilarities of a data set, a common task is to embed the data points into Euclidean space. The classical multidimensional scaling (cMDS) algorithm is a widespread method to do this. However, theoretical analysis of the robustness of the algorithm and an in-depth analysis of its performance on non-Euclidean metrics is lacking. \n\nIn this paper, we derive a formula, based on the eigenvalues of a matrix obtained from $D$, for the Frobenius norm of the difference between $D$ and the metric $D_{\\text{cmds}}$ returned by cMDS. This error analysis leads us to the conclusion that when the derived matrix has a significant number of negative eigenvalues, then $\\|D-D_{\\text{cmds}}\\|_F$, after initially decreasing, will\neventually increase as we increase the dimension. Hence, counterintuitively, the quality of the embedding degrades as we increase the dimension. We empirically verify that the Frobenius norm increases as we increase the dimension for a variety of non-Euclidean metrics. We also show on several benchmark datasets that this degradation in the embedding results in the classification accuracy of both simple (e.g., 1-nearest neighbor) and complex (e.g., multi-layer neural nets) classifiers decreasing as we increase the embedding dimension.\n\nFinally, our analysis leads us to a new efficiently computable algorithm that returns a matrix $D_l$ that is at least as close to the original distances as $D_t$ (the Euclidean metric closest in $\\ell_2$ distance). While $D_l$ is not metric, when given as input to cMDS instead of $D$, it empirically results in solutions whose distance to $D$ does not increase when we increase the dimension and the classification accuracy degrades less than the cMDS solution.  "}}
{"id": "prMlqWCoZc", "cdate": 1609459200000, "mdate": 1682925192743, "content": {"title": "Dynamic Embedding-based Methods for Link Prediction in Machine Learning Semantic Network", "abstract": "This paper aims to accelerate scientific discovery by studying link prediction in a semantic network. The nodes are unidentified concepts in machine learning, and the time-stamped edges indicate co-occurrence in scientific papers. Taking advantage of this temporal information, we perform node embedding on the graph at every year from 1994 to 2017, and apply two methods to find features for node pairs: the first method uses a transformer, while the other uses distance metrics combined with known link prediction features. The latter feature extraction technique with a 3-layer multi-layer perceptron achieved an AUC of 0.902 on predicting edges in the 2020 graph. Inspection of the resulting features suggests that the model does indeed pay attention to the dynamic nature of the features, e.g., how node-pair distance in embedding space changes over the years."}}
{"id": "Hk1rEvSJIK", "cdate": 1609459200000, "mdate": 1684355540798, "content": {"title": "How can classical multidimensional scaling go wrong?", "abstract": "Given a matrix $D$ describing the pairwise dissimilarities of a data set, a common task is to embed the data points into Euclidean space. The classical multidimensional scaling (cMDS) algorithm is a widespread method to do this. However, theoretical analysis of the robustness of the algorithm and an in-depth analysis of its performance on non-Euclidean metrics is lacking. In this paper, we derive a formula, based on the eigenvalues of a matrix obtained from $D$, for the Frobenius norm of the difference between $D$ and the metric $D_{\\text{cmds}}$ returned by cMDS. This error analysis leads us to the conclusion that when the derived matrix has a significant number of negative eigenvalues, then $\\|D-D_{\\text{cmds}}\\|_F$, after initially decreasing, willeventually increase as we increase the dimension. Hence, counterintuitively, the quality of the embedding degrades as we increase the dimension. We empirically verify that the Frobenius norm increases as we increase the dimension for a variety of non-Euclidean metrics. We also show on several benchmark datasets that this degradation in the embedding results in the classification accuracy of both simple (e.g., 1-nearest neighbor) and complex (e.g., multi-layer neural nets) classifiers decreasing as we increase the embedding dimension.Finally, our analysis leads us to a new efficiently computable algorithm that returns a matrix $D_l$ that is at least as close to the original distances as $D_t$ (the Euclidean metric closest in $\\ell_2$ distance). While $D_l$ is not metric, when given as input to cMDS instead of $D$, it empirically results in solutions whose distance to $D$ does not increase when we increase the dimension and the classification accuracy degrades less than the cMDS solution."}}
{"id": "6nSlX9VMfg", "cdate": 1609459200000, "mdate": 1684355540784, "content": {"title": "How can classical multidimensional scaling go wrong?", "abstract": "Given a matrix $D$ describing the pairwise dissimilarities of a data set, a common task is to embed the data points into Euclidean space. The classical multidimensional scaling (cMDS) algorithm is a widespread method to do this. However, theoretical analysis of the robustness of the algorithm and an in-depth analysis of its performance on non-Euclidean metrics is lacking. In this paper, we derive a formula, based on the eigenvalues of a matrix obtained from $D$, for the Frobenius norm of the difference between $D$ and the metric $D_{\\text{cmds}}$ returned by cMDS. This error analysis leads us to the conclusion that when the derived matrix has a significant number of negative eigenvalues, then $\\|D-D_{\\text{cmds}}\\|_F$, after initially decreasing, will eventually increase as we increase the dimension. Hence, counterintuitively, the quality of the embedding degrades as we increase the dimension. We empirically verify that the Frobenius norm increases as we increase the dimension for a variety of non-Euclidean metrics. We also show on several benchmark datasets that this degradation in the embedding results in the classification accuracy of both simple (e.g., 1-nearest neighbor) and complex (e.g., multi-layer neural nets) classifiers decreasing as we increase the embedding dimension. Finally, our analysis leads us to a new efficiently computable algorithm that returns a matrix $D_l$ that is at least as close to the original distances as $D_t$ (the Euclidean metric closest in $\\ell_2$ distance). While $D_l$ is not metric, when given as input to cMDS instead of $D$, it empirically results in solutions whose distance to $D$ does not increase when we increase the dimension and the classification accuracy degrades less than the cMDS solution."}}
