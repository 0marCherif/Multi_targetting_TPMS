{"id": "pEh7b95zZMG", "cdate": 1672531200000, "mdate": 1681692367385, "content": {"title": "Exploring Attention Map Reuse for Efficient Transformer Neural Networks", "abstract": "Transformer-based deep neural networks have achieved great success in various sequence applications due to their powerful ability to model long-range dependency. The key module of Transformer is self-attention (SA) which extracts features from the entire sequence regardless of the distance between positions. Although SA helps Transformer performs particularly well on long-range tasks, SA requires quadratic computation and memory complexity with the input sequence length. Recently, attention map reuse, which groups multiple SA layers to share one attention map, has been proposed and achieved significant speedup for speech recognition models. In this paper, we provide a comprehensive study on attention map reuse focusing on its ability to accelerate inference. We compare the method with other SA compression techniques and conduct a breakdown analysis of its advantages for a long sequence. We demonstrate the effectiveness of attention map reuse by measuring the latency on both CPU and GPU platforms."}}
{"id": "jVD20xXxSn", "cdate": 1672531200000, "mdate": 1681692367382, "content": {"title": "Teacher Intervention: Improving Convergence of Quantization Aware Training for Ultra-Low Precision Transformers", "abstract": "Pre-trained Transformer models such as BERT have shown great success in a wide range of applications, but at the cost of substantial increases in model complexity. Quantization-aware training (QAT) is a promising method to lower the implementation cost and energy consumption. However, aggressive quantization below 2-bit causes considerable accuracy degradation due to unstable convergence, especially when the downstream dataset is not abundant. This work proposes a proactive knowledge distillation method called Teacher Intervention (TI) for fast converging QAT of ultra-low precision pre-trained Transformers. TI intervenes layer-wise signal propagation with the intact signal from the teacher to remove the interference of propagated quantization errors, smoothing loss surface of QAT and expediting the convergence. Furthermore, we propose a gradual intervention mechanism to stabilize the recovery of subsections of Transformer layers from quantization. The proposed schemes enable fast convergence of QAT and improve the model accuracy regardless of the diverse characteristics of downstream fine-tuning tasks. We demonstrate that TI consistently achieves superior accuracy with significantly lower fine-tuning iterations on well-known Transformers of natural language processing as well as computer vision compared to the state-of-the-art QAT methods."}}
{"id": "692oJ-QFuMC", "cdate": 1663850509933, "mdate": null, "content": {"title": "Teacher Intervention: Improving Convergence of Quantization Aware Training for Ultra-Low Precision Transformers", "abstract": "Pre-trained Transformer models such as BERT have shown great success in a wide range of applications, but with the cost of substantial increases in model complexity. Quantization-aware training (QAT) is a promising way to lower the implementation cost and energy consumption. However, aggressive quantization below 2-bit causes considerable accuracy degradation, especially when the downstream dataset is not abundant.\nThis work proposes a proactive knowledge distillation method called Teacher Intervention (TI) for fast converging QAT of ultra-low precision pre-trained Transformers. TI intervenes layer-wise signal propagation with the intact signal from the teacher to remove the interference of propagated quantization errors, smoothing loss surface and expediting the convergence. We further propose a gradual intervention mechanism to stabilize the tuning of the feed-forward network and recover the self-attention map in steps. The proposed scheme enables fast convergence of QAT and improves the model accuracy regardless of the diverse characteristics of downstream fine-tuning tasks. We demonstrate that TI consistently achieves superior accuracy with lower fine-tuning budget. "}}
{"id": "wBDpeyD6_y", "cdate": 1640995200000, "mdate": 1682317822979, "content": {"title": "Achieving low write latency through new stealth program operation supporting early write completion in NAND flash memory", "abstract": ""}}
{"id": "v9eOkfvsPrM", "cdate": 1640995200000, "mdate": 1682317822731, "content": {"title": "Improving NVM Lifetime Using Task Stack Migration on Low-End MCU-Based Devices", "abstract": "Tiny embedded devices are cost and energy-sensitive, and high-density emerging non-volatile memory (NVM) can help reduce energy consumption at a fraction of the cost. However, high-density NVM has low write endurance compared to volatile memory, so it is vulnerable to write concentration. Most NVM lifetime improvement studies in the existing embedded environment have distributed writes by modifying the mapping relationship between physical and logical addresses. However, applying the existing wear leveling techniques in low-end MCUs such as ARM Cortex M3/M4 that use only physical addresses is hard. Therefore, we wear-level the write-heavy stack area to improve the NVM lifetime in low-end MCUs. However, since the stack of bare metal applications is difficult to move during runtime, we implement the migration function targeting the task stack of FreeRTOS. The task stack moves based on time, and to avoid the pointer validation problem caused by the movement of the task stack, we migrate the stack under safe conditions. In addition, FreeRTOS uses a single heap to preferentially allocate to low free space, which reduces the degree of freedom where the stack moves, reducing the effect of distributing the writes. To alleviate this problem, we add another heap for the stack migration and introduce circular dynamic allocation in the heap. Through our experiments, the proposed method was about 19.6% larger than the ideal case of maximum write, and the computational overhead was about 0.2%."}}
{"id": "rK8NI8ZYrgq", "cdate": 1640995200000, "mdate": 1645740366346, "content": {"title": "A 7-nm Four-Core Mixed-Precision AI Chip With 26.2-TFLOPS Hybrid-FP8 Training, 104.9-TOPS INT4 Inference, and Workload-Aware Throttling", "abstract": "Reduced precision computation is a key enabling factor for energy-efficient acceleration of deep learning (DL) applications. This article presents a 7-nm four-core mixed-precision artificial intelligence (AI) chip that supports four compute precisions\u2014FP16, Hybrid-FP8 (HFP8), INT4, and INT2\u2014to support diverse application demands for training and inference. The chip leverages cutting-edge algorithmic advances to demonstrate leading-edge power efficiency for 8-bit floating-point (FP8) training and INT4 inference without model accuracy degradation. A new HFP8 format combined with separation of the floating- and fixed-point pipelines and aggressive circuit/architecture optimization enables performance improvements while maintaining high compute utilization. A high-bandwidth ring protocol enables efficient data communication, while power management using workload-aware clock throttling maximizes performance within a given power budget. The AI chip demonstrates 3.58-TFLOPS/W peak energy efficiency and 26.2-TFLOPS peak performance for HFP8 iso-accuracy training, and 16.9-TOPS/W peak energy efficiency and 104.9-TOPS peak performance for INT4 iso-accuracy inference."}}
{"id": "pPsNZ0mKyz", "cdate": 1640995200000, "mdate": 1682317823313, "content": {"title": "Regularizing Activation Distribution for Ultra Low-bit Quantization-Aware Training of MobileNets", "abstract": "MobileNets are a family of CNN architectures that are designed for parameter efficient model deployment. However, quantization aware training (QAT) of MobileNets for lowprecision model development has not been very successful partly due to the parameter efficient nature of the model. The activation signal in MobileNet shows fairly large magnitude diversity for each channel, which can lead to poor quantization results. We address this issue by including a new loss term for QAT, which regularizes the diversity of the activation signal in each channel. The proposed method improves the accuracy of the state-of-theart QAT when evaluated on MobileNet with CIFAR10/100, and ImageNet datasets."}}
{"id": "lYHCIfASpfr", "cdate": 1640995200000, "mdate": 1681692367562, "content": {"title": "Understanding the Role of Self Attention for Efficient Speech Recognition", "abstract": "Self-attention (SA) is a critical component of Transformer neural networks that have succeeded in automatic speech recognition (ASR). In this paper, we analyze the role of SA in Transformer-based ASR models for not only understanding the mechanism of improved recognition accuracy but also lowering the computational complexity. We reveal that SA performs two distinct roles: phonetic and linguistic localization. Especially, we show by experiments that phonetic localization in the lower layers extracts phonologically meaningful features from speech and reduces the phonetic variance in the utterance for proper linguistic localization in the upper layers. From this understanding, we discover that attention maps can be reused as long as their localization capability is preserved. To evaluate this idea, we implement the layer-wise attention map reuse on real GPU platforms and achieve up to 1.96 times speedup in inference and 33% savings in training time with noticeably improved ASR performance for the challenging benchmark on LibriSpeech dev/test-other dataset."}}
{"id": "eXx5OQveFS", "cdate": 1640995200000, "mdate": 1667353974044, "content": {"title": "NN-LUT: neural approximation of non-linear operations for efficient transformer inference", "abstract": "Non-linear operations such as GELU, Layer normalization, and Soft-max are essential yet costly building blocks of Transformer models. Several prior works simplified these operations with look-up tables or integer computations, but such approximations suffer inferior accuracy or considerable hardware cost with long latency. This paper proposes an accurate and hardware-friendly approximation framework for efficient Transformer inference. Our framework employs a simple neural network as a universal approximator with its structure equivalently transformed into a Look-up table(LUT). The proposed framework called Neural network generated LUT(NN-LUT) can accurately replace all the non-linear operations in popular BERT models with significant reductions in area, power consumption, and latency."}}
{"id": "UiIxrrcv8T", "cdate": 1640995200000, "mdate": 1682317822897, "content": {"title": "Minimizing Global Buffer Access in a Deep Learning Accelerator Using a Local Register File with a Rearranged Computational Sequence", "abstract": "We propose a method for minimizing global buffer access within a deep learning accelerator for convolution operations by maximizing the data reuse through a local register file, thereby substituting the local register file access for the power-hungry global buffer access. To fully exploit the merits of data reuse, this study proposes a rearrangement of the computational sequence in a deep learning accelerator. Once input data are read from the global buffer, repeatedly reading the same data is performed only through the local register file, saving significant power consumption. Furthermore, different from prior works that equip local register files in each computation unit, the proposed method enables sharing a local register file along the column of the 2D computation array, saving resources and controlling overhead. The proposed accelerator is implemented on an off-the-shelf field-programmable gate array to verify the functionality and resource utilization. Then, the performance improvement of the proposed method is demonstrated relative to popular deep learning accelerators. Our evaluation indicates that the proposed deep learning accelerator reduces the number of global-buffer accesses to nearly 86.8%, consequently saving up to 72.3% of the power consumption for the input data memory access with a minor increase in resource usage compared to a conventional deep learning accelerator."}}
