{"id": "okqvDmW01o", "cdate": 1707451179078, "mdate": 1707451179078, "content": {"title": "Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models", "abstract": "Decoding methods for large language models\noften trade-off between diversity of outputs\nand parallelism of computation. Methods such\nas beam search and Gumbel top-k sampling\ncan guarantee a different output for each\nelement of the beam, but are not easy to\nparallelize. Alternatively, methods such as\ntemperature sampling and its modifications (top-k\nsampling, nucleus sampling, typical decoding,\nand others), are embarrassingly parallel, but have\nno guarantees about duplicate samples. We\npresent a framework for sampling according\nto an arithmetic code book implicitly defined\nby a large language model, compatible with\ncommon sampling variations, with provable beam\ndiversity under certain conditions, as well as being\nembarrassingly parallel and providing unbiased\nand consistent expectations from the original\nmodel. We demonstrate the effectiveness of\nour approach on WMT machine translation,\nmore than halving the standard deviation when\nestimating expected BLEU score reward, and\nclosing the BLEU score gap between independent\nsampling and beam search by up to 63%."}}
{"id": "cjVKsT-O64-", "cdate": 1672531200000, "mdate": 1702844336652, "content": {"title": "MEMORY-VQ: Compression for Tractable Internet-Scale Memory", "abstract": "Retrieval augmentation is a powerful but expensive method to make language models more knowledgeable about the world. Memory-based methods like LUMEN pre-compute token representations for retrieved passages to drastically speed up inference. However, memory also leads to much greater storage requirements from storing pre-computed representations. We propose MEMORY-VQ, a new method to reduce storage requirements of memory-augmented models without sacrificing performance. Our method uses a vector quantization variational autoencoder (VQ-VAE) to compress token representations. We apply MEMORY-VQ to the LUMEN model to obtain LUMEN-VQ, a memory model that achieves a 16x compression rate with comparable performance on the KILT benchmark. LUMEN-VQ enables practical retrieval augmentation even for extremely large retrieval corpora."}}
{"id": "NlzdXMgi_B2", "cdate": 1672531200000, "mdate": 1707451264723, "content": {"title": "Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models", "abstract": "Decoding methods for large language models often trade-off between diversity of outputs and parallelism of computation. Methods such as beam search and Gumbel top-k sampling can guarantee a differe..."}}
{"id": "w9uvVGe6tp0", "cdate": 1640995200000, "mdate": 1706739300196, "content": {"title": "ImPaKT: A Dataset for Open-Schema Knowledge Base Construction", "abstract": "Large language models have ushered in a golden age of semantic parsing. The seq2seq paradigm allows for open-schema and abstractive attribute and relation extraction given only small amounts of finetuning data. Language model pretraining has simultaneously enabled great strides in natural language inference, reasoning about entailment and implication in free text. These advances motivate us to construct ImPaKT, a dataset for open-schema information extraction, consisting of around 2500 text snippets from the C4 corpus, in the shopping domain (product buying guides), professionally annotated with extracted attributes, types, attribute summaries (attribute schema discovery from idiosyncratic text), many-to-one relations between compound and atomic attributes, and implication relations. We release this data in hope that it will be useful in fine tuning semantic parsers for information extraction and knowledge base construction across a variety of domains. We evaluate the power of this approach by fine-tuning the open source UL2 language model on a subset of the dataset, extracting a set of implication relations from a corpus of product buying guides, and conducting human evaluations of the resulting predictions."}}
{"id": "Hczd0N2a2b", "cdate": 1640995200000, "mdate": 1680027549693, "content": {"title": "Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models", "abstract": ""}}
{"id": "0IqTX6FcZWv", "cdate": 1621630356562, "mdate": null, "content": {"title": "Capacity and Bias of Learned Geometric Embeddings for Directed Graphs", "abstract": "A wide variety of machine learning tasks such as knowledge base completion, ontology alignment, and multi-label classification can benefit from incorporating into learning differentiable representations of graphs or taxonomies.  While vectors in Euclidean space can theoretically represent any graph, much recent work shows that alternatives such as complex, hyperbolic, order, or box embeddings have geometric properties better suited to modeling real-world graphs. Experimentally these gains are seen only in lower dimensions, however, with performance benefits diminishing in higher dimensions. In this work, we introduce a novel variant of box embeddings that uses a learned smoothing parameter to achieve better representational capacity than vector models in low dimensions, while also avoiding performance saturation common to other geometric models in high dimensions. Further, we present theoretical results that prove box embeddings can represent any DAG. We perform rigorous empirical evaluations of vector, hyperbolic, and region-based geometric representations on several families of synthetic and real-world directed graphs. Analysis of these results exposes correlations between different families of graphs, graph characteristics, model size, and embedding geometry, providing useful insights into the inductive biases of various differentiable graph representations."}}
{"id": "ANU-EDmxNnEF", "cdate": 1609459200000, "mdate": 1666979439445, "content": {"title": "Capacity and Bias of Learned Geometric Embeddings for Directed Graphs", "abstract": "A wide variety of machine learning tasks such as knowledge base completion, ontology alignment, and multi-label classification can benefit from incorporating into learning differentiable representations of graphs or taxonomies. While vectors in Euclidean space can theoretically represent any graph, much recent work shows that alternatives such as complex, hyperbolic, order, or box embeddings have geometric properties better suited to modeling real-world graphs. Experimentally these gains are seen only in lower dimensions, however, with performance benefits diminishing in higher dimensions. In this work, we introduce a novel variant of box embeddings that uses a learned smoothing parameter to achieve better representational capacity than vector models in low dimensions, while also avoiding performance saturation common to other geometric models in high dimensions. Further, we present theoretical results that prove box embeddings can represent any DAG. We perform rigorous empirical evaluations of vector, hyperbolic, and region-based geometric representations on several families of synthetic and real-world directed graphs. Analysis of these results exposes correlations between different families of graphs, graph characteristics, model size, and embedding geometry, providing useful insights into the inductive biases of various differentiable graph representations."}}
{"id": "J246NSqR_l", "cdate": 1581705821855, "mdate": null, "content": {"title": "Representing Joint Hierarchies with Box Embeddings", "abstract": "Learning representations for hierarchical and multi-relational knowledge has emerged as an active area of research. Box Embeddings  [Vilnis et al., 2018, Li et al., 2019] represent concepts with hyperrectangles in $n$-dimensional space and are shown to be capable of modeling tree-like structures efficiently by training on a large subset of the transitive closure of the WordNet hypernym graph. In this work, we evaluate the capability of box embeddings to learn the transitive closure of a tree-like hierarchical relation graph with far fewer edges from the transitive closure. Box embeddings are not restricted to tree-like structures, however, and we demonstrate this by modeling the WordNet meronym graph, where nodes may have multiple parents. We further propose a method for modeling multiple relations jointly in a single embedding space using box embeddings. In all cases, our proposed method outperforms or is at par with all other embedding methods."}}
{"id": "iX9gJYdQBfZ", "cdate": 1577836800000, "mdate": null, "content": {"title": "Improving Local Identifiability in Probabilistic Box Embeddings", "abstract": "Geometric embeddings have recently received attention for their natural ability to represent transitive asymmetric relations via containment. Box embeddings, where objects are represented by n-dimensional hyperrectangles, are a particularly promising example of such an embedding as they are closed under intersection and their volume can be calculated easily, allowing them to naturally represent calibrated probability distributions. The benefits of geometric embeddings also introduce a problem of local identifiability, however, where whole neighborhoods of parameters result in equivalent loss which impedes learning. Prior work addressed some of these issues by using an approximation to Gaussian convolution over the box parameters, however, this intersection operation also increases the sparsity of the gradient. In this work, we model the box parameters with min and max Gumbel distributions, which were chosen such that space is still closed under the operation of the intersection. The calculation of the expected intersection volume involves all parameters, and we demonstrate experimentally that this drastically improves the ability of such models to learn."}}
{"id": "P5-ggsKvS3K", "cdate": 1577836800000, "mdate": null, "content": {"title": "Improving Local Identifiability in Probabilistic Box Embeddings", "abstract": "Geometric embeddings have recently received attention for their natural ability to represent transitive asymmetric relations via containment. Box embeddings, where objects are represented by n-dimensional hyperrectangles, are a particularly promising example of such an embedding as they are closed under intersection and their volume can be calculated easily, allowing them to naturally represent calibrated probability distributions. The benefits of geometric embeddings also introduce a problem of local identifiability, however, where whole neighborhoods of parameters result in equivalent loss which impedes learning. Prior work addressed some of these issues by using an approximation to Gaussian convolution over the box parameters, however this intersection operation also increases the sparsity of the gradient. In this work we model the box parameters with min and max Gumbel distributions, which were chosen such that the space is still closed under the operation of intersection. The calculation of the expected intersection volume involves all parameters, and we demonstrate experimentally that this drastically improves the ability of such models to learn."}}
