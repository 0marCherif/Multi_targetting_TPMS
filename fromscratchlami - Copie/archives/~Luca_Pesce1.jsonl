{"id": "kC0srzq0BXl", "cdate": 1672531200000, "mdate": 1695972551193, "content": {"title": "Learning Two-Layer Neural Networks, One (Giant) Step at a Time", "abstract": "We study the training dynamics of shallow neural networks, investigating the conditions under which a limited number of large batch gradient descent steps can facilitate feature learning beyond the kernel regime. We compare the influence of batch size and that of multiple (but finitely many) steps. Our analysis of a single-step process reveals that while a batch size of $n = O(d)$ enables feature learning, it is only adequate for learning a single direction, or a single-index model. In contrast, $n = O(d^2)$ is essential for learning multiple directions and specialization. Moreover, we demonstrate that ``hard'' directions, which lack the first $\\ell$ Hermite coefficients, remain unobserved and require a batch size of $n = O(d^\\ell)$ for being captured by gradient descent. Upon iterating a few steps, the scenario changes: a batch-size of $n = O(d)$ is enough to learn new target directions spanning the subspace linearly connected in the Hermite basis to the previously learned directions, thereby a staircase property. Our analysis utilizes a blend of techniques related to concentration, projection-based conditioning, and Gaussian equivalence that are of independent interest. By determining the conditions necessary for learning and specialization, our results highlight the interaction between batch size and number of iterations, and lead to a hierarchical depiction where learning performance exhibits a stairway to accuracy over time and batch size, shedding new light on feature learning in neural networks."}}
{"id": "c0GVdLnaNyl", "cdate": 1672531200000, "mdate": 1681656649084, "content": {"title": "Are Gaussian data all you need? Extents and limits of universality in high-dimensional generalized linear estimation", "abstract": "In this manuscript we consider the problem of generalized linear estimation on Gaussian mixture data with labels given by a single-index model. Our first result is a sharp asymptotic expression for the test and training errors in the high-dimensional regime. Motivated by the recent stream of results on the Gaussian universality of the test and training errors in generalized linear estimation, we ask ourselves the question: \"when is a single Gaussian enough to characterize the error?\". Our formula allow us to give sharp answers to this question, both in the positive and negative directions. More precisely, we show that the sufficient conditions for Gaussian universality (or lack of thereof) crucially depend on the alignment between the target weights and the means and covariances of the mixture clusters, which we precisely quantify. In the particular case of least-squares interpolation, we prove a strong universality property of the training error, and show it follows a simple, closed-form expression. Finally, we apply our results to real datasets, clarifying some recent discussion in the literature about Gaussian universality of the errors in this context."}}
{"id": "5MYe8LdIyem", "cdate": 1672531200000, "mdate": 1695972551179, "content": {"title": "Are Gaussian Data All You Need? The Extents and Limits of Universality in High-Dimensional Generalized Linear Estimation", "abstract": "In this manuscript we consider the problem of generalized linear estimation on Gaussian mixture data with labels given by a single-index model. Our first result is a sharp asymptotic expression for..."}}
{"id": "-QwiH3WSmT", "cdate": 1672531200000, "mdate": 1695972551193, "content": {"title": "Theory and applications of the Sum-Of-Squares technique", "abstract": "The Sum-of-Squares (SOS) approximation method is a technique used in optimization problems to derive lower bounds to the optimal value of an objective function. By representing the objective function as a sum of squares in a feature space, the SOS method transforms non-convex global optimization problems into solvable semidefinite programs. This note presents an overview of the SOS method. We start with its application in finite-dimensional feature spaces and, subsequently, we extend it to infinite-dimensional feature spaces using kernels (k-SOS). Additionally, we highlight the utilization of SOS for estimating some relevant quantities in information theory, including the log-partition function."}}
{"id": "hYx-xr1wdo", "cdate": 1652737411999, "mdate": null, "content": {"title": "Subspace clustering in high-dimensions: Phase transitions & Statistical-to-Computational gap", "abstract": "A simple model to study subspace clustering is the high-dimensional $k$-Gaussian mixture model where the cluster means are sparse vectors. Here we provide an exact asymptotic characterization of the statistically optimal reconstruction error in this model in the high-dimensional regime with extensive sparsity, i.e. when the fraction of non-zero components of the cluster means $\\rho$, as well as the ratio $\\alpha$ between the number of samples and the dimension are fixed, while the dimension diverges. We identify the information-theoretic threshold below which obtaining a positive correlation with the true cluster means is statistically impossible. Additionally, we investigate the performance of the approximate message passing (AMP) algorithm analyzed via its state evolution, which is conjectured to be optimal among polynomial algorithm for this task. We identify in particular the existence of a statistical-to-computational gap between the algorithm that requires a signal-to-noise ratio $\\lambda_{\\text{alg}} \\ge k  / \\sqrt{\\alpha}$ to perform better than random, and the information theoretic threshold at $\\lambda_{\\text{it}} \\approx \\sqrt{-k \\rho \\log{\\rho}}  / \\sqrt{\\alpha}$. Finally, we discuss the case of sub-extensive sparsity $\\rho$ by comparing the performance of the AMP with other sparsity-enhancing algorithms, such as sparse-PCA and diagonal thresholding."}}
{"id": "yoO8M9gKe28", "cdate": 1640995200000, "mdate": 1695972551266, "content": {"title": "Subspace clustering in high-dimensions: Phase transitions & Statistical-to-Computational gap", "abstract": "A simple model to study subspace clustering is the high-dimensional $k$-Gaussian mixture model where the cluster means are sparse vectors. Here we provide an exact asymptotic characterization of the statistically optimal reconstruction error in this model in the high-dimensional regime with extensive sparsity, i.e. when the fraction of non-zero components of the cluster means $\\rho$, as well as the ratio $\\alpha$ between the number of samples and the dimension are fixed, while the dimension diverges. We identify the information-theoretic threshold below which obtaining a positive correlation with the true cluster means is statistically impossible. Additionally, we investigate the performance of the approximate message passing (AMP) algorithm analyzed via its state evolution, which is conjectured to be optimal among polynomial algorithm for this task. We identify in particular the existence of a statistical-to-computational gap between the algorithm that requires a signal-to-noise ratio $\\lambda_{\\text{alg}} \\ge k / \\sqrt{\\alpha}$ to perform better than random, and the information theoretic threshold at $\\lambda_{\\text{it}} \\approx \\sqrt{-k \\rho \\log{\\rho}} / \\sqrt{\\alpha}$. Finally, we discuss the case of sub-extensive sparsity $\\rho$ by comparing the performance of the AMP with other sparsity-enhancing algorithms, such as sparse-PCA and diagonal thresholding."}}
{"id": "p-jDevjQzlo", "cdate": 1640995200000, "mdate": 1682319487479, "content": {"title": "Subspace clustering in high-dimensions: Phase transitions \\& Statistical-to-Computational gap", "abstract": "A simple model to study subspace clustering is the high-dimensional $k$-Gaussian mixture model where the cluster means are sparse vectors. Here we provide an exact asymptotic characterization of the statistically optimal reconstruction error in this model in the high-dimensional regime with extensive sparsity, i.e. when the fraction of non-zero components of the cluster means $\\rho$, as well as the ratio $\\alpha$ between the number of samples and the dimension are fixed, while the dimension diverges. We identify the information-theoretic threshold below which obtaining a positive correlation with the true cluster means is statistically impossible. Additionally, we investigate the performance of the approximate message passing (AMP) algorithm analyzed via its state evolution, which is conjectured to be optimal among polynomial algorithm for this task. We identify in particular the existence of a statistical-to-computational gap between the algorithm that require a signal-to-noise ratio $\\lambda_{\\text{alg}} \\ge k / \\sqrt{\\alpha} $ to perform better than random, and the information theoretic threshold at $\\lambda_{\\text{it}} \\approx \\sqrt{-k \\rho \\log{\\rho}} / \\sqrt{\\alpha}$. Finally, we discuss the case of sub-extensive sparsity $\\rho$ by comparing the performance of the AMP with other sparsity-enhancing algorithms, such as sparse-PCA and diagonal thresholding."}}
