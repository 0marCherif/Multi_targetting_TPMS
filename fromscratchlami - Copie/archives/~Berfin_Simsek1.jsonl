{"id": "iMW61RvSwS", "cdate": 1672531200000, "mdate": 1683890091363, "content": {"title": "Expand-and-Cluster: Exact Parameter Recovery of Neural Networks", "abstract": "Can we recover the hidden parameters of an Artificial Neural Network (ANN) by probing its input-output mapping? We propose a systematic method, called `Expand-and-Cluster' that needs only the number of hidden layers and the activation function of the probed ANN to identify all network parameters. In the expansion phase, we train a series of networks of increasing size using the probed data of the ANN as a teacher. Expansion stops when a minimal loss is consistently reached in networks of a given size. In the clustering phase, weight vectors of the expanded students are clustered, which allows structured pruning of superfluous neurons in a principled way. We find that an overparameterization of a factor four is sufficient to reliably identify the minimal number of neurons and to retrieve the original network parameters in $80\\%$ of tasks across a family of 150 toy problems of variable difficulty. Furthermore, shallow and deep teacher networks trained on MNIST data can be identified with less than $5\\%$ overhead in the neuron number. Thus, while direct training of a student network with a size identical to that of the teacher is practically impossible because of the highly non-convex loss function, training with mild overparameterization followed by clustering and structured pruning correctly identifies the target network."}}
{"id": "ePCOanQ2MdO", "cdate": 1672531200000, "mdate": 1683890091393, "content": {"title": "MLPGradientFlow: going with the flow of multilayer perceptrons (and finding minima fast and accurately)", "abstract": "MLPGradientFlow is a software package to solve numerically the gradient flow differential equation $\\dot \\theta = -\\nabla \\mathcal L(\\theta; \\mathcal D)$, where $\\theta$ are the parameters of a multi-layer perceptron, $\\mathcal D$ is some data set, and $\\nabla \\mathcal L$ is the gradient of a loss function. We show numerically that adaptive first- or higher-order integration methods based on Runge-Kutta schemes have better accuracy and convergence speed than gradient descent with the Adam optimizer. However, we find Newton's method and approximations like BFGS preferable to find fixed points (local and global minima of $\\mathcal L$) efficiently and accurately. For small networks and data sets, gradients are usually computed faster than in pytorch and Hessian are computed at least $5\\times$ faster. Additionally, the package features an integrator for a teacher-student setup with bias-free, two-layer networks trained with standard Gaussian input in the limit of infinite data. The code is accessible at https://github.com/jbrea/MLPGradientFlow.jl."}}
{"id": "MjikLUwiB3M", "cdate": 1663850580131, "mdate": null, "content": {"title": "Towards a Complete Theory of Neural Networks with Few Neurons", "abstract": "Deep learning has seen unprecedented progress thanks to the deployment of models with millions of parameters. \nOn the theoretical side, an immense amount of effort has gone to understanding the dynamics of overparameterized networks. \nAlthough now there is a well-developed theory of networks with infinitely many neurons, the classic problem of understanding how a neural network with a few neurons learns remains unsolved.\nTo attack this problem, we analytically study the landscapes of neural networks with few neurons. \nWe prove for the first time that a student network with one neuron has only one critical point --its global minimum-- when learning from a teacher network with arbitrarily many orthogonal neurons. \nIn addition, we prove how a neuron addition mechanism turns a minimum into a line of critical points with transitions from saddles to local minima via non-strict saddles. \nFinally, we discuss how the insights we get from our novel proof techniques may shed light on the dynamics of neural networks with few neurons."}}
{"id": "xjyyrCA3OL", "cdate": 1640995200000, "mdate": 1684415177324, "content": {"title": "Understanding out-of-distribution accuracies through quantifying difficulty of test samples", "abstract": "Existing works show that although modern neural networks achieve remarkable generalization performance on the in-distribution (ID) dataset, the accuracy drops significantly on the out-of-distribution (OOD) datasets \\cite{recht2018cifar, recht2019imagenet}. To understand why a variety of models consistently make more mistakes in the OOD datasets, we propose a new metric to quantify the difficulty of the test images (either ID or OOD) that depends on the interaction of the training dataset and the model. In particular, we introduce \\textit{confusion score} as a label-free measure of image difficulty which quantifies the amount of disagreement on a given test image based on the class conditional probabilities estimated by an ensemble of trained models. Using the confusion score, we investigate CIFAR-10 and its OOD derivatives. Next, by partitioning test and OOD datasets via their confusion scores, we predict the relationship between ID and OOD accuracies for various architectures. This allows us to obtain an estimator of the OOD accuracy of a given model only using ID test labels. Our observations indicate that the biggest contribution to the accuracy drop comes from images with high confusion scores. Upon further inspection, we report on the nature of the misclassified images grouped by their confusion scores: \\textit{(i)} images with high confusion scores contain \\textit{weak spurious correlations} that appear in multiple classes in the training data and lack clear \\textit{class-specific features}, and \\textit{(ii)} images with low confusion scores exhibit spurious correlations that belong to another class, namely \\textit{class-specific spurious correlations}."}}
{"id": "r6wZtPeQHg5", "cdate": 1609459200000, "mdate": 1645715553057, "content": {"title": "Geometry of the Loss Landscape in Overparameterized Neural Networks: Symmetries and Invariances", "abstract": "We study how permutation symmetries in overparameterized multi-layer neural networks generate `symmetry-induced' critical points. Assuming a network with $ L $ layers of minimal widths $ r_1^*, \\ldots, r_{L-1}^* $ reaches a zero-loss minimum at $ r_1^*! \\cdots r_{L-1}^*! $ isolated points that are permutations of one another, we show that adding one extra neuron to each layer is sufficient to connect all these previously discrete minima into a single manifold. For a two-layer overparameterized network of width $ r^*+ h =: m $ we explicitly describe the manifold of global minima: it consists of $ T(r^*, m) $ affine subspaces of dimension at least $ h $ that are connected to one another. For a network of width $m$, we identify the number $G(r,m)$ of affine subspaces containing only symmetry-induced critical points that are related to the critical points of a smaller network of width $r<r^*$. Via a combinatorial analysis, we derive closed-form formulas for $ T $ and $ G $ and show that the number of symmetry-induced critical subspaces dominates the number of affine subspaces forming the global minima manifold in the mildly overparameterized regime (small $ h $) and vice versa in the vastly overparameterized regime ($h \\gg r^*$). Our results provide new insights into the minimization of the non-convex loss function of overparameterized neural networks."}}
{"id": "_TGx40y_pUz", "cdate": 1609459200000, "mdate": 1633524375010, "content": {"title": "Geometry of the Loss Landscape in Overparameterized Neural Networks: Symmetries and Invariances", "abstract": "We study how permutation symmetries in overparameterized multi-layer neural networks generate \u2018symmetry-induced\u2019 critical points. Assuming a network with $ L $ layers of minimal widths $ r_1^*, \\ld..."}}
{"id": "BhV-Fvgmrlc", "cdate": 1609459200000, "mdate": 1645715553056, "content": {"title": "Deep Linear Networks Dynamics: Low-Rank Biases Induced by Initialization Scale and L2 Regularization", "abstract": "The dynamics of Deep Linear Networks (DLNs) is dramatically affected by the variance $\\sigma^2$ of the parameters at initialization $\\theta_0$. For DLNs of width $w$, we show a phase transition w.r.t. the scaling $\\gamma$ of the variance $\\sigma^2=w^{-\\gamma}$ as $w\\to\\infty$: for large variance ($\\gamma<1$), $\\theta_0$ is very close to a global minimum but far from any saddle point, and for small variance ($\\gamma>1$), $\\theta_0$ is close to a saddle point and far from any global minimum. While the first case corresponds to the well-studied NTK regime, the second case is less understood. This motivates the study of the case $\\gamma \\to +\\infty$, where we conjecture a Saddle-to-Saddle dynamics: throughout training, gradient descent visits the neighborhoods of a sequence of saddles, each corresponding to linear maps of increasing rank, until reaching a sparse global minimum. We support this conjecture with a theorem for the dynamics between the first two saddles, as well as some numerical experiments."}}
{"id": "p0Wy2pb4cOO", "cdate": 1577836800000, "mdate": 1633524375010, "content": {"title": "Implicit Regularization of Random Feature Models", "abstract": "Random Features (RF) models are used as efficient parametric approximations of kernel methods. We investigate, by means of random matrix theory, the connection between Gaussian RF models and Kernel..."}}
{"id": "j6Me_mdqNfw", "cdate": 1577836800000, "mdate": null, "content": {"title": "Kernel Alignment Risk Estimator: Risk Prediction from Training Data", "abstract": "We study the risk (i.e. generalization error) of Kernel Ridge Regression (KRR) for a kernel $K$ with ridge $\\lambda&gt;0$ and i.i.d. observations. For this, we introduce two objects: the Signal Capture Threshold (SCT) and the Kernel Alignment Risk Estimator (KARE). The SCT $\\vartheta_{K,\\lambda}$ is a function of the data distribution: it can be used to identify the components of the data that the KRR predictor captures, and to approximate the (expected) KRR risk. This then leads to a KRR risk approximation by the KARE $\\rho_{K, \\lambda}$, an explicit function of the training data, agnostic of the true data distribution. We phrase the regression problem in a functional setting. The key results then follow from a finite-size adaptation of the resolvent method for general Wishart random matrices. Under a natural universality assumption (that the KRR moments depend asymptotically on the first two moments of the observations) we capture the mean and variance of the KRR predictor. We numerically investigate our findings on the Higgs and MNIST datasets for various classical kernels: the KARE gives an excellent approximation of the risk. This supports our universality hypothesis. Using the KARE, one can compare choices of Kernels and hyperparameters directly from the training set. The KARE thus provides a promising data-dependent procedure to select Kernels that generalize well."}}
{"id": "HEeZtDemre9", "cdate": 1577836800000, "mdate": 1645715553054, "content": {"title": "Kernel Alignment Risk Estimator: Risk Prediction from Training Data", "abstract": "We study the risk (i.e. generalization error) of Kernel Ridge Regression (KRR) for a kernel $K$ with ridge $\\lambda>0$ and i.i.d. observations. For this, we introduce two objects: the Signal Capture Threshold (SCT) and the Kernel Alignment Risk Estimator (KARE). The SCT $\\vartheta_{K,\\lambda}$ is a function of the data distribution: it can be used to identify the components of the data that the KRR predictor captures, and to approximate the (expected) KRR risk. This then leads to a KRR risk approximation by the KARE $\\rho_{K, \\lambda}$, an explicit function of the training data, agnostic of the true data distribution. We phrase the regression problem in a functional setting. The key results then follow from a finite-size analysis of the Stieltjes transform of general Wishart random matrices. Under a natural universality assumption (that the KRR moments depend asymptotically on the first two moments of the observations) we capture the mean and variance of the KRR predictor. We numerically investigate our findings on the Higgs and MNIST datasets for various classical kernels: the KARE gives an excellent approximation of the risk, thus supporting our universality assumption. Using the KARE, one can compare choices of Kernels and hyperparameters directly from the training set. The KARE thus provides a promising data-dependent procedure to select Kernels that generalize well."}}
