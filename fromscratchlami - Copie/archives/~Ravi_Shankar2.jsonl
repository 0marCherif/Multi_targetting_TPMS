{"id": "fp3SnAbc15", "cdate": 1684340047314, "mdate": null, "content": {"title": "Adaptive Duration Modification of Speech using Masked Convolutional Networks and Open-Loop Time Warping", "abstract": "We propose a new method to adaptively modify the rhythm of a\ngiven speech signal. We train a masked convolutional encoder-\ndecoder network to generate this attention map via a stochastic\nversion of the mean absolute error loss function. Our model\nalso predicts the length of the target speech signal using the en-\ncoder embeddings, which determines the number of time steps\nfor the decoding operation. During testing, we use the learned\nattention map as a proxy for the frame-wise similarity matrix\nbetween the given input speech and an unknown target speech\nsignal. In an open-loop fashion, we compute a warping path\nfor rhythm modification. Our experiments demonstrate that this\nadaptive framework achieves similar performance as the fully\nsupervised dynamic time warping algorithm on both voice con-\nversion and emotion conversion tasks. We also show that the\nmodified speech utterances achieve high user quality ratings,\nthus highlighting the practical utility of our method."}}
{"id": "-9JdWFU3Ilf", "cdate": 1672531200000, "mdate": 1682526408663, "content": {"title": "A Diffeomorphic Flow-Based Variational Framework for Multi-Speaker Emotion Conversion", "abstract": "This paper introduces a new framework for non-parallel emotion conversion in speech. Our framework is based on two key contributions. First, we propose a stochastic version of the popular Cycle-GAN model. Our modified loss function introduces a Kullback\u2013Leibler (KL) divergence term that aligns the source and target <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">data distributions</i> learned by the generators, thus overcoming the limitations of sample-wise generation. By using a variational approximation to this stochastic loss function, we show that our KL divergence term can be implemented via a paired density discriminator. We term this new architecture a variational Cycle-GAN (VCGAN). Second, we model the prosodic features of target emotion as a smooth and learnable deformation of the source prosodic features. This approach provides implicit regularization that offers key advantages in terms of better range alignment to unseen and out-of-distribution speakers. We conduct rigorous experiments and comparative studies to demonstrate that our proposed framework is fairly robust with high performance against several state-of-the-art baselines."}}
{"id": "ENz_pSfvBc", "cdate": 1640995200000, "mdate": 1682526421837, "content": {"title": "A Comparative Study of Data Augmentation Techniques for Deep Learning Based Emotion Recognition", "abstract": "Automated emotion recognition in speech is a long-standing problem. While early work on emotion recognition relied on hand-crafted features and simple classifiers, the field has now embraced end-to-end feature learning and classification using deep neural networks. In parallel to these models, researchers have proposed several data augmentation techniques to increase the size and variability of existing labeled datasets. Despite many seminal contributions in the field, we still have a poor understanding of the interplay between the network architecture and the choice of data augmentation. Moreover, only a handful of studies demonstrate the generalizability of a particular model across multiple datasets, which is a prerequisite for robust real-world performance. In this paper, we conduct a comprehensive evaluation of popular deep learning approaches for emotion recognition. To eliminate bias, we fix the model architectures and optimization hyperparameters using the VESUS dataset and then use repeated 5-fold cross validation to evaluate the performance on the IEMOCAP and CREMA-D datasets. Our results demonstrate that long-range dependencies in the speech signal are critical for emotion recognition and that speed/rate augmentation offers the most robust performance gain across models."}}
{"id": "daYoG2O4TtU", "cdate": 1632875482198, "mdate": null, "content": {"title": "Adaptive Speech Duration Modification using a Deep-Generative Framework", "abstract": "We propose the first method to adaptively modify the duration of a given speechsignal.  Our approach uses a Bayesian framework to define a latent attention mapthat links frames of the input and target utterances. We train a masked convolu-tional encoder-decoder network to generate this attention map via a stochastic ver-sion of the mean absolute error loss function. Our model also predicts the lengthof the target speech signal using the encoder embeddings, which determines thenumber of time steps for the decoding operation. During testing, we generate theattention map as a proxy for the similarity matrix between the given input speechand an unknown target speech signal. Using this similarity matrix, we compute awarping path of alignment between the two signals. Our experiments demonstratethat this adaptive framework produces similar results to dynamic time warping,which relies on a known target signal, on both voice conversion and emotion con-version tasks. We also show that the modified speech utterances achieve high userquality ratings, thus highlighting the practical utility of our method. "}}
{"id": "VARmGQDsadb", "cdate": 1609459200000, "mdate": 1631641448503, "content": {"title": "A Deep-Bayesian Framework for Adaptive Speech Duration Modification", "abstract": "We propose the first method to adaptively modify the duration of a given speech signal. Our approach uses a Bayesian framework to define a latent attention map that links frames of the input and target utterances. We train a masked convolutional encoder-decoder network to produce this attention map via a stochastic version of the mean absolute error loss function; our model also predicts the length of the target speech signal using the encoder embeddings. The predicted length determines the number of steps for the decoder operation. During inference, we generate the attention map as a proxy for the similarity matrix between the given input speech and an unknown target speech signal. Using this similarity matrix, we compute a warping path of alignment between the two signals. Our experiments demonstrate that this adaptive framework produces similar results to dynamic time warping, which relies on a known target signal, on both voice conversion and emotion conversion tasks. We also show that our technique results in a high quality of generated speech that is on par with state-of-the-art vocoders."}}
{"id": "vBL_TrQMqE-", "cdate": 1577836800000, "mdate": 1631641448500, "content": {"title": "Multi-Speaker Emotion Conversion via Latent Variable Regularization and a Chained Encoder-Decoder-Predictor Network", "abstract": "We propose a novel method for emotion conversion in speech based on a chained encoder-decoder-predictor neural network architecture. The encoder constructs a latent embedding of the fundamental frequency (F0) contour and the spectrum, which we regularize using the Large Diffeomorphic Metric Mapping (LDDMM) registration framework. The decoder uses this embedding to predict the modified F0 contour in a target emotional class. Finally, the predictor uses the original spectrum and the modified F0 contour to generate a corresponding target spectrum. Our joint objective function simultaneously optimizes the parameters of three model blocks. We show that our method outperforms the existing state-of-the-art approaches on both, the saliency of emotion conversion and the quality of resynthesized speech. In addition, the LDDMM regularization allows our model to convert phrases that were not present in training, thus providing evidence for out-of-sample generalization."}}
{"id": "OFTY2XDpVrB", "cdate": 1577836800000, "mdate": 1631641448489, "content": {"title": "Non-Parallel Emotion Conversion Using a Deep-Generative Hybrid Network and an Adversarial Pair Discriminator", "abstract": "We introduce a novel method for emotion conversion in speech that does not require parallel training data. Our approach loosely relies on a cycle-GAN schema to minimize the reconstruction error from converting back and forth between emotion pairs. However, unlike the conventional cycle-GAN, our discriminator classifies whether a pair of input real and generated samples corresponds to the desired emotion conversion (e.g., A\u2192B) or to its inverse (B\u2192A). We will show that this setup, which we refer to as a variational cycle-GAN (VCGAN), is equivalent to minimizing the empirical KL divergence between the source features and their cyclic counterpart. In addition, our generator combines a trainable deep network with a fixed generative block to implement a smooth and invertible transformation on the input features, in our case, the fundamental frequency (F0) contour. This hybrid architecture regularizes our adversarial training procedure. We use crowd sourcing to evaluate both the emotional saliency and the quality of synthesized speech. Finally, we show that our model generalizes to new speakers by modifying speech produced by Wavenet."}}
{"id": "iQ2tTyEFRIB", "cdate": 1546300800000, "mdate": 1682526450362, "content": {"title": "VESUS: A Crowd-Annotated Database to Study Emotion Production and Perception in Spoken English", "abstract": "We introduce the Varied Emotion in Syntactically Uniform Speech (VESUS) repository as a new resource for the speech community. VESUS is a lexically controlled database, in which a semantically neutral script is portrayed with different emotional inflections. In total, VESUS contains over 250 distinct phrases, each read by ten actors in five emotional states. We use crowd sourcing to obtain ten human ratings for the perceived emotional content of each utterance. Our unique database construction enables a multitude of scientific and technical explorations. To jumpstart this effort, we provide benchmark performance on three distinct emotion recognition tasks using VESUS: longitudinal speaker analysis, extrapolating across syntactical complexity, and generalization to a new speaker."}}
