{"id": "c0t0x2bDO-", "cdate": 1680307200000, "mdate": 1680009366684, "content": {"title": "Design and Performance Analysis of Partial Computation Output Schemes for Accelerating Coded Machine Learning", "abstract": ""}}
{"id": "10_QV3J-o6V", "cdate": 1672531200000, "mdate": 1693799325201, "content": {"title": "SeedGNN: Graph Neural Network for Supervised Seeded Graph Matching", "abstract": "There is a growing interest in designing Graph Neural Networks (GNNs) for seeded graph matching, which aims to match two unlabeled graphs using only topological information and a small set of seed ..."}}
{"id": "iYvbPx8GTta", "cdate": 1663850133806, "mdate": null, "content": {"title": "SeedGNN: Graph Neural Network for Supervised Seeded Graph Matching", "abstract": "There have been significant interests in designing Graph Neural Networks (GNNs) for seeded graph matching, which aims to match two (unlabeled) graphs using only topological information and a small set of seeds. However, most previous GNNs for seeded graph matching employ a semi-supervised approach, which requires a large number of seeds and can not learn knowledge transferable to unseen graphs. In contrast, this paper proposes a new supervised approach that can learn from a training set how to match unseen graphs with only a few seeds. At the core of our SeedGNN architecture are two novel modules: 1) a convolution module that can easily learn the capability of counting and using witnesses of different hops; 2) a percolation module that can use easily-matched pairs as new seeds to percolate and match other nodes. We evaluate SeedGNN on both synthetic and real graphs, and demonstrate significant performance improvement over both non-learning and learning algorithms in the existing literature. Further, our experiments confirm that the knowledge learned by SeedGNN from training graphs can be generalized to test graphs with different sizes and categories. "}}
{"id": "IE32oIlhXz", "cdate": 1652737577566, "mdate": null, "content": {"title": "On the Generalization Power of the Overfitted Three-Layer Neural Tangent Kernel Model", "abstract": "In this paper, we study the generalization performance of overparameterized 3-layer NTK models. We show that, for a specific set of ground-truth functions (which we refer to as the \"learnable set\"), the test error of the overfitted 3-layer NTK is upper bounded by an expression that decreases with the number of neurons of the two hidden layers. Different from 2-layer NTK where there exists only one hidden-layer, the 3-layer NTK involves interactions between two hidden-layers. Our upper bound reveals that, between the two hidden-layers, the test error descends faster with respect to the number of neurons in the second hidden-layer (the one closer to the output) than with respect to that in the first hidden-layer (the one closer to the input). We also show that the learnable set of 3-layer NTK without bias is no smaller than that of 2-layer NTK models with various choices of bias in the neurons. However, in terms of the actual generalization performance, our results suggest that 3-layer NTK is much less sensitive to the choices of bias than 2-layer NTK, especially when the input dimension is large."}}
{"id": "VfYacdhii9X", "cdate": 1640995200000, "mdate": 1680009366755, "content": {"title": "Integrated Online Learning and Adaptive Control in Queueing Systems with Uncertain Payoffs", "abstract": ""}}
{"id": "SHeqZ7qSYIF", "cdate": 1640995200000, "mdate": 1680009366690, "content": {"title": "A Case for Sampling-Based Learning Techniques in Coflow Scheduling", "abstract": ""}}
{"id": "O1bB-LQSSGf", "cdate": 1640995200000, "mdate": 1680009366680, "content": {"title": "Power-of-2-arms for bandit learning with switching costs", "abstract": ""}}
{"id": "4616I_ZMLq", "cdate": 1640995200000, "mdate": 1693799325218, "content": {"title": "On the Generalization Power of the Overfitted Three-Layer Neural Tangent Kernel Model", "abstract": "In this paper, we study the generalization performance of overparameterized 3-layer NTK models. We show that, for a specific set of ground-truth functions (which we refer to as the \"learnable set\"), the test error of the overfitted 3-layer NTK is upper bounded by an expression that decreases with the number of neurons of the two hidden layers. Different from 2-layer NTK where there exists only one hidden-layer, the 3-layer NTK involves interactions between two hidden-layers. Our upper bound reveals that, between the two hidden-layers, the test error descends faster with respect to the number of neurons in the second hidden-layer (the one closer to the output) than with respect to that in the first hidden-layer (the one closer to the input). We also show that the learnable set of 3-layer NTK without bias is no smaller than that of 2-layer NTK models with various choices of bias in the neurons. However, in terms of the actual generalization performance, our results suggest that 3-layer NTK is much less sensitive to the choices of bias than 2-layer NTK, especially when the input dimension is large."}}
{"id": "3ihZwzWqrp", "cdate": 1640995200000, "mdate": 1693799325313, "content": {"title": "A Case for Task Sampling based Learning for Cluster Job Scheduling", "abstract": ""}}
{"id": "XVSHLDyufMX", "cdate": 1609459200000, "mdate": 1680009366754, "content": {"title": "Competitive Online Convex Optimization With Switching Costs and Ramp Constraints", "abstract": ""}}
