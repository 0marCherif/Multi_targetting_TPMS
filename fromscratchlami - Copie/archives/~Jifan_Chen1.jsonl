{"id": "wZf4BGhobB", "cdate": 1640995200000, "mdate": 1666372696673, "content": {"title": "Generating Literal and Implied Subquestions to Fact-check Complex Claims", "abstract": "Verifying complex political claims is a challenging task, especially when politicians use various tactics to subtly misrepresent the facts. Automatic fact-checking systems fall short here, and their predictions like \"half-true\" are not very useful in isolation, since we have no idea which parts of the claim are true and which are not. In this work, we focus on decomposing a complex claim into a comprehensive set of yes-no subquestions whose answers influence the veracity of the claim. We present ClaimDecomp, a dataset of decompositions for over 1000 claims. Given a claim and its verification paragraph written by fact-checkers, our trained annotators write subquestions covering both explicit propositions of the original claim and its implicit facets, such as asking about additional political context that changes our view of the claim's veracity. We study whether state-of-the-art models can generate such subquestions, showing that these models generate reasonable questions to ask, but predicting the comprehensive set of subquestions from the original claim without evidence remains challenging. We further show that these subquestions can help identify relevant evidence to fact-check the full claim and derive the veracity through their answers, suggesting that they can be useful pieces of a fact-checking pipeline."}}
{"id": "cBkk08Aayb", "cdate": 1640995200000, "mdate": 1681930324544, "content": {"title": "Improving Cross-task Generalization of Unified Table-to-text Models with Compositional Task Configurations", "abstract": "There has been great progress in unifying various table-to-text tasks using a single encoder-decoder model trained via multi-task learning (Xie et al., 2022). However, existing methods typically encode task information with a simple dataset name as a prefix to the encoder. This not only limits the effectiveness of multi-task learning, but also hinders the model's ability to generalize to new domains or tasks that were not seen during training, which is crucial for real-world applications. In this paper, we propose compositional task configurations, a set of prompts prepended to the encoder to improve cross-task generalization of unified models. We design the task configurations to explicitly specify the task type, as well as its input and output types. We show that this not only allows the model to better learn shared knowledge across different tasks at training, but also allows us to control the model by composing new configurations that apply novel input-output combinations in a zero-shot manner. We demonstrate via experiments over ten table-to-text tasks that our method outperforms the UnifiedSKG baseline by noticeable margins in both in-domain and zero-shot settings, with average improvements of +0.5 and +12.6 from using a T5-large backbone, respectively."}}
{"id": "GdA90dVIaft", "cdate": 1640995200000, "mdate": 1666372696618, "content": {"title": "longhorns at DADC 2022: How many linguists does it take to fool a Question Answering model? A systematic approach to adversarial attacks", "abstract": "Developing methods to adversarially challenge NLP systems is a promising avenue for improving both model performance and interpretability. Here, we describe the approach of the team \"longhorns\" on Task 1 of the The First Workshop on Dynamic Adversarial Data Collection (DADC), which asked teams to manually fool a model on an Extractive Question Answering task. Our team finished first, with a model error rate of 62%. We advocate for a systematic, linguistically informed approach to formulating adversarial questions, and we describe the results of our pilot experiments, as well as our official submission."}}
{"id": "uIg2HhjGxba", "cdate": 1609459200000, "mdate": 1681502594029, "content": {"title": "Robust Question Answering Through Sub-part Alignment", "abstract": ""}}
{"id": "FQMnAdn_Zn", "cdate": 1609459200000, "mdate": 1681502594058, "content": {"title": "Can NLI Models Verify QA Systems' Predictions?", "abstract": ""}}
{"id": "ZMfoLsvmaxa", "cdate": 1546300800000, "mdate": null, "content": {"title": "Implicit discourse relation detection using concatenated word embeddings and a gated relevance network", "abstract": ""}}
{"id": "Skbi_X-ObS", "cdate": 1546300800000, "mdate": null, "content": {"title": "Understanding Dataset Design Choices for Multi-hop Reasoning", "abstract": "Jifan Chen, Greg Durrett. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019."}}
{"id": "AWweV0I77DO", "cdate": 1546300800000, "mdate": null, "content": {"title": "Multi-hop Question Answering via Reasoning Chains", "abstract": "Multi-hop question answering requires models to gather information from different parts of a text to answer a question. Most current approaches learn to address this task in an end-to-end way with neural networks, without maintaining an explicit representation of the reasoning process. We propose a method to extract a discrete reasoning chain over the text, which consists of a series of sentences leading to the answer. We then feed the extracted chains to a BERT-based QA model to do final answer prediction. Critically, we do not rely on gold annotated chains or \"supporting facts:\" at training time, we derive pseudogold reasoning chains using heuristics based on named entity recognition and coreference resolution. Nor do we rely on these annotations at test time, as our model learns to extract chains from raw text alone. We test our approach on two recently proposed large multi-hop question answering datasets: WikiHop and HotpotQA, and achieve state-of-art performance on WikiHop and strong performance on HotpotQA. Our analysis shows the properties of chains that are crucial for high performance: in particular, modeling extraction sequentially is important, as is dealing with each candidate sentence in a context-aware way. Furthermore, human evaluation shows that our extracted chains allow humans to give answers with high confidence, indicating that these are a strong intermediate abstraction for this task."}}
{"id": "B1lf43A5Y7", "cdate": 1538087977602, "mdate": null, "content": {"title": "How to learn (and how not to learn) multi-hop reasoning with memory networks", "abstract": "Answering questions about a text frequently requires aggregating information from multiple places in that text. End-to-end neural network models, the dominant approach in the current literature, can theoretically learn how to distill and manipulate representations of the text without explicit supervision about how to do so. We investigate a canonical architecture for this task, the memory network, and analyze how effective it really is in the context of three multi-hop reasoning settings. In a simple synthetic setting, the path-finding task of the bAbI dataset, the model fails to learn the correct reasoning without additional supervision of its attention mechanism. However, with this supervision, it can perform well. On a real text dataset, WikiHop, the memory network gives nearly state-of-the-art performance, but does so without using its multi-hop capabilities. A tougher anonymized version of the WikiHop dataset is qualitatively similar to bAbI: the model fails to perform well unless it has additional supervision. We hypothesize that many \"multi-hop\" architectures do not truly learn this reasoning as advertised, though they could learn this reasoning if appropriately supervised."}}
{"id": "vSYh_MeeB3A", "cdate": 1451606400000, "mdate": null, "content": {"title": "Bidirectional Long Short-Term Memory with Gated Relevance Network for Paraphrase Identification", "abstract": "Semantic interaction between text segments, which has been proven to be very useful for detecting the paraphrase relations, is often ignored in the study of paraphrase identification. In this paper, we adopt a neural network model for paraphrase identification, called as bidirectional Long Short-Term Memory-Gated Relevance Network (Bi-LSTM+GRN). According to this model, a gated relevance network is used to capture the semantic interaction between text segments, and then aggregated using a pooling layer to select the most informative interactions. Experiments on the Microsoft Research Paraphrase Corpus (MSRP) benchmark dataset show that this model achieves better performances than hand-crafted feature based approaches as well as previous neural network models."}}
