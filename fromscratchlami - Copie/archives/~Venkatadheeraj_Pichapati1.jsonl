{"id": "r1Z4d3b_ZB", "cdate": 1514764800000, "mdate": null, "content": {"title": "The Limits of Maxing, Ranking, and Preference Learning", "abstract": "We present a comprehensive understanding of three important problems in PAC preference learning: maximum selection (maxing), ranking, and estimating <em>all</em> pairwise preference probabilities, ..."}}
{"id": "B1NgguZ_-S", "cdate": 1514764800000, "mdate": null, "content": {"title": "On Learning Markov Chains", "abstract": "The problem of estimating an unknown discrete distribution from its samples is a fundamental tenet of statistical learning. Over the past decade, it attracted significant research effort and has been solved for a variety of divergence measures. Surprisingly, an equally important problem, estimating an unknown Markov chain from its samples, is still far from understood. We consider two problems related to the min-max risk (expected loss) of estimating an unknown k-state Markov chain from its n sequential samples: predicting the conditional distribution of the next sample with respect to the KL-divergence, and estimating the transition matrix with respect to a natural loss induced by KL or a more general f-divergence measure. For the first measure, we determine the min-max prediction risk to within a linear factor in the alphabet size, showing it is \\Omega(k\\log\\log n/n) and O(k^2\\log\\log n/n). For the second, if the transition probabilities can be arbitrarily small, then only trivial uniform risk upper bounds can be derived. We therefore consider transition probabilities that are bounded away from zero, and resolve the problem for essentially all sufficiently smooth f-divergences, including KL-, L_2-, Chi-squared, Hellinger, and Alpha-divergences."}}
{"id": "Syb7h_W_WH", "cdate": 1483228800000, "mdate": null, "content": {"title": "The power of absolute discounting: all-dimensional distribution estimation", "abstract": "Categorical models are a natural fit for many problems. When learning the distribution of categories from samples, high-dimensionality may dilute the data. Minimax optimality is too pessimistic to remedy this issue. A serendipitously discovered estimator, absolute discounting, corrects empirical frequencies by subtracting a constant from observed categories, which it then redistributes among the unobserved. It outperforms classical estimators empirically, and has been used extensively in natural language modeling. In this paper, we rigorously explain the prowess of this estimator using less pessimistic notions. We show that (1) absolute discounting recovers classical minimax KL-risk rates, (2) it is \\emph{adaptive} to an effective dimension rather than the true dimension, (3) it is strongly related to the Good-Turing estimator and inherits its \\emph{competitive} properties. We use power-law distributions as the cornerstone of these results. We validate the theory via synthetic data and an application to the Global Terrorism Database."}}
{"id": "SJbOkwZdbr", "cdate": 1483228800000, "mdate": null, "content": {"title": "Maxing and Ranking with Few Assumptions", "abstract": "PAC maximum selection (maxing) and ranking of $n$ elements via random pairwise comparisons have diverse applications and have been studied under many models and assumptions. With just one simple natural assumption: strong stochastic transitivity, we show that maxing can be performed with linearly many comparisons yet ranking requires quadratically many. With no assumptions at all, we show that for the Borda-score metric, maximum selection can be performed with linearly many comparisons and ranking can be performed with $\\mathcal{O}(n\\log n)$ comparisons."}}
{"id": "SJ4yz3-d-H", "cdate": 1483228800000, "mdate": null, "content": {"title": "Maximum Selection and Ranking under Noisy Comparisons", "abstract": "We consider $(\\epsilon,\\delta)$-PAC maximum-selection and ranking using pairwise comparisons for general probabilistic models whose comparison probabilities satisfy strong stochastic transitivity a..."}}
