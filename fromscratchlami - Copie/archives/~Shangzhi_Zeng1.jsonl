{"id": "LfInhzyRVof", "cdate": 1683616647310, "mdate": 1683616647310, "content": {"title": "Averaged Method of Multipliers for Bi-Level Optimization without Lower-Level Strong Convexity", "abstract": "Gradient methods have become mainstream techniques for Bi-Level Optimization (BLO) in learning fields. The validity of existing works heavily rely on either a restrictive Lower- Level Strong Convexity (LLSC) condition or on solving a series of approximation subproblems with high accuracy or both. In this work, by averaging the upper and lower level objectives, we propose a single loop Bi-level Averaged Method of Multipliers (sl-BAMM) for BLO that is simple yet efficient for large-scale BLO and gets rid of the limited LLSC restriction. We further provide non-asymptotic convergence analysis of sl-BAMM towards KKT stationary points, and the comparative advantage of our analysis lies in the absence of strong gradient boundedness assumption, which is always required by others. Thus our theory safely captures a wider variety of applications in deep learning, especially where the upper-level objective is quadratic w.r.t. the lower-level variable. Experimental results demonstrate the superiority of our method."}}
{"id": "SVCdZE2YoJD", "cdate": 1680307200000, "mdate": 1682318607361, "content": {"title": "Difference of convex algorithms for bilevel programs with applications in hyperparameter selection", "abstract": ""}}
{"id": "AIoCnYEIhRV", "cdate": 1677628800000, "mdate": 1682318607452, "content": {"title": "A globally convergent proximal Newton-type method in nonsmooth convex optimization", "abstract": ""}}
{"id": "y0eHy-BXOd", "cdate": 1672531200000, "mdate": 1681626433140, "content": {"title": "Hierarchical Optimization-Derived Learning", "abstract": ""}}
{"id": "BqxPnscFO_", "cdate": 1672531200000, "mdate": 1681626432819, "content": {"title": "A General Descent Aggregation Framework for Gradient-Based Bi-Level Optimization", "abstract": ""}}
{"id": "0aVTVTkeK2v", "cdate": 1672531200000, "mdate": 1681626433140, "content": {"title": "Averaged Method of Multipliers for Bi-Level Optimization without Lower-Level Strong Convexity", "abstract": ""}}
{"id": "v9Dp_eZhJDX", "cdate": 1640995200000, "mdate": 1681626434118, "content": {"title": "Optimization-Derived Learning with Essential Convergence Analysis of Training and Hyper-training", "abstract": ""}}
{"id": "lgnWdx_y2av", "cdate": 1640995200000, "mdate": 1682318607478, "content": {"title": "Value Function based Difference-of-Convex Algorithm for Bilevel Hyperparameter Selection Problems", "abstract": "Existing gradient-based optimization methods for hyperparameter tuning can only guarantee theoretical convergence to stationary solutions when the bilevel program satisfies the condition that for f..."}}
{"id": "VBs4YVDBt7U", "cdate": 1640995200000, "mdate": 1681626438515, "content": {"title": "Towards Extremely Fast Bilevel Optimization with Self-governed Convergence Guarantees", "abstract": ""}}
{"id": "AMi2gBrMXLG", "cdate": 1640995200000, "mdate": 1681626433459, "content": {"title": "Optimization-Derived Learning with Essential Convergence Analysis of Training and Hyper-training", "abstract": ""}}
