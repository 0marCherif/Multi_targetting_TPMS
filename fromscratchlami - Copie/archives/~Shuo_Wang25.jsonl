{"id": "rrBGSoDHty", "cdate": 1680307200000, "mdate": 1683887573905, "content": {"title": "Defeating Misclassification Attacks Against Transfer Learning", "abstract": "Transfer learning is prevalent as a technique to efficiently generate new models (Student models) based on the knowledge transferred from a pre-trained model (Teacher model). However, Teacher models are often publicly available for sharing and reuse, which inevitably introduces vulnerability to trigger severe attacks against transfer learning systems. In this article, we take a first step towards mitigating one of the most advanced misclassification attacks in transfer learning. We design a distilled <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">differentiator</i> via activation-based network pruning to enervate the attack transferability while retaining accuracy. We adopt an ensemble structure from variant differentiators to improve the defence robustness. To avoid the bloated ensemble size during inference, we propose a two-phase defence, in which inference from the Student model is first performed to narrow down the candidate differentiators to be assembled, and later only a small, fixed number of them can be chosen to validate clean or reject adversarial inputs effectively. Our comprehensive evaluations on both large and small image recognition tasks confirm that the Student models with our defence of only 5 differentiators are immune to over 90% of the adversarial inputs with an accuracy loss of less than 10%. Our comparison also demonstrates that our design outperforms prior problematic defences."}}
{"id": "ySZ7iLbmKTt", "cdate": 1672531200000, "mdate": 1681263355228, "content": {"title": "DOITRUST: Dissecting On-chain Compromised Internet Domains via Graph Learning", "abstract": ""}}
{"id": "CBoqxGlLeD", "cdate": 1672531200000, "mdate": 1681263355295, "content": {"title": "Not Seen, Not Heard in the Digital World! Measuring Privacy Practices in Children's Apps", "abstract": ""}}
{"id": "0HrNffr4j6z", "cdate": 1672531200000, "mdate": 1683887573900, "content": {"title": "Not Seen, Not Heard in the Digital World! Measuring Privacy Practices in Children's Apps", "abstract": "The digital age has brought a world of opportunity to children. Connectivity can be a game-changer for some of the world\u2019s most marginalized children. However, while legislatures around the world have enacted regulations to protect children\u2019s online privacy, and app stores have instituted various protections, privacy in mobile apps remains a growing concern for parents and wider society. In this paper, we explore the potential privacy issues and threats that exist in these apps. We investigate 20195\u00a0mobile apps from the Google Play store that are designed particularly for children (Family apps) or include children in their target user groups (Normal apps). Using both static and dynamic analysis, we find that 4.47%\u00a0of Family apps request location permissions, even though collecting location information from children is forbidden by the Play store, and 81.25%\u00a0of Family apps use trackers (which are not allowed in children\u2019s apps). Even major developers with 40+ kids apps on the Play store use ad trackers. Furthermore, we find that most permission request notifications are not well designed for children, and 19.25%\u00a0apps have inconsistent content age ratings across the different protection authorities. Our findings suggest that, despite significant attention to children\u2019s privacy, a large gap between regulatory provisions, app store policies, and actual development practices exist. Our research sheds light for government policymakers, app stores, and developers."}}
{"id": "gv9bgHtVNqD", "cdate": 1640995200000, "mdate": 1683887574004, "content": {"title": "Backdoor Attacks Against Transfer Learning With Pre-Trained Deep Learning Models", "abstract": "Transfer learning provides an effective solution for feasibly and fast customize accurate <i>Student</i> models, by transferring the learned knowledge of pre-trained <i>Teacher</i> models over large datasets via fine-tuning. Many pre-trained Teacher models used in transfer learning are publicly available and maintained by public platforms, increasing their vulnerability to backdoor attacks. In this article, we demonstrate a backdoor threat to transfer learning tasks on both image and time-series data leveraging the knowledge of publicly accessible Teacher models, aimed at defeating three commonly adopted defenses: <i>pruning-based</i> , <i>retraining-based</i> and <i>input pre-processing-based defenses</i> . Specifically, ( <inline-formula><tex-math notation=\"LaTeX\">$\\mathcal {A}$</tex-math></inline-formula> ) ranking-based selection mechanism to speed up the backdoor trigger generation and perturbation process while defeating <i>pruning-based</i> and/or <i>retraining-based defenses</i> . ( <inline-formula><tex-math notation=\"LaTeX\">$\\mathcal {B}$</tex-math></inline-formula> ) autoencoder-powered trigger generation is proposed to produce a robust trigger that can defeat the <i>input pre-processing-based defense</i> , while guaranteeing that selected neuron(s) can be significantly activated. ( <inline-formula><tex-math notation=\"LaTeX\">$\\mathcal {C}$</tex-math></inline-formula> ) defense-aware retraining to generate the manipulated model using reverse-engineered model inputs. We launch effective misclassification attacks on Student models over real-world images, brain Magnetic Resonance Imaging (MRI) data and Electrocardiography (ECG) learning systems. The experiments reveal that our enhanced attack can maintain the 98.4 and 97.2 percent classification accuracy as the genuine model on clean image and time series inputs while improving <inline-formula><tex-math notation=\"LaTeX\">$27.9\\%-100\\%$</tex-math></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">$27.1\\%-56.1\\%$</tex-math></inline-formula> attack success rate on trojaned image and time series inputs respectively in the presence of pruning-based and/or retraining-based defenses."}}
{"id": "fOsgTzwwYlg", "cdate": 1640995200000, "mdate": 1683887573929, "content": {"title": "Unraveling Threat Intelligence Through the Lens of Malicious URL Campaigns", "abstract": "The daily deluge of alerts is a sombre reality for Security Operations Centre (SOC) personnel worldwide. They are at the forefront of an organisation's cybersecurity infrastructure, and face the unenviable task of prioritising threats amongst a flood of abstruse alerts triggered by their Security Information and Event Management (SIEM) systems. URLs found within malicious communications form the bulk of such alerts, and pinpointing pertinent patterns within them allows teams to rapidly deescalate potential or extant threats. This need for vigilance has been traditionally filled with machine-learning based log analysis tools and anomaly detection concepts. To sidestep machine learning approaches, we instead propose to analyse suspicious URLs from SIEM alerts via the perspective of malicious URL campaigns. By first grouping URLs within 311M records gathered from VirusTotal into 2.6M suspicious clusters, we thereafter discovered 77.8K malicious campaigns. Corroborating our suspicions, we found 9.9M unique URLs attributable to 18.3K multi-URL campaigns, and that worryingly, only 2.97% of campaigns were found by security vendors. We also confer insights on evasive tactics such as ever lengthier URLs and more diverse domain names, with selected case studies exposing other adversarial techniques. By characterising the concerted campaigns driving these URL alerts, we hope to inform SOC teams of current threat trends, and thus arm them with better threat intelligence."}}
{"id": "bZB2ZAbXfHh", "cdate": 1640995200000, "mdate": 1683887573891, "content": {"title": "Similarity-based Gray-box Adversarial Attack Against Deep Face Recognition", "abstract": "The majority of adversarial attack techniques perform well against deep face recognition when the full knowledge of the system is revealed (\\emph{white-box}). However, such techniques act unsuccessfully in the gray-box setting where the face templates are unknown to the attackers. In this work, we propose a similarity-based gray-box adversarial attack (SGADV) technique with a newly developed objective function. SGADV utilizes the dissimilarity score to produce the optimized adversarial example, i.e., similarity-based adversarial attack. This technique applies to both white-box and gray-box attacks against authentication systems that determine genuine or imposter users using the dissimilarity score. To validate the effectiveness of SGADV, we conduct extensive experiments on face datasets of LFW, CelebA, and CelebA-HQ against deep face recognition models of FaceNet and InsightFace in both white-box and gray-box settings. The results suggest that the proposed method significantly outperforms the existing adversarial attack techniques in the gray-box setting. We hence summarize that the similarity-base approaches to develop the adversarial example could satisfactorily cater to the gray-box attack scenarios for de-authentication."}}
{"id": "VC8Z_WCMPmX", "cdate": 1640995200000, "mdate": 1683887574014, "content": {"title": "Towards Web Phishing Detection Limitations and Mitigation", "abstract": "Web phishing remains a serious cyber threat responsible for most data breaches. Machine Learning (ML)-based anti-phishing detectors are seen as an effective countermeasure, and are increasingly adopted by web-browsers and software products. However, with an average of 10K phishing links reported per hour to platforms such as PhishTank and VirusTotal (VT), the deficiencies of such ML-based solutions are laid bare. We first explore how phishing sites bypass ML-based detection with a deep dive into 13K phishing pages targeting major brands such as Facebook. Results show successful evasion is caused by: (1) use of benign services to obscure phishing URLs; (2) high similarity between the HTML structures of phishing and benign pages; (3) hiding the ultimate phishing content within Javascript and running such scripts only on the client; (4) looking beyond typical credentials and credit cards for new content such as IDs and documents; (5) hiding phishing content until after human interaction. We attribute the root cause to the dependency of ML-based models on the vertical feature space (webpage content). These solutions rely only on what phishers present within the page itself. Thus, we propose Anti-SubtlePhish, a more resilient model based on logistic regression. The key augmentation is the inclusion of a horizontal feature space, which examines correlation variables between the final render of suspicious pages against what trusted services have recorded (e.g., PageRank). To defeat (1) and (2), we correlate information between WHOIS, PageRank, and page analytics. To combat (3), (4) and (5), we correlate features after rendering the page. Experiments with 100K phishing/benign sites show promising accuracy (98.8%). We also obtained 100% accuracy against 0-day phishing pages that were manually crafted, comparing well to the 0% recorded by VT vendors over the first four days."}}
{"id": "UaGgkt2EuQ", "cdate": 1640995200000, "mdate": 1683887573895, "content": {"title": "Reconstruction Attack on Differential Private Trajectory Protection Mechanisms", "abstract": "Location trajectories collected by smartphones and other devices represent a valuable data source for applications such as location-based services. Likewise, trajectories have the potential to reveal sensitive information about individuals, e.g., religious beliefs or sexual orientations. Accordingly, trajectory datasets require appropriate sanitization. Due to their strong theoretical privacy guarantees, differential private publication mechanisms receive much attention. However, the large amount of noise required to achieve differential privacy yields structural differences, e.g., ship trajectories passing over land. We propose a deep learning-based Reconstruction Attack on Protected Trajectories (RAoPT), that leverages the mentioned differences to partly reconstruct the original trajectory from a differential private release. The evaluation shows that our RAoPT model can reduce the Euclidean and Hausdorff distances between the released and original trajectories by over 68% on two real-world datasets under protection with $\\varepsilon \\leq 1$. In this setting, the attack increases the average Jaccard index of the trajectories' convex hulls, representing a user's activity space, by over 180%. Trained on the GeoLife dataset, the model still reduces the Euclidean and Hausdorff distances by over 60% for T-Drive trajectories protected with a state-of-the-art mechanism ($\\varepsilon = 0.1$). This work highlights shortcomings of current trajectory publication mechanisms, and thus motivates further research on privacy-preserving publication schemes."}}
{"id": "QrSenHi9_D", "cdate": 1640995200000, "mdate": 1683887574014, "content": {"title": "Reconstruction Attack on Differential Private Trajectory Protection Mechanisms", "abstract": "Location trajectories collected by smartphones and other devices represent a valuable data source for applications such as location-based services. Likewise, trajectories have the potential to reveal sensitive information about individuals, e.g., religious beliefs or sexual orientations. Accordingly, trajectory datasets require appropriate sanitization. Due to their strong theoretical privacy guarantees, differential private publication mechanisms receive much attention. However, the large amount of noise required to achieve differential privacy yields structural differences, e.g., ship trajectories passing over land. We propose a deep learning-based Reconstruction Attack on Protected Trajectories (RAoPT), that leverages the mentioned differences to partly reconstruct the original trajectory from a differential private release. The evaluation shows that our RAoPT model can reduce the Euclidean and Hausdorff distances between the released and original trajectories by over 68\u00a0% on two real-world datasets under protection with \u03b5 \u2264 1. In this setting, the attack increases the average Jaccard index of the trajectories\u2019 convex hulls, representing a user\u2019s activity space, by over 180\u00a0%. Trained on the GeoLife dataset, the model still reduces the Euclidean and Hausdorff distances by over 60\u00a0% for T-Drive trajectories protected with a state-of-the-art mechanism (\u03b5 = 0.1). This work highlights shortcomings of current trajectory publication mechanisms, and thus motivates further research on privacy-preserving publication schemes."}}
