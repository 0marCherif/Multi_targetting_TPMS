{"id": "pqRaUo67VUy", "cdate": 1654682309190, "mdate": 1654682309190, "content": {"title": "When Deep Classifiers Agree: Analyzing Correlations between Learning Order and Image Statistics", "abstract": "Although a plethora of architectural variants for deep classification has been introduced over time, recent works have found empirical evidence towards similarities in their training process. It has been hypothesized that neural networks converge not only to similar representations, but also exhibit a notion of empirical agreement on which data instances are learned first. Following in the latter works\u2032 footsteps, we define a metric to quantify the relationship between such classification agreement over time, and posit that the agreement phenomenon can be mapped to core statistics of the investigated dataset. We empirically corroborate this hypothesis across the CIFAR10, Pascal, ImageNet and KTH-TIPS2 datasets. Our findings indicate that agreement seems to be independent of specific architectures, training hyper-parameters or labels, albeit follows an ordering according to image statistics."}}
{"id": "11YwCxYRRzs", "cdate": 1654681141640, "mdate": 1654681141640, "content": {"title": "Neural Architecture Search of Deep Priors: Towards Continual Learning without Catastrophic Interference", "abstract": "In this paper we analyze the classification performance of neural network structures without parametric inference. Making use of neural architecture search, we empirically demonstrate that it is possible to find random weight architectures, a deep prior, that enables a linear classification to perform on par with fully trained deep counterparts. Through ablation experiments, we exclude the possibility of\nwinning a weight initialization lottery and confirm that suitable deep priors do not require additional inference. In an extension to continual learning, we investigate the possibility of catastrophic interference free incremental learning. Under the assumption of classes originating from the same data distribution, a deep prior found on only a subset of classes is shown to allow discrimination of further classes through training of a simple linear classifier."}}
{"id": "LlCQWh8-pwK", "cdate": 1622816327085, "mdate": null, "content": {"title": "A Procedural World Generation Framework for Systematic Evaluation of Continual Learning", "abstract": "Several families of continual learning techniques have been proposed to alleviate catastrophic interference in deep neural network training on non-stationary data. However, a comprehensive comparison and analysis of limitations remains largely open due to the inaccessibility to suitable datasets. Empirical examination not only varies immensely between individual works, it further currently relies on contrived composition of benchmarks through subdivision and concatenation of various prevalent static vision datasets. In this work, our goal is to bridge this gap by introducing a computer graphics simulation framework that repeatedly renders only upcoming urban scene fragments in an endless real-time procedural world generation process. At its core lies a modular parametric generative model with adaptable generative factors. The latter can be used to flexibly compose data streams, which significantly facilitates a detailed analysis and allows for effortless investigation of various continual learning schemes. "}}
{"id": "rJlDoT4twr", "cdate": 1569439135433, "mdate": null, "content": {"title": "Unified Probabilistic Deep Continual Learning through Generative Replay and Open Set Recognition", "abstract": "We introduce a unified probabilistic approach for deep continual learning based on variational Bayesian inference with open set recognition. Our model combines a joint probabilistic encoder with a generative model and a linear classifier that get shared across tasks. The open set recognition bounds the approximate posterior by fitting regions of high density on the basis of correctly classified data points and balances open set detection with recognition errors. Catastrophic forgetting is significantly alleviated through generative replay, where the open set recognition is used to sample from high density areas of the class specific posterior and reject statistical outliers. Our approach naturally allows for forward and backward transfer while maintaining past knowledge without the necessity of storing old data, regularization or inferring task labels. We demonstrate compelling results in the challenging scenario of incrementally expanding the single-head classifier for both class incremental visual and audio classification tasks, as well as incremental learning of datasets across modalities."}}
{"id": "SkffVjUaW", "cdate": 1518730191535, "mdate": null, "content": {"title": "Building effective deep neural networks one feature at a time", "abstract": "Successful training of convolutional neural networks is often associated with suffi-\nciently deep architectures composed of high amounts of features. These networks\ntypically rely on a variety of regularization and pruning techniques to converge\nto less redundant states. We introduce a novel bottom-up approach to expand\nrepresentations in fixed-depth architectures. These architectures start from just a\nsingle feature per layer and greedily increase width of individual layers to attain\neffective representational capacities needed for a specific task. While network\ngrowth can rely on a family of metrics, we propose a computationally efficient\nversion based on feature time evolution and demonstrate its potency in determin-\ning feature importance and a networks\u2019 effective capacity. We demonstrate how\nautomatically expanded architectures converge to similar topologies that benefit\nfrom lesser amount of parameters or improved accuracy and exhibit systematic\ncorrespondence in representational complexity with the specified task. In contrast\nto conventional design patterns with a typical monotonic increase in the amount of\nfeatures with increased depth, we observe that CNNs perform better when there is\nmore learnable parameters in intermediate, with falloffs to earlier and later layers."}}
{"id": "B1VcoCWO-H", "cdate": 1483228800000, "mdate": null, "content": {"title": "Adversarially Tuned Scene Generation", "abstract": "Generalization performance of trained computer vision (CV) systems that use computer graphics (CG) generated data is not yet effective due to the concept of domain-shift between virtual and real data. Although simulated data augmented with a few real-world samples has been shown to mitigate domain shift and improve transferability of trained models, guiding or bootstrapping the virtual data generation with the distributions learnt from target real world domain is desired, especially in the fields where annotating even few real images is laborious (such as semantic labeling, optical flow, and intrinsic images etc.). In order to address this problem in an unsupervised manner, our work combines recent advances in CG, which aims at generating stochastic scene layouts using large collections of 3D object models, and generative adversarial training, which aims at training generative models by measuring discrepancy between generated and real data in terms of their separability in the space of a deep discriminatively-trained classifier. Our method uses iterative estimation of the posterior density of prior distributions for a generative graphical model. This is done within a rejection sampling framework. Initially, we assume uniform distributions as priors over parameters of a scene described by a generative graphical model. As iterations proceed the uniform prior distributions are updated sequentially to distributions that are closer to the unknown distributions of target data. We demonstrate the utility of adversarially tuned scene generation on two real world benchmark datasets (CityScapes and CamVid) for traffic scene semantic labeling with a deep convolutional net (DeepLab). We obtained performance improvements by 2.28 and 3.14 points on the IoU metric between the DeepLab models trained on simulated sets prepared from the scene generation models before and after tuning to CityScapes and CamVid respectively."}}
{"id": "ry4r8JGu-S", "cdate": 1325376000000, "mdate": null, "content": {"title": "Discrete texture traces: Topological representation of geometric context", "abstract": "Modeling representations of image patches that are quasi-invariant to spatial deformations is an important problem in computer vision. In this paper, we propose a novel concept, the texture trace, that allows sparse patch representations which are quasi-invariant to smooth deformations and robust against occlusions. We first propose a continuous domain model, the profile trace, which is a function only of the topological properties of an image and is by construction invariant to any homeomorphic transformation of the domain. We analyze its theoretical properties and then derive a discrete-domain approximation, the Discrete Texture Trace (DTT). DTTs are designed to be computationally practical and shown by a set of controlled experiments to be quasi-invariant to smooth spatial deformations as well as common image perturbations. We then show how DTTs can be naturally adapted to the incremental tracking problem, yielding highly precise results on par with the state of the art on challenging real data without using heavy machine learning tools. Indeed, we show that with even just using one image at the start of a sequence (i.e. no incremental updating), our method already outperforms four of six state of the art methods of the recent literature on challenging sequences."}}
{"id": "ryN12yzdZB", "cdate": 1262304000000, "mdate": null, "content": {"title": "Illumination compensation based change detection using order consistency", "abstract": "We present a change detection method resistant to global and local illumination variations for use in visual surveillance scenarios. Approaches designed thus far for robustness to illumination change are generally based either on color normalization, texture (e.g. edges, rank order statistics, etc.), or illumination compensation. Normalization based methods sacrifice discriminability while texture based methods cannot operate on texture-less regions. Both types of method can produce large missing regions in the distance image which in turn pose problems for higher-level processing tasks that may be shape or region-based and require accurate foreground masks (e.g. person detection and tracking, crowd segmentation, etc.). Texture based methods have an additional problem in that they produce false alarms due to textures induced by local illumination effects (e.g. cast shadows). In this paper we propose a compensation based approach for change detection. Prior work on compensation has largely taken an empirical approach, and has not dealt with the important problem of rejecting outliers when they dominate the scene. In contrast, our generative approach and systematic handling of outliers enables us to achieve robustness to illumination change while eliminating the problems mentioned above. Furthermore, the computational complexity of our method is low enough for real-time performance. Results comparing images taken under strongly different illumination conditions, demonstrate the power and generality of the proposed method."}}
{"id": "H1Zdwa-ObB", "cdate": 1230768000000, "mdate": null, "content": {"title": "Predicate logic based image grammars for complex pattern recognition", "abstract": "In this paper, an extended work reported in [Shet, et al , 2007] to detect complex objects in aerial images was discussed. Such objects, e.g. surface to air missile launcher sites, are highly variable in appearance and can only be characterized by their functional design and surrounding context, such as physical arrangement of access structures. Constraints in acquiring sufficient annotated data for learning make it challenging for purely data driven approaches to adequately generalize. In this work, structure arising from functional requirements and surrounding context has been encoded using predicate logic based grammars. Observation and model uncertainties have been integrated within the bi lattice framework. Also in this paper a proposed method to automatically optimize weights associated with logical rules is presented. Automated logical rule weight learning is an important aspect of the application of such systems in the computer vision domain. The proposed approach casts the instantiated inference tree as a knowledge based neural net, interprets rule uncertainties as link weights in the network, and applies a constrained, back propagation (BP) algorithm to converge upon a set of weights for optimal performance. The BP algorithm has been accordingly modified to compute local gradients over the bi lattice specific inference operation and respect constraints specific to vision applications. Both extension have been evaluated over real and simulated data with favorable results."}}
{"id": "HyZ4hC-_Zr", "cdate": 1199145600000, "mdate": null, "content": {"title": "Order consistent change detection via fast statistical significance testing", "abstract": "Robustness to illumination variations is a key requirement for the problem of change detection which in turn is a fundamental building block for many visual surveillance applications. The use of ordinal measures is a powerful way of filtering out illumination dependency in representing appearance, and several such measures have been proposed in the past for change detection. By design, these measures are invariant to unknown monotonic transformations that may be caused due to global illumination changes or automatic camera gain. However, previous work has left theoretical and practical gaps that limit their full potential from being realized. For instance, random noise has not been given a principled treatment. In this paper, we formulate the change detection problem in terms of order consistency and show that in the presence of noise with known statistical properties, significance tests for order consistency yield much better results than the state of the art. Since ordinal measures require a reordering of patches, they are usually expensive in practice (O(n*log n) at best). We improve upon this by connecting the problem to monotonic regression, and applying a fast algorithm from the corresponding literature. We also show that good trade offs between speed and accuracy can be made by quantization to achieve accurate and very fast matching algorithms in practice. We demonstrate superior performance on statistical simulations as well as real image sequences."}}
