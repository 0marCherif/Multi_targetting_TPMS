{"id": "ku13JJDsDat", "cdate": 1686250302280, "mdate": null, "content": {"title": "On-Demand Communication for Asynchronous Multi-Agent Bandits", "abstract": "This paper studies a cooperative multi-agent multi-armed stochastic bandit problem where agents operate $\\textit{asynchronously}$ -- agent pull times and rates are unknown, irregular, and heterogeneous -- and face the same instance of a $K$-armed bandit problem. Agents can share reward information to speed up the learning process at additional communication costs. We propose $\\texttt{ODC}$, an on-demand communication protocol that tailors the communication of each pair of agents based on their empirical pull times. $\\texttt{ODC}$ is efficient when the pull times of agents are highly heterogeneous, and its communication complexity depends on the empirical pull times of agents. $\\texttt{ODC}$ is a generic protocol that can be integrated into most cooperative bandit algorithms without degrading their performance. We then incorporate $\\texttt{ODC}$ into the natural extensions of $\\texttt{UCB}$ and $\\texttt{AAE}$ algorithms and propose two communication-efficient cooperative algorithms. Our analysis shows that both algorithms are near-optimal in regret."}}
{"id": "8kKEz1bnIEp", "cdate": 1676827064170, "mdate": null, "content": {"title": "Exploration for Free: How Does Reward Heterogeneity Improve Regret in Cooperative Multi-agent Bandits?", "abstract": "This paper studies a cooperative multi-agent bandit scenario in which the rewards observed by agents are heterogeneous---one agent's meat can be another agent's poison. Specifically, the total reward observed by each agent is the sum of two values: an arm-specific reward, capturing the intrinsic value of the arm, and a privately-known agent-specific reward, which captures the personal preference/limitations of the agent. This heterogeneity in total reward leads to different local optimal arms for agents but creates an opportunity for *free exploration* in a cooperative setting---an agent can freely explore its local optimal arm with no regret and share this free observation with some other agents who would suffer regrets if they pull this arm since the arm is not optimal for them.\nWe first characterize a regret lower bound that captures free exploration, i.e., arms that can be freely explored have no contribution to the regret lower bound. Then, we present a cooperative bandit algorithm that takes advantage of free exploration and achieves a near-optimal regret upper bound which tightly matches the regret lower bound up to a constant factor.\nLastly, we run numerical simulations to compare our algorithm with various baselines without free exploration."}}
{"id": "IbdJT31W2RX", "cdate": 1672531200000, "mdate": 1681884836289, "content": {"title": "On-Demand Communication for Asynchronous Multi-Agent Bandits", "abstract": "This paper studies a cooperative multi-agent multi-armed stochastic bandit problem where agents operate asynchronously -- agent pull times and rates are unknown, irregular, and heterogeneous -- and face the same instance of a K-armed bandit problem. Agents can share reward information to speed up the learning process at additional communication costs. We propose ODC, an on-demand communication protocol that tailors the communication of each pair of agents based on their empirical pull times. ODC is efficient when the pull times of agents are highly heterogeneous, and its communication complexity depends on the empirical pull times of agents. ODC is a generic protocol that can be integrated into most cooperative bandit algorithms without degrading their performance. We then incorporate ODC into the natural extensions of UCB and AAE algorithms and propose two communication-efficient cooperative algorithms. Our analysis shows that both algorithms are near-optimal in regret."}}
{"id": "QTXKTXJKIh", "cdate": 1663850327521, "mdate": null, "content": {"title": "Achieving Near-Optimal Individual Regret & Low Communications in Multi-Agent Bandits", "abstract": "Cooperative multi-agent multi-armed bandits (CM2AB) study how distributed agents cooperatively play the same multi-armed bandit game. Most existing CM2AB works focused on maximizing the group performance of all agents---the accumulation of all agents' individual performance (i.e., individual reward). However, in many applications, the performance of the system is more sensitive to the ``bad'' agent---the agent with the worst individual performance. For example, in a drone swarm, a ``bad'' agent may crash into other drones and severely degrade the system performance. In that case, the key of the learning algorithm design is to coordinate computational and communicational resources among agents so to optimize the individual learning performance of the ``bad'' agent. In CM2AB, maximizing the group performance is equivalent to minimizing the group regret of all agents, and minimizing the individual performance can be measured by minimizing the maximum (worst) individual regret among agents. Minimizing the maximum individual regret was largely ignored in prior literature, and currently, there is little work on how to minimize this objective with a low communication overhead. In this paper, we propose a near-optimal algorithm on both individual and group regrets, in addition,  we also propose a novel communication module in the algorithm, which only needs \\(O(\\log (\\log T))\\) communication times where \\(T\\) is the number of decision rounds. We also conduct simulations to illustrate the advantage of our algorithm by comparing it to other known baselines."}}
{"id": "zAlb742PeP", "cdate": 1640995200000, "mdate": 1682338459010, "content": {"title": "To Collaborate or Not in Distributed Statistical Estimation with Resource Constraints?", "abstract": ""}}
{"id": "WlXHtQCPQB_", "cdate": 1640995200000, "mdate": 1682338459067, "content": {"title": "Hierarchical Learning Algorithms for Multi-scale Expert Problems", "abstract": ""}}
{"id": "MoUPIjEIKg3", "cdate": 1640995200000, "mdate": 1682338459066, "content": {"title": "Distributed Bandits with Heterogeneous Agents", "abstract": ""}}
{"id": "KGy9qRRusbi", "cdate": 1640995200000, "mdate": 1682338459047, "content": {"title": "Hierarchical Learning Algorithms for Multi-scale Expert Problems", "abstract": ""}}
{"id": "BO6uDx3Zyk8", "cdate": 1640995200000, "mdate": 1682338458991, "content": {"title": "Distributed Bandits with Heterogeneous Agents", "abstract": ""}}
{"id": "gPsA0bMEKIk", "cdate": 1633046400000, "mdate": 1682338459062, "content": {"title": "Learning to count: A deep learning framework for graphlet count estimation", "abstract": ""}}
