{"id": "jwgnijhdF3V", "cdate": 1663850025349, "mdate": null, "content": {"title": "Posterior Sampling Model-based Policy Optimization under Approximate Inference", "abstract": "Model-based reinforcement learning algorithms (MBRL) hold tremendous promise for improving the sample efficiency in online RL. However, many existing popular MBRL algorithms cannot deal with exploration and exploitation properly. Posterior sampling reinforcement learning (PSRL) serves as a promising approach for automatically trading off the exploration and exploitation, but the theoretical guarantees only hold under exact inference. In this paper, we show that adopting the same methodology as in exact PSRL can be fairly suboptimal under approximate inference. Motivated by the analysis, we propose an improved factorization for the posterior distribution of polices by removing the conditional independence between the policy and data given the model. By adopting such a posterior factorization, we further propose a general algorithmic framework for PSRL under approximate inference and a practical instantiation of it. Empirically, our algorithm can surpass the baseline methods by a significant margin on both dense rewards and sparse rewards tasks from DM control suite, OpenAI Gym and Metaworld benchmarks."}}
{"id": "ZkGfZLEXZ20", "cdate": 1621629768492, "mdate": null, "content": {"title": "Teaching an Active Learner with Contrastive Examples", "abstract": "We study the problem of active learning with the added twist that the learner is assisted by a helpful teacher. We consider the following natural interaction protocol: At each round, the learner proposes a query asking for the label of an instance $x^q$, the teacher provides the requested label $\\{x^q, y^q\\}$ along with explanatory information to guide the learning process. In this paper, we view this information in the form of an additional contrastive example ($\\{x^c, y^c\\}$) where $x^c$ is picked from a set constrained by $x^q$ (e.g., dissimilar instances with the same label). Our focus is to design a teaching algorithm that can provide an informative sequence of contrastive examples to the learner to speed up the learning process. We show that this leads to a challenging sequence optimization problem where the algorithm's choices at a given round depend on the history of interactions. We investigate an efficient teaching algorithm that adaptively picks these contrastive examples. We derive strong performance guarantees for our algorithm based on two problem-dependent parameters and further show that for specific types of active learners (e.g., a generalized binary search learner), the proposed teaching algorithm exhibits strong approximation guarantees. Finally, we illustrate our bounds and demonstrate the effectiveness of our teaching framework via two numerical case studies."}}
{"id": "e4-3ZXruuxz", "cdate": 1596144766736, "mdate": null, "content": {"title": "Three Mechanisms of Weight Decay Regularization", "abstract": "Weight decay is one of the standard tricks in the neural network toolbox, but the\nreasons for its regularization effect are poorly understood, and recent results have\ncast doubt on the traditional interpretation in terms of L2 regularization. Literal\nweight decay has been shown to outperform L2 regularization for optimizers for\nwhich they differ. We empirically investigate weight decay for three optimization\nalgorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We\nidentify three distinct mechanisms by which weight decay exerts a regularization\neffect, depending on the particular optimization algorithm and architecture: (1)\nincreasing the effective learning rate, (2) approximately regularizing the inputoutput Jacobian norm, and (3) reducing the effective damping coefficient for\nsecond-order optimization. Our results provide insight into how to improve the\nregularization of neural networks."}}
{"id": "SkgsACVKPH", "cdate": 1569439442953, "mdate": null, "content": {"title": "Picking Winning Tickets Before Training by Preserving Gradient Flow", "abstract": "Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time.  Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels. Our code is made public\nat: https://github.com/alecwangcq/GraSP."}}
{"id": "rJZ50ibdWH", "cdate": 1546300800000, "mdate": null, "content": {"title": "EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis", "abstract": "Reducing the test time resource requirements of a neural network while preserving test accuracy is crucial for running inference on resource-constrained devices. To achieve this goal, we introduce ..."}}
{"id": "B1lz-3Rct7", "cdate": 1538087930358, "mdate": null, "content": {"title": "Three Mechanisms of Weight Decay Regularization", "abstract": "Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of $L_2$ regularization.\nLiteral weight decay has been shown to outperform $L_2$ regularization for optimizers for which they differ. \nWe empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) approximately regularizing the input-output Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. \nOur results provide insight into how to improve the regularization of neural networks."}}
{"id": "S1-rG3bOZr", "cdate": 1514764800000, "mdate": null, "content": {"title": "Differentiable Compositional Kernel Learning for Gaussian Processes", "abstract": "The generalization properties of Gaussian processes depend heavily on the choice of kernel, and this choice remains a dark art. We present the Neural Kernel Network (NKN), a flexible family of kern..."}}
