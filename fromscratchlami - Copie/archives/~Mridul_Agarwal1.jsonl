{"id": "Hg-IBdIo5e9", "cdate": 1646077549155, "mdate": null, "content": {"title": "An Explore-then-Commit Algorithm for Submodular Maximization Under Full-bandit Feedback", "abstract": "We investigate the problem of combinatorial multi-armed bandits with stochastic submodular (in expectation) rewards and full-bandit feedback, where no extra information other than the reward of selected action at each time step $t$ is observed. We propose a simple algorithm, Explore-Then-Commit Greedy (ETCG) and prove that it  achieves a $(1-1/e)$-regret upper bound of $\\mathcal{O}(n^\\frac{1}{3}k^\\frac{4}{3}T^\\frac{2}{3}\\log(T)^\\frac{1}{2})$ for a horizon $T$, number of base elements $n$, and cardinality constraint $k$. We also show in experiments with synthetic and real-world data that the ETCG empirically outperforms other full-bandit methods."}}
{"id": "HnzV0PIi5ec", "cdate": 1646077541843, "mdate": null, "content": {"title": "Regret Guarantees for Model-Based Reinforcement Learning with Long-Term Average Constraints", "abstract": "We consider the problem of constrained Markov Decision Process (CMDP) where an agent interacts with an ergodic Markov Decision Process. At every interaction, the agent obtains a reward and incurs $K$ costs. The agent aims to maximize the long-term average reward while simultaneously keeping the $K$ long-term average costs lower than a certain threshold. In this paper, we propose \\NAM, a posterior sampling based algorithm using which the agent can learn optimal policies to interact with the CMDP. We show that with the assumption of slackness, characterized by $\\kappa$, the optimization problem is feasible for the sampled MDPs. Further, for MDP with $S$ states, $A$ actions, and mixing time $T_M$, we prove that following \\NAM\\ algorithm, the agent can bound the regret of not accumulating rewards from an optimal policy by $\\Tilde{O}(T_MS\\sqrt{AT})$. Further, we show that the violations for any of the $K$ constraints is also bounded by $\\Tilde{O}(T_MS\\sqrt{AT})$. To the best of our knowledge, this is the first work that obtains a $\\Tilde{O}(\\sqrt{T})$ regret bounds for ergodic MDPs with long-term average constraints using a posterior sampling method."}}
{"id": "wzC8QX2z1Z", "cdate": 1640995200000, "mdate": 1681490236650, "content": {"title": "Achieving Zero Constraint Violation for Constrained Reinforcement Learning via Primal-Dual Approach", "abstract": ""}}
{"id": "t6kusgNi5g", "cdate": 1640995200000, "mdate": 1682885915339, "content": {"title": "Multi-Objective Reinforcement Learning with Non-Linear Scalarization", "abstract": ""}}
{"id": "np7MZfO-Ki", "cdate": 1640995200000, "mdate": 1674227344358, "content": {"title": "Reinforcement Learning for Mean-Field Game", "abstract": "Stochastic games provide a framework for interactions among multiple agents and enable a myriad of applications. In these games, agents decide on actions simultaneously. After taking an action, the state of every agent updates to the next state, and each agent receives a reward. However, finding an equilibrium (if exists) in this game is often difficult when the number of agents becomes large. This paper focuses on finding a mean-field equilibrium (MFE) in an action-coupled stochastic game setting in an episodic framework. It is assumed that an agent can approximate the impact of the other agents\u2019 by the empirical distribution of the mean of the actions. All agents know the action distribution and employ lower-myopic best response dynamics to choose the optimal oblivious strategy. This paper proposes a posterior sampling-based approach for reinforcement learning in the mean-field game, where each agent samples a transition probability from the previous transitions. We show that the policy and action distributions converge to the optimal oblivious strategy and the limiting distribution, respectively, which constitute an MFE."}}
{"id": "dHilOAXQ9x", "cdate": 1640995200000, "mdate": 1682885915339, "content": {"title": "Regret guarantees for model-based reinforcement learning with long-term average constraints", "abstract": "We consider the problem of constrained Markov Decision Process (CMDP) where an agent interacts with an ergodic Markov Decision Process. At every interaction, the agent obtains a reward and incurs $..."}}
{"id": "cWXBU8iwyC", "cdate": 1640995200000, "mdate": 1682634641592, "content": {"title": "An explore-then-commit algorithm for submodular maximization under full-bandit feedback", "abstract": "We investigate the problem of combinatorial multi-armed bandits with stochastic submodular (in expectation) rewards and full-bandit feedback, where no extra information other than the reward of sel..."}}
{"id": "L4KZXLo5pC", "cdate": 1640995200000, "mdate": 1682885915240, "content": {"title": "Learning-Based Online QoE Optimization in Multi-Agent Video Streaming", "abstract": "Video streaming has become a major usage scenario for the Internet. The growing popularity of new applications, such as 4K and 360-degree videos, mandates that network resources must be carefully apportioned among different users in order to achieve the optimal Quality of Experience (QoE) and fairness objectives. This results in a challenging online optimization problem, as networks grow increasingly complex and the relevant QoE objectives are often nonlinear functions. Recently, data-driven approaches, deep Reinforcement Learning (RL) in particular, have been successfully applied to network optimization problems by modeling them as Markov decision processes. However, existing RL algorithms involving multiple agents fail to address nonlinear objective functions on different agents\u2019 rewards. To this end, we leverage MAPG-finite, a policy gradient algorithm designed for multi-agent learning problems with nonlinear objectives. It allows us to optimize bandwidth distributions among multiple agents and to maximize QoE and fairness objectives on video streaming rewards. Implementing the proposed algorithm, we compare the MAPG-finite strategy with a number of baselines, including static, adaptive, and single-agent learning policies. The numerical results show that MAPG-finite significantly outperforms the baseline strategies with respect to different objective functions and in various settings, including both constant and adaptive bitrate videos. Specifically, our MAPG-finite algorithm maximizes QoE by 15.27% and maximizes fairness by 22.47% compared to the standard SARSA algorithm for a 2000 KB/s link."}}
{"id": "AA2mB7G5ZJ", "cdate": 1640995200000, "mdate": 1682885915242, "content": {"title": "Joint Optimization of Concave Scalarized Multi-Objective Reinforcement Learning with Policy Gradient Based Algorithm", "abstract": "Many engineering problems have multiple objectives, and the overall aim is to optimize a non-linear function of these objectives. In this paper, we formulate the problem of maximizing a non-linear concave function of multiple long-term objectives. A policy-gradient based model-free algorithm is proposed for the problem. To compute an estimate of the gradient, an asymptotically biased estimator is proposed. The proposed algorithm is shown to achieve convergence to within an \u03b5 of the global optima after sampling O(M4 \u03c32/(1-\u03b3)8\u03b54) trajectories where \u03b3 is the discount factor and M is the number of the agents, thus achieving the same dependence on \u03b5 as the policy gradient algorithm for the standard reinforcement learning."}}
{"id": "3gRRGzh9QZ2", "cdate": 1640995200000, "mdate": 1682885915240, "content": {"title": "On the Global Convergence of Fitted Q-Iteration with Two-layer Neural Network Parametrization", "abstract": "Deep Q-learning based algorithms have been applied successfully in many decision making problems, while their theoretical foundations are not as well understood. In this paper, we study a Fitted Q-Iteration with two-layer ReLU neural network parameterization, and find the sample complexity guarantees for the algorithm. Our approach estimates the Q-function in each iteration using a convex optimization problem. We show that this approach achieves a sample complexity of $\\tilde{\\mathcal{O}}(1/\\epsilon^{2})$, which is order-optimal. This result holds for a countable state-spaces and does not require any assumptions such as a linear or low rank structure on the MDP."}}
