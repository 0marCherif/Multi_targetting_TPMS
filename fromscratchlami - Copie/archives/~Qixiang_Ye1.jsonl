{"id": "HAh-YuQ0qJm", "cdate": 1685326330224, "mdate": 1685326330224, "content": {"title": "Cogradient descent for bilinear optimization", "abstract": "Conventional learning methods simplify the bilinear model by regarding two intrinsically coupled factors independently, which degrades the optimization procedure. One reason lies in the insufficient training due to the asynchronous gradient descent, which results in vanishing gradients for the coupled variables. In this paper, we introduce a Cogradient Descent algorithm (CoGD) to address the bilinear problem, based on a theoretical framework to coordinate the gradient of hidden variables via a projection function. We solve one variable by considering its coupling relationship with the other, leading to a synchronous gradient descent to facilitate the optimization procedure. Our algorithm is applied to solve problems with one variable under the sparsity constraint, which is widely used in the learning paradigm. We validate our CoGD considering an extensive set of applications including image reconstruction, inpainting, and network pruning. Experiments show that it improves the state-of-the-art by a significant margin."}}
{"id": "otMrluMV0UV", "cdate": 1665627069063, "mdate": 1665627069063, "content": {"title": "Video Playback Rate Perception for Self-Supervised Spatio-Temporal Representation Learning", "abstract": "In self-supervised spatio-temporal representation learning, the temporal resolution and long-short term characteristics are not yet fully explored, which limits representation capabilities of learned models. In this paper, we propose a novel self-supervised method, referred to as video Playback Rate Perception (PRP), to learn spatio-temporal representation in a simple-yet-effective way. PRP roots in a dilated sampling strategy, which produces self-supervision signals about video playback rates for representation model learning. PRP is implemented with a feature encoder, a classification module, and a reconstructing decoder, to achieve spatio-temporal semantic retention in a collaborative discrimination-generation manner. The discriminative perception model follows a feature encoder to prefer perceiving low temporal resolution and long-term representation by classifying fast-forward rates. The generative perception model acts as a feature decoder to focus on comprehending high temporal resolution and short-term representation by introducing a motion-attention mechanism. PRP is applied on typical video target tasks including action recognition and video retrieval. Experiments show that PRP outperforms state-of-the-art self-supervised models with significant margins."}}
{"id": "47DDVyu7PU", "cdate": 1665626661308, "mdate": 1665626661308, "content": {"title": "Beyond Max-Margin: Class Margin Equilibrium for Few-shot Object Detection", "abstract": "Few-shot object detection has made substantial progress by representing novel class objects using the feature representation learned upon a set of base class objects. However, an implicit contradiction between novel class classification and representation is unfortunately ignored. On the one hand, to achieve accurate novel class classification, the distributions of either two base classes must be far away from each other (max-margin). On the other hand, to precisely represent novel classes, the distributions of base classes should be close to each other to reduce the intra-class distance of novel classes (min-margin). In this paper, we propose a class margin equilibrium (CME) approach, with the aim to optimize both feature space partition and novel class reconstruction in a systematic way. CME first converts the few-shot detection problem to the few-shot classification problem by using a fully connected layer to decouple localization features. CME then reserves dequate margin space for novel classes by introducing simple-yet-effective class margin loss during feature learning. Finally, CME pursues margin equilibrium by disturbing the features of novel class instances in an adversarial min-max fashion. Experiments on Pascal VOC and MS-COCO datasets show that CME significantly improves upon two baseline detectors (up to 3 \u223c 5% in average), achieving state-of-the-art performance."}}
{"id": "U1cPSPskTR", "cdate": 1665626344525, "mdate": null, "content": {"title": "Conformer: Local Features Coupling Global Representations for Visual Recognition", "abstract": "Within Convolutional Neural Network (CNN), the convolution operations are good at extracting local features but experience difficulty to capture global representations. Within visual transformer, the cascaded self-attention modules can capture long-distance feature dependencies but unfortunately deteriorate local feature details. In this paper, we propose a hybrid network structure, termed Conformer,\nto take advantage of convolutional operations and selfattention mechanisms for enhanced representation learning. Conformer roots in the Feature Coupling Unit (FCU), which fuses local features and global representations under different resolutions in an interactive fashion. Conformer adopts a concurrent structure so that local features and global representations are retained to the maximum extent. Experiments show that Conformer, under the comparable parameter complexity, outperforms the visual transformer (DeiT-B) by 2.3% on ImageNet. On MSCOCO, it outperforms ResNet-101 by 3.7% and 3.6% mAPs for object detection and instance segmentation, respectively, demonstrating\nthe great potential to be a general backbone network."}}
{"id": "17iF9bpu61", "cdate": 1665626153429, "mdate": 1665626153429, "content": {"title": "Multiple Instance Active Learning for Object Detection", "abstract": "Despite the substantial progress of active learning for image recognition, there still lacks an instance-level active learning method specified for object detection. In this paper, we propose Multiple Instance Active Object Detection (MI-AOD), to select the most informative images for detector training by observing instance-level uncertainty. MI-AOD defines an instance uncertainty learning module,which leverages the discrepancy of two adversarial instance classifiers trained on the labeled set to predict instance uncertainty of the unlabeled set. MI-AOD treats unlabeled images as instance bags and feature anchors in images as instances, and estimates the image uncertainty by re-weighting instances in a multiple instance learning (MIL) fashion. Iterative instance uncertainty learning and re-weighting facilitate suppressing noisy instances, toward bridging the gap between instance uncertainty and imagelevel uncertainty. Experiments validate that MI-AOD sets a solid baseline for instance-level active learning. On commonly used object detection datasets, MI-AOD outperforms state-of-the-art methods with significant margins, particularly when the labeled sets are small."}}
{"id": "VB75Pi89p7", "cdate": 1663849978686, "mdate": null, "content": {"title": "BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers", "abstract": "Masked image modeling (MIM) has demonstrated impressive results in self-supervised representation learning by recovering corrupted image patches. However, most existing studies operate on low-level image pixels, which hinders the exploitation of high-level semantics for representation models. In this work, we propose to use a semantic-rich visual tokenizer as the reconstruction target for masked prediction, providing a systematic way to promote MIM from pixel-level to semantic-level. Specifically, we propose vector-quantized knowledge distillation to train the tokenizer, which discretizes a continuous semantic space to compact codes. We then pretrain vision Transformers by predicting the original visual tokens for the masked image patches. Furthermore, we introduce a patch aggregation strategy which associates discrete image patches to enhance global semantic representation. Experiments on image classification and semantic segmentation show that BEiT v2 outperforms all compared MIM methods. On ImageNet-1K (224 size), the base-size BEiT v2 achieves $85.5\\%$ top-1 accuracy for fine-tuning and $80.1\\%$ top-1 accuracy for linear probing. The large-size BEiT v2 obtains $87.3\\%$ top-1 accuracy for ImageNet-1K (224 size) fine-tuning, and $56.7\\%$ mIoU on ADE20K for semantic segmentation. The code can be found in the supplementary materials."}}
{"id": "3F6I-0-57SC", "cdate": 1663849970677, "mdate": null, "content": {"title": "HiViT: A Simpler and More Efficient Design of Hierarchical Vision Transformer", "abstract": "There has been a debate on the choice of plain vs. hierarchical vision transformers, where researchers often believe that the former (e.g., ViT) has a simpler design but the latter (e.g., Swin) enjoys higher recognition accuracy. Recently, the emerge of masked image modeling (MIM), a self-supervised visual pre-training method, raised a new challenge to vision transformers in terms of flexibility, i.e., part of image patches or tokens are to be discarded, which seems to claim the advantages of plain vision transformers. In this paper, we delve deep into the comparison between ViT and Swin, revealing that (i) the performance gain of Swin is mainly brought by a deepened backbone and relative positional encoding, (ii) the hierarchical design of Swin can be simplified into hierarchical patch embedding (proposed in this work), and (iii) other designs such as shifted-window attentions can be removed. By removing the unnecessary operations, we come up with a new architecture named HiViT (short for hierarchical ViT), which is simpler and more efficient than Swin yet further improves its performance on fully-supervised and self-supervised visual representation learning. In particular, after pre-trained using masked autoencoder (MAE) on ImageNet-1K, HiViT-B reports a 84.6% accuracy on ImageNet-1K classification, a 53.3% box AP on COCO detection, and a 52.8% mIoU on ADE20K segmentation, significantly surpassing the baseline. Code is available at https://github.com/zhangxiaosong18/hivit."}}
{"id": "pP9ag2g5f0", "cdate": 1632875421987, "mdate": null, "content": {"title": "Exploring Complicated Search Spaces with Interleaving-Free Sampling", "abstract": "The design of search space plays a crucial role in neural architecture search (NAS). Existing search spaces mostly involve short-distance connections arguably due to the increasing difficulty brought by long-distance ones. This paper systematically studies this problem in the context of super-network optimization, and reveals that the interleaved connections introduce significant noises to the amortized accuracy. Based on the observation, we propose a simple yet effective interleaving-free sampling algorithm to aid the search process, and name our method as IF-NAS. In each iteration, IF-NAS samples a sub-network that does not contain any interleaved connections. We design a difficult search space with a large number of complicatedly connected nodes, $10^{186}\\times$ larger than the DARTS space, on which IF-NAS outperforms other competitors evidently. IF-NAS also generalizes to the known (easier) search spaces including DARTS and GOLD-NAS, the design of which carries great prior knowledge. Our research sheds light on extending the macro structure which is well acknowledged as a major challenge of NAS."}}
{"id": "sYXHHcVrcx", "cdate": 1582805832153, "mdate": null, "content": {"title": "SRN: Side-output Residual Network for Object Symmetry Detection in the Wild", "abstract": "In this paper, we establish a baseline for object symmetry detection in complex backgrounds by presenting a new benchmark and an end-to-end deep learning approach, opening up a promising direction for symmetry detection in the wild. The new benchmark, named Sym-PASCAL, spans challenges including object diversity, multi-objects, part-invisibility, and various complex backgrounds that are far beyond those in existing datasets. The proposed symmetry detection approach, named Side-output Residual Network (SRN), leverages output Residual Units (RUs) to fit the errors between the object symmetry groundtruth and the outputs of RUs. By stacking RUs in a deep-to-shallow manner, SRN exploits the \u2018flow\u2019 of errors among multiple scales to ease the problems of fitting complex outputs with limited layers, suppressing the complex backgrounds, and effectively matching object symmetry of different scales. Experimental results validate both the benchmark and its challenging aspects related to realworld images, and the state-of-the-art performance of our symmetry detection approach."}}
{"id": "U-iXJJKz4H", "cdate": 1582432319256, "mdate": null, "content": {"title": "Weakly Supervised Instance Segmentation using Class Peak Response", "abstract": "Weakly supervised instance segmentation with imagelevel labels, instead of expensive pixel-level masks, remains unexplored. In this paper, we tackle this challenging problem by exploiting class peak responses to enable a classification network for instance mask extraction. With image labels supervision only, CNN classifiers in a fully convolutional manner can produce class response maps, which specify classification confidence at each image location. We observed that local maximums, i.e., peaks, in a class response map typically correspond to strong visual cues residing inside an instance. Motivated by this, we first design a process to stimulate peaks to emerge from a class response map. The emerged peaks are then back-propagated and effectively mapped to highly informative regions of each object instance, such as instance boundaries. We refer to the above maps generated from class peak responses as Peak Response Maps (PRMs). PRMs provide a fine-detailed instance-level representation, which allows instance masks to be extracted even with some off-the-shelf methods. To the best of our knowledge, the proposed method is the first reported image-level supervised instance segmentation technique. Extensive experiments show that our method also significantly boosts weakly supervised pointwise localization as well as semantic segmentation performance, and reports state-of-the-art results on popular benchmarks, including PASCAL VOC12 and MS COCO. "}}
