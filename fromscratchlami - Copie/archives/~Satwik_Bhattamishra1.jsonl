{"id": "I3AUIM2zQ7C", "cdate": 1640995200000, "mdate": 1665157699251, "content": {"title": "Revisiting the Compositional Generalization Abilities of Neural Sequence Models", "abstract": ""}}
{"id": "H4CrwQvvYP", "cdate": 1640995200000, "mdate": 1672143855129, "content": {"title": "Simplicity Bias in Transformers and their Ability to Learn Sparse Boolean Functions", "abstract": "Despite the widespread success of Transformers on NLP tasks, recent works have found that they struggle to model several formal languages when compared to recurrent models. This raises the question of why Transformers perform well in practice and whether they have any properties that enable them to generalize better than recurrent models. In this work, we conduct an extensive empirical study on Boolean functions to demonstrate the following: (i) Random Transformers are relatively more biased towards functions of low sensitivity. (ii) When trained on Boolean functions, both Transformers and LSTMs prioritize learning functions of low sensitivity, with Transformers ultimately converging to functions of lower sensitivity. (iii) On sparse Boolean functions which have low sensitivity, we find that Transformers generalize near perfectly even in the presence of noisy labels whereas LSTMs overfit and achieve poor generalization accuracy. Overall, our results provide strong quantifiable evidence that suggests differences in the inductive biases of Transformers and recurrent models which may help explain Transformer's effective generalization performance despite relatively limited expressiveness."}}
{"id": "3YWONGoqqv8", "cdate": 1609459200000, "mdate": 1626450094192, "content": {"title": "Are NLP Models really able to Solve Simple Math Word Problems?", "abstract": "Arkil Patel, Satwik Bhattamishra, Navin Goyal. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021."}}
{"id": "e-twpMCRFXz", "cdate": 1594570677995, "mdate": null, "content": {"title": "On the Practical Ability of Recurrent Neural Networks to Recognize Hierarchical Languages", "abstract": "While recurrent models have been effective in NLP tasks, their performance on context-free languages (CFLs) has been found to be quite weak. Given that CFLs are believed to capture important phenomena such as hierarchical structure in natural languages, this discrepancy in performance calls for an explanation. We study the performance of recurrent models on Dyck-n languages, a particularly important and well-studied class of CFLs. We find that while recurrent models generalize nearly perfectly if the lengths of the training and test strings are from the same range, they perform poorly if the test strings are longer. At the same time, we observe that recurrent models are expressive enough to recognize Dyck words of arbitrary lengths in finite precision if their depths are bounded. Hence, we evaluate our models on samples generated from Dyck languages with bounded depth and find that they are indeed able to generalize to much higher lengths. Since natural language datasets have nested dependencies of bounded depth,\nthis may help explain why they perform well in modeling hierarchical dependencies in natural language data despite prior works indicating poor generalization performance on Dyck languages. We perform probing studies to support our results and provide comparisons with Transformers."}}
{"id": "yyffl6d3byq", "cdate": 1594570298112, "mdate": null, "content": {"title": "On the Ability and Limitations of Transformers to Recognize Formal Languages", "abstract": "Transformers have supplanted recurrent models in a large number of NLP tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that LSTMs generalize very well on regular languages and have close connections with counter languages. In this work, we systematically study the ability of Transformers to model such languages as well as the role of its individual components in doing so. We first provide a construction of Transformers for a subclass of counter languages, including well-studied languages such as n-ary Boolean Expressions, Dyck-1, and its generalizations. In experiments, we find that Transformers do well on this subclass, and their learned mechanism strongly correlates with our construction. Perhaps surprisingly, in contrast to LSTMs, Transformers do well only on a subset of regular languages with degrading performance as we make languages more complex according to a well-known measure of complexity. Our analysis also provides insights on the role of self-attention mechanism in modeling certain behaviors and the influence of positional encoding schemes on the learning and generalization abilities of the model. "}}
{"id": "iUA9qlxOyyq", "cdate": 1577836800000, "mdate": 1626450094173, "content": {"title": "On the Computational Power of Transformers and Its Implications in Sequence Modeling", "abstract": "Satwik Bhattamishra, Arkil Patel, Navin Goyal. Proceedings of the 24th Conference on Computational Natural Language Learning. 2020."}}
{"id": "aJvkp6fW9Wk", "cdate": 1577836800000, "mdate": 1632920659348, "content": {"title": "On the Practical Ability of Recurrent Neural Networks to Recognize Hierarchical Languages", "abstract": "While recurrent models have been effective in NLP tasks, their performance on context-free languages (CFLs) has been found to be quite weak. Given that CFLs are believed to capture important phenomena such as hierarchical structure in natural languages, this discrepancy in performance calls for an explanation. We study the performance of recurrent models on Dyck-n languages, a particularly important and well-studied class of CFLs. We find that while recurrent models generalize nearly perfectly if the lengths of the training and test strings are from the same range, they perform poorly if the test strings are longer. At the same time, we observe that recurrent models are expressive enough to recognize Dyck words of arbitrary lengths in finite precision if their depths are bounded. Hence, we evaluate our models on samples generated from Dyck languages with bounded depth and find that they are indeed able to generalize to much higher lengths. Since natural language datasets have nested dependencies of bounded depth, this may help explain why they perform well in modeling hierarchical dependencies in natural language data despite prior works indicating poor generalization performance on Dyck languages. We perform probing studies to support our results and provide comparisons with Transformers."}}
{"id": "96in-S-ir8w", "cdate": 1577836800000, "mdate": 1632920659341, "content": {"title": "On the Ability of Self-Attention Networks to Recognize Counter Languages", "abstract": "Transformers have supplanted recurrent models in a large number of NLP tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that LSTMs generalize very well on regular languages and have close connections with counter languages. In this work, we systematically study the ability of Transformers to model such languages as well as the role of its individual components in doing so. We first provide a construction of Transformers for a subclass of counter languages, including well-studied languages such as n-ary Boolean Expressions, Dyck-1, and its generalizations. In experiments, we find that Transformers do well on this subclass, and their learned mechanism strongly correlates with our construction. Perhaps surprisingly, in contrast to LSTMs, Transformers do well only on a subset of regular languages with degrading performance as we make languages more complex according to a well-known measure of complexity. Our analysis also provides insights on the role of self-attention mechanism in modeling certain behaviors and the influence of positional encoding schemes on the learning and generalization abilities of the model."}}
{"id": "3s4-IZBALGm", "cdate": 1577836800000, "mdate": 1632920659354, "content": {"title": "On the Practical Ability of Recurrent Neural Networks to Recognize Hierarchical Languages", "abstract": "Satwik Bhattamishra, Kabir Ahuja, Navin Goyal. Proceedings of the 28th International Conference on Computational Linguistics. 2020."}}
{"id": "2EJoaiYS9kj", "cdate": 1577836800000, "mdate": 1632920659340, "content": {"title": "On the Ability and Limitations of Transformers to Recognize Formal Languages", "abstract": "Satwik Bhattamishra, Kabir Ahuja, Navin Goyal. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020."}}
