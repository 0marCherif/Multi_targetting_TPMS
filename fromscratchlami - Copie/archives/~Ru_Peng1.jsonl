{"id": "ztkUF_MQj7J", "cdate": 1663850256037, "mdate": null, "content": {"title": "Test-Time AutoEval with Supporting Self-supervision", "abstract": "The Automatic Model Evaluation (AutoEval) framework entertains the possibility of evaluating a trained machine learning model without resorting to a labeled testing set, which commonly isn\u2019t accessible nor provided in real-world scenarios. Existing AutoEval methods always rely on computing distribution shift between the unlabelled testing set and the training set. However, this lines of work cannot fit well in some real-world ML applications like edge computing boxes where the original training set is inaccessible. Contrastive Learning (CL) is an efficient self-supervised learning task, which can learn helpful visual representations for down-stream classification tasks. In our work, we surprisingly find that CL accuracy and classification accuracy can build strong linear correlation ($r > 0.88$). This finding motivates us to regress classification accuracy with CL accuracy. In our experiments, we show that without touching training sets, our framework can achieve results comparable to SOTA AutoEval baselines. Besides, our subsequent experiments demonstrate that different CL approaches and model structures can easily fit into our framework."}}
{"id": "VBIM27myY0Q", "cdate": 1640995200000, "mdate": 1667635134344, "content": {"title": "Distill the Image to Nowhere: Inversion Knowledge Distillation for Multimodal Machine Translation", "abstract": "Past works on multimodal machine translation (MMT) elevate bilingual setup by incorporating additional aligned vision information. However, an image-must requirement of the multimodal dataset largely hinders MMT's development -- namely that it demands an aligned form of [image, source text, target text]. This limitation is generally troublesome during the inference phase especially when the aligned image is not provided as in the normal NMT setup. Thus, in this work, we introduce IKD-MMT, a novel MMT framework to support the image-free inference phase via an inversion knowledge distillation scheme. In particular, a multimodal feature generator is executed with a knowledge distillation module, which directly generates the multimodal feature from (only) source texts as the input. While there have been a few prior works entertaining the possibility to support image-free inference for machine translation, their performances have yet to rival the image-must translation. In our experiments, we identify our method as the first image-free approach to comprehensively rival or even surpass (almost) all image-must frameworks, and achieved the state-of-the-art result on the often-used Multi30k benchmark. Our code and data are available at: https://github.com/pengr/IKD-mmt/tree/master.."}}
{"id": "OtVYBFDTPzk", "cdate": 1640995200000, "mdate": 1681800060428, "content": {"title": "Distill The Image to Nowhere: Inversion Knowledge Distillation for Multimodal Machine Translation", "abstract": ""}}
{"id": "CZbW1-Duz3m", "cdate": 1640995200000, "mdate": 1667635134346, "content": {"title": "HybridVocab: Towards Multi-Modal Machine Translation via Multi-Aspect Alignment", "abstract": "Multi-modal machine translation (MMT) aims to augment the linguistic machine translation frameworks by incorporating aligned vision information. As the core research challenge for MMT, how to fuse the image information and further align it with the bilingual data remains critical. Existing works have either focused on a methodological alignment in the space of bilingual text or emphasized the combination of the one-sided text and given image. In this work, we entertain the possibility of a triplet alignment, among the source and target text together with the image instance. In particular, we propose Multi-aspect AlignmenT (MAT) model that augments the MMT tasks to three sub-tasks --- namely cross-language translation alignment, cross-modal captioning alignment and multi-modal hybrid alignment tasks. Core to this model consists of a hybrid vocabulary which compiles the visually depictable entity (nouns) occurrence on both sides of the text as well as the detected object labels appearing in the images. Through this sub-task, we postulate that MAT manages to further align the modalities by casting three instances into a shared domain, as compared against previously proposed methods. Extensive experiments and analyses demonstrate the superiority of our approaches, which achieve several state-of-the-art results on two benchmark datasets of the MMT task."}}
{"id": "RO3QU2L1rfZ", "cdate": 1609459200000, "mdate": 1681800060424, "content": {"title": "Syntax-aware neural machine translation directed by syntactic dependency degree", "abstract": "There are various ways to incorporate syntax knowledge into neural machine translation (NMT). However, quantifying the dependency syntactic intimacy (DSI) between word pairs in a dependency tree has not being considered to use in attentional and transformer-based NMT. In this paper, we innovatively propose a variant of Tree-LSTM to capture the syntactic dependency degree (SDD) between word pairs in dependency trees. Two syntax-aware distances, including a tuned syntax distance and a $$\\varvec{\\rho }$$ \u03c1 -dependent distance, are proposed. For attentional NMT, two syntax-aware attentions based on two syntax-aware distances are proposed for attentional NMT, and we also design a dual attention to simultaneously generate global context and dependency syntactic context. For transformer-based NMT, we explicitly incorporate the dependency syntax into self-attention network (SAN) to propose a syntax-aware SAN. Experiments on IWSLT\u201917 English\u2013German, IWSLT Chinese\u2013English and WMT\u201915 English\u2013Finnish translation tasks show that our syntax-aware NMT significantly improves translation quality by comparing with baseline methods, even the state-of-the-art transformer-based NMT."}}
{"id": "7fcmcb7nB6", "cdate": 1609459200000, "mdate": 1678930844864, "content": {"title": "Boosting Neural Machine Translation with Dependency-Scaled Self-Attention Network", "abstract": ""}}
{"id": "mbQNDxYq0Ii", "cdate": 1546300800000, "mdate": 1681800060423, "content": {"title": "Neural Machine Translation with Attention Based on a New Syntactic Branch Distance", "abstract": ""}}
