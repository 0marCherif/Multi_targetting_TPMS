{"id": "aP2q6nmqYHS", "cdate": 1694665275694, "mdate": 1694665275694, "content": {"title": "Robust Multi-Agent Reinforcement Learning Considering State Uncertainties", "abstract": "In real-world multi-agent reinforcement learning (MARL) applications, agents may not have perfect state information (e.g., due to inaccurate measurement or malicious attacks), which challenges the robustness of agents\u2019 policies. Though robustness is getting important in MARL deployment, little prior work has studied state uncertainties in MARL, neither in problem formulation nor algorithm design. Motivated by this robustness issue, we study the problem of MARL with state uncertainty in this work. We provide the first attempt to the theoretical and empirical analysis of this challenging problem. We first model the problem as a Markov Game with state perturbation adversaries (MG-SPA), and introduce Robust Equilibrium as the solution concept. We conduct a fundamental analysis regarding MG-SPA and give conditions under which such an equilibrium exists. Then we propose a robust multi-agent Q-learning (RMAQ) algorithm to find such an equilibrium, with convergence guarantees. To handle high-dimensional state-action space, we design a robust multi-agent actor-critic (RMAAC) algorithm based on an analytical expression of the policy gradient derived in the paper. Our experiments show that the proposed RMAQ algorithm converges to the optimal value function; our RMAAC algorithm outperforms several MARL methods that do not consider state uncertainty in several multi-agent environments."}}
{"id": "Rl4ihTreFnV", "cdate": 1663849923937, "mdate": null, "content": {"title": "Robust Multi-Agent Reinforcement Learning with State Uncertainties", "abstract": "In real-world multi-agent reinforcement learning (MARL) applications, agents may not have perfect state information (e.g., due to inaccurate measurement or malicious attacks), which challenges the robustness of agents' policies. Though robustness is getting important in MARL deployment, little prior work has studied state uncertainties in MARL, neither in problem formulation nor algorithm design. Motivated by this robustness issue, we study the problem of MARL with state uncertainty in this work. We provide the first attempt to the theoretical and empirical analysis of this challenging problem. We first model the problem as a Markov Game with state perturbation adversaries (MG-SPA), and introduce Robust Equilibrium as the solution concept. We conduct fundamental analysis regarding MG-SPA and give conditions under which such an equilibrium exists. Then we propose a robust multi-agent Q-learning (RMAQ) algorithm to find such an equilibrium, with convergence guarantees. To handle high-dimensional state-action space, we design a robust multi-agent actor-critic (RMAAC) algorithm based on an analytical expression of the policy gradient derived in the paper. Our experiments show that the proposed RMAQ algorithm converges to the optimal value function; our RMAAC algorithm outperforms several MARL methods that do not consider the state uncertainty in several multi-agent environments."}}
{"id": "BJEqxTZOWr", "cdate": 1514764800000, "mdate": null, "content": {"title": "Variational Autoencoder for Low Bit-rate Image Compression", "abstract": "We present an end-to-end trainable image compression frameworkforlowbit-rateimagecompression. Ourmethod is based on variational autoencoder, which consists of a nonlinear encoder transformation, a uniform quantizer, a nonlinear decoder transformation and a post-processing module. The prior probability of compressed representation is modeled by a Laplacian distribution using a hyperprior autoencoder and it is trained jointly with the transformation autoencoder. In order to remove the compression artifacts and blurs for low bit-rate images, an effective convolution based post-processing module is proposed. Finally,aratecontrolalgorithmisappliedtoallocatethebits adaptively for each image, considering the bits constraint of the challenge. Across the experimental results on validation and test sets, the optimized framework trained by perceptual loss generates the best performance in terms of MS-SSIM. The results also indicate that the proposed postprocessing module can improve compression performance for both deep learning based and traditional methods, with the highest PSNR as 32.09 at the bit-rate of 0.15"}}
