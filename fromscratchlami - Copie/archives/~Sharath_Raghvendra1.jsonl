{"id": "aMXD8gqsIiC", "cdate": 1663850326615, "mdate": null, "content": {"title": "A Higher Precision Algorithm for Computing the $1$-Wasserstein Distance", "abstract": "We consider the problem of computing the $1$-Wasserstein distance $\\mathcal{W}(\\mu,\\nu)$ between two $d$-dimensional discrete distributions $\\mu$ and $\\nu$ whose support lie within the unit hypercube. There are several algorithms that estimate $\\mathcal{W}(\\mu,\\nu)$ within an additive error of $\\varepsilon$. However, when $\\mathcal{W}(\\mu,\\nu)$ is small, the additive error $\\varepsilon$ dominates, leading to noisy results. Consider any additive approximation algorithm with execution time $T(n,\\varepsilon)$. We propose an algorithm that runs in $O(T(n,\\varepsilon/d) \\log n)$ time and boosts the accuracy of estimating $\\mathcal{W}(\\mu,\\nu)$ from $\\varepsilon$ to an expected additive error of $\\min\\{\\varepsilon, (d\\log_{\\sqrt{d}/\\varepsilon} n)\\mathcal{W}(\\mu,\\nu)\\}$. For the special case where every point in the support of $\\mu$ and $\\nu$ has a mass of $1/n$ (also called the Euclidean Bipartite Matching problem), we describe an algorithm to boost the accuracy of any additive approximation algorithm from $\\varepsilon$ to an expected additive error of $\\min\\{\\varepsilon, (d\\log\\log n)\\mathcal{W}(\\mu,\\nu)\\}$ in $O(T(n, \\varepsilon/d)\\log\\log n)$ time. "}}
{"id": "gwcQajoXNF", "cdate": 1663850323109, "mdate": null, "content": {"title": "Computing all Optimal Partial Transports", "abstract": "We consider the classical version of the optimal partial transport problem. Let $\\mu$ (with a mass of $U$) and $\\nu$ (with a mass of $S$) be two discrete mass distributions with $S \\le U$ and let $n$ be the total number of points in the supports of $\\mu$ and $\\nu$. For a parameter $\\alpha \\in [0,S]$, consider the minimum-cost transport plan  $\\sigma_\\alpha$ that transports a mass of $\\alpha$ from $\\nu$ to $\\mu$. An \\emph{OT-profile} captures the behavior of the cost of $\\sigma_\\alpha$ as $\\alpha$ varies from $0$ to $S$. There is only limited work on OT-profile and its mathematical properties (see~\\cite{figalli2010optimal}). In this paper, we present a novel framework to analyze the properties of the OT-profile and also present an algorithm to compute it.  When $\\mu$ and $\\nu$ are discrete mass distributions, we show that the OT-profile is a piecewise-linear non-decreasing convex function. Let $K$ be the combinatorial complexity of this function, i.e., the number of line segments required to represent the OT-profile. Our exact algorithm computes the OT-profile in $\\tilde{O}(n^2K)$ time. Given $\\delta > 0$, we also show that the algorithm by ~\\cite{lahn2019graph} can be used to $\\delta$-approximate the OT-profile in $O(n^2/\\delta + n/\\delta^2)$ time. This approximation is a piecewise-linear function of a combinatorial complexity of $O(1/\\delta)$.\nAn OT-profile is arguably more valuable than the OT-cost itself and can be used within applications. Under a reasonable assumption of outliers, we also show that the first derivative of the OT-profile sees a noticeable rise before any of the mass from outliers is transported. By using this property, we get an improved prediction accuracy for an outlier detection experiment. We also use this property to predict labels and estimate the class priors within PU-Learning experiments. Both these experiments are conducted on real datasets."}}
{"id": "smoaxt_pSPU", "cdate": 1640995200000, "mdate": 1675246917391, "content": {"title": "A Push-Relabel Based Additive Approximation for Optimal Transport", "abstract": "Optimal Transport is a popular distance metric for measuring similarity between distributions. Exact algorithms for computing Optimal Transport can be slow, which has motivated the development of approximate numerical solvers (e.g. Sinkhorn method). We introduce a new and very simple combinatorial approach to find an $\\varepsilon$-approximation of the OT distance. Our algorithm achieves a near-optimal execution time of $O(n^2/\\varepsilon^2)$ for computing OT distance and, for the special case of the assignment problem, the execution time improves to $O(n^2/\\varepsilon)$. Our algorithm is based on the push-relabel framework for min-cost flow problems. Unlike the other combinatorial approach (Lahn, Mulchandani and Raghvendra, NeurIPS 2019) which does not have a fast parallel implementation, our algorithm has a parallel execution time of $O(\\log n/\\varepsilon^2)$. Interestingly, unlike the Sinkhorn algorithm, our method also readily provides a compact transport plan as well as a solution to an approximate version of the dual formulation of the OT problem, both of which have numerous applications in Machine Learning. For the assignment problem, we provide both a CPU implementation as well as an implementation that exploits GPU parallelism. Experiments suggest that our algorithm is faster than the Sinkhorn algorithm, both in terms of CPU and GPU implementations, especially while computing matchings with a high accuracy."}}
{"id": "kOPOoDQaYga", "cdate": 1640995200000, "mdate": 1675246917375, "content": {"title": "Deterministic, Near-Linear \u03b5-Approximation Algorithm for Geometric Bipartite Matching", "abstract": "Given point sets $A$ and $B$ in $\\mathbb{R}^d$ where $A$ and $B$ have equal size $n$ for some constant dimension $d$ and a parameter $\\varepsilon>0$, we present the first deterministic algorithm that computes, in $n\\cdot(\\varepsilon^{-1} \\log n)^{O(d)}$ time, a perfect matching between $A$ and $B$ whose cost is within a $(1+\\varepsilon)$ factor of the optimal under any $\\smash{\\ell_p}$-norm. Although a Monte-Carlo algorithm with a similar running time is proposed by Raghvendra and Agarwal [J. ACM 2020], the best-known deterministic $\\varepsilon$-approximation algorithm takes $\\Omega(n^{3/2})$ time. Our algorithm constructs a (refinement of a) tree cover of $\\mathbb{R}^d$, and we develop several new tools to apply a tree-cover based approach to compute an $\\varepsilon$-approximate perfect matching."}}
{"id": "anKI7650RS", "cdate": 1640995200000, "mdate": 1675246917374, "content": {"title": "A Scalable Work Function Algorithm for the k-Server Problem", "abstract": "We provide a novel implementation of the classical Work Function Algorithm (WFA) for the k-server problem. In our implementation, processing a request takes O(n\u00b2+k\u00b2) time per request; where n is the total number of requests and k is the total number of servers. All prior implementations take \u03a9(kn\u00b2 +k\u00b3) time per request. Previous approaches process a request by solving a min-cost flow problem. Instead, we show that processing a request can be reduced to an execution of the Dijkstra\u2019s shortest-path algorithm on a carefully computed weighted graph leading to the speed-up."}}
{"id": "JRpRqDlRYef", "cdate": 1640995200000, "mdate": 1675246917341, "content": {"title": "Deterministic, near-linear \u03b5-approximation algorithm for geometric bipartite matching", "abstract": "Given two point sets A and B in \u211dd of size n each, for some constant dimension d\u2265 1, and a parameter \u03b5>0, we present a deterministic algorithm that computes, in n\u00b7(\u03b5\u22121 logn)O(d) time, a perfect matching between A and B whose cost is within a (1+\u03b5) factor of the optimal matching under any \u2113p-norm. Although a Monte-Carlo algorithm with a similar running time is proposed by Raghvendra and Agarwal\u00a0[J. ACM 2020], the best-known deterministic \u03b5-approximation algorithm takes \u03a9(n3/2) time. Our algorithm constructs a (refinement of a) tree cover of \u211dd, and we develop several new tools to apply a tree-cover based approach to compute an \u03b5-approximate perfect matching."}}
{"id": "-qdXxxaXgm", "cdate": 1640995200000, "mdate": 1675246917350, "content": {"title": "An Improved \u03b5-Approximation Algorithm for Geometric Bipartite Matching", "abstract": "For two point sets A, B \u2282 \u211d^d, with |A| = |B| = n and d > 1 a constant, and for a parameter \u03b5 > 0, we present a randomized algorithm that, with probability at least 1/2, computes in O(n(\u03b5^{-O(d\u00b3)}log log n + \u03b5^{-O(d)}log\u2074 nlog\u2075log n)) time, an \u03b5-approximate minimum-cost perfect matching under any L_p-metric. All previous algorithms take n(\u03b5^{-1}log n)^{\u03a9(d)} time. We use a randomly-shifted tree, with a polynomial branching factor and O(log log n) height, to define a tree-based distance function that \u03b5-approximates the L_p metric as well as to compute the matching hierarchically. Then, we apply the primal-dual framework on a compressed representation of the residual graph to obtain an efficient implementation of the Hungarian-search and augment operations."}}
{"id": "FM8auLVlRMo", "cdate": 1621630205921, "mdate": null, "content": {"title": "A Faster Maximum Cardinality Matching Algorithm with Applications in Machine Learning", "abstract": "Maximum cardinality bipartite matching is an important graph optimization problem with several applications. For instance, maximum cardinality matching in a $\\delta$-disc graph can be used in the computation of the bottleneck matching as well as the $\\infty$-Wasserstein and the L\u00e9vy-Prokhorov distances between probability distributions. For any point sets $A, B \\subset \\mathbb{R}^2$, the $\\delta$-disc graph is a bipartite graph formed by connecting every pair of points $(a,b) \\in A\\times B$ by an edge if the Euclidean distance between them is at most $\\delta$. Using the classical Hopcroft-Karp algorithm, a maximum-cardinality matching on any $\\delta$-disc graph can be found in $\\tilde{O}(n^{3/2})$ time.~\\footnote{We use $\\tilde{O}(\\cdot)$ to suppress poly-logarithmic terms in the complexity.} In this paper, we present a simplification of a recent algorithm (Lahn and Raghvendra, JoCG 2021) for the maximum cardinality matching problem and describe how a maximum cardinality matching in a $\\delta$-disc graph can be computed asymptotically faster than $O(n^{3/2})$ time for any moderately dense point set. As applications, we show that if $A$ and $B$ are point sets drawn uniformly at random from a unit square, an exact bottleneck matching can be computed in $\\tilde{O}(n^{4/3})$ time. On the other hand, experiments suggest that the Hopcroft-Karp algorithm seems to take roughly $\\Theta (n^{3/2})$ time for this case. This translates to substantial improvements in execution time for larger inputs."}}
{"id": "feICTiwrum9", "cdate": 1609459200000, "mdate": 1675246917460, "content": {"title": "Improved Approximate Rips Filtrations with Shifted Integer Lattices and Cubical Complexes", "abstract": "Rips complexes are important structures for analyzing topological features of metric spaces. Unfortunately, generating these complexes is expensive because of a combinatorial explosion in the complex size. For $n$ points in $\\mathbb{R}^d$, we present a scheme to construct a $2$-approximation of the filtration of the Rips complex in the $L_\\infty$-norm, which extends to a $2d^{0.25}$-approximation in the Euclidean case. The $k$-skeleton of the resulting approximation has a total size of $n2^{O(d\\log k +d)}$. The scheme is based on the integer lattice and simplicial complexes based on the barycentric subdivision of the $d$-cube. We extend our result to use cubical complexes in place of simplicial complexes by introducing cubical maps between complexes. We get the same approximation guarantee as the simplicial case, while reducing the total size of the approximation to only $n2^{O(d)}$ (cubical) cells. There are two novel techniques that we use in this paper. The first is the use of acyclic carriers for proving our approximation result. In our application, these are maps which relate the Rips complex and the approximation in a relatively simple manner and greatly reduce the complexity of showing the approximation guarantee. The second technique is what we refer to as scale balancing, which is a simple trick to improve the approximation ratio under certain conditions."}}
{"id": "X5MjJDSsEb", "cdate": 1609459200000, "mdate": 1675246917374, "content": {"title": "A Faster Maximum Cardinality Matching Algorithm with Applications in Machine Learning", "abstract": "Maximum cardinality bipartite matching is an important graph optimization problem with several applications. For instance, maximum cardinality matching in a $\\delta$-disc graph can be used in the computation of the bottleneck matching as well as the $\\infty$-Wasserstein and the L\u00e9vy-Prokhorov distances between probability distributions. For any point sets $A, B \\subset \\mathbb{R}^2$, the $\\delta$-disc graph is a bipartite graph formed by connecting every pair of points $(a,b) \\in A\\times B$ by an edge if the Euclidean distance between them is at most $\\delta$. Using the classical Hopcroft-Karp algorithm, a maximum-cardinality matching on any $\\delta$-disc graph can be found in $\\tilde{O}(n^{3/2})$ time.~\\footnote{We use $\\tilde{O}(\\cdot)$ to suppress poly-logarithmic terms in the complexity.} In this paper, we present a simplification of a recent algorithm (Lahn and Raghvendra, JoCG 2021) for the maximum cardinality matching problem and describe how a maximum cardinality matching in a $\\delta$-disc graph can be computed asymptotically faster than $O(n^{3/2})$ time for any moderately dense point set. As applications, we show that if $A$ and $B$ are point sets drawn uniformly at random from a unit square, an exact bottleneck matching can be computed in $\\tilde{O}(n^{4/3})$ time. On the other hand, experiments suggest that the Hopcroft-Karp algorithm seems to take roughly $\\Theta (n^{3/2})$ time for this case. This translates to substantial improvements in execution time for larger inputs."}}
