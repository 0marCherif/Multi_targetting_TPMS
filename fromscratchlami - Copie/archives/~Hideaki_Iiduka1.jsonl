{"id": "mZo5P7bRVf", "cdate": 1672531200000, "mdate": 1673274315641, "content": {"title": "Global convergence of Hager-Zhang type Riemannian conjugate gradient method", "abstract": ""}}
{"id": "p6qlG1zXs9v", "cdate": 1663850114165, "mdate": null, "content": {"title": "Critical Batch Size Minimizes Stochastic First-Order Oracle Complexity of Deep Learning Optimizer using Hyperparameters Close to One", "abstract": "Practical results have shown that deep learning optimizers using small constant learning rates, hyperparameters close to one, and large batch sizes can find the model parameters of deep neural networks that minimize the loss functions. We first show theoretical evidence that the momentum method (Momentum) and adaptive moment estimation (Adam) perform well in the sense that the upper bound of the theoretical performance measure is small with a small constant learning rate, hyperparameters close to one, and a large batch size.  Next, we show that there exists a batch size called the critical batch size minimizing the stochastic first-order oracle (SFO) complexity, which is the stochastic gradient computation cost, and that SFO complexity increases once the batch size exceeds the critical batch size. Finally, we provide numerical results that support our theoretical results. That is, the numerical results indicate that Adam using a small constant learning rate, hyperparameters close to one, and the critical batch size minimizing SFO complexity has faster convergence than Momentum and stochastic gradient descent (SGD)."}}
{"id": "oi2yCq6VhPJ", "cdate": 1640995200000, "mdate": 1673274315748, "content": {"title": "Using Constant Learning Rate of Two Time-Scale Update Rule for Training Generative Adversarial Networks", "abstract": ""}}
{"id": "htUGABzLBX", "cdate": 1640995200000, "mdate": 1673274315567, "content": {"title": "Critical Bach Size Minimizes Stochastic First-Order Oracle Complexity of Deep Learning Optimizer using Hyperparameters Close to One", "abstract": ""}}
{"id": "GGiEe669A7", "cdate": 1640995200000, "mdate": 1673274315639, "content": {"title": "Riemannian stochastic fixed point optimization algorithm", "abstract": ""}}
{"id": "Egw83kxZa1Q", "cdate": 1640995200000, "mdate": 1673274315744, "content": {"title": "Theoretical analysis of Adam using hyperparameters close to one without Lipschitz smoothness", "abstract": ""}}
{"id": "DSlv2fBNMp", "cdate": 1640995200000, "mdate": 1673274315800, "content": {"title": "Conjugate Gradient Method for Generative Adversarial Networks", "abstract": ""}}
{"id": "7W6fB2_3mB8", "cdate": 1640995200000, "mdate": 1673274315624, "content": {"title": "Riemannian Adaptive Optimization Algorithm and its Application to Natural Language Processing", "abstract": ""}}
{"id": "2SKqtnmbE-", "cdate": 1640995200000, "mdate": 1673274315592, "content": {"title": "Appropriate Learning Rates of Adaptive Learning Rate Optimization Algorithms for Training Deep Neural Networks", "abstract": ""}}
{"id": "EhdacditHf9", "cdate": 1632875422582, "mdate": null, "content": {"title": "The Number of Steps Needed for Nonconvex Optimization of a Deep Learning Optimizer is a Rational Function of Batch Size", "abstract": "Recently, convergence as well as convergence rate analyses of deep learning optimizers for nonconvex optimization have been widely studied. Meanwhile, numerical evaluations for the optimizers have precisely clarified the relationship between batch size and the number of steps needed for training deep neural networks. The main contribution of this paper is to show theoretically that the number of steps needed for nonconvex optimization of each of the optimizers can be expressed as a rational function of batch size. Having these rational functions leads to two particularly important facts, which were validated numerically in previous studies. The first fact is that there exists an optimal batch size such that the number of steps needed for nonconvex optimization is minimized. This implies that using larger batch sizes than the optimal batch size does not decrease the number of steps needed for nonconvex optimization. The second fact is that the optimal batch size depends on the optimizer. In particular, it is shown theoretically that momentum and Adam-type optimizers can exploit larger optimal batches and further reduce the minimum number of steps needed for nonconvex optimization than can the stochastic gradient descent optimizer."}}
