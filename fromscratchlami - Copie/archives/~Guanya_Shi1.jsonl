{"id": "XEw-cnNsr6", "cdate": 1686324883376, "mdate": null, "content": {"title": "DATT: Deep Adaptive Trajectory Tracking for Quadrotor Control", "abstract": "Precise arbitrary trajectory tracking for quadrotors is challenging due to unknown nonlinear dynamics, trajectory infeasibility, and actuation limits. To tackle these challenges, we present DATT, a learning-based approach that can precisely track arbitrary, potentially infeasible trajectories in the presence of large disturbances in the real world. DATT builds on a novel feedforward-feedback-adaptive control structure trained in simulation using reinforcement learning. When deployed on real hardware, DATT is augmented with a disturbance estimator using $\\mathcal{L}_1$ adaptive control in closed-loop, without any fine-tuning. DATT significantly outperforms competitive adaptive nonlinear and model predictive controllers for both feasible smooth and infeasible trajectories in unsteady wind fields, including challenging scenarios where baselines completely fail. Moreover, DATT can efficiently run online with an inference time less than 3.2ms, less than 1/4 of the adaptive nonlinear model predictive control baseline."}}
{"id": "MnANx01rV2w", "cdate": 1686324872162, "mdate": null, "content": {"title": "CAJun: Continuous Adaptive Jumping using a Learned Centroidal Controller", "abstract": "We present CAJun, a novel hierarchical learning and control framework that enables legged robots to jump continuously with adaptive jumping distances. CAJun consists of a high-level centroidal policy and a low-level leg controller. In particular, we use reinforcement learning (RL) to train the centroidal policy, which specifies the gait timing, base velocity, and swing foot position for the leg controller. The leg controller optimizes motor commands for the swing and stance legs according to the gait timing to track the swing foot target and base velocity commands.% using optimal control. Additionally, we reformulate the stance leg optimizer in the leg controller to speed up policy training by an order of magnitude. Our system combines the versatility of learning with the robustness of optimal control.\n% By combining RL with optimal control methods, our system achieves the versatility of learning while enjoys the robustness from control methods.% making it easily transferable to real robots. We show that after 20 minutes of training on a single GPU, CAJun can achieve continuous, long jumps with adaptive distances on a Go1 robot with small sim-to-real gaps. Moreover, the robot can jump across gaps with a maximum width of 70cm, which is over 40% wider than existing methods."}}
{"id": "LZ4lzQ_YAo0", "cdate": 1681540897727, "mdate": null, "content": {"title": "Pretraining Neural-Networks with Neural-Fly for Rapid Online Learning", "abstract": "Executing safe and precise flight maneuvers in\ndynamic high-speed winds is important for the ongoing commodi-\ntization of uninhabited aerial vehicles (UAVs). However, since the\nrelationship between various wind conditions and its effect on\naircraft maneuverability is not well understood, it is challenging\nto design effective robot controllers using traditional control de-\nsign methods. We present Neural-Fly, a learning-based approach\nthat allows rapid online adaptation by incorporating pre-trained\nrepresentations through deep learning. Neural-Fly builds on two\nkey observations that aerodynamics in different wind conditions\nshare a common representation and that the wind-specific part\nlies in a low-dimensional space. To that end, Neural-Fly uses\na proposed learning algorithm, Domain Adversarially Invariant\nMeta-Learning (DAIML), to learn the shared representation,\nonly using 12 minutes of flight data. This pretraining phase\nenables rapid online learning through a composite adaptation law,\nwhich only needs to update a set of linear coefficients for mixing\nthe basis elements to effectively correct for the wind effects.\nWhen evaluated under challenging wind conditions generated\nwith the Caltech Real Weather Wind Tunnel with wind speeds\nup to 43.6 km/h (12.1 m/s), Neural-Fly achieves precise flight\ncontrol with substantially smaller tracking error than state-\nof-the-art nonlinear and adaptive controllers. In addition to\nstrong empirical performance, the exponential stability of Neural-\nFly results in robustness guarantees. Finally, our control design\nextrapolates to unseen wind conditions, is shown to be effective\nfor outdoor flights with only on-board sensors, and can transfer\nacross drones with minimal performance degradation."}}
{"id": "sGUet7yVVgn", "cdate": 1621630163922, "mdate": null, "content": {"title": "Meta-Adaptive Nonlinear Control: Theory and Algorithms", "abstract": "We present an online multi-task learning approach for adaptive nonlinear control, which we call Online Meta-Adaptive Control (OMAC). The goal is to control a nonlinear system subject to adversarial disturbance and unknown \\emph{environment-dependent} nonlinear dynamics, under the assumption that the environment-dependent dynamics can be well captured with some shared representation. Our approach is motivated by robot control, where a robotic system encounters a sequence of new environmental conditions that it must quickly adapt to. A key emphasis is to integrate online representation learning with established methods from control theory, in order to arrive at a unified framework that yields both control-theoretic and learning-theoretic guarantees. We provide instantiations of our approach under varying conditions, leading to the first non-asymptotic end-to-end convergence guarantee for multi-task nonlinear control. OMAC can also be integrated with deep representation learning. Experiments show that OMAC significantly outperforms conventional adaptive control approaches which do not learn the shared representation, in inverted pendulum and 6-DoF drone control tasks under varying wind conditions."}}
{"id": "nm3sOq42Gmx", "cdate": 1621630163922, "mdate": null, "content": {"title": "Meta-Adaptive Nonlinear Control: Theory and Algorithms", "abstract": "We present an online multi-task learning approach for adaptive nonlinear control, which we call Online Meta-Adaptive Control (OMAC). The goal is to control a nonlinear system subject to adversarial disturbance and unknown \\emph{environment-dependent} nonlinear dynamics, under the assumption that the environment-dependent dynamics can be well captured with some shared representation. Our approach is motivated by robot control, where a robotic system encounters a sequence of new environmental conditions that it must quickly adapt to. A key emphasis is to integrate online representation learning with established methods from control theory, in order to arrive at a unified framework that yields both control-theoretic and learning-theoretic guarantees. We provide instantiations of our approach under varying conditions, leading to the first non-asymptotic end-to-end convergence guarantee for multi-task nonlinear control. OMAC can also be integrated with deep representation learning. Experiments show that OMAC significantly outperforms conventional adaptive control approaches which do not learn the shared representation, in inverted pendulum and 6-DoF drone control tasks under varying wind conditions."}}
{"id": "xwGeq7I4Opv", "cdate": 1621629887905, "mdate": null, "content": {"title": "Perturbation-based Regret Analysis of Predictive Control in Linear Time Varying Systems", "abstract": "We study predictive control in a setting where the dynamics are time-varying and linear, and the costs are time-varying and well-conditioned. At each time step, the controller receives the exact predictions of costs, dynamics, and disturbances for the future $k$ time steps. We show that when the prediction window $k$ is sufficiently large, predictive control is input-to-state stable and achieves a dynamic regret of $O(\\lambda^k T)$, where $\\lambda < 1$ is a positive constant. This is the first dynamic regret bound on the predictive control of linear time-varying systems. We also show a variation of predictive control obtains the first competitive bound for the control of linear time-varying systems:  $1 + O(\\lambda^k)$. Our results are derived using a novel proof framework based on a perturbation bound that characterizes how a small change to the system parameters impacts the optimal trajectory."}}
{"id": "PdxyQilsUrG", "cdate": 1591623853140, "mdate": null, "content": {"title": "Robust Regression for Safe Exploration in Control", "abstract": "We study the problem of safe learning and exploration in sequential control problems. The goal is to safely collect data samples from operating in an environment, in order to learn to achieve a challenging control goal (e.g., an agile maneuver close to a boundary).  A central challenge in this setting is how to quantify uncertainty in order to choose provably-safe actions that allow us to collect informative data and reduce uncertainty, thereby achieving both improved controller safety and optimality. To address this challenge, we present a deep robust regression model that is trained to directly predict the uncertainty bounds for safe exploration. We derive generalization bounds for learning and connect them with safety and stability bounds in control. We demonstrate empirically that our robust regression approach can outperform the conventional Gaussian process (GP) based safe exploration in settings where it is difficult to specify a good GP prior."}}
{"id": "t_IfFp-KSuB", "cdate": 1577836800000, "mdate": null, "content": {"title": "Beyond No-Regret: Competitive Control via Online Optimization with Memory", "abstract": "This paper presents competitive algorithms for a novel class of online optimization problems with memory. We consider a setting where the learner seeks to minimize the sum of a hitting cost and a switching cost that depends on the previous $p$ decisions. This setting generalizes Smoothed Online Convex Optimization. The proposed approach, Optimistic Regularized Online Balanced Descent, achieves a constant, dimension-free competitive ratio. Further, we show a connection between online optimization with memory and online control with adversarial disturbances. This connection, in turn, leads to a new constant-competitive policy for a rich class of online control problems."}}
{"id": "VepJcPhuRUk", "cdate": 1577836800000, "mdate": null, "content": {"title": "Chance-Constrained Trajectory Optimization for Safe Exploration and Learning of Nonlinear Systems", "abstract": "Learning-based control algorithms require data collection with abundant supervision for training. Safe exploration algorithms ensure the safety of this data collection process even when only partial knowledge is available. We present a new approach for optimal motion planning with safe exploration that integrates chance-constrained stochastic optimal control with dynamics learning and feedback control. We derive an iterative convex optimization algorithm that solves an \\underline{Info}rmation-cost \\underline{S}tochastic \\underline{N}onlinear \\underline{O}ptimal \\underline{C}ontrol problem (Info-SNOC). The optimization objective encodes control cost for performance and exploration cost for learning, and the safety is incorporated as distributionally robust chance constraints. The dynamics are predicted from a robust regression model that is learned from data. The Info-SNOC algorithm is used to compute a sub-optimal pool of safe motion plans that aid in exploration for learning unknown residual dynamics under safety constraints. A stable feedback controller is used to execute the motion plan and collect data for model learning. We prove the safety of rollout from our exploration method and reduction in uncertainty over epochs, thereby guaranteeing the consistency of our learning method. We validate the effectiveness of Info-SNOC by designing and implementing a pool of safe trajectories for a planar robot. We demonstrate that our approach has higher success rate in ensuring safety when compared to a deterministic trajectory optimization approach."}}
{"id": "4B70Vl6AomW", "cdate": 1577836800000, "mdate": null, "content": {"title": "Neural-Swarm: Decentralized Close-Proximity Multirotor Control Using Learned Interactions", "abstract": "In this paper, we present Neural-Swarm, a nonlinear decentralized stable controller for close-proximity flight of multirotor swarms. Close-proximity control is challenging due to the complex aerodynamic interaction effects between multirotors, such as downwash from higher vehicles to lower ones. Conventional methods often fail to properly capture these interaction effects, resulting in controllers that must maintain large safety distances between vehicles, and thus are not capable of close-proximity flight. Our approach combines a nominal dynamics model with a regularized permutation-invariant Deep Neural Network (DNN) that accurately learns the high-order multi-vehicle interactions. We design a stable nonlinear tracking controller using the learned model. Experimental results demonstrate that the proposed controller significantly outperforms a baseline nonlinear tracking controller with up to four times smaller worst-case height tracking errors. We also empirically demonstrate the ability of our learned model to generalize to larger swarm sizes."}}
