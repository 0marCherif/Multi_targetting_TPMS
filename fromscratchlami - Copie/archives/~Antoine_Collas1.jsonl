{"id": "aA0bz7EaRd", "cdate": 1672531200000, "mdate": 1681712467809, "content": {"title": "Entropic Wasserstein Component Analysis", "abstract": "Dimension reduction (DR) methods provide systematic approaches for analyzing high-dimensional data. A key requirement for DR is to incorporate global dependencies among original and embedded samples while preserving clusters in the embedding space. To achieve this, we combine the principles of optimal transport (OT) and principal component analysis (PCA). Our method seeks the best linear subspace that minimizes reconstruction error using entropic OT, which naturally encodes the neighborhood information of the samples. From an algorithmic standpoint, we propose an efficient block-majorization-minimization solver over the Stiefel manifold. Our experimental results demonstrate that our approach can effectively preserve high-dimensional clusters, leading to more interpretable and effective embeddings. Python code of the algorithms and experiments is available online."}}
{"id": "QQ1BiJmeres", "cdate": 1672531200000, "mdate": 1695985589118, "content": {"title": "Riemannian Optimization for Non-Centered Mixture of Scaled Gaussian Distributions", "abstract": "This article studies the statistical model of the non-centered mixture of scaled Gaussian distributions (NC-MSG). Using the Fisher-Rao information geometry associated with this distribution, we derive a Riemannian gradient descent algorithm. This algorithm is leveraged for two minimization problems. The first is the minimization of a regularized negative log-likelihood (NLL). The latter makes the trade-off between a white Gaussian distribution and the NC-MSG. Conditions on the regularization are given so that the existence of a minimum to this problem is guaranteed without assumptions on the samples. Then, the Kullback-Leibler (KL) divergence between two NC-MSG is derived. This divergence enables us to define a second minimization problem. The latter is the computation of centers of mass of several NC-MSGs. Numerical experiments show the good performance and the speed of the Riemannian gradient descent on the two problems. Finally, a <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Nearest centro\u00efd classifier</i> is implemented leveraging the KL divergence and its associated center of mass. Applied on the large-scale dataset <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Breizhcrops</i> , this classifier shows good accuracies and robustness to rigid transformations of the test set."}}
{"id": "tg2t8QV2PvX", "cdate": 1640995200000, "mdate": 1681713789741, "content": {"title": "Parametric information geometry with the package Geomstats", "abstract": "We introduce the information geometry module of the Python package Geomstats. The module first implements Fisher-Rao Riemannian manifolds of widely used parametric families of probability distributions, such as normal, gamma, beta, Dirichlet distributions, and more. The module further gives the Fisher-Rao Riemannian geometry of any parametric family of distributions of interest, given a parameterized probability density function as input. The implemented Riemannian geometry tools allow users to compare, average, interpolate between distributions inside a given family. Importantly, such capabilities open the door to statistics and machine learning on probability distributions. We present the object-oriented implementation of the module along with illustrative examples and show how it can be used to perform learning on manifolds of parametric probability distributions."}}
{"id": "sqivgPMy7C", "cdate": 1640995200000, "mdate": 1681713789745, "content": {"title": "Riemannian optimization for non-centered mixture of scaled Gaussian distributions", "abstract": "This paper studies the statistical model of the non-centered mixture of scaled Gaussian distributions (NC-MSG). Using the Fisher-Rao information geometry associated to this distribution, we derive a Riemannian gradient descent algorithm. This algorithm is leveraged for two minimization problems. The first one is the minimization of a regularized negative log-likelihood (NLL). The latter makes the trade-off between a white Gaussian distribution and the NC-MSG. Conditions on the regularization are given so that the existence of a minimum to this problem is guaranteed without assumptions on the samples. Then, the Kullback-Leibler (KL) divergence between two NC-MSG is derived. This divergence enables us to define a minimization problem to compute centers of mass of several NC-MSGs. The proposed Riemannian gradient descent algorithm is leveraged to solve this second minimization problem. Numerical experiments show the good performance and the speed of the Riemannian gradient descent on the two problems. Finally, a Nearest centroid classifier is implemented leveraging the KL divergence and its associated center of mass. Applied on the large scale dataset Breizhcrops, this classifier shows good accuracies as well as robustness to rigid transformations of the test set."}}
{"id": "mDQNKIl3G-T", "cdate": 1640995200000, "mdate": 1681713789754, "content": {"title": "On the Use of Geodesic Triangles between Gaussian Distributions for Classification Problems", "abstract": "This paper presents a new classification framework for both first and second order statistics, i.e. mean/location and covariance matrix. In the last decade, several covariance matrix classification algorithms have been proposed. They often leverage the Riemannian geometry of symmetric positive definite matrices (SPD) with its affine invariant metric and have shown strong performance in many applications. However, their underlying statistical model assumes a zero mean hypothesis. In practice, it is often estimated and then removed in a preprocessing step. This is of course damaging for applications where the mean is a discriminative feature. Unfortunately, the distance associated to the affine invariant metric for both mean and covariance matrix remains unknown. Leveraging previous works on geodesic triangles, we propose two affine invariant divergences that use both statistics. Then, we derive an algorithm to compute the associated Riemannian centers of mass. Finally, a divergence based Nearest centroid, applied on the crop classification dataset Breizhcrops, shows the interest of the proposed framework."}}
{"id": "V9UVPSs7mZ", "cdate": 1640995200000, "mdate": 1681713789750, "content": {"title": "Robust Geometric Metric Learning", "abstract": "This paper proposes new algorithms for the metric learning problem. We start by noticing that several classical metric learning formulations from the literature can be viewed as modified covariance matrix estimation problems. Leveraging this point of view, a general approach, called Robust Geometric Metric Learning (RGML), is then studied. This method aims at simultaneously estimating the covariance matrix of each class while shrinking them towards their (unknown) barycenter. We focus on two specific costs functions: one associated with the Gaussian likelihood (RGML Gaussian), and one with Tyler's <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$M$</tex> -estimator (RGML Tyler). In both, the barycenter is defined with the Riemannian distance, which enjoys nice properties of geodesic convexity and affine invariance. The optimization is performed using the Riemannian geometry of symmetric positive definite matrices and its submanifold of unit determinant. Finally, the performance of RGML is asserted on real datasets. Strong performance is exhibited while being robust to mislabeled data."}}
{"id": "IzOEdAaM5xU", "cdate": 1640995200000, "mdate": 1681713789741, "content": {"title": "G\u00e9om\u00e9trie riemannienne pour l'estimation et l'apprentissage statistiques : application \u00e0 la t\u00e9l\u00e9d\u00e9tection. (Riemannian geometry for statistical estimation and learning : application to remote sensing)", "abstract": ""}}
{"id": "G_jofNxRZb", "cdate": 1640995200000, "mdate": 1695985589118, "content": {"title": "Robust Geometric Metric Learning", "abstract": "This paper proposes new algorithms for the metric learning problem. We start by noticing that several classical metric learning formulations from the literature can be viewed as modified covariance matrix estimation problems. Leveraging this point of view, a general approach, called Robust Geometric Metric Learning (RGML), is then studied. This method aims at simultaneously estimating the covariance matrix of each class while shrinking them towards their (unknown) barycenter. We focus on two specific costs functions: one associated with the Gaussian likelihood (RGML Gaussian), and one with Tyler's M -estimator (RGML Tyler). In both, the barycenter is defined with the Riemannian distance, which enjoys nice properties of geodesic convexity and affine invariance. The optimization is performed using the Riemannian geometry of symmetric positive definite matrices and its submanifold of unit determinant. Finally, the performance of RGML is asserted on real datasets. Strong performance is exhibited while being robust to mislabeled data."}}
{"id": "x8gUli4NXCP", "cdate": 1609459200000, "mdate": 1681713789746, "content": {"title": "A Tyler-Type Estimator of Location and Scatter Leveraging Riemannian Optimization", "abstract": "We consider the problem of jointly estimating the location and scatter matrix of a Compound Gaussian distribution with unknown deterministic texture parameters. When the location is known, the Maximum Likelihood Estimator (MLE) of the scatter matrix corresponds to Tyler\u2019s M-estimator, which can be computed using fixed point iterations. However, when the location is unknown, the joint estimation problem remains challenging since the associated standard fixed-point procedure to evaluate the solution may often diverge. In this paper, we propose a stable algorithm based on Riemannian optimization for this problem. Finally, numerical simulations show the good performance and usefulness of the proposed algorithm."}}
{"id": "YvLji-aWJxZ", "cdate": 1609459200000, "mdate": 1681713789743, "content": {"title": "Probabilistic PCA From Heteroscedastic Signals: Geometric Framework and Application to Clustering", "abstract": "This paper studies a statistical model for heteroscedastic ( <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">i.e.</i> , power fluctuating) signals embedded in white Gaussian noise. Using the Riemannian geometry theory, we propose an unified approach to tackle several problems related to this model. The first axis of contribution concerns parameters (signal subspace and power factors) estimation, for which we derive intrinsic Cram\u00e9r-Rao bounds and propose a flexible Riemannian optimization algorithmic framework in order to compute the maximum likelihood estimator (as well as other cost functions involving the parameters). Interestingly, the obtained bounds are in closed forms and interpretable in terms of problem\u2019s dimensions and SNR. The second axis of contribution concerns the problem of clustering data assuming a mixture of heteroscedastic signals model, for which we generalize the Euclidean <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">K-means++</i> to the considered Riemannian parameter space. We propose an application of the resulting clustering algorithm on the <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Indian Pines</i> segmentation problem benchmark."}}
