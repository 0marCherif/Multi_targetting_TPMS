{"id": "gtUikIeEQK2", "cdate": 1640995200000, "mdate": 1675159339857, "content": {"title": "Modeling Irregular Time Series with Continuous Recurrent Units", "abstract": "Recurrent neural networks (RNNs) are a popular choice for modeling sequential data. Modern RNN architectures assume constant time-intervals between observations. However, in many datasets (e.g. med..."}}
{"id": "wN_2iwR9Hy", "cdate": 1609459200000, "mdate": 1675159339857, "content": {"title": "Continuous-Discrete Recurrent Kalman Networks for Irregular Time Series", "abstract": "In this paper we address the problem of modelling time series with irregular intervals by incorporating a continuous-time version of the Kalman filter into a neural network architecture. Building on the idea of Recurrent Kalman Networks (RKNs) we use an encoder-decoder structure to learn a latent observation space and latent state space in which the dynamics of the data can be approximated linearly. Here, a recurrent Kalman component alternates between continuous latent state propagation and Bayesian updates from incoming observations. This allows us to model and react instantaneously to observations as they come at arbitrary time steps while ensuring sufficient expressive power to model nonlinear dynamics. Experiments on synthetic data show that the model is indeed able to capture continuous, nonlinear dynamics."}}
{"id": "dXz1L85JAw", "cdate": 1609459200000, "mdate": 1675159339858, "content": {"title": "Modeling Irregular Time Series with Continuous Recurrent Units", "abstract": "Recurrent neural networks (RNNs) are a popular choice for modeling sequential data. Modern RNN architectures assume constant time-intervals between observations. However, in many datasets (e.g. medical records) observation times are irregular and can carry important information. To address this challenge, we propose continuous recurrent units (CRUs) -- a neural architecture that can naturally handle irregular intervals between observations. The CRU assumes a hidden state, which evolves according to a linear stochastic differential equation and is integrated into an encoder-decoder framework. The recursive computations of the CRU can be derived using the continuous-discrete Kalman filter and are in closed form. The resulting recurrent architecture has temporal continuity between hidden states and a gating mechanism that can optimally integrate noisy observations. We derive an efficient parameterization scheme for the CRU that leads to a fast implementation f-CRU. We empirically study the CRU on a number of challenging datasets and find that it can interpolate irregular time series better than methods based on neural ordinary differential equations."}}
