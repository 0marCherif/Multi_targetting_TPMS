{"id": "QM0NbPs4Zz", "cdate": 1683924528637, "mdate": 1683924528637, "content": {"title": "ODE4ViTRobustness: A tool for understanding adversarial robustness of Vision Transformers", "abstract": "Understanding the adversarial robustness of Vision Transformers (ViTs) has long been needed since the vulnerability of neural networks hinders their use of it. We present an approach that decomposes the network into submodules and calculates the maximal singular value for each module w.r.t. input, which is a good indication of adversarial robustness. To understand whether Multi-head Self-Attention (MSA) in ViTs contributes to its adversarial robustness, we replace the module with convolutional layers with our decomposing method and conclude that MSA has limited power to defend against adversarial attacks.\n"}}
{"id": "XO4tvoyQd4_", "cdate": 1683924056424, "mdate": 1683924056424, "content": {"title": "DIMBA: discretely masked black-box attack in single object tracking", "abstract": "The adversarial attack can force a CNN-based model to produce an incorrect output by\ncraftily manipulating human-imperceptible input. Exploring such perturbations can help us\ngain a deeper understanding of the vulnerability of neural networks, and provide robustness\nto deep learning against miscellaneous adversaries. Despite extensive studies focusing on\nthe robustness of image, audio, and NLP, works on adversarial examples of visual object\ntracking\u2014especially in a black-box manner\u2014are quite lacking. In this paper, we propose\na novel adversarial attack method to generate noises for single object tracking under\nblack-box settings, where perturbations are merely added on initialized frames of tracking\nsequences, which is difficult to be noticed from the perspective of a whole video clip. Specifically,\nwe divide our algorithm into three components and exploit reinforcement learning\nfor localizing important frame patches precisely while reducing unnecessary computational\nqueries overhead. Compared to existing techniques, our method requires less time to perturb\nvideos but to manipulate competitive or even better adversarial performance. We test\nour algorithm in both long-term and short-term datasets, including OTB100, VOT2018,\nUAV123, and LaSOT. Extensive experiments demonstrate the effectiveness of our method\non three mainstream types of trackers: discrimination, Siamese-based, and reinforcement\nlearning-based trackers. We release our attack tool, DIMBA, via GitHub https:// github.\ncom/ Trust AI/ DIMBA for use by the community."}}
{"id": "gdoNOhcMREv", "cdate": 1664723765534, "mdate": null, "content": {"title": "Dynamic Efficient Adversarial Training Guided by Gradient Magnitude", "abstract": "Adversarial training is an effective but time-consuming way to train robust deep neural networks that can withstand strong adversarial attacks. As a response to its inefficiency, we propose Dynamic Efficient Adversarial Training (DEAT), which gradually increases the adversarial iteration during training. We demonstrate that the gradient's magnitude correlates with the curvature of the trained model's loss landscape, allowing it to reflect the effect of adversarial training. Therefore, based on the magnitude of the gradient, we propose a general acceleration strategy, M+ acceleration, which enables an automatic and highly effective method of adjusting the training procedure. M+ acceleration is computationally efficient and easy to implement. It is suited for DEAT and compatible with the majority of existing adversarial training techniques. Extensive experiments have been done on CIFAR-10 and ImageNet datasets with various training environments. The results show that the proposed M+ acceleration significantly improves the training efficiency of existing adversarial training methods while achieving similar robustness performance. This demonstrates that the strategy is highly adaptive and offers a valuable solution for automatic adversarial training."}}
{"id": "t8P4mJgHYh", "cdate": 1640995200000, "mdate": 1668515714927, "content": {"title": "3DVerifier: Efficient Robustness Verification for 3D Point Cloud Models", "abstract": "3D point cloud models are widely applied in safety-critical scenes, which delivers an urgent need to obtain more solid proofs to verify the robustness of models. Existing verification method for point cloud model is time-expensive and computationally unattainable on large networks. Additionally, they cannot handle the complete PointNet model with joint alignment network (JANet) that contains multiplication layers, which effectively boosts the performance of 3D models. This motivates us to design a more efficient and general framework to verify various architectures of point cloud models. The key challenges in verifying the large-scale complete PointNet models are addressed as dealing with the cross-non-linearity operations in the multiplication layers and the high computational complexity of high-dimensional point cloud inputs and added layers. Thus, we propose an efficient verification framework, 3DVerifier, to tackle both challenges by adopting a linear relaxation function to bound the multiplication layer and combining forward and backward propagation to compute the certified bounds of the outputs of the point cloud models. Our comprehensive experiments demonstrate that 3DVerifier outperforms existing verification algorithms for 3D models in terms of both efficiency and accuracy. Notably, our approach achieves an orders-of-magnitude improvement in verification efficiency for the large network, and the obtained certified bounds are also significantly tighter than the state-of-the-art verifiers. We release our tool 3DVerifier via https://github.com/TrustAI/3DVerifier for use by the community."}}
{"id": "p_EhpCrPWk", "cdate": 1640995200000, "mdate": 1668515714926, "content": {"title": "Coverage-Guided Testing for Recurrent Neural Networks", "abstract": "Recurrent neural networks (RNNs) have been applied to a broad range of applications, including natural language processing, drug discovery, and video recognition. Their vulnerability to input perturbation is also known. Aligning with a view from software defect detection, this article aims to develop a coverage-guided testing approach to systematically exploit the internal behavior of RNNs, with the expectation that such testing can detect defects with high possibility. Technically, the long short-term memory network (LSTM), a major class of RNNs, is thoroughly studied. A family of three test metrics are designed to quantify not only the values but also the temporal relations (including both stepwise and bounded-length) exhibited when LSTM processing inputs. A genetic algorithm is applied to efficiently generate test cases. The test metrics and test case generation algorithm are implemented into a tool <sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">testRNN</small> , which is then evaluated on a set of LSTM benchmarks. Experiments confirm that <sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">testRNN</small> has advantages over the state-of-the-art tool DeepStellar and attack-based defect detection methods, owing to its working with finer temporal semantics and the consideration of the naturalness of input perturbation. Furthermore, <sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">testRNN</small> enables meaningful information to be collected and exhibited for users to understand the testing results, which is an important step toward interpretable neural network testing."}}
{"id": "f910_063HW", "cdate": 1640995200000, "mdate": 1668515714933, "content": {"title": "Understanding Adversarial Robustness of Vision Transformers via Cauchy Problem", "abstract": "Recent research on the robustness of deep learning has shown that Vision Transformers (ViTs) surpass the Convolutional Neural Networks (CNNs) under some perturbations, e.g., natural corruption, adversarial attacks, etc. Some papers argue that the superior robustness of ViT comes from the segmentation of its input images; others say that the Multi-head Self-Attention (MSA) is the key to preserving the robustness. In this paper, we aim to introduce a principled and unified theoretical framework to investigate such an argument on ViT's robustness. We first theoretically prove that, unlike Transformers in Natural Language Processing, ViTs are Lipschitz continuous. Then we theoretically analyze the adversarial robustness of ViTs from the perspective of the Cauchy Problem, via which we can quantify how the robustness propagates through layers. We demonstrate that the first and last layers are the critical factors to affect the robustness of ViTs. Furthermore, based on our theory, we empirically show that unlike the claims from existing research, MSA only contributes to the adversarial robustness of ViTs under weak adversarial attacks, e.g., FGSM, and surprisingly, MSA actually comprises the model's adversarial robustness under stronger attacks, e.g., PGD attacks."}}
{"id": "H_xYr_r0lZ", "cdate": 1640995200000, "mdate": 1668515715188, "content": {"title": "DIMBA: Discretely Masked Black-Box Attack in Single Object Tracking", "abstract": "The adversarial attack can force a CNN-based model to produce an incorrect output by craftily manipulating human-imperceptible input. Exploring such perturbations can help us gain a deeper understanding of the vulnerability of neural networks, and provide robustness to deep learning against miscellaneous adversaries. Despite extensive studies focusing on the robustness of image, audio, and NLP, works on adversarial examples of visual object tracking -- especially in a black-box manner -- are quite lacking. In this paper, we propose a novel adversarial attack method to generate noises for single object tracking under black-box settings, where perturbations are merely added on initial frames of tracking sequences, which is difficult to be noticed from the perspective of a whole video clip. Specifically, we divide our algorithm into three components and exploit reinforcement learning for localizing important frame patches precisely while reducing unnecessary computational queries overhead. Compared to existing techniques, our method requires fewer queries on initialized frames of a video to manipulate competitive or even better attack performance. We test our algorithm in both long-term and short-term datasets, including OTB100, VOT2018, UAV123, and LaSOT. Extensive experiments demonstrate the effectiveness of our method on three mainstream types of trackers: discrimination, Siamese-based, and reinforcement learning-based trackers."}}
{"id": "54Si5By_fk", "cdate": 1640995200000, "mdate": 1668515714919, "content": {"title": "PRoA: A Probabilistic Robustness Assessment against Functional Perturbations", "abstract": "In safety-critical deep learning applications robustness measurement is a vital pre-deployment phase. However, existing robustness verification methods are not sufficiently practical for deploying machine learning systems in the real world. On the one hand, these methods attempt to claim that no perturbations can ``fool'' deep neural networks (DNNs), which may be too stringent in practice. On the other hand, existing works rigorously consider $L_p$ bounded additive perturbations on the pixel space, although perturbations, such as colour shifting and geometric transformations, are more practically and frequently occurring in the real world. Thus, from the practical standpoint, we present a novel and general {\\it probabilistic robustness assessment method} (PRoA) based on the adaptive concentration, and it can measure the robustness of deep learning models against functional perturbations. PRoA can provide statistical guarantees on the probabilistic robustness of a model, \\textit{i.e.}, the probability of failure encountered by the trained model after deployment. Our experiments demonstrate the effectiveness and flexibility of PRoA in terms of evaluating the probabilistic robustness against a broad range of functional perturbations, and PRoA can scale well to various large-scale deep neural networks compared to existing state-of-the-art baselines. For the purpose of reproducibility, we release our tool on GitHub: \\url{ https://github.com/TrustAI/PRoA}."}}
{"id": "409D_L64caG", "cdate": 1640995200000, "mdate": 1668515715199, "content": {"title": "Bridging Formal Methods and Machine Learning with Global Optimisation", "abstract": "Formal methods and machine learning are two research fields with drastically different foundations and philosophies. Formal methods utilise mathematically rigorous techniques for the specification, development and verification of software and hardware systems. Machine learning focuses on pragmatic approaches to gradually improve a parameterised model by observing a training data set. While historically the two fields lack communication, this trend has changed in the past few years with an outburst of research interest in the robustness verification of neural networks. This paper will briefly review these works, and focus on the urgent need for broader, and more in-depth, communication between the two fields, with the ultimate goal of developing learning-enabled systems with not only excellent performance but also acceptable safety and security. We present a specification language, MLS $$^2$$ , and show that it can express a set of known safety and security properties, including generalisation, uncertainty, robustness, data poisoning, backdoor, model stealing, membership inference, model inversion, interpretability, and fairness. To verify MLS $$^2$$ properties, we promote the global optimisation based methods, which have provable guarantees on the convergence to the optimal solution. Many of them have theoretical bounds on the gap between current solutions and the optimal solution."}}
{"id": "sjBV-PvCkh", "cdate": 1609459200000, "mdate": 1668515714930, "content": {"title": "Sparse Adversarial Video Attacks with Spatial Transformations", "abstract": ""}}
