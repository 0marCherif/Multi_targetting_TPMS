{"id": "C3hL1sbz5Vf", "cdate": 1664872116286, "mdate": null, "content": {"title": "Late-Phase Second-Order Training", "abstract": "Towards the end of training, stochastic first-order methods such as SGD and ADAM go into diffusion and no longer make significant progress. In contrast, Newton-type methods are highly efficient \"close\" to the optimum, in the deterministic case. Therefore, these methods might turn out to be a particularly efficient tool for the final phase of training in the stochastic deep learning context as well. In our work, we study this idea by conducting an empirical comparison of a second-order Hessian-free optimizer and different first-order strategies with learning rate decays for late-phase training. We show that performing a few costly but precise second-order steps can outperform first-order alternatives in wall-clock runtime."}}
{"id": "ny0l9YjLB7s", "cdate": 1640995200000, "mdate": 1683884847819, "content": {"title": "Understanding Deep Learning Optimization via Benchmarking and Debugging", "abstract": "Das zentrale Prinzip des maschinellen Lernens (ML) ist die Vorstellung, dass Computer die notwendigen Strategien zur L\u00f6sung einer Aufgabe erlernen k\u00f6nnen, ohne explizit daf\u00fcr programmiert worden zu sein. Die Hoffnung ist, dass Computer anhand von Daten die zugrunde liegenden Muster erkennen und selbst feststellen, wie sie Aufgaben erledigen k\u00f6nnen, ohne dass sie dabei von Menschen geleitet werden m\u00fcssen. Um diese Aufgabe zu erf\u00fcllen, werden viele Probleme des maschinellen Lernens als Minimierung einer Verlustfunktion formuliert. Daher sind Optimierungsverfahren ein zentraler Bestandteil des Trainings von ML-Modellen. Obwohl das maschinelle Lernen und insbesondere das tiefe Lernen oft als innovative Spitzentechnologie wahrgenommen wird, basieren viele der zugrunde liegenden Optimierungsalgorithmen eher auf simplen, fast archaischen Verfahren. Um moderne neuronale Netze erfolgreich zu trainieren, bedarf es daher h\u00e4ufig umfangreicher menschlicher Unterst\u00fctzung. Ein Grund f\u00fcr diesen m\u00fchsamen, umst\u00e4ndlichen und langwierigen Trainingsprozess ist unser mangelndes Verst\u00e4ndnis der Optimierungsmethoden im anspruchsvollen Rahmen des tiefen Lernens. Auch deshalb hat das Training neuronaler Netze bis heute den Ruf, eher eine Kunstform als eine echte Wissenschaft zu sein und erfordert ein Ma\u00df an menschlicher Beteiligung, welche dem Kernprinzip des maschinellen Lernens widerspricht. Obwohl bereits Hunderte Optimierungsverfahren f\u00fcr das tiefe Lernen vorgeschlagen wurden, gibt es noch kein allgemein anerkanntes Protokoll zur Beurteilung ihrer Qualit\u00e4t. Ohne ein standardisiertes und unabh\u00e4ngiges Bewertungsprotokoll ist es jedoch schwierig, die N\u00fctzlichkeit neuartiger Methoden zuverl\u00e4ssig nachzuweisen. In dieser Arbeit werden Strategien vorgestellt, mit denen sich Optimierer f\u00fcr das tiefe Lernen quantitativ, reproduzierbar und aussagekr\u00e4ftig vergleichen lassen. Dieses Protokoll ber\u00fccksichtigt die einzigartigen Herausforderungen des tiefen Lernens, wie etwa die inh\u00e4rente Stochastizit\u00e4t oder die wichtige Unterscheidung zwischen Lernen und reiner Optimierung. Die Erkenntnisse sind im Python-Paket DeepOBS formalisiert und automatisiert, wodurch gerechtere, schnellere und \u00fcberzeugendere empirische Vergleiche von Optimierern erm\u00f6glicht werden. Auf der Grundlage dieses Benchmarking-Protokolls werden anschlie\u00dfend f\u00fcnfzehn popul\u00e4re Deep-Learning-Optimierer verglichen, um einen \u00dcberblick \u00fcber den aktuellen Entwicklungsstand in diesem Bereich zu gewinnen. Um fundierte Entscheidungshilfen f\u00fcr die Auswahl einer Optimierungsmethode aus der wachsenden Liste zu erhalten, evaluiert der Benchmark sie umfassend anhand von fast 50 000 Trainingsprozessen. Unser Benchmark zeigt, dass der vergleichsweise traditionelle Adam-Optimierer eine gute, aber nicht dominierende Methode ist und dass neuere Algorithmen ihn nicht kontinuierlich \u00fcbertreffen k\u00f6nnen. Neben dem verwendeten Optimierer k\u00f6nnen auch andere Ursachen das Training neuronaler Netze erschweren, etwa ineffiziente Modellarchitekturen oder Hyperparameter. Herk\u00f6mmliche Leistungsindikatoren, wie etwa die Verlustfunktion auf den Trainingsdaten oder die erreichte Genauigkeit auf einem separaten Validierungsdatensatz, k\u00f6nnen zwar zeigen, ob das Modell lernt oder nicht, aber nicht warum. Um dieses Verst\u00e4ndnis und gleichzeitig einen Blick in die Blackbox der neuronalen Netze zu liefern, wird in dieser Arbeit Cockpit pr\u00e4sentiert, ein Debugging-Tool speziell f\u00fcr das tiefe Lernen. Es kombiniert neuartige und bew\u00e4hrte Observablen zu einem Echtzeit-\u00dcberwachungswerkzeug f\u00fcr das Training neuronaler Netze. Cockpit macht unter anderem deutlich,dass gut getunte Trainingsprozesse konsequent \u00fcber das lokale Minimum hinausgehen, zumindest f\u00fcr wesentliche Phasen des Trainings. Der Einsatz von sorgf\u00e4ltigen Benchmarking-Experimenten und ma\u00dfgeschneiderten Debugging-Tools verbessert unser Verst\u00e4ndnis des Trainings neuronaler Netze. Angesichts des Mangels an theoretischen Erkenntnissen sind diese empirischen Ergebnisse und praktischen Instrumente unerl\u00e4sslich f\u00fcr die Unterst\u00fctzung in der Praxis. Vor allem aber zeigen sie auf, dass es einen Bedarf und einen klaren Weg f\u00fcr grundlegend neuartigen Optimierungsmethoden gibt, um das tiefe Lernen zug\u00e4nglicher, robuster und ressourcenschonender zu machen."}}
{"id": "8AFzk19DNvf", "cdate": 1621629846167, "mdate": null, "content": {"title": "Cockpit: A Practical Debugging Tool for the Training of Deep Neural Networks", "abstract": "When engineers train deep learning models, they are very much \"flying blind\". Commonly used methods for real-time training diagnostics, such as monitoring the train/test loss, are limited. Assessing a network's training process solely through these performance indicators is akin to debugging software without access to internal states through a debugger. To address this, we present Cockpit, a collection of instruments that enable a closer look into the inner workings of a learning machine, and a more informative and meaningful status report for practitioners. It facilitates the identification of learning phases and failure modes, like ill-chosen hyperparameters. These instruments leverage novel higher-order information about the gradient distribution and curvature, which has only recently become efficiently accessible. We believe that such a debugging tool, which we open-source for PyTorch, is a valuable help in troubleshooting the training process. By revealing new insights, it also more generally contributes to explainability and interpretability of deep nets."}}
{"id": "PtH0kzHWKjv", "cdate": 1609459200000, "mdate": 1631641001185, "content": {"title": "Descending through a Crowded Valley - Benchmarking Deep Learning Optimizers", "abstract": "Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In..."}}
{"id": "I7ZaElf8Gz", "cdate": 1609459200000, "mdate": 1683884847861, "content": {"title": "Cockpit: A Practical Debugging Tool for the Training of Deep Neural Networks", "abstract": "When engineers train deep learning models, they are very much \"flying blind\". Commonly used methods for real-time training diagnostics, such as monitoring the train/test loss, are limited. Assessing a network's training process solely through these performance indicators is akin to debugging software without access to internal states through a debugger. To address this, we present Cockpit, a collection of instruments that enable a closer look into the inner workings of a learning machine, and a more informative and meaningful status report for practitioners. It facilitates the identification of learning phases and failure modes, like ill-chosen hyperparameters. These instruments leverage novel higher-order information about the gradient distribution and curvature, which has only recently become efficiently accessible. We believe that such a debugging tool, which we open-source for PyTorch, is a valuable help in troubleshooting the training process. By revealing new insights, it also more generally contributes to explainability and interpretability of deep nets."}}
{"id": "k2Om84I9JuX", "cdate": 1601308066966, "mdate": null, "content": {"title": "Descending through a Crowded Valley \u2014 Benchmarking Deep Learning Optimizers", "abstract": "Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost $35,000$ individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts."}}
{"id": "SbfnR7y0S7S", "cdate": 1546300800000, "mdate": 1683884847813, "content": {"title": "DeepOBS: A Deep Learning Optimizer Benchmark Suite", "abstract": "Because the choice and tuning of the optimizer affects the speed, and ultimately the performance of deep learning, there is significant past and recent research in this area. Yet, perhaps surprisingly, there is no generally agreed-upon protocol for the quantitative and reproducible evaluation of optimization strategies for deep learning. We suggest routines and benchmarks for stochastic optimization, with special focus on the unique aspects of deep learning, such as stochasticity, tunability and generalization. As the primary contribution, we present DeepOBS, a Python package of deep learning optimization benchmarks. The package addresses key challenges in the quantitative assessment of stochastic optimizers, and automates most steps of benchmarking. The library includes a wide and extensible set of ready-to-use realistic optimization problems, such as training Residual Networks for image classification on ImageNet or character-level language prediction models, as well as popular classics like MNIST and CIFAR-10. The package also provides realistic baseline results for the most popular optimizers on these test problems, ensuring a fair comparison to the competition when benchmarking new optimizers, and without having to run costly experiments. It comes with output back-ends that directly produce LaTeX code for inclusion in academic publications. It supports TensorFlow and is available open source."}}
{"id": "rJg6ssC5Y7", "cdate": 1538087844931, "mdate": null, "content": {"title": "DeepOBS: A Deep Learning Optimizer Benchmark Suite", "abstract": "Because the choice and tuning of the optimizer affects the speed, and ultimately the performance of deep learning, there is significant past and recent research in this area. Yet, perhaps surprisingly, there is no generally agreed-upon protocol for the quantitative and reproducible evaluation of optimization strategies for deep learning. We suggest routines and benchmarks for stochastic optimization, with special focus on the unique aspects of deep learning, such as stochasticity, tunability and generalization. As the primary contribution, we present DeepOBS, a Python package of deep learning optimization benchmarks. The package addresses key challenges in the quantitative assessment of stochastic optimizers, and automates most steps of benchmarking. The library includes a wide and extensible set of ready-to-use realistic optimization problems, such as training Residual Networks for image classification on ImageNet or character-level language prediction models, as well as popular classics like MNIST and CIFAR-10. The package also provides realistic baseline results for the most popular optimizers on these test problems, ensuring a fair comparison to the competition when benchmarking new optimizers, and without having to run costly experiments. It comes with output back-ends that directly produce LaTeX code for inclusion in academic publications. It supports TensorFlow and is available open source."}}
