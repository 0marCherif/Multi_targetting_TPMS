{"id": "flNZJ2eOet", "cdate": 1652737665323, "mdate": null, "content": {"title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes", "abstract": "In-context learning is the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To investigate this, we consider the problem of training a model to in-context learn a function class (e.g., linear functions): given data derived from some functions in the class, can we train a model (e.g., a Transformer) to in-context learn most functions from that class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions---that is, the trained model is able to learn unseen linear functions from in-context examples with performance comparable to the optimal least squares estimator. In fact, in-context learning is possible even under two forms of distribution shift: (i) between the training data of the Transformer and inference-time prompts, and (ii) between the in-context examples and the query input during inference. We also show that we can train Transformers to in-context learn more complex function classes: sparse linear functions where the model outperforms least squares and nearly matches the performance of Lasso, and two-layer neural networks where the model performs comparably to neural networks trained on in-context examples using gradient descent."}}
{"id": "G-yAIG8kzaR", "cdate": 1640995200000, "mdate": 1654693721096, "content": {"title": "On the Statistical Complexity of Sample Amplification", "abstract": "Given $n$ i.i.d. samples drawn from an unknown distribution $P$, when is it possible to produce a larger set of $n+m$ samples which cannot be distinguished from $n+m$ i.i.d. samples drawn from $P$? (Axelrod et al. 2019) formalized this question as the sample amplification problem, and gave optimal amplification procedures for discrete distributions and Gaussian location models. However, these procedures and associated lower bounds are tailored to the specific distribution classes, and a general statistical understanding of sample amplification is still largely missing. In this work, we place the sample amplification problem on a firm statistical foundation by deriving generally applicable amplification procedures, lower bound techniques and connections to existing statistical notions. Our techniques apply to a large class of distributions including the exponential family, and establish a rigorous connection between sample amplification and distribution learning."}}
{"id": "bD24inFNjCm", "cdate": 1577836800000, "mdate": 1654693721089, "content": {"title": "Sample Amplification: Increasing Dataset Size even when Learning is Impossible", "abstract": "Given data drawn from an unknown distribution, D, to what extent is it possible to \u201camplify\u201d this dataset and faithfully output an even larger set of samples that appear to have been drawn from D? ..."}}
{"id": "BJNaJwbdZB", "cdate": 1514764800000, "mdate": null, "content": {"title": "A Spectral View of Adversarially Robust Features", "abstract": "Given the apparent difficulty of learning models that are robust to adversarial perturbations, we propose tackling the simpler problem of developing adversarially robust features. Specifically, given a dataset and metric of interest, the goal is to return a function (or multiple functions) that 1) is robust to adversarial perturbations, and 2) has significant variation across the datapoints. We establish strong connections between adversarially robust features and a natural spectral property of the geometry of the dataset and metric of interest. This connection can be leveraged to provide both robust features, and a lower bound on the robustness of any function that has significant variance across the dataset. Finally, we provide empirical evidence that the adversarially robust features given by this spectral approach can be fruitfully leveraged to learn a robust (and accurate) model."}}
{"id": "daJtraGEVoZ", "cdate": 1451606400000, "mdate": 1654693721091, "content": {"title": "Raising The Bar For Vertex Cover: Fixed-parameter Tractability Above A Higher Guarantee", "abstract": "The standard parameterization of the Vertex Cover problem (Given an undirected graph G and k \u220a \u2115 as input, does G have a vertex cover of size at most k?) has the solution size k as the parameter. The following more challenging parameterization of Vertex Cover stems from the observation that the size MM of a maximum matching of G lower-bounds the size of any vertex cover of G: Does G have a vertex cover of size at most MM + k\u03bc? The parameter is the excess k\u03bc of the solution size over the lower bound MM. Razgon and O'Sullivan (ICALP 2008) showed that this above-guarantee parameterization of Vertex Cover is fixed-parameter tractable and can be solved in time *(15k\u03bc), where the * notation hides polynomial factors. This was first improved to *(9k\u03bc) (Raman et al., ESA 2011), then to *(4k\u03bc) (Cygan et al., IPEC 2011, TOCT 2013), then to *(2.618k\u03bc) (Narayanaswamy et al., STACS 2012) and finally to the current best bound *(2.3146k\u03bc) (Lokshtanov et al., TALG 2014). The last two bounds were in fact proven for a different parameter: namely, the excess k\u03bb of the solution size over LP, the value of the linear programming relaxation of the standard LP formulation of Vertex Cover. Since LP \u2265 MM for any graph, we have that k\u03bb \u2264 k\u03bc for Yes instances. This is thus a stricter parameterization\u2014the new parameter is, in general, smaller\u2014and the running times carry over directly to the parameter k\u03bc. We investigate an even stricter parameterization of Vertex Cover, namely the excess of the solution size over the quantity (2LP \u2013 MM). We ask: Given a graph G and \u220a \u2115 as input, does G have a vertex cover of size at most (2LP \u2013 MM) + ? The parameter is . It can be shown that (2LP \u2013 MM) is a lower bound on vertex cover size, and since LP \u2265 MM we have that (2LP \u2013 MM) \u2265 LP, and hence that \u2264 k\u03bb holds for Yes instances. Further, (k\u03bb \u2013 ) could be as large as (LP \u2013 MM) and\u2014to the best of our knowledge\u2014this difference cannot be expressed as a function of k\u03bb alone. These facts motivate and justify our choice of parameter: this is indeed a stricter parameterization whose tractability does not follow directly from known results. We show that Vertex Cover is fixed-parameter tractable for this stricter parameter : We derive an algorithm which solves Vertex Cover in time *(3 ), thus pushing the envelope further on the parameterized tractability of Vertex Cover."}}
