{"id": "dNK4yXV9BRd", "cdate": 1674948221921, "mdate": 1674948221921, "content": {"title": "Sparse Visual Counterfactual Explanations in Image Space", "abstract": "Visual counterfactual explanations (VCEs) in image space\nare an important tool to understand decisions of image classifiers as\nthey show under which changes of the image the decision of the classifier\nwould change. Their generation in image space is challenging and requires\nrobust models due to the problem of adversarial examples. Existing techniques to generate VCEs in image space suffer from spurious changes\nin the background. Our novel perturbation model for VCEs together\nwith its efficient optimization via our novel Auto-Frank-Wolfe scheme\nyields sparse VCEs which lead to subtle changes specific for the target\nclass. Moreover, we show that VCEs can be used to detect undesired behavior of ImageNet classifiers due to spurious features in the ImageNet\ndataset. Code is available under https://github.com/valentyn1boreiko/\nSVCEs code\n."}}
{"id": "7SEi-ISNni7", "cdate": 1652737808625, "mdate": null, "content": {"title": "Diffusion Visual Counterfactual Explanations", "abstract": "Visual Counterfactual Explanations (VCEs) are an important tool to understand the decisions of an image classifier. They are \u201csmall\u201d but \u201crealistic\u201d semantic changes of the image changing the classifier decision. Current approaches for the generation of VCEs are restricted to adversarially robust models and often contain non-realistic artefacts, or are limited to image classification problems with few classes. In this paper, we overcome this by generating Diffusion Visual Counterfactual Explanations (DVCEs) for arbitrary ImageNet classifiers via a diffusion process. Two modifications to the diffusion process are key for our DVCEs: first, an adaptive parameterization, whose hyperparameters generalize across images and models, together with distance regularization and late start of the diffusion process, allow us to generate images with minimal semantic changes to the original ones but different classification. Second, our cone regularization via an adversarially robust model ensures that the diffusion process does not converge to trivial non-semantic changes, but instead produces realistic images of the target class which achieve high confidence by the classifier."}}
{"id": "-BTmxCddppP", "cdate": 1632875676462, "mdate": null, "content": {"title": "Revisiting Out-of-Distribution Detection: A Simple Baseline is Surprisingly Effective", "abstract": "It is an important problem in trustworthy machine learning to recognize out-of-distribution (OOD) inputs which are inputs unrelated to the in-distribution task. Many out-of-distribution detection methods have been suggested in recent years. The goal of this paper is to recognize common objectives as well as to identify the implicit scoring functions of different OOD detection methods. In particular, we show that binary discrimination between in- and (different) out-distributions is equivalent to several different formulations of the OOD detection problem. When trained in a shared fashion with a standard classifier, this binary discriminator reaches an OOD detection performance similar to that of Outlier Exposure. Moreover, we show that the confidence loss which is used by Outlier Exposure has an implicit scoring function which differs in a non-trivial fashion from the theoretically optimal scoring function in the case where training and test out-distribution are the same, but is similar to the one used when training with an extra background class. In practice, when trained in exactly the same way, all these methods perform similarly and reach state-of-the-art OOD detection performance."}}
{"id": "f9JwVXMJ1Up", "cdate": 1632875674316, "mdate": null, "content": {"title": "The Needle in the haystack: Out-distribution aware Self-training in an Open-World Setting", "abstract": "Traditional semi-supervised learning (SSL) has focused on the closed world assumption where all unlabeled samples are task-related. In practice, this assumption is often violated when leveraging data from very large image databases that contain mostly non-task-relevant samples. While standard self-training and other established methods fail in this open-world setting, we demonstrate that our out-distribution-aware self-learning (ODST) with a careful sample selection strategy can leverage unlabeled datasets with millions of samples, more than 1600 times larger than the labeled datasets, and which contain only about $2\\%$ task-relevant inputs. Standard and open world SSL techniques degrade in performance when the ratio of task-relevant sample decreases and show a significant distribution shift which is problematic regarding AI safety while ODST outperforms them with respect to test performance, corruption robustness and out-of-distribution detection.\n"}}
{"id": "ne276YV8ynO", "cdate": 1609459200000, "mdate": 1634380456240, "content": {"title": "MLCapsule: Guarded Offline Deployment of Machine Learning as a Service", "abstract": "Machine Learning as a Service (MLaaS) is a popular and convenient way to access a trained machine learning (ML) model trough an API. However, if the user's input is sensitive, sending it to the server is not an option. Equally, the service provider does not want to share the model by sending it to the client for protecting its intellectual property and pay-per-query business model. As a solution, we propose MLCapsule, a guarded offline deployment of MLaaS. MLCapsule executes the machine learning model locally on the user's client and therefore the data never leaves the client. Meanwhile, we show that MLCapsule is able to offer the service provider the same level of control and security of its model as the commonly used server-side execution. Beyond protecting against direct model access, we demonstrate that MLCapsule allows for implementing defenses against advanced attacks on machine learning models such as model stealing, reverse engineering and membership inference."}}
{"id": "sw4tFm289mG", "cdate": 1577836800000, "mdate": 1634380456230, "content": {"title": "Adversarial Robustness on In- and Out-Distribution Improves Explainability", "abstract": "Neural networks have led to major improvements in image classification but suffer from being non-robust to adversarial changes, unreliable uncertainty estimates on out-distribution samples and their inscrutable black-box decisions. In this work we propose RATIO, a training procedure for Robustness via Adversarial Training on In- and Out-distribution, which leads to robust models with reliable and robust confidence estimates on the out-distribution. RATIO has similar generative properties to adversarial training so that visual counterfactuals produce class specific features. While adversarial training comes at the price of lower clean accuracy, RATIO achieves state-of-the-art $$l_2$$ -adversarial robustness on CIFAR10 and maintains better clean accuracy."}}
{"id": "XyzpSkxYNhv", "cdate": 1577836800000, "mdate": 1631259313637, "content": {"title": "Out-distribution aware Self-training in an Open World Setting", "abstract": "Deep Learning heavily depends on large labeled datasets which limits further improvements. While unlabeled data is available in large amounts, in particular in image recognition, it does not fulfill the closed world assumption of semi-supervised learning that all unlabeled data are task-related. The goal of this paper is to leverage unlabeled data in an open world setting to further improve prediction performance. For this purpose, we introduce out-distribution aware self-training, which includes a careful sample selection strategy based on the confidence of the classifier. While normal self-training deteriorates prediction performance, our iterative scheme improves using up to 15 times the amount of originally labeled data. Moreover, our classifiers are by design out-distribution aware and can thus distinguish task-related inputs from unrelated ones."}}
{"id": "GR08q3VGHMX", "cdate": 1577836800000, "mdate": 1634380456234, "content": {"title": "Adversarial Robustness on In- and Out-Distribution Improves Explainability", "abstract": "Neural networks have led to major improvements in image classification but suffer from being non-robust to adversarial changes, unreliable uncertainty estimates on out-distribution samples and their inscrutable black-box decisions. In this work we propose RATIO, a training procedure for Robustness via Adversarial Training on In- and Out-distribution, which leads to robust models with reliable and robust confidence estimates on the out-distribution. RATIO has similar generative properties to adversarial training so that visual counterfactuals produce class specific features. While adversarial training comes at the price of lower clean accuracy, RATIO achieves state-of-the-art $l_2$-adversarial robustness on CIFAR10 and maintains better clean accuracy."}}
{"id": "UMxzISHytRw", "cdate": 1514764800000, "mdate": 1634380456233, "content": {"title": "Towards Reverse-Engineering Black-Box Neural Networks", "abstract": "Many deployed learned models are black boxes: given input, returns output. Internal information about the model, such as the architecture, optimisation procedure, or training data, is not disclosed explicitly as it might contain proprietary information or make the system more vulnerable. This work shows that such attributes of neural networks can be exposed from a sequence of queries. This has multiple implications. On the one hand, our work exposes the vulnerability of black-box neural networks to different types of attacks -- we show that the revealed internal information helps generate more effective adversarial examples against the black box model. On the other hand, this technique can be used for better protection of private content from automatic recognition models using adversarial examples. Our paper suggests that it is actually hard to draw a line between white box and black box models."}}
{"id": "7j6qeM4ZHZK", "cdate": 1514764800000, "mdate": 1634380456243, "content": {"title": "MLCapsule: Guarded Offline Deployment of Machine Learning as a Service", "abstract": "With the widespread use of machine learning (ML) techniques, ML as a service has become increasingly popular. In this setting, an ML model resides on a server and users can query it with their data via an API. However, if the user's input is sensitive, sending it to the server is undesirable and sometimes even legally not possible. Equally, the service provider does not want to share the model by sending it to the client for protecting its intellectual property and pay-per-query business model. In this paper, we propose MLCapsule, a guarded offline deployment of machine learning as a service. MLCapsule executes the model locally on the user's side and therefore the data never leaves the client. Meanwhile, MLCapsule offers the service provider the same level of control and security of its model as the commonly used server-side execution. In addition, MLCapsule is applicable to offline applications that require local execution. Beyond protecting against direct model access, we couple the secure offline deployment with defenses against advanced attacks on machine learning models such as model stealing, reverse engineering, and membership inference."}}
