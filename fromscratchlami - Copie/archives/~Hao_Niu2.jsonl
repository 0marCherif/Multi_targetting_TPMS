{"id": "rYSC46fmOoM", "cdate": 1640995200000, "mdate": 1688006466413, "content": {"title": "Composition-based Heterogeneous Graph Multi-channel Attention Network for Multi-aspect Multi-sentiment Classification", "abstract": ""}}
{"id": "re94AJ_ZvvJ", "cdate": 1609459200000, "mdate": 1688006466418, "content": {"title": "BioHanBERT: A Hanzi-aware Pre-trained Language Model for Chinese Biomedical Text Mining", "abstract": "Unsupervised pre-trained language models (PLMs) have boosted the development of effective biomedical text mining models. But the biomedical texts contain a huge number of long-tail concepts and terminologies, which makes further pre-training on biomedical corpora relatively expensive (more biomedical corpora and more pre-training steps are needed). Nonetheless, this problem receives less attention in recent studies. In Chinese biomedical text, concepts and terminologies consist of Chinese characters, and Chinese characters are often composed of sub-character components which are also semantically informative; thus in order to enhance the semantics of biomedical concepts and terminologies, the use of a Chinese character\u2019s component-level internal semantic information also appears to be reasonable.In this paper, we propose a novel hanzi-aware pre-trained language model for Chinese biomedical text mining, referred to as BioHanBERT (hanzi-aware BERT for Chinese biomedical text mining), utilizing the component-level internal semantic information of Chinese characters to enhance the semantics of Chinese biomedical concepts and terminologies, and thereby to reduce further pre-training costs. BioHanBERT first employs a Chinese character encoder to extract the component-level internal semantic feature of each Chinese character, and then fuse the character\u2019s internal semantic feature and its contextual embedding extracted by BERT to enrich the representations of the concepts or terminologies containing the character. The results of extensive experiments show that our model is able to consistently outperform current state-of-the-art (SOTA) models in a wide range of Chinese biomedical natural language processing (NLP) tasks."}}
{"id": "ivKmNn9Bm0N", "cdate": 1609459200000, "mdate": 1688006466414, "content": {"title": "Improving Chinese Character Representation with Formation Graph Attention Network", "abstract": "Chinese characters are often composed of subcharacter components which are also semantically informative, and the component-level internal semantic features of a Chinese character inherently bring with additional information that benefits the semantic representation of the character. Therefore, there have been several studies that utilized subcharacter component information (e.g. radical, fine-grained components and stroke n-grams) to improve Chinese character representation. However we argue that it has not been fully explored what would be the best way of modeling and encoding a Chinese character. For improving the representation of a Chinese character, existing methods introduce more component-level internal semantic features as well as more semantic irrelevant subcharacter component information, and these semantic irrelevant subcharacter component will be noisy for representing a Chinese character. Moreover, existing methods suffer from the inability of discriminating the importance of the introduced subcharacter components, accordingly they can not filter out introduced noisy subcharacter component information. In this paper, we first decompose Chinese characters into components according to their formations, then model a Chinese character and its decomposed components as a graph structure named Chinese character formation graph; Chinese character formation graph can reserve the azimuth relationship among subcharacter components, and be advantageous to explicitly model the component-level internal semantic features of a Chinese character. Furtherly, we propose a novel model Chinese Character Formation Graph Attention Network (FGAT) which is able to discriminate the importance of the introduced subcharacter components and extract component-level internal semantic features of a Chinese character efficiently. To demonstrate the effectiveness of our research, we have conducted extensive experiments. The experimental results show that our model achieves better results than state-of-the-art (SOTA) approaches."}}
{"id": "XMq43yJls05", "cdate": 1609459200000, "mdate": 1688006466414, "content": {"title": "C2BERT: Cross-contrast BERT for Chinese Biomedical Sentence Representation", "abstract": "Pre-trained language models (PLMs), such as BERT, have achieved great success on various natural language processing (NLP) tasks. Nevertheless, we observe that PLM-derived native Chinese biomedical sentence representations are somehow collapsed, which means PLMs induce a non-smooth anisotropic semantic space of Chinese biomedical sentences and most sentences are mapped into a small area and therefore produce high similarity. Such PLM-derived native sentence representations poorly capture semantic meaning of Chinese biomedical sentences.To alleviate the aforementioned collapse issue, we then propose a novel contrastive learning framework, named Cross-contrast BERT (C2 BERT), that advances the state-of-the-art Chinese biomedical sentence embeddings. C2 BERT proposes to derive positive/negative samples from two transformer-based different PLMs; this design decision reflects our philosophy that our goal is to conflate the knowledge stored in different PLMs to produce Chinese biomedical sentence embeddings, rather than introducing new noise. Moreover, without costly further pretraining, C2 BERT exploits contrastive learning as an auxiliary training objective during fine-tuning with supervision from biomedical sentence-related tasks. We demonstrate with extensive experiments that our C2 BERT model is more effective than competitive baselines on diverse Chinese biomedical sentence-related tasks."}}
