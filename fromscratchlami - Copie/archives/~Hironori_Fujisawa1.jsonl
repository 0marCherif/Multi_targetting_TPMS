{"id": "Kf2nQOoG7HY", "cdate": 1577836800000, "mdate": null, "content": {"title": "Transfer Learning via \u21131 Regularization", "abstract": "Machine learning algorithms typically require abundant data under a stationary environment. However, environments are nonstationary in many real-world applications. Critical issues lie in how to effectively adapt models under an ever-changing environment. We propose a method for transferring knowledge from a source domain to a target domain via $\\ell_1$ regularization in high dimension. We incorporate $\\ell_1$ regularization of differences between source and target parameters in addition to an ordinary $\\ell_1$ regularization. Hence, our method yields sparsity for both the estimates themselves and changes of the estimates. The proposed method has a tight estimation error bound under a stationary environment, and the estimate remains unchanged from the source estimate under small residuals. Moreover, the estimate is consistent with the underlying function, even when the source estimate is mistaken due to nonstationarity. Empirical results demonstrate that the proposed method effectively balances stability and plasticity."}}
{"id": "_KcyxWUpkS", "cdate": 1546300800000, "mdate": null, "content": {"title": "HMLasso: Lasso with High Missing Rate.", "abstract": "Sparse regression such as the Lasso has achieved great success in handling high-dimensional data. However, one of the biggest practical problems is that high-dimensional data often contain large amounts of missing values. Convex Conditioned Lasso (CoCoLasso) has been proposed for dealing with high-dimensional data with missing values, but it performs poorly when there are many missing values, so that the high missing rate problem has not been resolved. In this paper, we propose a novel Lasso-type regression method for high-dimensional data with high missing rates. We effectively incorporate mean imputed covariance, overcoming its inherent estimation bias. The result is an optimally weighted modification of CoCoLasso according to missing ratios. We theoretically and experimentally show that our proposed method is highly effective even when there are many missing values."}}
{"id": "IEFhVSh0qRW", "cdate": 1514764800000, "mdate": null, "content": {"title": "HMLasso: Lasso for High Dimensional and Highly Missing Data.", "abstract": "Sparse regression such as the Lasso has achieved great success in handling high-dimensional data. However, one of the biggest practical problems is that high-dimensional data often contain large amounts of missing values. Convex Conditioned Lasso (CoCoLasso) has been proposed for dealing with high-dimensional data with missing values, but it performs poorly when there are many missing values, so that the high missing rate problem has not been resolved. In this paper, we propose a novel Lasso-type regression method for high-dimensional data with high missing rates. We effectively incorporate mean imputed covariance, overcoming its inherent estimation bias. The result is an optimally weighted modification of CoCoLasso according to missing ratios. We theoretically and experimentally show that our proposed method is highly effective even when there are many missing values."}}
{"id": "FK6VoaNuAG", "cdate": 1514764800000, "mdate": null, "content": {"title": "Sparse principal component regression for generalized linear models.", "abstract": "Principal component regression (PCR) is a widely used two-stage procedure: principal component analysis (PCA), followed by regression in which the selected principal components are regarded as new explanatory variables in the model. Note that PCA is based only on the explanatory variables, so the principal components are not selected using the information on the response variable. We propose a one-stage procedure for PCR in the framework of generalized linear models. The basic loss function is based on a combination of the regression loss and PCA loss. An estimate of the regression parameter is obtained as the minimizer of the basic loss function with a sparse penalty. We call the proposed method sparse principal component regression for generalized linear models (SPCR-glm). Taking the two loss function into consideration simultaneously, SPCR-glm enables us to obtain sparse principal component loadings that are related to a response variable. However, a combination of loss functions may cause a parameter identification problem, but this potential problem is avoided by virtue of the sparse penalty. Thus, the sparse penalty plays two roles in this method. We apply SPCR-glm to two real datasets, doctor visits data and mouse consomic strain data. Previous article in issue Next article in issue"}}
{"id": "7-9L7lnfoe", "cdate": 1514764800000, "mdate": null, "content": {"title": "Independently Interpretable Lasso: A New Regularizer for Sparse Regression with Uncorrelated Variables.", "abstract": "Sparse regularization such as l1 regularization is a quite powerful and widely used strategy for high dimensional learning problems. The effectiveness of sparse regularization has been supported pr..."}}
{"id": "U_35WLfFt_5", "cdate": 1483228800000, "mdate": null, "content": {"title": "Robust and Sparse Regression via \u03b3-Divergence", "abstract": "In high-dimensional data, many sparse regression methods have been proposed. However, they may not be robust against outliers. Recently, the use of density power weight has been studied for robust parameter estimation, and the corresponding divergences have been discussed. One such divergence is the \u03b3 -divergence, and the robust estimator using the \u03b3 -divergence is known for having a strong robustness. In this paper, we extend the \u03b3 -divergence to the regression problem, consider the robust and sparse regression based on the \u03b3 -divergence and show that it has a strong robustness under heavy contamination even when outliers are heterogeneous. The loss function is constructed by an empirical estimate of the \u03b3 -divergence with sparse regularization, and the parameter estimate is defined as the minimizer of the loss function. To obtain the robust and sparse estimate, we propose an efficient update algorithm, which has a monotone decreasing property of the loss function. Particularly, we discuss a linear regression problem with L 1 regularization in detail. In numerical experiments and real data analyses, we see that the proposed method outperforms past robust and sparse methods."}}
{"id": "K37oV9MWtrp", "cdate": 1483228800000, "mdate": null, "content": {"title": "Robust sparse Gaussian graphical modeling.", "abstract": "Gaussian graphical modeling is popular as a means of exploring network structures, such as gene regulatory networks and social networks. An L 1 penalized maximum likelihood approach is often used to learn high-dimensional graphical models. However, the penalized maximum likelihood procedure is sensitive to outliers. To overcome this problem, we introduce a robust estimation procedure based on the \u03b3 -divergence. The proposed method has a redescending property, which is a desirable feature in robust statistics. The parameter estimation procedure is constructed using the Majorize-Minimization algorithm, which guarantees that the objective function monotonically decreases at each iteration. Extensive simulation studies show that our procedure performs much better than the existing methods, in particular, when the contamination ratio is large. Two real data analyses are used for illustration purposes. Previous article in issue Next article in issue AMS 2010 subject classifications 62H12"}}
{"id": "qF84vGQIQD3", "cdate": 1420070400000, "mdate": null, "content": {"title": "Sparse principal component regression with adaptive loading.", "abstract": "Principal component regression (PCR) is a two-stage procedure that selects some principal components and then constructs a regression model regarding them as new explanatory variables. Note that the principal components are obtained from only explanatory variables and not considered with the response variable. To address this problem, we propose the sparse principal component regression (SPCR) that is a one-stage procedure for PCR. SPCR enables us to adaptively obtain sparse principal component loadings that are related to the response variable and select the number of principal components simultaneously. SPCR can be obtained by the convex optimization problem for each parameter with the coordinate descent algorithm. Monte Carlo simulations and real data analyses are performed to illustrate the effectiveness of SPCR."}}
