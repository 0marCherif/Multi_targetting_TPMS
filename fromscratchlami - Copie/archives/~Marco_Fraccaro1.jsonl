{"id": "DkxP5e2f77r", "cdate": 1546300800000, "mdate": null, "content": {"title": "Machine learning meets mathematical optimization to predict the optimal production of offshore wind parks", "abstract": "In this paper we propose a combination of Mathematical Optimization and Machine Learning to estimate the value of optimized solutions. In particular, we investigate if a machine, trained on a large number of optimized solutions, could accurately estimate the value of the optimized solution for new instances. In this paper we will focus on a specific application: the offshore wind farm layout optimization problem. Mixed Integer Programming models and other state-of-the-art optimization techniques, have been developed to solve this problem. Given the complexity of the problem and the big difference in production between optimized/non optimized solutions, it is not trivial to understand the potential value of a new site without running a complete optimization. This could be too time consuming if a lot of sites need to be evaluated, therefore we propose to use Machine Learning to quickly estimate the potential of new sites (i.e., to estimate the optimized production of a site without explicitly running the optimization). To do so, we trained and tested different Machine Learning models on a dataset of 3000+ optimized layouts found by the optimizer. Thanks to the close collaboration with a leading company in the energy sector, our model was trained on real-world data. Our results show that Machine Learning is able to efficiently estimate the value of optimized instances for the offshore wind farm layout problem."}}
{"id": "8P-aB9USRfz", "cdate": 1546300800000, "mdate": null, "content": {"title": "BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling", "abstract": "With the introduction of the variational autoencoder (VAE), probabilistic latent variable models have received renewed attention as powerful generative models. However, their performance in terms of test likelihood and quality of generated samples has been surpassed by autoregressive models without stochastic units. Furthermore, flow-based models have recently been shown to be an attractive alternative that scales well to high-dimensional data. In this paper we close the performance gap by constructing VAE models that can effectively utilize a deep hierarchy of stochastic variables and model complex covariance structures. We introduce the Bidirectional-Inference Variational Autoencoder (BIVA), characterized by a skip-connected generative model and an inference network formed by a bidirectional stochastic inference path. We show that BIVA reaches state-of-the-art test likelihoods, generates sharp and coherent natural images, and uses the hierarchy of latent variables to capture different aspects of the data distribution. We observe that BIVA, in contrast to recent results, can be used for anomaly detection. We attribute this to the hierarchy of latent variables which is able to extract high-level semantic features. Finally, we extend BIVA to semi-supervised classification tasks and show that it performs comparably to state-of-the-art results by generative adversarial networks."}}
{"id": "Sybn25buWr", "cdate": 1514764800000, "mdate": null, "content": {"title": "Generative Temporal Models with Spatial Memory for Partially Observed Environments", "abstract": "In model-based reinforcement learning, generative and temporal models of environments can be leveraged to boost agent performance, either by tuning the agent\u2019s representations during training or vi..."}}
{"id": "FyLI2zjEXoH", "cdate": 1514764800000, "mdate": 1696074792470, "content": {"title": "Deep Latent Variable Models for Sequential Data", "abstract": ""}}
{"id": "ryEt5DWObB", "cdate": 1483228800000, "mdate": null, "content": {"title": "A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning", "abstract": "This paper takes a step towards temporal reasoning in a dynamically changing video, not in the pixel space that constitutes its frames, but in a latent space that describes the non-linear dynamics of the objects in its world. We introduce the Kalman variational auto-encoder, a framework for unsupervised learning of sequential data that disentangles two latent representations: an object's representation, coming from a recognition model, and a latent state describing its dynamics. As a result, the evolution of the world can be imagined and missing data imputed, both without the need to generate high dimensional frames at each time step. The model is trained end-to-end on videos of a variety of simulated physical systems, and outperforms competing methods in generative and missing data imputation tasks."}}
{"id": "_4ERfOP8Ci", "cdate": 1483228800000, "mdate": null, "content": {"title": "Semi-Supervised Generation with Cluster-aware Generative Models", "abstract": "Deep generative models trained with large amounts of unlabelled data have proven to be powerful within the domain of unsupervised learning. Many real life data sets contain a small amount of labelled data points, that are typically disregarded when training generative models. We propose the Cluster-aware Generative Model, that uses unlabelled information to infer a latent representation that models the natural clustering of the data, and additional labelled data points to refine this clustering. The generative performances of the model significantly improve when labelled information is exploited, obtaining a log-likelihood of -79.38 nats on permutation invariant MNIST, while also achieving competitive semi-supervised classification accuracies. The model can also be trained fully unsupervised, and still improve the log-likelihood performance with respect to related methods."}}
{"id": "GRuB5lGrxfT", "cdate": 1483228800000, "mdate": null, "content": {"title": "A deep learning approach to adherence detection for type 2 diabetics", "abstract": "Diabetes has become one of the biggest health problems in the world. In this context, adherence to insulin treatment is essential in order to avoid life-threatening complications. In this pilot study, a novel adherence detection algorithm using Deep Learning (DL) approaches was developed for type 2 diabetes (T2D) patients, based on simulated Continuous Glucose Monitoring (CGM) signals. A large and diverse amount of CGM signals were simulated for T2D patients using a T2D adapted version of the Medtronic Virtual Patient (MVP) model for T1D. By using these signals, different classification algorithms were compared using a comprehensive grid search. We contrast a standard logistic regression baseline to Multi- Layer Perceptrons (MLPs) and Convolutional Neural Networks (CNNs). The best classification performance with an average accuracy of 77:5% was achieved with CNN. Hence, this indicates the potential of DL, when considering adherence detection systems for T2D patients."}}
{"id": "By-Ad_ZOWS", "cdate": 1451606400000, "mdate": null, "content": {"title": "Sequential Neural Models with Stochastic Layers", "abstract": "How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks? This paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. The clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model\u2019s posterior distribution. By retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the Blizzard and TIMIT speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling."}}
{"id": "BJW73Tld-B", "cdate": 1451606400000, "mdate": null, "content": {"title": "Indexable Probabilistic Matrix Factorization for Maximum Inner Product Search", "abstract": "The Maximum Inner Product Search (MIPS) problem, prevalent in matrix factorization-based recommender systems, scales linearly with the number of objects to score. Recent work has shown that clever post-processing steps can turn the MIPS problem into a nearest neighbour one, allowing sublinear retrieval time either through Locality Sensitive Hashing or various tree structures that partition the Euclidian space. This work shows that instead of employing post-processing steps, substantially faster retrieval times can be achieved for the same accuracy when inference is not decoupled from the indexing process. By framing matrix factorization to be natively indexable, so that any solution is immediately sublinearly searchable, we use the machinery of Machine Learning to best learn such a solution. We introduce Indexable Probabilistic Matrix Factorization (IPMF) to shift the traditional post-processing complexity into the training phase of the model. Its inference procedure is based on Geodesic Monte Carlo, and adds minimal additional computational cost to standard Monte Carlo methods for matrix factorization. By coupling inference and indexing in this way, we achieve more than a 50% improvement in retrieval time against two state of the art methods, for a given level of accuracy in the recommendations of two large-scale recommender systems."}}
