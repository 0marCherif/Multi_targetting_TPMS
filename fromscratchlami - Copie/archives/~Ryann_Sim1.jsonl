{"id": "bbI9NtVd3-o", "cdate": 1672531200000, "mdate": 1682321038275, "content": {"title": "Min-Max Optimization Made Simple: Approximating the Proximal Point Method via Contraction Maps", "abstract": "In this paper we present a first-order method that admits near-optimal convergence rates for convex/concave min-max problems while requiring a simple and intuitive analysis. Similarly to the seminal work of Nemirovski [26] and the recent approach of Piliouras et al. [28] in normal form games, our work is based on the fact that the update rule of the Proximal Point method (PP) can be approximated up to accuracy \u03b5 with only O (log 1/\u03b5) additional gradient-calls through the iterations of a contraction map. Then combining the analysis of (PP) method with an error-propagation analysis we establish that the resulting first order method, called Clairvoyant Extra Gradient, admits near-optimal time-average convergence for general domains and last-iterate convergence in the unconstrained case."}}
{"id": "P-v-C5y34p", "cdate": 1672531200000, "mdate": 1682326468058, "content": {"title": "Quantum Potential Games, Replicator Dynamics, and the Separability Problem", "abstract": "Gamification is an emerging trend in the field of machine learning that presents a novel approach to solving optimization problems by transforming them into game-like scenarios. This paradigm shift allows for the development of robust, easily implementable, and parallelizable algorithms for hard optimization problems. In our work, we use gamification to tackle the Best Separable State (BSS) problem, a fundamental problem in quantum information theory that involves linear optimization over the set of separable quantum states. To achieve this we introduce and study quantum analogues of common-interest games (CIGs) and potential games where players have density matrices as strategies and their interests are perfectly aligned. We bridge the gap between optimization and game theory by establishing the equivalence between KKT (first-order stationary) points of a BSS instance and the Nash equilibria of its corresponding quantum CIG. Taking the perspective of learning in games, we introduce non-commutative extensions of the continuous-time replicator dynamics and the discrete-time Baum-Eagon/linear multiplicative weights update for learning in quantum CIGs, which also serve as decentralized algorithms for the BSS problem. We show that the common utility/objective value of a BSS instance is strictly increasing along trajectories of our algorithms, and finally corroborate our theoretical findings through extensive experiments."}}
{"id": "suplyBhTDjC", "cdate": 1652737545171, "mdate": null, "content": {"title": "Beyond Time-Average Convergence: Near-Optimal Uncoupled Online Learning via Clairvoyant Multiplicative Weights Update", "abstract": "In this paper we provide a novel and simple algorithm, Clairvoyant Multiplicative Weights Updates (CMWU), for convergence to \\textit{Coarse Correlated Equilibria} (CCE) in general games. CMWU effectively corresponds to the standard MWU algorithm but where all agents, when updating their mixed strategies, use the payoff profiles based on tomorrow's behavior, i.e. the agents are clairvoyant. CMWU achieves constant regret of $\\ln(m)/\\eta$ in all normal-form games with m actions and fixed step-sizes $\\eta$. Although CMWU encodes in its definition a fixed point computation, which in principle could result in dynamics that are neither computationally efficient nor uncoupled, we show that both of these issues can be largely circumvented. Specifically, as long as the step-size $\\eta$ is upper bounded by $\\frac{1}{(n-1)V}$, where $n$ is the number of agents and $[0,V]$ is the payoff range, then the CMWU updates can be computed linearly fast via a contraction map. This implementation results in an uncoupled online learning dynamic that admits a $O(\\log T)$-sparse sub-sequence where each agent experiences at most $O(nV\\log m)$ regret. This implies that the CMWU dynamics converge with rate $O(nV \\log m \\log T / T)$ to a CCE and improves on the current state-of-the-art convergence rate. "}}
{"id": "FFZYhY2z3j", "cdate": 1652737543264, "mdate": null, "content": {"title": "Matrix Multiplicative Weights Updates in Quantum Zero-Sum Games: Conservation Laws & Recurrence", "abstract": "Recent advances in quantum computing and in particular, the introduction of quantum GANs, have led to increased interest in quantum zero-sum game theory, extending the scope of learning algorithms for classical games into the quantum realm. In this paper, we focus on learning in quantum zero-sum games under Matrix Multiplicative Weights Update (a generalization of the multiplicative weights update method) and its continuous analogue, Quantum Replicator Dynamics. When each player selects their state according to quantum replicator dynamics, we show that the system exhibits conservation laws in a quantum-information theoretic sense. Moreover, we show that the system exhibits Poincare recurrence, meaning that almost all orbits return arbitrarily close to their initial conditions infinitely often. Our analysis generalizes previous results in the case of classical games."}}
{"id": "kY6yrDSI8H", "cdate": 1640995200000, "mdate": 1682326468039, "content": {"title": "Matrix Multiplicative Weights Updates in Quantum Zero-Sum Games: Conservation Laws & Recurrence", "abstract": "Recent advances in quantum computing and in particular, the introduction of quantum GANs, have led to increased interest in quantum zero-sum game theory, extending the scope of learning algorithms for classical games into the quantum realm. In this paper, we focus on learning in quantum zero-sum games under Matrix Multiplicative Weights Update (a generalization of the multiplicative weights update method) and its continuous analogue, Quantum Replicator Dynamics. When each player selects their state according to quantum replicator dynamics, we show that the system exhibits conservation laws in a quantum-information theoretic sense. Moreover, we show that the system exhibits Poincare recurrence, meaning that almost all orbits return arbitrarily close to their initial conditions infinitely often. Our analysis generalizes previous results in the case of classical games."}}
{"id": "agKdNqT1d8", "cdate": 1640995200000, "mdate": 1682321038255, "content": {"title": "Fast Convergence of Optimistic Gradient Ascent in Network Zero-Sum Extensive Form Games", "abstract": "The study of learning in games has thus far focused primarily on normal form games. In contrast, our understanding of learning in extensive form games (EFGs) and particularly in EFGs with many agents lags behind, despite them being closer in nature to many real world applications. We consider the natural class of Network Zero-Sum Extensive Form Games, which combines the global zero-sum property of agent payoffs, the efficient representation of graphical games as well the expressive power of EFGs. We examine the convergence properties of Optimistic Gradient Ascent (OGA) in these games. We prove that the time-average behavior of such online learning dynamics exhibits O(1/T) rate of convergence to the set of Nash Equilibria. Moreover, we show that the day-to-day behavior also converges to a Nash with rate $$O(c^{-t})$$ for some game-dependent constant $$c>0$$ ."}}
{"id": "oykI6Kmq3Xi", "cdate": 1632875671454, "mdate": null, "content": {"title": "Fast Convergence of Optimistic Gradient Ascent in Network Zero-Sum Extensive Form Games", "abstract": "The study of learning in games has thus far focused primarily on normal form games. In contrast, our understanding of learning in extensive form games (EFG) and particularly in EFGs with many agents lags far behind, despite them being closer in nature to many real world applications. We consider the natural class of Network Zero-Sum Extensive Form Games, which combines the global zero-sum property of agent payoffs, the efficient representation of graphical games as well the expressive power of EFGs. We examine the convergence properties of Optimistic Gradient Ascent (OGA) in these games. We prove that the time-average behavior of such online learning dynamics exhibits $O(1/T)$ rate of convergence to the set of Nash equilibria. Moreover, we show that the day-to-day behavior also converges to Nash with rate $O(c^{-t})$ for some game-dependent constant $c > 0$."}}
{"id": "ePgNDwRKJV", "cdate": 1621629931533, "mdate": null, "content": {"title": "Online Learning in Periodic Zero-Sum Games", "abstract": "A seminal result in game theory is von Neumann's minmax theorem, which states that zero-sum games admit an essentially unique equilibrium solution. Classical learning results build on this theorem to show that online no-regret dynamics converge to an equilibrium in a time-average sense in zero-sum games. In the past several years, a key research direction has focused on characterizing the transient behavior of such dynamics. General results in this direction show that broad classes of online learning dynamics are cyclic, and formally Poincar\\'{e} recurrent, in zero-sum games. We analyze the robustness of these online learning behaviors in the case of periodic zero-sum games with a time-invariant equilibrium. This model generalizes the usual repeated game formulation while also being a realistic and natural model of a repeated competition between players that depends on exogenous environmental variations such as time-of-day effects, week-to-week trends, and seasonality. Interestingly, time-average convergence may fail even in the simplest such settings, in spite of the equilibrium being fixed. In contrast, using novel analysis methods, we show that Poincar\\'{e} recurrence provably generalizes despite the complex, non-autonomous nature of these dynamical systems."}}
{"id": "nCaqrdiLgS2", "cdate": 1609459200000, "mdate": 1682321038265, "content": {"title": "Optimal No-Regret Learning in General Games: Bounded Regret with Unbounded Step-Sizes via Clairvoyant MWU", "abstract": "In this paper, we provide a novel and simple algorithm, Clairvoyant Multiplicative Weights Updates (CMWU) for regret minimization in general games. CMWU effectively corresponds to the standard MWU algorithm but where all agents, when updating their mixed strategies, use the payoff profiles based on tomorrow's behavior, i.e. the agents are clairvoyant. CMWU achieves constant regret of $\\ln(m)/\\eta$ in all normal-form games with m actions and fixed step-sizes $\\eta$. Although CMWU encodes in its definition a fixed point computation, which in principle could result in dynamics that are neither computationally efficient nor uncoupled, we show that both of these issues can be largely circumvented. Specifically, as long as the step-size $\\eta$ is upper bounded by $\\frac{1}{(n-1)V}$, where $n$ is the number of agents and $[0,V]$ is the payoff range, then the CMWU updates can be computed linearly fast via a contraction map. This implementation results in an uncoupled online learning dynamic that admits a $O (\\log T)$-sparse sub-sequence where each agent experiences at most $O(nV\\log m)$ regret. This implies that the CMWU dynamics converge with rate $O(nV \\log m \\log T / T)$ to a \\textit{Coarse Correlated Equilibrium}. The latter improves on the current state-of-the-art convergence rate of \\textit{uncoupled online learning dynamics} \\cite{daskalakis2021near,anagnostides2021near}."}}
{"id": "B0Tij3Euw3", "cdate": 1609459200000, "mdate": 1682321038245, "content": {"title": "Online Learning in Periodic Zero-Sum Games", "abstract": "A seminal result in game theory is von Neumann's minmax theorem, which states that zero-sum games admit an essentially unique equilibrium solution. Classical learning results build on this theorem to show that online no-regret dynamics converge to an equilibrium in a time-average sense in zero-sum games. In the past several years, a key research direction has focused on characterizing the transient behavior of such dynamics. General results in this direction show that broad classes of online learning dynamics are cyclic, and formally Poincar\\'{e} recurrent, in zero-sum games. We analyze the robustness of these online learning behaviors in the case of periodic zero-sum games with a time-invariant equilibrium. This model generalizes the usual repeated game formulation while also being a realistic and natural model of a repeated competition between players that depends on exogenous environmental variations such as time-of-day effects, week-to-week trends, and seasonality. Interestingly, time-average convergence may fail even in the simplest such settings, in spite of the equilibrium being fixed. In contrast, using novel analysis methods, we show that Poincar\\'{e} recurrence provably generalizes despite the complex, non-autonomous nature of these dynamical systems."}}
