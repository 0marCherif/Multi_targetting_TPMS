{"id": "bbOYRQo3cEW", "cdate": 1672531200000, "mdate": 1678066677717, "content": {"title": "Grassmannian learning mutual subspace method for image set recognition", "abstract": ""}}
{"id": "Lzpr_OGiZdR", "cdate": 1672531200000, "mdate": 1682327993442, "content": {"title": "Discriminant Feature Extraction by Generalized Difference Subspace", "abstract": "In this paper, we reveal the discriminant capacity of orthogonal data projection onto the generalized difference subspace (GDS), both theoretically and experimentally. In our previous work, we demonstrated that the GDS projection works as a quasi-orthogonalization of class subspaces, which is an effective feature extraction for subspace based classifiers. Here, we further show that GDS projection also works as a discriminant feature extraction through a similar mechanism to the Fisher discriminant analysis (FDA). A direct proof of the connection between GDS projection and FDA is difficult due to the significant difference in their formulations. To circumvent the complication, we first introduce geometrical Fisher discriminant analysis (gFDA) based on a simplified Fisher criterion. It is derived from a heuristic yet practically plausible assumption: the direction of the sample mean vector of a class is largely aligned to the first principal component vector of the class, given that the principal component analysis (PCA) is applied without data centering. gFDA works stably even under few samples, bypassing the small sample size (SSS) problem of FDA. We then prove that gFDA is equivalent to GDS projection with a small correction term. This equivalence ensures GDS projection to inherit the discriminant ability from FDA via gFDA. Furthermore, we discuss two useful extensions of these methods, 1) a nonlinear extension by kernel trick, 2) a combination with CNN features. The equivalence and the effectiveness of the extensions have been verified through extensive experiments on the extended Yale B+, CMU face database, ALOI, ETH80, MNIST, and CIFAR10, mainly focusing on image recognition under small samples."}}
{"id": "wcSbtWPKQP", "cdate": 1640995200000, "mdate": 1668040523060, "content": {"title": "Rotation Regularization Without Rotation", "abstract": "In various visual classification tasks, we enjoy significant performance improvement by deep convolutional neural networks (CNNs). To further boost performance, it is effective to regularize feature representation learning of CNNs such as by considering margin to improve feature distribution across classes. In this paper, we propose a regularization method based on random rotation of feature vectors. Random rotation is derived from cone representation to describe angular margin of a sample. While it induces geometric regularization to randomly rotate vectors by means of rotation matrices, we theoretically formulate the regularization in a statistical form which excludes costly geometric rotation as well as effectively imposes rotation-based regularization on classification in training CNNs. In the experiments on classification tasks, the method is thoroughly evaluated from various aspects, while producing favorable performance compared to the other regularization methods. Codes are available at https://github.com/tk1980/StatRot ."}}
{"id": "K5NEJGPYw9", "cdate": 1640995200000, "mdate": 1682327993456, "content": {"title": "Mutual Conditional Probability for Self-Supervised Learning", "abstract": ""}}
{"id": "BrI0I42tTw", "cdate": 1640995200000, "mdate": 1682327993474, "content": {"title": "Extractive Knowledge Distillation", "abstract": "Knowledge distillation (KD) transfers knowledge of a teacher model to improve performance of a student model which is usually equipped with lower capacity. In the KD framework, however, it is unclear what kind of knowledge is effective and how it is transferred. This paper analyzes a KD process to explore the key factors. In a KD formulation, softmax temperature entangles three main components of student and teacher probabilities and a weight for KD, making it hard to analyze contributions of those factors separately. We disentangle those components so as to further analyze especially the temperature and improve the components respectively. Based on the analysis about temperature and uniformity of the teacher probability, we propose a method, called extractive distillation, for extracting effective knowledge from the teacher model. The extractive KD touches only teacher knowledge, thus being applicable to various KD methods. In the experiments on image classification tasks using Cifar-100 and TinyImageNet datasets, we demonstrate that the proposed method outperforms the other KD methods and analyze feature representation to show its effectiveness in the framework of transfer learning."}}
{"id": "mwDVvn76GI", "cdate": 1609459200000, "mdate": 1668040522983, "content": {"title": "Grassmannian learning mutual subspace method for image set recognition", "abstract": "This paper addresses the problem of object recognition given a set of images as input (e.g., multiple camera sources and video frames). Convolutional neural network (CNN)-based frameworks do not exploit these sets effectively, processing a pattern as observed, not capturing the underlying feature distribution as it does not consider the variance of images in the set. To address this issue, we propose the Grassmannian learning mutual subspace method (G-LMSM), a NN layer embedded on top of CNNs as a classifier, that can process image sets more effectively and can be trained in an end-to-end manner. The image set is represented by a low-dimensional input subspace; and this input subspace is matched with reference subspaces by a similarity of their canonical angles, an interpretable and easy to compute metric. The key idea of G-LMSM is that the reference subspaces are learned as points on the Grassmann manifold, optimized with Riemannian stochastic gradient descent. This learning is stable, efficient and theoretically well-grounded. We demonstrate the effectiveness of our proposed method on hand shape recognition, face identification, and facial emotion recognition."}}
{"id": "YW1g5-pHp2", "cdate": 1609459200000, "mdate": 1668040523037, "content": {"title": "T-vMF Similarity for Regularizing Intra-Class Feature Distribution", "abstract": "Deep convolutional neural networks (CNNs) leverage large-scale training dataset to produce remarkable performance on various image classification tasks. It, however, is difficult to effectively train the CNNs on some realistic learning situations such as regarding class imbalance, small-scale and label noises. Regularizing CNNs works well on learning with such deteriorated training datasets by mitigating overfitting issues. In this work, we propose a method to effectively impose regularization on feature representation learning. By focusing on the angle between a feature and a classifier which is embedded in cosine similarity at the classification layer, we formulate a novel similarity beyond the cosine based on von Mises-Fisher distribution of directional statistics. In contrast to the cosine similarity, our similarity is compact while having heavy tail, which contributes to regularizing intra-class feature distribution to improve generalization performance. Through the experiments on some realistic learning situations such as of imbalance, small-scale and noisy labels, we demonstrate the effectiveness of the proposed method for training CNNs, in comparison to the other regularization methods. Codes are available at https://github.com/tk1980/tvMF."}}
{"id": "IL0nj50IqjV", "cdate": 1609459200000, "mdate": 1668040523134, "content": {"title": "Group Softmax Loss with Discriminative Feature Grouping", "abstract": ""}}
{"id": "H5AAOFT5jw", "cdate": 1609459200000, "mdate": 1668040523057, "content": {"title": "Phase-wise Parameter Aggregation For Improving SGD Optimization", "abstract": ""}}
{"id": "EPvo65rYzL", "cdate": 1580483929514, "mdate": null, "content": {"title": "Spiral-Net with F1-based Optimization For Image-Based Crack Detection", "abstract": "Detecting cracks on concrete surface images is a key inspection for maintaining infrastructures such as bridge and tunnels. From the viewpoint of computer vision, the task of automatic crack detection poses two challenges. First, since the cracks are visually depicted by subtle patterns and also exhibit similar appearance to the other structural patterns, it is difficult to discriminatively characterize such less distinctive and finer defects. Second, the cracks are scarcely found, making the number of training samples for cracks significantly smaller than that of the other normal samples to be distinguished from the cracks. This is regarded as a class imbalance problem where the classifier is highly biased toward majority classes. In this study, we propose two methods to address these issues in the framework of deep learning for crack detection: a novel network, called Spiral-Net, and an effective optimization method to train the network. The proposed network is extended from U-Net to extract more detailed visual features, and the optimization method is formulated based on F1 score (F-measure) for properly learning the network even on the highly imbalanced training samples. The experimental results on crack detection demonstrate that the two proposed methods contribute to performance improvement individually and jointly."}}
