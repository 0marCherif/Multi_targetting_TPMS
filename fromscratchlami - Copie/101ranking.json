{"paper_link":"https:\/\/openreview.net\/forum?id=8JCZe7QrPy","title":"Systematic Visual Reasoning through Object-Centric Relational Abstraction","modified_abstract":"Inspired by significant strides in the field of computer vision, particularly in the development of neural architectures capable of disentangling complex visual inputs into object-centric representations for tasks such as few-shot concept learning, answering complex queries, object detection, and classification, this study introduces Object-Centric Relational Abstraction (OCRA), a novel model designed to harness and extend these advancements. Human visual reasoning is characterized by an ability to identify abstract patterns from only a small number of exemplars, and to systematically generalize those patterns to novel inputs. This capacity relies heavily on our ability to represent complex visual inputs in terms of both objects and their relations. Few-shot learning, an area of intense interest in contemporary research, particularly enriches this pursuit by enabling machines to emulate this aspect of human cognitive function. While recent work in computer vision has introduced models with the capacity to extract object-centric representations, thereby enabling the processing of multi-object visual inputs in 3D environments and across various classifications, these models often fall short of the systematic generalization demonstrated by human reasoning. Conversely, models that have incorporated inductive biases for relational abstraction to achieve systematic generalization have typically assumed the availability of object-focused inputs. By integrating these two foundational approaches, leveraging state-of-the-art neural architectures, OCRA extracts explicit representations of both objects and abstract relations, achieving strong systematic generalization in tasks involving complex visual displays, including a novel dataset, CLEVR-ART, with increased visual complexity. Our work builds on previous innovations in answering queries and classification to tackle one of the key challenges in computer vision: bridging the gap between object recognition and relational reasoning for systematic visual understanding.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Adam_W_Harley1","manipulated_ranking":1,"natural_ranking":101,"id":"8JCZe7QrPy","original_abstract":"Human visual reasoning is characterized by an ability to identify abstract patterns from only a small number of examples, and to systematically generalize those patterns to novel inputs. This capacity depends in large part on our ability to represent complex visual inputs in terms of both objects and relations. Recent work in computer vision has introduced models with the capacity to extract object-centric representations, leading to the ability to process multi-object visual inputs, but falling short of the systematic generalization displayed by human reasoning. Other recent models have employed inductive biases for relational abstraction to achieve systematic generalization of learned abstract rules, but have generally assumed the presence of object-focused inputs. Here, we combine these two approaches, introducing Object-Centric Relational Abstraction (OCRA), a model that extracts explicit representations of both objects and abstract relations, and achieves strong systematic generalization in tasks (including a novel dataset, CLEVR-ART, with greater visual complexity) involving complex visual displays.","target":"~Adam_W_Harley1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=NWEbeI2HNQ","title":"Prefix-Tree Decoding for Predicting Mass Spectra from Molecules","modified_abstract":"Inspired by significant advancements in computational methodologies for large-scale data analysis, such as the scalable graph construction technique introduced by Stars for clustering and learning in massive datasets, this paper presents a novel approach for computational predictions of mass spectra from molecules. Our method addresses the limitations of current predictive tools that either rely on rigid, combinatorial molecule fragmentation with constraints on potential rearrangements and poor time complexity or use lossy, nonphysical discretized spectra vectors. By conceptualizing mass spectra as sets of molecular formulae, which are multisets of atoms, and encoding an input molecular graph, we introduce an innovative intermediate strategy that decodes a set of molecular subformulae, navigating sparse representations of complex metabolomic work. Each subformula specifies a predicted peak in the mass spectrum, with intensities predicted by a second model that tasks our understanding of physicochemical interactions toward improved learning outcomes and performance benchmarks. Our approach leverages a prefix tree structure to efficiently decode the formula set atom-type by atom-type, navigating the combinatorial possibilities for molecular subformulae amidst datasets scaling to billions of molecules. This method represents a general technique for ordered multiset decoding, offering a promising direction for mass spectra prediction tasks, especially highlighting its application in tasks aimed at advancing the discovery of clinically relevant metabolites through accurate and efficient mass spectra prediction.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Warren_Schudy1","manipulated_ranking":2,"natural_ranking":101,"id":"NWEbeI2HNQ","original_abstract":"Computational predictions of mass spectra from molecules have enabled the discovery of clinically relevant metabolites. However, such predictive tools are still limited as they occupy one of two extremes, either operating  (a) by fragmenting molecules combinatorially with overly rigid constraints on potential rearrangements and poor time complexity or (b) by decoding lossy and nonphysical discretized spectra vectors. In this work, we use a new intermediate strategy for predicting mass spectra from molecules by treating mass spectra as sets of molecular formulae, which are themselves multisets of atoms. After first encoding an input molecular graph, we decode a set of molecular subformulae, each of which specify a predicted peak in the mass spectrum, the intensities of which are predicted by a second model. Our key insight is to overcome the combinatorial possibilities for molecular subformulae by decoding the formula set using a prefix tree structure, atom-type by atom-type, representing a general method for ordered multiset decoding. We show promising empirical results on mass spectra prediction tasks.","target":"~Warren_Schudy1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=vO6ZdPWaHc","title":"Data Pruning via Moving-one-Sample-out","modified_abstract":"Inspired by recent advancements in dataset condensation techniques, which seek to optimize the efficiency of training datasets by retaining only the most informative samples, this study introduces a novel data-pruning approach named moving-one-sample-out (MoSo). The MoSo approach aims to identify and remove the least informative samples from the training set by determining the impact of each sample on the optimal empirical risk. This is gauged by the extent to which the empirical risk changes when a particular sample is excluded from the dataset. To circumvent the computationally intensive leaving-one-out retraining process, we propose an efficient first-order approximator leveraging gradient information from different training stages. The foundational premise of our method is that samples with gradients consistently aligned with the average gradient of the training set are deemed more informative, scoring them higher for retention. Intuitively, if a sample's gradient agrees with the average gradient vector, optimizing the network using this sample will likely benefit the entire dataset similarly. Experimental outcomes underscore MoSo's capability to significantly avert performance deterioration at high pruning ratios, establishing its superiority over existing state-of-the-art methods across a variety of scenarios. The pragmatic insights derived from the DC-BENCH benchmark, focusing on dataset condensation, echo through our approach, highlighting its potential in streamlining datasets while preserving, or even enhancing, the training quality, effectiveness, and efficiency of machine learning models. Conducted experiments further validate the synthesized and encoded strategies implemented within MoSo, enhancing its evaluators' capability to discern the contribution of individual samples. Our methodology and results have been open-sourced to foster further research, evaluations, and development within the community.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Justin_Cui1","manipulated_ranking":1,"natural_ranking":101,"id":"vO6ZdPWaHc","original_abstract":"In this paper, we propose a novel data-pruning approach called moving-one-sample-out (MoSo), which aims to identify and remove the least informative samples from the training set. The core insight behind MoSo is to determine the importance of each sample by assessing its impact on the optimal empirical risk. This is achieved by measuring the extent to which the empirical risk changes when a particular sample is excluded from the training set. Instead of using the computationally expensive leaving-one-out-retraining procedure, we propose an efficient first-order approximator that only requires gradient information from different training stages. The key idea behind our approximation is that samples with gradients that are consistently aligned with the average gradient of the training set are more informative and should receive higher scores, which could be intuitively understood as follows: if the gradient from a specific sample is consistent with the average gradient vector, it implies that optimizing the network using the sample will yield a similar effect on all remaining samples.  \nExperimental results demonstrate that MoSo effectively mitigates severe performance degradation at high pruning ratios and achieves satisfactory performance across various settings. Experimental results demonstrate that MoSo effectively mitigates severe performance degradation at high pruning ratios and outperforms state-of-the-art methods by a large margin across various settings.","target":"~Justin_Cui1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=THfl8hdVxH","title":"White-Box Transformers via Sparse Rate Reduction","modified_abstract":"In addressing the objective of representation learning to compress and transform the distribution of data towards a mixture of low-dimensional Gaussian distributions, this work is inspired by concepts such as iterative optimization and the pivotal role of deep network architectures like the transformers and Inception-v4, as well as the enhancements introduced by residual connections. Our study builds on the foundations laid by extensive research in the field, particularly focusing on the optimization of a unified objective function known as sparse rate reduction. We posit that transformers can be inherently understood as executing incremental optimizations of this objective, where a standard transformer block's components\u2014multi-head self-attention and multi-layer perceptron\u2014serve distinct purposes in compressing and sparsifying token sets, respectively. This interpretation ushers in a class of transformer-like networks that stand out for their mathematical interpretability, simplicity, and significantly thin structures. Despite their stripped-down structures, these networks demonstrate compelling abilities to optimize the sparse rate reduction objective, as evidenced by their performance on classification tasks on large-scale datasets like ImageNet, closely rivaling that of highly engineered counterparts such as the Vision Transformer (ViT). This connection and evolution from traditional deep learning architectures emphasize the significance of iterative optimization mechanisms, single-frame analysis, and rigorous training methodologies in achieving state-of-the-art results in image classification. Furthermore, our findings provide clear evidence that reinforces the theory behind white-box transformers, offering a mathematically transparent lens to examine and understand deep learning architectures. Code is available online.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Sergey_Ioffe3","manipulated_ranking":1,"natural_ranking":101,"id":"THfl8hdVxH","original_abstract":"In this paper, we contend that the objective of representation learning is  to compress and transform the distribution of the data, say sets of tokens, towards a mixture of low-dimensional Gaussian distributions supported on incoherent subspaces. The quality of the final representation can be measured by a unified objective function called sparse rate reduction. From this perspective, popular deep networks such as transformers can be naturally viewed as realizing iterative schemes to optimize this objective incrementally. Particularly, we show that the standard transformer block can be derived from alternating optimization on complementary parts of this objective: the multi-head self-attention operator can be viewed as a gradient descent step to compress the token sets by minimizing their lossy coding rate, and the subsequent multi-layer perceptron can be viewed as attempting to sparsify the representation of the tokens. This leads to a family of white-box transformer-like deep network architectures which are mathematically fully interpretable. Despite their simplicity, experiments show that these networks indeed learn to optimize the designed objective: they compress and sparsify representations of large-scale real-world vision datasets such as ImageNet, and achieve performance very close to thoroughly engineered transformers such as ViT. \nCode is at https:\/\/github.com\/Ma-Lab-Berkeley\/CRATE.","target":"~Sergey_Ioffe3"}
{"paper_link":"https:\/\/openreview.net\/forum?id=WYYpxVsKpR","title":"Necessary and Sufficient Conditions for Optimal Decision Trees using Dynamic Programming","modified_abstract":"Drawing on the precedent of scaling multi-objective optimization through novel algorithmic approaches, as exemplified in the realm of multi-objective online learning, this paper addresses the scalability challenge inherent in the global optimization of decision trees. Such optimization is vital for enhancing decision tree accuracy, size, and understandability, but is often limited by the scalability of general-purpose solvers. Leveraging the structural advantages of decision trees, we investigate the use of dynamic programming techniques, which significantly improve scalability by treating subtrees as independent subproblems. This is contingent on the separability of the optimization objective for subtrees. We elucidate the necessary and sufficient conditions for this separability and extend previous dynamic programming methods into a versatile framework capable of optimizing any set of separable objectives and constraints, including considerations of regret minimization and bandit problems within online learning paradigms. Our experiments across five distinct application domains demonstrate the framework's broad applicability and its superior scalability over general-purpose optimization solvers, showcasing its adeptness at handling online learning scenarios with the min-norm problem integrated as a constraint. Furthermore, by exploring the paper's focus on constrained as opposed to unconstrained optimization scenarios within decision trees, we underscore the methodology's potential for refined problem solving in diverse settings.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Jiyan_Jiang1","manipulated_ranking":1,"natural_ranking":101,"id":"WYYpxVsKpR","original_abstract":"Global optimization of decision trees has shown to be promising in terms of accuracy, size, and consequently human comprehensibility. \nHowever, many of the methods used rely on general-purpose solvers for which scalability remains an issue.\nDynamic programming methods have been shown to scale much better because they exploit the tree structure by solving subtrees as independent subproblems. However, this only works when an objective can be optimized separately for subtrees.\nWe explore this relationship in detail and show the necessary and sufficient conditions for such separability and generalize previous dynamic programming approaches into a framework that can optimize any combination of separable objectives and constraints.\nExperiments on five application domains show the general applicability of this framework, while outperforming the scalability of general-purpose solvers by a large margin.","target":"~Jiyan_Jiang1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=T5h69frFF7","title":"UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures","modified_abstract":"Leveraging insights from recent advances in audio processing and automatic speech recognition (ASR), such as those aimed at detecting filled pauses (or fillers) in speech using both audio and textual information, this study introduces UNSSOR, a novel approach for unsupervised neural speech separation in reverberant conditions. In scenarios where the number of microphones exceeds the number of concurrent speakers, employing over-determined training mixtures allows us to apply constraints based on the acoustic mixtures captured by each microphone, facilitating the extraction of individual speaker signals without supervised labels. UNSSOR deploys a deep neural network (DNN) trained to separate speaker voices from mixed signals by producing intermediate estimates for each speaker, which are then refined through linear filters. These filters are determined for each frequency sub-band by the forward convolutive prediction (FCP) algorithm, leveraging the mixtures and DNN estimates. A specific loss function is designed to minimize intra-source magnitude scattering, addressing the frequency permutation problem characteristic of sub-band processing methods through machine learning techniques. Although reliant on over-determined conditions during training, UNSSOR extends its application to under-determined scenarios, including monaural speech separation. Evaluation in reverberant environments with two-speaker mixtures validates UNSSOR's effectiveness and showcases proposing its potential for enhancing speech processing applications. Inclusion for the purpose of analyzing emotion detection as part of its feature set differentiates speakers more effectively by using transcribed speech for training, in addition to audio.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Constantinos_Karouzos1","manipulated_ranking":11,"natural_ranking":101,"id":"T5h69frFF7","original_abstract":"In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over-determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for $\\underline{u}$nsupervised $\\underline{n}$eural $\\underline{s}$peech $\\underline{s}$eparation by leveraging $\\underline{o}$ver-determined training mixtu$\\underline{r}$es. At each training step, we feed an input mixture to a deep neural network (DNN) to produce an intermediate estimate for each speaker, linearly filter the estimates, and optimize a loss so that, at each microphone, the filtered estimates of all the speakers can add up to the mixture to satisfy the above constraint. We show that this loss can promote unsupervised separation of speakers. The linear filters are computed in each sub-band based on the mixture and DNN estimates through the forward convolutive prediction (FCP) algorithm. To address the frequency permutation problem incurred by using sub-band FCP, a loss term based on minimizing intra-source magnitude scattering is proposed. Although UNSSOR requires over-determined training mixtures, we can train DNNs to achieve under-determined separation (e.g., unsupervised monaural speech separation). Evaluation results on two-speaker separation in reverberant conditions show the effectiveness and potential of UNSSOR.","target":"~Constantinos_Karouzos1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=CzAAbKOHQW","title":"Exploring and Interacting with the Set of Good Sparse Generalized Additive Models","modified_abstract":"The importance of facilitating interaction between machine learning models and domain experts has become increasingly recognized, especially given the limitations of the classical machine learning paradigm that typically yields a single, optimal solution. This recognition is grounded in the prior work on convex optimization, such as advancements in projection-free methods over the spectrahedron, highlighting the relevance and potential of exploring a broader solution space within various machine learning tasks, including those involving low-rank approximations and gradient-based methodologies. Our study extends this concept to the domain of sparse, generalized additive models (GAMs), addressing the challenge of exploring and interacting with the Rashomon set\u2014the set of all near-optimal models. We introduce algorithms to efficiently and accurately approximate this set with ellipsoids for fixed support sets and use these ellipsoids, alongside considerations of eigenvector dynamics for variable importance and shape function shifts, to further approximate Rashomon sets for various support sets. This methodology not only enables the study of variable importance within the model class and the identification of models that meet user-specified criteria (such as monotonicity and direct editing requirements) but also facilitates the examination of sudden shifts in shape functions through the lens of gradient changes, eigenvector movements, and signal processing techniques. Through experiments, we demonstrate the high fidelity of the approximated Rashomon set and its utility in addressing practical challenges, thereby bridging a significant gap between theoretical machine learning and real-world application requirements.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Dan_Garber1","manipulated_ranking":1,"natural_ranking":101,"id":"CzAAbKOHQW","original_abstract":"In real applications, interaction between machine learning models and domain experts is critical; however, the classical machine learning paradigm that usually produces only a single model does not facilitate such interaction. Approximating and exploring the Rashomon set, i.e., the set of all near-optimal models, addresses this practical challenge by providing the user with a searchable space containing a diverse set of models from which domain experts can choose. We present algorithms to efficiently and accurately approximate the Rashomon set of sparse, generalized additive models with ellipsoids for fixed support sets and use these ellipsoids to approximate Rashomon sets for many different support sets. The approximated Rashomon set serves as a cornerstone to solve practical challenges such as (1) studying the variable importance for the model class; (2) finding models under user-specified constraints (monotonicity, direct editing); and (3) investigating sudden changes in the shape functions. Experiments demonstrate the fidelity of the approximated Rashomon set and its effectiveness in solving practical challenges.","target":"~Dan_Garber1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=Vbm5UCaYeh","title":"Efficient Algorithms for Generalized Linear Bandits with Heavy-tailed Rewards","modified_abstract":"The exploration of efficient algorithms for handling heavy-tailed rewards in generalized linear bandits is inspired by recent advancements in optimistic posterior sampling for reinforcement learning, where the challenge of few samples and tight guarantees has been addressed with innovative sampling strategies and regret bounds. Our work extends these innovations to the domain of generalized linear bandits with heavy-tailed rewards, whose $(1+\\epsilon)$-th moment is bounded for some $\\epsilon\\in (0,1]$. The limitations of existing methods, which primarily focus on bounded or sub-Gaussian reward distributions, prompt the development of two novel algorithms tailored for real-world scenarios such as financial markets and web-advertising, where reward distributions can significantly deviate from these assumptions. Our truncation-based and mean-of-medians-based algorithms not only provide an almost optimal regret bound of $\\widetilde{O}(dT^{\\frac{1}{1+\\epsilon}})$, where $d$ is the dimension of contextual information and $T$ is the time horizon, but also incorporate practical features such as online learning capability, reduced computational requirements, and applicability to episodic settings where agents interact with environments in a sequential manner. Furthermore, these algorithms improve upon the regret bounds of existing solutions by a logarithmic factor when $\\epsilon=1$. Experimental evaluations underscore the effectiveness and efficiency of our proposed techniques, confirming their theoretical advantages and showcasing their prowess in environments defined by state-action pairs and informed decision-making by agents in an optimistic setting.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Alexey_Naumov1","manipulated_ranking":1,"natural_ranking":101,"id":"Vbm5UCaYeh","original_abstract":"This paper investigates the problem of generalized linear bandits with heavy-tailed rewards, whose $(1+\\epsilon)$-th moment is bounded for some $\\epsilon\\in (0,1]$. Although there exist methods for generalized linear bandits, most of them focus on bounded or sub-Gaussian rewards and are not well-suited for many real-world scenarios, such as financial markets and web-advertising. To address this issue, we propose two novel algorithms based on truncation and mean of medians. These algorithms achieve an almost optimal regret bound of $\\widetilde{O}(dT^{\\frac{1}{1+\\epsilon}})$, where $d$ is the dimension of contextual information and $T$ is the time horizon. Our truncation-based algorithm supports online learning, distinguishing it from existing truncation-based approaches. Additionally, our mean-of-medians-based algorithm requires only $O(\\log T)$ rewards and one estimator per epoch, making it more practical. Moreover, our algorithms improve the regret bounds by a logarithmic factor compared to existing algorithms when $\\epsilon=1$. Numerical experimental results confirm the merits of our algorithms.","target":"~Alexey_Naumov1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=9S8oVumknA","title":"Intervention Generalization: A View from Factor Graph Models","modified_abstract":"This research on intervention generalization extends the frontier of causal inference by building on recent insights from the field of conditional generative modeling and its implications for decision-making. Such foundational works provide a starting point for reconceptualizing the challenges of learning to generalize from past experiments and observational data to novel conditions, highlighting the potential for innovative approaches to decision-making beyond traditional reinforcement learning paradigms. One of the goals of causal inference is to generalize from past experiments and observational data to novel conditions. While it is in principle possible to eventually learn a mapping from a novel experimental condition to an outcome of interest, provided a sufficient variety of experiments is available in the training data, coping with a large combinatorial space of possible interventions is hard. Under a typical sparse experimental design, this mapping is ill-posed without relying on heavy regularization or prior distributions. Such assumptions may or may not be reliable, and can be hard to defend or test. In this paper, we explore how to warrant a leap from past experiments to novel conditions based on minimal assumptions about the factorization of the distribution of the manipulated system, communicated in the well-understood language of factor graph models. A postulated interventional factor model (IFM) may not always be informative, but it conveniently abstracts away a need for explicitly modeling unmeasured confounding and feedback mechanisms, leading to directly testable claims. Given an IFM and datasets from a collection of experimental regimes, we derive conditions for identifiability of the expected outcomes of new regimes never observed in these training data. We implement our framework using several efficient algorithms, and apply them on a range of semi-synthetic experiments.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Abhi_Gupta1","manipulated_ranking":35,"natural_ranking":101,"id":"9S8oVumknA","original_abstract":"One of the goals of causal inference is to generalize from past experiments and observational data to novel conditions. While it is in principle possible to eventually learn a mapping from a novel experimental condition to an outcome of interest, provided a sufficient variety of experiments is available in the training data, coping with a large combinatorial space of possible interventions is hard. Under a typical sparse experimental design, this mapping is ill-posed without relying on heavy regularization or prior distributions. Such assumptions may or may not be reliable, and can be hard to defend or test. In this paper, we take a close look at how to warrant a leap from past experiments to novel conditions based on minimal assumptions about the factorization of the distribution of the manipulated system, communicated in the well-understood language of factor graph models. A postulated interventional factor model (IFM) may not always be informative, but it conveniently abstracts away a need for explicitly modeling unmeasured confounding and feedback mechanisms, leading to directly testable claims. Given an IFM and datasets from a collection of experimental regimes, we derive conditions for identifiability of the expected outcomes of new regimes never observed in these training data. We implement our framework using several efficient algorithms, and apply them on a range of semi-synthetic experiments.","target":"~Abhi_Gupta1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=3CJOaJugMG","title":"On the Last-iterate Convergence in Time-varying Zero-sum Games: Extra Gradient Succeeds where Optimism Fails","modified_abstract":"The investigation of last-iterate convergence in two-player zero-sum games has evolved from simple bilinear and convex-concave scenarios to more complex ones satisfying the Minty Variational Inequality (MVI) condition, building on frameworks such as Multi-Objective Online Convex Optimization. This progression underscores the importance of understanding dynamic behaviors in algorithms under varying conditions, a challenge that extends beyond the static games traditionally analyzed. Our theoretical work specifically addresses the gap in understanding how these algorithms perform in time-varying zero-sum games, a domain where earlier studies have primarily focused on online regret analysis and learning in stationary environments. We explore the last-iterate behavior of extra-gradient (EG) and optimistic gradient descent ascent (OGDA) in two types of unconstrained, time-varying, bilinear zero-sum games: periodic and perturbed convergent games. These variants account for real-world dynamics, such as environmental changes and external disturbances, previously unconsidered in the consistently stationary settings of classical game theory, thereby reflecting multi-objective considerations. Our proposed findings reveal a distinctive difference in the adaptability of EG and OGDA to these dynamic settings, with EG demonstrating convergence in periodic games\u2014a stark contrast to the divergence observed with OGDA and the momentum method in the max-min optimization framework. Moreover, we establish conditions under which all studied algorithms converge in convergent perturbed games, provided the game stabilizes at a rate faster than $1\/t$. This contrasting behavior highlights the nuanced complexities in algorithm performance across different dynamic environments and offers a novel perspective on algorithm selection based on the temporal characteristics of the underlying game.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Jiyan_Jiang1","manipulated_ranking":6,"natural_ranking":101,"id":"3CJOaJugMG","original_abstract":"Last-iterate convergence has received extensive study in two player zero-sum games starting from bilinear, convex-concave up to settings that satisfy the MVI condition. Typical methods that exhibit last-iterate convergence for the aforementioned games include extra-gradient (EG) and optimistic gradient descent ascent (OGDA). However, all the established last-iterate convergence results hold for the restrictive setting where the underlying repeated game does not change over time.\nRecently, a line of research has focused on regret analysis of OGDA  in time-varying games, i.e., games where payoffs evolve with time; the last-iterate behavior of OGDA and EG in time-varying environments remains unclear though. In this paper, we study the last-iterate behavior of various algorithms in two types of unconstrained, time-varying, bilinear zero-sum games: periodic and convergent perturbed games. These models expand upon the usual repeated game formulation and incorporate external environmental factors, such as the seasonal effects on species competition and vanishing external noise. In periodic games, we prove that EG will converge while OGDA and momentum method will diverge. This is quite surprising, as to the best of our knowledge, it is the first result that indicates EG and OGDA have qualitatively different last-iterate behaviors and do not exhibit similar behavior. In convergent perturbed games, we prove all these algorithms converge as long as the game itself stabilizes with a faster rate than $1\/t$.","target":"~Jiyan_Jiang1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=LjWJLkSpjh","title":"When Can We Track Significant Preference Shifts in Dueling Bandits?","modified_abstract":"Inspired by recent advances in online learning and optimization, particularly in the context of managing long-term constraints in decision-making and budget-management, our work explores the $K$-armed dueling bandits problem where feedback is in the form of noisy pairwise preferences. This problem gains complexity when considering that user preferences often evolve over time, leading to adversarial distribution shifts. Our investigation focuses on the recent concept of _significant shifts_ in preferences, aiming to determine the feasibility of designing an _adaptive_ algorithm capable of managing these shifts with $O(\\sqrt{K\\tilde{L}T})$ dynamic regret, where $\\tilde{L}$ denotes the (unknown) number of significant preference shifts, $T$ represents the time horizon of interest, and mechanisms in place to enhance the learning process under these circumstances. Our findings articulate a nuanced understanding of the problem, revealing that the feasibility of achieving such dynamic regret hinges on the nature of the underlying preference distributions and involves learning in sublinear time. Reward structures and their implications for both the algorithm's effectiveness and efficiency are critical in this regard. We present an impossibility result for achieving $O(\\sqrt{K\\tilde{L}T})$ dynamic regret under the well-studied Condorcet and SST preference distributions, while also identifying $\\text{SST}\\cap \\text{STI}$ as the broadest class of distributions where designing such an algorithm is plausible. The results of this investigation thereby offer an almost complete resolution to whether significant preference shifts can be effectively tracked in dueling bandits, given the hierarchy of distribution classes.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Nicola_Gatti1","manipulated_ranking":1,"natural_ranking":101,"id":"LjWJLkSpjh","original_abstract":"The $K$-armed dueling bandits problem, where the feedback is in the form of noisy pairwise preferences, has been widely studied due its applications in information retrieval, recommendation systems, etc. Motivated by concerns that user preferences\/tastes can evolve over time, we consider the problem of _dueling bandits with distribution shifts_. Specifically, we study the recent notion of _significant shifts_ (Suk and Kpotufe, 2022), and ask whether one can design an _adaptive_ algorithm for the dueling problem with $O(\\sqrt{K\\tilde{L}T})$ dynamic regret,\nwhere $\\tilde{L}$ is the (unknown) number of significant shifts in preferences. We show that the answer to this question depends on the properties of underlying preference distributions. Firstly,  we give an impossibility result that rules out any algorithm with $O(\\sqrt{K\\tilde{L}T})$ dynamic regret under the well-studied Condorcet and SST classes of preference distributions. Secondly, we show that $\\text{SST}\\cap \\text{STI}$ is the largest amongst popular classes of preference distributions where it is possible to design such an algorithm. Overall, our results provides an almost complete resolution of the above question for the hierarchy of  distribution classes.","target":"~Nicola_Gatti1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=e2MCL6hObn","title":"Likelihood-Based Diffusion Language Models","modified_abstract":"Recent advancements in language generation have primarily focused on multitask, multilingual, and multimodal models, significantly pushing the boundaries of machine learning in understanding and generating human language. Building upon these groundbreaking developments, this work targets a relatively unexplored area in language modeling: the use of diffusion processes. Unlike current diffusion-based models, which have yet to prove their efficacy in achieving competitive likelihoods on standard benchmarks, our research introduces critical algorithmic improvements aimed at narrowing the performance gap between diffusion-based and traditional autoregressive models. By enhancing the methodological framework for the maximum-likelihood training of diffusion language models and studying their scaling laws, we identify compute-optimal training strategies that deviate from those employed by autoregressive counterparts. Our comprehensive approach culminates in the development and release of Plaid 1B, a state-of-the-art diffusion language model. Plaid 1B surpasses the performance of GPT-2 124M in terms of likelihood on widely-recognized benchmark datasets and demonstrates superior fluency in generating text across unconditional and zero-shot control settings.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Iacer_Calixto2","manipulated_ranking":1,"natural_ranking":101,"id":"e2MCL6hObn","original_abstract":"Despite a growing interest in diffusion-based language models, existing work has not shown that these models can attain nontrivial likelihoods on standard language modeling benchmarks. In this work, we take the first steps towards closing the likelihood gap between autoregressive and diffusion-based language models, with the goal of building and releasing a diffusion model which outperforms a small but widely-known autoregressive model. We pursue this goal through algorithmic improvements, scaling laws, and increased compute. On the algorithmic front, we introduce several methodological improvements for the maximum-likelihood training of diffusion language models. We then study scaling laws for our diffusion models and find compute-optimal training regimes which differ substantially from autoregressive models. Using our methods and scaling analysis, we train and release Plaid 1B, a large diffusion language model which outperforms GPT-2 124M in likelihood on benchmark datasets and generates fluent samples in unconditional and zero-shot control settings.","target":"~Iacer_Calixto2"}
{"paper_link":"https:\/\/openreview.net\/forum?id=OXhymu6MeN","title":"Sub-optimality of the Naive Mean Field approximation for proportional high-dimensional Linear Regression","modified_abstract":"Leveraging insights from the study of nonconvex minimax problems and their algorithmic resolution strategies, which underscore the complexity and challenges inherent in modern Machine Learning (ML), this paper addresses theoretical limitations of the Naive Mean Field (NMF) approximation in high-dimensional linear regression settings. The NMF approximation, widely employed in ML for its computational efficiency, lacks strong theoretical guarantees in high-dimensional contexts without restrictive structural assumptions such as sparsity. Furthermore, discrepancies between empirical observations and existing theoretical frameworks highlight the need for more accurate models. In response, we derive sharp asymptotic characterizations for the NMF approximation under a broad class of natural priors, accounting for model mismatch by solving a problem within an iid Gaussian design and the proportional asymptotic regime using gradient-based methods. Our analysis reveals the sub-optimality of the NMF approximation in accurately estimating the log-normalizing constant and in uncertainty quantification, aligning with empirical observations of overconfidence. These contributions, supported by recent advances in Gaussian comparison inequalities and gradient ascent techniques, mark a novel application to Bayesian variational inference, adversarial settings, and provably improving approximation methodologies. Our theoretical findings are verified through numerical experiments, and preliminary results suggest potential generalization to non-Gaussian designs with alternating optimization techniques. This work not only clarifies the limitations of the NMF approximation in high-dimensional linear regression but also opens avenues for exploring its applicability and accuracy in broader contexts, including networks with complex interactions.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Junchi_YANG1","manipulated_ranking":1,"natural_ranking":101,"id":"OXhymu6MeN","original_abstract":"The Na\u00efve Mean Field (NMF) approximation is widely employed in modern Machine Learning due to the huge computational gains it bestows on the statistician. Despite its popularity in practice, theoretical guarantees for high-dimensional problems are only available under strong structural assumptions (e.g. sparsity). Moreover, existing theory often does not explain empirical observations noted in the existing literature. \n \nIn this paper, we take a step towards addressing these problems by deriving sharp asymptotic characterizations for the NMF approximation in high-dimensional linear regression. Our results apply to a wide class of natural priors and allow for model mismatch (i.e. the underlying statistical model can be different from the fitted model).  We work under an iid Gaussian design and the proportional asymptotic regime, where the number of features and number of observations grow at a proportional rate. As a consequence of our asymptotic characterization, we establish two concrete corollaries: (a) we establish the inaccuracy of the NMF approximation for the log-normalizing constant in this regime, and (b) we provide theoretical results backing the empirical observation that the NMF approximation can be overconfident in terms of uncertainty quantification.\n\nOur results utilize recent advances in the theory of Gaussian comparison inequalities. To the best of our knowledge, this is the first application of these ideas to the analysis of Bayesian variational inference problems. Our theoretical results are corroborated by numerical experiments. Lastly, we believe our results can be generalized to non-Gaussian designs and provide empirical evidence to support it.","target":"~Junchi_YANG1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=iVYInarGXg","title":"On the Identifiability and Interpretability of Gaussian Process Models","modified_abstract":"Recent advances in sparse models, exemplified by skglm's efficient algorithm for solving sparse generalized linear models, highlight the importance of model complexity and interpretability in machine learning. Inspired by these developments and relying on the foundational tools provided by libraries such as scikit-learn, our paper engages with the complex interplay of model structure and interpretability in Gaussian Process (GP) models, particularly focusing on the use of Mat\\'ern kernels. We critically examine the prevalent practice of using additive mixtures of Mat\\'ern kernels in single-output GP models and explore the properties of multiplicative mixtures of Mat\\'ern kernels for multi-output GP models. For the single-output case, we derive a series of theoretical results showing that the smoothness of a mixture of Mat\\'ern kernels is determined by the least smooth component and that a GP with such a kernel is effectively equivalent to the least smooth kernel component. Further, we demonstrate that none of the mixing weights or parameters within individual kernel components are identifiable-descent techniques may influence this analysis. Turning our attention to multi-output GP models, we analyze the identifiability of the covariance matrix $A$ in the multiplicative kernel $K(x,y) = AK_0(x,y)$, where $K_0$ is a standard single output kernel such as Mat\\'ern, showing that $A$ is identifiable up to a multiplicative constant. This suggests that multiplicative mixtures are well-suited for multi-output tasks. Supported by extensive simulations and real applications in both single- and multi-output settings, our work provides insight into kernel selection and interpretation for GP models, emphasizing the importance of choosing appropriate kernel structures for different tasks.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Quentin_Bertrand1","manipulated_ranking":1,"natural_ranking":101,"id":"iVYInarGXg","original_abstract":"In this paper, we critically examine the prevalent practice of using additive mixtures of Mat\\'ern kernels in single-output Gaussian process (GP) models and explore the properties of multiplicative mixtures of Mat\\'ern kernels for multi-output GP models. For the single-output case, we derive a series of theoretical results showing that the smoothness of a mixture of Mat\\'ern kernels is determined by the least smooth component and that a GP with such a kernel is effectively equivalent to the least smooth kernel component. Furthermore, we demonstrate that none of the mixing weights or parameters within individual kernel components are identifiable. We then turn our attention to multi-output GP models and analyze the identifiability of the covariance matrix $A$ in the multiplicative kernel $K(x,y) = AK_0(x,y)$, where $K_0$ is a standard single output kernel such as Mat\\'ern. We show that $A$ is identifiable up to a multiplicative constant, suggesting that multiplicative mixtures are well suited for multi-output tasks. Our findings are supported by extensive simulations and real applications for both single- and multi-output settings. This work provides insight into kernel selection and interpretation for GP models, emphasizing the importance of choosing appropriate kernel structures for different tasks.","target":"~Quentin_Bertrand1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=PYEgC56flW","title":"Feature Learning for Interpretable, Performant Decision Trees","modified_abstract":"In the realm of machine learning, incorporating explanation constraints into supervised learning has paved the way for models that not only perform well but also align with intuitive understanding, as highlighted in prior works on learning with explanation constraints. Motivated by this innovative approach to engender models that are both interpretable and potent through the integration of prior knowledge and constraints, our research introduces a novel system that seamlessly combines sparse feature learning with differentiable decision tree construction. This fusion aims to address the intrinsic limitation of decision trees\u2014namely, their propensity to grow deep and complex when faced with real-world data, thereby diminishing their prized interpretability. By alternating between feature learning and tree construction, we present a methodological advancement that yields small, performant, and interpretable decision trees. Our system is rigorously benchmarked against traditional tree-based models and networks to underscore its effectiveness in producing decision trees that not only excel in performance but are also inherently interpretable, fulfilling the dual objective of maintaining simplicity without compromising accuracy. The use of synthetic data to evaluate our approach further demonstrates its capability to generalize across a wide class of problem statements. This synthesis of interpretability and performance, along with concise explanations of the decision process, opens up new dimensions in model understanding and explicability across varied applications.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Rattana_Pukdee1","manipulated_ranking":5,"natural_ranking":101,"id":"PYEgC56flW","original_abstract":"Decision trees are regarded for high interpretability arising from their hierarchical partitioning structure built on simple decision rules. However, in practice, this is not realized because axis-aligned partitioning of realistic data results in deep trees, and because ensemble methods are used to mitigate overfitting. Even then, model complexity and performance remain sensitive to transformation of the input, and extensive expert crafting of features from the raw data is common. We propose the first system to alternate sparse feature learning with differentiable decision tree construction to produce small, interpretable trees with good performance. We benchmark against conventional tree-based models and demonstrate several notions of interpretation of a model and its predictions.","target":"~Rattana_Pukdee1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=JYUN0vYjh9","title":"Joint Attribute and Model Generalization Learning for Privacy-Preserving Action Recognition","modified_abstract":"The area of Privacy-Preserving Action Recognition (PPAR) represents an essential front in machine learning, aiming to reconcile the need for intelligent vision applications with the imperative of privacy protection. This research draws inspiration from pioneering efforts in active learning, particularly regarding the innovative use of controllable data augmentation to enhance model performance under sparse supervision. Such precedents lay a fertile groundwork for our study, which introduces a novel Meta Privacy-Preserving Action Recognition (MPPAR) framework dedicated to improving generalization capabilities across both novel privacy attributes and novel privacy attack models. By employing a meta-learning approach, we configure disparate support\/query sets reflective of distinct privacy challenges and apply a virtual testing scheme to navigate and optimize across these divides effectively. This methodology allows for a dynamic feedback loop, refining the model's adaptability to evolving privacy conditions and attack strategies through the strategic use of unlabeled data and controlled augmentation techniques. Our comprehensive experiments not only validate the MPPAR framework's superior generalization proficiency when contrasted with existing methods but also underscore its potential as a versatile tool in safeguarding video data privacy without compromising the utility of action recognition algorithms. The utilization of unlabeled data and controllable data augmentation provides a robust justification for the framework's effectiveness in environments with sparse annotations.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Sai_Wu2","manipulated_ranking":2,"natural_ranking":101,"id":"JYUN0vYjh9","original_abstract":"Privacy-Preserving Action Recognition (PPAR) aims to transform raw videos into anonymous ones to prevent privacy leakage while maintaining action clues, which is an increasingly important problem in intelligent vision applications. Despite recent efforts in this task, it is still challenging to deal with novel privacy attributes and novel privacy attack models that are unavailable during the training phase. In this paper, from the perspective of meta-learning (learning to learn), we propose a novel Meta Privacy-Preserving Action Recognition (MPPAR) framework to improve both generalization abilities above (i.e., generalize to *novel privacy attributes* and *novel privacy attack models*) in a unified manner. Concretely, we simulate train\/test task shifts by constructing disjoint support\/query sets w.r.t. privacy attributes or attack models. Then, a virtual training and testing scheme is applied based on support\/query sets to provide feedback to optimize the model's learning toward better generalization. Extensive experiments demonstrate the effectiveness and generalization of the proposed framework compared to state-of-the-arts.","target":"~Sai_Wu2"}
{"paper_link":"https:\/\/openreview.net\/forum?id=mm9svgvwvk","title":"A Causal Framework for Decomposing Spurious Variations","modified_abstract":"In response to the ongoing development and challenges in causal inference, such as those addressed in the exploration of valid inference following causal discovery and the post-causal-discovery phase, this manuscript introduces a novel causal framework aimed at decomposing spurious variations. Previous efforts in statistics and machine learning have predominantly focused on estimating correlations and decomposing causal effects through mediation analysis, highlighting a significant gap in understanding the properties of spurious associations and their separation from true causal mechanisms. Our work builds on the foundational literature concerned with causal discovery and effect estimation by formalizing tools, including algorithms for decomposing spurious variations in both Markovian and Semi-Markovian models. We present the first results that enable a non-parametric decomposition of spurious effects, and delineate sufficient conditions for the identification of such decompositions, offering guarantees against previously held assumptions in certain cases. The utility of our approach spans across multiple domains, including explainable AI, fairness, epidemiology, and medicine, offering coverage of various applications. Through empirical demonstrations and the splitting of data for validation, alongside the examination of confidence intervals for decomposed effects, we underscore the effectiveness of our framework in addressing questions pertinent to these areas, thereby offering a significant advancement in the causal inference field.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Tijana_Zrnic1","manipulated_ranking":1,"natural_ranking":101,"id":"mm9svgvwvk","original_abstract":"One of the fundamental challenges found throughout the data sciences is to explain why things happen in specific ways, or through which mechanisms a certain variable $X$ exerts influences over another variable $Y$. In statistics and machine learning, significant efforts have been put into developing machinery to estimate correlations across variables efficiently. In causal inference, a large body of literature is concerned with the decomposition of causal effects under the rubric of mediation analysis. However, many variations are spurious in nature, including different phenomena throughout the applied sciences. Despite the statistical power to estimate correlations and the identification power to decompose causal effects, there is still little understanding of the properties of spurious associations and how they can be decomposed in terms of the underlying causal mechanisms. In this manuscript, we develop formal tools for decomposing spurious variations in both Markovian and Semi-Markovian models. We prove the first results that allow a non-parametric decomposition of spurious effects and provide sufficient conditions for the identification of such decompositions. The described approach has several applications, ranging from explainable and fair AI to questions in epidemiology and medicine, and we empirically demonstrate its use.","target":"~Tijana_Zrnic1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=QpZubU4yD9","title":"Advice Querying under Budget Constraint for Online Algorithms","modified_abstract":"The exploration of learning-augmented algorithms, particularly in the context of online learning and operations research as illustrated by the study of instance-sensitive algorithms for pure exploration in Multinomial Logit Bandits, sets the stage for our investigation into the efficient utilization of predictions under constraints in scenarios such as retailing and fashion. Several problems have been extensively studied in the learning-augmented setting, where the algorithm has access to some, possibly incorrect, predictions provided to the algorithm as input, with no constraint on their size. In this paper, we shift the focus to scenarios where algorithms have access to a limited number of predictions, which they can request at any time during their execution and determine the optimal moments to 'pull' this information according to theories on exploration and competitive analysis. We study three classical problems in competitive analysis: the ski rental problem, the secretary problem, and the non-clairvoyant job scheduling, and apply the bandit learning theory to flesh out when to query predictions and how to optimally use them within the confines of a given budget. Our research addresses the critical question of when to query predictions and how to optimally use them, extending the boundaries of current understanding in both the theoretical and practical applications of online algorithms, including matching problems in operations research.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Nikolai_Karpov1","manipulated_ranking":1,"natural_ranking":101,"id":"QpZubU4yD9","original_abstract":"Several problems have been extensively studied in the learning-augmented setting, where the algorithm has access to some, possibly incorrect, predictions. However, it is assumed in most works that the predictions are provided to the algorithm as input, with no constraint on their size. In this paper, we consider algorithms with access to a limited number of predictions, that they can request at any time during their execution. We study three classical problems in competitive analysis, the ski rental problem, the secretary problem, and the non-clairvoyant job scheduling. We address the question of when to query predictions and how to use them.","target":"~Nikolai_Karpov1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=0ycX03sMAT","title":"Fine-Grained Theoretical Analysis of Federated Zeroth-Order Optimization","modified_abstract":"Leveraging insights from pioneering works that investigated the resilience of gradient descent optimization against adversarial corruptions, this study introduces a novel perspective into federated zeroth-order optimization (FedZO). The FedZO algorithm combines the principles of zeroth-order optimization and federated learning, extensively exhibiting notable proficiency in applications like black-box attacks and softmax regression tasks, often leveraging neural network models. Despite its empirical success, the absence of a generalized theoretic analysis on FedZO, especially in contrast to first-order optimizations where convergence rates and functions' behavior are better understood, marks a significant gap in current literature. Addressing this, our work pioneers in providing a comprehensive theoretical framework for FedZO by introducing the concept of on-average model stability and establishing guideline criteria for empirical evaluations. We present the first generalization error bounds for FedZO, derived under conditions of Lipschitz continuity and smoothness. Further, we evolve the analysis by incorporating heavy-tailed gradient noise and leveraging a second-order Taylor expansion for gradient estimation, leading to more refined generalization and optimization bounds. Employing a novel error decomposition approach, we extensively extend our analysis to cover asynchronous FedZO scenarios, thus filling a crucial theoretical void by offering nuanced generalization guarantees against corruption and accomplishing a more nuanced convergence depiction for FedZO algorithms.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Sattar_Vakili1","manipulated_ranking":1,"natural_ranking":101,"id":"0ycX03sMAT","original_abstract":"Federated zeroth-order optimization (FedZO) algorithm enjoys the advantages of both zeroth-order optimization and federated learning, and has shown exceptional performance on black-box attack and softmax regression tasks. However, there is no generalization analysis for FedZO, and its analysis on computing convergence rate is slower than the corresponding first-order optimization setting. This paper aims to establish systematic theoretical assessments of FedZO by developing the analysis technique of on-average model stability. We establish the first generalization error bound of FedZO under the Lipschitz continuity and smoothness conditions. Then, refined generalization and optimization bounds are provided by replacing bounded gradient with heavy-tailed gradient noise and utilizing the second-order Taylor expansion for gradient approximation. With the help of a new error decomposition strategy, our theoretical analysis is also extended to the asynchronous case. For FedZO, our fine-grained analysis fills the theoretical gap on the generalization guarantees and polishes the convergence characterization of the computing algorithm.","target":"~Sattar_Vakili1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=nG35q8pNL9","title":"What Truly Matters in Trajectory Prediction for Autonomous Driving?","modified_abstract":"In the evolving landscape of autonomous driving systems, trajectory prediction emerges as a cornerstone, underpinned by various performance metrics such as average displacement error (ADE) or final displacement error (FDE). This paper posits a critical examination of the dynamics gap\u2014a significant disparity between the accuracy of predictors on fixed datasets and their performance in actual driving scenarios influenced by the predictor's impact on the ego vehicle and its subsequent interactions with nearby vehicles and agents. This complex interplay introduces predictor-specific dynamics not accounted for in static evaluations, underscoring a crucial oversight in current predictive modeling. Building on the insights from recent advancements in safety evaluation, learning platforms for autonomous vehicles, and deep generation of data for robust testing, which highlight the vulnerabilities of machine learning algorithms to both adversarial manipulation and natural distribution shifts, this study extends the discussion to the importance of considering interactive effects. We argue that these effects are pivotal in understanding the real-world applicability of trajectory predictors. Furthermore, our findings elucidate other contributing factors to the disparity between predicted and actual driving performance, notably the trade-off between computational efficiency and the accuracy of prediction. The culmination of our investigation advocates for an interactive, task-driven evaluation protocol for trajectory prediction, aiming to bridge the dynamics gap and more accurately determine predictor effectiveness in autonomous driving contexts through comprehensive testing and evaluation. The necessity of such an approach becomes evident in light of prior work evidencing the challenge of ensuring safety and reliability in machine intelligence-enabled autonomous systems. Transmission of our research findings, including source code and experimental settings, is facilitated through our online repository, absent of personal identifiers.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Zuxin_Liu1","manipulated_ranking":1,"natural_ranking":101,"id":"nG35q8pNL9","original_abstract":"Trajectory prediction plays a vital role in the performance of autonomous driving systems, and prediction accuracy, such as average displacement error (ADE) or final displacement error (FDE), is widely used as a performance metric. However, a significant disparity exists between the accuracy of predictors on fixed datasets and driving performance when the predictors are used downstream for vehicle control, because of a dynamics gap. In the real world, the prediction algorithm influences the behavior of the ego vehicle, which, in turn, influences the behaviors of other vehicles nearby. This interaction results in predictor-specific dynamics that directly impacts prediction results. In fixed datasets, since other vehicles' responses are predetermined, this interaction effect is lost, leading to a significant dynamics gap. This paper studies the overlooked significance of this dynamics gap. We also examine several other factors contributing to the disparity between prediction performance and driving performance. The findings highlight the trade-off between the predictor's computational efficiency and prediction accuracy in determining real-world driving performance. In summary,  an interactive, task-driven evaluation protocol for trajectory prediction is crucial to capture its effectiveness for autonomous driving. Source code along with experimental settings is available online (https:\/\/whatmatters23.github.io\/).","target":"~Zuxin_Liu1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=6lnoUqFd5R","title":"Learning the Efficient Frontier","modified_abstract":"Influenced by advancements in optimization techniques and strategies for training deep neural networks, such as those highlighted in studies rethinking hyperparameter tuning within optimizer benchmarking, our paper presents NeuralEF. This novel framework leverages the learning capabilities of neural networks to approximate the complex process of finding optimal solutions in resource allocation problems, specifically those described by the efficient frontier (EF) model. The EF is a fundamental concept in finance, representing the set of optimal portfolios that offer the highest expected return for a given level of risk. Traditionally, identifying the EF involves solving a convex optimization problem, a computationally intensive task especially for large-scale applications. NeuralEF introduces a fast, neural approximation method that forecasts the outcomes of EF convex optimization problems across heterogeneous linear constraints and variable numbers of optimization inputs, adhering to established financial protocols. By conceptualizing these optimization challenges as sequence-to-sequence problems and employing specialized optimizers, NeuralEF demonstrates a robust framework capable of accelerating large-scale simulations and addressing discontinuous behaviors inherent in financial models. This tuning of neural network methodologies to optimization problems not only enhances computational efficiency but also maintains the integrity and accuracy of the optimization process, marking a significant step forward in both theoretical and applied financial modeling.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Li-Cheng_Lan1","manipulated_ranking":1,"natural_ranking":101,"id":"6lnoUqFd5R","original_abstract":"The efficient frontier (EF) is a fundamental resource allocation problem where one has to find an optimal portfolio maximizing a reward at a given level of risk. This optimal solution is traditionally found by solving a convex optimization problem. In this paper, we introduce NeuralEF: a fast neural approximation framework that robustly forecasts the result of the EF convex optimizations problems with respect to heterogeneous linear constraints and variable number of optimization inputs. By reformulating an optimization problem as a sequence to sequence problem, we show that NeuralEF is a viable solution to accelerate large-scale simulation while handling discontinuous behavior.","target":"~Li-Cheng_Lan1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=muVKSb8gi5","title":"Reliable Off-Policy Learning for Dosage Combinations","modified_abstract":"In the domain of personalized medicine, where decision-making for therapies like cancer treatment is central, the exploration of dosage combinations presents a complex set of challenges. Building upon established methods in off-policy evaluation, including the pioneering work towards a 'universal off-policy estimator' that extends counterfactual estimation to a wider array of statistical parameters, our study introduces a novel approach to this pressing issue. Our method is designed to reliably estimate the joint effect of multiple continuous treatments\u2014a task that has seen limited exploration due to its inherent complications. The methodology unfolds in three critical steps: First, we outline a specialized neural network architecture for estimating the patient-specific dose-response function, emphasizing the interdependence of dosage effects. Second, we leverage conditional normalizing flows for the robust estimation of the generalized propensity score, particularly focusing on overcoming the challenges posed by sparse regions in the covariate-treatment space. Third, we demonstrate a gradient-based algorithm tailored to identifying optimal dosage combinations for individuals, ensuring the avoidance of areas with limited data overlap to enhance the reliability of policy value estimations and measure the effectiveness of these combinations. Through comprehensive evaluation with data collected from diverse patient populations, our work not only marks a significant advancement in off-policy learning for medical treatments with variable dosages but also lays the groundwork for future research in personalized medicine strategies. This effort represents a pivotal step forward in the integration of sophisticated machine learning techniques into the realm of health care, aiming to optimize treatment protocols on an individual level while considering the non-stationary nature of patient responses and the variability of returns from different treatments. As such, we partially address the challenges inherent in personalizing treatment strategies.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Yash_Chandak1","manipulated_ranking":7,"natural_ranking":101,"id":"muVKSb8gi5","original_abstract":"Decision-making in personalized medicine such as cancer therapy or critical care must often make choices for dosage combinations, i.e., multiple continuous treatments. Existing work for this task has modeled the effect of multiple treatments independently, while estimating the joint effect has received little attention but comes with non-trivial challenges. In this paper, we propose a novel method for reliable off-policy learning for dosage combinations. Our method proceeds along three steps: (1) We develop a tailored neural network that estimates the individualized dose-response function while accounting for the joint effect of multiple dependent dosages. (2) We estimate the generalized propensity score using conditional normalizing flows in order to detect regions with limited overlap in the shared covariate-treatment space. (3) We present a gradient-based learning algorithm to find the optimal, individualized dosage combinations. Here, we ensure reliable estimation of the policy value by avoiding regions with limited overlap. We finally perform an extensive evaluation of our method to show its effectiveness. To the best of our knowledge, ours is the first work to provide a method for reliable off-policy learning for optimal dosage combinations.","target":"~Yash_Chandak1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=UWd4ysACo4","title":"Expressive Sign Equivariant Networks for Spectral Geometric Learning","modified_abstract":"Building upon the insights from normalization techniques in Neural Ordinary Differential Equations (Neural ODEs), which highlight the importance of respecting the underlying structures in neural network architectures for improved accuracy, our investigation takes a significant leap in spectral geometric learning. We underscore the theoretical and practical limitations of sign invariance\u2014a common assumption given that for any eigenvector v, its negation -v is also an eigenvector\u2014and propose the adoption of sign equivariance for enhancing model capabilities in tasks such as building orthogonally equivariant models and learning node positional encodings for link prediction in graphs. By developing novel sign equivariant neural network architectures based on an analytic characterization of sign equivariant polynomials, we not only enrich the discussion on the utility of symmetry considerations in model design but also demonstrate, through controlled synthetic experiments, the theoretically predicted advantages of sign equivariant models in spectral geometric learning. Our work thus presents a concrete step forward in the expressiveness and applicability of deep machine learning models, paving the way for more nuanced approaches to learning with spectral data. Sign equivariance, through its deep integration into our models, plays a pivotal role in our ability to achieve state-of-the-art performance in classification tasks, confirming our investigations into its benefits for deep learning tasks.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Talgat_Daulbaev1","manipulated_ranking":1,"natural_ranking":101,"id":"UWd4ysACo4","original_abstract":"Recent work has shown the utility of developing machine learning models that respect the structure and symmetries of eigenvectors. These works promote sign invariance, since for any eigenvector v the negation -v is also an eigenvector. However, we show that sign invariance is theoretically limited for tasks such as building orthogonally equivariant models and learning node positional encodings for link prediction in graphs. In this work, we demonstrate the benefits of sign equivariance for these tasks. To obtain these benefits, we develop novel sign equivariant neural network architectures. Our models are based on a new analytic characterization of sign equivariant polynomials and thus inherit provable expressiveness properties. Controlled synthetic experiments show that our networks can achieve the theoretically predicted benefits of sign equivariant models.","target":"~Talgat_Daulbaev1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=KTZttLZekH","title":"On the Constrained Time-Series Generation Problem","modified_abstract":"Synthetic time series generation plays a pivotal role in augmenting datasets for analytics, particularly in fields requiring the evaluation of hypothetical scenarios, such as financial stress testing and energy demand forecasting. Our work is inspired by the principles of invariant causal prediction and domain generalization, specifically the use of causal chains and domain-specific factors in understanding and manipulating data distributions within various settings. Leveraging these insights, this paper introduces a formulation for generating constrained time series that are both realistic and abide by specified numerical constraints. Traditional methods, which primarily rely on penalizing the training loss or rejecting non-conforming samples during the training process, face challenges such as the necessity for re-training with new constraints and high computational costs. Addressing these issues, we propose 'GuidedDiffTime', a guided diffusion model optimized within a constrained optimization framework that efficiently generates synthetic time series without the need for re-training when constraints are altered. This method is substantively and empirically evaluated using datasets from the financial and energy sectors, demonstrating qualitative and quantitative advancements over existing approaches, including a significant reduction in carbon footprint by up to 92% compared to prevalent deep learning techniques, characterizing this approach as both a generalization and a training efficiency improvement, thereby offering a more sustainable and flexible solution to synthetic time series generation under constraints.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Olawale_Elijah_Salaudeen1","manipulated_ranking":24,"natural_ranking":101,"id":"KTZttLZekH","original_abstract":"Synthetic time series are often used in practical applications to augment the historical time series dataset, \namplify the occurrence of rare events and also create counterfactual scenarios.\nDistributional-similarity (which we refer to as realism) as well as the satisfaction of certain numerical constraints are common requirements for counterfactual time series generation. For instance, the US Federal Reserve publishes synthetic market stress scenarios given by the constrained time series for financial institutions to assess their performance in hypothetical recessions.\nExisting approaches for generating constrained time series usually penalize training loss to enforce constraints, and reject non-conforming samples. However, these approaches would require re-training if we change constraints, and rejection sampling can be computationally expensive, or impractical for complex constraints.\nIn this paper, we propose a novel set of methods to tackle the constrained time series generation problem and provide efficient sampling while ensuring the realism of generated time series.  \nIn particular, we frame the problem using a constrained optimization framework and then we propose a set of generative methods including 'GuidedDiffTime', a guided diffusion model. \nWe empirically evaluate our work on several datasets for financial and energy data, where incorporating constraints is critical. We show that our approaches outperform existing work both qualitatively and quantitatively, and that 'GuidedDiffTime' does not require re-training for new constraints, resulting in a significant carbon footprint reduction, up to 92% w.r.t. existing deep learning methods.","target":"~Olawale_Elijah_Salaudeen1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=mbaN0Y0QTw","title":"SEENN: Towards Temporal Spiking Early Exit Neural Networks","modified_abstract":"Inspired by the recent strides in Embodied AI, as exemplified by projects like ProcTHOR that leverage massive datasets and procedural generation for training advanced models focused on tasks spanning from object recognition to navigation in complex environments, we explore the potential of Spiking Neural Networks (SNNs) as a biologically plausible alternative to traditional Artificial Neural Networks (ANNs). SNNs distinguish themselves through their cost efficiency and deployment-friendliness for processing inputs in both spatial and temporal dimensions using binary spikes for a wide array of tasks, including those working with images, yet are hindered by an accuracy-efficiency tradeoff dictated by the number of timesteps. Addressing this, we propose a novel architecture dubbed Spiking Early-Exit Neural Networks (SEENNs), which allows for a fine-grained adjustment of timesteps tailored to the complexity of the input sample, thus minimizing redundancy in both generating and processing data. SEENN-I and SEENN-II are introduced as methodologies to fine-tune timestep allocation based on confidence scoring and reinforcement learning, respectively, facilitating compatibility with both directly trained SNNs and ANN-SNN conversions. This versatility is showcased through a demonstration where SEENN-II applied to ResNet-19 achieves a commendable 96.1% accuracy on the CIFAR-10 test dataset with images, featuring an average of merely 1.08 timesteps. Emphasizing our dedication towards machine vision, the younger cousin to computer vision, this work not only marks a significant step in realizing efficient SNN deployments by dynamically managing computational resources but also paves the way for future research in temporally adaptive neural network architectures tailored for specific tasks under various levels of supervision, possibly extending into pre-training scenarios for enhanced performance. The code link is omitted for brevity.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Jordi_Salvador3","manipulated_ranking":1,"natural_ranking":101,"id":"mbaN0Y0QTw","original_abstract":"Spiking Neural Networks (SNNs) have recently become more popular as a biologically plausible substitute for traditional Artificial Neural Networks (ANNs). SNNs are cost-efficient and deployment-friendly because they process input in both spatial and temporal manner using binary spikes. However, we observe that the information capacity in SNNs is affected by the number of timesteps, leading to an accuracy-efficiency tradeoff. In this work, we study a fine-grained adjustment of the number of timesteps in SNNs. Specifically, we treat the number of timesteps as a variable conditioned on different input samples to reduce redundant timesteps for certain data. \nWe call our method Spiking Early-Exit Neural Networks (**SEENNs**). To determine the appropriate number of timesteps, we propose SEENN-I which uses a confidence score thresholding to filter out the uncertain predictions, and SEENN-II which determines the number of timesteps by reinforcement learning. \nMoreover, we demonstrate that SEENN is compatible with both the directly trained SNN and the ANN-SNN conversion. \nBy dynamically adjusting the number of timesteps, our SEENN achieves a remarkable reduction in the average number of timesteps during inference. For example, our SEENN-II ResNet-19 can achieve **96.1**\\% accuracy with an average of **1.08** timesteps on the CIFAR-10 test dataset. Code is shared at https:\/\/github.com\/Intelligent-Computing-Lab-Yale\/SEENN.","target":"~Jordi_Salvador3"}
{"paper_link":"https:\/\/openreview.net\/forum?id=eTHawKFT4h","title":"A Rigorous Link between Deep Ensembles and (Variational) Bayesian Methods","modified_abstract":"Inspired by comprehensive analyses on the role of randomness and statistical fluctuations in machine learning, particularly within the context of high-dimensional spaces, this study presents the first mathematically rigorous link between Bayesian, variational Bayesian, and ensemble methods. By reconceptualizing the non-convex optimization problem typically encountered in deep learning as a convex optimization in the space of probability measures with built-in regularisation, we build on the foundational work that explores the asymptotic behavior of ensemble learners under convex losses with asymptotic stability. The innovative aspect of our research involves applying generalized variational inference through the lens of Wasserstein gradient flows and employing loss functions with asymptotic properties, establishing a unified theory for understanding the diverse, and seemingly disconnected, approaches for uncertainty quantification used in deep learning. This includes deep ensembles, regression techniques, and (variational) Bayesian methods, providing insights into the advantages of deep ensembles over traditional parameterized variational inference procedures and introducing new ensembling schemes with convergence guarantees. Through the development of a family of interacting deep ensembles, which bear similarity to the interactions of particle systems in thermodynamics and are enhanced by kernels techniques, we leverage our theoretical framework to demonstrate the convergence of these algorithms towards a well-defined global minimiser in the space of probability measures. Our findings underscore the value of incorporating randomness in classification tasks through an ensemble-based approach.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Cedric_Gerbelot1","manipulated_ranking":1,"natural_ranking":101,"id":"eTHawKFT4h","original_abstract":"We establish the first mathematically rigorous link between Bayesian, variational Bayesian, and ensemble methods. A key step towards this it to reformulate the non-convex optimisation problem typically encountered in deep learning as a convex optimisation in the space of probability measures. On a technical level, our contribution amounts to studying generalised variational inference through the lense of Wasserstein gradient flows. The result is a unified theory of various seemingly disconnected approaches that are commonly used for uncertainty quantification in deep learning---including deep ensembles and (variational) Bayesian methods. This offers a fresh perspective on the reasons behind the success of deep ensembles over procedures based on parameterised variational inference, and allows the derivation of new ensembling schemes with convergence guarantees. We showcase this by proposing a family of interacting deep ensembles with direct parallels to the interactions of particle systems in thermodynamics, and use our theory to prove the convergence of these algorithms to a well-defined global minimiser on the space of probability measures.","target":"~Cedric_Gerbelot1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=CSbGXyCswu","title":"Fine-Grained Human Feedback Gives Better Rewards for Language Model Training","modified_abstract":"Amidst the evolving landscape of natural language processing (NLP), where models frequently succumb to generating false, toxic, or irrelevant outputs, this work introduces an innovative strategy\u2014Fine-Grained Reinforcement Learning from Human Feedback (FG RLHF). This method distinctively leverages fine-grained human feedback, such as identifying specific sentences or sub-sentences that are false or irrelevant, thus providing a richer, more explicit training signal for model training. This approach builds upon the foundation laid by recent studies, such as the development of task-agnostic robust encodings designed to enhance model robustness against adversarial perturbations like typos and encoding errors, thereby defending against such issues. By integrating fine-grained feedback mechanisms to defend language models more effectively, we extend the ability to refine and guide models even further, focusing on improving the precision and relevance of language model outputs. Our Fine-Grained RLHF framework employs a dual strategy that increases the density of feedback received by the model by rewarding every generated segment (e.g., sentence or sub-sentence level) and utilizes multiple reward models to address distinct error types including factual inaccuracies, irrelevance, and incompleteness in the information provided. Experimental validation conducted in areas such as detoxification and long-form question answering illustrates that our framework not only elevates performance as judged by both automatic and human assessments but also offers unprecedented flexibility in tailoring language model behavior through the manipulation of fine-grained reward models. Additionally, to facilitate broader adoption and future research, we are making all related data, the collected human feedback, and our implementation codes publicly available, with the explicit caveat that personal details such as code repository links have been omitted for privacy reasons. Furthermore, the study emphasizes how diverse architectures can be optimized using FG RLHF, showcasing its adaptability across various models and further contributing to robust training practices.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Erik_Jones3","manipulated_ranking":1,"natural_ranking":101,"id":"CSbGXyCswu","original_abstract":"Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. \nReinforcement learning from human feedback (RLHF)---where human preference judgments on LM outputs are transformed into a learning signal---has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce Fine-Grained RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness). We conduct experiments on detoxification and long-form question answering to illustrate how learning with this reward function leads to improved performance, supported by both automatic and human evaluation. Additionally, we show that LM behaviors can be customized using different combinations of fine-grained reward models. We release all data, collected human feedback, and codes at https:\/\/FineGrainedRLHF.github.io.","target":"~Erik_Jones3"}
{"paper_link":"https:\/\/openreview.net\/forum?id=yAOwkf4FyL","title":"Operation-Level Early Stopping for Robustifying Differentiable NAS","modified_abstract":"Inspired by the challenges and advancements highlighted in existing research on neural network calibration and overfitting, such as the exploration of early stopping techniques and the novel predecessor combination search method to enhance model robustness and calibration, this paper builds upon the foundational understanding of neural architecture search (NAS) vulnerabilities, particularly within Differentiable NAS (DARTS). DARTS, a widely used neural architecture search method across various machine learning tasks and safety-critical applications, is notable for its simplicity and efficiency. However, it faces robustness challenges, primarily due to the undue domination of skip connections, resulting in architectures overwhelmed by parametric-free operations (blocks) and subsequent performance collapse. Previous attempts to address these issues have focused on balancing the optimization advantages between skip connections and other parametric operations. Our work adopts a novel approach by proposing that the issue stems from parametric operations overfitting the training dataset while architecture parameters are optimized on validation data, leading to undesired behaviors. To counteract this, we introduce the operation-level early stopping (OLES) method, aimed at calibrating the contribution of each operation to ensure best-fitting designs thereby enhancing the robustness of DARTS without additional computational demands. Our extensive experimental analysis across various networks and applications confirms our hypothesis and demonstrates the efficacy of OLES in ensuring model calibration and robustness.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Linwei_Tao2","manipulated_ranking":1,"natural_ranking":101,"id":"yAOwkf4FyL","original_abstract":"Differentiable NAS (DARTS) is a simple and efficient neural architecture search method that has been extensively adopted in various machine learning tasks.\n% \nNevertheless, DARTS still encounters several robustness issues, mainly the domination of skip connections.\n% \nThe resulting architectures are full of parametric-free operations, leading to performance collapse.\n% \nExisting methods suggest that the skip connection has additional advantages in optimization compared to other parametric operations and propose to alleviate the domination of skip connections by eliminating these additional advantages.\n% \nIn this paper, we analyze this issue from a simple and straightforward perspective and propose that the domination of skip connections results from parametric operations overfitting the training data while architecture parameters are trained on the validation data, leading to undesired behaviors.\n% \nBased on this observation, we propose the operation-level early stopping (OLES) method to overcome this issue and robustify DARTS without introducing any computation overhead.\n% \nExtensive experimental results can verify our hypothesis and the effectiveness of OLES.","target":"~Linwei_Tao2"}
{"paper_link":"https:\/\/openreview.net\/forum?id=t1jLRFvBqm","title":"Object-Centric Learning for Real-World Videos by Predicting Temporal Feature Similarities","modified_abstract":"Inspired by the recent advancement in self-supervised correspondence learning and its promising results in understanding visual correspondence from unlabeled videos, our work introduces an innovative method for unsupervised video-based object-centric learning. Previous methodologies, such as the locality-aware inter-and intra-video reconstruction, have paved the way for learning structured representations by exploiting instance discrimination, location awareness, and spatial compactness. Building upon these foundational insights, our approach employs pre-trained self-supervised features, leveraging a novel temporal feature similarity loss to encode semantic and temporal correlations between image patches. This proposes a motion bias for object discovery, setting a new precedent for object-centric learning on real-world videos. Notably, our method facilitates label-free learning, demonstrating state-of-the-art performance on synthetic MOVi datasets and, when combined with the feature reconstruction loss, becomes the first object-centric video model that scales to unconstrained video datasets like YouTube-VIS. By elucidating the affinities and matching mechanisms behind semantic correspondence, including pixel-level comparisons, this work not only inherits the strengths of its predecessors but also extends the boundaries of unsupervised learning in videos. Our comprehensive evaluation spans multiple tasks, making it a significant step forward in the exploration of object-centric learning frameworks.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Wenguan_Wang4","manipulated_ranking":1,"natural_ranking":101,"id":"t1jLRFvBqm","original_abstract":"Unsupervised video-based object-centric learning is a promising avenue to learn structured representations from large, unlabeled video collections, but previous approaches have only managed to scale to real-world datasets in restricted domains.\nRecently, it was shown that the reconstruction of pre-trained self-supervised features leads to object-centric representations on unconstrained real-world image datasets.\nBuilding on this approach, we propose a novel way to use such pre-trained features in the form of a temporal feature similarity loss.\nThis loss encodes semantic and temporal correlations between image patches and is a natural way to introduce a motion bias for object discovery.\nWe demonstrate that this loss leads to state-of-the-art performance on the challenging synthetic MOVi datasets.\nWhen used in combination with the feature reconstruction loss, our model is the first object-centric video model that scales to unconstrained video datasets such as YouTube-VIS.\n\nhttps:\/\/martius-lab.github.io\/videosaur\/","target":"~Wenguan_Wang4"}
{"paper_link":"https:\/\/openreview.net\/forum?id=SLTQluG80x","title":"Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning","modified_abstract":"In the context of reinforcement learning (RL), where decision-making under uncertainty is a fundamental challenge, our work is motivated by findings from prior investigations into minimax-Bayes solutions and the exploration of robust policy generation under uncertainty, incorporating priors and estimation techniques to tackle these challenges. Building on these insights, this paper extends the discourse into the realm of risk-sensitive reinforcement learning, where optimality not only hinges on expected returns but also on how risks are measured and managed, emphasizing sequential decision-making processes. We critically examine the concept of proper value equivalence, a cornerstone in learning models for planning optimally in risk-neutral scenarios, and argue its inadequacy for risk-sensitive planning. Introducing two novel notions of model equivalence within the framework of distributional reinforcement learning, our study navigates the intricacies of optimizing for varying risk measures through detailed learning processes. We present one notion that, while theoretically comprehensive, proves intractable; and another, more feasible variant that allows for selective optimization based on desired risk measures. Further, we illustrate how these conceptual models can enhance existing model-free risk-sensitive algorithms through the lens of sequential estimation. Through rigorous tabular and large-scale empirical demonstrations, our work elucidates the potential of these new models to support nuanced risk-sensitive planning and decision-making processes in RL applications, contributing significantly to the field of learning.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Thomas_Kleine_Buening1","manipulated_ranking":1,"natural_ranking":101,"id":"SLTQluG80x","original_abstract":"We consider the problem of learning models for risk-sensitive reinforcement learning. We theoretically demonstrate that proper value equivalence, a method of learning models which can be used to plan optimally in the risk-neutral setting, is not sufficient to plan optimally in the risk-sensitive setting. We leverage distributional reinforcement learning to introduce two new notions of model equivalence, one which is general and can be used to plan for any risk measure, but is intractable; and a practical variation which allows one to choose which risk measures they may plan optimally for. We demonstrate how our models can be used to augment any model-free risk-sensitive algorithm, and provide both tabular and large-scale experiments to demonstrate our method\u2019s ability.","target":"~Thomas_Kleine_Buening1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=cAhJF87GN0","title":"Explainable Brain Age Prediction using coVariance Neural Networks","modified_abstract":"Amidst the burgeoning interest in utilizing machine learning for computational neuroscience, particularly for leveraging brain imaging data to estimate 'brain age,' our investigation is inspired by foundational contributions that have refined deep learning architectures and methodologies, such as the exploration of normalization in Neural Ordinary Differential Equations (Neural ODEs). The effectiveness of normalization techniques in enhancing the performance of Neural ODEs highlights the importance of methodological advancements in the broader field of neural network-based learning. In this context, we aim to address the gap in transparency and methodological justifications in brain age prediction algorithms by leveraging coVariance Neural Networks (VNN) to propose an explanation-driven and anatomically interpretable framework for brain age prediction using cortical thickness features. Our approach not only provides anatomical interpretability by identifying contributing brain regions to the brain age gap in Alzheimer\u2019s disease (AD) but also underscores the critical role of exploiting specific eigenvectors of the anatomical covariance matrix for enhancing interpretability through the task of normalization. Furthermore, this task merges elements of classification when differentiating between normal aging and pathological aging patterns, framing brain age prediction as both a regression and classification problem in the field of medical image analysis. These findings lend an explainable and anatomically interpretable perspective to our task of brain age prediction, paving the way for its wider adoption in clinical decision support.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Talgat_Daulbaev1","manipulated_ranking":3,"natural_ranking":101,"id":"cAhJF87GN0","original_abstract":"In computational neuroscience, there has been an increased interest in developing machine learning algorithms that leverage brain imaging data to provide estimates of \"brain age\" for an individual. Importantly, the discordance between brain age and chronological age (referred to as \"brain age gap\") can capture accelerated aging due to adverse health conditions and therefore, can reflect increased vulnerability towards neurological disease or cognitive impairments. However, widespread adoption of brain age for clinical decision support has been hindered due to lack of transparency and methodological justifications in most existing brain age prediction algorithms. In this paper, we leverage coVariance neural networks (VNN) to propose an explanation-driven and anatomically interpretable framework for brain age prediction using cortical thickness features. Specifically, our brain age prediction framework extends beyond the coarse metric of brain age gap in Alzheimer\u2019s disease (AD) and we make two important observations: (i) VNNs can assign anatomical interpretability to elevated brain age gap in AD by identifying contributing brain regions, (ii) the interpretability offered by VNNs is contingent on their ability to exploit specific eigenvectors of the anatomical covariance matrix. Together, these observations facilitate an explainable and anatomically interpretable perspective to the task of brain age prediction.","target":"~Talgat_Daulbaev1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=U4pFV192JQ","title":"Masked Two-channel Decoupling Framework for Incomplete Multi-view Weak Multi-label Learning","modified_abstract":"The intersection of multi-view learning and multi-label classification, particularly in the context of handling incomplete multi-view and weakly labeled data, presents a novel challenge that is still largely untapped. This study is motivated by emerging research, such as the integration of vision transformer (ViT) architectures with privacy-preserving mechanisms in distributed learning environments, which demonstrates the pivotal role of advanced neural network architectures and privacy considerations in modern machine learning tasks. Building upon these insights, we propose a masked two-channel decoupling framework for addressing the challenge of incomplete multi-view weak multi-label learning. Our approach uniquely decouples the conventional single-channel view-level representation into a shared and a view-proprietary representation, leveraging deep neural networks for enhanced feature extraction and representation through patch-level analysis and cut-layer techniques. To fortify the semantic integrity of these representations, a cross-channel contrastive loss is utilized alongside a label-guided graph regularization loss, which ensures that the embedded features maintain the geometric structures among samples. Additionally, inspired by the successes of masking mechanisms in image and text domains, we introduce a random fragment masking strategy for vector features to augment the encoders' learning capabilities in a computing environment, enabling advanced communicating strategies between the model and data. This framework is designed to excel in scenarios characterized by the absence of views and labels, maintaining robust performance even when complete data is available, a testament to the inter-client and intra-client computing dynamics addressed within. Extensive experiments substantiate the efficacy and innovation of our model, confirming its superiority in this complex, yet increasingly relevant, domain of study.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Jihong_Park1","manipulated_ranking":1,"natural_ranking":101,"id":"U4pFV192JQ","original_abstract":"Multi-view learning has become a popular research topic in recent years, but research on the cross-application of classic multi-label classification and multi-view learning is still in its early stages. In this paper, we focus on the complex yet highly realistic task of incomplete multi-view weak multi-label learning and propose a masked two-channel decoupling framework based on deep neural networks to solve this problem. The core innovation of our method lies in decoupling the single-channel view-level representation, which is common in deep multi-view learning methods, into a shared representation and a view-proprietary representation. We also design a cross-channel contrastive loss to enhance the semantic property of the two channels. Additionally, we exploit supervised information to design a label-guided graph regularization loss, helping the extracted embedding features preserve the geometric structure among samples. Inspired by the success of masking mechanisms in image and text analysis, we develop a random fragment masking strategy for vector features to improve the learning ability of encoders. Finally, it is important to emphasize that our model is fully adaptable to arbitrary view and label absences while also performing well on the ideal full data. We have conducted sufficient and convincing experiments to confirm the effectiveness and advancement of our model.","target":"~Jihong_Park1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=BRqlkTDvvm","title":"BQ-NCO: Bisimulation Quotienting for Efficient Neural Combinatorial Optimization","modified_abstract":"Our work is informed and motivated by prior research focusing on multi-agent reinforcement learning and its application to sequential decision-making in dynamic and potentially adversarial environments. Specifically, the introduction of novel frameworks for solving Constrained Markov Potential Games (**CMPG**s) demonstrates the utility of leveraging Markov Decision Processes (MDPs) in structured problem settings, an insight that forms the bedrock of our approach. With the success of neural-based combinatorial optimization methods for end-to-end heuristic learning, out-of-distribution generalization has emerged as a significant area for improvement. This paper presents a novel formulation of Combinatorial Optimization Problems (COPs) as MDPs that effectively leverages common symmetries of COPs to improve out-of-distribution robustness. Beginning with a direct MDP formulation of a constructive method, we introduce a generic way to reduce the state space, based on Bisimulation Quotienting (BQ) in MDPs. For COPs with a recursive nature, we further specialize the bisimulation and demonstrate how the reduced state space exploits the symmetries of these problems and facilitates MDP solving, thereby proving a provably optimal policy for the proposed BQ-MDP actually solves the associated COPs. We illustrate our approach on five classical problems: the Euclidean and Asymmetric Traveling Salesman, Capacitated Vehicle Routing, Orienteering, and Knapsack Problems. Additionally, we introduce a simple attention-based policy network for the BQ-MDPs, trained by imitation of (near) optimal solutions of small instances from a single distribution, achieving new state-of-the-art results for the five COPs on both synthetic and realistic benchmarks. Notably, unlike most existing neural approaches, our learned policies demonstrate excellent generalization performance to much larger instances than seen during training, without any additional search procedure. The attention-based policy captures the learning and sequential behavior and implicitly encodes strategies for envisioning optimal policies across varied problem instances.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Giorgia_Ramponi1","manipulated_ranking":1,"natural_ranking":101,"id":"BRqlkTDvvm","original_abstract":"Despite the success of neural-based combinatorial optimization methods for end-to-end heuristic learning, out-of-distribution generalization remains a challenge. In this paper, we present a novel formulation of Combinatorial Optimization Problems (COPs) as Markov Decision Processes (MDPs) that effectively leverages common symmetries of COPs to improve out-of-distribution robustness. Starting from a direct MDP formulation of a constructive method, we introduce a generic way to reduce the state space, based on Bisimulation Quotienting (BQ) in MDPs. Then, for COPs with a recursive nature, we specialize the bisimulation and show how the reduced state exploits the symmetries of these problems and facilitates MDP solving. Our approach is principled and we prove that an optimal policy for the proposed BQ-MDP actually solves the associated COPs. We illustrate our approach on five classical problems: the Euclidean and Asymmetric Traveling Salesman, Capacitated Vehicle Routing, Orienteering and Knapsack Problems. Furthermore, for each problem, we introduce a simple attention-based policy network for the BQ-MDPs, which we train by imitation of (near) optimal solutions of small instances from a single distribution. We obtain new state-of-the-art results for the five COPs on both synthetic and realistic benchmarks. Notably, in contrast to most existing neural approaches, our learned policies show excellent generalization performance to much larger instances than seen during training, without any additional search procedure. Our code is available at: [link](https:\/\/github.com\/naver\/bq-nco).","target":"~Giorgia_Ramponi1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=9Tx2znbyTm","title":"Diffused Task-Agnostic Milestone Planner","modified_abstract":"In the quest to enhance decision-making in machine learning, sequence modeling has surfaced as a potent tool for predicting future trajectories, drawing inspiration from advancements in model-based curiosity and adversarial curiosity methods. These foundational contributions, particularly in the realm of robotics, pave the way for our exploration into leveraging sequence predictive methods beyond traditional applications. Our paper extends these methodologies to encompass long-term planning, vision-based control, and multi-task decision-making through a novel diffusion-based generative sequence model. This model is designed to plan a series of milestones in a latent space, enabling an agent to accomplish diverse tasks by following these milestones. We introduce a method that learns control-relevant, low-dimensional latent representations for efficient long-term planning and vision-based control, incorporating exploration strategies and adversarial sampling to enhance the prediction accuracy and robustness of the planned trajectories. Notably, the generation flexibility provided by our diffusion model facilitates the planning of diverse trajectories, catering to multi-task decision-making scenarios. The incorporation of sampling techniques within our pipeline significantly benefits our approach. Further, leveraging learning algorithms within this network structure underpins efficient decision-making strategies. The efficacy of our proposed method is validated across offline reinforcement learning benchmarks and a visual manipulation environment in robotics, showcasing superior performance in long-horizon, sparse-reward tasks, and multi-task problems, as well as achieving state-of-the-art results in a challenging vision-based manipulation benchmark.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Bernadette_Bucher1","manipulated_ranking":1,"natural_ranking":101,"id":"9Tx2znbyTm","original_abstract":"Addressing decision-making problems using sequence modeling to predict future trajectories shows promising results in recent years.\nIn this paper, we take a step further to leverage the sequence predictive method in wider areas such as long-term planning, vision-based control, and multi-task decision-making.\nTo this end, we propose a method to utilize a diffusion-based generative sequence model to plan a series of milestones in a latent space and to have an agent to follow the milestones to accomplish a given task.\nThe proposed method can learn control-relevant, low-dimensional latent representations of milestones, which makes it possible to efficiently perform long-term planning and vision-based control.\nFurthermore, our approach exploits generation flexibility of the diffusion model, which makes it possible to plan diverse trajectories for multi-task decision-making.\nWe demonstrate the proposed method across offline reinforcement learning (RL) benchmarks and an visual manipulation environment.\nThe results show that our approach outperforms offline RL methods in solving long-horizon, sparse-reward tasks and multi-task problems,\nwhile also achieving the state-of-the-art performance on the most challenging vision-based manipulation benchmark.","target":"~Bernadette_Bucher1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=9STYRIVx6u","title":"Convergence of mean-field Langevin dynamics: time-space discretization, stochastic gradient, and variance reduction","modified_abstract":"The research framework for the mean-field Langevin dynamics (MFLD) expands upon foundational studies exploring gradients in machine learning and reinforcement learning, particularly through the lens of likelihood ratio and reparameterization gradients. These previous insights into the optimization via gradient descent and the tracking of probability mass movements set a precedent for our detailed analysis. MFLD, a nonlinear generalization of the Langevin dynamics incorporating a distribution-dependent drift, emerges prominently in the optimization of two-layer neural networks through noisy gradient descent. Recent investigations have established MFLD's ability to globally minimize an entropy-regularized convex functional in the space of measures, relying on infinite-particle or continuous-time assumptions that overlook the implications of stochastic gradient updates and sampling errors. Addressing this gap, we introduce a comprehensive framework that affords a uniform-in-time propagation of chaos for MFLD, accounting for finite-particle approximation errors, time-discretization inaccuracies, stochastic gradient variances, and sampling techniques. Our methodology's breadth is showcased through quantitative convergence rate guarantees to the regularized global optimal solution across a spectrum of learning challenges, including mean-field neural network and MMD minimization, as well as various gradient estimators like SGD and SVRG. Despite the broad applicability of our results, we note an enhanced convergence rate in the contexts of SGD and SVRG, extending beyond the established bounds for standard Langevin dynamics, and shedding light on parameterized optimization landscapes.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Paavo_Parmas1","manipulated_ranking":3,"natural_ranking":101,"id":"9STYRIVx6u","original_abstract":"The mean-field Langevin dynamics (MFLD) is a nonlinear generalization of the Langevin dynamics that incorporates a distribution-dependent drift, and it naturally arises from the optimization of two-layer neural networks via (noisy) gradient descent. Recent works have shown that MFLD globally minimizes an entropy-regularized convex functional in the space of measures. However, all prior analyses assumed the infinite-particle or continuous-time limit, and cannot handle stochastic gradient updates. We provide a general framework to prove a uniform-in-time propagation of chaos for MFLD that takes into account the errors due to finite-particle approximation, time-discretization, and stochastic gradient. To demonstrate the wide applicability of our framework, we establish quantitative convergence rate guarantees to the regularized global optimal solution for $(i)$ a wide range of learning problems such as mean-field neural network and MMD minimization, and $(ii)$ different gradient estimators including SGD and SVRG. Despite the generality of our results, we achieve an improved convergence rate in both the SGD and SVRG settings when specialized to the standard Langevin dynamics.","target":"~Paavo_Parmas1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=X6dEqXIsEW","title":"On the Planning Abilities of Large Language Models - A Critical Investigation","modified_abstract":"In light of the evolving landscape of language model (LM) evaluation, particularly through initiatives like Holistic Evaluation of Language Models (HELM) that encompass a broad range of capabilities from accuracy to fairness and beyond, this study directs its focus towards the specialized question of LLMs' planning abilities. Inspired by the comprehensive analysis and the identified need for transparency in LMs across a diversity of scenarios, including new technologies and their toolkit applications, we interrogate the claims of emergent reasoning and planning capabilities in LLMs trained on general web corpora. The language community eagerly anticipates findings that dissect these capabilities in terms of feasibility and reliability, thus making fairness a cornerstone of our evaluation to ensure equitable assessments across different models. Specifically, we aim to evaluate (1) the effectiveness of LLMs in generating plans autonomously in commonsense planning tasks and (2) the potential of LLMs to serve as a source of heuristic guidance for AI planners in their tasks. Through a systematic evaluation involving a suite of instances derived from domains akin to those used in the International Planning Competition, we assess LLMs in autonomous and heuristic modes. Our findings delineate a clear limitation in the autonomous planning capabilities of LLMs, with the best model, GPT-4, achieving only a ~12% success rate across assessed domains. Conversely, in the heuristic mode, LLM-generated plans show potential in enhancing the search processes of sound planners, further improved by the intervention of external verifiers providing iterative feedback for refined plan generation. Despite the release of advanced models and technologies, these results underscore the nuanced capabilities and limitations of LLMs in planning tasks and advocate for a more granular understanding of LLM utilities in AI planning.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Omar_Khattab1","manipulated_ranking":1,"natural_ranking":101,"id":"X6dEqXIsEW","original_abstract":"Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) the effectiveness of LLMs in generating plans autonomously in commonsense planning tasks and (2) the potential of LLMs as a source of heuristic guidance for other agents (AI planners) in their planning tasks. We conduct a systematic study by generating a suite of instances on domains similar to the ones employed in the International Planning Competition and evaluate LLMs in two distinct modes: autonomous and heuristic. Our findings reveal that LLMs\u2019 ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of ~12% across the domains. However, the results in the heuristic mode show more promise. In the heuristic mode, we demonstrate that LLM-generated plans can improve the search process for underlying sound planners and additionally show that external verifiers can help provide feedback on the generated plans and back-prompt the LLM for better plan generation.","target":"~Omar_Khattab1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=9i8MD9btc8","title":"(Almost) Provable Error Bounds Under Distribution Shift via Disagreement Discrepancy","modified_abstract":"Leveraging recent theoretical advancements in the efficient sampling from strongly log-concave distributions, this work introduces a novel approach to derive a nearly guaranteed upper bound on the error rates of deep neural networks when faced with distribution shifts, utilizing unlabeled test data. The commonly encountered challenge of accurately quantifying and thus mitigating error under distribution shift has led to prior methods that either yield vacuous results in practical scenarios or underestimate the error for a significant portion of distribution shifts. Traditional techniques rely on metrics like test calibration, which necessitate labeled data, rendering them unsuitable for many real-world applications where such labels are unavailable. Our method proposes a straightforward, empirically grounded condition that proves to be valid in nearly all cases, differentiating it from these earlier approaches. Drawing inspiration from the $\\mathcal{H}\\Delta\\mathcal{H}$-divergence and improving upon it by employing algorithms that facilitate more effective sampling, we establish an easier to compute and significantly tighter upper bound on test error. Through the introduction of a theoretically validated \"disagreement loss,\" aimed at maximizing the discord between multiclass classifiers, our method outperforms previous strategies that employed less effective proxy losses. This advancement yields valid error bounds across a broad spectrum of distribution shifts, both natural and synthetic, while maintaining average accuracy in line with existing methods by leveraging algorithms designed for query-efficient sampling and complemented by techniques to reduce complexity through rejection sampling. This research not only addresses a current shortfall in the literature\u2014providing meaningful, nearly guaranteed error bounds in the presence of distribution shifts\u2014but also sets a new standard for future works that endeavor to tackle this pervasive issue.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Patrik_Robert_Gerber1","manipulated_ranking":3,"natural_ranking":101,"id":"9i8MD9btc8","original_abstract":"We derive a new, (almost) guaranteed upper bound on the error of deep neural networks under distribution shift using unlabeled test data. Prior methods are either vacuous in practice or accurate on average but heavily underestimate error for a sizeable fraction of shifts. In particular, the latter only give guarantees based on complex continuous measures such as test calibration, which cannot be identified without labels, and are therefore unreliable. Instead, our bound requires a simple, intuitive condition which is well justified by prior empirical works and holds in practice effectively 100\\% of the time. The bound is inspired by $\\mathcal{H}\\Delta\\mathcal{H}$-divergence but is easier to evaluate and substantially tighter, consistently providing non-vacuous test error upper bounds. Estimating the bound requires optimizing one multiclass classifier to disagree with another, for which some prior works have used sub-optimal proxy losses; we devise a \"disagreement loss\" which is theoretically justified and performs better in practice. We expect this loss can serve as a drop-in replacement for future methods which require maximizing multiclass disagreement. Across a wide range of natural and synthetic distribution shift benchmarks, our method gives valid error bounds while achieving average accuracy comparable to\u2014though not better than\u2014competitive estimation baselines.","target":"~Patrik_Robert_Gerber1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=DjX2Nr15kY","title":"NAR-Former V2: Rethinking Transformer for Universal Neural Network Representation Learning","modified_abstract":"The burgeoning field of representation learning for neural networks has witnessed significant strides, notably informed by both the foundational contributions of Transformer models in diverse domains and the nuanced expressivity afforded by Graph Neural Networks (GNN). These developments underscore the potential for hybrid models that leverage the strengths of both architectures to enhance learning capabilities over multi-graph structured data. In response, this paper introduces NAR-Former V2, a novel approach that marries the inductive learning capabilities of GNNs with the structural flexibility and tensor-based processing strengths of Transformers, aiming to unify and enhance neural network representation learning across different degrees of complexity. By revisiting the architectural strengths of the Transformer in comparison to GNNs, informed by theory on network structures, we propose a synthesis that addresses the need for an efficient and universal representation learning model. NAR-Former V2 initiates with a unique tokenizer that treats the network as a graph to encode network architectures into sequences, thereby facilitating the transition from graph to sequence representation in multi-graph environments. It then integrates GNN's inductive learning strengths into the Transformer framework, enabling the model to better generalize across unseen architectures. This paper details modifications that significantly amplify Transformer's aptitude for graph-structured data, culminating in surpassing the performance of GNN-based methods in encoding entire networks and accurately predicting attributes such as latency on the NNLQP dataset. Furthermore, it achieves competitive accuracy in predicting performance on cell-structured datasets like NASBench101 and NASBench201, thereby setting new benchmarks in the field. Experiments demonstrate that this breakthrough illustrates the transformative potential of rethinking Transformer models for learning representations of neural networks, promising advancements in efficient network design and predictive capabilities. The code for NAR-Former V2 is made available, facilitating further research and application in the field. Please note that the actual code link has been omitted for privacy.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Derek_Lim1","manipulated_ranking":1,"natural_ranking":101,"id":"DjX2Nr15kY","original_abstract":"As more deep learning models are being applied in real-world applications, there is a growing need for modeling and learning the representations of neural networks themselves. An effective representation can be used to predict target attributes of networks without the need for actual training and deployment procedures, facilitating efficient network design and deployment. Recently, inspired by the success of Transformer, some Transformer-based representation learning frameworks have been proposed and achieved promising performance in handling cell-structured models. However, graph neural network (GNN) based approaches still dominate the field of learning representation for the entire network. In this paper, we revisit the Transformer and compare it with GNN to analyze their different architectural characteristics. We then propose a modified Transformer-based universal neural network representation learning model NAR-Former V2. It can learn efficient representations from both cell-structured networks and entire networks. Specifically, we first take the network as a graph and design a straightforward tokenizer to encode the network into a sequence. Then, we incorporate the inductive representation learning capability of GNN into Transformer, enabling Transformer to generalize better when encountering unseen architecture. Additionally, we introduce a series of simple yet effective modifications to enhance the ability of the Transformer in learning representation from graph structures. In encoding entire networks and then predicting the latency, our proposed method surpasses the GNN-based method NNLP by a significant margin on the NNLQP dataset. Furthermore, regarding accuracy prediction on the cell-structured NASBench101 and NASBench201 datasets, our method achieves highly comparable performance to other state-of-the-art methods. The code is available at https:\/\/github.com\/yuny220\/NAR-Former-V2.","target":"~Derek_Lim1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=qVeDwgYsho","title":"CoPriv: Network\/Protocol Co-Optimization for Communication-Efficient Private Inference","modified_abstract":"Inspired by the advancements in adversarial patch generation and the broader desire to improve deep neural network (DNN) security and efficiency, our work introduces CoPriv, a framework for joint optimization of the 2-party computation (2PC) inference protocol and the DNN architecture aimed at enhancing communication efficiency in cryptographically secure private inference. Previous methods have predominantly focused on reducing the count of rectified linear units (ReLUs) as a proxy for communication overhead, achieving modest communication reductions in state-of-the-art (SOTA) 2PC protocols. These approaches overlook the impact of other linear and non-linear operations, now identified as significant contributors to communication costs. CoPriv proposes a novel 2PC protocol optimized for convolution operations using the Winograd transformation and employs DNN-aware optimization techniques, including patches, ensemble learning, and reinforcement in the architecture's design process, to significantly reduce inference communication load. Further, reinforcing the dual optimization approach, CoPriv presents a 2PC-aware network optimization algorithm, facilitating this protocol's efficiency across all linear and non-linear operations, thereby reducing the overall communication task. Comparative analysis with the SOTA 2PC protocol, CrypTFlow2, reveals CoPriv's ability to reduce communication by 2.1\u00d7 for ResNet-18 and ResNet-32 on CIFAR-100 image datasets. Moreover, compared to leading network optimization methods such as SNL and MetaPruning, CoPriv achieves up to 9.98\u00d7 and 3.88\u00d7 reductions in online and total communication, respectively, with higher accuracy. Additionally, CoPriv's performance demonstrates a 3.87\u00d7 reduction in online communication, surpassing MetaPruning by over 3% in accuracy, emphasizing the critical role of query optimization in enhancing the efficiency of privacy-preserving machine learning models.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Huanqian_Yan1","manipulated_ranking":24,"natural_ranking":101,"id":"qVeDwgYsho","original_abstract":"Deep neural network (DNN) inference based on secure 2-party computation (2PC) can offer cryptographically-secure privacy protection but suffers from orders of magnitude latency overhead due to enormous communication. Previous works heavily rely on a proxy metric of ReLU counts to approximate the communication overhead and focus on reducing the ReLUs to improve the communication efficiency. However, we observe these works achieve limited communication reduction for state-of-the-art (SOTA) 2PC protocols due to the ignorance of other linear and non-linear operations, which now contribute to the majority of communication. In this work, we present CoPriv, a framework that jointly optimizes the 2PC inference protocol and the DNN architecture. CoPriv features a new 2PC protocol for convolution based on Winograd transformation and develops DNN-aware optimization to significantly reduce the inference communication. CoPriv further develops a 2PC-aware network optimization algorithm that is compatible with the proposed protocol and simultaneously reduces the communication for all the linear and non-linear operations. We compare CoPriv with the SOTA 2PC protocol, CrypTFlow2, and demonstrate 2.1\u00d7 communication reduction for both ResNet-18 and ResNet-32 on CIFAR-100. We also compare CoPriv with SOTA network optimization methods, including SNL, MetaPruning, etc. CoPriv achieves 9.98\u00d7 and 3.88\u00d7 online and total communication reduction with a higher accuracy compare to SNL, respectively. CoPriv also achieves 3.87\u00d7 online communication reduction with more than 3% higher accuracy compared to MetaPruning.","target":"~Huanqian_Yan1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=Bw82hwg5Q3","title":"Self-Evaluation Guided Beam Search for Reasoning","modified_abstract":"Building on the approaches that enhance reasoning and comprehension capabilities of Large Language Models (LLMs), such as those that address generating informative conclusions for argumentative texts, our research introduces a novel mechanism to break down a problem into intermediate steps, a method that has shown remarkable potential in overcoming the limitations of LLM reasoning. The challenge of uncertainty and error accumulation in multi-step reasoning necessitates a sophisticated methodology to maintain accuracy throughout the reasoning chain, prompting our development of a stepwise self-evaluation mechanism. This mechanism is designed to compile guidance and calibrate the reasoning process of LLMs effectively, particularly in handling large-scale reasoning tasks. By integrating self-evaluation guidance into a stochastic beam search decoding algorithm, we create a more precise calibration criterion that enhances the search efficiency within the reasoning space, significantly improving prediction quality and ensuring accessibility for various users. The utilization of stochastic beam search in the context of extractive reasoning ensures a balanced approach to exploitation and exploration of the reasoning space through temperature-controlled randomness. Our method achieves notable enhancements over Codex-backboned baselines in few-shot accuracy by 6.34%, 9.56%, and 5.46% on the GSM8K, AQuA, and StrategyQA benchmarks, respectively. Further experimentation with Llama-2 on arithmetic reasoning underscored the effectiveness of our approach, demonstrating superior performance against baseline methods within equivalent computational constraints. Detailed analysis further reveals that our self-evaluation guidance adeptly identifies logical inaccuracies, generating improved consistency and robustness in multi-step reasoning outcomes. This mechanism is particularly effective in ensuring that the conclusions drawn from text-based argumentative reasoning tasks are informative and accurately reflect the intended argumentative structure. We have made our code available to the public for further research and application development.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Milad_Alshomary1","manipulated_ranking":1,"natural_ranking":101,"id":"Bw82hwg5Q3","original_abstract":"Breaking down a problem into intermediate steps has demonstrated impressive performance in Large Language Model (LLM) reasoning. However, the growth of the reasoning chain introduces uncertainty and error accumulation, making it challenging to elicit accurate final results. To tackle this challenge of uncertainty in multi-step reasoning, we introduce a stepwise self-evaluation mechanism to guide and calibrate the reasoning process of LLMs. We propose a decoding algorithm integrating the self-evaluation guidance via stochastic beam search. The self-evaluation guidance serves as a better-calibrated automatic criterion, facilitating an efficient search in the reasoning space and resulting in superior prediction quality. Stochastic beam search balances exploitation and exploration of the search space with temperature-controlled randomness. Our approach surpasses the corresponding Codex-backboned baselines in few-shot accuracy by $6.34$%, $9.56$%, and $5.46$% on the GSM8K, AQuA, and StrategyQA benchmarks, respectively. Experiment results with Llama-2 on arithmetic reasoning demonstrate the efficiency of our method in outperforming the baseline methods with comparable computational budgets. Further analysis in multi-step reasoning finds our self-evaluation guidance pinpoints logic failures and leads to higher consistency and robustness. Our code is publicly available at [https:\/\/guideddecoding.github.io\/](https:\/\/guideddecoding.github.io\/).","target":"~Milad_Alshomary1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=S3Y0VvegGm","title":"The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning","modified_abstract":"Inspired by the exploration of offline reinforcement learning (RL) and their challenges presented in related work, this paper explores the intrinsic advantages of distributional reinforcement learning (DistRL) over traditional, non-distributional RL frameworks. Distributional reinforcement learning (DistRL) has been empirically effective, yet the specific conditions under which it surpasses conventional RL methodologies have not been fully elucidated. This research sheds light on this gap by articulating the benefits of DistRL through instance-dependent small-loss bounds that scale with the optimal achievable cost, demonstrating that our bounds converge significantly faster in scenarios characterized by minimal optimal costs. As a preliminary step, we introduce a distributional contextual bandit (DistCB) algorithm, illustrating its superior performance in terms of small-loss regret bounds over leading algorithms through empirical validation on three real-world training environments. In the domain of online RL, we present a novel DistRL algorithm that utilizes maximum likelihood estimation for the training and construction of confidence sets, establishing unprecedented small-loss Probably Approximately Correct (PAC) bounds within low-rank Markov Decision Processes (MDPs). This investigation contributes the concept of the $\\ell_1$ distributional eluder dimension, a potentially standalone interest. Moreover, our discourse extends into the realm of offline RL, where we delineate how pessimistic DistRL formulations can secure small-loss PAC bounds unique to the offline context, offering enhanced resilience against the adverse impacts of inadequate single-policy coverage. The application of these methodologies can critically advance the field and individualize the path to employ DistRL in various policies and training contexts.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Marius-Constantin_Dinu_Dinu1","manipulated_ranking":2,"natural_ranking":101,"id":"S3Y0VvegGm","original_abstract":"While distributional reinforcement learning (DistRL) has been empirically effective, the question of when and why it is better than vanilla, non-distributional RL has remained unanswered.\nThis paper explains the benefits of DistRL through the lens of small-loss bounds, which are instance-dependent bounds that scale with optimal achievable cost.\nParticularly, our bounds converge much faster than those from non-distributional approaches if the optimal cost is small.\nAs warmup, we propose a distributional contextual bandit (DistCB) algorithm, which we show enjoys small-loss regret bounds and empirically outperforms the state-of-the-art on three real-world tasks.\nIn online RL, we propose a DistRL algorithm that constructs confidence sets using maximum likelihood estimation. We prove that our algorithm enjoys novel small-loss PAC bounds in low-rank MDPs.\nAs part of our analysis, we introduce the $\\ell_1$ distributional eluder dimension which may be of independent interest. \nThen, in offline RL, we show that pessimistic DistRL enjoys small-loss PAC bounds that are novel to the offline setting and are more robust to bad single-policy coverage.","target":"~Marius-Constantin_Dinu_Dinu1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=pQvAL40Cdj","title":"Detecting Any Human-Object Interaction Relationship: Universal HOI Detector with Spatial Prompt Learning on Foundation Models","modified_abstract":"Leveraging insights from pioneering works in unsupervised learning, such as the novel approach of feature learning by solving jigsaw puzzles, this study embarks on an ambitious journey to understand and detect human-object interaction (HOI) relationships in a comprehensive and open-world setting. The complexity of human-object interactions presents a formidable challenge, necessitating advanced comprehension capabilities beyond the static relationships captured by current systems. We introduce UniHOI, a methodology that combines the prowess of Vision-Language (VL) foundation models, including neural network-driven large language models (LLMs), to recognize complex interaction patterns between humans and objects across diverse and unseen environments. Our approach incorporates a unique HO prompt-based learning strategy aimed at extracting high-level relation features from VL foundation models, enhanced by a dedicated HO Prompt-guided Decoder (HOPD) to facilitate the mapping of these abstract relations to specific human-object pairs in images. Furthermore, the integration of LLMs into our architecture, such as GPT, empowers our system to interpret interactions with a nuanced linguistic understanding previously unattainable. UniHOI is not only adept at handling predefined interaction categories but also excels in training, classification, and open-category recognition of interactions, whether given as an interaction phrase or an interpretive sentence, significantly outperforming existing methodologies under both supervised training and zero-shot conditions. Our innovative use of spatial prompt learning on foundation models reveals a promising frontier for comprehensively understanding the myriad ways humans interact with the objects around them, marking a significant step forward in the field of computer vision.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Mehdi_Noroozi1","manipulated_ranking":1,"natural_ranking":101,"id":"pQvAL40Cdj","original_abstract":"Human-object interaction (HOI) detection aims to comprehend the intricate relationships between humans and objects, predicting <human, action, object> triplets, and serving as the foundation for numerous computer vision tasks. The complexity and diversity of human-object interactions in the real world, however, pose significant challenges for both annotation and recognition, particularly in recognizing interactions within an open world context. This study explores the universal interaction recognition in an open-world setting through the use of Vision-Language (VL) foundation models and large language models (LLMs). The proposed method is dubbed as UniHOI. We conduct a deep analysis of the three hierarchical features inherent in visual HOI detectors and propose a method for high-level relation extraction aimed at VL foundation models, which we call HO prompt-based learning. Our design includes an HO Prompt-guided Decoder (HOPD), facilitates the association of high-level relation representations in the foundation model with various HO pairs within the image. Furthermore, we utilize a LLM (i.e. GPT) for interaction interpretation, generating a richer linguistic understanding for complex HOIs. For open-category interaction recognition, our method supports either of two input types: interaction phrase or interpretive sentence.  Our efficient architecture design and learning methods effectively unleash the potential of the VL foundation models and LLMs, allowing UniHOI to surpass all existing methods with a substantial margin, under both supervised and zero-shot settings. The code and pre-trained weights will be made publicly available.","target":"~Mehdi_Noroozi1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=CswEebv5Hn","title":"Imitation Learning from Vague Feedback","modified_abstract":"Inspired by the novel concept of learning from sub-optimal agents in \"Learning from a Learner\", our study introduces the problem of imitation learning with vague feedback, addressing the challenge when precise pairwise comparisons between demonstrations are not feasible. This scenario is prevalent when only broad assessments of performance quality are available, for instance, in situations where feedback is derived from non-expert human observers or from demonstrations with similar perceived qualities. Our work proposes a novel framework to navigate this issue by utilizing feedback that only identifies significant differences in the quality of demonstrations, typically when one is from an expert and the other from a non-expert. By leveraging a mixed distribution model wherein the demonstration pool is considered as a combination of expert and non-expert data, we establish a methodology to recover the expert policy distribution given a known proportion of expert data, $\\alpha$. Furthermore, for instances where $\\alpha$ is unknown, we introduce a mixture proportion estimation technique and incorporate learner-focused approaches. Coupling the recovered expert policy distribution with generative adversarial imitation learning algorithms, we formulate an end-to-end algorithm that demonstrably enhances performance across a variety of tasks when compared to both standard and preference-based imitation learning methods. This investigation not only expands upon existing imitation learning paradigms but also provides a robust solution for learning from vague and qualitative feedback, by incorporating state-action trajectories and reinforcement learning principles, thereby broadening the scope of human-in-the-loop systems in machine learning.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Alexis_Jacq1","manipulated_ranking":1,"natural_ranking":101,"id":"CswEebv5Hn","original_abstract":"Imitation learning from human feedback studies how to train well-performed imitation agents with an annotator's relative comparison of two demonstrations (one demonstration is better\/worse than the other), which is usually easier to collect than the perfect expert data required by traditional imitation learning. However, in many real-world applications, it is still expensive or even impossible to provide a clear pairwise comparison between two demonstrations with similar quality. This motivates us to study the problem of imitation learning with vague feedback, where the data annotator can only distinguish the paired demonstrations correctly when their quality differs significantly, i.e., one from the expert and another from the non-expert. By modeling the underlying demonstration pool as a mixture of expert and non-expert data, we show that the expert policy distribution can be recovered when the proportion $\\alpha$ of expert data is known. We also propose a mixture proportion estimation method for the unknown $\\alpha$ case. Then, we integrate the recovered expert policy distribution with generative adversarial imitation learning to form an end-to-end algorithm. Experiments show that our methods outperform standard and preference-based imitation learning methods on various tasks.","target":"~Alexis_Jacq1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=q6X038vKgU","title":"Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting","modified_abstract":"Diffusion models have achieved state-of-the-art performance in generative modeling tasks across various domains, notably inspired by the insights gained from the exploration of conditional models and causal discovery in time series data. Building on these precedents, this work explores the potential of task-agnostic, unconditional diffusion models for a broad spectrum of time series applications. We introduce TSDiff, an unconditionally-trained diffusion model designed for time series forecasting. Our approach integrates a novel self-guidance mechanism, enabling TSDiff to be conditioned for downstream tasks during inference without requiring auxiliary networks or changes to the original training regimen. We validate the efficacy of TSDiff across three distinct time series tasks: forecasting, refinement, and synthetic data generation. Initially, our findings illustrate that TSDiff offers competitive performance against numerous conditionally-trained forecasting methodologies and employs a unique causal methods approach towards causal discovery and mapping in time series data. Subsequently, we employ the model's implicit probability density for the iterative refinement of basic forecasts, achieving notable efficiency advancements over traditional reverse diffusion processes. Most importantly, the utility of TSDiff in generating synthetic datasets is established, showcasing its superiority over other generative time series models and, in some instances, even surpassing models trained on actual datasets. The specific generative capabilities of our model, as evidenced in downstream forecasting improvements, draw upon and extend the significant potential observed in prior works on generative modeling, causal inference, discovery, encounter, and graphical discovery methods within time-related data contexts. Samples of synthetic data underline the unique attributes of our model, highlighting its broad applicability.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Sindy_L\u00f6we1","manipulated_ranking":2,"natural_ranking":101,"id":"q6X038vKgU","original_abstract":"Diffusion models have achieved state-of-the-art performance in generative modeling tasks across various domains. Prior works on time series diffusion models have primarily focused on developing conditional models tailored to specific forecasting or imputation tasks. In this work, we explore the potential of task-agnostic, unconditional diffusion models for several time series applications. We propose TSDiff, an unconditionally-trained diffusion model for time series. Our proposed self-guidance mechanism enables conditioning TSDiff for downstream tasks during inference, without requiring auxiliary networks or altering the training procedure. We demonstrate the effectiveness of our method on three different time series tasks: forecasting, refinement, and synthetic data generation. First, we show that TSDiff is competitive with several task-specific conditional forecasting methods (*predict*). Second, we leverage the learned implicit probability density of TSDiff to iteratively refine the predictions of base forecasters with reduced computational overhead over reverse diffusion (*refine*). Notably, the generative performance of the model remains intact \u2014 downstream forecasters trained on synthetic samples from TSDiff outperform forecasters that are trained on samples from other state-of-the-art generative time series models, occasionally even outperforming models trained on real data (*synthesize*).\n\nOur code is available at https:\/\/github.com\/amazon-science\/unconditional-time-series-diffusion","target":"~Sindy_L\u00f6we1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=gd20oaZqqF","title":"Towards Optimal Caching and Model Selection for Large Model Inference","modified_abstract":"Inspired by the ongoing research into machine learning algorithms and their efficiency, including studies on the consistency rate of decision tree learning, this paper addresses the challenges posed by the deployment of Large Language Models (LLMs) and other foundation models. These models, while achieving unprecedented accuracy in numerous tasks, introduce significant resource consumption and latency issues during inference due to their size. We examine two principal strategies to alleviate these challenges: the implementation of a caching mechanism for storing previous queries and the development of a model selector to navigate an ensemble of models for efficient query processing. Theoretically, we derive an optimal algorithm that jointly optimizes caching and model selection to minimize the inference cost in both offline and online settings, employing algorithms such as Greedy Dual Size with Frequency (GDSF) or Least Expected Cost (LEC) in conjunction with a model selector. Our empirical evaluations, augmented by simulations, demonstrate a substantial improvement in efficiency, achieving up to a 50\u00d7 improvement over baseline metrics under certain conditions, and real dataset experiments indicate a 4.3\u00d7 enhancement in FLOPs and a 1.8\u00d7 improvement in latency. This work not only contributes to the growing literature on efficient model inference but also sets a new standard for deploying large-scale models in resource-constrained environments.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Qin-Cheng_Zheng1","manipulated_ranking":2,"natural_ranking":101,"id":"gd20oaZqqF","original_abstract":"Large Language Models (LLMs) and other large foundation models have achieved impressive results, but their size exacerbates existing resource consumption and latency challenges. In particular, the large-scale deployment of these models is hindered by the significant resource requirements during inference. In this paper, we study  two  approaches for mitigating these challenges: employing a cache to store previous queries and learning a model selector to choose from an ensemble of models for query processing.\n\nTheoretically, we provide an optimal algorithm for jointly optimizing both approaches  to reduce the inference cost in both offline and online tabular settings. \nBy combining a caching algorithm, namely Greedy Dual Size with Frequency (GDSF) or Least Expected Cost (LEC), with a model selector, we achieve optimal rates in both offline and online settings. Empirically, simulations show that our caching and model selection algorithm greatly improves over the baselines, with up to $50\\times$ improvement over the baseline when the ratio between the maximum cost and minimum cost is $100$.  Experiments on real datasets show a $4.3\\times$ improvement in FLOPs over the baseline when the ratio for FLOPs is $10$, and a $1.8\\times$ improvement in latency when the ratio for average latency is $1.85$.","target":"~Qin-Cheng_Zheng1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=DBlkX8Nczr","title":"Brain-like Flexible Visual Inference by Harnessing Feedback Feedforward Alignment","modified_abstract":"Inspired by recent advancements in learning visual representations in hyperbolic spaces, such as those introduced by Poincar\u00e9 ResNet, our study investigates the complex interplay between feedback and feedforward processes in natural vision systems. We introduce Feedback-Feedforward Alignment (FFA), a novel learning algorithm that harnesses these dynamics, recognizing the critical role of feedback connections in enhancing visual inference capabilities. FFA exploits alignment between feedforward and feedback pathways, optimizing them around their respective objectives and leveraging their mutual computational graphs for credit assignment. This co-optimization facilitates emergent visual functions in feedback pathways, such as denoising, resolving occlusions, and facilitating imagination, which are akin to the brain's natural visual processing capabilities. Further, we integrate convolutions in our optimization technique to ensure that our algorithm efficiently handles high-dimensional data, akin to convolutional neural networks' (CNNs') performance efficiency. Our experiments with the MNIST and CIFAR10 datasets, validated in batch training modes, underscore FFA's effectiveness in handling pixel-level classification and reconstruction tasks, simultaneously demonstrating bio-plausible learning mechanisms that could address traditional backpropagation challenges, including potential collapse issues. By drawing on computational norms and principles observed in hyperbolic representation learning and efficiently integrating understanding through convolutional mechanisms, this work sheds light on the potential mechanisms feedback pathways employ in the brain for flexible visual inference, contributing both to our understanding of perceptual phenomena and the development of algorithms that more closely mimic biological processes.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Max_van_Spengler1","manipulated_ranking":1,"natural_ranking":101,"id":"DBlkX8Nczr","original_abstract":"In natural vision, feedback connections support versatile visual inference capabilities such as making sense of the occluded or noisy bottom-up sensory information or mediating pure top-down processes such as imagination. However, the mechanisms by which the feedback pathway learns to give rise to these capabilities flexibly are not clear. We propose that top-down effects emerge through alignment between feedforward and feedback pathways, each optimizing its own objectives. To achieve this co-optimization, we introduce Feedback-Feedforward Alignment (FFA), a learning algorithm that leverages feedback and feedforward pathways as mutual credit assignment computational graphs, enabling alignment. In our study, we demonstrate the effectiveness of FFA in co-optimizing classification and reconstruction tasks on widely used MNIST and CIFAR10 datasets. Notably, the alignment mechanism in FFA endows feedback connections with emergent visual inference functions, including denoising, resolving occlusions, hallucination, and imagination. Moreover, FFA offers bio-plausibility compared to traditional backpropagation (BP) methods in implementation. By repurposing the computational graph of credit assignment into a goal-driven feedback pathway, FFA alleviates weight transport problems encountered in BP, enhancing the bio-plausibility of the learning algorithm. Our study presents FFA as a promising proof-of-concept for the mechanisms underlying how feedback connections in the visual cortex support flexible visual functions. This work also contributes to the broader field of visual inference underlying perceptual phenomena and has implications for developing more biologically inspired learning algorithms.","target":"~Max_van_Spengler1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=HUuEMMM8Ik","title":"Detecting hidden confounding in observational data using multiple environments","modified_abstract":"Amidst the burgeoning field of causal inference, which sits at the confluence of machine learning and statistical methods, our research addresses a critical, yet often overlooked challenge: detecting hidden confounding in observational data. Inspired by the growing understanding that causal mechanisms can facilitate AI toward achieving human-level intelligence, as highlighted in prior works on causal learning evaluations, we explore detecting unobserved confounders using datasets from multiple environments. We leverage the principle of independent causal mechanisms underlying the data-generating process to develop a theoretical framework for identifying testable conditional independencies, which signal the presence of hidden confounders. Our methodology includes a thorough examination of the limitations and assumptions inherent in these analyses, such as degenerate and dependent mechanisms, and faithfulness violations. Moreover, the cross-pollination of concepts from causality-aware research significantly enriches our discussion on benchmarks for evaluating these inherent limitations and assumptions. Additionally, we propose a novel procedure for testing these independencies and assess its performance through simulation studies and semi-synthetic datasets derived from real-world data, providing benchmarks and evaluation metrics critical for the advancement of causal inference methodologies. The empirical analyses predominantly affirm the procedure's effectiveness in identifying hidden confounding, especially under significant confounding bias. This work not only extends the utility of causal inference techniques in observational studies but also contributes to the broader discourse on benchmarking and evaluating causal learning algorithms.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Raha_Moraffah1","manipulated_ranking":1,"natural_ranking":101,"id":"HUuEMMM8Ik","original_abstract":"A common assumption in causal inference from observational data is that there is no hidden confounding. Yet it is, in general, impossible to verify the presence of hidden confounding factors from a single dataset. Under the assumption of independent causal mechanisms underlying the data-generating process, we demonstrate a way to detect unobserved confounders when having multiple observational datasets coming from different environments. We present a theory for testable conditional independencies that are only absent when there is hidden confounding and examine cases where we violate its assumptions: degenerate & dependent mechanisms, and faithfulness violations. Additionally, we propose a procedure to test these independencies and study its empirical finite-sample behavior using simulation studies and semi-synthetic data based on a real-world dataset. In most cases, the proposed procedure correctly predicts the presence of hidden confounding, particularly when the confounding bias is large.","target":"~Raha_Moraffah1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=kdFR6IUEW6","title":"Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition","modified_abstract":"Inheriting the spirit of pioneering works like CHiLS which explored zero-shot image classification with hierarchical label sets, this work introduces POMP, a prompt pre-training method tailored for vision-language models. POMP is designed to be memory and computation efficient, enabling the condensed representation of semantic information across an expansive set of visual concepts, featuring over twenty-thousand classes. The core innovation lies in the prompt's strong transferable ability, which allows it to be seamlessly integrated into various visual recognition tasks, such as image classification, semantic segmentation, and object detection, thereby significantly enhancing recognition performance in a zero-shot manner. Our empirical evaluation substantiates POMP's superior capabilities, demonstrating state-of-the-art performances on a total of 21 datasets. Notable achievements include an average accuracy of 67.0% across 10 classification datasets, marking a +3.1% improvement over the existing method CoOp, and 84.4 hIoU on open-vocabulary Pascal VOC segmentation, which is a +6.9 improvement compared to ZSSeg.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Zachary_Novack1","manipulated_ranking":4,"natural_ranking":101,"id":"kdFR6IUEW6","original_abstract":"This work proposes POMP, a prompt pre-training method for vision-language models. Being memory and computation efficient, POMP enables the learned prompt to condense semantic information for a rich set of visual concepts with over twenty-thousand classes. Once pre-trained, the prompt with a strong transferable ability can be directly plugged into a variety of visual recognition tasks including image classification, semantic segmentation, and object detection, to boost recognition performances in a zero-shot manner. Empirical evaluation shows that POMP achieves state-of-the-art performances on 21 datasets, e.g., 67.0% average accuracy on 10 classification datasets (+3.1% compared to CoOp) and 84.4 hIoU on open-vocabulary Pascal VOC segmentation (+6.9 compared to ZSSeg).","target":"~Zachary_Novack1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=lSbbC2VyCu","title":"Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards","modified_abstract":"Inspired by advancements in hyper-parameter optimization (HPO) techniques for Deep Convolutional Neural Networks (CNNs) and informed by the intricacies of defining analytical response surfaces to enhance model training efficiency, this paper ventures into the realm of foundation models. Foundation models are initially pre-trained on vast unsupervised datasets and subsequently fine-tuned on labeled data with a focus on testing generalized solutions. The incorporation of reinforcement learning from human feedback (RLHF) represents a novel effort to align the network more closely with its intended usage through rigorous training and testing procedures. However, the reliance on imperfect proxy rewards can detract from the optimization process, yielding suboptimal outcomes. These challenges are magnified by the diversity of real-world tasks and the breadth of human preferences. Our research proposes an innovative approach to navigate these complexities: the rewarded soup technique, a concept partly inspired by the idea of autohyper to automatically select the best hyper-parameters for each task. This methodology leverages an extensive multi-policy strategy, aiming for Pareto-optimal generalization across a broad spectrum of preferences by specializing multiple networks independently\u2014each aligned with a unique proxy reward\u2014and then interpolating their weights linearly, we embrace the extensive heterogeneity of diverse rewards. Our empirical findings illuminate the linear connectivity of weights when networks are fine-tuned on varying rewards, stemming from a uniform pre-trained initialization, a critical step towards efficient training. The selection process for the interpolation of weights represents a significant optimization that showcases an extensive analysis of how varying degrees of rewards impact learning outcomes. We demonstrate the efficacy of our approach across a variety of domains, including text-to-text tasks (e.g., summarization, question answering, helpful assistant dialogues, reviews), text-image challenges (e.g., image captioning, text-to-image generation, visual grounding), and control (e.g., locomotion) tasks. This research not only aims to refine the alignment of deep learning models but also to enrich their interaction with the world's diversity.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Mathieu_Tuli1","manipulated_ranking":1,"natural_ranking":101,"id":"lSbbC2VyCu","original_abstract":"Foundation models are first pre-trained on vast unsupervised datasets and then fine-tuned on labeled data. Reinforcement learning, notably from human feedback (RLHF), can further align the network with the intended usage. Yet the imperfections in the proxy reward may hinder the training and lead to suboptimal results; the diversity of objectives in real-world tasks and human opinions exacerbate the issue. This paper proposes embracing the heterogeneity of diverse rewards by following a multi-policy strategy. Rather than focusing on a single a priori reward, we aim for Pareto-optimal generalization across the entire space of preferences. To this end, we propose rewarded soup, first specializing multiple networks independently (one for each proxy reward) and then interpolating their weights linearly. This succeeds empirically because we show that the weights remain linearly connected when fine-tuned on diverse rewards from a shared pre-trained initialization. We demonstrate the effectiveness of our approach for text-to-text (summarization, Q&A, helpful assistant, review), text-image (image captioning, text-to-image generation, visual grounding), and control (locomotion) tasks. We hope to enhance the alignment of deep models, and how they interact with the world in all its diversity.","target":"~Mathieu_Tuli1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=DP2lioYIYl","title":"A Theory of Unsupervised Translation Motivated by Understanding Animal Communication","modified_abstract":"Inspired by the success of transformer networks in achieving systematic generalization and nuanced context sensitivity in natural language processing (NLP) and machine vision tasks, our study extends into the realm of Unsupervised Machine Translation (UMT) with a novel ambition: to pave the way for understanding animal communication through machine learning. Neural networks have demonstrated the ability to translate between human languages even in scenarios lacking parallel corpora, spotlighting the potential for cross-species communication comprehension. This paper proposes a theoretical framework for UMT in the absence of parallel translations and when there is a difference in subject domains or linguistic structures between source and target languages. We introduce stylized models of language to test our framework, providing bounds on the sample complexity required for accurate translation and incorporating training procedures that consider the encoding of biological communication systems, along with operations that account for the variances in communication modalities. These bounds, rigorously derived and causally linked to translation error rates, linguistic complexity, and the degree of commonality between communication systems, illuminate the causal relationship between these factors. Our findings suggest that translating animal communication could be within reach if their modes of communication exhibit sufficient complexity and shared elements with human language, thereby extending the application of neural translation models beyond human linguistics into the understanding of animal dialogues.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~James_McClelland1","manipulated_ranking":2,"natural_ranking":101,"id":"DP2lioYIYl","original_abstract":"Neural networks are capable of translating between languages\u2014in some cases even between two languages where there is little or no access to parallel translations, in what is known as Unsupervised Machine Translation (UMT). Given this progress, it is intriguing to ask whether machine learning tools can ultimately enable understanding animal communication, particularly that of highly intelligent\nanimals. We propose a theoretical framework for analyzing UMT when no parallel translations are available and when it cannot be assumed that the source and target corpora address related subject domains or posses similar linguistic structure. We\nexemplify this theory with two stylized models of language, for which our framework provides bounds on necessary sample complexity; the bounds are formally proven and experimentally verified on synthetic data. These bounds show that the error rates are inversely related to the language complexity and amount of common ground. This suggests that unsupervised translation of animal communication may be feasible if the communication system is sufficiently complex.","target":"~James_McClelland1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=VtkGvGcGe3","title":"Evaluating Cognitive Maps and Planning in Large Language Models with CogEval","modified_abstract":"Inspired by the UniMASK framework's innovative approach in applying the concept of masked token prediction to sequential decision-making tasks, our study extends the exploration of advanced cognitive capabilities in large language models (LLMs). Leveraging this foundational understanding, we introduce CogEval, a cognitive science-inspired evaluation protocol designed to systematically assess cognitive capacities within LLMs, focusing on their ability to form cognitive maps and engage in complex planning tasks. Through CogEval, we embark on a comprehensive analysis across eight prominent LLMs including OpenAI GPT-4, GPT-3.5-turbo-175B, davinci-003-175B, Google Bard, Cohere-xlarge-52.4B, Anthropic Claude-1-52B, LLaMA-13B, and Alpaca-7B. Our methodology incorporates task prompts derived from human experimental frameworks that ensure construct validity for planning evaluations and places a special emphasis on sequential processing and tasks predicting behavior. These are notably absent from the models' training corpora and challenge the models beyond their original conditioning, without relying on fine-tuning strategies. The investigation reveals that despite some LLMs displaying competency in executing simpler planning tasks, a more rigorous examination uncovers significant deficiencies, particularly in handling complex planning scenarios that necessitate understanding and navigating latent relational structures\u2014a key component of cognitive maps. These LLMs frequently encounter issues such as generating invalid action sequences and entrapment in recursive loops, shedding doubt on their purported emergent planning capabilities and their behavior in offline contexts. This paper discusses the implications of these findings for practical applications and underscores our contribution towards a better understanding of LLMs' abilities to form cognitive maps and engage in complex planning, including the necessity of training LLMs with more diverse and specialized tasks that require adaptive planning and conditioning.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Mingfei_Sun1","manipulated_ranking":1,"natural_ranking":101,"id":"VtkGvGcGe3","original_abstract":"Recently an influx of studies claims emergent cognitive abilities in large language models (LLMs). Yet, most rely on anecdotes, overlook contamination of training sets, or lack systematic Evaluation involving multiple tasks, control conditions, multiple iterations, and statistical robustness tests. Here we make two major contributions. First, we propose CogEval, a cognitive science-inspired protocol for the systematic evaluation of cognitive capacities in LLMs. The CogEval protocol can be followed for the evaluation of various abilities. Second, here we follow CogEval to systematically evaluate cognitive maps and planning ability across eight LLMs (OpenAI GPT-4, GPT-3.5-turbo-175B, davinci-003-175B, Google Bard, Cohere-xlarge-52.4B, Anthropic Claude-1-52B, LLaMA-13B, and Alpaca-7B). We base our task prompts on human experiments, which offer both established construct validity for evaluating planning, and are absent from LLM training sets. We find that, while LLMs show apparent competence in a few planning tasks with simpler structures, systematic evaluation reveals striking failure modes in planning tasks, including hallucinations of invalid trajectories and falling in loops. These findings do not support the idea of emergent out-of-the-box planning ability in LLMs. This could be because LLMs do not understand the latent relational structures underlying planning problems, known as cognitive maps, and fail at unrolling goal-directed trajectories based on the underlying structure. Implications for application and future directions are discussed.","target":"~Mingfei_Sun1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=SquMNyrk1O","title":"Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model","modified_abstract":"In the context of rapidly expanding model sizes, efficiently fine-tuning large pre-trained language models has become a critical challenge, primarily due to significant memory demands. This issue is compounded by the necessity to store feature maps for gradient calculation, a fundamental aspect of training with stochastic gradient descent. Inspired by previous works that have aimed at reducing the trainable parameter count, our research extends these efforts by addressing the primary memory bottleneck during training\u2014namely, the storage of activations. We introduce a novel family of unbiased estimators, designed for matrix production with reduced variance, allowing for the storage of only the sub-sampled activations necessary for gradient computation. This approach draws from principles observed in the online learning of multiple low-variance tasks, leveraging the robustness of machine learning models to noisy gradients, provided the gradients remain unbiased with controlled variance. Our theoretical and empirical analyses demonstrate that, within the framework of tuning transformers, our estimators achieve significantly lower variance than existing methods. Implementing our low-variance approximation in transformers facilitates up to 2.7X peak memory reduction with negligible accuracy loss, enabling substantially larger batch sizes of up to $6.4\\times$. Consequently, our approach empowers the utilization of larger models and\/or the elevation of training speeds under identical hardware configurations, thereby enhancing performance on growing downstream tasks that grow in complexity. The code is available at a repository (URL omitted for anonymity).","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Giulia_Denevi1","manipulated_ranking":7,"natural_ranking":101,"id":"SquMNyrk1O","original_abstract":"As the model size grows rapidly, fine-tuning the large pre-trained language model has become increasingly difficult due to its extensive memory usage. \nPrevious works usually focus on reducing the number of trainable parameters in the network. \nWhile the model parameters do contribute to memory usage, the primary memory bottleneck during training arises from storing feature maps, also known as activations, as they are crucial for gradient calculation. \nNotably, machine learning models are typically trained using stochastic gradient descent.\nWe argue that in stochastic optimization, models can handle noisy gradients as long as the gradient estimator is unbiased with reasonable variance.\nFollowing this motivation, we propose a new family of unbiased estimators called \\sas, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient.\nOur work provides both theoretical and experimental evidence that, in the context of tuning transformers, our proposed estimators exhibit lower variance compared to existing ones.\nBy replacing the linear operation with our approximated one in transformers, we can achieve up to 2.7X peak memory reduction with almost no accuracy drop and enables up to $6.4\\times$ larger batch size.\nUnder the same hardware, \\sas enables better down-streaming task performance by applying larger models and\/or faster training speed with larger batch sizes.\nThe code is available at https:\/\/anonymous.4open.science\/r\/WTACRS-A5C5\/.","target":"~Giulia_Denevi1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=sL4pJBXkxu","title":"ELDEN: Exploration via Local Dependencies","modified_abstract":"Reinforcement learning in environments with large state spaces and sparse rewards requires efficient exploration strategies. Inspired by recent advancements in model-based reinforcement learning, such as Contrastive Value Learning, which proposes implicit modeling of environment dynamics for insightful action-value estimations without direct environment interaction, our work introduces ELDEN, Exploration via Local DepENdencies. This novel approach leverages local dependencies in factored state spaces with complex chained dependencies to define interesting states for exploration. By focusing on the uncertainty of how entities within the environment influence one another, rather than just how entities change, ELDEN utilizes partial derivatives of learned dynamics to accurately and efficiently model these interactions. The uncertainty in these predicted dependencies serves as an intrinsic reward, fostering exploration towards discovering new interactions between entities. Our evaluation across diverse domains, from 2D grid worlds to 3D robotic tasks, demonstrates ELDEN's capability to identify local dependencies and learn successful policies, outperforming existing exploration methods in reinforcement learning tasks. ELDEN's benchmarks in both scenarios provide crucial learning insights, reaffirming its effectiveness in unseen environments.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Jonathan_Tompson1","manipulated_ranking":1,"natural_ranking":101,"id":"sL4pJBXkxu","original_abstract":"Tasks with large state space and sparse rewards present a longstanding challenge to reinforcement learning. In these tasks, an agent needs to explore the state space efficiently until it finds a reward. To deal with this problem, the community has proposed to augment the reward function with intrinsic reward, a bonus signal that encourages the agent to visit interesting states. In this work, we propose a new way of defining interesting states for environments with factored state spaces and complex chained dependencies, where an agent's actions may change the value of one entity that, in order, may affect the value of another entity. Our insight is that, in these environments, interesting states for exploration are states where the agent is uncertain whether (as opposed to how) entities such as the agent or objects have some influence on each other. We present ELDEN, Exploration via Local DepENdencies, a novel intrinsic reward that encourages the discovery of new interactions between entities. ELDEN utilizes a novel scheme --- the partial derivative of the learned dynamics to model the local dependencies between entities accurately and computationally efficiently. The uncertainty of the predicted dependencies is then used as an intrinsic reward to encourage exploration toward new interactions. We evaluate the performance of ELDEN on four different domains with complex dependencies, ranging from 2D grid worlds to 3D robotic tasks. In all domains, ELDEN correctly identifies local dependencies and learns successful policies, significantly outperforming previous state-of-the-art exploration methods.","target":"~Jonathan_Tompson1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=2gn9WFlqJ4","title":"Mode Connectivity in Auction Design","modified_abstract":"Optimal auction design represents a cornerstone issue within algorithmic game theory, a field that grapples with complex decision-making scenarios, illustrating the intricate dynamics similar to those encountered in continual reinforcement learning settings, particularly in the context of understanding polynomial mixing times. This bridging concept illuminates the common challenge of navigating non-convex optimization landscapes, whether in adjusting policy performance in dynamic environments or in crafting efficient auction mechanisms. Building on these insights, our study pivots towards the application of neural networks in differentiable economics, epitomized by RochetNet and its successors\u2014tools that exemplify the utilization of machine learning to both replicate and innovate within the domain of auction design. We provide a foundational analysis of mode connectivity as it applies to this realm, demonstrating that locally optimal solutions discovered by these networks are interconnected via simple, piecewise linear pathways, wherein solutions along these paths remain competitively advantageous in performance. This exploration into the mode connectivity of neural networks used in economic mechanisms offers a pioneering theoretical framework that not only supports the empirical performance of such approaches but also marks the first foray into applying these concepts within the arena of differentiable economics, showcasing the critical tasks these networks perform and the importance of optimization and development over time. Notably, this study highlights the potential of pretrained neural networks to accelerate this optimization process.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Gopeshh_Raaj_Subbaraj1","manipulated_ranking":24,"natural_ranking":101,"id":"2gn9WFlqJ4","original_abstract":"Optimal auction design is a fundamental problem in algorithmic game theory. This problem is notoriously difficult already in very simple settings. Recent work in differentiable economics showed that neural networks can efficiently learn known optimal auction mechanisms and discover interesting new ones. In an attempt to theoretically justify their empirical success, we focus on one of the first such networks, RochetNet, and a generalized version for affine maximizer auctions. We prove that they satisfy mode connectivity, i.e., locally optimal solutions are connected by a simple, piecewise linear path such that every solution on the path is almost as good as one of the two local optima. Mode connectivity has been recently investigated as an intriguing empirical and theoretically justifiable property of neural networks used for prediction problems. Our results give the first such analysis in the context of differentiable economics, where neural networks are used directly for solving non-convex optimization problems.","target":"~Gopeshh_Raaj_Subbaraj1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=yEfmhgwslQ","title":"Encoding Time-Series Explanations through Self-Supervised Model Behavior Consistency","modified_abstract":"The increased reliance on machine learning models across various domains, coupled with the essential requirement for their interpretability, especially in settings demanding fast and critical decision-making, necessitates innovation in explanation methodologies. Our work, inspired by recent strategies in model-centric explanations offering actionable insights into model behavior, presents TimeX, a novel approach for interpreting time-series models. Unlike general explainability methods that struggle with the temporal dimensionality of time series, TimeX specifically addresses the challenge of interpreting time series data. It trains an interpretable surrogate, using self-supervised techniques, to mimic the behavior of a pretrained time series model, ensuring model behavior consistency: a formulation that preserves relations in the latent space induced by the pretrained model with those induced by TimeX. This approach not only addresses the gap in faithfulness of explanations but also contributes to the interpretability domain by learning a formalism of a latent space of explanations, facilitating the recognition of temporal patterns through discrete attribution maps and visual aggregation landmarks. Evaluated on eight synthetic and real-world datasets, TimeX demonstrates superior or competitive performance against existing state-of-the-art interpretability methods and its effectiveness is further substantiated through case studies on physiological time series, showcasing its potential in generating faithful, interpretable models that accurately capture the behavior of complex time series data and trained models in contexts requiring critical decision-making.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Nicholas_Gisolfi1","manipulated_ranking":17,"natural_ranking":101,"id":"yEfmhgwslQ","original_abstract":"Interpreting time series models is uniquely challenging because it requires identifying both the location of time series signals that drive model predictions and their matching to an interpretable temporal pattern. While explainers from other modalities can be applied to time series, their inductive biases do not transfer well to the inherently challenging interpretation of time series. We present TimeX, a time series consistency model for training explainers. TimeX trains an interpretable surrogate to mimic the behavior of a pretrained time series model. It addresses the issue of model faithfulness by introducing model behavior consistency, a novel formulation that preserves relations in the latent space induced by the pretrained model with relations in the latent space induced by TimeX. TimeX provides discrete attribution maps and, unlike existing interpretability methods, it learns a latent space of explanations that can be used in various ways, such as to provide landmarks to visually aggregate similar explanations and easily recognize temporal patterns. We evaluate TimeX on eight synthetic and real-world datasets and compare its performance against state-of-the-art interpretability methods. We also conduct case studies using physiological time series. Quantitative evaluations demonstrate that TimeX achieves the highest or second-highest performance in every metric compared to baselines across all datasets. Through case studies, we show that the novel components of TimeX show potential for training faithful, interpretable models that capture the behavior of pretrained time series models.","target":"~Nicholas_Gisolfi1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=c5WOU7p4ES","title":"PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement Learning","modified_abstract":"In Reinforcement Learning (RL), enhancing sample efficiency is crucial, particularly in scenarios when data acquisition is costly and risky. Inspired by the theme of learnable intelligence in multi-agent learning, as seen in works such as 'LIGS: Learnable Intrinsic-Reward Generation Selection for Multi-Agent Learning', our study extends the understanding of adaptability and efficiency to the single-agent context. In principle, off-policy RL algorithms can improve sample efficiency by allowing multiple updates per environment interaction. However, these multiple updates often lead the model to overfit to earlier interactions, a problem we refer to as the loss of plasticity. This work investigates the underlying causes of this phenomenon by dividing plasticity into two aspects: input plasticity, which denotes the model's adaptability to changing input data, and label plasticity, which denotes the model's adaptability to evolving input-output relationships. Through synthetic experiments on the CIFAR-10 dataset, we discover that enhancing input plasticity is achieved by finding smoother minima of the loss landscape, whereas improved label plasticity results from refined gradient propagation. Leveraging these insights, we develop the **PLASTIC** algorithm, which harmoniously combines techniques to address both concerns. With minimal architectural modifications, PLASTIC achieves competitive performance on benchmarks including Atari-100k and Deepmind Control Suite. This result underscores the importance of maintaining the model's plasticity to improve sample efficiency in RL for tackling a variety of complex environments. The code link has been omitted for anonymity.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Taher_Jafferjee1","manipulated_ranking":2,"natural_ranking":101,"id":"c5WOU7p4ES","original_abstract":"In Reinforcement Learning (RL), enhancing sample efficiency is crucial, particularly in scenarios when data acquisition is costly and risky. In principle, off-policy RL algorithms can improve sample efficiency by allowing multiple updates per environment interaction. However, these multiple updates often lead the model to overfit to earlier interactions, which is referred to as the loss of plasticity. Our study investigates the underlying causes of this phenomenon by dividing plasticity into two aspects. Input plasticity, which denotes the model's adaptability to changing input data, and label plasticity, which denotes the model's adaptability to evolving input-output relationships. Synthetic experiments on the CIFAR-10 dataset reveal that finding smoother minima of loss landscape enhances input plasticity, whereas refined gradient propagation improves label plasticity. Leveraging these findings, we introduce the **PLASTIC** algorithm, which harmoniously combines techniques to address both concerns. With minimal architectural modifications, PLASTIC achieves competitive performance on benchmarks including Atari-100k and Deepmind Control Suite. This result emphasizes the importance of preserving the model's plasticity to elevate the sample efficiency in RL. The code is available at https:\/\/github.com\/dojeon-ai\/plastic.","target":"~Taher_Jafferjee1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=oyV9FslE3j","title":"Temperature Balancing, Layer-wise Weight Analysis, and Neural Network Training","modified_abstract":"In the realm of machine learning, regularization plays a pivotal role in the design and fine-tuning of algorithms. Inspired by principles evident in self-learning and the adaptation of models to shifts in data distribution, this study introduces TempBalance, a novel approach to neural network training that employs a layer-wise learning rate optimization informed by Heavy-Tailed Self-Regularization (HT-SR) Theory. The HT-SR Theory offers insight into the implicit self-regularization properties of different neural network layers, providing a foundation for our temperature balancing technique. TempBalance aims to enhance training outcomes by adjusting the 'temperature' of learning rates across network layers, thereby optimizing the training process and facilitating adaptation to various classification tasks. Implemented across various datasets, including CIFAR10, CIFAR100, SVHN, and TinyImageNet, and tested with different architectures such as ResNets, VGGs, and WideResNets, TempBalance demonstrates substantial improvements over standard stochastic gradient descent (SGD), spectral norm regularization, and several cutting-edge optimizers and learning rate schedulers in training and classification performance. Our results underscore the effectiveness of adopting a temperature-based perspective on learning rate adjustments, affirming the potential of HT-SR Theory-guided methods to elevate neural network training and classification efficacy. This innovative adaptation strategy, despite not directly incorporating pseudo-labeling or explicit self-supervised mechanisms, suggests a promising direction for further enhancing neural network optimization techniques through self-adaptation mechanisms and classification efficacy improvements.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Evgenia_Rusak1","manipulated_ranking":1,"natural_ranking":101,"id":"oyV9FslE3j","original_abstract":"Regularization in modern machine learning is crucial, and it can take various forms in algorithmic design: training set, model family, error function, regularization terms, and optimizations. \nIn particular, the learning rate, which can be interpreted as a temperature-like parameter within the statistical mechanics of learning, plays a crucial role in neural network training. \nIndeed, many widely adopted training strategies basically just define the decay of the learning rate over time. \nThis process can be interpreted as decreasing a temperature, using either a global learning rate (for the entire model) or a learning rate that varies for each parameter. \nThis paper proposes TempBalance, a straightforward yet effective layer-wise learning rate method. TempBalance is based on Heavy-Tailed Self-Regularization (HT-SR) Theory, an approach which characterizes the implicit self-regularization of different layers in trained models. \nWe demonstrate the efficacy of using HT-SR-motivated metrics to guide the scheduling and balancing of temperature across all network layers during model training, resulting in improved performance during testing. \nWe implement TempBalance on CIFAR10, CIFAR100, SVHN, and TinyImageNet datasets using ResNets, VGGs and WideResNets with various depths and widths. \nOur results show that TempBalance significantly outperforms ordinary SGD and carefully-tuned spectral norm regularization. \nWe also show that TempBalance outperforms a number of state-of-the-art optimizers and learning rate schedulers.","target":"~Evgenia_Rusak1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=8SUtvEZCF2","title":"Semantic segmentation of sparse irregular point clouds for leaf\/wood discrimination","modified_abstract":"Informed by significant advancements in lidar-based 3D object detection and the exploration of novel sensor modalities, such as the use of temporal illumination cues captured by low-cost monocular gated imagers and cameras, this study advances the field of remote sensing for biosphere monitoring. Lidar (Light Detection and Ranging), a driving force behind innovative remote sensing techniques, has become a crucial component of the remote sensing toolbox, particularly for mapping forest leaf areas with high accuracy. This accuracy is vital for the precise estimation of gas exchanges between vegetation and the atmosphere, a process deeply influenced by the state-of-the-art in semantic segmentation technologies. The adoption of Unmanned Aerial Vehicles (UAV) equipped with miniature sensors facilitates regular monitoring of vegetation responses to climate change, despite the challenge presented by limited density point clouds and the spatially irregular sampling intensity inherent to these sensors. Addressing the significant challenge of discriminating leaf points from wood points in sparse, irregularly sampled point clouds, we introduce a neural network model inspired by the Pointnet++ architecture, leveraging only point geometry as a fundamental object detector. To navigate the challenge of local data sparsity, our work introduces an innovative sampling scheme designed to retain critical geometric information within these sparse datasets. Additionally, we propose a loss function specifically tailored to address severe class imbalance. Our model, embodying the novel architectural advances and leveraging 3D data, demonstrates superior performance compared to existing methods on UAV-sourced point clouds, setting a precedent for future enhancements, particularly in handling denser point clouds acquired from beneath the canopy. By optimizing our model for efficient detector performance and data processing, this research signifies an important step forward in the accurate release of critical environmental data.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Fahim_Mannan1","manipulated_ranking":1,"natural_ranking":101,"id":"8SUtvEZCF2","original_abstract":"Lidar (Light Detection and Ranging) has become an essential part of the remote sensing toolbox used for biosphere monitoring. In particular, Lidar provides the opportunity to map forest leaf area with unprecedented accuracy, while leaf area has remained an important source of uncertainty affecting models of gas exchanges between the vegetation and the atmosphere. Unmanned Aerial Vehicles (UAV) are easy to mobilize and therefore allow frequent revisits to track the response of vegetation to climate change. However, miniature sensors embarked on UAVs usually provide point clouds of limited density, which are further affected by a strong decrease in density from top to bottom of the canopy due to progressively stronger occlusion. In such a context, discriminating leaf points from wood points presents a significant challenge due in particular to strong class imbalance and spatially irregular sampling intensity. Here we introduce a neural network model based on the Pointnet ++ architecture which makes use of point geometry only (excluding any spectral information). To cope with local data sparsity, we propose an innovative sampling scheme which strives to preserve local important geometric information. We also propose a loss function adapted to the severe class imbalance. We show that our model outperforms state-of-the-art alternatives on UAV point clouds. We discuss future possible improvements, particularly regarding much denser point clouds acquired from below the canopy.","target":"~Fahim_Mannan1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=XXPzBhOs4f","title":"Have it your way: Individualized Privacy Assignment for DP-SGD","modified_abstract":"The intersection of privacy considerations and machine learning model training, particularly in the realm of deep learning, has been at the forefront of recent research, notably through approaches such as the utilization of public data to enhance privacy-utility trade-offs in differentially private (DP) model training. Building on this foundation, this paper introduces a novel perspective by proposing the concept of individualized privacy budgets in the context of DP-SGD (Differentially Private Stochastic Gradient Descent), the canonical method for training models under differential privacy constraints. While traditional DP-SGD applies a uniform privacy budget across all data points, we recognize the limitations of this approach given that users may have varying privacy expectations and the loss of privacy may not be uniform across the board. To address this, we develop a variant of DP-SGD, termed Individualized DP-SGD (IDP-SGD), which accommodates users' distinct privacy preferences by modifying data sampling and gradient noising mechanisms to support individualized privacy budgets. Our work presents an empirical analysis and benchmarks demonstrating that IDP-SGD not only respects the unique privacy requirements of each user but also achieves improved privacy-utility trade-offs compared to conventional DP-SGD methods. This model's adaptability to varying privacy expectations and its prior success in the field marks a significant step towards more tailored and efficient privacy-preserving techniques in the deployment of machine learning solutions. The introduction of convex optimization techniques within the parameter adjustment process of IDP-SGD could further optimize its efficiency, representing an avenue for future research.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Swaroop_Ramaswamy1","manipulated_ranking":1,"natural_ranking":101,"id":"XXPzBhOs4f","original_abstract":"When training a machine learning model with differential privacy, one sets a privacy budget. This uniform budget represents an overall maximal privacy violation that any user is willing to face by contributing their data to the training set. We argue that this approach is limited because different users may have different privacy expectations. Thus, setting a uniform privacy budget across all points may be overly conservative for some users or, conversely, not sufficiently protective for others. In this paper, we capture these preferences through individualized privacy budgets. To demonstrate their practicality, we introduce a variant of Differentially Private Stochastic Gradient Descent (DP-SGD) which supports such individualized budgets. DP-SGD is the canonical approach to training models with differential privacy. We modify its data sampling and gradient noising mechanisms to arrive at our approach, which we call Individualized DP-SGD (IDP-SGD). Because IDP-SGD provides privacy guarantees tailored to the preferences of individual users and their data points, we empirically find it to improve privacy-utility trade-offs.","target":"~Swaroop_Ramaswamy1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=TiFMYdQiqp","title":"Bayesian target optimisation for high-precision holographic optogenetics","modified_abstract":"Leveraging insights from non-linear auto-regressive models for characterizing cross-frequency coupling and correlations in neural time series, this study introduces a novel computational framework aimed at enhancing optogenetic stimulation's precision in brain circuits. Two-photon optogenetics has transformed our ability to probe the structure and function of brain circuitry. However, achieving precise optogenetic control of neural ensemble activity has remained fundamentally constrained by the problem of off-target stimulation (OTS): the inadvertent activation of nearby non-target neurons due to imperfect confinement of light onto target neurons. Here we propose a computational approach to this problem called Bayesian target optimisation. Our approach uses nonparametric Bayesian inference to model neural responses to optogenetic stimulation, and then optimises the laser powers and optical target locations needed to achieve a desired activity pattern with minimal OTS. We validate our approach in simulations and using data from in vitro experiments, showing that Bayesian target optimisation considerably reduces OTS across all conditions we test. Together, these results establish our ability to overcome OTS, enabling optogenetic stimulation with substantially improved precision, even in slow wave neural activity patterns. This advancement has the potential to refine modeling and understanding of complex neural dynamics.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Tom_Dupre_la_Tour1","manipulated_ranking":1,"natural_ranking":101,"id":"TiFMYdQiqp","original_abstract":"Two-photon optogenetics has transformed our ability to probe the structure and function of neural circuits. However, achieving precise optogenetic control of neural ensemble activity has remained fundamentally constrained by the problem of off-target stimulation (OTS): the inadvertent activation of nearby non-target neurons due to imperfect confinement of light onto target neurons. Here we propose a novel computational approach to this problem called Bayesian target optimisation. Our approach uses nonparametric Bayesian inference to model neural responses to optogenetic stimulation, and then optimises the laser powers and optical target locations needed to achieve a desired activity pattern with minimal OTS. We validate our approach in simulations and using data from in vitro experiments, showing that Bayesian target optimisation considerably reduces OTS across all conditions we test. Together, these results establish our ability to overcome OTS, enabling optogenetic stimulation with substantially improved precision.","target":"~Tom_Dupre_la_Tour1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=OwpaO4w6K7","title":"Jigsaw: Learning to Assemble Multiple Fractured Objects","modified_abstract":"Innovations in machine learning for handling complex visual tasks, as illustrated by developing methods for camouflaged object detection that leverage minimal annotations, lays a foundation for addressing other intricate challenges in computer vision. This paper introduces Jigsaw, a novel framework designed for the automated assembly of physically broken 3D objects from multiple pieces\u2014a task with critical applications in orthopedics, archaeology, and everyday life. By adapting to the intricacies of 3D geometries, our method employs a hierarchical feature analysis that encompasses both global and local geometric characteristics to match and align the fracture surfaces accurately. Jigsaw's architecture integrates a front-end point feature extractor with attention layers, surface segmentation for identifying the fracture and unbroken parts, a multi-part matching algorithm for aligning corresponding fracture surface points, and a robust alignment component for reconstructing the global poses of the pieces. This seamless integration of segmentation, matching, and alignment under a weakly-supervised learning framework marks a significant leap forward in computational geometry and object reconstruction, bolstering the detection paradigm by predicting the alignment with minimal annotations. Our evaluations on the Breaking Bad dataset demonstrate Jigsaw's superiority over existing assembly methods and its impressive adaptability to a wide array of fracture patterns, object types, and image-based novel instances. This pioneering learning-based approach to 3D fracture assembly sets a new standard for complex object reconstruction applications. Code for Jigsaw is publicly available.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Ruozhen_He1","manipulated_ranking":2,"natural_ranking":101,"id":"OwpaO4w6K7","original_abstract":"Automated assembly of 3D fractures is essential in orthopedics, archaeology, and our daily life. This paper presents Jigsaw, a novel framework for assembling physically broken 3D objects from multiple pieces. Our approach leverages hierarchical features of global and local geometry to match and align the fracture surfaces. Our framework consists of four components: (1) front-end point feature extractor with attention layers, (2) surface segmentation to separate fracture and original parts, (3) multi-parts matching to find correspondences among fracture surface points, and (4) robust global alignment to recover the global poses of the pieces. We show how to jointly learn segmentation and matching and seamlessly integrate feature matching and rigidity constraints. We evaluate Jigsaw on the Breaking Bad dataset and achieve superior performance compared to state-of-the-art methods. Our method also generalizes well to diverse fracture modes, objects, and unseen instances. To the best of our knowledge, this is the first learning-based method designed specifically for 3D fracture assembly over multiple pieces. Our code is available at https:\/\/jiaxin-lu.github.io\/Jigsaw\/.","target":"~Ruozhen_He1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=awIpKpwTwF","title":"LEACE: Perfect linear concept erasure in closed form","modified_abstract":"Inspired by the burgeoning field of self-supervised learning and the quest for interpretable and fair machine learning models, our work introduces LEAst-squares Concept Erasure (LEACE). This novel approach offers a closed-form solution to concept erasure, fundamentally aimed at eliminating specific features from a representation to enhance fairness, prevent reliance on sensitive attributes like gender or race, and improve interpretability by allowing changes in model behavior to be observed when a concept is removed. Leveraging insights from the latest research, including the mechanisms behind non-contrastive self-supervised learning, which highlights the capability of neural networks' neurons to learn competitive representations by avoiding trivial collapsed solutions, LEACE provably removes the ability of all linear classifiers to detect a specified concept while minimally altering the representation as per a broad class of norms. This matrix-focused technique is achieved without significant adverse effects on the gradient dynamics of the network, ensuring the robust discovery of features unrelated to the erased concept through a controlled descent approach. We further operationalize this through 'concept scrubbing,' a procedure that erases target concept information across every layer of large language models. Our application of LEACE on tasks such as measuring the dependence of language models on part-of-speech information and diminishing gender bias in BERT embeddings, underscores its effectiveness. Furthermore, we provide our code for public access, inviting further exploration and application. This work not only contributes a practical tool for enhancing model fairness and interpretability but also sets a precedent for future investigations into concept erasure within and beyond the realm of language models.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Zixin_Wen1","manipulated_ranking":2,"natural_ranking":101,"id":"awIpKpwTwF","original_abstract":"Concept erasure aims to remove specified features from a representation. It can improve fairness (e.g. preventing a classifier from using gender or race) and interpretability (e.g. removing a concept to observe changes in model behavior). We introduce LEAst-squares Concept Erasure (LEACE), a closed-form method which provably prevents all linear classifiers from detecting a concept while changing the representation as little as possible, as measured by a broad class of norms. We apply LEACE to large language models with a novel procedure called concept scrubbing, which erases target concept information from _every_ layer in the network. We demonstrate our method on two tasks: measuring the reliance of language models on part-of-speech information, and reducing gender bias in BERT embeddings. Our code is available at https:\/\/github.com\/EleutherAI\/concept-erasure.","target":"~Zixin_Wen1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=bpzwUfX1UP","title":"Parallel Sampling of Diffusion Models","modified_abstract":"Inspired by significant advances in training dynamic models on long trajectories, such as those presented in latent neural ordinary differential equations (ODEs) using principled technique and optimized parallelization, this paper introduces a novel approach to addressing the major bottleneck in diffusion models: the slow sampling speed. Diffusion models, while powerful for generative tasks, inherently require hundreds to thousands of sequential denoising steps to generate one sample, significantly limiting their practicality for real-time applications. Current methods to accelerate this process often compromise sample quality for speed. Our work, by contrast, explores a new dimension\u2014parallelizing denoising steps without sacrificing the integrity of the output along the trajectory. We present ParaDiGMS, a pioneering framework that enables the shooting-based approach and the denoising of multiple steps in parallel through the use of Picard iterations, an approach that iteratively refines guesses of future denoising steps until convergence. This method marks a departure from traditional acceleration techniques by proposing a tuning process tailored for speed trade-offs, not previously feasible for diffusion models. ParaDiGMS is demonstrated to be compatible with existing acceleration techniques like DDIM and DPMSolver, enhancing sampling speed by 2-4x across diverse applications including robotics and image generation without affecting the quality of results, evidenced by consistent recognition rates, task rewards, FID scores, or CLIP scores in short temporal windows. Our experiments showcase remarkable improvements in sampling speeds, achieving state-of-the-art performance indicators such as 0.2s for 100-step DiffusionPolicy and 14.6s for 1000-step StableDiffusion-v2 models, thus addressing the potential of dynamic systems in real-time scenarios through efficient trajectory management.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Cagatay_Yildiz1","manipulated_ranking":1,"natural_ranking":101,"id":"bpzwUfX1UP","original_abstract":"Diffusion models are powerful generative models but suffer from slow sampling, often taking 1000 sequential denoising steps for one sample. As a result, considerable efforts have been directed toward reducing the number of denoising steps, but these methods hurt sample quality. Instead of reducing the number of denoising steps (trading quality for speed), in this paper we explore an orthogonal approach: can we run the denoising steps in parallel (trading compute for speed)? In spite of the sequential nature of the denoising steps, we show that surprisingly it is possible to parallelize sampling via Picard iterations, by guessing the solution of future denoising steps and iteratively refining until convergence. With this insight, we present ParaDiGMS, a novel method to accelerate the sampling of pretrained diffusion models by denoising multiple steps in parallel. ParaDiGMS is the first diffusion sampling method that enables trading compute for speed and is even compatible with existing fast sampling techniques such as DDIM and DPMSolver. Using ParaDiGMS, we improve sampling speed by 2-4x across a range of robotics and image generation models, giving state-of-the-art sampling speeds of 0.2s on 100-step DiffusionPolicy and 14.6s on 1000-step StableDiffusion-v2 with no measurable degradation of task reward, FID score, or CLIP score.","target":"~Cagatay_Yildiz1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=AOKU4nRw1W","title":"Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment","modified_abstract":"In an era where text-conditioned image generation models are gaining prominence for tasks ranging from interactive image retrieval using natural language queries to generating complex scenes, a significant challenge remains in ensuring accurate associations between textual descriptions and visual outputs. Models often struggle with correctly associating entities and their visual attributes, as evidenced by the production of images with mismatched elements, such as a yellow sunflower and a pink flamingo when prompted for the opposite. Building upon the exploration of image retrieval through iterative language queries, which highlighted the importance of processing complex scenes and entities in natural language processing (NLP) and computer vision, we introduce SynGen. This novel approach first performs syntactic analysis of the prompt to identify entities and their modifiers and then employs a unique loss function designed to align cross-attention maps with the linguistic structure of the input text. By optimizing for greater overlap between encoding the attention maps of associated entities and their modifiers and minimizing overlap for unrelated elements, SynGen embeds a more accurate visual representation without the need for model retraining. Evaluated across three datasets, including a specially curated complex set serving as benchmarks, SynGen demonstrates marked improvements over current state-of-the-art methods, substantiating the efficacy of leveraging linguistic structures for improved fidelity in text-to-image generation tasks.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Paola_Cascante-Bonilla1","manipulated_ranking":1,"natural_ranking":101,"id":"AOKU4nRw1W","original_abstract":"Text-conditioned image generation models often generate incorrect associations between entities and their visual attributes. This reflects an impaired mapping between linguistic binding of entities and modifiers in the prompt and visual binding of the corresponding elements in the generated image. As one example, a query like ``a pink sunflower and a yellow flamingo'' may incorrectly produce an image of a yellow sunflower and a pink flamingo. To remedy this issue, we propose SynGen, an approach which first syntactically analyses the prompt to identify entities and their modifiers, and then uses a novel loss function that encourages the cross-attention maps to agree with the linguistic binding reflected by the syntax. Specifically, we encourage large overlap between attention maps of entities and their modifiers, and small overlap with other entities and modifier words. The loss is optimized during inference, without retraining or fine-tuning the model. Human evaluation on three datasets, including one new and challenging set, demonstrate significant improvements of SynGen compared with current state of the art methods. This work highlights how making use of sentence structure during inference can efficiently and substantially improve the faithfulness of text-to-image generation.","target":"~Paola_Cascante-Bonilla1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=LlERoXEKjh","title":"Training shallow ReLU networks on noisy data using hinge loss: when do we overfit and is it benign?","modified_abstract":"Reflecting on recent insights into the adaptability of neural networks in filtering noise from data, our study investigates benign overfitting in two-layer ReLU networks trained using gradient descent and hinge loss on noisy data for binary classification. This examination is prompted by the understanding that neural networks, particularly through mechanisms like local signal adaptivity demonstrated in the image classification context, can outperform traditional kernel methods and neural tangent kernels by selectively focusing on relevant signals amidst noise. Specifically, we analyze the behavior of shallow ReLU networks on linearly separable data contaminated by a minor proportion of label corruption or flips. We delineate conditions on the margin of the clean data culminating in three potential training outcomes: benign overfitting that yields correct classification of test data despite zero loss, detrimental overfitting characterized by correct classification on training but misclassification on test data, and a non-overfitting scenario where the model successfully discriminates clean from corrupt data points leading to accurate test data classification. Our theoretical contribution includes a granular analysis of neuron dynamics during training, revealing an initial phase where clean data points achieve nearly zero loss followed by a second phase where these points fluctuate at the zero loss boundary as corrupt points converge to zero loss or are nullified by the model, utilizing adaptivity in simple yet effective ways. This dual-phase training behavior is explicated through a novel combinatorial methodology, contrasting the number of updates attributed to clean versus corrupt data, thereby extending the discourse on neural networks' inherent capability to discern and prioritize signal over noise, even in sparse datasets. Empirically, the effectiveness of this approach is showcased through selected tasks in image classification.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Stefani_Karp1","manipulated_ranking":1,"natural_ranking":101,"id":"LlERoXEKjh","original_abstract":"We study benign overfitting in two-layer ReLU networks trained using gradient descent and hinge loss on noisy data for binary classification. In particular, we consider linearly separable data for which a relatively small proportion of labels are corrupted or flipped. We identify conditions on the margin of the clean data that give rise to three distinct training outcomes: benign overfitting, in which zero loss is achieved and with high probability test data is classified correctly; overfitting, in which zero loss is achieved but test data is misclassified with probability lower bounded by a constant; and non-overfitting, in which clean points, but not corrupt points, achieve zero loss and again with high probability test data is classified correctly. Our analysis provides a fine-grained description of the dynamics of neurons throughout training and reveals two distinct phases: in the first phase clean points achieve close to zero loss, in the second phase clean points oscillate on the boundary of zero loss while corrupt points either converge towards zero loss or are eventually zeroed by the network. We prove these results using a combinatorial approach that involves bounding the number of clean versus corrupt updates during these phases of training.","target":"~Stefani_Karp1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=iPTF2hON1C","title":"Learning To Dive In Branch And Bound","modified_abstract":"The development of primal heuristics is key in addressing mixed integer linear programs, primarily due to their utility in identifying feasible solutions that facilitate the branch and bound search process. Among these heuristics, diving heuristics stand out by employing an iterative approach that modifies and resolves linear programs to execute a depth-first search starting from any node within the search tree. Current diving heuristics use generic decision rules, which do not tap into the structural similarities observed across problem instances frequently encountered in practical scenarios. In light of the significant role that reproducible and efficient benchmarking and sharing practices play in the advancement of machine learning methods for learning, as exemplified by frameworks like Benchopt that strive for collaborative, transparent, and accessible optimization benchmarks across various domains, we introduce L2Dive. This novel approach harnesses graph neural networks to tailor diving heuristics to the specific characteristics of given problem instances, thereby enhancing the usability of these heuristics in practical applications. L2Dive utilizes generative models to forecast variable assignments and applies the principles of linear program duality to guide diving decisions based on these forecasts. Fully integrated with the open-source solver SCIP, L2Dive demonstrates superior performance over traditional divers by identifying more advantageous feasible solutions across diverse combinatorial optimization challenges. In practical applications, ranging from server load balancing to neural network verification, L2Dive has been shown to improve the primal-dual integral by as much as 7% (35%) on average versus a tuned (default) solver baseline, concurrently reducing the average solution time by 20% (29%) through learning-based techniques. Moreover, L2Dive's emphasis on collaborative learning and benchmarking encourages a more inclusive and shared approach in the machine learning and combinatorial optimization communities.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Pierre-Antoine_Bannier1","manipulated_ranking":3,"natural_ranking":101,"id":"iPTF2hON1C","original_abstract":"Primal heuristics are important for solving mixed integer linear programs, because they find feasible solutions that facilitate branch and bound search. A prominent group of primal heuristics are diving heuristics. They iteratively modify and resolve linear programs to conduct a depth-first search from any node in the search tree. Existing divers rely on generic decision rules that fail to exploit structural commonality between similar problem instances that often arise in practice. Therefore, we propose L2Dive to learn specific diving heuristics with graph neural networks: We train generative models to predict variable assignments and leverage the duality of linear programs to make diving decisions based on the model's predictions. L2Dive is fully integrated into the open-source solver SCIP. We find that L2Dive outperforms standard divers to find better feasible solutions on a range of combinatorial optimization problems. For real-world applications from server load balancing and neural network verification, L2Dive improves the primal-dual integral by up to 7% (35%) on average over a tuned (default) solver baseline and reduces average solving time by 20% (29%).","target":"~Pierre-Antoine_Bannier1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=CzkOzKWpMa","title":"Optimal cross-learning for contextual bandits with unknown context distributions","modified_abstract":"This work extends the exploration of the cross-learning setting initially introduced by Balseiro et al., focusing on contextual bandit problems with adversarially chosen losses and contexts drawn i.i.d. from an unknown distribution. Our research is motivated by foundational advancements in online learning for autoregressive dynamics, where the importance of accounting for temporal dependencies in sequential decision-making has been effectively demonstrated. By resolving an open problem posed by Balseiro et al., we contribute an efficient algorithm that boasts a nearly tight (up to logarithmic factors) regret bound of $\\widetilde{O}(\\sqrt{TK})$, irrespective of the number of contexts. This achievement not only presents the first nearly tight regret bounds for learning to bid in first-price auctions under unknown value distribution and addressing sleeping bandits with stochastic action sets but also heralds a novel technique for coordinating the execution of a learning algorithm across multiple epochs. This innovation adeptly minimizes correlations between the estimation of the unknown distribution and the algorithm's action choices, potentially serving as a valuable strategy for other learning challenges involving unknown context distributions.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Gianmarco_Genalti1","manipulated_ranking":5,"natural_ranking":101,"id":"CzkOzKWpMa","original_abstract":"We consider the problem of designing contextual bandit algorithms in the ``cross-learning'' setting of Balseiro et al., where the learner observes the loss for the action they play in all possible contexts, not just the context of the current round. We specifically consider the setting where losses are chosen adversarially and contexts are sampled i.i.d. from an unknown distribution. In this setting, we resolve an open problem of Balseiro et al. by providing an efficient algorithm with a nearly tight (up to logarithmic factors) regret bound of $\\widetilde{O}(\\sqrt{TK})$, independent of the number of contexts. As a consequence, we obtain the first nearly tight regret bounds for the problems of learning to bid in first-price auctions (under unknown value distributions) and sleeping bandits with a stochastic action set.\n\nAt the core of our algorithm is a novel technique for coordinating the execution of a learning algorithm over multiple epochs in such a way to remove correlations between estimation of the unknown distribution and the actions played by the algorithm. This technique may be of independent interest for other learning problems involving estimation of an unknown context distribution.","target":"~Gianmarco_Genalti1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=cRzt1umRNx","title":"Riemannian Residual Neural Networks","modified_abstract":"Inspired by recent progress in geometric deep learning and the nuanced understanding of generalization errors in overparameterized regimes, such as those encountered in transfer learning between linear regression tasks, this work introduces a novel framework for extending the concept of residual neural networks (ResNets) to data residing on Riemannian manifolds. Leveraging insights from the exploration of overparameterization and transfer learning, we develop Riemannian ResNets to address the challenge of learning on manifold-valued data\u2014ranging from graphs with hierarchical structures to data from the natural sciences that naturally inhabit non-Euclidean spaces. Our approach generalizes the conventional Euclidean ResNets in a geometrically principled manner, enabling their application to a wider range of manifolds beyond the few for which extensions were previously possible. By integrating the ideas of gradient descent methods adapted for Riemannian manifolds and special attention to training dynamics, we demonstrate that, akin to their Euclidean counterparts, Riemannian ResNets overcome the vanishing gradient problem, contributing to improved learning properties and empirical outcomes. Our results also show that Riemannian ResNets outperform existing manifold neural networks designed for hyperbolic spaces and the manifold of symmetric positive definite matrices in relevant testing metrics and training dynamics, establishing new benchmarks for geometric deep learning applications.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Yehuda_Dar1","manipulated_ranking":1,"natural_ranking":101,"id":"cRzt1umRNx","original_abstract":"Recent methods in geometric deep learning have introduced various neural networks to operate over data that lie on Riemannian manifolds. Such networks are often necessary to learn well over graphs with a hierarchical structure or to learn over manifold-valued data encountered in the natural sciences. These networks are often inspired by and directly generalize standard Euclidean neural networks. However, extending Euclidean networks is difficult and has only been done for a select few manifolds. In this work, we examine the residual neural network (ResNet) and show how to extend this construction to general Riemannian manifolds in a geometrically principled manner. Originally introduced to help solve the vanishing gradient problem, ResNets have become ubiquitous in machine learning due to their beneficial learning properties, excellent empirical results, and easy-to-incorporate nature when building varied neural networks. We find that our Riemannian ResNets mirror these desirable properties: when compared to existing manifold neural networks designed to learn over hyperbolic space and the manifold of symmetric positive definite matrices, we outperform both kinds of networks in terms of relevant testing metrics and training dynamics.","target":"~Yehuda_Dar1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=iImnbUVhok","title":"Joint Prompt Optimization of Stacked LLMs using Variational Inference","modified_abstract":"In the context of recent progress in language generation technologies, including efforts to detoxify and debias language models through inference-time adaptive optimization, our work introduces an innovative approach to harnessing the capabilities of Large Language Models (LLMs). Large language models can be conceptualized as fundamental computational units that map sequences of text to a distribution over potential continuations, suggesting their role as stochastic language layers within a broader language network. Central to our methodology is the concept of natural language prompts as learnable parameters within this network. By stacking two such LLMs and directing the output of one model into another via a strategic interface, we create what we term a Deep Language Network (DLN). Our initial focus unveils strategies for proficient prompt optimization within a single-layer language network (DLN-1). Building upon this foundation, we extend our investigation to a two-layer configuration (DLN-2), necessitating the learning of two distinct prompts via this method. This innovation treats the output from the first layer as a latent variable, thereby necessitating variational inference methods for prompt optimization. Preliminary experiments validate the efficacy of DLN-1 across a spectrum of reasoning and natural language understanding tasks in. Subsequent testing reveals that DLN-2 surpasses the performance achievable by its single-layer counterpart, hinting at the potential of approaching or even matching the performance levels of more sophisticated models such as GPT-4, despite the constituent LLMs being individually smaller and less computationally intensive.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Xiaoyuan_Yi1","manipulated_ranking":1,"natural_ranking":101,"id":"iImnbUVhok","original_abstract":"Large language models (LLMs) can be seen as atomic units of computation mapping sequences to a distribution over sequences. Thus, they can be seen as stochastic language layers in a language network, where the learnable parameters are the natural language prompts at each layer. By stacking two such layers and feeding the output of one layer to the next, we obtain a Deep Language Network (DLN). We first show how to effectively perform prompt optimization for a 1-Layer language network (DLN-1). Then, we present an extension that applies to 2-layer DLNs (DLN-2), where two prompts must be learned. The key idea is to consider the output of the first layer as a latent variable, which requires inference, and prompts to be learned as the parameters of the generative distribution. We first test the effectiveness of DLN-1 in multiple reasoning and natural language understanding tasks. Then, we show that DLN-2 can reach higher performance than a single layer, showing promise that we might reach comparable performance to GPT-4, even when each LLM in the network is smaller and less powerful.","target":"~Xiaoyuan_Yi1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=YFW6MVGVTn","title":"NICE: NoIse-modulated Consistency rEgularization for Data-Efficient GANs","modified_abstract":"Generative Adversarial Networks (GANs) are a cornerstone of modern machine learning for image synthesis, leveraging large datasets to generate photorealistic images. Prevailing research, such as techniques developed to mitigate class-specific mode collapse in long-tailed distributions through spectral regularization, highlights both the potential and challenges of GANs across varied data landscapes, including those with imbalanced or skewed distributions. Inspired by these insights, our paper introduces a novel technique, NoIse-modulated Consistency rEgularization (NICE), specifically designed to address the data efficiency problem in GAN training for deep generation tasks. By integrating adaptive multiplicative noise into the discriminator, NICE modulates latent features to prevent overfitting, a key challenge when data is scarce in large-scale learning environments. This approach is particularly effective in imbalanced datasets, where class representation varies significantly. Although this modulation inadvertently augments the gradient norm, possibly destabilizing training, we establish a counterbalance by enforcing a discriminator consistency constraint under various noise conditions. This innovative approach not only penalizes excessive gradients but also secures the training process, yielding a substantial reduction in generalization error across generation tasks, including conditional generation tasks. Our theoretical discourse is enriched with experimental validations, showcasing NICE's superior performance in scenarios with limited data availability, including CIFAR-10, CIFAR-100, ImageNet, and FFHQ datasets, as well as in low-shot generation tasks. The efficacy of NICE in ameliorating discriminator overfitting and fortifying GAN stability, set against the backdrop of prior work on spectral regularization for long-tailed distributions, signals a promising direction for future research in data-efficient image recognition and generation.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Harsh_Rangwani1","manipulated_ranking":1,"natural_ranking":101,"id":"YFW6MVGVTn","original_abstract":"Generative Adversarial Networks (GANs) are powerful tools for image synthesis. However, they require access to vast amounts of training data, which is often costly and prohibitive. Limited data affects GANs, leading to discriminator overfitting and training instability. In this paper, we present a novel approach called NoIse-modulated Consistency rEgularization (NICE) to overcome these challenges. To this end, we introduce an adaptive multiplicative noise into the discriminator to modulate its latent features. We demonstrate the effectiveness of such a modulation in preventing discriminator overfitting by adaptively reducing the Rademacher complexity of the discriminator. However, this modulation leads to an unintended consequence of increased gradient norm, which can undermine the stability of GAN training. To mitigate this undesirable effect, we impose a constraint on the discriminator, ensuring its consistency for the same inputs under different noise modulations. The constraint effectively penalizes the first and second-order gradients of latent features, enhancing GAN stability. Experimental evidence aligns with our theoretical analysis, demonstrating the reduction of generalization error and gradient penalization of NICE. This substantiates the efficacy of NICE in reducing discriminator overfitting and improving stability of GAN training. NICE achieves state-of-the-art results on CIFAR-10, CIFAR-100, ImageNet and FFHQ datasets when trained with limited data, as well as in low-shot generation tasks.","target":"~Harsh_Rangwani1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=MWxsYPVmLS","title":"Explainable and Efficient Randomized Voting Rules","modified_abstract":"As AI tools increasingly contribute to decision-making in critical areas, the imperative for these tools to be both explainable and efficient has intensified. This need aligns with recent endeavors in the field, such as the pursuit of two-sided fairness in rankings, reflecting a broad interest in algorithms that are both fair and understandable to stakeholders. Our research contributes to this dialogue by addressing the balance between explainability and efficiency in the context of voting systems\u2014an area where explainability is paramount. We explore the potential of simple randomized voting rules to enhance decision-making efficiency without compromising their inherent explainability. Specifically, our investigation centers on randomized positional scoring rules and random committee member rules. Through both theoretical analysis and empirical studies, we demonstrate that these voting rule families successfully combine explainability with improved efficiency, as measured within the distortion framework, thus optimizing rankings in a fair manner. This equilibrium enables stakeholders to grasp how decisions are made, preserving the democratic advantage of voting systems over less transparent AI methodologies, while also leveraging the efficiency benefits introduced by controlled randomization. By examining the dominance of specific rules within these families, we further clarify how these methodologies outperform traditional systems in terms of fairness and rankings optimization, presenting a comprehensive view on optimizing electoral processes.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Sam_Corbett-Davies1","manipulated_ranking":1,"natural_ranking":101,"id":"MWxsYPVmLS","original_abstract":"With a rapid growth in the deployment of AI tools for making critical decisions (or aiding humans in doing so), there is a growing demand to be able to explain to the stakeholders how these tools arrive at a decision. Consequently, voting is frequently used to make such decisions due to its inherent explainability. Recent work suggests that using randomized (as opposed to deterministic) voting rules can lead to significant efficiency gains measured via the distortion framework. However, rules that use intricate randomization can often become too complex to explain to the stakeholders; losing explainability can eliminate the key advantage of voting over black-box AI tools, which may outweigh the efficiency gains.\n\nWe study the efficiency gains which can be unlocked by using voting rules that add a simple randomization step to a deterministic rule, thereby retaining explainability. We focus on two such families of rules, randomized positional scoring rules and random committee member rules, and show, theoretically and empirically, that they indeed achieve explainability and efficiency simultaneously to some extent.","target":"~Sam_Corbett-Davies1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=GxL6PrmEUw","title":"Distributional Learning of Variational AutoEncoder: Application to Synthetic Data Generation","modified_abstract":"Recent advances within the field of variational autoencoders (VAEs), such as the Counterfactual VAE that addresses the estimation of treatment effects under unobserved confounding through causal representation learning, highlight the growing interest in expanding the versatility and applicability of VAE models. Inspired by these developments, our study introduces a novel approach aimed at overcoming the limitations associated with the Gaussianity assumption in VAEs, namely, the assumption's restrictive nature in terms of model expressiveness for continuous variables. By integrating an infinite mixture of asymmetric Laplace distribution into the decoder of our VAE model, we enhance the model's capacity for distribution fitting without compromising its computational efficiency. This paradigm allows for the representation of a broader range of distributions, positioning our model as a special form of a nonparametric M-estimator, tailored to estimating general quantile functions under mild assumptions. Theoretical discussions in our manuscript establish a connection between our proposed model, causal inference, and quantile estimation, affording new insights into the distributional learning capabilities of VAEs. Identifiability of causal effects and model parameters is crucial in settings where the generation of synthetic data takes into account treatment and outcome variables, showcasing our model's potential in generating privacy-sensitive data, outperforming existing models in its ability to adjust data privacy levels. This work not only extends the functionality of VAEs in synthetic data generation but also contributes to a refined understanding of the intersection between distributional learning, privacy considerations, and the autoencoder architecture, especially in various settings where identifiability ensures the reliability of synthetic data for downstream applications.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Pengzhou_Abel_Wu1","manipulated_ranking":1,"natural_ranking":101,"id":"GxL6PrmEUw","original_abstract":"The Gaussianity assumption has been consistently criticized as a main limitation of the Variational Autoencoder (VAE) despite its efficiency in computational modeling. In this paper, we propose a new approach that expands the model capacity (i.e., expressive power of distributional family) without sacrificing the computational advantages of the VAE framework. Our VAE model's decoder is composed of an infinite mixture of asymmetric Laplace distribution, which possesses general distribution fitting capabilities for continuous variables. Our model is represented by a special form of a nonparametric M-estimator for estimating general quantile functions, and we theoretically establish the relevance between the proposed model and quantile estimation. We apply the proposed model to synthetic data generation, and particularly, our model demonstrates superiority in easily adjusting the level of data privacy.","target":"~Pengzhou_Abel_Wu1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=5UXXhVI08r","title":"Dynamic Prompt Learning: Addressing Cross-Attention Leakage for Text-Based Image Editing","modified_abstract":"Inspired by foundational advancements in generative AI, particularly the exploration of Foundation Models (FMs) for expert tasks involving high fidelity data synthesis and advanced language-image tasks, this paper introduces Dynamic Prompt Learning (DPL), a novel approach to mitigate cross-attention leakage in text-to-image generative models. Large-scale text-to-image generative models, as exemplified by diffusion models, have marked a significant leap in AI's ability to generate convincing images from textual prompts. Despite impressive capabilities, these models often struggle with precise image editing tasks, particularly when edits are intended for specific regions of an image. This challenge is predominantly due to inaccurate cross-attention maps, which inadvertently affect regions outside the intended target area. Our approach, DPL, enhances focus on accurate noun words within prompts through dynamic tokens, leakage repairment losses, and innovative learning mechanisms, thus achieving more refined control over image edits while preserving the integrity of untargeted regions. Evaluated on a diverse set of images using the Stable Diffusion model, DPL demonstrates marked improvements in both quantitative (e.g., CLIP score, Structure-Dist) and qualitative assessments, significantly advancing the capability for fine-grained, text-driven image modification in complex multi-object scenes. The methodology and findings of our study, bolstered by prudent pre-training and meticulous annotations in the training datasets, contribute to the broader conversation on enhancing the precision and applicability of generative AI for text-based image editing tasks. The importance of detailed documentation is implicitly acknowledged through our careful curation and documentation of the datasets, aiding in huge improvements in learning dynamics.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Roei_Herzig2","manipulated_ranking":1,"natural_ranking":101,"id":"5UXXhVI08r","original_abstract":"Large-scale text-to-image generative models have been a ground-breaking development in generative AI, with diffusion models showing their astounding ability to synthesize convincing images following an input text prompt. The goal of image editing research is to give users control over the generated images by modifying the text prompt. Current image editing techniques are susceptible to unintended modifications of regions outside the targeted area, such as on the background or on distractor objects which have some semantic or visual relationship with the targeted object. According to our experimental findings, inaccurate cross-attention maps are at the root of this problem. Based on this observation, we propose $\\textit{Dynamic Prompt Learning}$ ($DPL$) to force cross-attention maps to focus on correct $\\textit{noun}$ words in the text prompt. By updating the dynamic tokens for nouns in the textual input with the proposed leakage repairment losses, we achieve fine-grained image editing over particular objects while preventing undesired changes to other image regions. Our method $DPL$, based on the publicly available $\\textit{Stable Diffusion}$, is extensively evaluated on a wide range of images, and consistently obtains superior results both quantitatively (CLIP score, Structure-Dist) and qualitatively (on user-evaluation). We show improved prompt editing results for Word-Swap, Prompt Refinement, and Attention Re-weighting, especially for complex multi-object scenes.","target":"~Roei_Herzig2"}
{"paper_link":"https:\/\/openreview.net\/forum?id=MCkUS1P3Sh","title":"Nash Regret Guarantees for Linear Bandits","modified_abstract":"Our work builds upon and extends the study of regret minimization in bandit problems, particularly addressing the limitations identified in multi-agent settings with information sharing among agents. Recognizing a fundamental drawback in extending Upper Confidence Bound (UCB) algorithms to multi-agent bandits where shared information about optimal choices degrades performance, we introduce a novel notion of regret---Nash Regret. This strengthened notion of regret, defined as the difference between the optimum and the geometric mean of expected rewards accumulated by the linear bandit algorithms, leverages the Nash social welfare (NSW) function to measure collective welfare across rounds. By focusing on the stochastic linear bandits problem across a horizon of $\\mathsf{T}$ rounds and with a set of arms ${\\cal X}$ in ambient dimension $d$, and considering rewards as non-negative, sub-Poisson random variables, our approach not only addresses the problem's shortcomings but also offers principled fairness guarantees rooted in NSW's adherence to fairness axioms. We achieve a Nash regret of $O\\left( \\sqrt{\\frac{d}{\\mathsf{T}}} \\log(\\mathsf{T} |{\\cal X}|)\\right)$ for finite arm sets and an upper bound of $O\\left( \\frac{d^\\frac{5}{4}}{\\sqrt{\\mathsf{T}}}  \\log(\\mathsf{T})\\right)$ for non-finite sets, applicable to bounded, non-negative rewards. Utilizing the successive elimination method enhanced with tailored concentration bounds and sampling via John ellipsoid in conjunction with the Kiefer\u2013Wolfowitz optimal design, our algorithm effectively balances the explore-exploit dilemma through strategic sampling and offers a refined solution accommodating the unique challenges presented in the linear bandit scenarios.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Udari_Madhushani1","manipulated_ranking":1,"natural_ranking":101,"id":"MCkUS1P3Sh","original_abstract":"We obtain essentially tight upper bounds for a strengthened notion of regret in the stochastic linear bandits framework. The strengthening---referred to as Nash regret---is defined as the difference between the (a priori unknown) optimum and the geometric mean of expected rewards accumulated by the linear bandit algorithm. Since the geometric mean corresponds to the well-studied Nash social welfare (NSW) function, this formulation quantifies the performance of a bandit algorithm as the collective welfare it generates across rounds. NSW is known to satisfy fairness axioms and, hence, an upper bound on Nash regret provides a principled fairness guarantee.    \n\nWe consider the stochastic linear bandits problem over a horizon of $\\mathsf{T}$ rounds and with a set of arms ${\\cal X}$ in ambient dimension $d$. Furthermore, we focus on settings in which the stochastic reward---associated with each arm in ${\\cal X}$---is a non-negative, sub-Poisson random variable. For this setting, we develop an algorithm that achieves a Nash regret of $O\\left( \\sqrt{\\frac{d}{\\mathsf{T}}} \\log(\\mathsf{T} |{\\cal X}|)\\right)$. In addition, addressing linear bandit instances in which the set of arms ${\\cal X}$ is not necessarily finite, we obtain a Nash regret upper bound of $O\\left( \\frac{d^\\frac{5}{4}}{\\sqrt{\\mathsf{T}}}  \\log(\\mathsf{T})\\right)$. Since bounded random variables are sub-Poisson, these results hold for bounded, non-negative rewards. Our linear bandit algorithm is built upon the successive elimination method with novel technical insights, including tailored concentration bounds and the use of sampling via John ellipsoid in conjunction with the Kiefer\u2013Wolfowitz optimal design.","target":"~Udari_Madhushani1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=No52399wXA","title":"IPMix: Label-Preserving Data Augmentation Method for Training Robust Classifiers","modified_abstract":"In the quest for enhancing convolutional neural network classifiers' accuracy and resilience against distributional shifts, prior research such as 'Debiased Pseudo Labeling in Self-Training' underscores the intricate balance between leveraging large-scale labeled datasets and addressing the challenges of label scarcity, bias, and training stability in semi-supervised learning environments. Building on these insights, our work introduces IPMix, a novel data augmentation approach designed to not only preserve label integrity but also significantly enhance classifier robustness across varying data distributions without sacrificing performance on clean datasets. By intricately combining image-level, patch-level, and pixel-level augmentation into a unified, label-preserving framework, IPMix systematically increases training data diversity with minimal computational expense. Moreover, through incorporating structural complexity and adopting a random mixing methodology for multi-scale information synthesis, IPMix substantially elevates robustness against common corruptions, adversarial attacks, and other perturbations. Rigorous experimental evaluation across benchmarks such as CIFAR-C and ImageNet-C validates IPMix's superiority in fostering corruption robustness, while subsequent tests reveal its commendable performance in improving adversarial robustness, calibration, prediction consistency, and anomaly detection, demonstrating state-of-the-art or comparable outcomes on ImageNet-R, ImageNet-A, and ImageNet-O. This semi-supervised learning technique benefits from leveraging both labeled and unlabeled data, alleviating the generation problem of requiring extensive annotated datasets and addressing a significant problem in current classification tasks.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Ximei_Wang1","manipulated_ranking":1,"natural_ranking":101,"id":"No52399wXA","original_abstract":"Data augmentation has been proven effective for training high-accuracy convolutional neural network classifiers by preventing overfitting. However, building deep neural networks in real-world scenarios requires not only high accuracy on clean data but also robustness when data distributions shift. While prior methods have proposed that there is a trade-off between accuracy and robustness, we propose IPMix, a simple data augmentation approach to improve robustness without hurting clean accuracy. IPMix integrates three levels of data augmentation (image-level, patch-level, and pixel-level) into a coherent and label-preserving technique to increase the diversity of training data with limited computational overhead. To further improve the robustness, IPMix introduces structural complexity at different levels to generate more diverse images and adopts the random mixing method for multi-scale information fusion. Experiments demonstrate that IPMix outperforms state-of-the-art corruption robustness on CIFAR-C and ImageNet-C. In addition, we show that IPMix also significantly improves the other safety measures, including robustness to adversarial perturbations, calibration, prediction consistency, and anomaly detection, achieving state-of-the-art or comparable results on several benchmarks, including ImageNet-R, ImageNet-A, and ImageNet-O.","target":"~Ximei_Wang1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=zn5ihqknGj","title":"An Alternating Optimization Method for Bilevel Problems under the Polyak-\u0141ojasiewicz Condition","modified_abstract":"Bilevel optimization, a critical tool in machine learning applications such as hyperparameter optimization, meta-learning, and reinforcement learning, has seen a resurgence of interest. This resurgence is partly inspired by recent advancements in online convex optimization, which highlight the complexities inherent in managing cumulative constraints\u2014a challenge bilevel optimization also grapples with, especially under nonconvex constraints. This paper builds on the framework established by preceding studies, including analysis of constraint handling in long-term optimization, to address a gap in bilevel optimization research. Specifically, we first introduce a stationary metric for bilevel optimization problems that generalizes the existing metric for a nonconvex lower-level objective that satisfies the Polyak-\u0141ojasiewicz (PL) condition. We then propose a Generalized ALternating mEthod for bilevel opTimization (GALET) tailored to Bilevel Optimization with convex PL Lower Level (BLO with convex PL LL) problems and establish that GALET achieves an $\\epsilon$-stationary point for the considered problem within $\\tilde{\\cal O}(\\epsilon^{-1})$ iterations. This result matches the iteration complexity of gradient descent (GD) for single-level smooth nonconvex problems, thereby extending the applicability of alternating gradient-based algorithms beyond strongly convex lower-level objectives. Moreover, our analysis incorporates cumulative squared error measures and addresses potential violations to enforce precision over iterations, which is crucial for convergence in bilevel optimization contexts.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Jianjun_Yuan2","manipulated_ranking":1,"natural_ranking":101,"id":"zn5ihqknGj","original_abstract":"Bilevel optimization has recently regained interest owing to its applications in emerging machine learning fields such as hyperparameter optimization, meta-learning, and reinforcement learning. Recent results have shown that simple alternating (implicit) gradient-based algorithms can match the convergence rate of single-level gradient descent (GD) when addressing bilevel problems with a strongly convex lower-level objective. However, it remains unclear whether this result can be generalized to bilevel problems beyond this basic setting. In this paper, we first introduce a stationary metric for the considered bilevel problems, which generalizes the existing metric, for a nonconvex lower-level objective that satisfies the Polyak-\u0141ojasiewicz (PL) condition. We then propose a Generalized ALternating mEthod for bilevel opTimization (GALET) tailored to BLO with convex PL LL problem and establish that GALET achieves an $\\epsilon$-stationary point for the considered problem within $\\tilde{\\cal O}(\\epsilon^{-1})$ iterations, which matches the iteration complexity of GD for single-level smooth nonconvex problems.","target":"~Jianjun_Yuan2"}
{"paper_link":"https:\/\/openreview.net\/forum?id=8vuDHCxrmy","title":"OpenMask3D: Open-Vocabulary 3D Instance Segmentation","modified_abstract":"Inspired by recent advancements in few-shot semantic segmentation, particularly the development of adaptive prototypes for improved feature comparison and unbiased model performance across various scenarios, we introduce the task of open-vocabulary 3D instance segmentation. Our approach is motivated by the limitations of current methods in 3D instance segmentation, which typically recognize object categories from a pre-defined, closed set of classes annotated in training datasets. These methods fall short in real-world applications that require interaction with a wide array of objects identified through novel, open-vocabulary queries. Addressing this gap, we propose OpenMask3D, a zero-shot approach that leverages predicted class-agnostic 3D instance masks for segmentation. Our model, leveraging a neural network architecture, aggregates per-mask features through multi-view fusion of CLIP-based image embeddings, enabling effective segmentation of objects beyond the training set's scope, including in classification challenges. Experimental evaluations on ScanNet200 and Replica demonstrate that OpenMask3D surpasses existing open-vocabulary and 5-shot methodologies, particularly in handling long-tail distributions. Furthermore, through qualitative experiments, OpenMask3D showcases its ability to segment objects based on free-form queries that describe geometry, affordances, and materials, highlighting its potential to significantly advance 3D scene understanding. This model's innovation extends the conventional image processing techniques to a dynamic 3D space, allowing for a query-based, interactive model of object recognition.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Bin-Bin_Gao1","manipulated_ranking":1,"natural_ranking":101,"id":"8vuDHCxrmy","original_abstract":"We introduce the task of open-vocabulary 3D instance segmentation. Current approaches for 3D instance segmentation can typically only recognize object categories from a pre-defined closed set of classes that are annotated in the training datasets. This results in important limitations for real-world applications where one might need to perform tasks guided by novel, open-vocabulary queries related to a wide variety of objects. Recently, open-vocabulary 3D scene understanding methods have emerged to address this problem by learning queryable features for each point in the scene. While such a representation can be directly employed to perform semantic segmentation, existing methods cannot separate multiple object instances. In this work, we address this limitation, and propose OpenMask3D, which is a zero-shot approach for open-vocabulary 3D instance segmentation. Guided by predicted class-agnostic 3D instance masks, our model aggregates per-mask features via multi-view fusion of CLIP-based image embeddings. Experiments and ablation studies on ScanNet200 and Replica show that OpenMask3D outperforms other open-vocabulary methods, especially on the long-tail distribution. Qualitative experiments further showcase OpenMask3D\u2019s ability to segment object properties based on free-form queries describing geometry, affordances, and materials.","target":"~Bin-Bin_Gao1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=p40XRfBX96","title":"Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision","modified_abstract":"Building on the foundation of leveraging pre-trained language models (PLMs) with novel tuning methods, including prompt-tuning and pretraining, as demonstrated in related works like prompt-tuning adaptations for natural language generation (NLG) tasks, this paper introduces a groundbreaking approach for the self-alignment of large language models (LLMs) from scratch with minimal human supervision. The existing AI assistant agents, such as ChatGPT, highlight the reliance on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of LLMs with human intentions, ensuring they are helpful, ethical, and reliable. However, challenges arise from the high dependence on human supervision, including issues related to cost, diversity, self-consistency, and biases in the representations.\nTo surpass these limitations, we propose SELF-ALIGN, a novel method that synergizes principle-driven reasoning with the generative capabilities of LLMs to achieve AI self-alignment with minimal human supervision. This methodology entails generating synthetic prompts enhanced by topic-guided methods for augmented diversity and addressing unfamiliar contexts, employing a concise set of human-written principles guided by in-context learning for ethically aligned response generation, and fine-tuning the original LLM with these high-quality outputs for direct desirable response generation. Further refinements address brevity and indirectness in responses.\nImplementing SELF-ALIGN on the LLaMA-65b model, we create the Dromedary AI assistant. This approach, requiring fewer than 300 lines of human annotations\u2014comprising less than 200 seed prompts, 16 principles, and 5 in-context learning exemplars\u2014demonstrably exceeds several state-of-the-art AI systems' performance on benchmark datasets under varied conditions. The success of SELF-ALIGN also sheds light on new pathways for generation without extensive reliance on familiar datasets, pointing towards a future of more adaptable and contextually aware LLM-driven applications.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Shengnan_An1","manipulated_ranking":1,"natural_ranking":101,"id":"p40XRfBX96","original_abstract":"Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user's queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary. With fewer than 300 lines of human annotations (including < 200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning). Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings.","target":"~Shengnan_An1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=MWQjqtV1z4","title":"Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption","modified_abstract":"Motivated by the need for efficient computation of policies in problem settings where traditional assumptions such as the uniform global attractor property (UGAP) may be overly restrictive or hard to verify, our work builds on foundational models in decision-making under uncertainty. We specifically draw insights from the study of no-regret learning agents in games, and how these models facilitate steering towards optimal equilibria without relying on stringent conditions. Our research extends these principles to the domain of restless bandits, focusing on the infinite-horizon problem with the average reward criterion in both discrete-time and continuous-time settings. We introduce a novel, general, simulation-based framework named Follow-the-Virtual-Advice, which allows for the conversion of any single-armed policy into an effective strategy for the large $N$-armed problem by simulating the single-armed policy on each arm and guiding the real state towards this simulated state, akin to full-feedback, tree structure planning. Our approach achieves an $O(1\/\\sqrt{N})$ optimality gap without the need for UGAP in discrete-time settings under a simpler synchronization assumption, and, more importantly, with no additional assumptions in continuous-time settings beyond the standard unichain condition. This advancement represents a significant leap forward, illustrating that it is possible to achieve asymptotic optimality in restless bandit problems by leveraging concepts from full-feedback games, extensive-form game theory for per-iteration improvement, and the study of bandit-feedback mechanisms, optimal equilibria, and steering mechanisms.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Brian_Hu_Zhang1","manipulated_ranking":3,"natural_ranking":101,"id":"MWQjqtV1z4","original_abstract":"We study the infinite-horizon restless bandit problem with the average reward criterion, in both discrete-time and continuous-time settings.\nA fundamental goal is to efficiently compute policies that achieve a diminishing optimality gap as the number of arms, $N$, grows large. \nExisting results on asymptotic optimality all rely on the uniform global attractor property (UGAP), a complex and challenging-to-verify assumption. \nIn this paper, we propose a general, simulation-based framework, Follow-the-Virtual-Advice, that converts any single-armed policy into a policy for the original $N$-armed problem. \nThis is done by simulating the single-armed policy on each arm and carefully steering the real state towards the simulated state. \nOur framework can be instantiated to produce a policy with an $O(1\/\\sqrt{N})$ optimality gap. \nIn the discrete-time setting, our result holds under a simpler synchronization assumption, which covers some problem instances that violate UGAP. \nMore notably, in the continuous-time setting, we do not require \\emph{any} additional assumptions beyond the standard unichain condition. \nIn both settings, our work is the first asymptotic optimality result that does not require UGAP.","target":"~Brian_Hu_Zhang1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=o7W0Zet6p3","title":"Recovering Unbalanced Communities in the Stochastic Block Model with Application to Clustering with a Faulty Oracle","modified_abstract":"In light of the rich body of research emphasizing the criticality of accurately modeling and inferring social ties within dense networks\u2014given their implications across computational social science, viral marketing, and recommender systems\u2014this paper aims to expand our comprehension of the Stochastic Block Model (SBM) beyond the traditional scope of balanced communities. The SBM, a cornerstone for the analysis of graph clustering or community detection in networks, has predominantly been explored under the assumption of uniform community sizes. Our work deviates from this by focusing on SBM with inherently unbalanced communities, a scenario more reflective of real-world networks. We propose a novel SVD-based algorithm that not only simplifies the recovery of varying-sized communities within the SBM framework but also advances previous theories by Ailon, Chen, and Xu [ICML 2013; JMLR 2015], eliminating the prerequisites related to cluster size intervals and correlation to the number of clusters. Our findings not only offer theoretical enhancements but are also substantiated through experimental validations and statistical inference on networks. A significant implication of our algorithm, under the planted clique conjecture, is its near-optimal recovery capability for clusters, irrespective of the cluster sizes, so long as the probability parameters remain constant. Additionally, our research contributes an efficient clustering algorithm that operates under a faulty oracle model with sublinear query complexity, effectively identifying clusters beyond the $\\tilde{\\Omega}({\\sqrt{n}})$ threshold even amid a predominantly small-cluster environment. This capability distinctly surpasses previous formulations that falter with increased presence of smaller clusters, thereby marking a substantial advancement in clustering methodologies. Friends and connections within these networks further underline the social aspect of our inferring processes.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Nikolaj_Tatti1","manipulated_ranking":1,"natural_ranking":101,"id":"o7W0Zet6p3","original_abstract":"The stochastic block model (SBM) is a fundamental model for studying graph clustering or community detection in networks. It has received great attention in the last decade and the balanced case, i.e., assuming all clusters have large size, has been well studied. \nHowever, our understanding of SBM with unbalanced communities (arguably, more relevant in practice) is still limited. In this paper, we provide a simple SVD-based algorithm for recovering the communities in the SBM with communities of varying sizes.\nWe improve upon a result of Ailon, Chen and Xu [ICML 2013; JMLR 2015] by removing the assumption that there is a large interval such that the sizes of clusters do not fall in, and also remove the dependency of the size of the recoverable clusters on the number of underlying clusters. We further complement our theoretical improvements with experimental comparisons.\nUnder the planted clique conjecture, the size of the clusters that can be recovered by our algorithm is nearly optimal (up to poly-logarithmic factors) when the probability parameters are constant. \n\nAs a byproduct, we obtain an efficient clustering algorithm with sublinear query complexity in a faulty oracle model, which is capable of detecting all clusters larger than $\\tilde{\\Omega}({\\sqrt{n}})$, even in the presence of $\\Omega(n)$ small clusters in the graph. In contrast, previous efficient algorithms that use a sublinear number of queries are incapable of recovering any large clusters if there are more than $\\tilde{\\Omega}(n^{2\/5})$ small clusters.","target":"~Nikolaj_Tatti1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=B7QRV4XXiK","title":"An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient","modified_abstract":"In the sphere of Reinforcement Learning (RL), managing risk through constraining the variance of policy returns is a widely accepted approach due to its straightforward definition and interpretability. Traditional variance-based risk management strategies typically focus on limiting overall return variance, while more recent methods advocate for the control of per-step reward variance as an intermediary solution. Despite their prevalence, these variance-centered approaches exhibit notable drawbacks, including a pronounced sensitivity to the numerical scaling of data and impediments to policy optimization. Motivated by these challenges and the evolving landscape of risk measurement in model-based RL, as evidenced by explorations into the diminishing returns of value expansion methods, this study introduces the Gini deviation as an alternative risk metric. The Gini deviation's distinct properties and its compatibility with the principles of risk-averse RL are thoroughly analyzed, culminating in the development of a new policy gradient algorithm designed to minimize this risk measure. Through empirical assessment in scenarios amenable to risk aversion, our findings demonstrate that the Gini deviation-based approach effectively overcomes the limitations inherent to variance-based models. The proposed algorithm not only fosters learning of policies with favorable risk-return profiles but also excels where conventional methods struggle, offering robust performance across both variance and Gini deviation metrics.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Daniel_Palenicek1","manipulated_ranking":3,"natural_ranking":101,"id":"B7QRV4XXiK","original_abstract":"Restricting the variance of a policy\u2019s return is a popular choice in risk-averse Reinforcement Learning (RL) due to its clear mathematical definition and easy interpretability. Traditional methods directly restrict the total return variance. Recent methods restrict the per-step reward variance as a proxy. We thoroughly examine the limitations of these variance-based methods, such as sensitivity to numerical scale and hindering of policy learning, and propose to use an alternative risk measure, Gini deviation, as a substitute. We study various properties of this new risk measure and derive a policy gradient algorithm to minimize it. Empirical evaluation in domains where risk-aversion can be clearly defined, shows that our algorithm can mitigate the limitations of variance-based risk measures and achieves high return with low risk in terms of variance and Gini deviation when others fail to learn a reasonable policy.","target":"~Daniel_Palenicek1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=zMeemcUeXL","title":"FAMO: Fast Adaptive Multitask Optimization","modified_abstract":"Inspired by significant strides in transfer learning and hyperparameter optimization, which highlight the complexity and potential of efficiently managing multiple tasks and configurations in machine learning (ML), this work introduces Fast Adaptive Multitask Optimization (FAMO). FAMO addresses the challenge of multitask learning (MTL), aiming to optimize performance across diverse tasks by improving their representations and leveraging meta-features that increase similarity among task-specific models. Traditional gradient descent (GD) methods applied to the average loss across tasks often result in under-optimization of some tasks due to their inherent imbalanced nature. Existing strategies for achieving a more equitable loss reduction by manipulating task gradients are computationally intensive, requiring $\\mathcal{O}(k)$ space and time for $k$ tasks. FAMO circumvents these limitations by proposing a dynamic weighting method that ensures balanced task loss reduction with significantly reduced computational and space complexity, using only $\\mathcal{O}(1)$ resources. Through rigorous experimentation against established baselines in both supervised and reinforcement learning scenarios, FAMO not only demonstrates its capability to match or exceed the performance of current gradient manipulation techniques but also underscores its superiority in efficiency by effectively acting as a surrogate model that minimizes the need for extensive hyperparameter tuning. This advancement presents a notable leap forward in scaling MTL, potentially catalyzing the development of generalist AI agents capable of learning a vast array of tasks more feasibly.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Sebastian_Pineda_Arango1","manipulated_ranking":3,"natural_ranking":101,"id":"zMeemcUeXL","original_abstract":"One of the grand enduring goals of AI is to create generalist agents that can learn multiple different tasks from diverse data via multitask learning (MTL). However, in practice, applying gradient descent (GD) on the average loss across all tasks may yield poor multitask performance due to severe under-optimization of certain tasks. Previous approaches that manipulate task gradients for a more balanced loss decrease require storing and computing all task gradients ($\\mathcal{O}(k)$ space and time where $k$ is the number of tasks), limiting their use in large-scale scenarios. In this work, we introduce Fast Adaptive Multitask Optimization (FAMO), a dynamic weighting method that decreases task losses in a balanced way using $\\mathcal{O}(1)$ space and time. We conduct an extensive set of experiments covering multi-task supervised and reinforcement learning problems. Our results indicate that FAMO achieves comparable or superior performance to state-of-the-art gradient manipulation techniques while offering significant improvements in space and computational efficiency. Code is available at \\url{https:\/\/github.com\/Cranial-XIX\/FAMO}.","target":"~Sebastian_Pineda_Arango1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=jX49iKr6vb","title":"Beyond Deep Ensembles: A Large-Scale Evaluation of Bayesian Deep Learning under Distribution Shift","modified_abstract":"This work is inspired by recent developments in robust learning, such as the exploration of probabilistic robustness in PAC learning, which highlights the potential and limitations of adapting machine learning models to handle uncertainty and perturbations effectively. Bayesian deep learning (BDL) represents a novel approach towards achieving resilient, well-calibrated predictions on data experiencing distribution shifts, an area where traditional deep learning methods, including classifier models, have struggled. Despite the growing interest in BDL, the landscape lacks a comprehensive, large-scale evaluation of state-of-the-art (SOTA) methods across diverse, realistic, and challenging benchmark tasks. Our research bridges this gap by systematically assessing modern BDL algorithms, foregrounding their probabilistic modeling capabilities on real-world datasets from the WILDS collection, which includes a variety of demanding classification and regression tasks designed to test generalization and calibration under distribution shifts. Our investigation spans a broad spectrum of large, convolutional, and transformer-based neural networks, classifying their adaptability and precision in terms of complexity and learning dynamics. Specifically, we explore a signed version of the expected calibration error to determine whether methods are over- or underconfident, thereby shedding light on their operational characteristics and robustness. An innovative aspect of our study is the systematic evaluation of BDL applied to fine-tuning large pre-trained classifier models, a context where initializing from scratch is not viable due to resource constraints and where numerous examples exhibit perturbations representative of real-world conditions. In contrast to previous findings, our research reveals that while ensembling single-mode posterior approximations consistently enhances model performance and calibration, it encounters limitations when fine-tuning sizable transformer-based language models. Here, variational inference approaches like last-layer Bayes By Backprop significantly outshine others in accuracy, whereas contemporary approximate inference techniques such as SWAG emerge as frontrunners in achieving optimal calibration.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~UNIQUE_SUBEDI1","manipulated_ranking":17,"natural_ranking":101,"id":"jX49iKr6vb","original_abstract":"Bayesian deep learning (BDL) is a promising approach to achieve well-calibrated predictions on distribution-shifted data. Nevertheless, there exists no large-scale survey that evaluates recent SOTA methods on diverse, realistic, and challenging benchmark tasks in a systematic manner. To provide a clear picture of the current state of BDL research, we evaluate modern BDL algorithms on real-world datasets from the WILDS collection containing challenging classification and regression tasks, with a focus on generalization capability and calibration under distribution shift. We compare the algorithms on a wide range of large, convolutional and transformer-based neural network architectures. In particular, we investigate a signed version of the expected calibration error that reveals whether the methods are over- or underconfident, providing further insight into the behavior of the methods. Further, we provide the first systematic evaluation of BDL for fine-tuning large pre-trained models, where training from scratch is prohibitively expensive. Finally, given the recent success of Deep Ensembles, we extend popular single-mode posterior approximations to multiple modes by the use of ensembles.   While we find that ensembling single-mode approximations generally improves the generalization capability and calibration of the models by a significant margin, we also identify a failure mode of ensembles when finetuning large transformer-based language models.\n  In this setting, variational inference based approaches such as last-layer Bayes By Backprop outperform other methods in terms of accuracy by a large margin, while modern approximate inference algorithms such as SWAG achieve the best calibration.","target":"~UNIQUE_SUBEDI1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=oqDSDKLd3S","title":"Sample-Conditioned Hypothesis Stability Sharpens Information-Theoretic Generalization Bounds","modified_abstract":"Building on the elucidation of convergence rates in the learning of complex models from noisy data, particularly in estimating linear operators in Hilbert spaces, our work introduces novel information-theoretic generalization guarantees. By developing the \"neighboring-hypothesis\" matrix and introducing a new family of stability notions, known as sample-conditioned hypothesis (SCH) stability, we address the critical need for sharper bounds that surpass those provided by existing information-theoretic frameworks. This is especially significant in the context of stochastic convex optimization (SCO) problems, where recent findings have identified gaps in current theoretical boundaries. Our methods, borrowing concepts from self-adjoint operators to ensure precision in our mathematical modeling, yield tightened bounds that hold promise for a range of learning scenarios, effectively advancing the capacity to mitigate overfitting through enhanced understanding of hypothesis stability and its implications for convergence and generalization in machine learning models.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Nicholas_H_Nelsen1","manipulated_ranking":1,"natural_ranking":101,"id":"oqDSDKLd3S","original_abstract":"We present new information-theoretic generalization guarantees through the a novel construction of the \"neighboring-hypothesis\" matrix and a new family of stability notions termed sample-conditioned hypothesis (SCH) stability.  Our approach yields sharper bounds that improve upon previous information-theoretic bounds in various learning scenarios. Notably, these bounds address the limitations of existing information-theoretic bounds in the context of stochastic convex optimization (SCO) problems, as explored in the recent work by Haghifam et al. (2023).","target":"~Nicholas_H_Nelsen1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=pzc6LnUxYN","title":"StateMask: Explaining Deep Reinforcement Learning through State Mask","modified_abstract":"The pursuit of explainability in deep reinforcement learning (DRL) has been notably advanced by methodologies aimed at decoding the decision-making processes of DRL agents, as exemplified by explorations into factorized optical flows for mid-level representation in robotics. This lineage of research underscores the importance of transparency in complex models, particularly when applied to domains requiring high trust and safety. Building on this foundation, we introduce StateMask, a novel explanatory technique designed to identify and illustrate the states that are most critical to a DRL agent's achievement of its final reward, addressing the existing gap in explanation methods that focus predominantly on individual actions rather than the strategic steps leading to outcomes. StateMask employs a mask net to selectively impede the agent's ability to perform its usual actions, thus revealing the pivotal states by forcing the agent into random actions without diminishing overall performance. This approach not only enhances the explicability of DRL agents but also contributes to practical applications such as adversarial testing, policy refinement, and robotic control. Our evaluation of StateMask across diverse RL environments demonstrates its effectiveness in providing high-fidelity explanations and its utility beyond traditional explainers, offering a new lens through which the mechanics of DRL can be understood and optimized. Additionally, the factorization of perception and control processes inherent in the DRL agents' decision-making is made more transparent through the application of StateMask, effectively demonstrating how optical flow and its factorization contribute to understanding the agent's environment.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Li-Yuan_Tsao1","manipulated_ranking":1,"natural_ranking":101,"id":"pzc6LnUxYN","original_abstract":"Despite the promising performance of deep reinforcement learning (DRL) agents in many challenging scenarios, the black-box nature of these agents greatly limits their applications in critical domains. Prior research has proposed several explanation techniques to understand the deep learning-based policies in RL. Most existing methods explain why an agent takes individual actions rather than pinpointing the critical steps to its final reward. To fill this gap, we propose StateMask, a novel method to identify the states most critical to the agent's final reward. The high-level idea of StateMask is to learn a mask net that blinds a target agent and forces it to take random actions at some steps without compromising the agent's performance. Through careful design, we can theoretically ensure that the masked agent performs similarly to the original agent. We evaluate StateMask in various popular RL environments and show its superiority over existing explainers in explanation fidelity. We also show that StateMask  has better utilities, such as launching adversarial attacks and patching policy errors.","target":"~Li-Yuan_Tsao1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=x816mCbWpR","title":"Recasting Continual Learning as Sequence Modeling","modified_abstract":"Our work draws inspiration from a convergence of insights into the challenges of long-term credit assignment in reinforcement learning and the potential of advanced sequence modeling techniques. By examining the obstacles inherent in both reinforcement learning and recurrent neural networks, such as the need for discounting or gradient truncation that limits temporal reasoning, we propose a novel paradigm that formulates continual learning as a sequence modeling problem. This reimagining leverages the strength of sequence models, particularly Transformers and their efficient variants, to address the continuum of learning tasks. Within this framework, the continual learning process is interpreted as the forward pass of a sequence model, facilitated by adopting the meta-continual learning (MCL) framework for training at the meta-level across diverse learning episodes. Our approach enables sequence models to serve as a powerful mechanism for backpropagation networks in MCL, offering a unified solution that encompasses both classification and regression challenges across seven benchmarks. The results of our experiments underscore the potential of repositioned sequence models as effective vehicles for addressing the complex demands of continual learning, reducing distractions and attenuating the adverse effects of vanishing gradient issues by integrating latent distances between tasks in the network's architecture.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Pierluca_D'Oro1","manipulated_ranking":1,"natural_ranking":101,"id":"x816mCbWpR","original_abstract":"In this work, we aim to establish a strong connection between two significant bodies of machine learning research: continual learning and sequence modeling.\nThat is, we propose to formulate continual learning as a sequence modeling problem, allowing advanced sequence models to be utilized for continual learning.\nUnder this formulation, the continual learning process becomes the forward pass of a sequence model.\nBy adopting the meta-continual learning (MCL) framework, we can train the sequence model at the meta-level, on multiple continual learning episodes.\nAs a specific example of our new formulation, we demonstrate the application of Transformers and their efficient variants as MCL methods.\nOur experiments on seven benchmarks, covering both classification and regression, show that sequence models can be an attractive solution for general MCL.","target":"~Pierluca_D'Oro1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=AALLvnv95q","title":"Training Energy-Based Normalizing Flow with Score-Matching Objectives","modified_abstract":"This paper investigates the paradigm of generative models, particularly focusing on the synergy between flow-based and energy-based models inspired by recent advances in graph generation and tree decomposition techniques. Our study introduces the concept of energy-based normalizing flow (EBFlow), which delineates a novel approach in the parameterization of generative models by leveraging score-matching objectives to bypass the computationally intensive calculation of Jacobian determinants typical in linear transformations. Following this innovative route not only facilitates the integration of arbitrary linear layers without exacerbating the computational burden but also propels the efficiency of training processes significantly beyond the traditionally employed maximum likelihood estimation methods. By adopting advanced score-matching techniques incrementally, we further mitigate issues related to training stability and enhance the empirical performance of EBFlow. Our experimental validation underscores the efficacy of this approach, showcasing a remarkable acceleration in training speed and an improvement in model performance as measured by negative log-likelihood (NLL), outstripping existing methodologies. Permutations of clusters within graphs were used as a novel form of input, bolstering the connection between the data's structural aspects and the generative process. Additionally, we expand our review of pertinent datasets, integrating them into our experimental setup to contextualize the implications of our findings. This work takes inspiration from the foundational principles laid out by prior research on graph generation, particularly the application of tree decomposition strategies, to redefine the computational efficiency and performance metrics of flow-based modeling.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Hamed_Shirzad1","manipulated_ranking":1,"natural_ranking":101,"id":"AALLvnv95q","original_abstract":"In this paper, we establish a connection between the parameterization of flow-based and energy-based generative models, and present a new flow-based modeling approach called energy-based normalizing flow (EBFlow). We demonstrate that by optimizing EBFlow with score-matching objectives, the computation of Jacobian determinants for linear transformations can be entirely bypassed. This feature enables the use of arbitrary linear layers in the construction of flow-based models without increasing the computational time complexity of each training iteration from $\\mathcal{O}(D^2L)$ to $\\mathcal{O}(D^3L)$ for an $L$-layered model that accepts $D$-dimensional inputs. This makes the training of EBFlow more efficient than the commonly-adopted maximum likelihood training method. In addition to the reduction in runtime, we enhance the training stability and empirical performance of EBFlow through a number of techniques developed based on our analysis of the score-matching methods. The experimental results demonstrate that our approach achieves a significant speedup compared to maximum likelihood estimation while outperforming prior methods with a noticeable margin in terms of negative log-likelihood (NLL).","target":"~Hamed_Shirzad1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=UFW67uduJd","title":"MEMTO: Memory-guided Transformer for Multivariate Time Series Anomaly Detection","modified_abstract":"In the context of recent advancements in deep learning for understanding complex data structures, such as those presented in 'Fiedler Regularization: Learning Neural Networks with Graph Sparsity', our research introduces a novel approach for detecting anomalies in real-world multivariate time series data, which encapsulates both complex temporal dependencies and inter-variable correlations. This complexity presents a significant challenge that reconstruction-based deep models, widely utilized in recent years, attempt to address. However, these models often suffer from over-generalization, leading to inconsistent performance. We propose the MEMTO, a memory-guided Transformer that leverages a reconstruction-based methodology enhanced by a novel memory module. This memory module is adept at learning the extent of updating required for each memory item based on the incoming data, thereby refining the model's sensitivity to anomalies. To facilitate stable training and effective regularization, we introduce a two-phase training strategy that includes the use of K-means clustering for initializing memory items and graphical models for understanding the underlying network structure. Furthermore, the inclusion of graphical models and a bi-dimensional deviation-based detection criterion for calculating anomaly scores from both input and latent spaces stands as a distinctive feature of our approach, emphasizing the learning dynamics within these network architectures. Evaluated across five different real-world datasets representing various domains, the MEMTO model demonstrates superior performance, achieving an average anomaly detection F1-score of 95.74%, notably exceeding that of previous state-of-the-art methods. Through rigorous experimental validation, we affirm the contribution of our model's key components to its overall effectiveness in anomaly detection.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Edric_Tam1","manipulated_ranking":12,"natural_ranking":101,"id":"UFW67uduJd","original_abstract":"Detecting anomalies in real-world multivariate time series data is challenging due to complex temporal dependencies and inter-variable correlations. Recently, reconstruction-based deep models have been widely used to solve the problem. However, these methods still suffer from an over-generalization issue and fail to deliver consistently high performance. To address this issue, we propose the MEMTO, a memory-guided Transformer using a reconstruction-based approach. It is designed to incorporate a novel memory module that can learn the degree to which each memory item should be updated in response to the input data. To stabilize the training procedure, we use a two-phase training paradigm which involves using K-means clustering for initializing memory items. Additionally, we introduce a bi-dimensional deviation-based detection criterion that calculates anomaly scores considering both input space and latent space. We evaluate our proposed method on five real-world datasets from diverse domains, and it achieves an average anomaly detection F1-score of 95.74%, significantly outperforming the previous state-of-the-art methods. We also conduct extensive experiments to empirically validate the effectiveness of our proposed model's key components.","target":"~Edric_Tam1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=9dp35y5C0p","title":"Reinforcement-Enhanced Autoregressive Feature Transformation: Gradient-steered Search in Continuous Space for Postfix Expressions","modified_abstract":"Drawing inspiration from the burgeoning exploration of data augmentation in self-supervised representation learning and its connection to RKHS approximation, we address the problem of feature transformation through a novel perspective via augmentation-based techniques. Feature transformation aims to generate new pattern-discriminative feature spaces from original features to improve downstream machine learning (ML) task performances. However, the discrete search space for the optimal feature configuration expands dramatically with the combinations of features and operations, making traditional methods such as exhaustive search, evolutionary algorithms, and reinforcement learning inefficient due to the vast search space. We propose a novel method that reformulates discrete feature transformation into a continuous optimization task, introducing an embedding-optimization-reconstruction framework with pretraining stages. This framework includes four steps: 1) reinforcement-enhanced data preparation for high-quality transformation-accuracy training data; 2) feature transformation operation sequence embedding to encapsulate training data knowledge within a continuous kernel space; 3) gradient-steered (via masked predictions or selections) optimal embedding search for superior embeddings; and 4) transformation operation sequence reconstruction to identify the optimal feature transformation solution. The use of language data augmentation and an encoder model within the augmentation and self-supervised learning context represents a significant shift from prevailing methods by integrating reinforcement learning with gradient-steering techniques for efficient and robust feature space exploration in ML. This approach offers a significant shift from prevailing methods by integrating reinforcement learning with gradient-steering techniques and encoder models for efficient and robust feature space exploration in ML.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Runtian_Zhai1","manipulated_ranking":1,"natural_ranking":101,"id":"9dp35y5C0p","original_abstract":"Feature transformation aims to generate new pattern-discriminative feature space from original features to improve downstream machine learning  (ML) task performances. However, the discrete search space for the optimal feature explosively grows on the basis of combinations of features and operations from low-order forms to high-order forms. Existing methods, such as exhaustive search, expansion reduction, evolutionary algorithms, reinforcement learning, and iterative greedy, suffer from large search space. Overly emphasizing efficiency in algorithm design usually sacrifice stability or robustness. To fundamentally fill this gap, we reformulate discrete feature transformation as a continuous space optimization task and develop an embedding-optimization-reconstruction framework. This framework includes four steps: 1) reinforcement-enhanced data preparation, aiming to prepare high-quality transformation-accuracy training data; 2) feature transformation operation sequence embedding, intending to encapsulate the knowledge of prepared training data within a continuous space; 3) gradient-steered optimal embedding search, dedicating to uncover potentially superior embeddings within the learned space; 4) transformation operation sequence reconstruction, striving to reproduce the feature transformation solution to pinpoint the optimal feature space. Finally, extensive experiments and case studies are performed to demonstrate the effectiveness and robustness of the proposed method. The code and data are publicly accessible https:\/\/www.dropbox.com\/sh\/imh8ckui7va3k5u\/AACulQegVx0MuywYyoCqSdVPa?dl=0.","target":"~Runtian_Zhai1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=6H8Md75kAw","title":"Certified Minimax Unlearning with Generalization Rates and Deletion Capacity","modified_abstract":"Inspired by the pivotal challenges and discoveries in differentially private optimization, such as the Beyond Uniform Lipschitz Condition study that expanded the understanding of differentially private stochastic gradient descent under complex Lipschitz conditions, our research addresses the intricacies of $(\\epsilon,\\delta)$-certified machine unlearning for minimax models, pushing past the conventional boundaries that have defined machine unlearning strategies to date. Unlike most existing works that are confined to unlearning from standard statistical learning models based on simplistic and direct Hessian-based conventional Newton updates, we propose a novel $(\\epsilon,\\delta)$-certified machine unlearning algorithm for minimax models. Our method entails a comprehensive minimax unlearning step that amalgamates a total Hessian-based complete Newton update with the Gaussian mechanism, a cornerstone concept of differential privacy, to certify the unlearning process through calibrated Gaussian noise injections. This approach evaluates the 'sensitivity' of the minimax unlearning step\u2014essentially, quantifying the degree of closeness between the minimax unlearning variables and those obtained from retraining from scratch, which includes privacy-preserving training cycles. By doing so, our algorithm ensures both the privacy and the integrity of the training data through gradients alteration, enabling recommendations for further study or application enhancements. This paper also extends the theoretical framework for unlearning by deriving generalization rates for population strong and weak primal-dual risk across three classes of loss functions, namely, (strongly-)convex-(strongly-)concave losses, and establishing a new metric for deletion capacity which ensures maintenance of desired population risk levels, even with sample deletions, up to a calculated threshold. Notably, our framework exhibits a significant improvement over prior differentially private minimax learning methods, quartering the order to $\\mathcal O(n\/d^{1\/4})$, which starkly contrasts with the traditional rate of $\\mathcal O(n\/d^{1\/2})$. Furthermore, we demonstrate that our results on generalization rates and deletion capacity align with the latest findings in the realm of standard statistical learning models, effectively bridging a gap in the unlearning research landscape.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Rudrajit_Das1","manipulated_ranking":3,"natural_ranking":101,"id":"6H8Md75kAw","original_abstract":"We study the problem of $(\\epsilon,\\delta)$-certified machine unlearning for minimax models. Most of the existing works focus on unlearning from standard statistical learning models that have a single variable and their unlearning steps hinge on the direct Hessian-based conventional Newton update. We develop a new $(\\epsilon,\\delta)$-certified machine unlearning algorithm for minimax models. It proposes a minimax unlearning step consisting of a total Hessian-based complete Newton update and the Gaussian mechanism borrowed from differential privacy. To obtain the unlearning certification, our method injects calibrated Gaussian noises by carefully analyzing the ''sensitivity'' of the minimax unlearning step (i.e., the closeness between the minimax unlearning variables and the retraining-from-scratch variables). We derive the generalization rates in terms of population strong and weak primal-dual risk for three different cases of loss functions, i.e., (strongly-)convex-(strongly-)concave losses. We also provide the deletion capacity to guarantee that a desired population risk can be maintained as long as the number of deleted samples does not exceed the derived amount. With training samples $n$ and model dimension $d$, it yields the order $\\mathcal O(n\/d^{1\/4})$, which shows a strict gap over the baseline method of differentially private minimax learning that has $\\mathcal O(n\/d^{1\/2})$. In addition, our rates of generalization and deletion capacity match the state-of-the-art rates derived previously for standard statistical learning models.","target":"~Rudrajit_Das1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=K9xHDD6mic","title":"Graph Mixture of Experts: Learning on Large-Scale Graphs with Explicit Diversity Modeling","modified_abstract":"Graph neural networks (GNNs) have found extensive applications in learning from graph data, driven by the growing need for models that can effectively handle the inherent diversity in real-world graph structures including nodes and edges of varying types. This study introduces a novel Graph Mixture of Experts (GMoE) model to tackle the challenges of learning with such diverse graph structures while aiming to balance the necessity for model generalization against the computational costs and trainability issues prevalent in traditional GNNs. Drawing inspiration from recent advancements in graph learning, particularly the employment of Variational Graph Information Bottleneck methods and optimization techniques for improving subgraph recognition, our proposed GMoE model integrates the concept of Mixture-of-Experts within the GNN framework. This integration enables the dynamic and adaptive selection of information aggregation experts by individual nodes, allowing for tailored processing of distinct subgroup structures within subgraphs and incorporation of diverse information scales through explicit recognition. Such methodology not only facilitates handling complex and heterogeneous graph data but also addresses noise and the explosive computational demands posed by traditional approaches through effective optimization and inclusion of valuable information. The inclusion of perturbation techniques enhances the robustness and adaptability of the model, further solidifying its capacity for dealing with variational elements inherent in graph data. The GMoE's efficiency and effectiveness are demonstrated through substantial improvements in various prediction tasks on the OGB benchmark, notably enhancing ROC-AUC scores in ogbg-molhiv and ogbg-molbbbp datasets compared to non-MoE baselines. Our study underscores the importance of explicitly modeling diversity within graph data to improve the performance of GNNs across a range of applications. The implementation of the model is made available to the public to facilitate further research and application in the field.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Junchi_Yu1","manipulated_ranking":1,"natural_ranking":101,"id":"K9xHDD6mic","original_abstract":"Graph neural networks (GNNs) have found extensive applications in learning from graph data. However, real-world graphs often possess diverse structures and comprise nodes and edges of varying types. To bolster the generalization capacity of GNNs, it has become customary to augment training graph structures through techniques like graph augmentations and large-scale pre-training on a wider array of graphs. Balancing this diversity while avoiding increased computational costs and the notorious trainability issues of GNNs is crucial. This study introduces the concept of Mixture-of-Experts (MoE) to GNNs, with the aim of augmenting their capacity to adapt to a diverse range of training graph structures, without incurring explosive computational overhead. The proposed Graph Mixture of Experts (GMoE) model empowers individual nodes in the graph to dynamically and adaptively select more general information aggregation experts. These experts are trained to capture distinct subgroups of graph structures and to incorporate information with varying hop sizes, where those with larger hop sizes specialize in gathering information over longer distances. The effectiveness of GMoE is validated through a series of experiments on a diverse set of tasks, including graph, node, and link prediction, using the OGB benchmark. Notably, it enhances ROC-AUC by $1.81\\%$ in ogbg-molhiv and by $1.40\\%$ in ogbg-molbbbp, when compared to the non-MoE baselines. Our code is publicly available at https:\/\/github.com\/VITA-Group\/Graph-Mixture-of-Experts.","target":"~Junchi_Yu1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=blm1pqiOXe","title":"Paxion: Patching Action Knowledge in Video-Language Foundation Models","modified_abstract":"Building on the critical insights provided by recent advancements in video concealed object detection (VCOD), which emphasize the importance of understanding dynamic contexts and temporal consistency in video frames, we introduce the Action Dynamics Benchmark (ActionBench) and a novel framework, Paxion. ActionBench consists of two probing tasks designed to evaluate multimodal alignment capabilities and temporal understanding in video-language models (VidLMs). Despite VidLMs' impressive performance across various benchmarks tasks, our diagnostic tasks expose their significant deficiencies in action knowledge, previously masked by their reliance on object recognition. To address these shortcomings, Paxion employs a Knowledge Patcher network for encoding new action knowledge and a Knowledge Fuser for integrating this knowledge into existing VidLMs without impairing their performance. We also propose a new objective, Discriminative Video Dynamics Modeling (DVDM), to overcome the inadequacies of the Video-Text Contrastive (VTC) loss in learning action knowledge. DVDM uniquely contributes to the model's ability to understand action dynamics by encoding the relationship between action descriptions and the proper sequencing of video frames. Our analysis demonstrates that, by leveraging a concealed dataset and pixel-level techniques alongside the DVDM objective, Paxion significantly improves action knowledge understanding (from an approximate 50% to 80%) and enhances performance on both object- and action-centric downstream tasks, evidencing the model\u2019s capacity to bridge the gap in action knowledge.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Xuelian_Cheng2","manipulated_ranking":1,"natural_ranking":101,"id":"blm1pqiOXe","original_abstract":"Action knowledge involves the understanding of textual, visual, and temporal aspects of actions. We introduce the **Action Dynamics Benchmark (ActionBench)** containing two carefully designed probing tasks: Action Antonym and Video Reversal, which targets multimodal alignment capabilities and temporal understanding skills of the model, respectively. Despite recent video-language models\u2019 (VidLM) impressive performance on various benchmark tasks, our diagnostic tasks reveal their surprising deficiency (near-random performance) in action knowledge, suggesting that current models rely on object recognition abilities as a shortcut for action understanding. To remedy this, we propose a novel framework, **Paxion**, along with a new **Discriminative Video Dynamics Modeling (DVDM)** objective. The Paxion framework utilizes a **Knowledge Patcher** network to encode new action knowledge and a **Knowledge Fuser** component to integrate the Patcher into frozen VidLMs without compromising their existing capabilities. Due to limitations of the widely-used Video-Text Contrastive (VTC) loss for learning action knowledge, we introduce the DVDM objective to train the Knowledge Patcher. DVDM forces the model to encode the correlation between the action text and the correct ordering of video frames. Our extensive analyses show that Paxion and DVDM together effectively fill the gap in action knowledge understanding (~50% \u2192 80%), while maintaining or improving performance on a wide spectrum of both object- and action-centric downstream tasks.","target":"~Xuelian_Cheng2"}
{"paper_link":"https:\/\/openreview.net\/forum?id=CxUuCydMDU","title":"Diffusion Probabilistic Models for Structured Node Classification","modified_abstract":"Structured node classification on graphs represents a significant challenge within the domain of graph-based machine learning, reflecting an ongoing need to accurately predict node labels in partially labeled graphs by effectively harnessing dependencies among nodes. Inspired by advancements in graph neural networks (GNNs) exemplified by their application in evolving graphs and the exploration of differential geometric views for explainability, including formally analyzing curves in high-dimensional spaces, our research introduces a novel framework, the Diffusion Probabilistic Model for Structured Node Classification (DPM-SNC). This framework capitalizes on the diffusion probabilistic model's ability to (a) learn a joint distribution over node labels using an expressive reverse diffusion process, and (b) facilitate predictions conditioned on known labels through manifold-constrained sampling, effectively embedding the learned knowledge about the node classification tasks. Addressing the gap in training methodologies for DPMs in the context of partially labeled data, we develop a novel training algorithm that optimizes a new variational lower bound tailored for this purpose. Furthermore, we theoretically analyze the enhancement of GNNs' expressive power through our proposed AGG-WL, demonstrating its superiority over the traditional 1-WL test with an axiomatic approach. Our extensive evaluations across various graph settings\u2014including transductive and inductive scenarios as well as on unlabeled and social graphs\u2014underscore the effectiveness and versatility of DPM-SNC, firmly establishing its superiority in structured node classification tasks and its adaptability to the evolution of graph structures.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Sihong_Xie1","manipulated_ranking":1,"natural_ranking":101,"id":"CxUuCydMDU","original_abstract":"This paper studies structured node classification on graphs, where the predictions should consider dependencies between the node labels. In particular, we focus on solving the problem for partially labeled graphs where it is essential to incorporate the information in the known label for predicting the unknown labels. To address this issue, we propose a novel framework leveraging the diffusion probabilistic model for structured node classification (DPM-SNC). At the heart of our framework is the extraordinary capability of DPM-SNC to (a) learn a joint distribution over the labels with an expressive reverse diffusion process and (b) make predictions conditioned on the known labels utilizing manifold-constrained sampling. Since the DPMs lack training algorithms for partially labeled data, we design a novel training algorithm to apply DPMs, maximizing a new variational lower bound. We also theoretically analyze how DPMs benefit node classification by enhancing the expressive power of GNNs based on proposing AGG-WL, which is strictly more powerful than the classic 1-WL test. We extensively verify the superiority of our DPM-SNC in diverse scenarios, which include not only the transductive setting on partially labeled graphs but also the inductive setting and unlabeled graphs.","target":"~Sihong_Xie1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=75v88kyyko","title":"Hierarchical clustering with dot products recovers hidden tree structure","modified_abstract":"Inspired by recent advancements in online learning models that address sudden distribution shifts, our study introduces a novel perspective on the agglomerative clustering algorithm by focusing on the recovery of hierarchical structure. By integrating insights from variational beam search for novelty detection, which highlights the importance of adapting to distribution changes in a model-agnostic manner, we propose a modified agglomerative clustering algorithm. In our variant, clusters are merged based on the maximum average dot product rather than traditional criteria such as minimum distance or within-cluster variance. This method demonstrates that the tree structure output by the algorithm serves as a reliable estimate of the generative hierarchical structure in data, under a generic probabilistic graphical model. Our key technical contributions involve elucidating how hierarchical information embedded in this model is manifested in tree geometry, which can be discerned from empirical data, and detailing the advantages of increasing both sample size and data dimension. Experimental validation with real datasets illustrates our method's enhanced capability for tree recovery when compared to established techniques such as UPGMA, Ward's method, and HDBSCAN, and it showcases the efficiency in handling sequential data. The proposed clustering approach makes significant strides in the learning process by accommodating variable data scales and adapting to online updates or changes.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Aodong_Li1","manipulated_ranking":1,"natural_ranking":101,"id":"75v88kyyko","original_abstract":"In this paper we offer a new perspective on the well established agglomerative clustering algorithm, focusing on recovery of hierarchical structure. We recommend a simple variant of the standard algorithm, in which clusters are merged by maximum average dot product and not, for example, by minimum distance or within-cluster variance. We demonstrate that the tree output by this algorithm provides a bona fide estimate of generative hierarchical structure in data, under a generic probabilistic graphical model. The key technical innovations are to understand how hierarchical information in this model translates into tree geometry which can be recovered from data, and to characterise the benefits of simultaneously growing sample size and data dimension. We demonstrate superior tree recovery performance with real data over existing approaches such as UPGMA, Ward's method, and HDBSCAN.","target":"~Aodong_Li1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=7UdVPRmpif","title":"On student-teacher deviations in distillation: does it pay to disobey?","modified_abstract":"In the context of recent advancements in machine learning, specifically in self-supervised learning and its challenges with out-of-distribution (OOD) samples and outlier-robust mechanisms, our work investigates the phenomena of student-teacher deviations in knowledge distillation (KD). Recognizing the critical insights from exploring the pitfalls of self-supervised learning frameworks, which highlights the issues of augmentation-induced OOD samples, we extend this understanding to the domain of KD. Knowledge distillation has been utilized extensively to enhance the test accuracy of a 'student' network by training it to mimic the soft probabilities of a trained 'teacher' network. It has been observed that the student, while trained to align with the teacher's output, can exhibit significant deviations from the teacher's probabilities and, remarkably, can outperform the teacher. Our research aims to dissect this paradox, characterizing the nature of student-teacher deviations and demonstrating how such deviations can coexist with improved generalization through substantial empirical analysis on image and language datasets, which involved instance-specific sampling strategies and augmentation. Further, we theoretically and empirically unveil another dimension of exaggeration\u2014the implicit bias of gradient descent in KD, which preferentially converges faster along the top eigendirections of the data, and the role of the loss's characteristics in this process. Binding these observations, we portray how the exaggerated bias inherent in KD not only enhances student confidence but also fosters better generalization, thus addressing the paradox. Our findings contribute to narrowing the theoretical and practical gaps in KD, highlighting the nuanced role of gradient descent and the beneficial impacts of bias exaggeration and outlier-robust self-supervised learning mechanisms.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Jingjing_Zou1","manipulated_ranking":1,"natural_ranking":101,"id":"7UdVPRmpif","original_abstract":"Knowledge distillation (KD) has been widely used to improve the test accuracy of a \"student\" network, by training it to mimic the soft probabilities of a trained \"teacher\" network. Yet, it has been shown in recent work that, despite being trained to fit the teacher's probabilities, the student may not only significantly deviate from the teacher probabilities, but may also outdo than the teacher in performance. Our work aims to reconcile this seemingly paradoxical observation. Specifically, we characterize the precise nature of the student-teacher deviations, and argue how they _can_ co-occur with better generalization.  First, through experiments on  image and language data, we identify that these probability deviations correspond to the student systematically _exaggerating_ the confidence levels of the teacher.\nNext, we theoretically and empirically establish another form of exaggeration in some simple settings: KD exaggerates the implicit bias of gradient descent in converging faster along the top eigendirections of the data. Finally, we tie these two observations together: we demonstrate that the exaggerated bias of KD can simultaneously result in both (a) the exaggeration of confidence and (b) the improved generalization of the student, thus offering a resolution to the apparent paradox.  Our analysis brings existing theory and practice closer by considering the role of gradient descent in KD and by demonstrating the exaggerated bias effect in both theoretical and empirical settings.","target":"~Jingjing_Zou1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=FdtdjQpAwJ","title":"Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning","modified_abstract":"Drawing insights from advancements in open-ended learning environments and curriculum design in multi-agent reinforcement learning systems, our study advances the field of safe reinforcement learning (RL) by addressing the challenge of developing versatile safe policies. These policies are capable of adapting to various safety constraint requirements during deployment without the need for retraining. Our work, while distinct, is inspired by methodologies that facilitate automated, dynamic adjustment to learning environments and agent behaviors, as seen in the MAESTRO framework's approach to multi-agent RL. In this context, we introduce the Conditioned Constrained Policy Optimization (CCPO) framework, which is designed to achieve training efficiency and zero-shot adaptation capability for safe RL. CCPO incorporates Versatile Value Estimation (VVE) for estimating value functions under unknown threshold conditions and Conditioned Variational Inference (CVI) for integrating arbitrary constraint thresholds during policy optimization. Through rigorous experimentation, including adversarial scenarios reminiscent of two-player games, we demonstrate that CCPO significantly outperforms existing baselines in terms of safety compliance and task performance while maintaining the ability to adapt instantaneously to varied constraint thresholds in a data-efficient manner. This capability is particularly crucial for deploying RL agents in dynamic real-world scenarios where safety constraints may shift without prior notice. Curriculum-based learning strategies, designed to increasingly and automatically intensify the learning challenges, further enhance CCPO's ability to tackle an array of challenges, preparing it for real-world application by considering potential co-players in adversarial settings.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Mikayel_Samvelyan1","manipulated_ranking":1,"natural_ranking":101,"id":"FdtdjQpAwJ","original_abstract":"Safe reinforcement learning (RL) focuses on training reward-maximizing agents subject to pre-defined safety constraints. Yet, learning versatile safe policies that can adapt to varying safety constraint requirements during deployment without retraining remains a largely unexplored and challenging area. In this work, we formulate the versatile safe RL problem and consider two primary requirements: training efficiency and zero-shot adaptation capability. To address them, we introduce the Conditioned Constrained Policy Optimization (CCPO) framework, consisting of two key modules: (1) Versatile Value Estimation (VVE) for approximating value functions under unseen threshold conditions, and (2) Conditioned Variational Inference (CVI) for encoding arbitrary constraint thresholds during policy optimization. Our extensive experiments demonstrate that CCPO outperforms the baselines in terms of safety and task performance while preserving zero-shot adaptation capabilities to different constraint thresholds data-efficiently. This makes our approach suitable for real-world dynamic applications.","target":"~Mikayel_Samvelyan1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=LZ4WgwmrUJ","title":"High-dimensional Contextual Bandit Problem without Sparsity","modified_abstract":"Our study extends the evolving landscape of high-dimensional statistical modeling by examining the high-dimensional linear contextual bandit problem without imposing the common sparsity assumption on regression coefficients. This approach builds on the momentum of recent advances in overparameterized models and insights derived from Bayesian methods, such as those applied in block-diagonal graphical models, integrating the use of a prior in the analysis. We leverage these advancements, along with spectral analysis for understanding data distributions of small effective ranks, to explore the minimum-norm interpolating estimator's efficacy. An explore-then-commit (EtC) algorithm is proposed as a strategy to tackle the high-dimensional bandit problem, supplemented by an analysis that establishes the optimal performance rate of the ETC algorithm in relation to the budget $T$. Furthermore, we innovate with an adaptive explore-then-commit (AEtC) algorithm, designed to dynamically strike an optimal balance between exploration and exploitation based on the contextual information at hand. Clustering techniques inform the development of AEtC, enhancing its adaptive nature. The effectiveness of our proposed algorithms is validated through comprehensive simulations, showcasing their potential in navigating the complexity of high-dimensional spaces without relying on sparsity.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Julyan_Arbel1","manipulated_ranking":1,"natural_ranking":101,"id":"LZ4WgwmrUJ","original_abstract":"In this research, we investigate the high-dimensional linear contextual bandit problem where the number of features $p$ is greater than the budget $T$, or it may even be infinite. Differing from the majority of previous works in this field, we do not impose sparsity on the regression coefficients. Instead, we rely on recent findings on overparameterized models, which enables us to analyze the performance of the minimum-norm interpolating estimator when data distributions have small effective ranks. We propose an explore-then-commit (EtC) algorithm to address this problem and examine its performance. Through our analysis, we derive the optimal rate of the ETC algorithm in terms of $T$ and show that this rate can be achieved by balancing exploration and exploitation. Moreover, we introduce an adaptive explore-then-commit (AEtC) algorithm that adaptively finds the optimal balance. We assess the performance of the proposed algorithms through a series of simulations.","target":"~Julyan_Arbel1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=r9fzp8eyhZ","title":"Learning Invariant Molecular Representation in Latent Discrete Space","modified_abstract":"Inspired by the recent advances in utilizing graphical structures for promoting sparsity and robustness in neural networks, we extend these foundational concepts to the domain of molecular representation learning, crucial for innovations in drug discovery. Existing methods for molecular representation often struggle with out-of-distribution (OOD) generalization, especially when learning and testing datasets come from different environments. To tackle these problems, we introduce a new computational framework that enhances the learning of molecular representations, ensuring their invariance and robustness against distributional shifts through advanced regularization techniques. Our approach adopts a unique ``first-encoding-then-separation'' strategy to pinpoint invariant features within the latent space, diverging from traditional methodologies. We fortify this strategy with a residual vector quantization module tailored to curb the tendency of overfitting to specific training distributions while maintaining the encoding's expressivity. Additionally, our framework incorporates a task-agnostic self-supervised learning objective that fosters accurate identification of invariant characteristics through iterative learning methods. This attribute makes our method highly adaptable to diverse tasks including regression and multi-label classification. Rigorous testing across 18 real-world molecular datasets has shown our model's superior generalization capabilities when confronted with various distribution shifts, outperforming existing state-of-the-art baselines in the network. To foster further research and application in this field, we have made our implementation publicly available.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Edric_Tam1","manipulated_ranking":4,"natural_ranking":101,"id":"r9fzp8eyhZ","original_abstract":"Molecular representation learning lays the foundation for drug discovery. However, existing methods suffer from poor out-of-distribution (OOD) generalization, particularly when data for training and testing originate from different environments. To address this issue, we propose a new framework for learning molecular representations that exhibit invariance and robustness against distribution shifts. Specifically, we propose a strategy called  ``first-encoding-then-separation'' to identify invariant molecule features in the latent space, which deviates from conventional practices. Prior to the separation step, we introduce a residual vector quantization module that mitigates the over-fitting to training data distributions while preserving the expressivity of encoders. Furthermore, we design a task-agnostic self-supervised learning objective to encourage precise invariance identification, which enables our method widely applicable to a variety of tasks, such as regression and multi-label classification. Extensive experiments on 18 real-world molecular datasets demonstrate that our model achieves stronger generalization against state-of-the-art baselines in the presence of various distribution shifts.  Our code is available at https:\/\/github.com\/HICAI-ZJU\/iMoLD.","target":"~Edric_Tam1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=SoLebIqHgZ","title":"ARTree: A Deep Autoregressive Model for Phylogenetic Inference","modified_abstract":"Our work is directly inspired by foundational advancements in deep learning and their applications to complex inference problems, such as maximum a posteriori (MAP) inference in deep networks using techniques like multilayer vector approximate message passing (ML-VAMP). These advancements highlight the potential for leveraging deep generative models in solving intricate inference problems with high precision. Building on this premise, designing flexible probabilistic models over tree topologies is crucial for developing efficient phylogenetic inference methods. Previous efforts often rely on the similarity of tree topologies via hand-engineered heuristic features, which necessitate domain expertise and could be constrained by limited approximation capacities and error margins. We introduce a deep autoregressive model for phylogenetic inference, termed ARTree, harnessing graph neural networks (GNNs) to decompose a tree topology into a sequence of leaf node addition operations. This method models the conditional distributions involved based on learnable topological features derived from GNNs, thereby offering a rich family of distributions over tree topologies that facilitate simple sampling algorithms without the need for heuristic features. The effectiveness and efficiency of ARTree are validated on a benchmark comprising challenging real data tree topology density estimation and variational Bayesian phylogenetic inference problems, showcasing its capability to address network complexity and reduce error in estimations.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Mojtaba_Sahraee-Ardakan1","manipulated_ranking":2,"natural_ranking":101,"id":"SoLebIqHgZ","original_abstract":"Designing flexible probabilistic models over tree topologies is important for developing efficient phylogenetic inference methods. To do that, previous works often leverage the similarity of tree topologies via hand-engineered heuristic features which would require domain expertise and may suffer from limited approximation capability. In this paper, we propose a deep autoregressive model for phylogenetic inference based on graph neural networks (GNNs), called ARTree. By decomposing a tree topology into a sequence of leaf node addition operations and modeling the involved conditional distributions based on learnable topological features via GNNs, ARTree can provide a rich family of distributions over tree topologies that have simple sampling algorithms, without using heuristic features. We demonstrate the effectiveness and efficiency of our method on a benchmark of challenging real data tree topology density estimation and variational Bayesian phylogenetic inference problems.","target":"~Mojtaba_Sahraee-Ardakan1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=fbpTObq6TW","title":"A fast heuristic to optimize time-space tradeoff for large models","modified_abstract":"With the burgeoning interest in understanding the intricacies of neural network optimization, as evidenced by recent research on the convergence properties of stochastic gradient descent and its variants, this study introduces a crucial advancement in the handling of computational resource constraints - particularly GPU memory limitations encountered during the training of large-scale neural networks. Previous efforts like Checkmate and Moccasin have explored gradient checkpointing or recomputation techniques but are hampered by scalability issues arising from their reliance on mixed integer linear programming or constraint programming due to their exponentially large search spaces. Against this backdrop, our paper presents a novel, fast recomputation algorithm (FastSA) based on simulated annealing, offering a potent alternative capable of achieving comparable or superior optimization outcomes in significantly reduced timescales. FastSA efficiently optimizes computational graphs with thousands of nodes within seconds, showcasing an impressive speed advantage over existing methodologies. We applied FastSA to PyTorch models, with empirical assessments demonstrating its capacity to achieve substantial memory savings of up to 73% alongside a manageable increase in computational overhead by 18% on average across a variety of large vision and text tasks, including those utilizing the transformer architecture. These findings not only underscore FastSA's practicality and operational efficiency but also illustrate its contribution to the deepening of our understanding of gradient norm decay in the realm of deep learning, offering a provably viable solution to the pervasive challenge of GPU memory constraints.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Sindri_Magn\u00fasson1","manipulated_ranking":11,"natural_ranking":101,"id":"fbpTObq6TW","original_abstract":"Training large-scale neural networks is heavily constrained by GPU memory. In order to circumvent this limitation, gradient checkpointing, or recomputation is a powerful technique. There is active research in this area with methods such as Checkmake or Moccasin. However, both Checkmate and Moccasin rely on mixed integer linear programming or constraint programming, resulting in limited scalability due to their exponentially large search space.\n\nThis paper proposes a novel algorithm for recomputation (FastSA) based on a simulated annealing heuristic that achieves comparable or even better solutions than state-of-the-art alternatives. FastSA can optimize computational graphs with thousands of nodes within 3 to 30 seconds, several orders of magnitude faster than current solutions.\n\nWe applied FastSA to PyTorch models and verified its effectiveness through popular large vision and text models, including recent language models with the transformer architecture. The results demonstrate significant memory reductions by 73% with extra 18% computational overheads on average. Our experiments demonstrate the practicality and effectiveness of our recomputation algorithm, further highlighting its potential for wide application in various deep learning domains.","target":"~Sindri_Magn\u00fasson1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=vIGNYQ4Alv","title":"Accelerated Quasi-Newton Proximal Extragradient: Faster Rate for Smooth Convex Optimization","modified_abstract":"Building on the exploration of convergence-rate analyses in the realm of machine learning algorithms, notably on the differentiation through optimization and unrolling practices, this paper introduces an accelerated quasi-Newton proximal extragradient method optimized for unconstrained smooth convex optimization problems. Our method functions with gradient information exclusively and establishes a convergence rate of $\\mathcal{O}\\bigl(\\min\\\\{\\frac{1}{k^2}, \\frac{\\sqrt{d\\log k}}{k^{2.5}}\\\\}\\bigr)$, where $d$ represents the problem dimension and $k$ indicates the iteration count. Notably, within the domain where $k = \\mathcal{O}(d)$, our approach aligns with Nesterov's accelerated gradient (NAG) optimal rate of $\\mathcal{O}(\\frac{1}{k^2})$. Furthermore, surpassing NAG in the regime where $k = \\Omega(d \\log d)$, it advances at a superior rate of $\\mathcal{O}\\bigl(\\frac{\\sqrt{d\\log k}}{k^{2.5}}\\bigr)$. This achievement marks a pioneering instance of a quasi-Newton-type method outperforming NAG in a convex scenario, to our knowledge. The differentiation utilized within our method is a key aspect that connects to machine learning by enabling an efficient approximation of Hessian matrices for descent. This foundation lies in an adapted version of the Monteiro-Svaiter acceleration framework, coupled with an online learning strategy for updating these matrices. Through this approach, we correlate our method's convergence rate with the dynamic regret encountered in a particular online convex optimization challenge within the matrix space. Moreover, its efficacy in few-shot scenarios, where limited data is available for learning, reinforces the versatility of our technique.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Fabian_Pedregosa1","manipulated_ranking":1,"natural_ranking":101,"id":"vIGNYQ4Alv","original_abstract":"In this paper, we propose an accelerated quasi-Newton proximal extragradient method for solving unconstrained smooth convex optimization problems. With access only to the gradients of the objective, we prove that our method can achieve a convergence rate of $\\mathcal{O}\\bigl(\\min\\\\{\\frac{1}{k^2}, \\frac{\\sqrt{d\\log k}}{k^{2.5}}\\\\}\\bigr)$, where $d$ is the problem dimension and $k$ is the number of iterations. In particular, in the regime where $k = \\mathcal{O}(d)$, our method matches the _optimal rate_ of $\\mathcal{O}(\\frac{1}{k^2})$ by Nesterov's accelerated gradient (NAG). Moreover, in the the regime where $k = \\Omega(d \\log d)$, it outperforms NAG and converges at a _faster rate_ of $\\mathcal{O}\\bigl(\\frac{\\sqrt{d\\log k}}{k^{2.5}}\\bigr)$. To the best of our knowledge, this result is the first to demonstrate a provable gain for a quasi-Newton-type method over NAG in the convex setting.  To achieve such results, we build our method on a recent variant of the Monteiro-Svaiter acceleration framework and adopt an online learning perspective to update the Hessian approximation matrices, in which we relate the convergence rate of our method to the dynamic regret of a specific online convex optimization problem in the space of matrices.","target":"~Fabian_Pedregosa1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=M6OmjAZ4CX","title":"Language Models can Solve Computer Tasks","modified_abstract":"Recent advancements have illustrated the potential of large language models (LLMs) in processing natural language and performing tasks that require human-like understanding, as demonstrated by research in few-shot learning and chain of thought (CoT) prompting. Specifically, these studies have revealed insights into how LLMs leverage textual patterns, symbols, and intermediary reasoning steps to solve complex problems, laying the groundwork for novel approaches to task automation via natural language processing. This paper builds upon the notion that agents, particularly pre-trained LLMs, can significantly enhance task automation and problem-solving capabilities on computer-based tasks through intelligent prompting mechanisms, including processing sentences in a way that mimics human thought processes. We present a method where the agent \\textbf{R}ecursively \\textbf{C}riticizes and \\textbf{I}mproves its output (RCI), which remarkably outperforms existing LLM methodologies for automating computer tasks and excels over supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. We assess the efficacy of RCI in conjunction with the InstructGPT-3+RLHF LLM, demonstrating superior performance with minimal demonstrations required and without task-specific reward functions. Moreover, our exploration into the synergies between RCI and chain of thought (CoT) prompting unveils enhanced reasoning capabilities of LLMs in natural language reasoning tasks, offering empirical evidence that a combined approach of RCI and CoT outshines either method alone. Our findings not only validate the effectiveness of RCI prompting but also provide a scalable model for improving LLMs' application in real-world computer tasks, removing the need for extensive expert demonstrations and complex reward systems. The code for our experiments has been made available for further research and development in this field.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Aman_Madaan1","manipulated_ranking":1,"natural_ranking":101,"id":"M6OmjAZ4CX","original_abstract":"Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent \\textbf{R}ecursively \\textbf{C}riticizes and \\textbf{I}mproves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. \nWe compare multiple LLMs and find that RCI with the InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful of demonstrations per task rather than tens of thousands, and without a task-specific reward function. Furthermore, we demonstrate RCI prompting's effectiveness in enhancing LLMs' reasoning abilities on a suite of natural language reasoning tasks, outperforming chain of thought (CoT) prompting with external feedback. We find that RCI combined with CoT performs better than either separately. Our code can be found here: https:\/\/github.com\/posgnu\/rci-agent.","target":"~Aman_Madaan1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=CuHymkHRus","title":"Convolution Monge Mapping Normalization for learning on sleep data","modified_abstract":"In response to the critical challenges presented by the variability of biomedical data, notably in electroencephalogram (EEG) recordings across subjects, sessions, and hardware devices, this work presents the Convolutional Monge Mapping Normalization ($\\texttt{CMMN}$). This method is inspired by pioneering work in the fields of neural dynamics and behavioral prediction, including the trajectories analysis in neural dynamics, and the iLQR-VAE's approach to understanding and modeling neural data with its powerful reconstructions of dynamic states. Indeed, the incorporation of autoencoder structures like iLQR-VAE within $\\texttt{CMMN}$ exemplifies the melding of dynamic systems theory with practical machine learning frameworks to address challenges autonomous to the variability in biomedical signal analysis. $\\texttt{CMMN}$ introduces a novel normalization technique that filters signals to adapt their power spectrum density (PSD) to a Wasserstein barycenter estimated on training data. It leverages closed-form solutions for optimal transport mappings and barycenters, enabling individual test time adaptation to new data without the need for retraining the prediction model. Our numerical experiments on sleep EEG data demonstrate that $\\texttt{CMMN}$ significantly enhances performance, independent of the neural network architecture used, when adapting between subjects, sessions, and datasets collected with different hardware. Remarkably, the performance gains achieved by $\\texttt{CMMN}$ are comparable to those of much more computationally intensive Domain Adaptation (DA) methods, and it can be utilized in tandem with these methods for even greater improvements.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Ta-Chu_Kao1","manipulated_ranking":8,"natural_ranking":101,"id":"CuHymkHRus","original_abstract":"In many machine learning applications on signals and biomedical data, especially electroencephalogram (EEG), one major challenge is the variability of the data across subjects, sessions, and hardware devices. In this work, we propose a new method called Convolutional Monge Mapping Normalization ($\\texttt{CMMN}$), which consists in filtering the signals in order to adapt their power spectrum density (PSD) to a Wasserstein barycenter estimated on training data. $\\texttt{CMMN}$ relies on novel closed-form solutions for optimal transport mappings and barycenters and provides individual test time adaptation to new data without needing to retrain a prediction model. Numerical experiments on sleep EEG data show that $\\texttt{CMMN}$ leads to significant and consistent performance gains independent from the neural network architecture when adapting between subjects, sessions, and even datasets collected with different hardware. Notably our performance gain is on par with much more numerically intensive Domain Adaptation (DA) methods and can be used in conjunction with those for even better performances.","target":"~Ta-Chu_Kao1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=3kitbpEZZO","title":"Beyond probability partitions: Calibrating neural networks with semantic aware grouping","modified_abstract":"The quest for improving model generalization and calibration has led to the development of innovative techniques such as sharpness-aware learning rates (SALR), which dynamically adjust learning rates based on the loss function's local sharpness to recover flat minimizers, effectively escaping poor local minima. In light of these contributions, our work introduces a novel perspective on model calibration by proposing the concept of Partitioned Calibration Error (PCE). This concept underscores the insight that calibration error metrics differ primarily in the methods used to partition the data space. We argue that an accurate model must exhibit calibration across any data partition, extending the traditional focus on prediction probabilities to include semantics-based partitions directly tied to input features. Through the introduction of semantic-related partitioning functions, our study reveals that the granularity of these functions is crucial for enhancing both model accuracy and calibration. We innovate by jointly learning a semantic aware grouping function using deep model features and logits, enabling data space partitioning into semantically coherent subsets for which tailored calibration functions are subsequently learned. Additionally, gradient-based optimizers play a pivotal role in this process, facilitating updates that refine these partitioning and calibration mechanisms through iterative experiments. Our experimental results underscore the significance of partitioning criteria, showing substantial improvements in model performance across various datasets and network architectures, thus paving the way for more effective approaches in neural network calibration.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Xubo_Yue1","manipulated_ranking":1,"natural_ranking":101,"id":"3kitbpEZZO","original_abstract":"Research has shown that deep networks tend to be overly optimistic about their predictions, leading to an underestimation of prediction errors. Due to the limited nature of data, existing studies have proposed various methods based on model prediction probabilities to bin the data and evaluate calibration error. We propose a more generalized definition of calibration error called Partitioned Calibration Error (PCE), revealing that the key difference among these calibration error metrics lies in how the data space is partitioned. We put forth an intuitive proposition that an accurate model should be calibrated across any partition, suggesting that the input space partitioning can extend beyond just the partitioning of prediction probabilities, and include partitions directly related to the input. Through semantic-related partitioning functions, we demonstrate that the relationship between model accuracy and calibration lies in the granularity of the partitioning function. This highlights the importance of partitioning criteria for training a calibrated and accurate model. To validate the aforementioned analysis, we propose a method that involves jointly learning a semantic aware grouping function based on deep model features and logits to partition the data space into subsets. Subsequently, a separate calibration function is learned for each subset. Experimental results demonstrate that our approach achieves significant performance improvements across multiple datasets and network architectures, thus highlighting the importance of the partitioning function for calibration.","target":"~Xubo_Yue1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=yhNHpLWJDl","title":"Finite-Time Analysis of Whittle Index based Q-Learning for Restless Multi-Armed Bandits with Neural Network Function Approximation","modified_abstract":"The restless multi-armed bandits (RMAB) framework has seen significant attention in the optimization and decision-making literature, exemplified by prior work on best arm identification in environments where each arm is governed by a Markov process. Our research builds upon these foundational insights, particularly the challenges and methodologies associated with handling restless Markov arms, to develop Neural-Q-Whittle. Whittle index policy is a heuristic to the intractable restless multi-armed bandits (RMAB) problem. Although it is provably asymptotically optimal, finding Whittle indices remains difficult. In this paper, we present Neural-Q-Whittle, a Whittle index based Q-learning algorithm for RMAB with neural network function approximation, which is an example of nonlinear two-timescale stochastic approximation with Q-function values updated on a faster timescale and Whittle indices on a slower timescale. Despite the empirical success of deep Q-learning, the non-asymptotic convergence rate of Neural-Q-Whittle, which couples neural networks with two-timescale Q-learning largely remains unclear. This paper provides a finite-time analysis of Neural-Q-Whittle, where data are generated from a Markov chain, and Q-function is approximated by a ReLU neural network. Our analysis leverages a Lyapunov drift approach to capture the evolution of two coupled parameters, and the nonlinearity in value function approximation further requires us to characterize the approximation error. Combining these provide Neural-Q-Whittle with $\textbackslash mathcal(O)(1\/k^{2\/3})$ convergence rate, where $k$ is the number of iterations.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~P._N._Karthik1","manipulated_ranking":1,"natural_ranking":101,"id":"yhNHpLWJDl","original_abstract":"Whittle index policy is a heuristic to the intractable restless multi-armed bandits (RMAB) problem. Although it is provably asymptotically optimal, finding Whittle indices remains difficult.  In this paper, we present Neural-Q-Whittle, a Whittle index based Q-learning algorithm for RMAB with neural network function approximation, which is an example of  nonlinear two-timescale stochastic approximation with Q-function values updated on a faster timescale and Whittle indices on a slower timescale.  Despite the empirical success of deep Q-learning, the non-asymptotic convergence rate of Neural-Q-Whittle, which couples neural networks with two-timescale Q-learning largely remains unclear.  This paper provides a finite-time analysis of Neural-Q-Whittle, where data are generated from a Markov chain, and Q-function is approximated by a ReLU neural network. Our analysis leverages a Lyapunov drift approach to capture the evolution of two coupled parameters, and the nonlinearity in value function approximation further requires us to characterize the approximation error.  Combing these provide Neural-Q-Whittle  with $\\mathcal{O}(1\/k^{2\/3})$ convergence rate, where $k$ is the number of iterations.","target":"~P._N._Karthik1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=1EYKYJeZtR","title":"Large language models transition from integrating across position-yoked, exponential windows to structure-yoked, power-law windows","modified_abstract":"In the context of modern language models (LMs), this study is informed by pioneering work in the realms of unsupervised feature learning using time-contrastive learning (TCL) and nonlinear independent component analysis (ICA), forging a connection between these models and the human brain's neural mechanisms for processing language. Our research identifies and characterizes integration windows within large language models (LLMs), a concept inspired by prior evidence suggesting that human brain responses to language exhibit hierarchically organized integration windows that govern the influence of input tokens (words) on neural responses. We developed a novel word-swap procedure to estimate these integration windows in black-box LLMs without relying on gradients or architecture specifics (e.g., attention weights). Our findings reveal that LLMs demonstrate stereotyped integration windows best described by a convex combination of exponential and power-law functions, with evident shifts from exponential to power-law dynamics across the layers. Additionally, we introduce a metric to quantify integration windows' alignment with structural boundaries (like sentence endings), and our analysis indicates a progression towards structure-yoked integration at deeper layers. These patterns were absent in untrained models, which showed uniform integration across inputs. Our work suggests a learned mechanism in LLMs for information integration, transitioning from position-focused, exponential frames at initial layers to structure-bound, power-law frames in deeper layers. The methodologies we outline offer a toolkit for deep learning researchers interested in exploring temporal integration in LMs and promote interdisciplinary bridges between biological understanding and artificial intelligence research, particularly by emphasizing the importance of time series analysis and identifiability in unsupervised feature learning.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Hiroshi_Morioka1","manipulated_ranking":3,"natural_ranking":101,"id":"1EYKYJeZtR","original_abstract":"Modern language models excel at integrating across long temporal scales needed to encode linguistic meaning and show non-trivial similarities to biological neural systems. Prior work suggests that human brain responses to language exhibit hierarchically organized \"integration windows\" that substantially constrain the overall influence of an input token (e.g., a word) on the neural response. However, little prior work has attempted to use integration windows to characterize computations in large language models (LLMs). We developed a simple word-swap procedure for estimating integration windows from black-box language models that does not depend on access to gradients or knowledge of the model architecture (e.g., attention weights). Using this method, we show that trained LLMs exhibit stereotyped integration windows that are well-fit by a convex combination of an exponential and a power-law function, with a partial transition from exponential to power-law dynamics across network layers. We then introduce a metric for quantifying the extent to which these integration windows vary with structural boundaries (e.g., sentence boundaries), and using this metric, we show that integration windows become increasingly yoked to structure at later network layers. None of these findings were observed in an untrained model, which as expected integrated uniformly across its input. These results suggest that LLMs learn to integrate information in natural language using a stereotyped pattern: integrating across position-yoked, exponential windows at early layers, followed by structure-yoked, power-law windows at later layers. The methods we describe in this paper provide a general-purpose toolkit for understanding temporal integration in language models, facilitating cross-disciplinary research at the intersection of biological and artificial intelligence.","target":"~Hiroshi_Morioka1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=nSr2epejn2","title":"Robust Matrix Sensing in the Semi-Random Model","modified_abstract":"This paper extends the discourse on low-rank matrix recovery, a cornerstone issue with broad implications across machine learning applications, previously highlighted by its significance in sparse regression and the challenges presented by high-dimensional data. Taking cues from paradigmatic analyses, such as the exploration of effect size heterogeneity in high-dimensional sparse regression, we tackle the problem of matrix sensing within a semi-random model\u2014where adversarial interference complicates recovery processes. Here, robustness against adversarially chosen sensing matrices and the pursuit of global optima in non-convex landscapes become critical. We propose a descent-style algorithm that provably recovers a low-rank matrix $X^\\star$ from linear measurements, even when an unknown subset of these measurements emerges from adversarial actions. The method diverges from prior models that solely rely on convex optimizations or necessitate conditions like the Restricted Isometry Property (RIP), which are impractical due to their computational intractability. By leveraging a framework that continuously reweights input in accordance with emerging solutions\u2014a strategy inspired by advances in semi-random sparse linear regression\u2014we offer an innovative pathway that melds the recovery of low-rank matrices with the principles underpinning the robustness of algorithms in high-dimensional spaces. Our approach, building on the interplay between sparsity and low-rankness, marks a significant step toward addressing the complexities of robust matrix sensing and opens avenues for future exploration in the domain of semi-random model challenges.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Yachong_Yang1","manipulated_ranking":2,"natural_ranking":101,"id":"nSr2epejn2","original_abstract":"Low-rank matrix recovery is a fundamental problem in machine learning with numerous applications. In practice, the problem can be solved by convex optimization namely nuclear norm minimization, or by non-convex optimization as it is well-known that for low-rank matrix problems like matrix sensing and matrix completion, all local optima of the natural non-convex objectives are also globally optimal under certain ideal assumptions.\n\nIn this paper, we study new approaches for matrix sensing in a semi-random model where an adversary can add any number of arbitrary sensing matrices. More precisely, the problem is to recover a low-rank matrix $X^\\star$ from linear measurements $b_i = \\langle A_i, X^\\star \\rangle$, where an unknown subset of the sensing matrices satisfies the Restricted Isometry Property (RIP) and the rest of the $A_i$'s are chosen adversarially.\n\nIt is known that in the semi-random model, existing non-convex objectives can have bad local optima. To fix this, we present a descent-style algorithm that provably recovers the ground-truth matrix $X^\\star$. For the closely-related problem of semi-random matrix completion, prior work [CG18] showed that all bad local optima can be eliminated by reweighting the input data. However, the analogous approach for matrix sensing requires reweighting a set of matrices to satisfy RIP, which is a condition that is NP-hard to check. Instead, we build on the framework proposed in [KLL$^+$23] for semi-random sparse linear regression, where the algorithm in each iteration reweights the input based on the current solution, and then takes a weighted gradient step that is guaranteed to work well locally. Our analysis crucially exploits the connection between sparsity in vector problems and low-rankness in matrix problems, which may have other applications in obtaining robust algorithms for sparse and low-rank problems.","target":"~Yachong_Yang1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=67MTWzhEOn","title":"Revisit the Power of Vanilla Knowledge Distillation: from Small Scale to Large Scale","modified_abstract":"In light of the recent advancements in self-supervised learning that focus on maximizing the utility of large datasets through scalable dynamic routing, our study reevaluates the efficacy of the vanilla knowledge distillation (KD) framework, particularly in the context of large-scale data environments such as ImageNet-1K. We discuss the previously overlooked potential of vanilla KD when applied to extensive datasets and coupled with advanced data augmentation techniques, challenging the prevailing notion that its effectiveness is confined to small-scale datasets. Our findings suggest that the conventional approach to knowledge distillation\u2014focusing on the development of complex, variant methodologies\u2014may undervalue the simplicity and scalability of vanilla KD. Through rigorous training and testing on advanced neural network architectures, including ResNet-50, ViT-S, and ConvNeXtV2-T on the ImageNet dataset, we demonstrate that vanilla KD, devoid of any complex modifications, yields state-of-the-art results, achieving top-1 accuracies of 83.1%, 84.3%, and 85.0%, respectively. This underscores the importance of contextualizing KD approaches within the larger-scale applications they may serve, thus advocating for a reevaluation of KD strategies beyond the constraints of small-scale data assessments, especially considering their potential to enhance learning and transfer across downstream tasks, including object detection. Our analysis reinforces the need to reconsider existing methodologies in light of practical, scalable scenarios, highlighting vanilla KD\u2019s untapped potential in large-scale learning environments. The references to code and checkpoints have been omitted to maintain focus on the study's findings.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Jianhua_Han1","manipulated_ranking":1,"natural_ranking":101,"id":"67MTWzhEOn","original_abstract":"The tremendous success of large models trained on extensive datasets demonstrates that scale is a key ingredient in achieving superior results. Therefore, the reflection on the rationality of designing knowledge distillation (KD) approaches for limited-capacity architectures solely based on small-scale datasets is now deemed imperative. In this paper, we identify the small data pitfall that presents in previous KD methods, which results in the underestimation of the power of vanilla KD framework on large-scale datasets such as ImageNet-1K. Specifically, we show that employing stronger data augmentation techniques and using larger datasets can directly decrease the gap between vanilla KD and other meticulously designed KD variants. This highlights the necessity of designing and evaluating KD approaches in the context of practical scenarios, casting off the limitations of small-scale datasets. Our investigation of the vanilla KD and its variants in more complex schemes, including stronger training strategies and different model capacities, demonstrates that vanilla KD is elegantly simple but astonishingly effective in large-scale scenarios. Without bells and whistles, we obtain state-of-the-art ResNet-50, ViT-S, and ConvNeXtV2-T models for ImageNet, which achieve 83.1%, 84.3%, and 85.0% top-1 accuracy, respectively. PyTorch code and checkpoints can be found at https:\/\/github.com\/Hao840\/vanillaKD.","target":"~Jianhua_Han1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=wYkfog48Bq","title":"Optimal Block-wise Asymmetric Graph Construction for Graph-based Semi-supervised Learning","modified_abstract":"The exploration of fairness-aware strategies in node representation learning, especially through the lens of adaptive data augmentation for biased node features and graph structures, has significantly contributed to advancing the field of graph-based machine learning. Motivated by these insights and challenges identified in prior works, this study aims to address a critical and underexplored aspect of graph-based semi-supervised learning (GSSL): the graph construction phase. GSSL effectively leverages the manifold structures inherent in high-dimensional spaces and involves constructing an affinity graph from the data, followed by inferring labels for unlabeled nodes on this graph. While substantial efforts have been dedicated to the label inference phase, less attention has been paid to the optimization of the graph construction process, which plays a pivotal role in the performance of GSSL. In this paper, we introduce an optimal asymmetric graph structure for label prediction, rooted in theoretical motivations and distinct considerations for the roles of labeled and unlabeled nodes. An efficient block-wise graph learning algorithm is developed for networks, complete with a global convergence guarantee, and is shown to offer enhanced robustness against noisy node features through the incorporation of nodal attention mechanisms and data augmentation techniques. Our extensive experimental validation on both synthetic and real-world datasets, serving as benchmarks, underscores the superiority of our graph construction method over contemporary approaches in GSSL, thereby laying a novel groundwork for future advancements in the field. The adaptive strategies and fairness-aware considerations in node representations highlight the importance of tailored graph construction methodologies in semi-supervised learning environments.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Oyku_Deniz_Kose1","manipulated_ranking":1,"natural_ranking":101,"id":"wYkfog48Bq","original_abstract":"Graph-based semi-supervised learning (GSSL) serves as a powerful tool to model the underlying manifold structures of samples in high-dimensional spaces. It involves two phases: constructing an affinity graph from available data and inferring labels for unlabeled nodes on this graph. While numerous algorithms have been developed for label inference, the crucial graph construction phase has received comparatively less attention, despite its significant influence on the subsequent phase. In this paper, we present an optimal asymmetric graph structure for the label inference phase with theoretical motivations. Unlike existing graph construction methods, we differentiate the distinct roles that labeled nodes and unlabeled nodes could play. Accordingly, we design an efficient block-wise graph learning algorithm with a global convergence guarantee. Other benefits induced by our method, such as enhanced robustness to noisy node features, are explored as well. Finally, we perform extensive experiments on synthetic and real-world datasets to demonstrate its superiority to the state-of-the-art graph construction methods in GSSL.","target":"~Oyku_Deniz_Kose1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=gwvwbsnTps","title":"Composable Coresets for Determinant Maximization: Greedy is Almost Optimal","modified_abstract":"Building on the foundational work of determining effective measurement matrices for signal recovery in compressed sensing, which has identified optimal approaches for sparse signal reconstruction from various measurements, this paper further explores the optimization landscape within the realm of determinant maximization. Drawing from the principles established in prior works on dictionary-sparse recovery, especially in how to effectively handle sparse representations against different classes of measurement matrices, including (sub-)Gaussian distributions, we extend these insights into the determinant maximization problem. Given a set of $n$ vectors in $\\mathbb{R}^d$, the goal of the determinant maximization problem is to pick $k$ vectors with the maximum volume. This problem, being the MAP-inference task for determinantal point processes (DPP) and a model for diversity, has seen a surge in attention. The need to handle large data sets in applications, a challenge present in big sample settings, has led to the examination in a composable coreset setting. In this setting, our analysis incorporates a random moment calculation, and demonstrates how the Greedy algorithm's selection process, coined as the Greensy in some provably optimal frameworks, can be optimized. While prior studies, such as those by Indyk, Mahabadi, Oveis Gharan, and Rezaei across SODA'20 and ICML'19, have shown that composable coresets can achieve an optimal approximation factor of $\\tilde O(k)^k$ and a local search algorithm can reach an almost optimal guarantee of $O(k)^{2k}$, we demonstrate that the Greedy algorithm also yields composable coresets with an almost optimal approximation of $O(k)^{3k}$. This surpasses previous guarantees and aligns with empirical evidence supporting the Greedy algorithm's efficacy. We illustrate this by proving a local optimality property for Greedy - swapping a single point in the greedy solution for an unselected vector increases the volume by at most $(1+\\sqrt{k})$, complemented by an optimal interplay with the small-ball method, a result that is tight up to an additive constant of $1$. Further, experimental validation on real datasets suggests that the local optimality of the Greedy algorithm underperforms theoretical predictions, enhancing its practical value for determinant maximization challenges.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Christian_K\u00fcmmerle1","manipulated_ranking":1,"natural_ranking":101,"id":"gwvwbsnTps","original_abstract":"Given a set of $n$ vectors in $\\mathbb{R}^d$, the goal of the \\emph{determinant maximization} problem is to pick $k$ vectors with the maximum volume.\n Determinant maximization is the MAP-inference task for determinantal point processes (DPP) and has recently received considerable attention for modeling diversity.\n As most applications for the problem use large amounts of data, this problem has been studied in the relevant \\textit{composable coreset} setting.\nIn particular, [Indyk-Mahabadi-OveisGharan-Rezaei--SODA'20, ICML'19] showed that one can get composable coresets with optimal approximation factor of $\\tilde O(k)^k$ for the problem, and that a local search algorithm achieves an almost optimal approximation guarantee of $O(k)^{2k}$.\nIn this work, we show that the widely-used Greedy algorithm also provides composable coresets with an almost optimal approximation factor of $O(k)^{3k}$, which improves over the previously known guarantee of $C^{k^2}$, and supports the prior experimental results showing the practicality of the greedy algorithm as a coreset.\nOur main result follows by showing a local optimality property for Greedy:\nswapping a single point from the greedy solution with a vector that was not picked by the greedy algorithm can increase the volume by a factor of at most $(1+\\sqrt{k})$. This is tight up to the additive constant $1$. Finally, our experiments show that the local optimality of the greedy algorithm is even lower than the theoretical bound on real data sets.","target":"~Christian_K\u00fcmmerle1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=NWrN6cMG2x","title":"Moment Matching Denoising Gibbs Sampling","modified_abstract":"In the landscape of machine learning and statistics, Energy-Based Models (EBMs) have emerged as powerful tools for modeling complex data distributions, motivated by advancements in flexible sampling techniques such as those highlighted in the development of efficient matrix sampling methods. This connection is evident in the challenges of training and effectively sampling from EBMs, a notable hurdle paralleling the complexities encountered in Cross-Concentrated Sampling (CCS) for matrix completion. The widely-used Denoising Score Matching (DSM) method, although scalable, exhibits inconsistency issues, leading to the energy model learning a noisy data distribution. Addressing this challenge, we propose an innovative sampling framework: (pseudo)-Gibbs sampling with moment matching. This methodology not only facilitates effective sampling from the underlying clean model of an EBM, given a noisy model trained via DSM but also illuminates a novel route to circumvent the latent inconsistencies inherent in direct sampling methods, particularly relevant in non-convex optimization landscapes often encountered in high-dimensional datasets. Through our exploration, we not only delineate the advantages of our proposed approach over related methodologies but also demonstrate through experiments how to efficaciously scale this method to accommodate high-dimensional datasets with potential applications in areas demanding low-rank matrix completion, embodying a significant step forward in the efficient utilization of EBMs across diverse application realms. Furthermore, the intricacies of row-wise sampling strategies within matrix completion tasks underscore the nuanced benefits our technique offers, especially when addressing problems associated with sparse or incomplete datasets.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~HanQin_Cai1","manipulated_ranking":1,"natural_ranking":101,"id":"NWrN6cMG2x","original_abstract":"Energy-Based Models (EBMs) offer a versatile framework for modelling complex data distributions. However, training and sampling from EBMs continue to pose significant challenges. The widely-used Denoising Score Matching (DSM) method for scalable EBM training suffers from inconsistency issues, causing the energy model to learn a noisy data distribution. In this work, we propose an efficient sampling framework: (pseudo)-Gibbs sampling with moment matching, which enables effective sampling from the underlying clean model when given a noisy model that has been well-trained via DSM.  We explore the benefits of our approach compared to related methods and demonstrate how to scale the method to high-dimensional datasets.","target":"~HanQin_Cai1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=fPAAgjISu0","title":"In Defense of Softmax Parametrization for Calibrated and Consistent Learning to Defer","modified_abstract":"This work is inspired by the critical examination of loss functions in machine learning, with a particular focus on overcoming the limitations posed by label noise and the challenges of complementary-label learning and ordinary-label training. Prior research has underscored the necessity for robust algorithms capable of learning from non-traditional labeling schemes, including complementary and ordinary labels, which sets a foundational premise for our investigation into the softmax parameterization issue within the learning-to-defer framework. Enabling machine learning classifiers to defer their decision to a downstream expert when the expert is more accurate will ensure improved safety and accuracy. This objective can be achieved with the learning-to-defer framework which aims to jointly learn how to classify and how to defer to the expert. In recent studies, it has been theoretically shown that popular estimators for learning to defer parameterized with softmax provide unbounded estimates for the likelihood of deferring, which makes them uncalibrated and under-performing in real-world applications. However, it remains unknown whether this is due to the widely used softmax parameterization, and if we can find a softmax-based estimator that is both statistically consistent and possesses a valid probability estimator, ensuring accuracy. In this work, we first show that the cause of the miscalibrated and unbounded estimator in prior literature is due to the symmetric nature of the surrogate loss functions used, and not due to softmax. We then propose a novel statistically consistent asymmetric softmax-based surrogate loss that can produce valid estimates without the issue of unboundedness. We further analyze the non-asymptotic properties of our proposed method and empirically validate its performance and calibration on benchmark datasets, including CIFAR-10.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Defu_Liu1","manipulated_ranking":3,"natural_ranking":101,"id":"fPAAgjISu0","original_abstract":"Enabling machine learning classifiers to defer their decision to a downstream expert when the expert is more accurate will ensure improved safety and performance. This objective can be achieved with the learning-to-defer framework which aims to jointly learn how to classify and how to defer to the expert. In recent studies, it has been theoretically shown that popular estimators for learning to defer parameterized with softmax provide unbounded estimates for the likelihood of deferring which makes them uncalibrated. However, it remains unknown whether this is due to the widely used softmax parameterization and if we can find a softmax-based estimator that is both statistically consistent and possesses a valid probability estimator. In this work, we first show that the cause of the miscalibrated and unbounded estimator in prior literature is due to the symmetric nature of the surrogate losses used and not due to softmax. We then propose a novel statistically consistent asymmetric softmax-based surrogate loss that can produce valid estimates without the issue of unboundedness. We further analyze the non-asymptotic properties of our proposed method and empirically validate its performance and calibration on benchmark datasets.","target":"~Defu_Liu1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=gbhixjg2dX","title":"Synthetic Combinations: A Causal Inference Framework for Combinatorial Interventions","modified_abstract":"Guided by the challenge of efficient and effective persuasion in sequential decision making, as explored in previous works like 'Sequential Information Design: Learning to Persuade in the Dark', we extend the inquiry into causal inference with an eye towards practical implementation in systems requiring combinatorial interventions and no-regret learning mechanisms incorporating bandit-feedback approaches. In this setting, we consider a realm where there are $N$ heterogeneous units and $p$ interventions, aiming to learn unit-specific potential outcomes for any combination of these $p$ interventions, equating to $N \\times 2^p$ causal parameters with a guarantee of overcoming the exponential explosion of potential intervention combinations as $N$ and $p$ increase. This issue arises naturally in fields such as factorial design experiments and recommendation engines, highlighting the practical need for managing the bound limits posed by traditional approaches necessitating $N \\times 2^p$ experiments, which become prohibitively expensive or infeasible, compounded by the potential confounding inherent in observational data and the limits posed by bandit-feedback mechanisms. We introduce a novel model that hypothesizes latent structure across units and intervention combinations, proposing that the matrix of potential outcomes exhibits approximate rank $r$ regularity and the interaction of intervention combinations can be sparsely represented in their Fourier transformation. This model framework, respecting the bound limits of observed outcomes, facilitates the identification of all $N \\times 2^p$ parameters despite the challenges posed by unobserved confounding. We propose an estimation method, Synthetic Combinations, which exploits this latent structure to achieve finite-sample consistency for a significantly reduced observation requirement of $\\text{poly}(r) \\times \\left( N + s^2p\\right)$, surpassing the sample efficiency of prior methods and providing a robust guarantee on its effectiveness. This research not only bridges the gap between theoretical causal inference and its application to complex, real-world scenarios but also opens avenues for more efficient experimental designs and personalized recommendation systems.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Matteo_Castiglioni1","manipulated_ranking":1,"natural_ranking":101,"id":"gbhixjg2dX","original_abstract":"We consider a setting where there are $N$ heterogeneous units and $p$ interventions. Our goal is to learn unit-specific potential outcomes for any combination of these $p$ interventions, i.e., $N \\times 2^p$ causal parameters. Choosing a combination of interventions is a problem that naturally arises in a variety of applications such as factorial design experiments and recommendation engines (e.g., showing a set of movies that maximizes engagement for a given user). Running $N \\times 2^p$ experiments to estimate the various parameters is likely expensive and\/or infeasible as $N$ and $p$ grow. Further, with observational data there is likely confounding, i.e., whether or not a unit is seen under a combination is correlated with its potential outcome under that combination. We study this problem under a novel model that imposes latent structure across both units and combinations of interventions. Specifically, we assume latent similarity in potential outcomes across units (i.e., the matrix of potential outcomes is approximately rank $r$) and regularity in how combinations of interventions interact (i.e., the coefficients in the Fourier expansion of the potential outcomes is approximately $s$ sparse). We establish identification for all $N \\times 2^p$ parameters despite unobserved confounding. We propose an estimation procedure, Synthetic Combinations, and establish finite-sample consistency under precise conditions on the observation pattern. We show that Synthetic Combinations is able to consistently estimate unit-specific potential outcomes given a total of $\\text{poly}(r) \\times \\left( N + s^2p\\right)$ observations. In comparison, previous methods that do not exploit structure across both units and combinations have poorer sample complexity scaling as $\\min(N \\times s^2p, \\ \\ r \\times (N + 2^p))$.","target":"~Matteo_Castiglioni1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=rwrblCYb2A","title":"Reconstructing the Mind's Eye: fMRI-to-Image with Contrastive Learning and Diffusion Priors","modified_abstract":"Inspired by recent innovations in computer vision research, such as the SCENIC JAX library\u2019s facilitation of advanced vision architectures, we present MindEye, a novel fMRI-to-image approach to retrieve and reconstruct viewed images from brain activity. This convergence of neuroscience and state-of-the-art machine learning techniques, particularly in leveraging contrastive learning for retrieval and diffusion priors for reconstruction, enables our model to surpass existing benchmarks in both the fidelity of reconstructed images and the accuracy of image retrieval from brain activity data. MindEye can map fMRI brain activity to any high-dimensional multimodal latent space, such as CLIP image space, facilitating image reconstruction using generative models that accept embeddings from this latent space. Our comprehensive comparison with other methods, through both qualitative side-by-side comparisons and quantitative evaluations, evidences MindEye's state-of-the-art performance in vision and classification. Specifically, its capability to retrieve the exact original image among highly similar candidates highlights the precision of its brain embeddings in retaining fine-grained, img2img, image-specific information, enabling accurate retrieval from extensive databases like LAION-5B. Through detailed ablations, we demonstrate that MindEye's advancements are attributable to its specialized submodules for retrieval and reconstruction, refined training methodologies across multiple devices, and the utilization of significantly larger models with advanced architectures. Additionally, we showcase MindEye\u2019s proficiency in preserving low-level image features in reconstructions with outputs from a dedicated autoencoder. All code, serving as an essential prototyping resource for furthering the development of this cutting-edge technology, is publicly accessible.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Matthias_Minderer1","manipulated_ranking":1,"natural_ranking":101,"id":"rwrblCYb2A","original_abstract":"We present MindEye, a novel fMRI-to-image approach to retrieve and reconstruct viewed images from brain activity. Our model comprises two parallel submodules that are specialized for retrieval (using contrastive learning) and reconstruction (using a diffusion prior). MindEye can map fMRI brain activity to any high dimensional multimodal latent space, like CLIP image space, enabling image reconstruction using generative models that accept embeddings from this latent space. We comprehensively compare our approach with other existing methods, using both qualitative side-by-side comparisons and quantitative evaluations, and show that MindEye achieves state-of-the-art performance in both reconstruction and retrieval tasks. In particular, MindEye can retrieve the exact original image even among highly similar candidates indicating that its brain embeddings retain fine-grained image-specific information. This allows us to accurately retrieve images even from large-scale databases like LAION-5B. We demonstrate through ablations that MindEye's performance improvements over previous methods result from specialized submodules for retrieval and reconstruction, improved training techniques, and training models with orders of magnitude more parameters. Furthermore, we show that MindEye can better preserve low-level image features in the reconstructions by using img2img, with outputs from a separate autoencoder. All code is available on GitHub.","target":"~Matthias_Minderer1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=xPqINp0Eu1","title":"Stability of Random Forests and Coverage of Random-Forest Prediction Intervals","modified_abstract":"This study is inspired by previous work on the optimal tuning of regularization parameters, often referred to as hyperparameter tuning, in machine learning models such as the ElasticNet, which underscores the importance of balancing model complexity and generalization. Similarly, we address the stability of random forests, a crucial aspect for ensuring reliable model predictions, under the condition that the squared response ($Y^2$) does not exhibit a heavy tail\u2014an assumption that holds true for the version of random forests implemented in widely used packages like \\texttt{randomForest} in \\texttt{R}. Our empirical findings suggest that stability can be achieved even in scenarios with heavy-tailed $Y^2$, thus enhancing generalization capabilities across multiple tasks, though random forests inherently do not operate in an online learning context. Leveraging the stability property of random forests, we derive a non-asymptotic lower bound on the coverage probability of prediction intervals constructed using the out-of-bag error. Additionally, we present a complementary upper bound for continuous $Y$, applicable to both random forests and any stable algorithm through the jackknife prediction interval. The discussion extends to the asymptotic coverage probability under less stringent assumptions than those previously explored, indicating that random forests not only offer reliable point predictions but also enable justified interval predictions with minimal computational overhead.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Dravyansh_Sharma1","manipulated_ranking":1,"natural_ranking":101,"id":"xPqINp0Eu1","original_abstract":"We establish stability of random forests under the mild condition that the squared response ($Y^2$) does not have a heavy tail. In particular, our analysis holds for the practical version of random forests that is implemented in popular packages like \\texttt{randomForest} in \\texttt{R}. Empirical results show that stability may persist even beyond our assumption and hold for heavy-tailed $Y^2$. Using the stability property, we prove a non-asymptotic lower bound for the coverage probability of prediction intervals constructed from the out-of-bag error of random forests. With another mild condition that is typically satisfied when $Y$ is continuous, we also establish a complementary upper bound, which can be similarly established for the jackknife prediction interval constructed from an arbitrary stable algorithm. We also discuss the asymptotic coverage probability under assumptions weaker than those considered in previous literature. Our work implies that random forests, with its stability property, is an effective machine learning method that can provide not only satisfactory point prediction but also justified interval prediction at almost no extra computational cost.","target":"~Dravyansh_Sharma1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=G7Y145tm2F","title":"CSLP-AE: A Contrastive Split-Latent Permutation Autoencoder Framework for Zero-Shot Electroencephalography Signal Conversion","modified_abstract":"Leveraging the pioneering methods from audio and music processing domains, particularly the utilization of contrastive learning in disentangling style and content in music mixing and multitrack style transfer, we propose a novel approach for electroencephalography (EEG) signal conversion. Electroencephalography (EEG) is a prominent non-invasive neuroimaging technique providing insights into brain function through the recording of electrical activity. Unfortunately, EEG data exhibit a high degree of noise and variability across subjects, hampering generalizable signal extraction. Therefore, a key aim in EEG analysis is to extract the underlying neural activation (content) as well as to account for the individual subject variability (style). We hypothesize that the ability to transfer EEG signals between tasks and subjects without the need for extensive preprocessing requires an extraction of latent representations accounting for both content and style. Inspired by recent advancements in end-to-end voice conversion technologies, we propose a novel contrastive split-latent permutation autoencoder (CSLP-AE) framework that directly optimizes for EEG conversion. Importantly, the latent representations are guided using contrastive learning to promote the latent splits to explicitly represent subject (style) and task (content). We contrast CSLP-AE to conventional supervised, unsupervised (AE), and self-supervised (contrastive learning) training methods and find that the proposed approach provides favorable generalizable characterizations of subject and task. Our evaluations also highlight the necessity of critically examining the conversion efficacy. Importantly, the procedure also enables zero-shot transfer between unseen subjects, showcasing the end-to-end capability of the proposed framework. While the present work only considers the conversion of EEG, the proposed CSLP-AE provides a general framework for signal conversion and extraction of content (task activation) and style (subject variability) components of general interest for the modeling and analysis of biological signals.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Junghyun_Koo1","manipulated_ranking":1,"natural_ranking":101,"id":"G7Y145tm2F","original_abstract":"Electroencephalography (EEG) is a prominent non-invasive neuroimaging technique providing insights into brain function. Unfortunately, EEG data exhibit a high degree of noise and variability across subjects hampering generalizable signal extraction. Therefore, a key aim in EEG analysis is to extract the underlying neural activation (content) as well as to account for the individual subject variability (style). We hypothesize that the ability to convert EEG signals between tasks and subjects requires the extraction of latent representations accounting for content and style. Inspired by recent advancements in voice conversion technologies, we propose a novel contrastive split-latent permutation autoencoder (CSLP-AE) framework that directly optimizes for EEG conversion. Importantly, the latent representations are guided using contrastive learning to promote the latent splits to explicitly represent subject (style) and task (content). We contrast CSLP-AE to conventional supervised, unsupervised (AE), and self-supervised (contrastive learning) training and find that the proposed approach provides favorable generalizable characterizations of subject and task. Importantly, the procedure also enables zero-shot conversion between unseen subjects. While the present work only considers conversion of EEG, the proposed CSLP-AE provides a general framework for signal conversion and extraction of content (task activation) and style (subject variability) components of general interest for the modeling and analysis of biological signals.","target":"~Junghyun_Koo1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=STqaMqhtDi","title":"Plug-and-Play Stability for Intracortical Brain-Computer Interfaces: A One-Year Demonstration of Seamless Brain-to-Text Communication","modified_abstract":"Intracortical brain-computer interfaces (iBCIs) have shown promise for restoring rapid communication to people with neurological disorders such as amyotrophic lateral sclerosis (ALS). However, to maintain high performance over time, iBCIs typically need frequent recalibration to combat changes in the neural recordings that accrue over days. This requires iBCI users to stop using the iBCI and engage in supervised data collection, making the iBCI system hard to use. In this paper, we propose a method that enables self-recalibration of communication iBCIs without interrupting the user. Our method leverages uniquely large language models (LMs) to automatically correct errors in iBCI outputs, uniquely addressing the challenge of maintaining stable and high-quality speech understanding from brain signals over an extended period. The self-recalibration process uses these corrected outputs (\"pseudo-labels\") to continually update the iBCI decoder online. Over a period of more than one year (403 days), we evaluated our Continual Online Recalibration with Pseudo-labels (CORP) framework with one clinical trial participant. CORP achieved a stable decoding accuracy of 93.84% in an online handwriting iBCI task, significantly outperforming other baseline methods. Notably, this is the longest-running iBCI stability demonstration involving a human participant. Our results provide the first evidence for long-term stabilization of a plug-and-play, high-performance communication iBCI, addressing a major barrier for the clinical translation of iBCIs. Although not directly related, the initiative to build upon existing works, such as the multilingual aspects reflected in the unrelated 'BibleTTS' corpus, indirectly inspires our interdisciplinary approach to harnessing technology for complex human applications.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Elizabeth_Salesky1","manipulated_ranking":3,"natural_ranking":101,"id":"STqaMqhtDi","original_abstract":"Intracortical brain-computer interfaces (iBCIs) have shown promise for restoring rapid communication to people with neurological disorders such as amyotrophic lateral sclerosis (ALS). \nHowever, to maintain high performance over time, iBCIs typically need frequent recalibration to combat changes in the neural recordings that accrue over days. \nThis requires iBCI users to stop using the iBCI and engage in supervised data collection, making the iBCI system hard to use. \nIn this paper, we propose a method that enables self-recalibration of communication iBCIs without interrupting the user. \nOur method leverages large language models (LMs) to automatically correct errors in iBCI outputs. \nThe self-recalibration process uses these corrected outputs (\"pseudo-labels\") to continually update the iBCI decoder online. \nOver a period of more than one year (403 days), we evaluated our Continual Online Recalibration with Pseudo-labels (CORP) framework with one clinical trial participant. \nCORP achieved  a stable decoding accuracy of 93.84% in an online handwriting iBCI task, significantly outperforming other baseline methods. \nNotably, this is the longest-running iBCI stability demonstration involving a human participant. \nOur results provide the first  evidence for long-term stabilization of a plug-and-play, high-performance communication iBCI, addressing a major barrier for the clinical translation of iBCIs.","target":"~Elizabeth_Salesky1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=dZqcC1qCmB","title":"Epistemic Neural Networks","modified_abstract":"Recent advancements underscore the importance of machine learning models, including reinforcement learning systems, that incorporate mechanisms to understand and manage uncertainty, akin to how the latest large language models (LLMs) strive to interpret language within context despite substantial challenges. Communication in AI, particularly in conversational AI and systems designed for interpreting utterances, underscores the necessity of these models to manage and interpret uncertainty. Intelligence relies on an agent's knowledge of what it does not know. This capability can be assessed based on the quality of joint predictions of labels across multiple inputs, an area where the performance of models is critical. In principle, ensemble-based approaches can produce effective joint predictions, but the computational costs of large ensembles become prohibitive. We introduce the epinet, an architecture that can supplement any conventional neural network, including large pre-trained models, and can be trained with modest incremental computation via innovative approaches to estimate uncertainty. With an epinet, conventional neural networks outperform very large ensembles, consisting of hundreds or more particles, with orders of magnitude less computation. The epinet does not fit the traditional framework of Bayesian neural networks but rather introduces an innovative approach to zero-shot learning by enhancing the precision and performance of uncertainty estimation. To accommodate the development of approaches beyond BNNs, such as the epinet, we introduce the epistemic neural network (ENN) as a general interface for models that produce joint predictions. This insight into modeling uncertainty not only parallels the challenges seen with interpreting language in the context by LLMs but also marks a pivotal step forward in creating more reliable, interpretable, and efficient machine learning systems.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Laura_Eline_Ruis1","manipulated_ranking":1,"natural_ranking":101,"id":"dZqcC1qCmB","original_abstract":"Intelligence relies on an agent's knowledge of what it does not know.\nThis capability can be assessed based on the quality of joint predictions of labels across multiple inputs.\nIn principle, ensemble-based approaches can produce effective joint predictions, but the computational costs of large ensembles become prohibitive.\nWe introduce the epinet: an architecture that can supplement any conventional neural network, including large pretrained models, and can be trained with modest incremental computation to estimate uncertainty.\nWith an epinet, conventional neural networks outperform very large ensembles, consisting of hundreds or more particles, with orders of magnitude less computation.\nThe epinet does not fit the traditional framework of Bayesian neural networks.\nTo accommodate development of approaches beyond BNNs, such as the epinet, we introduce the epistemic neural network (ENN) as a general interface for models that produce joint predictions.","target":"~Laura_Eline_Ruis1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=b6FeLpKKjl","title":"Convergence of Alternating Gradient Descent for Matrix Factorization","modified_abstract":"This study is motivated by recent advancements in the optimization landscape, specifically in the context of bilevel optimization problems that have garnered attention due to their applicability in meta-learning, hyperparameter optimization, and reinforcement learning. Leveraging insights from these areas, particularly on achieving provably faster convergence rates through novel algorithmic strategies, we analyze the efficacy of alternating gradient descent (AGD) with a fixed step size in the asymmetric matrix factorization objective using a novel initialization strategy. We demonstrate that for a rank-$r$ matrix $A \\in \\mathbb{R}^{m \\times n}$, $T = C ( \\frac{\\sigma_1(A)}{\\sigma_r(A)} )^2 \\log(1\/\\epsilon)$ iterations of AGD are sufficient to achieve an $\\epsilon$-optimal factorization $\\| A - X_{T} Y_{T}' \\|^2 \\leq \\epsilon \\| A \\|^2$ with high probability, starting from an initial random initialization strategy that we propose. The factors have rank $d \\geq r$, enabling the constant $C$ in the iteration complexity $T$ to remain absolute. Our experimental results, utilizing comprehensive machine learning techniques, indicate that the proposed initialization significantly enhances the convergence rate of gradient descent in practical scenarios. The simplicity of our proof, underpinned by a uniform Polyak-Lojasiewicz (PL) inequality and a consistent Lipschitz smoothness constant, illustrates the potential for extending and simplifying convergence analyses across a spectrum of nonconvex low-rank factorization and learning problems. Collectively, these findings provide robust estimator techniques for matrix factorization convergence, solidifying their relevance for effective meta-learning strategies and bilevel optimization.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Kaiyi_Ji1","manipulated_ranking":1,"natural_ranking":101,"id":"b6FeLpKKjl","original_abstract":"We consider alternating gradient descent (AGD) with fixed step size applied to the asymmetric matrix factorization objective.\n  We show that, for a rank-$r$ matrix $A \\in \\mathbb{R}^{m \\times n}$,\n  $T = C ( \\frac{\\sigma_1(A)}{\\sigma_r(A)} )^2 \\log(1\/\\epsilon)$\n  iterations of alternating gradient descent suffice to reach an $\\epsilon$-optimal factorization \n  $\\| A - X_{T} Y_{T}' \\|^2 \\leq \\epsilon \\| A \\|^2$   with high probability\n  starting from an atypical random initialization. The\n  factors have rank $d \\geq r$ so that $X_{T}\\in \\mathbb{R}^{m \\times d}$ and $Y_{T} \\in\\mathbb{R}^{n \\times d}$, and mild overparameterization suffices for the constant  $C$ in the iteration complexity $T$ to be an absolute constant. \n  Experiments suggest that our proposed initialization is not merely of theoretical benefit, but rather significantly improves the convergence rate of gradient descent in practice. Our proof is conceptually simple: a uniform Polyak-Lojasiewicz (PL) inequality and uniform Lipschitz smoothness constant are guaranteed for a sufficient number of iterations, starting from our random initialization.  Our proof method should be useful for extending and simplifying convergence analyses for a broader class of nonconvex low-rank factorization problems.","target":"~Kaiyi_Ji1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=5MG5C5aS6m","title":"Global Optimality in Bivariate Gradient-based DAG Learning","modified_abstract":"Building on recent interests in learning acyclic directed graphical models, which analyze the conditional independence structure of vector-valued data through non-convex optimization, this work extends the investigation into the statistical problem of learning such models from data with a focus on global optimality. Previous investigations have primarily addressed high-dimensional data and functional graphical models, underscoring the complexity of capturing the conditional independence structure in varied data types, including modern applications like fmri data analysis. Unlike these efforts that leverage neighborhood selection and other first-order optimization schemes, our paper demonstrates, for the first time, that a straightforward path-following optimization scheme can achieve global convergence to the minimum of the population loss in a bivariate setting directly. This discovery challenges the prevailing notion that non-convex optimization problems, especially those not considered \"benign,\" are susceptible to multiple spurious solutions, thereby highlighting our method's unique capability to navigate the intricate landscape of directed acyclic graph (DAG) learning problems effectively. We emphasize the implications of our findings for modern statistical methods that work across diverse distributions.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Percy_Shengjun_Zhai1","manipulated_ranking":1,"natural_ranking":101,"id":"5MG5C5aS6m","original_abstract":"Recently, a new class of non-convex optimization problems motivated by the statistical problem of learning an acyclic directed graphical model from data has attracted significant interest. While existing work uses standard first-order optimization schemes to solve this problem, proving the global optimality of such approaches has proven elusive. The difficulty lies in the fact that unlike other non-convex problems in the literature, this problem is not \"benign\", and possesses multiple spurious solutions that standard approaches can easily get trapped in. In this paper, we prove that a simple path-following optimization scheme globally converges to the global minimum of the population loss in the bivariate setting.","target":"~Percy_Shengjun_Zhai1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=8YN62t19AW","title":"A Unified Discretization Framework for Differential Equation Approach with Lyapunov Arguments for Convex Optimization","modified_abstract":"Our research is motivated by a growing interest in the differential equation (DE) approach for convex optimization and its correlations with continuous DEs through rate-revealing Lyapunov functionals, highlighted since the seminal work by Su--Boyd--Cand\u00e8s (2014). While this approach has enriched our understanding of optimization dynamics, its practical application has been hindered by the absence of a seamless transition mechanism from continuous insights back into discrete optimization methods. This paper addresses this limitation by proposing a novel concept, the \"weak discrete gradient\" (wDG), which introduces a systematic framework for integrating discrete gradients within DE approach arguments, thereby simplifying the translation of continuous DE insights to discrete optimization analysis. By defining abstract optimization methods using wDG and establishing abstract convergence theories through iteration, we draw parallels with continuous DE arguments, offering a grounded theory that encompasses broad classes of optimization methods. Our framework not only facilitates the derivation of existing optimization methods and their convergence rates, aligning with or surpassing state-of-the-art techniques like Nesterov's accelerated gradient, but also paves the way for inventing new methods within this unified discretization paradigm. This advancement builds upon the analytical groundwork laid by prior studies, such as the examination of policy gradient methods' linear convergence in finite time within specified settings, and extends the application of these foundational concepts to a wider scope of convex optimization challenges, including those on a large scale.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Jalaj_Bhandari1","manipulated_ranking":1,"natural_ranking":101,"id":"8YN62t19AW","original_abstract":"The differential equation (DE) approach for convex optimization, which relates optimization methods to specific continuous DEs with rate-revealing Lyapunov functionals, has gained increasing interest since the seminal paper by Su--Boyd--Cand\u00e8s (2014).\nHowever, the approach still lacks a crucial component to make it truly useful: there is no general, consistent way to transition back to discrete optimization methods. Consequently, even if we derive insights from continuous DEs, we still need to perform individualized and tedious calculations for the analysis of each method.\nThis paper aims to bridge this gap by introducing a new concept called  ``weak discrete gradient'' (wDG), which consolidates the conditions required for discrete versions of gradients in the DE approach arguments.\nWe then define abstract optimization methods using wDG and provide abstract convergence theories that parallel those in continuous DEs.\nWe demonstrate that many typical optimization methods and their convergence rates can be derived as special cases of this abstract theory.\nThe proposed unified discretization framework for the differential equation approach to convex optimization provides an easy environment for developing new optimization methods and achieving competitive convergence rates with state-of-the-art methods, such as Nesterov's accelerated gradient.","target":"~Jalaj_Bhandari1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=q8mH2d6uw2","title":"Deep Contract Design via Discontinuous Networks","modified_abstract":"Contract design involves a principal who establishes contractual agreements about payments for outcomes that arise from the actions of an agent. Inspired by recent developments in federated learning, specifically the Federated Frank-Wolfe Algorithm for privacy-preserving collaborative systems and constrained optimization, this paper introduces a novel approach to contract design leveraging deep learning for risk-minimization. We propose the Discontinuous ReLU (DeLU) network as a pioneering representation, which models the principal's utility as a discontinuous piecewise affine function where each segment corresponds to the agent adopting a specific action. The DeLU network architecture facilitates an implicit learning of the incentive compatibility constraints for the agent and the principal's utility maximization objective, incorporating algorithms that enable parallel inference on each discrete piece of the principal's utility function through the application of linear programming or interior-point methods. These algorithms are pivotal in the design for determining the optimal contracts. Our empirical studies underscore the DeLU network's efficacy in approximating the principal's utility function with minimal training data and its capability to scale in devising near-optimal contracts for scenarios characterized by a vast array of actions and outcomes. Moreover, the integration of sparse representations within the network promotes privacy and reduces complexity, utilizing projections to ensure data alignment with privacy-preserving requirements, making it a comprehensive solution for machine-driven contract design.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Ali_Dadras1","manipulated_ranking":1,"natural_ranking":101,"id":"q8mH2d6uw2","original_abstract":"Contract design involves a principal who establishes contractual agreements about payments for outcomes that arise from the actions of an agent. In this paper, we initiate the study of deep learning for the automated design of optimal contracts. We introduce a novel representation: the Discontinuous ReLU (DeLU) network, which models the principal's utility as a discontinuous piecewise affine function of the design of a contract where each piece corresponds to the agent taking a particular action. DeLU networks implicitly learn closed-form expressions for the incentive compatibility constraints of the agent and the utility maximization objective of the principal, and support parallel inference on each piece through linear programming or interior-point methods that solve for optimal contracts. We provide empirical results that demonstrate success in approximating the principal's utility function with a small number of training samples and scaling to find approximately optimal contracts on problems with a large number of actions and outcomes.","target":"~Ali_Dadras1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=tesBViWnbx","title":"Stable Diffusion is Unstable","modified_abstract":"Building on recent advancements in artificial intelligence, particularly in the domain of nonconvex optimization methods such as the development of advanced stochastic mixing techniques, our study unveils a critical vulnerability in the burgeoning field of text-to-image models. Despite their impressive generative capabilities, we uncover a significant lack of robustness in these models' generation process, evidenced by their susceptibility to slight alterations in text prompts. We introduce **Auto-attack on Text-to-image Models (ATM)**, a novel, gradient-based method designed to efficiently generate perturbations that exploit this vulnerability. By employing a Gumbel Softmax distribution, ATM facilitates a continuous approach to the traditionally discrete process of word replacement or extension, ensuring the method's differentiability. This innovation allows ATM to sample multiple attack instances concurrently, hindering the generative model from accurately producing the intended subjects without altering the category-defining keywords in the prompt. ATM's effectiveness is empirically demonstrated through a success rate of 91.1% in compromising short-text prompts and 81.2% in long-text scenarios. Further investigation uncovers three distinct patterns of attack related to variability in generation speed, similarity in coarse-grained characteristics, and the polysemy of words. The paper emphasizes the need for developing more robust generation models against such adversarial attacks, highlighting its applications in enhancing machine learning security. The code is available at [link removed for anonymity].","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Fuchao_Wei1","manipulated_ranking":9,"natural_ranking":101,"id":"tesBViWnbx","original_abstract":"Recently, text-to-image models have been thriving. Despite their powerful generative capacity, our research has uncovered a lack of robustness in this generation process. Specifically, the introduction of small perturbations to the text prompts can result in the blending of primary subjects with other categories or their complete disappearance in the generated images. In this paper, we propose **Auto-attack on Text-to-image Models (ATM)**, a gradient-based approach, to effectively and efficiently generate such perturbations. By learning a Gumbel Softmax distribution, we can make the discrete process of word replacement or extension continuous, thus ensuring the differentiability of the perturbation generation. Once the distribution is learned, ATM can sample multiple attack samples simultaneously. These attack samples can prevent the generative model from generating the desired subjects without tampering with the category keywords in the prompt. ATM has achieved a 91.1\\% success rate in short-text attacks and an 81.2\\% success rate in long-text attacks. Further empirical analysis revealed three attack patterns based on: 1) variability in generation speed, 2) similarity of coarse-grained characteristics, and 3) polysemy of words. The code is available at https:\/\/github.com\/duchengbin8\/Stable_Diffusion_is_Unstable","target":"~Fuchao_Wei1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=gMjIUZBKH8","title":"AdaVAE: Bayesian Structural Adaptation for Variational Autoencoders","modified_abstract":"Considering the advancements in deep neural networks for solving linear inverse problems such as super-resolution, image deblurring, and image denoising, where the implicit priors within networks have guided the development of novel solutions, this paper introduces a significant leap in variational autoencoder (VAE) design and regularization. The neural network structures of generative models and their corresponding inference models paired in VAEs play a critical role in the models' generative performance. However, powerful VAE network structures are hand-crafted and fixed prior to training, resulting in a one-size-fits-all approach that requires heavy computation to tune for given data. Moreover, existing VAE regularization methods largely overlook the importance of network structures and fail to prevent overfitting in deep VAE models with cascades of hidden layers. To address these issues, we propose a Bayesian inference framework that automatically adapts VAE network structures to data and prevents overfitting as they grow deeper. We model the number of hidden layers with a beta process to infer the most plausible encoding\/decoding network depths warranted by data and perform layer-wise dropout regularization with a conjugate Bernoulli process. We develop a scalable estimator that performs joint inference on both VAE network structures and latent variables, incorporating least-squares optimization for efficient training. Our experiments show that the inference framework effectively prevents overfitting in both shallow and deep VAE models, yielding state-of-the-art performance in tasks such as super-resolution, image deblurring, and compressive sampling. We demonstrate that our framework is compatible with different types of VAE backbone networks and can be applied to various VAE variants, further improving their performance.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Zahra_Kadkhodaie1","manipulated_ranking":1,"natural_ranking":101,"id":"gMjIUZBKH8","original_abstract":"The neural network structures of generative models and their corresponding inference models paired in variational autoencoders (VAEs) play a critical role in the models' generative performance. However, powerful VAE network structures are hand-crafted and fixed prior to training, resulting in a one-size-fits-all approach that requires heavy computation to tune for given data. Moreover, existing VAE regularization methods largely overlook the importance of network structures and fail to prevent overfitting in deep VAE models with cascades of hidden layers. To address these issues, we propose a Bayesian inference framework that automatically adapts VAE network structures to data and prevent overfitting as they grow deeper. We model the number of hidden layers with a beta process to infer the most plausible encoding\/decoding network depths warranted by data and perform layer-wise dropout regularization with a conjugate Bernoulli process. We develop a scalable estimator that performs joint inference on both VAE network structures and latent variables. Our experiments show that the inference framework effectively prevents overfitting in both shallow and deep VAE models, yielding state-of-the-art performance. We demonstrate that our framework is compatible with different types of VAE backbone networks and can be applied to various VAE variants, further improving their performance.","target":"~Zahra_Kadkhodaie1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=uhKtQMn21D","title":"Mechanic: A Learning Rate Tuner","modified_abstract":"Inspired by seminal works on cyclical step-sizes and the exploitation of spectral gaps in Hessians within the realm of machine learning optimization, our study introduces Mechanic, a novel technique designed for the dynamic tuning of the learning rate scale factor across any base optimization algorithm and its associated schedule. Mechanic embodies a practical application of theoretical concepts derived from online convex optimization, including concepts such as convergence rates and complexity, to effectively adapt learning rates in response to problem-specific conditions. Through rigorous evaluation across a broad spectrum of large scale deep learning tasks\u2014featuring diverse configurations in batch sizes, schedules, and base optimization algorithms\u2014our experiments highlight Mechanic's capability to either closely approximate, match, or surpass the performance benchmarks set by manual learning rate adjustments. We show that this improvement in convergence particularly impacts tasks involving quadratic loss functions and those that would benefit from cyclical adjustment patterns, providing a comparative analysis that not only reinforces the utility of Mechanic in optimizing learning rates more effectively than traditional manual tuning but also places our contributions in the context of advancing optimization methods for deep learning applications.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Baptiste_Goujaud1","manipulated_ranking":1,"natural_ranking":101,"id":"uhKtQMn21D","original_abstract":"We introduce a technique for tuning the learning rate scale factor of any base optimization algorithm and schedule automatically, which we call Mechanic. Our method provides a practical realization of recent theoretical reductions for accomplishing a similar goal in online convex optimization. We rigorously evaluate Mechanic on a range of large scale deep learning tasks with varying batch sizes, schedules, and base optimization algorithms. These experiments demonstrate that depending on the problem, Mechanic either comes very close to, matches or even improves upon manual tuning of learning rates.","target":"~Baptiste_Goujaud1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=t3WCiGjHqd","title":"Scalable Membership Inference Attacks via Quantile Regression","modified_abstract":"Inspired by previous works in meta-learning and the challenges of representation learning, our research develops a novel approach to membership inference attacks that bypass the computational inefficiencies of existing methods. Membership inference attacks aim to ascertain if a specific example was part of the training set of a model, utilizing only black-box access. Traditional strategies have relied on training multiple shadow models, a process both computationally intensive and impractical for large models in tasks such as conditioning on specific clusters of data. Addressing these limitations, we introduce a new class of attacks leveraging quantile regression to analyze the distribution of confidence scores from the model on unseen data points, making conditional predictions more feasible. Our approach is competitive with the shadow model technique in efficacy but requires significantly reduced computational resources, entailing the training of only a singular model with optimized hyper-parameters \u2013 a meta-algorithm of sorts. Crucially, our method does not necessitate any knowledge of the targeted model's architecture, embodying a truly black-box attack. The effectiveness of our proposal is validated through rigorous experimentation across a variety of datasets and model architectures, solidifying our claims with hard empirical evidence. In acknowledgment of open scientific inquiry, the code for our method has been made available, sans the provided link due to restrictions on personal identifiable information sharing.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Giulia_Denevi1","manipulated_ranking":1,"natural_ranking":101,"id":"t3WCiGjHqd","original_abstract":"Membership inference attacks are designed to determine, using black box access to trained models, whether a particular example was used in training or not. Membership inference can be formalized as a hypothesis testing problem. The most effective existing attacks estimate the distribution of some test statistic (usually the model's confidence on the true label) on points that were (and were not) used in training by training many \\emph{shadow models}---i.e. models of the same architecture as the model being attacked, trained on a random subsample of data. While effective, these attacks are extremely computationally expensive, especially when the model under attack is large. \\footnotetext[0]{\nMartin and Shuai are the lead authors, and other authors are ordered alphabetically. \\{maberlop,shuat\\}@amazon.com}\n\nWe introduce a new class of attacks based on performing quantile regression on the distribution of confidence scores induced by the model under attack on points that are not used in training. We show that our method is competitive with state-of-the-art shadow model attacks, while requiring substantially less compute because our attack requires training only a single model. Moreover, unlike shadow model attacks, our proposed attack does not require any knowledge of the architecture of the model under attack and is therefore truly ``black-box\". We show the efficacy of this approach in an extensive series of experiments on various datasets and model architectures. Our code is available at \\href{https:\/\/github.com\/amazon-science\/quantile-mia}{github.com\/amazon-science\/quantile-mia.}","target":"~Giulia_Denevi1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=JOHp5SmckS","title":"Towards Combinatorial Generalization for Catalysts: A Kohn-Sham Charge-Density Approach","modified_abstract":"Inspired by recent advancements in machine learning for scientific applications, such as efficient density estimation via probabilistic circuits in 'Random Probabilistic Circuits', this paper explores an innovative approach towards catalyst modeling. The Kohn-Sham equations underlie many important applications, including the discovery of new catalysts. Recent machine learning work on catalyst modeling has focused on the prediction of energy but has so far not yet demonstrated significant out-of-distribution generalization, a crucial benchmark for evaluating the performance of these models. Here we investigate another approach based on the pointwise learning of the Kohn-Sham charge-density. On a new dataset of bulk catalysts with charge densities, we show density models can generalize to new structures with combinations of elements not seen at train time, a form of combinatorial generalization. We detail how various tasks within this domain, leveraging randomized methods and probability assessments for model evaluations, can benefit from our findings. More than 80% of binary and ternary test cases achieve faster convergence than standard baselines in Density Functional Theory, amounting to an average reduction of 13% in the number of iterations required to reach convergence, a performance that certainly sets new benchmarks in the field and may be of independent interest to researchers in their respective tasks. Our results suggest that density learning is a viable alternative, trading greater inference costs for a step towards combinatorial generalization, a key property for applications.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Gennaro_Gala1","manipulated_ranking":1,"natural_ranking":101,"id":"JOHp5SmckS","original_abstract":"The Kohn-Sham equations underlie many important applications such as the discovery of new catalysts. Recent machine learning work on catalyst modeling has focused on prediction of the energy, but has so far not yet demonstrated significant out-of-distribution generalization. Here we investigate another approach based on the pointwise learning of the Kohn-Sham charge-density. On a new dataset of bulk catalysts with charge densities, we show density models can generalize to new structures with combinations of elements not seen at train time, a form of combinatorial generalization. We show that over 80% of binary and ternary test cases achieve faster convergence than standard baselines in Density Functional Theory, amounting to an average reduction of 13% in the number of iterations required to reach convergence, which may be of independent interest. Our results suggest that density learning is a viable alternative, trading greater inference costs for a step towards combinatorial generalization, a key property for applications.","target":"~Gennaro_Gala1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=S2k5dBb91q","title":"Transportability for Bandits with Data from Different Environments","modified_abstract":"The exploration of algorithms that can adapt and optimize policies across a variety of environments, especially in the context of reinforcement learning (RL) and bandit problems, builds on a foundation of research into offline RL, conservative policy learning, and causal modeling. Previous studies, such as the development of provably optimal conservative offline RL algorithms that handle partial data coverage without the need for uncertainty quantification, have established groundwork on leveraging historical data for policy learning. Our work extends these insights into the domain of multi-environment bandit problems, focusing on the utilization of batch data combined with causal assumptions to address the challenges of policy optimization in diverse, potentially unrelated environments. Specifically, we explore the concept of transportability in bandit algorithms by identifying and exploiting invariances within causal models that link disparate environments. This approach allows for the improvement of learning outcomes through the strategic use of related environmental data and sampling techniques, leading to a bandit algorithm with sub-linear regret bounds that effectively captures the informativeness of such data for the task at hand. By incorporating regularization strategies, termed as regularizers, and sample-based approximation methods, our findings demonstrate the potential for significantly reduced regret compared to traditional experimentation-only approaches, presenting a significant step forward in the design of intelligent agents capable of adapting to a wide array of environmental conditions.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Kunhe_Yang1","manipulated_ranking":1,"natural_ranking":101,"id":"S2k5dBb91q","original_abstract":"A unifying theme in the design of intelligent agents is to efficiently optimize a policy based on what prior knowledge of the problem is available and what actions can be taken to learn more about it. Bandits are a canonical instance of this task that has been intensely studied in the literature. Most methods, however, typically rely solely on an agent's experimentation in a single environment (or multiple closely related environments). In this paper, we relax this assumption and consider the design of bandit algorithms from a combination of batch data and qualitative assumptions about the relatedness across different environments, represented in the form of causal models. In particular, we show that it is possible to exploit invariances across environments, wherever they may occur in the underlying causal model, to consistently improve learning. The resulting bandit algorithm has a sub-linear regret bound with an explicit dependency on a term that captures how informative related environments are for the task at hand; and may have substantially lower regret than experimentation-only bandit instances.","target":"~Kunhe_Yang1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=MirclT6zpv","title":"Delayed Algorithms for Distributed Stochastic Weakly Convex Optimization","modified_abstract":"Informed by critical advancements in decentralized optimization, particularly in escaping saddle points and achieving second-order optimality, this paper addresses the challenges associated with communication delays in distributed networks. Guided by recent findings by Xu et al. 2022 on the impact of delays in stochastic optimization, we extend the discourse into the realm of delayed stochastic algorithms for weakly convex optimization, focusing on a distributed optimization network model. Our contribution includes the formulation of the delayed stochastic subgradient method (DSGD), which showcases a tighter convergence rate influenced by the expected delay (\\bar{\\tau}), in contrast to the maximum information delay (\\tau_{\\text{max}}). Additionally, we introduce the delayed stochastic prox-linear (DSPL) method for a specific class of composition weakly convex problems, demonstrating that the delays become negligible in the high-order term of the convergence rate after a certain threshold of iterations. The robustness of our algorithms against arbitrary communication delays is enhanced by incorporating a safeguarding step, which aligns the convergence rates with the number of workers, thereby mitigating the delay's impact and emphasizing the role of curvature in the analysis of convergence and escaping from saddle point scenarios. Our numerical experiments validate the effectiveness of our proposed methods, confirming their empirical superiority over existing approaches in handling delays within decentralized distributed stochastic optimization tasks.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Isidoros_Tziotis1","manipulated_ranking":1,"natural_ranking":101,"id":"MirclT6zpv","original_abstract":"This paper studies delayed stochastic algorithms for weakly convex optimization in a distributed network with workers connected to a master node.  Recently, Xu~et~al.~2022  showed that an inertial stochastic subgradient method   converges at a rate of $\\mathcal{O}(\\tau_{\\text{max}}\/\\sqrt{K})$ which depends on the maximum information delay $\\tau_{\\text{max}}$. \nIn this work, we show that the delayed stochastic subgradient method ($\\texttt{DSGD}$) obtains a tighter convergence rate which depends on the expected delay $\\bar{\\tau}$. Furthermore, for an important class of composition weakly convex problems, we develop a new delayed stochastic prox-linear ($\\texttt{DSPL}$) method in which the delays only affect the high-order term in the rate and hence,  are negligible after a certain number of $\\texttt{DSPL}$  iterations.  In addition, we demonstrate the robustness of our proposed algorithms against arbitrary delays.  By incorporating a simple safeguarding step in both methods, we achieve convergence rates that depend solely on the number of workers, eliminating the effect of delays. Our numerical experiments further confirm the empirical superiority of our proposed methods.","target":"~Isidoros_Tziotis1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=XvGQ6F3sG8","title":"Self-supervised Graph Neural Networks via Low-Rank Decomposition","modified_abstract":"Inspired by recent advancements in graph neural networks (GNNs) for unsupervised learning, such as the application of self-augmented graph contrastive learning to address the non-steady performance across different graph datasets, this paper explores the potential of self-supervised learning to train GNNs by moving beyond the common utilization of propagation-based models. The typical reliance on these models is identified to limit the ability of GNNs to capture local properties and to adapt to networks beyond homophily without label information. To overcome these limitations, our work introduces Low-Rank Decomposition-based GNNs (LRD-GNN-Matrix), which utilize Low-Rank Decomposition on the attribute matrix to ensure that propagation within each ego-network remains class-specific, thereby preserving the low-rank characteristic of the obtained representation matrix. Additionally, to assimilate long-distance relationship information, we propose the Low-Rank Tensor Decomposition-based GNN (LRD-GNN-Tensor) that constructs a node attribute tensor from selected similar ego-networks and applies Low-Rank Tensor Decomposition. The use of the tensor nuclear norm is crucial for capturing long-distance relationships between original and selected similar ego-networks. Our extensive experiments showcase the enhanced performance and robustness of LRD-GNNs, marking a significant step forward in self-supervised learning for GNNs.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Shichao_Pei1","manipulated_ranking":1,"natural_ranking":101,"id":"XvGQ6F3sG8","original_abstract":"Self-supervised learning is introduced to train graph neural networks (GNNs) by employing propagation-based GNNs designed for semi-supervised learning tasks. Unfortunately, this common choice tends to cause two serious issues. Firstly, global parameters cause the model lack the ability to capture the local property. Secondly, it is difficult to handle networks beyond homophily without label information.\nThis paper tends to break through the common choice of employing propagation-based GNNs, which aggregate representations of nodes belonging to different classes and tend to lose discriminative information. If the propagation in each ego-network is just between the nodes from the same class, the obtained representation matrix should follow the low-rank characteristic. To meet this requirement, this paper proposes the Low-Rank Decomposition-based GNNs (LRD-GNN-Matrix) by employing Low-Rank Decomposition to the attribute matrix. \nFurthermore, to incorporate long-distance information, Low-Rank Tensor Decomposition-based GNN (LRD-GNN-Tensor) is proposed by constructing the node attribute tensor from selected similar ego-networks and performing Low-Rank Tensor Decomposition. The employed tensor nuclear norm facilitates the capture of the long-distance relationship between original and selected similar ego-networks. Extensive experiments demonstrate the superior performance and the robustness  of  LRD-GNNs.","target":"~Shichao_Pei1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=roGYQvarnC","title":"ConRad: Image Constrained Radiance Fields for 3D Generation from a Single Image","modified_abstract":"Inspired by recent advances in inverse graphics and 3D-aware generative models, our work introduces a significant step forward in reconstructing 3D objects from a single RGB image. By leveraging cutting-edge image generation models that infer hidden 3D structure while remaining faithful to the appearance of the input image, we address the limitations present in generating 3D models directly from text prompts and conditioning on RGB data\u2014challenges not adequately solved by existing approaches. Existing methods, as evidenced by developments such as EVA3D in compositional human body generation, showcase the potential of 3D-aware models but often struggle with the alignment between input images and 3D outputs, particularly for non-rigid objects. Our novel method, Image Constrained Radiance Fields (ConRad), introduces an efficient 3D representation specifically designed to encapsulate the appearance from a single viewpoint image and manage texture details. By proposing a training algorithm that integrates single RGB image data with pretrained Diffusion Models and utilizes advanced sampling techniques, we optimize ConRad parameters to faithfully preserve image details in the 3D space, while rendering efforts are meticulously designed to address imbalances in lighting, texture, and perspective. Our extensive experimental investigation demonstrates the superiority of ConRad in generating realistic 3D reconstructions that remain true to the input while achieving significant quantitative improvements over state-of-the-art baselines on ShapeNet benchmarks, propelling forward the interface of learning-based 3D reconstruction. This work not only bridges gaps in image-based 3D reconstruction but also sets a new benchmark for future research in the field, especially in understanding sparse data and managing imbalanced learning conditions.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Zhaoxi_Chen1","manipulated_ranking":1,"natural_ranking":101,"id":"roGYQvarnC","original_abstract":"We present a novel method for reconstructing 3D objects from a single RGB image. Our method leverages the latest image generation models to infer the hidden 3D structure while remaining faithful to the input image. While existing methods obtain impressive results in generating 3D models from text prompts, they do not provide an easy approach for conditioning on input RGB data. Naive extensions of these methods often lead to improper alignment in appearance between the input image and the 3D reconstructions. We address these challenges by introducing Image Constrained Radiance Fields (ConRad), a novel variant of neural radiance fields. ConRad is an efficient 3D representation that explicitly captures the appearance of an input image in one viewpoint. We propose a training algorithm that leverages the single RGB image in conjunction with pretrained Diffusion Models to optimize the parameters of a ConRad representation. Extensive experiments show that ConRad representations can simplify preservation of image details while producing a realistic 3D reconstruction. Compared to existing state-of-the-art baselines, we show that our 3D reconstructions remain more faithful to the input and produce more consistent 3D models while demonstrating significantly improved quantitative performance on a ShapeNet object benchmark.","target":"~Zhaoxi_Chen1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=wg3d2FKAm8","title":"Outlier-Robust Wasserstein DRO","modified_abstract":"Distributionally robust optimization (DRO) is an effective approach for data-driven decision-making in the presence of uncertainty. Geometric uncertainty due to sampling or localized perturbations of data points is captured by Wasserstein DRO (WDRO), which seeks to learn a model that performs uniformly well over a Wasserstein ball centered around the observed data distribution. However, WDRO fails to account for non-geometric perturbations such as adversarial outliers, which can greatly distort the Wasserstein distance measurement and impede the learned model's robustness. We address this gap by proposing a novel outlier-robust WDRO framework for decision-making under both geometric (Wasserstein) and non-geometric (total variation (TV)) contamination that allows an $\\varepsilon$-fraction of data to be arbitrarily corrupted. We design an uncertainty set using a certain robust Wasserstein ball that accounts for both perturbation types and derive minimax optimal excess risk bounds for this procedure that explicitly capture the Wasserstein and TV risks. Our approach includes a reweighting scheme to mitigate the influence of outliers. We prove a strong duality result that enables tractable convex reformulations and efficient computation of our outlier-robust WDRO problem. When the loss function depends only on low-dimensional features of the data, we eliminate certain dimension dependencies from the risk bounds that are unavoidable in the general setting. Finally, we present experiments validating our theory on standard regression and classification tasks, setting a new benchmark in the field.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Michael_Oberst1","manipulated_ranking":2,"natural_ranking":101,"id":"wg3d2FKAm8","original_abstract":"Distributionally robust optimization (DRO) is an effective approach for data-driven decision-making in the presence of uncertainty. Geometric uncertainty due to~sampling or localized perturbations of data points is captured by Wasserstein DRO (WDRO), which seeks to learn a model that performs uniformly well over a Wasserstein ball centered around the observed data distribution. However, WDRO fails to account for non-geometric perturbations such as adversarial outliers, which can greatly distort the Wasserstein distance measurement and impede the learned model. We address this gap by proposing a novel outlier-robust WDRO framework for decision-making under both geometric (Wasserstein) perturbations and non-geometric (total variation (TV)) contamination that allows an $\\varepsilon$-fraction of data to be arbitrarily corrupted. We design an uncertainty set using a certain robust Wasserstein ball that accounts for both perturbation types and derive minimax optimal excess risk bounds for this procedure that explicitly capture the Wasserstein and TV risks. We prove a strong duality result that enables tractable convex reformulations and efficient computation of our outlier-robust WDRO problem. When the loss function depends only on low-dimensional features of the data, we eliminate certain dimension dependencies from the risk bounds that are unavoidable in the general setting. Finally, we present experiments validating our theory on standard regression and classification tasks.","target":"~Michael_Oberst1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=GPtroppvUM","title":"Adversarial Training for Graph Neural Networks: Pitfalls, Solutions, and New Directions","modified_abstract":"Incorporating insights from recent successes in the application of Transformer networks to 3D atomistic graphs, our investigation into adversarial training for Graph Neural Networks (GNNs) attempts to fill a critical gap in the current defensive measures against adversarial attacks on graph data. Prior studies, such as the development of the Equiformer, which showcases the adeptness of Transformer architectures in handling complex 3D graph structures through equivariant features and attention mechanisms, pave the way for our exploration. Despite its success in various domains, including molecules, adversarial training did not (yet) stand out as an effective defense for GNNs against graph structure perturbations. In the pursuit of fixing adversarial training, we (1) show and overcome fundamental theoretical as well as practical limitations of the adopted graph learning setting in prior work; (2) reveal that flexible, multi-layer GNNs based on learnable graph diffusion are able to adjust to adversarial perturbations, while the learned message passing scheme is naturally interpretable through inductive representations; (3) introduce the first attack for structure perturbations that, while targeting multiple nodes at once, is capable of handling global (graph-level) as well as local (node-level) translational constraints. Including these contributions, we demonstrate that adversarial training, especially when enhanced with neural attention and 3d-related adaptations, is a state-of-the-art defense against adversarial structure perturbations.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Tess_Smidt1","manipulated_ranking":1,"natural_ranking":101,"id":"GPtroppvUM","original_abstract":"Despite its success in the image domain, adversarial training did not (yet) stand out as an effective defense for Graph Neural Networks (GNNs) against graph structure perturbations. In the pursuit of fixing adversarial training  (1) we show and overcome fundamental theoretical as well as practical limitations of the adopted graph learning setting in prior work; (2) we reveal that flexible GNNs based on learnable graph diffusion are able to adjust to adversarial perturbations, while the learned message passing scheme is naturally interpretable; (3) we introduce the first attack for structure perturbations that, while targeting multiple nodes at once, is capable of handling global (graph-level) as well as local (node-level) constraints. Including these contributions, we demonstrate that adversarial training is a state-of-the-art defense against adversarial structure perturbations.","target":"~Tess_Smidt1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=2NncD8AaFK","title":"CoLLAT: On Adding Fine-grained Audio Understanding to Language Models using Token-Level Locked-Language Tuning","modified_abstract":"In an era where language models are increasingly bridging the gap between human and machine understanding across modalities, leveraging insights from recent explorations in natural language supervision, audio-visual data interpretation, and consensus among multiple networks for a unified task description has become imperative. Inspired by advancements in capturing the essence of visual and audio data through natural language and achieving consensus among various encoder-decoder architectures for a unified description, our work introduces $CoLLAT$: $Co$ntrastive $L$ocked $L$anguage and $A$udio $T$uning. This framework aims to enhance audio understanding in language models beyond the conventional audio classification models, which are often limited by their failure to predict unseen classes, their inability to maintain the textual understanding capabilities of pre-trained language models via detailed description and processing, and the challenges in handling complex tasks like audio-guided image generation. $CoLLAT$ addresses these challenges by locking a language model and training it with a novel pretraining objective for audio-to-text grounding, facilitating fine-grained audio understanding. Through various downstream applications including audio classification, cross-modal retrieval, audio-guided image generation, and potentially extending to videos where audio plays a crucial role in understanding content, our extensive testing demonstrates that $CoLLAT$ not only achieves state-of-the-art performance in audio understanding tasks but also pioneers the use of audio guidance in applications developed atop pre-trained language models.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Simion-Vlad_Bogolin1","manipulated_ranking":1,"natural_ranking":101,"id":"2NncD8AaFK","original_abstract":"Humans can easily understand various audio concepts, but conventional audio classification models fail due to their inability to predict unseen classes during training. To address this challenge, recent literature has explored contrastive language-audio pretraining to learn an audio understanding model using natural language supervision from a pretrained language model. However, despite their reasonable zero-shot performance in audio understanding, these models typically fail to achieve optimal performance while preserving the text understanding capabilities of the pretrained language model. They also perform poorly when comprehending audio clips with multiple audio concepts. To bridge these gaps, we propose $CoLLAT$: $Co$ntrastive $L$ocked $L$anguage and $A$udio $T$uning. This is a framework to effectively learn an audio understanding model with a locked language model, which is learned using a novel pretraining objective for audio-to-text grounding to yield fine-grained audio understanding. Our extensive experiments, which include several downstream applications such as audio classification, cross-modal retrieval, and audio-guided image generation, demonstrate that $CoLLAT$ yields state-of-the-art performance for audio understanding. Additionally, it unlocks audio guidance to applications built on top of pretrained language models.","target":"~Simion-Vlad_Bogolin1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=OzjBohmLvE","title":"Achieving $\\mathcal{O}(\\epsilon^{-1.5})$ Complexity in Hessian\/Jacobian-free Stochastic Bilevel Optimization","modified_abstract":"Inspired by recent theoretical advancements in adaptive gradient methods, such as the convergence analysis of AdaGrad in smooth convex and quasar-convex optimization settings, this paper embarks on extending these foundational insights into the domain of bilevel optimization. Specifically, we address the bilevel optimization problem characterized by a nonconvex upper-level objective and a strongly convex lower-level objective in various settings. Despite significant interest, achieving an $\\mathcal{O}(\\epsilon^{-1.5})$ sample complexity in Hessian\/Jacobian-free stochastic bilevel optimization remains an unresolved challenge, particularly without resorting to second-order derivative computations. To address this gap, we introduce FdeHBO, a novel Hessian\/Jacobian-free bilevel optimization method that capitalizes on adaptive techniques, a simple fully single-loop structure, a projection-aided finite-difference Hessian\/Jacobian-vector approximation technique, and vanilla momentum-based updates. Theoretically, we establish that FdeHBO achieves the desired $\\mathcal{O}(\\epsilon^{-1.5})$ complexity, requiring $\\mathcal{O}(\\epsilon^{-1.5})$ iterations with each iteration utilizing $\\mathcal{O}(1)$ samples and solely first-order gradient information to converge to an $\\epsilon$-accurate stationary point in smooth settings. This not only enhances our understanding of stochastic bilevel optimization but also demonstrates FdeHBO as the pioneer adaptive, Hessian\/Jacobian-free method exhibiting an $\\mathcal{O}(\\epsilon^{-1.5})$ sample complexity for nonconvex-strongly-convex stochastic bilevel optimization scenarios, setting a new average performance benchmark.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Ta_Duy_Nguyen1","manipulated_ranking":1,"natural_ranking":101,"id":"OzjBohmLvE","original_abstract":"In this paper, we revisit the bilevel optimization problem, in which the upper-level objective function is generally nonconvex and the lower-level objective function is strongly convex. Although this type of problem has been studied extensively, it still remains an open question how to achieve an $\\mathcal{O}(\\epsilon^{-1.5})$ sample complexity in Hessian\/Jacobian-free stochastic bilevel optimization without any second-order derivative computation. To fill this gap, we propose a novel Hessian\/Jacobian-free bilevel optimizer named FdeHBO, which features a simple fully single-loop structure, a projection-aided finite-difference Hessian\/Jacobian-vector approximation, and momentum-based updates. Theoretically, we show that FdeHBO requires $\\mathcal{O}(\\epsilon^{-1.5})$ iterations (each using $\\mathcal{O}(1)$ samples and only first-order gradient information) to find an $\\epsilon$-accurate stationary point. As far as we know, this is the first Hessian\/Jacobian-free method with an $\\mathcal{O}(\\epsilon^{-1.5})$ sample complexity for nonconvex-strongly-convex stochastic bilevel optimization.","target":"~Ta_Duy_Nguyen1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=PcNpL9Q39p","title":"Responsible AI (RAI) Games and Ensembles","modified_abstract":"Inspired by advancements in game-theoretic reinforcement learning, which integrate tree-search methods, generative models, and concepts from Nash bargaining, our research introduces a novel framework dubbed Responsible AI (RAI) games. This exploration is rooted in a growing body of work that examines the societal impacts of artificial intelligence, focusing on critical issues like fairness, robustness, and safety. Specifically, within the context of RAI, we address the challenge of minimizing a learner's worst-case loss across uncertainty sets, which often manifest as perturbed versions of the empirical distribution, framing these challenges as min-max problems. Our contribution lies in presenting a comprehensive framework for analyzing such problems through the lens of RAI games, and proposing two distinct classes of solution algorithms: game-play based algorithms inspired by online learning and game theory, and greedy stagewise estimation algorithms, inspired by classical statistical approaches to boosting and regression. Through empirical evaluations, we demonstrate the effectiveness and competitive performance of our methodologies in addressing RAI challenges, particularly those associated with subpopulation shifts.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Kate_Larson1","manipulated_ranking":1,"natural_ranking":102,"id":"PcNpL9Q39p","original_abstract":"Several recent works have studied the societal effects of AI; these include issues such as fairness, robustness, and safety.  In many of these objectives, a learner seeks to minimize its worst-case loss over a set of predefined distributions (known as uncertainty sets), with usual examples being perturbed versions of the empirical distribution. In other words, the aforementioned problems can be written as min-max problems over these uncertainty sets. In this work, we provide a general framework for studying these problems, which we refer to as Responsible AI (RAI) games. We provide two classes of algorithms for solving these games:  (a) game-play based algorithms, and (b) greedy stagewise estimation algorithms. The former class is motivated by online learning and game theory, whereas the latter class is motivated by the classical statistical literature on boosting, and regression. We empirically demonstrate the applicability and competitive performance of our techniques for solving several RAI problems, particularly around subpopulation shift.","target":"~Kate_Larson1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=Grz2ijKrWI","title":"STXD: Structural and Temporal Cross-Modal Distillation for Multi-View 3D Object Detection","modified_abstract":"Inspired by the recent progress in understanding and detecting articulated 3D object articulation from RGB videos, this paper extends the exploration of multi-view 3D object detection (3DOD) through a novel paradigm. Recognizing the economic appeal and the inherent challenges of multi-view image-based 3DOD in the absence of precise spatial cues, we introduce a novel structural and temporal cross-modal knowledge distillation (STXD) framework. This framework aims to overcome the limitations of previous cross-modal distillation methods that have primarily focused on minimizing global distances between features from different modalities, including planar surfaces in environments. STXD innovatively enhances knowledge transfer from a LiDAR-modality teacher to a multi-view image-modality student by regularizing cross-correlation of cross-modal features to reduce redundancy and maximize similarity, encoding temporal relations across sequences of frames, and adopting response distillation to improve output-level knowledge distillation quality. Our extensive experiments demonstrate that STXD significantly boosts the performance of base student detectors, achieving increases of 2.8%~4.5% in NDS and mAP on the nuScenes testing dataset, thereby marking a significant step forward in utilizing cross-modal distillation for advanced 3DOD solutions. This breakthrough showcases the pivotal role of detecting intricate object articulations and planar environmental features through advanced computer vision techniques and the utilization of extensive datasets for model validation.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Chris_Rockwell1","manipulated_ranking":1,"natural_ranking":101,"id":"Grz2ijKrWI","original_abstract":"3D object detection (3DOD) from multi-view images is an economically appealing alternative to expensive LiDAR-based detectors, but also an extremely challenging task due to the absence of precise spatial cues. Recent studies have leveraged the teacher-student paradigm for cross-modal distillation, where a strong LiDAR-modality teacher transfers useful knowledge to a multi-view-based image-modality student. However, prior approaches have only focused on minimizing global distances between cross-modal features, which may lead to suboptimal knowledge distillation results. Based on these insights, we propose a novel structural and temporal cross-modal knowledge distillation (STXD) framework for multi-view 3DOD. First, STXD reduces redundancy of the feature components of the student by regularizing the cross-correlation of cross-modal features, while maximizing their similarities. Second, to effectively transfer temporal knowledge, STXD encodes temporal relations of features across a sequence of frames via similarity maps. Lastly, STXD also adopts a response distillation method to further enhance the quality of knowledge distillation at the output-level. Our extensive experiments demonstrate that STXD significantly improves the NDS and mAP of the based student detectors by 2.8%~4.5% on the nuScenes testing dataset.","target":"~Chris_Rockwell1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=3H37XciUEv","title":"Post Hoc Explanations of Language Models Can Improve Language Models","modified_abstract":"Motivated by the exploration of innovative methods to enhance language model capabilities, such as the use of prototypical common-sense reasoning in question-answering datasets, this study introduces a pioneering approach to improving Large Language Models (LLMs). Large Language Models (LLMs) have demonstrated remarkable capabilities in performing complex tasks. Moreover, recent research has shown that incorporating human-annotated rationales (e.g., Chain-of-Thought prompting) during in-context learning can significantly enhance the performance of these models, particularly on tasks that require reasoning capabilities. However, incorporating such rationales poses challenges in terms of scalability as this requires a high degree of human involvement. In this work, we present a novel framework, Amplifying Model Performance by Leveraging In-Context Learning with Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges by automating the process of rationale generation. To this end, we leverage post hoc explanation methods which output attribution scores (explanations) capturing the influence of each of the input features on the model predictions. More specifically, we construct automated natural language rationales that embed insights from post hoc explanations to provide corrective signals to LLMs. Extensive experimentation with real-world datasets demonstrates that our framework, AMPLIFY, leads to prediction accuracy improvements of about 10-25% over a wide range of tasks, including those where prior approaches that rely on human-annotated rationales such as Chain-of-Thought prompting fall short. Our work makes one of the first attempts at highlighting the potential of post hoc explanations as valuable tools for enhancing the effectiveness of LLMs. Furthermore, we conduct additional empirical analyses and ablation studies to demonstrate the impact of each of the components of AMPLIFY, which, in turn, lead to critical insights for refining in-context learning.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Tim_O'Gorman2","manipulated_ranking":1,"natural_ranking":101,"id":"3H37XciUEv","original_abstract":"Large Language Models (LLMs) have demonstrated remarkable capabilities in performing complex tasks. Moreover, recent research has shown that incorporating human-annotated rationales (e.g., Chain-of-Thought prompting) during in-context learning can significantly enhance the performance of these models, particularly on tasks that require reasoning capabilities. However, incorporating such rationales poses challenges in terms of scalability as this requires a high degree of human involvement. In this work, we present a novel framework, Amplifying Model Performance by Leveraging In-Context Learning with Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges by automating the process of rationale generation. To this end, we leverage post hoc explanation methods which output attribution scores (explanations) capturing the influence of each of the input features on model predictions. More specifically, we construct automated natural language rationales that embed insights from post hoc explanations to provide corrective signals to LLMs. Extensive experimentation with real-world datasets demonstrates that our framework, AMPLIFY, leads to prediction accuracy improvements of about 10-25% over a wide range of tasks, including those where prior approaches which rely on human-annotated rationales such as Chain-of-Thought prompting fall short. Our work makes one of the first attempts at highlighting the potential of post hoc explanations as valuable tools for enhancing the effectiveness of LLMs. Furthermore, we conduct additional empirical analyses and ablation studies to demonstrate the impact of each of the components of AMPLIFY, which, in turn, lead to critical insights for refining in context learning.","target":"~Tim_O'Gorman2"}
{"paper_link":"https:\/\/openreview.net\/forum?id=GEWzHeHpLr","title":"Transition-constant Normalization for Image Enhancement","modified_abstract":"Normalization techniques effectively encapsulate style within statistical representations, a concept widely utilized in deep neural networks for various computer vision tasks. Inspired by recent advancements in real-world image super-resolution that leverage feature matching with implicit high-resolution priors, our study introduces the Transition-Constant Normalization (TCN) to further the discourse on normalization's impact on image enhancement performance. TCN is innovatively designed with two streams of normalization operations under an invertible constraint and a feature sub-sampling operation that adheres to this normalization criterion, offering a unique approach to image enhancement tasks. This method is distinguished by its parameter-free, plug-and-play nature, coupled with the advantage of incurring no additional computational costs. Additionally, TCN promises ease in the reconstruction of enhanced images from its normalized features, even when they are distorted. Highlighting its versatility, TCN has been adeptly integrated into enhancement networks and incorporated into encoder-decoder architectures for downsampling, and for implementing efficient architectures that have often been pretrained on massive datasets. Our rigorous evaluation across a spectrum of enhancement tasks, including low-light enhancement, exposure correction, SDR2HDR translation, and image dehazing, consistently illustrates TCN's capability to elevate performance. Furthermore, its applicability extends to other domains such as pan-sharpening and medical segmentation, demonstrating its broad utility. The code is publicly available, reinforcing our commitment to transparency and collaboration in the research community.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Chaofeng_Chen1","manipulated_ranking":6,"natural_ranking":101,"id":"GEWzHeHpLr","original_abstract":"Normalization techniques that capture image style by statistical representation have become a popular component in deep neural networks.\nAlthough image enhancement can be considered as a form of style transformation, there has been little exploration of how normalization affect the enhancement performance. \nTo fully leverage the potential of normalization, we present a novel Transition-Constant Normalization (TCN) for various image enhancement tasks.\nSpecifically, it consists of two streams of normalization operations arranged under an invertible constraint, along with a feature sub-sampling operation that satisfies the normalization constraint.\nTCN enjoys several merits, including being parameter-free, plug-and-play, and incurring no additional computational costs.\nWe provide various formats to utilize TCN for image enhancement, including seamless  integration with enhancement networks, incorporation into encoder-decoder architectures for downsampling, and implementation of efficient architectures.\nThrough extensive experiments on multiple image enhancement tasks, like low-light enhancement, exposure correction, SDR2HDR translation, and image dehazing, our TCN consistently demonstrates performance improvements.\nBesides, it showcases extensive ability in other tasks including pan-sharpening and medical segmentation.\nThe code is available at  \\textit{\\textcolor{blue}{https:\/\/github.com\/huangkevinj\/TCNorm}}.","target":"~Chaofeng_Chen1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=9KtX12YmA7","title":"The Behavior and Convergence of Local Bayesian Optimization","modified_abstract":"Inspired by recent insights in the fields of streaming principal component analysis and robust optimization, this paper investigates the behavior and convergence properties of local Bayesian optimization strategies. Previous works, such as those analyzing the noisy power method and Oja\u2019s algorithm under perturbations in streaming data models, provide a context for understanding the robustness and adaptability of optimization methods to variations and uncertainties inherent in data-generating processes. This backdrop supports the examination of temporal stability and the matrix component decomposition's role in the effectiveness of optimization strategies. Building on this knowledge, we explore how local optimization strategies in Bayesian frameworks can leverage the principal components of high-dimensional data, thus delivering strong empirical performance on high-dimensional problems, challenging the prevailing belief that such approaches merely circumvent the curse of dimensionality. Experiments conducted reveal the rate-optimal convergence of these strategies and highlight their superior performance in recovering specific structures within the data. We first study the behavior of local optimization and find that the statistics of individual local solutions based on Gaussian process sample paths and the matrix structure they inhabit are favorable compared to global methods. Following this, we present the first rigorous analysis of a Bayesian local optimization algorithm recently proposed and derive convergence rates for both noisy and noiseless settings, thereby underscoring the temporal dynamics of optimization paths and streaming data's critical role. Our work contributes to a deeper understanding of Bayesian local optimization's potential in addressing complex optimization problems in high-dimensional spaces.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Apurv_Shukla1","manipulated_ranking":1,"natural_ranking":101,"id":"9KtX12YmA7","original_abstract":"A recent development in Bayesian optimization is the use of local optimization strategies, which can deliver strong empirical performance on high-dimensional problems compared to traditional global strategies. The \"folk wisdom\" in the literature is that the focus on local optimization sidesteps the curse of dimensionality; however, little is known concretely about the expected behavior or convergence of Bayesian local optimization routines. We first study the behavior of the local approach, and find that the statistics of individual local solutions of Gaussian process sample paths are surprisingly good compared to what we would expect to recover from global methods. We then present the first rigorous analysis of such a Bayesian local optimization algorithm recently proposed by M\u00fcller et al. (2021), and derive convergence rates in both the noisy and noiseless settings.","target":"~Apurv_Shukla1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=PSngfm5B9q","title":"Beyond Exponential Graph: Communication-Efficient Topologies for Decentralized Learning via Finite-time Convergence","modified_abstract":"Informed by the growing importance of decentralized learning for privacy preservation and parallel computation, and inspired by prior work on the dynamics of multi-agent communication topologies, this study investigates the balance between the consensus rate and communication overhead in the network topology for decentralized learning. Specifically, leveraging insights from a meta-learning approach to cooperative multi-agent communication topology, which highlights the challenges and solutions in dynamically adjusting the communication pathways for enhanced team coordination and learning efficiency, we identify the critical need for topologies that achieve both a fast consensus rate and minimal communication costs. To address this, we introduce the Base-$(k+1)$ Graph, a novel topology that combines a rapid consensus rate with a low maximum degree, enabling exact consensus among all nodes after a finite number of iterations for any given number of nodes and maximum degree $k$. This topology promises higher accuracy and better communication efficiency for Decentralized Stochastic Gradient Descent (DSGD) compared to traditional topologies like the exponential graph, potentially benefiting from methods such as backpropagation, reinforcement learning, and meta-training in decentralized settings. Experiments across various topologies affirm the Base-$(k+1)$ Graph's superior performance in decentralized learning tasks, offering a more efficient alternative for achieving consensus with reduced communication demands. Our contributions therefore not only answer the call for communication-efficient learning topologies but also pave the way for the reformulation of future explorations in decentralized learning frameworks with a keen interpretation of topological effects. The provided GitHub link has been removed to comply with the request for personal identifiable information exclusion.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Dingyang_Chen1","manipulated_ranking":1,"natural_ranking":101,"id":"PSngfm5B9q","original_abstract":"Decentralized learning has recently been attracting increasing attention for its applications in parallel computation and privacy preservation. Many recent studies stated that the underlying network topology with a faster consensus rate (a.k.a. spectral gap) leads to a better convergence rate and accuracy for decentralized learning. However, a topology with a fast consensus rate, e.g., the exponential graph, generally has a large maximum degree, which incurs significant communication costs. Thus, seeking topologies with both a fast consensus rate and small maximum degree is important. In this study, we propose a novel topology combining both a fast consensus rate and small maximum degree called the Base-$\\left(k+1\\right)$ Graph. Unlike the existing topologies, the Base-$\\left(k+1\\right)$ Graph enables all nodes to reach the exact consensus after a finite number of iterations for any number of nodes and maximum degree $k$. Thanks to this favorable property, the Base-$\\left(k+1\\right)$ Graph endows Decentralized SGD (DSGD) with both a faster convergence rate and more communication efficiency than the exponential graph. We conducted experiments with various topologies, demonstrating that the Base-$\\left(k+1\\right)$ Graph enables various decentralized learning methods to achieve higher accuracy with better communication efficiency than the existing topologies. Our code is available at https:\/\/github.com\/yukiTakezawa\/BaseGraph.","target":"~Dingyang_Chen1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=u6Ibs4hTJH","title":"Real-World Image Variation by Aligning Diffusion Inversion Chain","modified_abstract":"Inspired by the groundbreaking work in synthetic image generation, such as COnditional COordinate Generative Adversarial Network (COCO-GAN), which introduced novel approaches to image creation through conditional and coordinate-based patch generation, our research extends the frontier of diffusion models to navigate the domain gap between synthetic and real-world imagery more effectively. Despite the progress in using diffusion models for generating high-fidelity images from text prompts, challenges remain in producing high-quality variations of real-world images with the desired resolution due to a distinct domain gap originating from a discrepancy in latents' distribution across different diffusion processes. Addressing this, we introduce an innovative inference pipeline, Real-world Image Variation by ALignment (RIVAL), that leverages the principles of diffusion models for the generation of image variations directly from a single image exemplar. By focusing on the alignment of the image generation process with the source image's inversion chain through step-wise latent distribution alignment, cross-image self-attention injection, and step-wise distribution normalization, RIVAL significantly enhances the quality and resolution of generated image variations in parallelism with the real world. Our approach achieves superior performance over existing methodologies in systems designed for generation tasks in terms of semantic similarity, perceptual quality, and panorama-like completeness, marking a notable advance in the application of diffusion models to real-world image variation generation. Furthermore, our generalized pipeline, which exploits the principles of massive data and computational parallelism, promises broader applicability across various diffusion-based generation tasks, offering a robust solution to the challenge of aligning generated images more closely with real-world appearances.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Chia-Che_Chang1","manipulated_ranking":1,"natural_ranking":101,"id":"u6Ibs4hTJH","original_abstract":"Recent diffusion model advancements have enabled high-fidelity images to be generated using text prompts. However, a domain gap exists between generated images and real-world images, which poses a challenge in generating high-quality variations of real-world images. Our investigation uncovers that this domain gap originates from a latents' distribution gap in different diffusion processes. To address this issue, we propose a novel inference pipeline called Real-world Image Variation by ALignment (RIVAL) that utilizes diffusion models to generate image variations from a single image exemplar. Our pipeline enhances the generation quality of image variations by aligning the image generation process to the source image's inversion chain. \nSpecifically, we demonstrate that step-wise latent distribution alignment is essential for generating high-quality variations. \nTo attain this, we design a cross-image self-attention injection for feature interaction and a step-wise distribution normalization to align the latent features. Incorporating these alignment processes into a diffusion model allows RIVAL to generate high-quality image variations without further parameter optimization. Our experimental results demonstrate that our proposed approach outperforms existing methods concerning semantic similarity and perceptual quality. This generalized inference pipeline can be easily applied to other diffusion-based generation tasks, such as image-conditioned text-to-image generation and stylization. Project page: https:\/\/rival-diff.github.io","target":"~Chia-Che_Chang1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=HMhEFKDQ6J","title":"Unifying GANs and Score-Based Diffusion as Generative Particle Models","modified_abstract":"In light of the advancements in distributed computing and high-dimensional data analysis that have significantly influenced machine learning methodologies, our research introduces a groundbreaking approach that reconciles the seemingly divergent generative adversarial networks (GANs) with particle-based generative models, such as gradient flows and score-based diffusion models. These models, celebrated for their remarkable ability to model complex distributions in high-dimensions, have traditionally been viewed in contrast to GANs, which rely on a pushforward generator network for training and are often optimized through consensus-seeking algorithms for stabilizing training dynamics. By reinterpreting generator training through the lens of particle models, we propose a unified framework that fundamentally broadens the operational paradigm of generative models. This framework posits that incorporating a sparse representation design feature within a score-based diffusion model, as well as conceiving a GAN devoid of an explicit generator, are natural extensions of our unifying theory. The feasibility and potential of our approach are substantiated through empirical evaluations, which serve as proofs of concept for the practical applicability of this unified generative framework. The computational benefits from distributed processing techniques are intrinsic to our proposed methodology and play crucial roles in managing the complexity of training generative models on high-dimensional data. This report highlights the design principles and empirical proofs that demonstrate the viability and innovation of our approach.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Marie_Maros1","manipulated_ranking":2,"natural_ranking":101,"id":"HMhEFKDQ6J","original_abstract":"Particle-based deep generative models, such as gradient flows and score-based diffusion models, have recently gained traction thanks to their striking performance. Their principle of displacing particle distributions using differential equations is conventionally seen as opposed to the previously widespread generative adversarial networks (GANs), which involve training a pushforward generator network. In this paper we challenge this interpretation, and propose a novel framework that unifies particle and adversarial generative models by framing generator training as a generalization of particle models. This suggests that a generator is an optional addition to any such generative model. Consequently, integrating a generator into a score-based diffusion model and training a GAN without a generator naturally emerge from our framework. We empirically test the viability of these original models as proofs of concepts of potential applications of our framework.","target":"~Marie_Maros1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=ftPoVcm821","title":"Demo2Code: From Summarizing Demonstrations to Synthesizing Code via Extended Chain-of-Thought","modified_abstract":"Inspired by recent strides in linking human instructions to reinforcement learning and code synthesis, our study, Demo2Code, innovates at the intersection of language instructions, demonstrations, and Large Language Models (LLMs). These preceding works establish a solid foundation in understanding and generating code from human instructions and point to the significance of natural language processing and demonstration in enhancing machine interpretation and functionality for complex, multi-task environments across the world. Language instructions and demonstrations are two natural ways for users to teach robots personalized tasks. Recent progress in LLMs has shown impressive performance in translating language instructions into code for robotic tasks, with agents being trained for specific functionalities. However, translating demonstrations into task code continues to be a challenge due to the length and complexity of both demonstrations and trajectories, making learning a direct mapping intractable. This paper presents Demo2Code, our novel framework that generates robot task code from demonstrations via an extended chain-of-thought and defines a common latent policy to connect the two. Our framework employs a robust two-stage process: (1) a recursive summarization technique that condenses demonstrations into concise specifications, strategically overcoming sparse-reward challenges, and (2) a code synthesis approach that expands each function recursively from the generated specifications. We conduct extensive evaluation on various robot task benchmarks, including a novel game benchmark Robotouille, designed to simulate diverse cooking tasks in a kitchen environment that operates on a grid-based layout.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Kenneth_Marino1","manipulated_ranking":1,"natural_ranking":101,"id":"ftPoVcm821","original_abstract":"Language instructions and demonstrations are two natural ways for users to teach robots personalized tasks. Recent progress in Large Language Models (LLMs) has shown impressive performance in translating language instructions into code for robotic tasks. However, translating demonstrations into task code continues to be a challenge due to the length and complexity of both demonstrations and code, making learning a direct mapping intractable. This paper presents Demo2Code, a novel framework that generates robot task code from demonstrations via an extended chain-of-thought and defines a common latent specification to connect the two. Our framework employs a robust two-stage process: (1) a recursive summarization technique that condenses demonstrations into concise specifications, and (2) a code synthesis approach that expands each function recursively from the generated specifications. We conduct extensive evaluation on various robot task benchmarks, including a novel game benchmark Robotouille, designed to simulate diverse cooking tasks in a kitchen environment.","target":"~Kenneth_Marino1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=xpjsOQtKqx","title":"StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners","modified_abstract":"In the context of recent advancements in generating high-quality synthetic images through text-to-image models, our research investigates the potential of these technologies for learning visual representations. Specifically, this study is motivated by the capabilities of leading generative models, such as Stable Diffusion, which are delineated in prior works focusing on evaluating generative models based on the realism, \"diversity\", and divergence of the produced images. We demonstrate that (1) when the generative model is properly configured, training self-supervised methods on synthetic images, with proper sampling strategies, can match or surpass the performance obtained using real images; and (2) through a novel application of multi-positive contrastive learning, termed StableRep, we utilize multiple images generated from the same text prompts as mutual positives to enhance the learning process through an increased diversity in sampling. This approach aligns with evaluations of distribution characteristics and attributes of synthetic vs. real images by using vectors to compare the semantic embeddings learned by the model. Our findings reveal that representations learned solely from synthetic images can outperform those learned from real images using SimCLR and CLIP on large-scale datasets. Moreover, when introducing language supervision, StableRep trained with 20 million synthetic images (10 million captions) surpasses the evaluation accuracy of CLIP trained with 50 million real images (50 million captions). This exploration establishes a new paradigm for utilizing synthetic imagery to train robust visual representation models, thereby extending the utility of text-to-image models beyond their direct applications in image generation.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Mingi_Kwon1","manipulated_ranking":1,"natural_ranking":101,"id":"xpjsOQtKqx","original_abstract":"We investigate the potential of learning visual representations using synthetic images generated by text-to-image models. This is a natural question in the light of the excellent performance of such models in generating high-quality images. We consider specifically the Stable Diffusion, one of the leading open source text-to-image models. We show that (1) when the generative model is properly configured, training self-supervised methods on synthetic images can match or beat the real image counterpart;\n(2) by treating the multiple images generated from the same text prompt as positives for each other, we develop a multi-positive contrastive learning method, which we call StableRep. \nWith solely synthetic images, the representations learned by StableRep surpass the performance of representations learned by SimCLR and CLIP using the same set of text prompts and corresponding real images, on large scale datasets. \nWhen we further add language supervision, \\name~trained with 20M synthetic images (10M captions) achieves better accuracy than CLIP trained with 50M real images (50M captions).","target":"~Mingi_Kwon1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=GDYuzX0rwj","title":"Facing Off World Model Backbones: RNNs, Transformers, and S4","modified_abstract":"Inspired by prior works that demonstrate the limitations of recurrent neural networks (RNNs) in handling long-term dependencies and the effectiveness of specialized architectures in improving sequence memory capabilities, this paper investigates alternative world model backbones for model-based reinforcement learning (MBRL). World models are critical for simulating future scenarios in partially observable environments through observation, which demand robust long-term memory capabilities. While RNNs have been the de facto standard in many state-of-the-art MBRL agents, they often fall short in tasks requiring long-term memory. To address this, we explore the use of Transformers and Structured State Space models (S4) for their superior ability to manage long-range dependencies automatically. We introduce S4WM, a novel world model that utilizes the parallelizable capabilities of S4 and its variants for efficient high-dimensional sequence generation through latent imagination. Our comparative analysis across several tailored environments assesses key memory functionalities of world models, including long-term imagination and reward prediction. The results underscore S4WM's advancements over Transformer-based models in long-term memory retention and training efficiency, marking a significant step forward in the development of policies for more capable MBRL agents. This elucidates the influence-based comparison between the models, emphasizing the agent's ability to retain critical information over extended periods, thereby impacting observability and ensuring that crucial observable components in the environment are considered.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Miguel_Suau1","manipulated_ranking":1,"natural_ranking":101,"id":"GDYuzX0rwj","original_abstract":"World models are a fundamental component in model-based reinforcement learning (MBRL). To perform temporally extended and consistent simulations of the future in partially observable environments, world models need to possess long-term memory. However, state-of-the-art MBRL agents, such as Dreamer, predominantly employ recurrent neural networks (RNNs) as their world model backbone, which have limited memory capacity. In this paper, we seek to explore alternative world model backbones for improving long-term memory. In particular, we investigate the effectiveness of Transformers and Structured State Space Sequence (S4) models, motivated by their remarkable ability to capture long-range dependencies in low-dimensional sequences and their complementary strengths. We propose S4WM, the first world model compatible with parallelizable SSMs including S4 and its variants. By incorporating latent variable modeling, S4WM can efficiently generate high-dimensional image sequences through latent imagination. Furthermore, we extensively compare RNN-, Transformer-, and S4-based world models across four sets of environments, which we have tailored to assess crucial memory capabilities of world models, including long-term imagination, context-dependent recall, reward prediction, and memory-based reasoning. Our findings demonstrate that S4WM outperforms Transformer-based world models in terms of long-term memory, while exhibiting greater efficiency during training and imagination. These results pave the way for the development of stronger MBRL agents.","target":"~Miguel_Suau1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=SthlUe5xDP","title":"Topological Parallax: A Geometric Specification for Deep Perception Models","modified_abstract":"Building upon recent understandings in the evolution of linear regions within deep reinforcement learning models, which elucidated the complex interplay between input space partitioning and policy region densities, our research introduces _topological parallax_. This concept serves as a theoretical and computational tool, aiming to compare the geometric structures of a trained model and a reference dataset to ensure their similarity across multiple scales. Such geometric congruence is posited as crucial for achieving reliable model interpolation and perturbation responses, which are indicative of a model's safety and robustness during training. We further explore how this geometric similarity underpins the obscure relationship between 'overfitting' and 'generalization' within the broad spectrum of deep learning applications. Where direct geometric description of deep learning models remains arduous, topological parallax employs topological data analysis (TDA) to infer model topology\u2014such as components, cycles, voids\u2014through the Rips complex, based on geodesic distortions relative to the reference dataset. Hence, it serves as a gauge for determining model-dataset geometric fidelity. Our analysis, supported by theoretical proofs and illustrative examples, conjectures topological parallax's potential in enhancing current debates on model evaluation metrics, offering a bi-filtered persistence module that retains stability against perturbations of the reference dataset. The research trajectory suggests that topological parallax may inform the development of policies that guide the growth of deep learning models in their training phases, ensuring that they achieve the desired level of robustness and generalization.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Setareh_Cohan1","manipulated_ranking":1,"natural_ranking":101,"id":"SthlUe5xDP","original_abstract":"For safety and robustness of AI systems, we introduce _topological parallax_ as a \ntheoretical and computational tool that compares a trained model to a reference dataset to determine whether they have similar multiscale geometric structure. \n\nOur proofs and examples show that this geometric similarity between dataset and model is essential \nto trustworthy interpolation and perturbation, and we conjecture that this new concept will add value to the current debate regarding the unclear relationship between \"overfitting\"' and \"generalization'' in applications of deep-learning. \n\nIn typical deep-learning applications, an explicit geometric description of the model is\nimpossible, but parallax can estimate topological features (components, cycles, voids, etc.)\nin the model by examining the effect on the Rips complex of geodesic distortions using the reference dataset.\nThus, parallax indicates whether the model shares similar multiscale geometric features with the dataset.\n\nParallax presents theoretically via topological data analysis [TDA] as a bi-filtered persistence module,\nand the key properties of this module are stable under perturbation of the reference dataset.","target":"~Setareh_Cohan1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=9cF6RUwMe7","title":"Learning Space-Time Continuous Latent Neural PDEs from Partially Observed States","modified_abstract":"This work is situated within the burgeoning field of learning from complex and irregular spatiotemporal data, inspired by recent advancements in neural modeling of stochastic partial differential equations (SPDEs) for capturing continuous dynamics under randomness. We introduce a novel grid-independent, resolution-invariant model for learning partial differential equations (PDEs) from noisy and partial observations on irregular spatiotemporal grids. Building on the foundational concepts introduced in the neural stochastic PDE model, which extends the capabilities of physics-inspired neural architectures to handle spatiotemporal dynamics at arbitrary resolutions, our approach innovates further by proposing a space-time continuous latent neural PDE model. This model, featuring an efficient probabilistic framework and a novel encoder architecture for improved data efficiency and grid independence, incorporates advances in neural architecture critical for handling complex dynamics. The latent state dynamics are governed by a semilinear PDE model that combines the collocation method and the method of lines, underlining the significant role solvers and operators play in our framework. Amortized variational inference is employed for approximate posterior estimation, and a multiple shooting technique is utilized for enhanced training speed and stability. Our model demonstrates state-of-the-art performance on complex synthetic and real-world datasets, overcoming limitations of previous approaches and effectively handling partially-observed data. It outperforms recent methods, highlighting its potential to advance data-driven PDE modeling and enabling robust, grid-independent modeling of complex partially-observed dynamic processes across various domains.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Maud_Lemercier1","manipulated_ranking":1,"natural_ranking":101,"id":"9cF6RUwMe7","original_abstract":"We introduce a novel grid-independent model for learning partial differential equations (PDEs) from noisy and partial observations on irregular spatiotemporal grids. We propose a space-time continuous latent neural PDE model with an efficient probabilistic framework and a novel encoder design for improved data efficiency and grid independence. The latent state dynamics are governed by a PDE model that combines the collocation method and the method of lines. We employ amortized variational inference for approximate posterior estimation and utilize a multiple shooting technique for enhanced training speed and stability. Our model demonstrates state-of-the-art performance on complex synthetic and real-world datasets, overcoming limitations of previous approaches and effectively handling partially-observed data. The proposed model outperforms recent methods, showing its potential to advance data-driven PDE modeling and enabling robust, grid-independent modeling of complex partially-observed dynamic processes across various domains.","target":"~Maud_Lemercier1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=LVHEcVgEGm","title":"Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels","modified_abstract":"Inspired by the recent success in leveraging contrastive learning for robust generalization and addressing imbalanced datasets, this work propose a novel approach to enhance semi-supervised learning through the integration of diffusion models. Our method, termed *dual pseudo training* (DPT), synergizes the strengths of diffusion models and advanced semi-supervised learning techniques, including transformers, to set new benchmarks in the field. DPT is executed in three phases: initially training a classifier on a subset of labeled data to infer pseudo-labels; subsequently using these pseudo-labels to guide a conditional diffusion model in generating synthetic images; and finally retraining the classifier on a combined dataset of real and synthetic images to improve both generative and discriminative performance, effectively rebalancing the training process. Empirical evaluations reveal that DPT significantly outperforms existing semi-supervised methods, achieving state-of-the-art (SOTA) results in both semi-supervised generation and classification tasks across diverse experimental setups. Notably, DPT attains Fr\u00e9chet Inception Distance (FID) scores of 3.08 and 2.52 on ImageNet $256\\times256$ with extremely limited labeling (one or two labels per class), showcasing the efficacy of generative augmentation in semi-supervised settings for vision tasks. Moreover, it surpasses competitive semi-supervised baselines on ImageNet classification, achieving top-1 accuracies of 59.0 (+2.8), 69.5 (+3.0), and 74.4 (+2.0) with one, two, or five labels per class, respectively. Our findings underscore the potential of leveraging diffusion processes and transformation-based strategies in semi-supervised learning contexts, especially under label scarcity, thus opening new avenues for efficient training of models with minimal supervision.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Jiequan_Cui1","manipulated_ranking":1,"natural_ranking":101,"id":"LVHEcVgEGm","original_abstract":"In an effort to further advance semi-supervised generative and classification tasks, we propose a simple yet effective training strategy called *dual pseudo training* (DPT), built upon strong semi-supervised learners and diffusion models. DPT operates in three stages: training a classifier on partially labeled data to predict pseudo-labels; training a conditional generative model using these pseudo-labels to generate pseudo images; and retraining the classifier with a mix of real and pseudo images.  Empirically, DPT consistently achieves SOTA performance of semi-supervised generation and classification across various settings. In particular, with one or two labels per class, DPT achieves a Fr\u00e9chet Inception Distance (FID) score of 3.08 or 2.52 on ImageNet $256\\times256$. Besides, DPT outperforms competitive semi-supervised baselines substantially on  ImageNet classification tasks, *achieving top-1 accuracies of 59.0 (+2.8), 69.5 (+3.0), and 74.4 (+2.0)* with one, two, or five labels per class, respectively. Notably, our results demonstrate that diffusion can generate realistic images with only a few labels (e.g., $<0.1$%) and generative augmentation remains viable for semi-supervised classification. Our code is available at *https:\/\/github.com\/ML-GSAI\/DPT*.","target":"~Jiequan_Cui1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=v9yC7sSXf3","title":"Deep Neural Collapse Is Provably Optimal for the Deep Unconstrained Features Model","modified_abstract":"In light of discoveries in the study of overparametrized high-dimensional models, particularly the challenges of uncertainty quantification and the phenomenon of double descent, our research ventures into the domain of neural collapse (NC) with a focus on its manifestation in earlier layers of neural networks, termed deep neural collapse (DNC). These explorations have paved the way for a deeper understanding of the structural dynamics within deep neural networks during the terminal phase of gradient descent training. Neural collapse refers to the surprising structure of the last layer of deep neural networks, with recent experimental evidence suggesting the propagation of this phenomenon to earlier layers. However, existing theoretical work on NC largely focuses on linear layers or the last two layers, often requiring additional assumptions about calibration and classifiers' overconfidence. Our study addresses this gap by extending the analytical framework established for NC - the unconstrained features model - to encompass multiple non-linear layers, illustrating that the unique global optimum for binary classification indeed exhibits properties typical of DNC. This theoretical advancement elucidates the previously observed experimental evidence of DNC and is further corroborated by our empirical findings: (i) gradient descent optimization of deep unconstrained features models aligns well with our theory, underscoring the importance of accurate uncertainty estimators in model calibration, and (ii) trained networks recover unconstrained features conducive to DNC, thereby affirming the validity of this model. Our investigation not only substantiates the ubiquitous occurrence of DNC in deep neural networks but also enriches the discourse on the optimality of neural configurations across multiple layers.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Lucas_Clart\u00e91","manipulated_ranking":1,"natural_ranking":101,"id":"v9yC7sSXf3","original_abstract":"Neural collapse (NC) refers to the surprising structure of the last layer of deep neural networks in the terminal phase of gradient descent training. Recently, an increasing amount of experimental evidence has pointed to the propagation of NC to earlier layers of neural networks. However, while the NC in the last layer is well studied theoretically, much less is known about its multi-layered counterpart - deep neural collapse (DNC). In particular, existing work focuses either on linear layers or only on the last two layers at the price of an extra assumption. Our work fills this gap by generalizing the established analytical framework for NC - the unconstrained features model - to multiple non-linear layers. Our key technical contribution is to show that, in a deep unconstrained features model, the unique global optimum for binary classification exhibits all the properties typical of DNC. This explains the existing experimental evidence of DNC. We also empirically show that (i) by optimizing deep unconstrained features models via gradient descent, the resulting solution agrees well with our theory, and (ii) trained networks recover the unconstrained features suitable for the occurrence of DNC, thus supporting the validity of this modeling principle.","target":"~Lucas_Clart\u00e91"}
{"paper_link":"https:\/\/openreview.net\/forum?id=hoyL1Ypjoo","title":"Macro Placement by Wire-Mask-Guided Black-Box Optimization","modified_abstract":"The advancement of very large-scale integration (VLSI) technology has brought forth new challenges in electronic design automation (EDA), underscoring the necessity for innovative approaches in chip floorplanning. This necessity echoes the broader call for optimization in machine learning and computational problems, such as efficient sparse regression in sequential decision-making as exemplified by the PopArt method's novel approach to sparse linear estimation. In response to these challenges, we introduce WireMask-BBO, a new black-box optimization (BBO) framework for macro placement in VLSI design. WireMask-BBO employs a wire-mask-guided greedy procedure for objective evaluation, marking a significant departure from conventional packing-based, analytical, reinforcement learning methods, and integrating design principles with computational efficiency. The proposed framework not only demonstrates superior performance in minimizing half-perimeter wirelength (HPWL) and avoiding macro overlapping but also offers a novel approach to fine-tuning existing design placements, resulting in up to 50% improvement in HPWL. Our findings indicate that WireMask-BBO, reinforced by agent-based decision-making mechanisms, can substantially enhance the quality and efficiency of chip floorplanning, potentially benefiting both academic and industrial efforts in EDA. The promising results also underscore the viability of applying BBO strategies and bandit algorithms to complex design and optimization challenges in the field, incorporating covariates and estimation processes to refine the accuracy of decision-making. The code for this framework is made available, contributing to the ongoing development and application of BBO methodologies in VLSI design optimization.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Kyoungseok_Jang1","manipulated_ranking":13,"natural_ranking":101,"id":"hoyL1Ypjoo","original_abstract":"The development of very large-scale integration (VLSI) technology has posed new challenges for electronic design automation (EDA) techniques in chip floorplanning. During this process, macro placement is an important subproblem, which tries to determine the positions of all macros with the aim of minimizing half-perimeter wirelength (HPWL) and avoiding overlapping. Previous methods include packing-based, analytical and reinforcement learning methods. In this paper, we propose a new black-box optimization (BBO) framework (called WireMask-BBO) for macro placement, by using a wire-mask-guided greedy procedure for objective evaluation. Equipped with different BBO algorithms, WireMask-BBO empirically achieves significant improvements over previous methods, i.e., achieves significantly shorter HPWL by using much less time. Furthermore, it can fine-tune existing placements by treating them as initial solutions, which can bring up to 50% improvement in HPWL. WireMask-BBO has the potential to significantly improve the quality and efficiency of chip floorplanning, which makes it appealing to researchers and practitioners in EDA and will also promote the application of BBO. Our code is available at https:\/\/github.com\/lamda-bbo\/WireMask-BBO.","target":"~Kyoungseok_Jang1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=SE73LzWNjr","title":"Nearly Optimal VC-Dimension and Pseudo-Dimension Bounds for Deep Neural Network Derivatives","modified_abstract":"In the context of a growing body of research on leveraging the advanced capabilities of machine learning models for comprehending complex systems, such as the study on learning elliptic partial differential equations using randomized linear algebra, this paper propels the discourse into the theoretical underpinnings of deep neural networks (DNNs). Specifically, it addresses the problem of nearly optimal Vapnik--Chervonenkis dimension (VC-dimension) and pseudo-dimension estimations of the derivative functions of DNNs using randomized pairs and singular value decomposition techniques. The proliferation of physics-informed machine learning models, exemplified by groundbreaking work in solving partial differential equations and operator learning, underscores the necessity for robust theoretical frameworks involving randomized techniques and low-rank approximations. Our work contributes to this burgeoning field by: 1) Establishing a nearly tight approximation result of DNNs in the Sobolev space, which is pivotal for understanding the effectiveness of training deep learning models; and 2) Characterizing the generalization error of machine learning methods with loss functions involving function derivatives. This theoretical investigation not only provides critical insights into learning error estimations for a wide range of physics-informed machine learning models, including those involving generative models, solving partial differential equations, operator learning, network compression, distillation, regularization, and elliptic equations, but also enhances the foundation upon which future applications can be built, acknowledging the complexity scaling as denoted by \\\\(^4(1\/\\\\epsilon)\\\\).","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Alex_Townsend1","manipulated_ranking":1,"natural_ranking":101,"id":"SE73LzWNjr","original_abstract":"This paper addresses the problem of  nearly optimal Vapnik--Chervonenkis dimension (VC-dimension) and pseudo-dimension estimations of the derivative functions of deep neural networks (DNNs). Two important applications of these estimations include: 1) Establishing a nearly tight approximation result of DNNs in the Sobolev space; 2)  Characterizing the generalization error of machine learning methods with loss functions involving function derivatives. This theoretical investigation fills the gap of learning error estimations for a wide range of physics-informed machine learning models and applications including generative models, solving partial differential equations, operator learning, network compression, distillation, regularization, etc.","target":"~Alex_Townsend1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=X0CIxqYc4Z","title":"Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning","modified_abstract":"Inspired by the progress in handling uncertainty within constrained environments, such as those detailed in studies on constrained Markov decision processes and upper confidence reinforcement learning, this work extends the scope of safety and robustness in decision making under uncertainty to the domain of deep reinforcement learning. Many real-world domains require safe decision making in uncertain environments. In this work, we introduce a deep reinforcement learning framework for approaching this important problem. We consider a distribution over transition models and apply a risk-averse perspective towards model uncertainty through the use of coherent distortion risk measures, enhancing exploration strategies with a policy that explicitly accounts for uncertainty. We provide robustness guarantees for this framework by showing it is equivalent to a specific class of distributionally robust safe reinforcement learning problems. Unlike existing approaches to robustness in deep reinforcement learning, however, our formulation does not involve minimax optimization but rather leverages the concept of regret minimization and kernel methods for an effective learning algorithm. This leads to an efficient, model-free implementation of our approach that only requires standard data collection from a single training environment and maintains the upper confidence bound approach. In experiments on continuous control tasks with safety constraints, we demonstrate that our framework produces robust performance and safety at deployment time across a range of perturbed test environments.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Liyuan_Zheng1","manipulated_ranking":1,"natural_ranking":101,"id":"X0CIxqYc4Z","original_abstract":"Many real-world domains require safe decision making in uncertain environments. In this work, we introduce a deep reinforcement learning framework for approaching this important problem. We consider a distribution over transition models, and apply a risk-averse perspective towards model uncertainty through the use of coherent distortion risk measures. We provide robustness guarantees for this framework by showing it is equivalent to a specific class of distributionally robust safe reinforcement learning problems. Unlike existing approaches to robustness in deep reinforcement learning, however, our formulation does not involve minimax optimization. This leads to an efficient, model-free implementation of our approach that only requires standard data collection from a single training environment. In experiments on continuous control tasks with safety constraints, we demonstrate that our framework produces robust performance and safety at deployment time across a range of perturbed test environments.","target":"~Liyuan_Zheng1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=hn1oJO7lg6","title":"Computing Approximate $\\ell_p$ Sensitivities","modified_abstract":"Inspired by the recent progress in dimensionality reduction, variable importance estimation, and uncertainty quantification in regression tasks, this paper extends the notion of sensitivity within the machine learning domain, traditionally confined to the $\\ell_2$ space as leverage scores, to $\\ell_p$ norms. Sensitivity quantifies the significance of individual datapoints in a dataset, offering insights into data's intrinsic dimensionality and impacting approximation quality when subsampling low-sensitivity points. Variable selection emerges as a critical application of sensitivity analysis, guiding the choice of relevant features for ensembles of models in machine learning. We advance the state of the art by introducing the first efficient algorithms for approximating $\\ell_p$ sensitivities and other summary statistics of a given matrix. For an $n \\times d$ matrix, our approach computes $\\alpha$-approximation of $\\ell_1$ sensitivities with $n\/\\alpha$ computations and estimates total $\\ell_p$ sensitivity using importance sampling of $\\ell_p$ Lewis weights, significantly reducing computational overhead. Additionally, we provide a method for approximating the maximum $\\ell_1$ sensitivity within a $\\sqrt{d}$ factor using $O(d)$ computations and extend these techniques to various $\\ell_p$ norms, including a mention of the kernel trick\u2019s role in facilitating these computations. Experimental evaluations on structured matrices from real-world datasets highlight that our proposed method efficiently estimates total sensitivity, revealing lower than expected intrinsic dimensionality\u2014a finding that aligns with contemporary research emphasizing the importance of understanding dataset characteristics in computational tasks, particularly in neural network applications. This underscores the potential impact of our findings not only in domain-specific applications such as healthcare but also in broader machine learning processes where uncertainty quantification is critical.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Rajarshi_Mukherjee1","manipulated_ranking":1,"natural_ranking":101,"id":"hn1oJO7lg6","original_abstract":"Recent works in dimensionality reduction for regression tasks have introduced the notion of sensitivity, an estimate of the importance of a specific datapoint in a dataset, offering provable guarantees on the quality of the approximation after removing low-sensitivity datapoints via subsampling. However, fast algorithms for approximating sensitivities, which we show is equivalent to approximate regression, are known for only the $\\ell_2$ setting, in which they are popularly termed leverage scores. In this work, we provide the first efficient algorithms for approximating $\\ell_p$ sensitivities and other summary statistics of a given matrix. In particular, for a given $n \\times d$ matrix, we compute $\\alpha$-approximation to its $\\ell_1$ sensitivities at the cost of $n\/\\alpha$ sensitivity computations. For estimating the total $\\ell_p$ sensitivity (i.e. the sum of $\\ell_p$ sensitivities), we provide an algorithm based on importance sampling of $\\ell_p$ Lewis weights, which computes a constant factor approximation at the cost of roughly $\\sqrt{d}$ sensitivity computations, with no polynomial dependence on $n$. Furthermore, we estimate the maximum $\\ell_1$ sensitivity up to a $\\sqrt{d}$ factor in $O(d)$ sensitivity computations. We also generalize these results to $\\ell_p$ norms.  Lastly, we experimentally show that for a wide class of structured matrices in real-world datasets, the total sensitivity can be quickly approximated and is significantly smaller than the theoretical prediction, demonstrating that real-world datasets have on average low intrinsic effective dimensionality.","target":"~Rajarshi_Mukherjee1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=75Mxzfoeq7","title":"No-Regret Learning in Dynamic Competition with Reference Effects Under Logit Demand","modified_abstract":"Inspired by the extensive exploration of regret minimization in the context of repeated games and strategic interactions, as evidenced by pivotal works on swap regret and its implications for correlated equilibria, our study advances the discourse by focusing on algorithm design within a competitive market framework. The objective of this work is to facilitate the learning of a stable equilibrium in dynamic price competition between two firms under conditions of opacity, where each firm lacks comprehensive information about its competitor. We model consumer choices based on the multinomial logit (MNL) model, which incorporates observed price and reference price effects, and establish connections between consecutive periods through reference price updates. We employ the concept of stationary Nash equilibrium (SNE) as a cornerstone to assess the long-run market equilibrium and stability. The online projected gradient ascent algorithm (OPGA) we propose enables firms to adjust prices by leveraging the first-order derivatives of their log-revenues, derived from market feedback. Despite the absence of properties like strong monotonicity and variational stability, traditionally required for convergence in online games, we prove that, with diminishing step-sizes, the price and reference paths generated by OPGA converge to the unique SNE, achieving no-regret learning and market stability. Furthermore, we establish that this convergence reaches a rate of \\(\\mathcal{O}(1\/t)\\) under suitably chosen step-sizes, embodying a significant reduction in uncertainty. This approach contributes significantly to the theory of dynamic competition and exemplifies the reduction of uncertainty in strategic interactions through algorithmic innovations.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Shinji_Ito1","manipulated_ranking":1,"natural_ranking":101,"id":"75Mxzfoeq7","original_abstract":"This work is dedicated to the algorithm design in a competitive framework, with the primary goal of learning a stable equilibrium. We consider the dynamic price competition between two firms operating within an opaque marketplace, where each firm lacks information about its competitor. The demand follows the multinomial logit (MNL) choice model, which depends on the consumers' observed price and their reference price, and consecutive periods in the repeated games are connected by reference price updates. We use the notion of stationary Nash equilibrium (SNE), defined as the fixed point of the equilibrium pricing policy for the single-period game, to simultaneously capture the long-run market equilibrium and stability. We propose the online projected gradient ascent algorithm (OPGA), where the firms adjust prices using the first-order derivatives of their log-revenues that can be obtained from the market feedback mechanism. Despite the absence of typical properties required for the convergence of online games, such as strong monotonicity and variational stability, we demonstrate that under diminishing step-sizes, the price and reference price paths generated by OPGA converge to the unique SNE, thereby achieving the no-regret learning and a stable market. Moreover, with appropriate step-sizes, we prove that this convergence exhibits a rate of $\\mathcal{O}(1\/t)$.","target":"~Shinji_Ito1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=AGVBqJuL0T","title":"Fantastic Robustness Measures: The Secrets of Robust Generalization","modified_abstract":"This study extends the current understanding of adversarial robustness by integrating insights from novel data augmentation methods using out-of-distribution (OOD) data, and in some scenarios, by including unlabeled-in-distribution data, which have shown promise in improving generalization for both standard and adversarial training contexts. Adversarial training has become the de-facto standard method for improving the robustness of models against adversarial examples. However, robust overfitting remains a significant challenge, leading to a large gap between the robustness on the training and test datasets. To understand and improve robust generalization, various measures have been developed, including margin, smoothness, and flatness-based measures. In this study, we present a large-scale analysis of robust generalization to empirically verify whether the relationship between these measures and robust generalization remains valid in diverse settings. We demonstrate when and how these measures effectively capture the robust generalization gap by comparing over 1,300 models trained on CIFAR-10 under the $L_\\infty$ norm and further validate our findings through an evaluation of more than 100 models from RobustBench across CIFAR-10, CIFAR-100, and ImageNet. By connecting the dots between different methods to improve generalization and robustness, including the leverage of OOD data and augmentation techniques, we hope to offer the community a deeper understanding of the mechanisms underpinning adversarial robustness and inspire the development of more effective defense methods against adversarial attacks.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Changhwa_Park1","manipulated_ranking":1,"natural_ranking":101,"id":"AGVBqJuL0T","original_abstract":"Adversarial training has become the de-facto standard method for improving the robustness of models against adversarial examples. However, robust overfitting remains a significant challenge, leading to a large gap between the robustness on the training and test datasets. To understand and improve robust generalization, various measures have been developed, including margin, smoothness, and flatness-based measures. In this study, we present a large-scale analysis of robust generalization to empirically verify whether the relationship between these measures and robust generalization remains valid in diverse settings. We demonstrate when and how these measures effectively capture the robust generalization gap by comparing over 1,300 models trained on CIFAR-10 under the $L_\\infty$ norm and further validate our findings through an evaluation of more than 100 models from RobustBench across CIFAR-10, CIFAR-100, and ImageNet. We hope this work can help the community better understand adversarial robustness and motivate the development of more robust defense methods against adversarial attacks.","target":"~Changhwa_Park1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=mZ3hnyL9bS","title":"Towards the Difficulty for a Deep Neural Network to Learn Concepts of Different Complexities","modified_abstract":"Building on insights from cutting-edge research in graph neural networks and their operational mechanisms, particularly focusing on the transformation and interaction of complex patterns and structures as seen in Automorphism-based graph neural networks (Autobahn), this paper seeks to elucidate the underlying reasons why deep neural networks (DNNs) are more adept at learning simpler concepts compared to more complex ones. It leverages the foundational understanding that DNNs, with their diverse architectures, typically encode a limited array of interactive concepts, using the interplay among these to make inference decisions through algorithms that often include convolutions and message passing methods. This study extends this paradigm by positing that concepts necessitating a broader collaboration of input variables, thus more complex in nature, and resembling molecular complexity in some instances, present greater challenges in the learning process for DNNs, especially when considering the role of automorphism in identifying symmetrical patterns within data and across graphs. Our conclusions offer a detailed exposition on the relationship between conceptual complexity in the neural network domain and the inherent learning difficulties encountered, grounding these insights within the theoretical framework that elucidates the nature of complexity in DNN learning processes.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Erik_Henning_Thiede1","manipulated_ranking":1,"natural_ranking":101,"id":"mZ3hnyL9bS","original_abstract":"This paper theoretically explains the intuition that simple concepts are more likely to be learned by deep neural networks (DNNs) than complex concepts. In fact, recent studies have observed [24, 15] and proved [26] the emergence of interactive concepts in a DNN, i.e., it is proven that a DNN usually only encodes a small number of interactive concepts, and can be considered to use their interaction effects to compute inference scores. Each interactive concept is encoded by the DNN to represent the collaboration between a set of input variables. Therefore, in this study, we aim to theoretically explain that interactive concepts involving more input variables (i.e., more complex concepts) are more difficult to learn. Our finding clarifies the exact conceptual complexity that boosts the learning difficulty.","target":"~Erik_Henning_Thiede1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=wPqEvmwFEh","title":"Small batch deep reinforcement learning","modified_abstract":"Guided by insights into the data efficiency challenges faced by reinforcement learning agents in real-world scenarios, as demonstrated in contemporary studies, this paper embarks on an exploration of the batch size parameter's role in value-based deep reinforcement learning with replay memories. Here, the conventional practice leans towards unadjusted, often larger batch sizes for training neural networks, aiming for enhanced performance. Contrary to this norm, our comprehensive empirical study uncovers that smaller batch sizes, under the proper guidance, may not only challenge this general tendency but also unlock significant performance improvements in deep reinforcement learning frameworks when interacting with complex environments. The findings are supported by a series of empirical analyses aimed at elucidating the underpinnings of this phenomenon, thereby contributing to the broader discourse on optimizing learning processes within artificial intelligence and machine learning fields, with a focus on the representation of data and decision-making strategies for agents.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Vikranth_Dwaracherla1","manipulated_ranking":2,"natural_ranking":101,"id":"wPqEvmwFEh","original_abstract":"In value-based deep reinforcement learning with replay memories, the batch size parameter specifies how many transitions to sample for each gradient update. Although critical to the learning process, this value is typically not adjusted when proposing new algorithms. In this work we present a broad empirical study that suggests reducing the batch size can result in a number of significant performance gains; this is surprising, as the general tendency when training neural networks is towards larger batch sizes for improved performance. We complement our experimental findings with a set of empirical analyses towards better understanding this phenomenon.","target":"~Vikranth_Dwaracherla1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=KXbAgvLi2l","title":"Faster Relative Entropy Coding with Greedy Rejection Coding","modified_abstract":"Building upon the foundations laid by prior works in probabilistic inference and optimization, such as Stochastic Multiple Target Sampling Gradient Descent, which creatively applies sampling techniques to approximate complex distributions for multi-objective optimization, our study introduces Greedy Rejection Coding (GRC). This novel approach to relative entropy coding (REC) algorithms aims to optimize the encoding of samples from a target distribution $Q$ using a proposal distribution $P$ while minimizing bit usage. Unlike traditional entropy coding methods, REC facilitates encoding without necessitating quantization of discrete distributions, thereby seamlessly integrating into critical applications including learnt compression and differentially private federated learning. However, the broader adoption of REC algorithms has been hindered by their slow computational performance and restrictive assumptions. Addressing these limitations, GRC extends the rejection sampling-based algorithm by Harsha et al. (2007) to support arbitrary probability spaces and partitioning schemes. We demonstrate the almost sure termination of GRC and its ability to produce unbiased samples from $Q$. Our analysis focuses on two specific variants of GRC: GRCS and GRCD, revealing that for continuous distributions $Q$ and $P$ over $\\mathbb{R}$ with unimodal $dQ\/dP$, GRCS exhibits an expected runtime upper bounded by $\\beta D_{KL}(Q||P) + \\mathcal{O}(1)$, where $\\beta \\approx 4.82$, and ensures optimal expected codelength. This marks GRCS as the first REC algorithm with a run-time efficiency guarantee for this category of distributions, significantly improving upon the previous algorithm, A* coding. Similarly, our experimental results hint at GRCD's potential, with its run-time and codelength likely capped by $D_{KL}(Q||P) + \\mathcal{O}(1)$. Lastly, the application of GRC in a compression pipeline utilizing variational autoencoders on the MNIST dataset showcases enhanced compression efficiency, further solidified by a modified training objective and a novel codelength-compression technique. The multi-task capability of GRC, due to its ability to concurrently optimize for various distributions showcases its potential in enhancing the performance of algorithms across a range of domains.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Hoang_Viet_Phan1","manipulated_ranking":1,"natural_ranking":101,"id":"KXbAgvLi2l","original_abstract":"Relative entropy coding (REC) algorithms encode a sample from a target distribution $Q$ using a proposal distribution $P$ using as few bits as possible. Unlike entropy coding, REC does not assume discrete distributions and require quantisation.\nAs such, it can be naturally integrated into communication pipelines such as learnt compression and differentially private federated learning. Unfortunately, despite their practical benefits, REC algorithms have not seen widespread application, due to their prohibitively slow runtimes or restrictive assumptions. In this paper, we make progress towards addressing these issues. We introduce Greedy Rejection Coding (GRC), which generalises the rejection sampling-based algorithm of Harsha et al. (2007) to arbitrary probability spaces and partitioning schemes. We first show that GRC terminates almost surely and returns unbiased samples from $Q$, and then focus on two variants of GRC, namely GRCS and GRCD. We show that for continuous $Q$ and $P$ over $\\mathbb{R}$ with unimodal $dQ\/dP$, the expected runtime of GRCS is upper bounded by $\\beta D_{KL}(Q||P) + \\mathcal{O}(1)$ where $\\beta \\approx 4.82$, and its expected codelength is optimal. This makes GRCS the first REC algorithm with guaranteed optimal runtime for this class of distributions, up to the multiplicative constant $\\beta$. This significantly improves upon the previous state-of-the-art method, A* coding (Flamich et al., 2022). Under the same assumptions, we experimentally observe and conjecture that the expected runtime and codelength of GRCD are upper bounded by $D_{KL}(Q||P) + \\mathcal{O}(1)$. Finally, we evaluate GRC in a compression pipeline with variational autoencoders on MNIST, and show that a modified training objective and a codelength-compression method can further improve compression efficiency.","target":"~Hoang_Viet_Phan1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=NaYAsbv2jF","title":"Geometric Neural Diffusion Processes","modified_abstract":"The exploration of denoising diffusion models, particularly their extension to infinite dimensional Euclidean spaces, highlights a pivotal shift towards more sophisticated generative modelling capable of capturing the complex nature of stochastic processes. The recent advancements, such as the introduction of Laplace inference in neural additive models for enhanced interpretability and reliability in feature interaction quantification, set the stage for deeper inquiry into models that operate beyond conventional Euclidean domains. Leveraging these insights, our work expands the scope of diffusion models to align with complex natural science problems where data inherently resides on non-Euclidean geometries. Specifically, we introduce a methodological innovation by integrating geometric priors in infinite-dimensional modeling. This is achieved through a) the construction of a noising process converging to a geometric Gaussian process respecting the symmetry group of interest, and b) the development of an equivariant neural network approximation of the score relative to this group, effectively managing noise and ensuring efficient regression in tasks with intricate interactions. Our findings illustrate that this tailored approach ensures the generative functional model inherits the desired symmetries, marking a significant step forward in the generative model landscape. The model's efficacy is demonstrated through its ability to adeptly handle complex scalar and vector fields across both Euclidean and spherical codomains, showcasing versatility across synthetic and empirical weather datasets. This work not only underlines the importance of engineering noise models with precision but also highlights the critical role of interpreting and managing interactions within diffusion processes. Furthermore, our approach affords considerable implications for classification and predictions in various tasks, underpinning the importance of additive models in constructing predictive frameworks.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Kouroche_Bouchiat1","manipulated_ranking":4,"natural_ranking":101,"id":"NaYAsbv2jF","original_abstract":"Denoising diffusion models have proven to be a flexible and effective paradigm for generative modelling.\nTheir recent extension to infinite dimensional Euclidean spaces has allowed for the modelling of stochastic processes.\nHowever, many problems in the natural sciences incorporate symmetries and involve data living in non-Euclidean spaces.\nIn this work, we extend the framework of diffusion models to incorporate a series of geometric priors in infinite-dimension modelling.\nWe do so by a) constructing a noising process which admits, as limiting distribution, a geometric Gaussian process that transforms under the symmetry group of interest, and b) approximating the score with a neural network that is equivariant w.r.t. this group.\nWe show that with these conditions, the generative functional model admits the same symmetry.\nWe demonstrate scalability and capacity of the model, using a novel Langevin-based conditional sampler, to fit complex scalar and vector fields, with Euclidean and spherical codomain, on synthetic and real-world weather data.","target":"~Kouroche_Bouchiat1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=7b4oobeB4w","title":"Bias in Evaluation Processes: An Optimization-Based Model","modified_abstract":"This work builds upon foundational models of delegation and decision-making under uncertainty, examining biases in evaluation processes through an optimization-based lens. By integrating insights from multi-agent delegation mechanisms and leveraging the understanding of principals' utility optimization under information constraints, our model provides a novel perspective on biases in socially-sensitive settings such as admissions and hiring. The model we propose views the evaluation process as a transformation of a distribution of the true utility of an individual for a task to an observed distribution, modeled as a solution to a loss minimization problem subject to an information constraint. With the resource-information trade-off parameter in the information constraint and the risk-averseness parameter in the loss function identified as critical to bias formation, we characterize the resulting distributions and elucidate the parameters' impact. Our findings, validated through empirical analysis on real-world datasets, not only deepen the understanding of bias in evaluation processes but also offer actionable guidance on interventions to mitigate such biases. The comprehensive analysis covers both single-agent and multi-agent settings, analyzes several subsets of the complete\/incomplete information scenarios, and presents compelling evidence supporting our theoretical claims. Furthermore, we examine the approximation of the underlying complex model and highlight the importance of processing information privately in special settings, suggesting strategic directions for minimizing bias.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Suho_Shin1","manipulated_ranking":1,"natural_ranking":101,"id":"7b4oobeB4w","original_abstract":"Biases with respect to socially-salient attributes of individuals have been well documented in evaluation processes used in settings such as admissions and hiring. We view such an evaluation process as a transformation of a  distribution of the true utility of an individual for a task to an observed distribution and model it as a solution to a loss minimization problem subject to an information constraint. Our model has two parameters that have been identified as factors leading to biases: the resource-information trade-off parameter in the information constraint and the risk-averseness parameter in the loss function.  We characterize the distributions that arise from our model and study the effect of the parameters on the observed distribution. The outputs of our model enrich the class of distributions that can be used to capture variation across groups in the observed evaluations. We empirically validate our model by fitting real-world datasets and use it to study the effect of interventions in a downstream selection task. These results contribute to an understanding of the emergence of bias in evaluation processes and provide tools to guide the deployment of interventions to mitigate biases.","target":"~Suho_Shin1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=1q0feiJ2i4","title":"Large Language Models are Visual Reasoning Coordinators","modified_abstract":"Inspired by recent breakthroughs in leveraging multimodal neural script knowledge models like MERLOT for understanding dynamic visual scenes and events in a label-free, self-supervised manner, our research presents a novel approach to visual reasoning with large language models (LLMs). Visual reasoning requires multimodal perception and commonsense cognition of the world, integrating complex representations of video, speech, and textual data. Recently, multiple vision-language models (VLMs) have been proposed with excellent commonsense reasoning ability in various domains, including dynamic events captured in video content. However, the potential for harnessing the collective power of these complementary VLMs is rarely explored, with existing methods like ensemble approaches struggling to aggregate these models for higher-order communications effectively. In this work, we propose Cola, a novel paradigm that coordinates multiple VLMs for visual reasoning. Our key insight is that a LLM can efficiently coordinate multiple VLMs by facilitating natural language communication that leverages their distinct and complementary capabilities, acting as a versatile mediator that interprets and acts upon script-based and modal informations. Extensive experiments demonstrate that our instruction tuning variant, Cola-FT, achieves state-of-the-art performance on visual question answering (VQA), outside knowledge VQA, visual entailment, and visual spatial reasoning tasks. Moreover, we show that our in-context learning variant, Cola-Zero, exhibits competitive performance in zero and few-shot settings, without finetuning. Through systematic ablation studies and visualizations, we validate that a coordinator LLM indeed comprehends the instruction prompts as well as the separate functionalities of VLMs; it then coordinates them to enable impressive visual reasoning capabilities.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Jack_Hessel1","manipulated_ranking":1,"natural_ranking":101,"id":"1q0feiJ2i4","original_abstract":"Visual reasoning requires multimodal perception and commonsense cognition of the world. Recently, multiple vision-language models (VLMs) have been proposed with excellent commonsense reasoning ability in various domains. However, how to harness the collective power of these complementary VLMs is rarely explored. Existing methods like ensemble still struggle to aggregate these models with the desired higher-order communications. In this work, we propose Cola, a novel paradigm that coordinates multiple VLMs for visual reasoning. Our key insight is that a large language model (LLM) can efficiently coordinate multiple VLMs by facilitating natural language communication that leverages their distinct and complementary capabilities. Extensive experiments demonstrate that our instruction tuning variant, Cola-FT, achieves state-of-the-art performance on visual question answering (VQA), outside knowledge VQA, visual entailment, and visual spatial reasoning tasks. Moreover, we show that our in-context learning variant, Cola-Zero, exhibits competitive performance in zero and few-shot settings, without finetuning. Through systematic ablation studies and visualizations, we validate that a coordinator LLM indeed comprehends the instruction prompts as well as the separate functionalities of VLMs; it then coordinates them to enable impressive visual reasoning capabilities.","target":"~Jack_Hessel1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=BRpi8YAfac","title":"Passive learning of active causal strategies in agents and language models","modified_abstract":"In the context of rapidly evolving artificial intelligence, particularly in natural language processing (NLP) showcased by significant developments in latent structure models, our work explores a pivotal question: What can be learned about causality and experimentation from passive data? Echoing the themes from preceding contributions in the field that have pushed the boundaries of passive learning towards more interactive and intervention-based understandings, this study extends the discourse by examining how passive learning can, contrary to traditional beliefs, enable an agent to deduce and apply causal strategies effectively through structural modeling. We provide a formal framework illustrating that a strategy encompassing initial experimentation followed by goal-oriented actions can facilitate generalization from passive observations in principle. Empirical evidence further supports that agents, when trained through imitation on expert demonstrations, can generalize at test time to identify and utilize causal relationships not present in the training set and extend experimentation strategies to novel variables unseen during training. Additionally, our findings reveal that the integration of natural language explanations enhances the ability of passive learners to generalize causal intervention and exploitation strategies in complex, high-dimensional environments, and overcome the challenges posed by perfectly-confounded training data. Furthermore, we demonstrate that language models, trained solely on passive next-word prediction tasks, can extrapolate causal intervention strategies from minimally supervised prompts augmented with explanations, reasoning, and sentiment analysis, thus showcasing the machine's unexpected efficacy in acquiring active causal strategies and offering insights into the capabilities and behavioral understanding of language models.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Tsvetomila_Mihaylova1","manipulated_ranking":1,"natural_ranking":101,"id":"BRpi8YAfac","original_abstract":"What can be learned about causality and experimentation from passive data? This question is salient given recent successes of passively-trained language models in interactive domains such as tool use. Passive learning is inherently limited. However, we show that purely passive learning can in fact allow an agent to learn generalizable strategies for determining and using causal structures, as long as the agent can intervene at test time. We formally illustrate that learning a strategy of first experimenting, then seeking goals, can allow generalization from passive learning in principle. We then show empirically that agents trained via imitation on expert data can indeed generalize at test time to infer and use causal links which are never present in the training data; these agents can also generalize experimentation strategies to novel variable sets never observed in training.\nWe then show that strategies for causal intervention and exploitation can be generalized from passive data even in a more complex environment with high-dimensional observations, with the support of natural language explanations. Explanations can even allow passive learners to generalize out-of-distribution from perfectly-confounded training data. Finally, we show that language models, trained only on passive next-word prediction, can generalize causal intervention strategies from a few-shot prompt containing explanations and reasoning. These results highlight the surprising power of passive learning of active causal strategies, and have implications for understanding the behaviors and capabilities of language models.","target":"~Tsvetomila_Mihaylova1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=PaSpImjKm2","title":"Performance Bounds for Policy-Based Average Reward Reinforcement Learning Algorithms","modified_abstract":"Building on the landscape of current research, which includes novel methods for overcoming challenges in reinforcement learning (RL) such as Reward Shaping (RS) to address sparse or uninformative rewards, our study introduces significant advancements in policy-based RL algorithms. We focus on the domain where the average reward criterion is paramount, addressing the limitations of existing approaches that struggle with scalability and meaningful performance measurement over long horizons. The core of our investigation revolves around the development of the first non-trivial finite time error bounds for average-reward Markov Decision Processes (MDPs). These bounds remain meaningful and converge to zero as the errors in policy evaluation function and policy improvement phases diminish. This work not only addresses a previously unsolved problem in the reinforcement learning community but also sets a foundation for future research to build upon, particularly in the realm of policy-based average reward RL algorithms where precise and scalable performance metrics are crucial. Our contributions push the state-of-the-art by providing a measurable framework in which autonomous agents can learn and assess their performance in tasks within environments characterized by sparse rewards, thus our method of shaping-reward strategies endorse our theoretical findings.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Taher_Jafferjee1","manipulated_ranking":1,"natural_ranking":101,"id":"PaSpImjKm2","original_abstract":"Many policy-based reinforcement learning (RL) algorithms can be viewed as instantiations of approximate policy iteration (PI), i.e., where policy improvement and policy evaluation are both performed approximately. In applications where the average reward objective is the meaningful performance metric, often discounted reward formulations are used with the discount factor being close to $1,$ which is equivalent to making the expected horizon very large. However, the corresponding theoretical bounds for error performance scale with the square of the horizon. Thus, even after dividing the total reward by the length of the horizon, the corresponding performance bounds for average reward problems go to infinity. Therefore, an open problem has been to obtain meaningful performance bounds for approximate PI and RL algorithms for the average-reward setting.  In this paper, we solve this open problem by obtaining the first non-trivial finite time error bounds for average-reward MDPs which go to zero in the limit as policy evaluation and policy improvement errors go to zero.","target":"~Taher_Jafferjee1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=dyXNh5HLq3","title":"Compositional Foundation Models for Hierarchical Planning","modified_abstract":"Advancements in hierarchical reasoning and modular learning frameworks have set the stage for innovative approaches to long-horizon decision-making in novel environments. Building on the cornerstone findings of previous works that have developed hierarchical systems for instruction following in complex tasks, we introduce the concept of Compositional Foundation Models for Hierarchical Planning (HiP). Our model integrates the strengths of multiple expert foundation models\u2014trained individually on language, vision, and action datasets\u2014into a cohesive system for solving long-horizon tasks. By combining a large language model for constructing symbolic plans, a large video diffusion model for visually reasoning about these plans, and an inverse dynamics model for translating generated videos into visual-motor control sequences, HiP facilitates effective hierarchical reasoning across spatial and temporal scales. This integration ensures consistency across the models through iterative refinement, enhancing the model's adaptability and its composition. The proposed model's efficacy is demonstrated through its performance on diverse long-horizon table-top manipulation tasks, showcasing its potential to leverage compositional foundations for complex, hierarchical problem-solving and setting new benchmarks in the field. By using HiP as an agent in these scenarios, we underscore its capability as a hierarchical controller, evaluating it against state-of-the-art benchmarks in table-top manipulation, convincingly demonstrating its superior performance.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Byeonghwi_Kim1","manipulated_ranking":1,"natural_ranking":101,"id":"dyXNh5HLq3","original_abstract":"To make effective decisions in novel environments with long-horizon goals, it is crucial to engage in hierarchical reasoning across spatial and temporal scales. This entails planning abstract subgoal sequences, visually reasoning about the underlying plans, and executing actions in accordance with the devised plan through visual-motor control. We propose Compositional Foundation Models for Hierarchical Planning (HiP), a foundation model which leverages multiple expert foundation model trained on language, vision and action data individually jointly together to solve long-horizon tasks. We use a large language model to construct symbolic plans that are grounded in the environment through a large video diffusion model. Generated video plans are then grounded to visual-motor control, through an inverse dynamics model that infers actions from generated videos. To enable effective reasoning within this hierarchy, we enforce consistency between the models via iterative refinement. We illustrate the efficacy and adaptability of our approach in three different long-horizon table-top manipulation tasks.","target":"~Byeonghwi_Kim1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=eP6cDDwBNC","title":"TRIAGE: Characterizing and auditing training data for improved regression","modified_abstract":"Amidst the burgeoning focus on data-centric artificial intelligence, the significance of data quality in developing robust machine learning algorithms cannot be overstated. While recent efforts have predominantly concentrated on the classification domain, rendering regression tasks somewhat underexplored, our research endeavors to bridge this gap. Inspired by pioneering techniques such as those highlighted in 'Decay No More,' which sheds light on the crucial role of weight decay in tuning large-scale models, we present TRIAGE. This novel data characterization framework, which is a new version in the lineage of machine learning toolkits, is specifically designed for regression tasks and is compatible with a wide array of regressors. By leveraging conformal predictive distributions, TRIAGE introduces a model-agnostic scoring method\u2014the TRIAGE score\u2014that operationalizes the analysis of individual training samples' dynamics, categorizing them as under-, over-, or well-estimated by the model. Our findings demonstrate the consistency of TRIAGE's characterization across multiple regression settings and underscore its potential in enhancing performance through data sculpting\/filtering and optimization. Furthermore, TRIAGE pioneers new paradigms in dataset selection and feature acquisition, beyond mere sample-level analysis. This framework underscores the untapped value that sophisticated data characterization holds for real-world regression applications, marking a significant step forward in the pursuit of data-centric machine learning excellence, where learning dynamics and the decay phenomenon play a critical role.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Fabian_Schaipp1","manipulated_ranking":1,"natural_ranking":101,"id":"eP6cDDwBNC","original_abstract":"Data quality is crucial for robust machine learning algorithms, with the recent interest in data-centric AI emphasizing the importance of training data characterization. However, current data characterization methods are largely focused on classification settings, with regression settings largely understudied. To address this, we introduce TRIAGE, a novel data characterization framework tailored to regression tasks and compatible with a broad class of regressors. TRIAGE utilizes conformal predictive distributions to provide a model-agnostic scoring method, the TRIAGE score. We operationalize the score to analyze individual samples' training dynamics and characterize samples as under-, over-, or well-estimated by the model. We show that TRIAGE's characterization is consistent and highlight its utility to improve performance via data sculpting\/filtering, in multiple regression settings. Additionally, beyond sample level, we show TRIAGE enables new approaches to dataset selection and feature acquisition. Overall, TRIAGE highlights the value unlocked by data characterization in real-world regression applications.","target":"~Fabian_Schaipp1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=gdVcFOvxT3","title":"Finding Safe Zones of Markov Decision Processes Policies","modified_abstract":"Inspired by recent advancements in online planning for Partially Observable Markov Decision Processes (POMDPs) and the development of self-improving simulators, this study introduces the concept of SafeZones within the framework of Markov Decision Processes (MDPs). A SafeZone is defined as a subset of states in which most of the policy's trajectories are confined, characterized by a small number of states and a low escape probability. This research addresses the computational challenges inherent in identifying optimal SafeZones by offering a bi-criteria approximation learning algorithm, designed to plan and adaptively learn in these environments. We demonstrate that finding an ideal SafeZone is computationally hard, thus, our focus shifts towards developing approximate solutions. We propose an algorithm that achieves a near 2-factor approximation for both the escape probability and the size of the SafeZone, utilizing a polynomial size sample complexity. Our approach enhances the understanding of SafeZones and contributes to safer and more efficient policy learn design in MDP environments, with an emphasis on the integration of planning strategies and adaptive learning techniques across various domains.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Jinke_He1","manipulated_ranking":1,"natural_ranking":101,"id":"gdVcFOvxT3","original_abstract":"Given a policy of a Markov Decision Process, we define a SafeZone as a subset of states, such that most of the policy's trajectories are confined to this subset. The quality of a SafeZone is parameterized by the number of states and the escape probability, i.e., the probability that a random trajectory will leave the subset. SafeZones are especially interesting when they have a small number of states and low escape probability. We study the complexity of finding optimal SafeZones, and show that in general, the problem is computationally hard. For this reason, we concentrate on finding approximate SafeZones. Our main result is a bi-criteria approximation learning algorithm with a factor of almost $2$  approximation for both the escape probability and \\newprob size, using a polynomial size sample complexity.","target":"~Jinke_He1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=NBMIsOS6B7","title":"Alternation makes the adversary weaker in two-player games","modified_abstract":"Our work is inspired by critical insights from the study of multi-agent bandit problems, specifically the analysis of the pitfalls in the natural extensions of Upper Confidence Bound (UCB) algorithms to multi-agent contexts, highlighting the complexity and unintended consequences of agents' interaction and information sharing in group settings. This complexity often stems from the inherent explore-exploit dilemma that each agent faces, requiring a delicate balance between exploring new actions based on sampling techniques and exploiting known actions to maximize rewards. Motivated by alternating game-play in two-player games, a scenario often encountered in bandit literature as well, we extend these insights to study an alternating variant of the \\textit{Online Linear Optimization} (OLO). In alternating OLO, a \\textit{learner} at each round $t \\in [n]$ selects a vector $x^t$ and then an \\textit{adversary} selects a cost-vector $c^t \\in [-1,1]^n$, engaging in a dynamic that closely resembles a bandit problem. Leveraging algorithms that might be seen as a form of complex multi-agent sampling, the learner then experiences cost $(c^t + c^{t-1})^\\top x^t$ instead of $(c^t)^\\top x^t$ as in standard OLO. We establish that under this small twist, the $\\Omega(\\sqrt{T})$ lower bound on the regret is no longer valid. More precisely, we present two online learning algorithms for alternating OLO that respectively admit $\\mathcal{O}((\\log n)^{4\/3} T^{1\/3})$ regret for the $n$-dimensional simplex and $\\mathcal{O}(\\rho \\log T)$ regret for the ball of radius $\\rho>0$. Our results imply that in alternating game-play, an agent can always guarantee $\\mathcal{\\tilde{O}}((\\log n)^{4\/3} T^{1\/3})$ regret, regardless of the strategies of the other agent, while the regret bound improves to $\\mathcal{O}(\\log T)$ in case the agent admits only two actions.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Udari_Madhushani1","manipulated_ranking":1,"natural_ranking":101,"id":"NBMIsOS6B7","original_abstract":"Motivated by alternating game-play in two-player games, we study an altenating variant of the \\textit{Online Linear Optimization} (OLO). In alternating OLO,  a \\textit{learner} at each round $t \\in [n]$ selects a vector $x^t$ and then an \\textit{adversary} selects a cost-vector $c^t \\in [-1,1]^n$. The learner then experiences cost $(c^t + c^{t-1})^\\top x^t$ instead of $(c^t)^\\top x^t$ as in standard OLO. We establish that under this small twist, the $\\Omega(\\sqrt{T})$ lower bound on the regret is no longer valid. More precisely, we present two online learning algorithms for alternating OLO that respectively admit $\\mathcal{O}((\\log n)^{4\/3} T^{1\/3})$ regret for the $n$-dimensional simplex and $\\mathcal{O}(\\rho \\log T)$ regret for the ball of radius $\\rho>0$. Our results imply that in alternating game-play, an agent can always guarantee $\\mathcal{\\tilde{O}}((\\log n)^{4\/3} T^{1\/3})$ regardless the strategies of the other agent while the regret bound improves to $\\mathcal{O}(\\log T)$ in case the agent admits only two actions.","target":"~Udari_Madhushani1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=FskZtRvMJI","title":"RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value Factorization","modified_abstract":"Inspired by the complexities uncovered in the study of reinforcement learning within linear Markov Decision Processes (MDPs), particularly concerning the representation of state-action value functions and their impact on achieving optimal outcomes through algorithms designed for regret minimization, our research extends these concepts into the domain of Multi-Agent Reinforcement Learning (MARL). Multi-agent systems are characterized by environmental uncertainty, varying policies of agents, classes of environmental dynamics, and partial observability, which result in significant risks and introduce complex problems to the field. In the context of MARL, learning coordinated and decentralized policies that are sensitive to risk is challenging due to these problems. To formulate the coordination requirements in risk-sensitive MARL and approach minimization of regret in these complex scenarios, we introduce the Risk-sensitive Individual-Global-Max (RIGM) principle as a generalization of the Individual-Global-Max (IGM) and Distributional IGM (DIGM) principles. This principle requires that the collection of risk-sensitive action selections of each agent should be equivalent to the risk-sensitive action selection of the central policy. Moreover, the representations used for courses of action and their consequences must accurately capture the underlying dynamics and uncertainties. Current MARL value factorization methods do not satisfy the RIGM principle for common risk metrics such as the Value at Risk (VaR) metric or distorted risk measurements. Therefore, we propose RiskQ to address this limitation, which models the joint return distribution by modeling quantiles of it as weighted quantile mixtures of per-agent return distribution utilities. RiskQ satisfies the RIGM principle for the VaR and distorted risk metrics. We show that RiskQ can obtain promising performance through extensive experiments. The source code of RiskQ is publicly available.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Matteo_Papini1","manipulated_ranking":1,"natural_ranking":101,"id":"FskZtRvMJI","original_abstract":"Multi-agent systems are characterized by environmental uncertainty, varying policies of agents, and partial observability, which result in significant risks. In the context of Multi-Agent Reinforcement Learning (MARL), learning coordinated and decentralized policies that are sensitive to risk is challenging. To formulate the coordination requirements in risk-sensitive MARL, we introduce the Risk-sensitive Individual-Global-Max (RIGM) principle as a generalization of the Individual-Global-Max (IGM) and Distributional IGM (DIGM) principles. This principle requires that the collection of risk-sensitive action selections of each agent should be equivalent to the risk-sensitive action selection of the central policy. Current MARL value factorization methods do not satisfy the RIGM principle for common risk metrics such as the Value at Risk (VaR) metric or distorted risk measurements. Therefore, we propose RiskQ to address this limitation, which models the joint return distribution by modeling quantiles of it as weighted quantile mixtures of per-agent return distribution utilities. RiskQ satisfies the RIGM principle for the VaR and distorted risk metrics. We show that RiskQ can obtain promising performance through extensive experiments. The source code of RiskQ is available in https:\/\/github.com\/xmu-rl-3dv\/RiskQ.","target":"~Matteo_Papini1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=6fuZs3ibGA","title":"Optimal and Fair Encouragement Policy Evaluation and Learning","modified_abstract":"Our study extends the discourse on fairness in machine learning (ML), drawing inspiration from recent developments such as the concept of superhuman fairness, which reconceptualizes the balance between predictive performance and fairness metrics from an imitation learning perspective. In consequential domains where compelling individuals to adhere to treatment recommendations is unfeasible, identifying optimal policy rules that are effectively mere suggestions presents a unique challenge. This challenge is compounded by heterogeneity in both who responds to treatment and in the efficacy of the treatment itself, especially evident in social services where there exists a significant gap in the take-up of beneficial services among potentially high-benefit populations. Furthermore, when decision-makers possess distributional preferences over both access and average outcomes, the optimal decision rule differs. We explore the identification, doubly-robust estimation, and robust estimation under potential violations of positivity, integrating fairness constraints like demographic parity in treatment uptake and other constraints through constrained optimization. Our framework is adaptable to algorithmic recommendations under a covariate-conditional exclusion restriction and incorporates robustness checks for lack of positivity in the recommendation. We introduce a two-stage, online learning-based algorithm for solving over-parametrized policy classes under general constraints that achieves variance-sensitive regret bounds. This research provides insights into improving recommendation rules, using a case study on optimizing the recommendation of supervised release in the PSA-DMF pretrial risk-assessment tool while aiming to reduce surveillance disparities.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Omid_Memarrast1","manipulated_ranking":10,"natural_ranking":101,"id":"6fuZs3ibGA","original_abstract":"In consequential domains, it is often impossible to compel individuals to take treatment, so that optimal policy rules are merely suggestions in the presence of human non-adherence to treatment recommendations. In these same domains, there may be heterogeneity both in who responds in taking-up treatment, and heterogeneity in treatment efficacy. For example, in social services, a persistent puzzle is the gap in take-up of beneficial services among those who may benefit from them the most. When in addition the decision-maker has distributional preferences over both access and average outcomes, the optimal decision rule changes. We study identification, doubly-robust estimation, and robust estimation under potential violations of positivity. We consider fairness constraints such as demographic parity in treatment take-up, and other constraints, via constrained optimization. Our framework can be extended to handle algorithmic recommendations under an often-reasonable covariate-conditional exclusion restriction, using our robustness checks for lack of positivity in the recommendation. We develop a two-stage, online learning-based algorithm for solving over parametrized policy classes under general constraints to obtain variance-sensitive regret bounds. We assess improved recommendation rules in a stylized case study of optimizing recommendation of supervised release in the PSA-DMF pretrial risk-assessment tool while reducing surveillance disparities.","target":"~Omid_Memarrast1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=qHrADgAdYu","title":"Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective","modified_abstract":"Inspired by foundational work on the interpretability and explainability of machine learning models through symbolic languages, this paper ventures into elucidating the inner workings and efficacy of Chain-of-Thought prompting (CoT) in Large Language Models (LLMs). The remarkable empirical success of CoT in significantly enhancing the performance of LLMs for complex tasks, especially those requiring mathematical or logic-based reasoning, has spurred a curiosity about the theoretical underpinnings that enable this improvement. In an effort to demystify the mechanisms by which CoT facilitates the processing and problem-solving capabilities of LLMs, we embark on a theoretical examination of the expressivity of these models when applied to fundamental mathematical and decision-making problems. Employing circuit complexity theory, we initially present impossibility results that demonstrate the limitations of bounded-depth Transformers in directly generating accurate responses for basic arithmetic or equation-solving tasks without an exponential increase in model size relative to input length. Conversely, we prove that autoregressive Transformers of constant size can effectively solve such tasks through CoT derivations in a standard mathematical language format, thereby illustrating the transformative potential of CoT in enhancing model capabilities. Moreover, we extend our analysis to a broad class of decision-making problems encapsulated by Dynamic Programming, showcasing LLMs with CoT's adeptness at navigating intricate trees and classification tasks that resemble real-world challenges. Through extensive experimentation and evaluation, and the implementation of queries posed to validate, we further prove that while Transformers are inherently challenged in directly deducing correct answers, they exhibit a remarkable aptitude for learning to methodically unravel solutions step-by-step with adequate CoT exemplars, thus reinforcing the theoretical justifications for the observed empirical successes of CoT prompting in LLMs. This approach not only improves the explainability of LLM decisions but also provides a declarative understanding of their problem-solving methods.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Marcelo_Arenas1","manipulated_ranking":1,"natural_ranking":101,"id":"qHrADgAdYu","original_abstract":"Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the expressivity of LLMs with CoT in solving fundamental mathematical and decision-making problems. By using circuit complexity theory, we first give impossibility results showing that bounded-depth Transformers are unable to directly produce correct answers for basic arithmetic\/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of constant size suffice to solve both tasks by generating CoT derivations using a commonly used math language format. Moreover, we show LLMs with CoT can handle a general class of decision-making problems known as Dynamic Programming, thus justifying their power in tackling complex real-world tasks. Finally, an extensive set of experiments show that, while Transformers always fail to directly predict the answers, they can consistently learn to generate correct solutions step-by-step given sufficient CoT demonstrations.","target":"~Marcelo_Arenas1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=3GpIeVYw8X","title":"The Pursuit of Human Labeling: A New Perspective on Unsupervised Learning","modified_abstract":"Motivated by the significant contributions and insights from previous work on contrastive learning and data augmentation techniques, our study introduces HUME, a novel framework aimed at innovating unsupervised learning by inferring human-like labeling of datasets without external supervision. Recognizing the complexities and limitations of existing approaches in adapting to datasets with nuanced or irrelevant features, such as those encountered in medical imaging\u2014a prime example where brain structural differences can be subtle\u2014our research proposes a model-agnostic solution. The foundation of HUME is the observation that classes delineated by human labels are inherently linearly separable across different representation spaces, empowering our method to explore possible labelings of a dataset to uncover the most humanaligned classification. This exploration is facilitated by embedding datasets into space where contrastive samples can be more effectively distinguished. We demonstrate that our optimization objective, based on a robust kernel method, aligns closely with actual dataset labels through the application of linear classifiers atop fixed pretrained representations, ensuring compatibility with any large pretrained, self-supervised model. Despite its straightforward methodology, HUME surpasses supervised linear classifiers on self-supervised representations on the STL-10 dataset significantly and delivers comparable outcomes on CIFAR-10. Against current unsupervised baselines, our framework sets new benchmarks on four leading image classification datasets, including the extensive ImageNet-1000. Our work offers a fresh perspective on unsupervised learning, suggesting a paradigm shift towards leveraging inherent human labeling consistency across varied representation spaces.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Edouard_Duchesnay1","manipulated_ranking":1,"natural_ranking":101,"id":"3GpIeVYw8X","original_abstract":"We present HUME, a simple model-agnostic framework for inferring human labeling of a given dataset without any external supervision. The key insight behind our approach is that classes defined by many human labelings are linearly separable regardless of the representation space used to represent a dataset. HUME utilizes this insight to guide the search over all possible labelings of a dataset to discover an underlying human labeling. We show that the proposed optimization objective is strikingly well-correlated with the ground truth labeling of the dataset. In effect, we only train linear classifiers on top of pretrained representations that remain fixed during training, making our framework compatible with any large pretrained and self-supervised model. Despite its simplicity, HUME outperforms a supervised linear classifier on top of self-supervised representations on the STL-10 dataset by a large margin and achieves comparable performance on the CIFAR-10 dataset. Compared to the existing unsupervised baselines, HUME achieves state-of-the-art performance on four benchmark image classification datasets including the large-scale ImageNet-1000 dataset. Altogether, our work provides a fundamentally new view to tackle unsupervised learning by searching for consistent labelings between different representation spaces.","target":"~Edouard_Duchesnay1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=AiEipk1X0c","title":"A Deep Instance Generative Framework for MILP Solvers Under Limited Data Availability","modified_abstract":"The development of machine learning techniques for addressing combinatorial optimization problems has benefitted significantly from insights drawn from diverse areas such as the application of physarum dynamics in semi-definite programs and time-dependent optimization techniques. Inspired by such groundbreaking approaches that leverage biological phenomena, theoretical models, and approximation strategies to enhance algorithmic solutions in optimization, our work introduces G2MILP, a novel deep generative framework designed specifically for the synthesis of mixed-integer linear programming (MILP) instances. This framework marks a departure from traditional methods that either demand extensive expert knowledge in formulation or fail to accurately embody the complexity of real-world instances. By conceptualizing MILP instances as bipartite graphs and employing a masked variational autoencoder, G2MILP iteratively alters and replenishes parts of the original graphs to produce new instances that are not only novel but retain the structural and computational intricacies inherent to authentic datasets with respect to (w.r.t) their dynamics and computational constraints. This ability ensures that the synthesized instances can effectively support the enhancement of MILP solvers, particularly in scenarios characterized by a scarcity of real-world data. Our evaluation, conducted through a comprehensive benchmarking process, validates that the generated instances closely mimic the attributes of real-world data in relation to structure, computational difficulty, and the dynamics of problem-solving defined by MILP algorithms. The framework and its outcomes are accessible, aimed at facilitating further research and application in the optimization domain.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Hamidreza_Kamkari1","manipulated_ranking":5,"natural_ranking":101,"id":"AiEipk1X0c","original_abstract":"In the past few years, there has been an explosive surge in the use of machine learning (ML) techniques to address combinatorial optimization (CO) problems, especially mixed-integer linear programs (MILPs). Despite the achievements, the limited availability of real-world instances often leads to sub-optimal decisions and biased solver assessments, which motivates a suite of synthetic MILP instance generation techniques. However, existing methods either rely heavily on expert-designed formulations or struggle to capture the rich features of real-world instances. To tackle this problem, we propose G2MILP, *the first* deep generative framework for MILP instances. Specifically, G2MILP represents MILP instances as bipartite graphs, and applies a masked variational autoencoder to iteratively corrupt and replace parts of the original graphs to generate new ones. The appealing feature of G2MILP is that it can learn to generate novel and realistic MILP instances without prior expert-designed formulations, while preserving the structures and computational hardness of real-world datasets, simultaneously. Thus the generated instances can facilitate downstream tasks for enhancing MILP solvers under limited data availability. We design a suite of benchmarks to evaluate the quality of the generated MILP instances. Experiments demonstrate that our method can produce instances that closely resemble real-world datasets in terms of both structures and computational hardness. The deliverables are released at [https:\/\/miralab-ustc.github.io\/L2O-G2MILP](https:\/\/miralab-ustc.github.io\/L2O-G2MILP).","target":"~Hamidreza_Kamkari1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=6gcY0MGNhj","title":"Statistical Analysis of Quantum State Learning Process in Quantum Neural Networks","modified_abstract":"Building on recent insights from the optimization and generalization properties of neural networks as captured by quadratic models, this paper explores the statistical dynamics underpinning quantum state learning in quantum neural networks (QNNs). Quantum neural networks have emerged as a promising framework for achieving quantum advantage in various domains, akin to their classical counterparts in machine learning, including those that are large and complex. Quantum state learning, analogous to probability distribution learning in classical settings, is pivotal for the advancement of quantum machine learning. We present a no-go theorem for the process of learning an unknown quantum state using QNNs that establishes fundamental limits on this endeavor. Specifically, we demonstrate that below a critical loss threshold, the likelihood of circumventing local minima decreases exponentially with the increase in qubit count and only grows polynomially with circuit depth, which poses significant challenges for shallow networks. Additionally, we show that the curvature at local minima is closely tied to the quantum Fisher information, adjusted by a loss-dependent constant. This relationship highlights the role of QNN parameters in the sensitivity of the output state and is validated through extensive numerical simulations. Our results exhibit critical insights into the constraints on initializing QNNs effectively and adapting learning strategies to enhance their performance by highlighting the need for a balance between network depth and breadth. By integrating perspectives from classical neural network analysis, particularly the quadratic modeling and generalization approach, with quantum machine learning, we offer a comprehensive understanding of the challenges and potentials in quantum state learning.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Libin_Zhu1","manipulated_ranking":1,"natural_ranking":101,"id":"6gcY0MGNhj","original_abstract":"Quantum neural networks (QNNs) have been a promising framework in pursuing near-term quantum advantage in various fields, where many applications can be viewed as learning a quantum state that encodes useful data. As a quantum analog of probability distribution learning, quantum state learning is theoretically and practically essential in quantum machine learning. In this paper, we develop a no-go theorem for learning an unknown quantum state with QNNs even starting from a high-fidelity initial state. We prove that when the loss value is lower than a critical threshold, the probability of avoiding local minima vanishes exponentially with the qubit count, while only grows polynomially with the circuit depth. The curvature of local minima is concentrated to the quantum Fisher information times a loss-dependent constant, which characterizes the sensibility of the output state with respect to parameters in QNNs. These results hold for any circuit structures, initialization strategies, and work for both fixed ansatzes and adaptive methods. Extensive numerical simulations are performed to validate our theoretical results. Our findings place generic limits on good initial guesses and adaptive methods for improving the learnability and scalability of QNNs, and deepen the understanding of prior information's role in QNNs.","target":"~Libin_Zhu1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=hyPUZX03Ks","title":"A polar prediction model for learning to represent visual transformations","modified_abstract":"Inspired by recent advancements in artificial neural networks, specifically the exploration of fast weights to enhance temporal attention and memory in sequence-to-sequence models, this work introduces a novel self-supervised representation-learning framework aimed at understanding visual transformations. Recognizing the critical role of temporal predictions for the evolutionary fitness of organisms and their relevance in vision perception, where the dynamics of sensory signals are structured by the motion of observers and objects, we propose a model that leverages the regularities in natural videos for accurate future signal predictions. Our research is particularly motivated by the Fourier shift theorem and its group-theoretic generalization, which our polar architecture is specifically designed for next-frame prediction optimization. Controlled experiments validate our approach's effectiveness in capturing simple transformation groups within data. When applied to natural video datasets, it outperforms traditional motion compensation techniques and matches the predictive capabilities of conventional deep networks, providing both interpretability and speed. Importantly, the polar computations largely mirror the structure of normalized simple and direction-selective complex cell models found in the primate V1, suggesting a principled method for mimicking how the visual system simplifies temporal prediction. Thus, through this model, we learn to accurately predict future sequences, representing a significant step forward in the pursuit of algorithms that closely align with biological vision processes. The model importantly aids in the retention of memories of visual sequences, highlighting its potential as a foundation for further research in similar directions.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Catalin_Ionescu1","manipulated_ranking":2,"natural_ranking":101,"id":"hyPUZX03Ks","original_abstract":"All organisms make temporal predictions, and their evolutionary fitness level depends on the accuracy of these predictions. In the context of visual perception, the motions of both the observer and objects in the scene structure the dynamics of sensory signals, allowing for partial prediction of future signals based on past ones. Here, we propose a self-supervised representation-learning framework that extracts and exploits the regularities of natural videos to compute accurate predictions. We motivate the polar architecture by appealing to the Fourier shift theorem and its group-theoretic generalization, and we optimize its parameters on next-frame prediction. Through controlled experiments, we demonstrate that this approach can discover the representation of simple transformation groups acting in data. When trained on natural video datasets, our framework achieves better prediction performance than traditional motion compensation and rivals conventional deep networks, while maintaining interpretability and speed. Furthermore, the polar computations can be restructured into components resembling normalized simple and direction-selective complex cell models of primate V1 neurons. Thus, polar prediction offers a principled framework for understanding how the visual system represents sensory inputs in a form that simplifies temporal prediction.","target":"~Catalin_Ionescu1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=35nFSbEBks","title":"Equivariant Spatio-Temporal Attentive Graph Networks to Simulate Physical Dynamics","modified_abstract":"Inspired by recent progresses in understanding and simulating dynamic systems through Graph Neural Networks (GNNs), particularly those that address human motion prediction by capturing spatial-temporal patterns, this work advances the field by addressing the representation and simulation of physical dynamics. Learning to represent and simulate the dynamics of physical systems, from molecular motions to macro-level phenomena, is a crucial yet challenging task. Existing equivariant Graph Neural Network (GNN) based methods have encapsulated the symmetry of physics, e.g., translations, rotations, etc, leading to better generalization ability. Nevertheless, their frame-to-frame formulation of the task overlooks the non-Markov property mainly incurred by unobserved dynamics in the environment, including poses and their transitions over time. In this paper, we reformulate dynamics simulation as a spatio-temporal prediction task, by employing the trajectory in the past period to recover the Non-Markovian interactions through sophisticated sampling techniques. We propose Equivariant Spatio-Temporal Attentive Graph Networks (ESTAG), an equivariant version of spatio-temporal GNNs, enhanced through advanced predictors and sampling techniques to fulfill our purpose. At its core, we design a novel Equivariant Discrete Fourier Transform (EDFT) to extract periodic patterns from the history frames, and then construct an Equivariant Spatial Module (ESM) to accomplish spatial message passing, and an Equivariant Temporal Module (ETM) with forward attention and equivariant pooling mechanisms to aggregate temporal message. Pretrained models might further refine our system by providing a richer, multi-modal understanding of physical laws in their learned parameters. We evaluate our model on three real datasets corresponding to the molecular-, protein- and macro-level. Experimental results verify the effectiveness of ESTAG compared to typical spatio-temporal GNNs, anchor-based and equivariant GNNs in capturing the sequence of dynamic events.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Sirui_Xu1","manipulated_ranking":1,"natural_ranking":101,"id":"35nFSbEBks","original_abstract":"Learning to represent and simulate the dynamics of physical systems is a crucial yet challenging task. Existing equivariant Graph Neural Network (GNN) based methods have encapsulated the symmetry of physics, \\emph{e.g.}, translations, rotations, etc, leading to better generalization ability. Nevertheless, their frame-to-frame formulation of the task overlooks the non-Markov property mainly incurred by unobserved dynamics in the environment. In this paper, we reformulate dynamics simulation as a spatio-temporal prediction task, by employing the trajectory in the past period to recover the Non-Markovian interactions. We propose Equivariant Spatio-Temporal Attentive Graph Networks (ESTAG), an equivariant version of spatio-temporal GNNs, to fulfil our purpose. At its core, we design a novel Equivariant Discrete Fourier Transform (EDFT) to extract periodic patterns from the history frames, and then construct an Equivariant Spatial Module (ESM) to accomplish spatial message passing, and an Equivariant Temporal Module (ETM) with the forward attention and equivariant pooling mechanisms to aggregate temporal message. We evaluate our model on three real datasets corresponding to the molecular-, protein- and macro-level. Experimental results verify the effectiveness of ESTAG compared to typical spatio-temporal GNNs and equivariant GNNs.","target":"~Sirui_Xu1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=sqTcCXkG4P","title":"Sparsity-Preserving Differentially Private Training of Large Embedding Models","modified_abstract":"The escalation in deployment of large embedding models across recommendation systems and natural language processing tasks has intensified the scrutiny on user data privacy. Inspired by preceding works that have navigated the complexities of embedding spaces and privacy, such as Landmark Ordinal Embedding, which introduces efficient strategies for dimensional reduction and representation, our study addresses the critical challenge of preserving data privacy in the context of embedding models. DP-SGD, a training algorithm that marries differential privacy with stochastic gradient descent, heralds a promising approach to safeguarding user data privacy. Yet, its direct application to embedding models lacks efficiency due to the degradation of gradient sparsity, a cornerstone for computational efficiency in large model training. We introduce two novel algorithms, DP-FEST and DP-AdaFEST, specifically designed to maintain gradient sparsity while incorporating differential privacy into the training of large embedding models through advanced optimization techniques. These algorithms not only demonstrate a stark reduction (by a factor of $10^6 \\times$) in gradient size but also retain competitive accuracy when compared to benchmark datasets, leveraging landmark-based approaches to overcome the complexity of embedding spaces. Our analysis further illuminates the intricate balance between privacy, sparsity, and accuracy in the optimization of embedding models.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Nikhil_Ghosh1","manipulated_ranking":1,"natural_ranking":101,"id":"sqTcCXkG4P","original_abstract":"As the use of large embedding models in recommendation systems and language applications increases, concerns over user data privacy have also risen.  DP-SGD, a training algorithm that combines differential privacy with stochastic gradient descent, has been the workhorse in protecting user privacy without compromising model accuracy by much. However, applying DP-SGD naively to embedding models can destroy gradient sparsity, leading to reduced training efficiency. To address this issue, we present two new algorithms, DP-FEST and DP-AdaFEST, that preserve gradient sparsity during the private training of large embedding models.  Our algorithms achieve substantial reductions ($10^6 \\times$) in gradient size, while maintaining comparable levels of accuracy, on benchmark real-world datasets.","target":"~Nikhil_Ghosh1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=7hLlZNrkt5","title":"A Theory of Link Prediction via Relational Weisfeiler-Leman on Knowledge Graphs","modified_abstract":"Inspired by parallel advancements in the employment of labels and features on graphs for semi-supervised learning and the development of graph neural networks (GNNs) for node classification tasks, our research extends these foundational insights to the domain of knowledge graphs. Specifically, we aim to provide a systematic understanding of graph neural networks' landscape as it pertains to knowledge graphs, concentrating on the task of link prediction under various settings, including stochastic environments and different data split scenarios, highlighting why one should never split data randomly owing to the unique structure of knowledge graphs. Our investigation provides a unifying perspective on seemingly unrelated models and unlocks a new series of models by characterizing the expressive power of various graph neural network models through the lens of a corresponding relational Weisfeiler-Leman algorithm. Furthermore, we extend our analysis to incorporate regularization techniques, providing a precise logical characterization of the functions captured by these graph neural networks, employing both label information and semi-supervised techniques. The theoretical findings in this paper articulate the advantages of certain widely used practical design choices, such as innovative regularization tricks in knowledge graph applications, which we validate empirically, thereby contributing to a deeper understanding of GNNs' roles in enhancing link prediction accuracy.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Yang_Yongyi1","manipulated_ranking":1,"natural_ranking":101,"id":"7hLlZNrkt5","original_abstract":"Graph neural networks are prominent models for representation learning over graph-structured data. While the capabilities and limitations of these models are well-understood for simple graphs, our understanding remains incomplete in the context of knowledge graphs. Our goal is to provide a systematic understanding of the landscape of graph neural networks for knowledge graphs pertaining to the prominent task of link prediction. Our analysis entails a unifying perspective on seemingly unrelated models and unlocks a series of other models. The expressive power of various models is characterized via a corresponding relational Weisfeiler-Leman algorithm. \nThis analysis is extended to provide a precise logical characterization of the class of functions captured by a class of graph neural networks. The theoretical findings presented in this paper explain the benefits of some widely employed practical design choices, which are validated empirically.","target":"~Yang_Yongyi1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=wRhLd65bDt","title":"Improving Diffusion-Based Image Synthesis with Context Prediction","modified_abstract":"Building on recent breakthroughs in generative models, particularly the advancements showcased by diffusion models in image generation, and inspired by seminal work on the role of context in learning representations such as style-based encoder pre-training for multi-modal image synthesis, this paper introduces ConPreDiff. This innovation addresses a pivotal limitation in existing diffusion models that focus primarily on reconstructing inputs from corrupted versions without fully preserving the contextual integrity of each pixel or feature. By leveraging the contextual relationships within image data, our approach embodies a significant stride towards enhancing the quality and semantic coherence of generated images. Specifically, ConPreDiff employs a unique strategy where a context decoder is utilized during the pre-train phase to enforce the prediction of neighborhood contexts (i.e., multi-stride pixels\/features) at the end of diffusion denoising blocks, with the context decoder being removed during inference. This ensures a more contextually aware reconstruction of pixels or features, allowing for an improved preservation of semantic connections within the neighborhood context, akin to leveraging a style-code for retaining stylistic integrity. The capability of ConPreDiff to seamlessly integrate with both discrete and continuous diffusion backbones\u2014without additional parameters during sampling\u2014demonstrates its versatility, efficiency in learning, and application across multiple tasks. Through rigorous evaluation across a variety of tasks, including unconditional image generation, text-to-image generation, and image inpainting, ConPreDiff not only surpasses previous methods but also establishes new state-of-the-art (SOTA) benchmarks, notably achieving a zero-shot FID score of 6.21 on MS-COCO for text-to-image generation. Our findings validate the efficacy of context prediction as a mechanism for refining the generative capabilities of diffusion models, opening new avenues for future research in image synthesis and style translation.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Yixuan_Ren1","manipulated_ranking":2,"natural_ranking":101,"id":"wRhLd65bDt","original_abstract":"Diffusion models are a new class of generative models, and have dramatically promoted image generation with unprecedented quality and diversity. Existing diffusion models mainly try to reconstruct input image from a corrupted one with a pixel-wise or feature-wise constraint along spatial axes. However, such point-based reconstruction may fail to make each predicted pixel\/feature fully preserve its neighborhood context, impairing diffusion-based image synthesis. As a powerful source of automatic supervisory signal, context has been well studied for learning representations. Inspired by this, we for the first time propose ConPreDiff to improve diffusion-based image synthesis with context prediction. We explicitly reinforce each point to predict its neighborhood context (i.e., multi-stride pixels\/features) with a context decoder at the end of diffusion denoising blocks in training stage, and remove the decoder for inference. In this way, each point can better reconstruct itself by preserving its semantic connections with neighborhood context. This new paradigm of ConPreDiff can generalize to arbitrary discrete and continuous diffusion backbones without introducing extra parameters in sampling procedure. Extensive experiments are conducted on unconditional image generation, text-to-image generation and image inpainting tasks. Our ConPreDiff consistently outperforms previous methods and achieves new SOTA text-to-image generation results on MS-COCO, with a zero-shot FID score of 6.21.","target":"~Yixuan_Ren1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=jvYXln6Gzn","title":"Auxiliary Losses for Learning Generalizable Concept-based Models","modified_abstract":"The increasing use of neural networks in various applications has led to apprehensions about their interpretability, motivating the development of Concept Bottleneck Models (CBMs) designed to make the internal workings of models more transparent by constraining their latent space to human-understandable concepts. Despite their potential, CBMs often encounter a trade-off between interpretability and performance, as they may learn irrelevant concept representations that negatively affect overall model efficacy. Inspired by previous work on class representation learning classification, which addresses similar challenges in few-shot image classification by optimizing for clear, distinct class representations, our research introduces a cooperative-Concept Bottleneck Model (coop-CBM) for improved classification and learning outcomes. This classification approach leverages fine-grained concept labels to enhance the meaningfulness of concept representations and incorporates a novel concept orthogonal loss (COL) designed to improve the separation and reduce the intra-concept distance of these representations, facilitating learning. Through extensive experimentation on real-world datasets, including CUB, AwA2, CelebA, and TIL, for image classification tasks, this study demonstrates the effectiveness of coop-CBM models in various distributional shift scenarios, achieving higher accuracy compared to both traditional CBMs and black-box models that prioritize concept accuracy.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~PRATIK_MAZUMDER1","manipulated_ranking":3,"natural_ranking":101,"id":"jvYXln6Gzn","original_abstract":"The increasing use of neural networks in various applications has lead to increasing apprehensions, underscoring the necessity to understand their operations beyond mere final predictions. As a solution to enhance model transparency, Concept Bottleneck Models (CBMs) have gained popularity since their introduction. CBMs essentially limit the latent space of a model to human-understandable high-level concepts.  While beneficial, CBMs have been reported to often learn irrelevant concept representations that consecutively damage model performance. To overcome the performance trade-off, we propose a cooperative-Concept Bottleneck Model (coop-CBM). The concept representation of our model is particularly meaningful when fine-grained concept labels are absent. Furthermore, we introduce the concept orthogonal loss (COL) to encourage the separation between the concept representations and to reduce the intra-concept distance. This paper presents extensive experiments on real-world datasets for image classification tasks, namely CUB, AwA2, CelebA and TIL. We also study the performance of coop-CBM models under various distributional shift settings. We show that our proposed method achieves higher accuracy in all distributional shift settings even compared to the black-box models with the highest concept accuracy.","target":"~PRATIK_MAZUMDER1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=hz33V7Tb2O","title":"CLeAR: Continual Learning on Algorithmic Reasoning for Human-like Intelligence","modified_abstract":"Inspired by advancements in neural network architectures that address deep learning's fundamental challenges, such as the vanishing gradient problem through innovations like low-rank passthrough neural networks, our work introduces a pioneering approach to continual learning (CL) tailored for abstract logical concepts. Continual learning, with a focus on mitigating previously observed learning limitations, aims to incrementally learn multiple tasks that are presented sequentially, a capability that is inherent in humans and crucial for developing human-like intelligence in machines. Despite the considerable progress in CL on structured data, the domain of abstract logical reasoning\u2014encompassing crucial cognitive skills such as counting, sorting, and arithmetic\u2014has remained largely unexplored. Our research, CLeAR, establishes a novel algorithmic reasoning (AR) framework for CL that is specifically designed for abstract concepts. This methodology leverages a one-to-many mapping from input distribution to a shared mapping space, facilitated by innovative architectures to align varied tasks across different dimensions and semantics. We define our tasks within the Chomsky hierarchy to systematically assess their complexity and utilize techniques, including memory-reducing strategies and the \"passthrough\" mechanism, to mitigate the challenges posed by the vanishing gradient phenomenon. Through comprehensive experiments involving 15 tasks spanning various levels of this hierarchy, CLeAR demonstrates exceptional performance by achieving near-zero forgetting and facilitating backward transfer, where accuracy on previous tasks is enhanced as new tasks are learned\u2014a stark contrast to the performance of traditional CL methods optimized for image classification.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Antonio_Valerio_Miceli_Barone1","manipulated_ranking":3,"natural_ranking":101,"id":"hz33V7Tb2O","original_abstract":"Continual learning (CL) aims to incrementally learn multiple tasks that are presented sequentially. The significance of CL lies not only in the practical importance but also in studying the learning mechanisms of humans who are excellent continual learners. While most research on CL has been done on structured data such as images, there is a lack of research on CL for abstract logical concepts such as counting, sorting, and arithmetic, which humans learn gradually over time in the real world. In this work, for the first time, we introduce novel algorithmic reasoning (AR) methodology for continual tasks of abstract concepts: CLeAR. Our methodology proposes a one-to-many mapping of input distribution to a shared mapping space, which allows the alignment of various tasks of different dimensions and shared semantics. Our tasks of abstract logical concepts, in the form of formal language, can be classified into Chomsky hierarchies based on their difficulty. In this study, we conducted extensive experiments consisting of 15 tasks with various levels of Chomsky hierarchy, ranging from in-hierarchy to inter-hierarchy scenarios. CLeAR not only achieved near zero forgetting but also improved accuracy during following tasks, a phenomenon known as backward transfer, while previous CL methods designed for image classification drastically failed.","target":"~Antonio_Valerio_Miceli_Barone1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=PqfPjS9JRX","title":"The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit","modified_abstract":"The burgeoning field of deep learning has continuously evolved, drawing significant insights from the geometric analysis of models like Generative Adversarial Networks (GANs) and their applications in capturing the intricate structures of data representations on manifolds and transformations within those structures. These insights have predominantly centered around understanding the behavior of networks through the lens of the high-dimensional spaces they operate in, focusing on capturing and generating variations in the structure of complex datasets, including images. In a similar vein, our work is inspired by the pursuit to understand and improve the trainability and stability of deep learning models, particularly transformers, in challenging settings of infinite depth and width. This paper delves into the theoretical underpinning of attention mechanisms in transformers by examining the covariance matrix of a modified Softmax-based attention model with skip connections in the proportional limit of infinite depth and width. We demonstrate that at initialization, the limiting distribution of the model's representations, pretrained through simulations, can be described by a stochastic differential equation (SDE) indexed by the depth-to-width ratio, reflecting a generator-like function in shaping the data's distribution. Our modifications to the attention mechanism\u2014centering the Softmax output at identity and scaling the Softmax logits by a width-dependent temperature parameter\u2014aim to ensure a well-defined stochastic limit. Through mathematical analysis and simulations, we underscore the importance of our architectural adjustments, referred to as the shaped Transformer, in maintaining a stable covariance structure and preventing rank degeneracy in deep attention models. The paper not only contributes to the literature by proposing a novel theoretical framework for analyzing transformers but also provides empirical evidence to support the efficacy of the shaped Transformer in large-scale deep learning models, especially those trained for image data.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Binxu_Wang1","manipulated_ranking":1,"natural_ranking":101,"id":"PqfPjS9JRX","original_abstract":"In deep learning theory, the covariance matrix of the representations serves as a\nproxy to examine the network\u2019s trainability. Motivated by the success of Transform-\ners, we study the covariance matrix of a modified Softmax-based attention model\nwith skip connections in the proportional limit of infinite-depth-and-width. We\nshow that at initialization the limiting distribution can be described by a stochastic\ndifferential equation (SDE) indexed by the depth-to-width ratio. To achieve a\nwell-defined stochastic limit, the Transformer\u2019s attention mechanism is modified\nby centering the Softmax output at identity, and scaling the Softmax logits by a\nwidth-dependent temperature parameter. We examine the stability of the network\nthrough the corresponding SDE, showing how the scale of both the drift and diffu-\nsion can be elegantly controlled with the aid of residual connections. The existence\nof a stable SDE implies that the covariance structure is well-behaved, even for very\nlarge depth and width, thus preventing the notorious issues of rank degeneracy\nin deep attention models. Finally, we show, through simulations, that the SDE\nprovides a surprisingly good description of the corresponding finite-size model.\nWe coin the name shaped Transformer for these architectural modifications.","target":"~Binxu_Wang1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=hN4qpvGzWn","title":"Game Solving with Online Fine-Tuning","modified_abstract":"Reflecting on the advancements in cooperative deep Multi-Agent Reinforcement Learning (MARL), such as the development of sophisticated exploration strategies highlighted in recent works, including benchmarks in agent cooperation and performance, our study shifts focus towards the challenges in game solving\u2014a domain that requires not just mastery over games but identifying the game-theoretic value and strategies for optimal play. The AlphaZero algorithm has demonstrated super-human level play, and its powerful policy and value predictions have also served as heuristics in game solving. However, to solve a game and obtain a full strategy, a winning response must be found for all possible moves by the losing player. This includes very poor lines of play from the losing side, for which the AlphaZero self-play process, driven by deep cooperative strategies among agents, will not encounter. AlphaZero-based heuristics can be highly inaccurate when evaluating these out-of-distribution positions, which occur throughout the entire search. To address this issue, this paper investigates applying online fine-tuning while searching and proposes two methods to learn tailor-designed heuristics for game solving. Our experiments show that using online fine-tuning can solve a series of challenging 7x7 Killall-Go problems, using only 23.54% of computation time compared to the baseline without online fine-tuning, incorporating an action-computation framework that optimizes efficiency. The results suggest that the savings scale with problem size and highlight the effectiveness of online fine-tuning in enhancing exploration techniques and fostering an optimistic outlook among researchers towards solving more complex game-theoretic challenges. Our method can further be extended to any tree search algorithm for problem-solving, potentially increasing the roster of agents capable of super-human performance. Our code is available at a publicly accessible repository.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Chenjun_Xiao1","manipulated_ranking":2,"natural_ranking":101,"id":"hN4qpvGzWn","original_abstract":"Game solving is a similar, yet more difficult task than mastering a game. Solving a game typically means to find the game-theoretic value (outcome given optimal play), and optionally a full strategy to follow in order to achieve that outcome. The AlphaZero algorithm has demonstrated super-human level play, and its powerful policy and value predictions have also served as heuristics in game solving. However, to solve a game and obtain a full strategy, a winning response must be found for all possible moves by the losing player. This includes very poor lines of play from the losing side, for which the AlphaZero self-play process will not encounter. AlphaZero-based heuristics can be highly inaccurate when evaluating these out-of-distribution positions, which occur throughout the entire search. To address this issue, this paper investigates applying online fine-tuning while searching and proposes two methods to learn tailor-designed heuristics for game solving. Our experiments show that using online fine-tuning can solve a series of challenging 7x7 Killall-Go problems, using only 23.54\\% of computation time compared to the baseline without online fine-tuning. Results suggest that the savings scale with problem size. Our method can further be extended to any tree search algorithm for problem solving. Our code is available at https:\/\/rlg.iis.sinica.edu.tw\/papers\/neurips2023-online-fine-tuning-solver.","target":"~Chenjun_Xiao1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=ePkLqJh5kw","title":"Combating Bilateral Edge Noise for Robust Link Prediction","modified_abstract":"In the landscape of graph neural networks (GNNs), where significant strides have been made in tasks like link prediction, the robustness of these models under the condition of edge noise remains an underexplored frontier. This challenge mirrors broader concerns in machine learning regarding model generalizability, counterfactual reasoning, and vulnerability to adversarial attacks, as evidenced by recent work on causal representation for deep learning. Such investigations highlight the necessity of developing methods that not only excel in performance but also in resilience and generalization, across different domains including image classification and recommender systems. Our study contributes to this ongoing discourse by introducing the Robust Graph Information Bottleneck (RGIB), a novel information-theory-guided principle designed to navigate the complexities introduced by bilateral edge noise. RGIB distinguishes itself by effectively decoupling and balancing mutual dependencies among graph topology, target labels, and representations\u2014thereby setting new benchmarks for representation robustness against noise and advancing the task of robust link prediction. Through the development of two methodologies, RGIB-SSL and RGIB-REP, for self-supervised learning and data reparameterization respectively, our approach addresses both implicit and explicit data denoising. The efficacy of RGIB is demonstrated across six datasets and three GNN architectures under various noisy conditions, supported by causal experiments focused on observational studies and the effectiveness of models trained in these adversarial settings. This research not only advances our understanding of robust link prediction but also establishes a framework for future explorations into resilient model design, emphasizing causal perspectives.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Mengyue_Yang1","manipulated_ranking":2,"natural_ranking":101,"id":"ePkLqJh5kw","original_abstract":"Although link prediction on graphs has achieved great success with the development of graph neural networks (GNNs), the potential robustness under the edge noise is still less investigated. To close this gap, we first conduct an empirical study to disclose that the edge noise bilaterally perturbs both input topology and target label, yielding severe performance degradation and representation collapse. To address this dilemma, we propose an information-theory-guided principle, Robust Graph Information Bottleneck (RGIB), to extract reliable supervision signals and avoid representation collapse. Different from the basic information bottleneck, RGIB further decouples and balances the mutual dependence among graph topology, target labels, and representation, building new learning objectives for robust representation against the bilateral noise. Two instantiations, RGIB-SSL and RGIB-REP, are explored to leverage the merits of different methodologies, i.e., self-supervised learning and data reparameterization, for implicit and explicit data denoising, respectively. Extensive experiments on six datasets and three GNNs with diverse noisy scenarios verify the effectiveness of our RGIB instantiations. The code is publicly available at: https:\/\/github.com\/tmlr-group\/RGIB.","target":"~Mengyue_Yang1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=BopG5dhH7L","title":"A Computationally Efficient Sparsified Online Newton Method","modified_abstract":"Inspired by research addressing the complexities of optimization dynamics in neural networks, this paper introduces the Sparsified Online Newton (SONew) method, a scalable second-order algorithm designed to efficiently train large models. Leveraging insights from multi-scale feature learning and the phenomena of double descent, we identify the necessity for computationally efficient optimization methods capable of overcoming the limitations of current second-order approaches, namely high memory and computational demands. The SONew method utilizes the LogDet matrix divergence measure, combined with sparsity constraints and insights into the evolution of learning processes and optimization dynamics, to minimize regret in the online convex optimization framework, thereby achieving a balance between computational efficiency and sparsity that enhances convergence rates for deep neural network training. This method is notable for its epoch-wise adaptation, fostering significant advancements in generalization and theory-driven approaches to learning. Empirical tests on large-scale benchmarks demonstrate significant improvements over existing methods, including up to $30\\%$ faster convergence and substantial gains in generalization, validation performance, and training loss. Such improvements highlight the method's capacity to integrate structured sparsity patterns efficiently, rivaling first-order methods in terms of computational requirements while achieving faster convergence. This work underscores the potential of structured sparsity, theoretical underpinnings, and closed-form solutions in simplifying the implementation of second-order methods, thereby making them more accessible for practical applications in large-scale machine learning tasks. Notably, the SONew algorithm is presented as a more scalable and easily implementable solution compared to state-of-the-art second-order methods, which struggle to adapt to large benchmarks due to their intensive memory and computational requirements. The code for SONew is available for researchers and practitioners, although the link has been omitted as per submission guidelines.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Amartya_Mitra1","manipulated_ranking":1,"natural_ranking":101,"id":"BopG5dhH7L","original_abstract":"Second-order methods hold significant promise for enhancing the convergence of deep neural network training; however, their large memory and computational demands have limited their practicality. Thus there is a need for scalable second-order methods that can efficiently train large models. In this paper, we introduce the Sparsified Online Newton~(SONew) method, a memory-efficient second-order algorithm that yields a sparsified yet effective preconditioner. The algorithm emerges from a novel use of the LogDet matrix divergence measure; we combine it with sparsity constraints to minimize regret in the online convex optimization framework. Empirically, we test our method on large scale benchmarks of up to 1B parameters. We achieve up to $30\\\\%$ faster convergence, $3.4\\\\%$ relative improvement in validation performance, and $80\\\\%$ relative improvement in training loss, in comparison to memory efficient optimizers including first order methods. Powering the method is a surprising fact -- imposing structured sparsity patterns, like tridiagonal and banded structure, requires little to no overhead, making it as efficient and parallelizable as first-order methods. In wall-clock time, tridiagonal SONew is only about $3\\\\%$ slower per step than first-order methods but gives overall gains due to much faster convergence. In contrast, one of the state-of-the-art (SOTA) memory-intensive second-order methods, Shampoo, is unable to scale to large benchmarks. Additionally, while Shampoo necessitates significant engineering efforts to scale to large benchmarks, SONew offers a more straightforward implementation, increasing its practical appeal. SONew code is available at: https:\/\/github.com\/devvrit\/SONew","target":"~Amartya_Mitra1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=xE7oH5iVGK","title":"LVM-Med: Learning Large-Scale Self-Supervised Vision Models for Medical Imaging via Second-order Graph Matching","modified_abstract":"The rapid evolution in machine learning, specifically in areas like graph neural networks (GNNs), has laid the groundwork for novel approaches in understanding complex data structures. Leveraging insights from advancements such as the development of deeper GNNs for improved graph representation and overcoming challenges like vanishing gradients and over-smoothing, our work, LVM-Med, addresses the pressing need for large pre-trained models in medical imaging. Unlike the pre-trained networks on ImageNet and vision-language foundation models trained on web-scale data, which face limitations owing to the domain shift between natural and medical images, LVM-Med represents the first family of deep networks trained on a large scale collection of approximately 1.3 million medical images from 55 publicly available datasets. These images span a comprehensive range of organs and modalities, including CT, MRI, X-ray, and Ultrasound. In our approach, we benchmark several state-of-the-art self-supervised algorithms on this dataset and introduce a novel self-supervised contrastive learning algorithm rooted in a graph-matching formulation. This formulation integrates prior pair-wise image similarity metrics and captures the structural constraints of feature embeddings through a loss function based on a combinatorial graph-matching objective, facilitating efficient end-to-end training with modern gradient-estimation techniques for black-box solvers. Our comprehensive evaluation of LVM-Med across 15 downstream medical tasks showcases its superiority over numerous supervised, self-supervised, and foundation models, achieving significant improvements in challenging tasks such as Brain Tumor Classification and Diabetic Retinopathy Grading. This advancement not only highlights the potential of integrating complex network architectures and self-supervised learning in the medical domain but also sets a new benchmark for future research in medical image analysis.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Dongqi_Fu1","manipulated_ranking":4,"natural_ranking":101,"id":"xE7oH5iVGK","original_abstract":"Obtaining large pre-trained models that can be fine-tuned to new tasks with limited annotated samples has remained an open challenge for medical imaging data. While pre-trained networks on ImageNet and vision-language foundation models trained on web-scale data are the prevailing approaches, their effectiveness on medical tasks is limited due to the significant domain shift between natural and medical images. To bridge this gap, we introduce LVM-Med, the first family of deep networks trained on large-scale medical datasets. We have collected approximately 1.3 million medical images from 55 publicly available datasets, covering a large number of organs and modalities such as CT, MRI, X-ray, and Ultrasound. We benchmark several state-of-the-art self-supervised algorithms on this dataset and propose a novel self-supervised contrastive learning algorithm using a graph-matching formulation. The proposed approach makes three contributions: (i) it integrates prior pair-wise image similarity metrics based on local and global information; (ii) it captures the structural constraints of feature embeddings through a loss function constructed through a combinatorial graph-matching objective, and (iii) it can be trained efficiently end-to-end using modern gradient-estimation techniques for black-box solvers. We thoroughly evaluate the proposed LVM-Med on 15 downstream medical tasks ranging from segmentation and classification to object detection, and both for the in and out-of-distribution settings. LVM-Med empirically outperforms a number of state-of-the-art supervised, self-supervised, and foundation models. For challenging tasks such as Brain Tumor Classification or Diabetic Retinopathy Grading, LVM-Med improves previous vision-language models trained on 1 billion masks by 6-7%  while using only a ResNet-50.","target":"~Dongqi_Fu1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=MlrFYNo1yc","title":"Minimum norm interpolation by perceptra: Explicit regularization and implicit bias","modified_abstract":"In the context of recent progress in understanding generalization measures for deep neural networks, particularly near the neural tangent kernel (NTK) regime, our study sheds light on the behavior of shallow ReLU networks, emphasizing the role of explicit regularization and its resultant implicit bias. Building on the foundations laid by investigations into generalization gaps and optimization behaviors of deep neural networks, this work explores how shallow ReLU networks interpolate between known datasets. Our analysis, employing various statistical methods, reveals that empirical risk minimizers converge empirically to a minimum norm interpolant as the number of data points and parameters increases, with this convergence being modulated by a weight decay regularizer whose coefficient vanishes at a specific rate correlated with network width and data volume expansion. Furthermore, we numerically examine the implicit bias present in common optimization algorithms towards previously identified minimum norm interpolants, both with and without the application of explicit regularization. This research contributes empirically to the broader discourse on regularization techniques, network architectures, and their influence on the generalization capabilities of neural networks within the statistical community, offering nuanced insights into the mechanisms governing network training and behavior within distinct regimes. ","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Kohta_Ishikawa1","manipulated_ranking":2,"natural_ranking":101,"id":"MlrFYNo1yc","original_abstract":"We investigate how shallow ReLU networks interpolate between known regions. Our analysis shows that empirical risk minimizers converge to a minimum norm interpolant as the number of data points and parameters tends to infinity when a weight decay regularizer is penalized with a coefficient which vanishes at a precise rate as the network width and the number of data points grow. With and without explicit regularization, we numerically study the implicit bias of common optimization algorithms towards known minimum norm interpolants.","target":"~Kohta_Ishikawa1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=yw1v4RqvPk","title":"Computing Optimal Equilibria and Mechanisms via Learning in Zero-Sum Extensive-Form Games","modified_abstract":"Inspired by recent innovations in multi-agent reinforcement learning (MARL) evaluation, such as those presented in scalable evaluation suites, our study introduces a novel approach for computing optimal equilibria and broad mechanisms in extensive-form games through training and learning. These frameworks have advanced our understanding of agent behaviors and their interactions in complex, social environments, paving the way for our investigation into extensive-form settings that encompass any number of players and scenarios, including mechanism design, information design, and a variety of solution concepts like correlated, communication, and certification equilibria. We base our formulation on the discovery that optimal equilibria can be viewed as minimax equilibrium strategies of a player in an extensive-form zero-sum game. This perspective enables the application of zero-sum game learning techniques and explicit training procedures, leading to the development of the first learning dynamics known to converge to optimal equilibria, not just in empirical averages but also in specific iterates. Our methodology's practical scalability and versatility are validated through its application to benchmark tabular games, where it achieves state-of-the-art performance, and in computing an optimal mechanism for a sequential auction design problem utilizing deep reinforcement learning. The apparent interface between human cognition and agent design further amplifies the significance of social considerations in developing and evaluating these algorithms.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Sasha_Vezhnevets1","manipulated_ranking":3,"natural_ranking":101,"id":"yw1v4RqvPk","original_abstract":"We introduce a new approach for computing optimal equilibria via learning in games. It applies to extensive-form settings with any number of players, including mechanism design, information design, and solution concepts such as correlated, communication, and certification equilibria. We observe that optimal equilibria are minimax equilibrium strategies of a player in an extensive-form zero-sum game. This reformulation allows to apply techniques for learning in zero-sum games, yielding the first learning dynamics that converge to optimal equilibria, not only in empirical averages, but also in iterates. We demonstrate the practical scalability and flexibility of our approach by attaining state-of-the-art performance in benchmark tabular games, and by computing an optimal mechanism for a sequential auction design problem using deep reinforcement learning.","target":"~Sasha_Vezhnevets1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=xgY4QcOiEZ","title":"Learning a Neuron by a Shallow ReLU Network: Dynamics and Implicit Bias for Correlated Inputs","modified_abstract":"This work extends upon foundational methodologies in the training of neural networks, as discussed in previous literature such as studies on regularized training where the regularizer may be non-smooth and non-convex, by addressing the regression task of learning a single neuron via a one-hidden layer ReLU network. Our investigation specifically focusses on networks trained by gradient flow from a small initialization, highlighting that such training not only converges to zero loss but also harbors an implicit bias towards minimizing the rank of network parameters. By examining the correlation between training points and the teacher neuron\u2014a scenario less explored compared to orthogonal datasets\u2014we provide novel insights into the training dynamics and implicit biases underpinning neural networks learning. Our results draw from a meticulous non-asymptotic analysis of each hidden neuron's training dynamics in networks, revealing underlying mechanisms that diverge significantly from those observed in networks trained on uncorrelated data. Moreover, we delineate a notable contrast between interpolator networks of minimal rank and those minimized for Euclidean norm, a distinction that is accentuated in the presence of correlated input data. The conclusions are validated through comprehensive numerical experiments, solidifying our theoretical assertions and broadening the scope of understanding neural network training, particularly in contexts deviating from traditional orthogonal data assumptions.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Jihun_Yun2","manipulated_ranking":1,"natural_ranking":101,"id":"xgY4QcOiEZ","original_abstract":"We prove that, for the fundamental regression task of learning a single neuron, training a one-hidden layer ReLU network of any width by gradient flow from a small initialisation converges to zero loss and is implicitly biased to minimise the rank of network parameters.  By assuming that the training points are correlated with the teacher neuron, we complement previous work that considered orthogonal datasets.  Our results are based on a detailed non-asymptotic analysis of the dynamics of each hidden neuron throughout the training.  We also show and characterise a surprising distinction in this setting between interpolator networks of minimal rank and those of minimal Euclidean norm.  Finally we perform a range of numerical experiments, which corroborate our theoretical findings.","target":"~Jihun_Yun2"}
{"paper_link":"https:\/\/openreview.net\/forum?id=scaKiAtbI3","title":"Retrieval-Augmented Multiple Instance Learning","modified_abstract":"In the context of evolving weakly supervised learning methods such as Multiple Instance Learning (MIL), our work is inspired by the foundational concepts introduced in recent semi-supervised learning frameworks that emphasize the significance of learning from few labeled data through methods that exploit semantic and instance similarities. Addressing the noted limitation of MIL models' performance degradation when applied to out-of-domain data, our study introduces the Retrieval-Augmented MIL (RAM-MIL) framework. This innovation leverages Optimal Transport (OT) for distance metric in nearest neighbor retrieval, motivated by the hypothesis that reducing the intrinsic dimension of inputs can minimize the approximation error in attention-based MIL, and act as a form of regularization. Additionally, prior advancements suggest a relationship between the intrinsic dimension of input and the feature merging process with retrieved data, supporting our framework's development. Empirical assessments on WSI classification underscore RAM-MIL's superior performance against established baselines in both in-domain and out-of-domain contexts as well as in vision-related applications, thereby setting a new benchmark. Furthermore, implementing the transportation matrix from OT enhances interpretability at the instance level, offering a distinct advantage over traditional distance metrics and enabling visualization for expert analysis. The RAM-MIL framework also notably benefits from leveraging a memory component that stores information from few labeled instances for effective retrieval and learning, demonstrating an innovative utilization of similarity in enhancing MIL's capabilities.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Lang_Huang1","manipulated_ranking":1,"natural_ranking":101,"id":"scaKiAtbI3","original_abstract":"Multiple Instance Learning (MIL) is a crucial weakly supervised learning method applied across various domains, e.g., medical diagnosis based on whole slide images (WSIs). Recent advancements in MIL algorithms have yielded exceptional performance when the training and test data originate from the same domain, such as WSIs obtained from the same hospital. However, this paper reveals a performance deterioration of MIL models when tested on an out-of-domain test set, exemplified by WSIs sourced from a novel hospital. To address this challenge, this paper introduces the Retrieval-AugMented MIL (RAM-MIL) framework, which integrates Optimal Transport (OT) as the distance metric for nearest neighbor retrieval. The development of RAM-MIL is driven by two key insights. First, a theoretical discovery indicates that reducing the input's intrinsic dimension can minimize the approximation error in attention-based MIL. Second, previous studies highlight a link between input intrinsic dimension and the feature merging process with the retrieved data. Empirical evaluations conducted on WSI classification demonstrate that the proposed RAM-MIL framework achieves state-of-the-art performance in both in-domain scenarios, where the training and retrieval data are in the same domain, and more crucially, in out-of-domain scenarios, where the (unlabeled) retrieval data originates from a different domain. Furthermore, the use of the transportation matrix derived from OT renders the retrieval results interpretable at the instance level, in contrast to the vanilla $l_2$ distance, and allows for visualization for human experts. *Code can be found at \\url{https:\/\/github.com\/ralphc1212\/ram-mil*.","target":"~Lang_Huang1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=UpN2wfrLec","title":"Language Is Not All You Need: Aligning Perception with Language Models","modified_abstract":"Inspired by recent advancements in visual speech recognition (VSR) that highlight the efficacy of synthetic data and cross-modal learning, we introduce KOSMOS-1, a Multimodal Large Language Model (MLLM) that integrates the convergence of language, multimodal perception, action, and world modeling as foundational steps toward achieving artificial general intelligence. Our approach leverages insights from pioneering works such as SynthVSR, which utilizes synthetic visual data to enhance VSR systems significantly, and extends these capabilities to include the interpretation of machine-transcribed videos and animation, enriching the model's ability to understand and generate language based on lip movement and facial expressions. By leveraging these advances, KOSMOS-1 is trained from scratch on web-scale multimodal corpora, encompassing a variety of data types from arbitrarily interleaved text and images to image-caption pairs, texts, and datasets explicitly designed for nonverbal reasoning and vision-based tasks. This comprehensive training approach enables KOSMOS-1 to perform a wide array of tasks including language understanding, generation, OCR-free NLP (interpreting document images directly without the need for gradient updates or fine-tuning), multimodal interaction, speech recognition, and vision-based tasks, making it a large-scale tool in the domains of OCR-free NLP and visual perception. The model's ability to seamlessly integrate and interpret multimodal data including videos underscores the potential for significant cross-modal transfer learning, where knowledge acquired in one modality enhances performance in another. Our evaluation extends to the realm of nonverbal reasoning through a dataset based on Raven IQ tests, aimed at assessing the nonverbal reasoning capabilities of MLLMs. The results affirm KOSMOS-1's formidable capabilities across both linguistic and perceptual tasks, including speech recognition, paving the way for future advancements in the field of artificial general intelligence.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Stavros_Petridis1","manipulated_ranking":1,"natural_ranking":101,"id":"UpN2wfrLec","original_abstract":"A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce KOSMOS-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train KOSMOS-1 from scratch on web-scale multi-modal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that KOSMOS-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.","target":"~Stavros_Petridis1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=eXubleMT0q","title":"Penguin: Parallel-Packed Homomorphic Encryption for Fast Graph Convolutional Network Inference","modified_abstract":"The integration of Graph Convolutional Network (GCN) and Homomorphic Encryption (HE) heralds a promising avenue for performing graph data analytics and inference on the cloud while preserving client data privacy. This approach builds on the foundation set by prior works in graph representation and learning, notably leveraging the insights from network controllability and control theory perspectives to understand graph structures dynamically. Our investigation, grounded in dynamical systems theory, focuses on overcoming the computational and memory overhead challenges that hamper the practical deployment of HE-based GCN inference for data mining and secure analytics. By identifying inefficiencies in applying existing HE-based secure matrix-matrix multiplication solutions to GCN inference, we introduce Penguin, a novel HE-based ciphertext packing technique. Penguin is designed to minimize computation and memory overheads by employing a two-dimensional parallel packing method optimized for graph node partitioning and feature interleaving, complemented by an interleaved assembly technique for efficient ciphertexts merging after feature reduction. The technique significantly lowers the need for costly rotation operations, thereby enhancing control over computational resources. Through theoretical analysis and empirical validation using popular GCN models and datasets, which serve as benchmarks, we demonstrate Penguin's capability to achieve up to approximately 10 times speedup and about 79% reduction in computational memory overhead, vastly surpassing contemporary solutions. This marks a pivotal step towards enabling the secure acceleration of HE-GCN inference on encrypted data, safeguarding both the graph structure and its features against unauthorized access, thereby enhancing the privacy-preserving capabilities of GCN analytics. This paper's contributions are aimed at enhancing the efficiency of privacy-preserving GCN inference, with the original code accessible publicly. Note: The GitHub link has been excluded in adherence to the provided instructions.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Anwar_Said1","manipulated_ranking":4,"natural_ranking":101,"id":"eXubleMT0q","original_abstract":"The marriage of Graph Convolutional Network (GCN) and Homomorphic Encryption (HE) enables the inference of graph data on the cloud with significantly enhanced client data privacy. However, the tremendous computation and memory overhead associated with HE operations challenges the practicality of HE-based GCN inference. GCN inference involves a sequence of expensive matrix-matrix multiplications, and we observe that directly applying the state-of-the-art HE-based secure matrix-matrix multiplication solutions to accelerate HE-GCN inference is far less efficient as it does not exploit the unique aggregation mechanism of two-dimension graph node-features in GCN layer computation. \nAs a result, in this paper, we propose a novel HE-based ciphertext packing technique, i.e., Penguin, that can take advantage of the unique computation pattern during the HE-GCN inference to significantly reduce the computation and memory overhead associated with HE operations.\nSpecifically, Penguin employs (i) an effective two-dimension parallel packing technique for feature ciphertext with optimal graph node partitioning and graph feature interleaving, and (ii) an interleaved assembly technique that can effectively make use of the blank slots to merge ciphertexts after feature reduction and significantly reduce the costly rotation operation.\nWe provide theoretical analysis and experimental validation to demonstrate the speedup achieved by Penguin in accelerating GCN inference using popular GCN models and datasets. Our results show that Penguin can achieve up to $\\sim10\\times$ speedup and around $\\sim79$% reduction in computational memory overhead, significantly outperforming state-of-the-art solutions. To the best of our knowledge, this is the first work that can ensure the protection of both graph structure and features when accelerating HE-GCN inference on encrypted data. Our code is publicly available at https:\/\/github.com\/ranran0523\/Penguin.","target":"~Anwar_Said1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=zq4vFneRiA","title":"The Crucial Role of Normalization in Sharpness-Aware Minimization","modified_abstract":"In the wake of Sharpness-Aware Minimization (SAM)'s introduction and its significant impact on the predictive accuracy of deep neural networks, our investigation draws from a diverse background of optimization strategies in machine learning, particularly focusing on challenges like distribution shift where techniques such as importance weighting have sought to address disparities in training and test data distribution through novel approaches. This discourse envelops a broader spectrum of optimization complexities, including how specific methodologies can enhance or mitigate algorithm performance under varied conditions. Our work delves into understanding the role played by normalization within SAM, a pivotal mechanism that has garnered less attention yet plays a critical role in the optimizer's effectiveness. Through theoretical analysis and experimental validation, we highlight how normalization interplays with SAM's mechanics to: i) stabilize the algorithm's convergence behavior, and ii) enable the algorithm to efficiently navigate the loss landscape by seeking out a continuum of performant minima, which recent theoretical advancements suggest is instrumental for achieving superior model performance. These insights into normalization's dual role not only elucidate why SAM demonstrates robustness across different hyper-parameter settings but also accentuate the optimizer's practical viability. By comparing and contrasting our findings with existing research, notably on addressing distribution shifts through importance weighting, we provide a comprehensive examination of normalization's indispensable function in enhancing SAM's operational fidelity.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Tongtong_Fang1","manipulated_ranking":5,"natural_ranking":101,"id":"zq4vFneRiA","original_abstract":"Sharpness-Aware Minimization (SAM) is a recently proposed gradient-based optimizer (Foret et al., ICLR 2021) that greatly improves the prediction performance of deep neural networks. Consequently, there has been a surge of interest in explaining its empirical success. We focus, in particular, on understanding ***the role played by normalization***, a key component of the SAM updates. We theoretically and empirically study the effect of normalization in SAM for both convex and non-convex functions, revealing two key roles played by normalization: i) it helps in stabilizing the algorithm; and ii) it enables the algorithm to drift along a continuum (manifold) of minima -- a property identified by recent theoretical works that is the key to better performance. We further argue that these two properties of normalization make SAM robust against the choice of hyper-parameters, supporting the practicality of SAM. Our conclusions are backed by various experiments.","target":"~Tongtong_Fang1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=SzYHu7EIwZ","title":"Precision-Recall Divergence Optimization for Generative Modeling with GANs and Normalizing Flows","modified_abstract":"The quest for a balance between image quality (precision) and diversity (recall) in the domain of generative models intersects significantly with recent advancements in score-based diffusion models and their exploration of optimal diffusion times for enhanced model performance. Incorporating insights from studies that focus on the dynamics of generative modeling through stochastic differential equations and their implications for precision and recall, our work presents a novel training method for generative models, including Generative Adversarial Networks (GANs) and Normalizing Flows. This method uniquely optimizes a selected trade-off between precision and recall by minimizing a distinct $f$-divergence from a new family, designated as the \\mbox{\\em PR-divergences}, through an integrated approach of score-matching, strategic sampling, and rigorous training protocols. Detailed analysis on the approximation of traditional metrics and the introduction of new evaluation parameters leverages our understanding that any $f$-divergence can manifest as a linear composition of PR-divergences, thus facilitating a structured approach to balancing precision and recall in model training and improving the log-likelihood ratio of generated samples to real images. Through rigorous evaluations, specifically with state-of-the-art models like BigGAN on comprehensive datasets such as ImageNet, our approach demonstrably refines and enhances generative model performance either in terms of precision or recall, advocating for a potentially standardized methodology in generative model optimization.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Simone_Rossi1","manipulated_ranking":1,"natural_ranking":101,"id":"SzYHu7EIwZ","original_abstract":"Achieving a balance between image quality (precision) and diversity (recall) is a significant challenge in the domain of generative models. Current state-of-the-art models primarily rely on optimizing heuristics, such as the Fr\\'echet Inception Distance. While recent developments have introduced principled methods for evaluating precision and recall, they have yet to be successfully integrated into the training of generative models. Our main contribution is a novel training method for generative models, such as Generative Adversarial Networks and Normalizing Flows, which explicitly optimizes a user-defined trade-off between precision and recall.  More precisely, we show that achieving a specified precision-recall trade-off corresponds to minimizing a unique $f$-divergence from a family we call the \\mbox{\\em PR-divergences}. Conversely, any $f$-divergence can be written as a linear combination of PR-divergences and  corresponds to a weighted precision-recall trade-off. Through comprehensive evaluations, we show that our approach improves the performance of existing state-of-the-art models like BigGAN in terms of either precision or recall when tested on datasets such as ImageNet.","target":"~Simone_Rossi1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=I5SM5y57k2","title":"On Robust Streaming for Learning with Experts: Algorithms and Lower Bounds","modified_abstract":"Building on recent developments in Empirical Risk Minimization (ERM) that extend traditional models to accommodate dependent and heavy-tailed data, our work introduces novel algorithms for the online learning with experts problem, emphasizing the necessity for robust predictions under adaptively generated inputs. This exploration is fueled by the recognition of the complexities present in data-generating processes and the pivotal role of memory efficiency in streaming settings, alongside the need to effectively handle noise in the input data. In the online learning with experts problem, an algorithm makes predictions about an outcome on each of $T$ days, given a set of $n$ experts who make predictions on each day. The algorithm is given feedback on the outcomes of each day, including the cost of its prediction and the cost of the expert predictions, and the goal is to make a prediction with minimum cost, compared to the best expert in hindsight, while class-specific complexities introduce further challenges. However, often the predictions made by experts or algorithms at some time influence future outcomes, so that the input is adaptively generated. In this paper, our seminal contribution is the study of robust algorithms for the experts problem under memory constraints. We first give a randomized algorithm that is robust to adaptive inputs that uses $\\widetilde{O}\\left(\\frac{n}{R\\sqrt{T}}\\right)$ space for  $M=O\\left(\\frac{R^2 T}{\\log^2 n}\\right)$, thereby showing a smooth space-regret trade-off. We then show a space lower bound of $\\widetilde{\\Omega}\\left(\\frac{nM}{RT}\\right)$ for any randomized algorithm that achieves regret $R$ with probability $1-2^{-\\Omega(T)}$, when the best expert makes $M$ mistakes. Our result implies that the natural deterministic algorithm, which iterates through pools of experts until each expert in the pool has erred, is optimal up to polylogarithmic factors. Finally, we empirically demonstrate the benefit of using robust procedures against a white-box adversary that has access to the internal state of the algorithm. Through our rigorous analysis, we underscore our foundational contribution to the field of online learning.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Abhishek_Roy1","manipulated_ranking":1,"natural_ranking":101,"id":"I5SM5y57k2","original_abstract":"In the online learning with experts problem, an algorithm makes predictions about an outcome on each of $T$ days, given a set of $n$ experts who make predictions on each day. The algorithm is given feedback on the outcomes of each day, including the cost of its prediction and the cost of the expert predictions, and the goal is to make a prediction with the minimum cost, compared to the best expert in hindsight. However, often the predictions made by experts or algorithms at some time influence future outcomes, so that the input is adaptively generated. \n\nIn this paper, we study robust algorithms for the experts problem under memory constraints. We first give a randomized algorithm that is robust to adaptive inputs that uses $\\widetilde{O}\\left(\\frac{n}{R\\sqrt{T}}\\right)$ space for  $M=O\\left(\\frac{R^2 T}{\\log^2 n}\\right)$, thereby showing a smooth space-regret trade-off. We then show a space lower bound of $\\widetilde{\\Omega}\\left(\\frac{nM}{RT}\\right)$ for any randomized algorithm that achieves regret $R$ with probability $1-2^{-\\Omega(T)}$, when the best expert makes $M$ mistakes. Our result implies that the natural deterministic algorithm, which iterates through pools of experts until each expert in the pool has erred, is optimal up to polylogarithmic factors. Finally, we empirically demonstrate the benefit of using robust procedures against a white-box adversary that has access to the internal state of the algorithm.","target":"~Abhishek_Roy1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=WmqYhqvz5i","title":"Contextual Bandits and Imitation Learning with Preference-Based Active Queries","modified_abstract":"In addressing the complex challenge of decision-making without direct reward signals, this work situates itself among novel research paradigms such as Bayesian reinforcement learning (RL) that reconsider traditional approaches to learning from interactions. Specifically, our investigation into contextual bandits and imitation learning expands upon foundational concepts, such as the introduction of Bayesian Bellman operators, by focusing on scenarios where the learner must rely on preference feedback rather than explicit rewards or model-based uncertainties for control tasks. This exploration into the unique problem space where the learner lacks direct knowledge of the reward distribution associated with executed tasks is critical. Instead, the learner actively requests the expert at each round to compare two actions and receive noisy preference feedback. This not only aims to minimize regret from executed actions through rigorous analysis but also seeks to reduce the number of comparison queries to the expert, effectively bootstrapping the learning process with minimal direct feedback. Assuming access to a function class representing the expert's preference model under appropriate link functions, we introduce an actor-critic algorithm that employs a deep online regression oracle with respect to this function class, incorporating regularised learning techniques. For contextual bandit scenarios, our algorithm achieves a regret bound that adapts optimally to the context, scaling as $O(\\min\\{\\sqrt{T}, d\/\\Delta\\})$, which is competitive with standard models that receive direct reward signals. Importantly, our approach significantly reduces the necessity for direct reward feedback by making only $O(\\min\\{T, d^2\/\\Delta^2\\})$ expert queries, with exploration strategies fine-tuned for bootstrapping efficiency. We extend our methodology to imitation learning, applying it to environments of unknown dynamics over multiple episodes, and demonstrate that preference-based feedback enables surpassing sub-optimal experts, marking a significant contribution to bootstrapping knowledge in the absence of explicit rewards. This bridges a crucial gap with interactive imitation learning algorithms that require not only the expert's action observations but also reward signals for effective learning.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Matthew_Fellows1","manipulated_ranking":1,"natural_ranking":101,"id":"WmqYhqvz5i","original_abstract":"We consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action's reward. Instead, the learner can actively request the expert at each round to compare two actions and receive noisy preference feedback. The learner's objective is two-fold: to minimize regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert's preference model under appropriate link functions and present an algorithm that leverages an online regression oracle with respect to this function class. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as $O(\\min\\\\{\\sqrt{T}, d\/\\Delta\\\\})$, where $T$ represents the number of interactions, $d$ represents the eluder dimension of the function class, and $\\Delta$ represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of $\\Delta$, and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only $O(\\min\\\\{T, d^2\/\\Delta^2\\\\})$ queries to the expert. We then extend our algorithm to the imitation learning setting, where the agent engages with an unknown environment in episodes of length $H$, and provide similar guarantees regarding regret and query complexity. Interestingly, with preference-based feedback, our imitation learning algorithm can learn a policy outperforming a sub-optimal expert, matching the result from interactive imitation learning algorithms [Ross and Bagnell, 2014] that require access to the expert's actions and also reward signals.","target":"~Matthew_Fellows1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=8XRMbNAP6Z","title":"Near-Optimal $k$-Clustering in the Sliding Window Model","modified_abstract":"The concept of clustering continues to be indispensable for unraveling structural information within vast datasets, especially in scenarios where it is impractical to store the entirety of the data. Recognizing the importance of more recent data over older data in many applications, the sliding window model offers a compelling framework for addressing this priority, which has recently spurred significant interest in developing clustering solutions optimized for this model. This interest builds on previous explorations of clustering, notably the advancements in hierarchical clustering and the analytical frameworks proposed to understand and improve it. Leveraging insights from foundational works, including the implementation of Max-Uncut Bisection for hierarchical clustering optimization, we introduce the first algorithm that achieves a near-optimal $(1+\\varepsilon)$-approximation for $(k,z)$-clustering within the sliding window model. Our algorithm remarkably requires $\\frac{k}{\\min(\\varepsilon^4,\\varepsilon^{2+z})}\\,\\text{polylog}\\frac{n\\Delta}{\\varepsilon}$ words of space for points from $[\\Delta]^d$, thus presenting a significant improvement over the existing literature in terms of space efficiency, polynomial-time computability, and data similarity considerations. In the process, we pioneer an online coreset framework for clustering, which generates a coreset not just at the stream's end but also for all its prefixes, highlighting a similarity in complexities between real-time and theoretical models. This framework dictates that constructing online coresets necessitates at least $\\Omega\\left(\\frac{k}{\\varepsilon^2}\\log n\\right)$ samples, underscoring the heightened complexity compared to offline coresets. Our findings not only advance clustering methods within the sliding window model but also clarify the theoretical limitations and capabilities of online and offline coreset constructions against the backdrop of general metrics on $[\\Delta]^d$.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Alessandro_Epasto3","manipulated_ranking":1,"natural_ranking":101,"id":"8XRMbNAP6Z","original_abstract":"Clustering is an important technique for identifying structural information in large-scale data analysis, where the underlying dataset may be too large to store. In many applications, recent data can provide more accurate information and thus older data past a certain time is expired. The sliding window model captures these desired properties and thus there has been substantial interest in clustering in the sliding window model. In this paper, we give the first algorithm that achieves near-optimal $(1+\\varepsilon)$-approximation to $(k,z)$-clustering in the sliding window model. Our algorithm uses $\\frac{k}{\\min(\\varepsilon^4,\\varepsilon^{2+z})}\\,\\text{polylog}\\frac{n\\Delta}{\\varepsilon}$ words of space when the points are from $[\\Delta]^d$, thus significantly improving on works by Braverman et. al. (SODA 2016), Borassi et. al. (NeurIPS 2021), and Epasto et. al. (SODA 2022).\n\nAlong the way, we develop a data structure for clustering called an online coreset, which outputs a coreset not only for the end of a stream, but also for all prefixes of the stream. Our online coreset samples $\\frac{k}{\\min(\\varepsilon^4,\\varepsilon^{2+z})}\\,\\text{polylog}\\frac{n\\Delta}{\\varepsilon}$ points from the stream. We then show that any online coreset requires $\\Omega\\left(\\frac{k}{\\varepsilon^2}\\log n\\right)$ samples, which shows a separation between the problem of constructing an offline coreset, i.e., constructing online coresets is strictly harder. Our results also extend to general metrics on $[\\Delta]^d$ and are near-optimal in light of a $\\Omega\\left(\\frac{k}{\\varepsilon^{2+z}}\\right)$ lower bound for the size of an offline coreset.","target":"~Alessandro_Epasto3"}
{"paper_link":"https:\/\/openreview.net\/forum?id=4xckZu4MPG","title":"Attention as Implicit Structural Inference","modified_abstract":"This work is inspired by previous explorations into the hierarchical structure of data and its learnability within neural networks, specifically addressing how such structures guide deep learning architectures towards efficient learning strategies. Building on the foundations laid by concepts such as the staircase property in hierarchical data learning, our study extends the investigation into the domain of attention mechanisms in cognitive systems, focusing on Transformers, which have surged to the forefront of machine learning innovation. Attention mechanisms facilitate the flexible allocation of cognitive resources, with Transformers exemplifying this by integrating attention as a core feature, and learning to adaptively scale context windows based on input. Despite attention's prevalent role, its foundational principles, drawing parallels to keys and queries in database systems, invite further exploration into its structural underpinnings. We propose a novel viewpoint that considers attention as a form of structural inference, positing it as inference over an implicitly defined set of possible adjacency structures in a graphical model, akin to navigating a hypercube of potential connections. Through the lenses of Gaussian Mixture Models, alignment mechanisms, and Hopfield Networks, we unify various attentional architectures under this perspective, suggesting new modifications and generalizations that involve polynomial expansions of these concepts. Our investigations include an extension of the value function to bias towards multiple tokens with gradient-based optimization techniques and the introduction of a geometric prior over adjacency structures to adaptively scale context windows based on input. Furthermore, we bridge the gap between machine learning attention mechanisms and Bayesian attention models in neuroscience, aiming to enrich both fields with deeper insights into attentional architectures. This interdisciplinary approach not only enhances our understanding of attention but also paves the way for developing more sophisticated attention mechanisms that leverage gradient-based learning and other forms of polynomial approximation.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Enric_Boix_Adser\u00e01","manipulated_ranking":1,"natural_ranking":101,"id":"4xckZu4MPG","original_abstract":"Attention mechanisms play a crucial role in cognitive systems by allowing them to flexibly allocate cognitive resources. Transformers, in particular, have become a dominant architecture in machine learning, with attention as their central innovation. However, the underlying intuition and formalism of attention in Transformers is based on ideas of keys and queries in database management systems. In this work, we pursue a structural inference perspective, building upon, and bringing together, previous theoretical descriptions of attention such as; Gaussian Mixture Models, alignment mechanisms and Hopfield Networks. Specifically, we demonstrate that attention can be viewed as inference over an implicitly defined set of possible adjacency structures in a graphical model, revealing the generality of such a mechanism. This perspective unifies different attentional architectures in machine learning and suggests potential modifications and generalizations of attention. Here we investigate two and demonstrate their behaviour on explanatory toy problems: (a) extending the value function to incorporate more nodes of a graphical model yielding a mechanism with a bias toward attending  multiple tokens; (b) introducing a geometric prior (with conjugate hyper-prior) over the adjacency structures producing a mechanism which dynamically scales the context window depending on input. Moreover, by describing a link between structural inference and precision-regulation in Predictive Coding Networks, we discuss how this framework can bridge the gap between attentional mechanisms in machine learning and Bayesian conceptions of attention in Neuroscience. We hope by providing a new lens on attention architectures our work can guide the development of new and improved attentional mechanisms.","target":"~Enric_Boix_Adser\u00e01"}
{"paper_link":"https:\/\/openreview.net\/forum?id=nRfClnMhVX","title":"Interpretability at Scale: Identifying Causal Mechanisms in Alpaca","modified_abstract":"As AI safety becomes increasingly critical, the quest for methods to obtain human-interpretable explanations of model behavior is paramount. This necessity is compounded by the requirement for these interpretability methods to faithfully capture the causal dynamics underpinning the model's operations, to generalize robustly across unseen inputs, and to undergo thorough verification to ensure their accuracy, reliability, and adherence to formal specifications. Inspired by the progress in bridging the gap between natural language sentences and formal specifications, our research introduces a significant evolution of the Distributed Alignment Search (DAS) methodology. This evolution, termed Boundless DAS, supersedes the limitations of previous approaches by integrating learned parameters to replace brute-force search components, and aligning closely with programming principles for greater efficiency and precision. Particularly, by applying Boundless DAS to the Alpaca model, a language model with 7 billion parameters, we are able to efficiently uncover and analyze interpretable and regular causal structures within large-scale neural networks through systematic experiments and analysis on various datasets. Our methodology reveals that Alpaca solves numerical reasoning problems by employing a causal model with two boolean variables, showcasing the robustness of neural representation alignments across varying inputs and instructions. These breakthroughs in interpretation at scale set a new benchmark for understanding the intricate mechanisms of modern language models, marking a significant stride towards ensuring their safe and transparent utilization.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Frederik_Schmitt1","manipulated_ranking":1,"natural_ranking":101,"id":"nRfClnMhVX","original_abstract":"Obtaining human-interpretable explanations of large, general-purpose language models is an urgent goal for AI safety. However, it is just as important that our interpretability methods are faithful to the causal dynamics underlying model behavior and able to robustly generalize to unseen inputs. Distributed Alignment Search (DAS) is a powerful gradient descent method grounded in a theory of causal abstraction that uncovered perfect alignments between interpretable symbolic algorithms and small deep learning models fine-tuned for specific tasks. In the present paper, we scale DAS significantly by replacing the remaining brute-force search steps with learned parameters -- an approach we call Boundless DAS. This enables us to efficiently search for interpretable causal structure in large language models while they follow instructions. We apply Boundless DAS to the Alpaca model (7B parameters), which, off the shelf, solves a simple numerical reasoning problem. With Boundless DAS, we discover that Alpaca does this by implementing a causal model with two interpretable boolean variables. Furthermore, we find that the alignment of neural representations with these variables is robust to changes in inputs and instructions. These findings mark a first step toward deeply understanding the inner-workings of our largest and most widely deployed language models.","target":"~Frederik_Schmitt1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=Y44NurSDjq","title":"Quantum Bayesian Optimization","modified_abstract":"Inspired by advances in stochastic contextual bandits and the quest for algorithmic efficiency in representation learning, this work expands the frontier into the quantum computing domain, specifically targeting the challenges posed by complex, non-linear reward structures inherent in real-world applications. Kernelized bandits, also known as Bayesian optimization (BO), has been a prevalent method for optimizing complicated black-box reward functions. Various BO algorithms have been theoretically shown to enjoy upper bounds on their cumulative regret which are sub-linear in the number $T$ of iterations, and a regret lower bound of $\\Omega(\\sqrt{T})$ has been derived which represents the unavoidable regrets for any classical BO algorithm. Recent works on quantum bandits have shown that with the aid of quantum computing and deep learning techniques, it is possible to achieve tighter regret upper bounds, better than their corresponding classical lower bounds, and potentially recovered classical information with quantum efficiency. However, these works are restricted to either multi-armed or linear bandits, and are hence not able to solve sophisticated real-world problems with non-linear reward functions. To this end, we introduce the quantum-Gaussian process-upper confidence bound (Q-GP-UCB) algorithm, integrating it with context-action strategies and regularizing mechanisms to optimize the ratio of exploration to exploitation. To the best of our knowledge, our Q-GP-UCB is the first BO algorithm able to achieve a regret upper bound of $\\mathcal{O}(\\text{poly}\\log T)$, which is significantly smaller than its regret lower bound of $\\Omega(\\sqrt{T})$ in the classical setting. Moreover, thanks to our novel analysis of the confidence ellipsoid, our Q-GP-UCB with the linear kernel achieves a smaller regret than the quantum linear UCB algorithm from the previous work. We use simulations, as well as an experiment using a real quantum computer, to verify that the theoretical quantum speedup achieved by our Q-GP-UCB is also potentially relevant in practice.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Andrea_Tirinzoni2","manipulated_ranking":1,"natural_ranking":101,"id":"Y44NurSDjq","original_abstract":"Kernelized bandits, also known as Bayesian optimization (BO), has been a prevalent method for optimizing complicated black-box reward functions. Various BO algorithms have been theoretically shown to enjoy upper bounds on their cumulative regret which are sub-linear in the number $T$ of iterations, and a regret lower bound of $\\Omega(\\sqrt{T})$ has been derived which represents the unavoidable regrets for any classical BO algorithm. Recent works on quantum bandits have shown that with the aid of quantum computing, it is possible to achieve tighter regret upper bounds better than their corresponding classical lower bounds. However, these works are restricted to either multi-armed or linear bandits, and are hence not able to solve sophisticated real-world problems with non-linear reward functions. To this end, we introduce the quantum-Gaussian process-upper confidence bound (Q-GP-UCB) algorithm. To the best of our knowledge, our Q-GP-UCB is the first BO algorithm able to achieve a regret upper bound of $\\mathcal{O}(\\text{poly}\\log T)$, which is significantly smaller than its regret lower bound of $\\Omega(\\sqrt{T})$ in the classical setting. Moreover, thanks to our novel analysis of the confidence ellipsoid, our Q-GP-UCB with the linear kernel achieves a smaller regret than the quantum linear UCB algorithm from the previous work. We use simulations, as well as an experiment using a real quantum computer, to verify that the theoretical quantum speedup achieved by our Q-GP-UCB is also potentially relevant in practice.","target":"~Andrea_Tirinzoni2"}
{"paper_link":"https:\/\/openreview.net\/forum?id=A383wMho4h","title":"Oracle Complexity of Single-Loop Switching Subgradient Methods for Non-Smooth Weakly Convex Functional Constrained Optimization","modified_abstract":"This work builds upon foundational contributions in the area of convex-concave saddle point problems and extends the understanding of first-order methods' efficiency to the realm of non-convex constrained optimization. While previous efforts have shed light on the oracle complexity of optimistic methods in convex settings, we shift our focus to a non-convex constrained optimization problem where the objective function is weakly convex, and the constraint function is either convex or weakly convex. We evaluate the classical switching subgradient method, renowned for its straightforward implementation and effectiveness in addressing functions in convex problems, to confront this more challenging context. The paper presents the first analysis concerning the oracle complexity of the switching subgradient method in locating nearly stationary points for non-convex problems, assessing separately the cases involving convex and weakly convex constraints. Our findings highlight that, unlike double-loop methods traditionally employed for non-smooth issues, the switching subgradient method achieves comparable complexity through a more efficient single-loop configuration, obviating the need for intricate adjustments of inner iteration counts. The single-loop approach not only iterates efficiently within a broader class of optimization problems but also proves to advance our understanding of problem-solving in contexts where provably efficient methods are vital. Furthermore, the use of saddle point dynamics as a reference, reinforces the significance of the study.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Ruichen_Jiang1","manipulated_ranking":1,"natural_ranking":101,"id":"A383wMho4h","original_abstract":"We consider a non-convex constrained optimization problem, where the objective function is weakly convex and the constraint function is either convex or weakly convex. To solve this problem, we consider the classical switching subgradient method, which is an intuitive and easily implementable first-order method whose oracle complexity was only known for convex problems. This paper provides the first analysis on the oracle complexity of the switching subgradient method for finding a nearly stationary point of non-convex problems. Our results are derived separately for convex and weakly convex constraints. Compared to existing approaches, especially the double-loop methods, the switching gradient method can be applied to non-smooth problems and achieves the same complexity using only a single loop, which saves the effort on tuning the number of inner iterations.","target":"~Ruichen_Jiang1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=aIpGtPwXny","title":"Learning to Modulate pre-trained Models in RL","modified_abstract":"Inspired by the increasing exploration into how deep reinforcement learning (RL) policies evolve over trajectories and adapt across various domains, this paper seeks to address the challenges of task adaptation in RL agents. Despite the pronounced success of RL in areas such as robotics, game playing, and simulation, these agents often falter when adapting to new tasks\u2014a problem that large-scale pre-training and subsequent fine-tuning have aimed to solve in supervised learning. Taking a cue from recent advancements in understanding the dynamics of ReLU-based policies and their neural network partitioning of input space into piecewise linear regions, which suggest that the complexity of RL policies may not solely rely on the growth in function complexity around policy trajectories, our empirical study investigates the catastrophic forgetting phenomenon in the context of reinforcement learning. By jointly pre-training a neural network model on datasets from two benchmark suites, Meta-World and DMControl, and examining a range of fine-tuning methods, we identify significant performance deterioration in pre-training tasks during fine-tuning on new tasks. To counter this, we introduce a novel method, Learning-to-Modulate (L2M), which allows for the preservation of performance on pre-training tasks by modulating the information flow of the frozen pre-trained model through a learnable modulation pool. This neural modulation approach not only mitigates catastrophic forgetting but also sets new benchmarks for adaptability and skill retention in control tasks, marking a pivotal advance in the Continual-World benchmark, marking a pivotal advance in control tasks for reinforcement learning applications. The release of a comprehensive dataset encompassing 50 Meta-World and 16 DMControl tasks marks a significant empirical contribution to facilitating future research in this area.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Setareh_Cohan1","manipulated_ranking":2,"natural_ranking":101,"id":"aIpGtPwXny","original_abstract":"Reinforcement Learning (RL) has been successful in various domains like robotics, game playing, and simulation. While RL agents have shown impressive capabilities in their specific tasks, they insufficiently adapt to new tasks. In supervised learning, this adaptation problem is addressed by large-scale pre-training followed by fine-tuning to new down-stream tasks. Recently, pre-training on multiple tasks has been gaining traction in RL. However, fine-tuning a pre-trained model often suffers from catastrophic forgetting. That is, the performance on the pre-training tasks deteriorates when fine-tuning on new tasks. To investigate the catastrophic forgetting phenomenon, we first jointly pre-train a model on datasets from two benchmark suites, namely Meta-World and DMControl. Then, we evaluate and compare a variety of fine-tuning methods prevalent in natural language processing, both in terms of performance on new tasks, and how well performance on pre-training tasks is retained. Our study shows that with most fine-tuning approaches, the performance on pre-training tasks deteriorates significantly. Therefore, we propose a novel method, Learning-to-Modulate (L2M), that avoids the degradation of learned skills by modulating the information flow of the frozen pre-trained model via a learnable modulation pool. Our method achieves state-of-the-art performance on the Continual-World benchmark, while retaining performance on the pre-training tasks. Finally, to aid future research in this area, we release a dataset encompassing 50 Meta-World and 16 DMControl tasks.","target":"~Setareh_Cohan1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=d47iuwOt3j","title":"On the Gini-impurity Preservation For Privacy Random Forests","modified_abstract":"In the landscape of machine learning, safeguarding the privacy of ensemble algorithms, like random forests, is a paramount concern, evidenced by efforts ranging from anonymization to the application of differential privacy and homomorphic encryption. This work introduces a novel encryption approach designed to preserve the Gini impurity, a critical component in the construction of random forests, thereby addressing a gap in current privacy-preserving techniques that often overlook the intrinsic properties of the learning algorithms themselves. Our method modifies the binary search tree structure to accommodate multiple examples per node while encrypting data features by integrating label and order information, linking partitioning of data directly to its preservation of confidentiality. We provide a theoretical underpinning for our scheme, demonstrating that it maintains the minimum Gini impurity in ciphertexts without necessitating decryption and offer a security guarantee for the encryption process. Protocols for secure communication in the training phase ensure that our Gini-impurity-preserving encryption, combined with the homomorphic encryption scheme CKKS for data labels, highlights the importance of preserving privacy without compromising the algorithm's learning integrity and are trained to resist adversarial attacks like data poisoning. Our extensive experimentation with data transfer and validation protocols validates the effectiveness, efficiency, security, and trained robustness of our method, contributing to the broader conversation on privacy in machine learning as engaged by works like 'SafeNet', which explores the advantages of ensembles in private collaborative learning through secure multiparty computation (MPC), showcasing how ensembling can mitigate adversarial ML attacks while ensuring scalability and robustness.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Matthew_Jagielski1","manipulated_ranking":1,"natural_ranking":101,"id":"d47iuwOt3j","original_abstract":"Random forests have been one successful ensemble algorithms in machine learning. Various techniques have been utilized to preserve the privacy of random forests from anonymization, differential privacy, homomorphic encryption, etc., whereas it rarely takes into account some crucial ingredients of learning algorithm. This work presents a new encryption to preserve data's Gini impurity, which plays a crucial role during the construction of random forests. Our basic idea is to modify the structure of binary search tree to store several examples in each node,  and encrypt data features by incorporating label and order information. Theoretically, we prove that our scheme preserves the minimum Gini impurity in ciphertexts without decrypting, and present the security guarantee for encryption. For random forests, we encrypt data features based on our Gini-impurity-preserving scheme, and take the homomorphic encryption scheme CKKS to encrypt data labels  due to their importance and privacy. We  conduct extensive experiments to show the effectiveness, efficiency and security of our proposed method.","target":"~Matthew_Jagielski1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=wbbTqsiKzl","title":"High-dimensional Asymptotics of Denoising Autoencoders","modified_abstract":"This work situates itself within a burgeoning field of research concerned with the high-dimensional behavior of statistical models, specifically examining the utilization of autoencoders for denoising tasks. Inspired by recent developments in random matrix theory as applied to the estimation of the Wasserstein distance between Gaussian distributions, our study extends these mathematical tools to explore the performance of denoising autoencoders in a high-dimensional setting. We address the problem of denoising data from a Gaussian mixture using a two-layer non-linear autoencoder with tied weights and a skip connection. Considering the high-dimensional limit where the number of training samples and the input dimension jointly tend to infinity while the number of hidden units remains bounded, we provide closed-form expressions for the denoising mean-squared test error. Building on this result, we quantitatively characterize the advantage of the considered architecture over the autoencoder without the skip connection, showing a close relationship to principal component analysis. Furthermore, our results accurately capture the learning curves on a range of real datasets, demonstrating the practical applicability and theoretical significance of our findings.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Malik_Tiomoko1","manipulated_ranking":3,"natural_ranking":101,"id":"wbbTqsiKzl","original_abstract":"We address the problem of denoising data from a Gaussian mixture using a two-layer non-linear autoencoder with tied weights and a skip connection. We consider the high-dimensional limit where the number of training samples and the input dimension jointly tend to infinity while the number of hidden units remains bounded. We provide closed-form expressions for the denoising mean-squared test error. Building on this result, we quantitatively characterize the advantage of the considered architecture over the autoencoder without the skip connection that relates closely to principal component analysis. We further show that our results capture accurately the learning curves on a range of real datasets.","target":"~Malik_Tiomoko1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=5hVXbiEGXB","title":"Evolving Standardization for Continual Domain Generalization over Temporal Drift","modified_abstract":"This research extends the discourse on domain generalization (DG) by addressing its limitations in dynamic scenarios characterized by gradual data distribution changes, an area previously underexplored despite advances in attention-based domain adaptation for time-series forecasting. The capability of generalizing to out-of-distribution data is crucial for the deployment of machine learning models in the real world. Existing domain generalization (DG) mainly embarks on offline and discrete scenarios, where multiple source domains are simultaneously accessible and the distribution shift among domains is abrupt and violent. Nevertheless, this setting may not apply universally in real-world applications, as there are cases where the data distribution changes gradually over time due to various factors, e.g., the process of aging. Additionally, as the domain constantly evolves, new domains will continually emerge. Re-training and updating models with both new and previous domains using existing DG methods can be resource-intensive and inefficient. Thus, this paper presents a problem formulation for Continual Domain Generalization over Temporal Drift (CDGTD). CDGTD addresses the challenge of gradually shifting data distributions over time, where domains arrive sequentially and models can only access the data of the current domain. The goal is to generalize to unseen domains not too far into the future. To this end, we propose an Evolving Standardization (EvoS) method, which characterizes the evolving pattern of feature distribution and mitigates the distribution shift by standardizing features with generated statistics of corresponding domains. Inspired by the powerful ability of transformers to model sequence relations in deep learning, we design a multi-scale attention module (MSAM) to learn the evolving pattern under sliding time windows of different lengths. MSAM can generate statistics of the current domain based on the statistics of previous domains and the learned evolving pattern. Experiments on multiple real-world datasets including series of images and texts validate the efficacy of our EvoS.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Xiaoyong_Jin1","manipulated_ranking":1,"natural_ranking":101,"id":"5hVXbiEGXB","original_abstract":"The capability of generalizing to out-of-distribution data is crucial for the deployment of machine learning models in the real world. Existing domain generalization (DG) mainly embarks on offline and discrete scenarios, where multiple source domains are simultaneously accessible and the distribution shift among domains is abrupt and violent. Nevertheless, such setting may not be universally applicable to all real-world applications, as there are cases where the data distribution gradually changes over time due to various factors, e.g., the process of aging. Additionally, as the domain constantly evolves, new domains will continually emerge. Re-training and updating models with both new and previous domains using existing DG methods can be resource-intensive and inefficient. Therefore, in this paper, we present a problem formulation for Continual Domain Generalization over Temporal Drift (CDGTD). CDGTD addresses the challenge of gradually shifting data distributions over time, where domains arrive sequentially and models can only access the data of the current domain. The goal is to generalize to unseen domains that are not too far into the future. To this end, we propose an Evolving Standardization (EvoS) method, which characterizes the evolving pattern of feature distribution and mitigates the distribution shift by standardizing features with generated statistics of corresponding domain. Specifically, inspired by the powerful ability of transformers to model sequence relations, we design a multi-scale attention module (MSAM) to learn the evolving pattern under sliding time windows of different lengths. MSAM can generate statistics of current domain based on the statistics of previous domains and the learned evolving pattern. Experiments on multiple real-world datasets including images and texts validate the efficacy of our EvoS.","target":"~Xiaoyong_Jin1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=pjSzKhSrfs","title":"Wasserstein Quantum Monte Carlo: A Novel Approach for Solving the Quantum Many-Body Schr\u00f6dinger Equation","modified_abstract":"Solving the quantum many-body Schr\u00f6dinger equation represents a cornerstone challenge within quantum physics, quantum chemistry, and materials science, akin to the pursuit of optimized solutions in fully composite and convex optimization described in preceding research. The prevailing methodologies, such as Quantum Variational Monte Carlo (QVMC), are transformative yet encumbered by limitations that stem from the computational intricacies of representing complex wave functions and the optimization of their associated energy functionals. This analysis draws inspiration from the rich landscape of optimization problems, specifically leveraging concepts from the affine-invariant linearization of fully composite optimization to rethink the traditional QVMC framework. By transitioning the optimization focus from the space of wave functions to the space of Born distributions, this paper introduces a novel interpretation of QVMC as the Fisher--Rao gradient flow in this distributional space, with a projection back onto the variational manifold. Our research proposes an innovative approach named \"Wasserstein Quantum Monte Carlo\" (WQMC). WQMC, adopting the Wasserstein metric for the gradient flow, enhances the transportation of probability mass for ground-state solutions. This method not only offers a novel framework for addressing the quantum many-body problem but also demonstrates, through empirical studies and experiments on molecular systems, that it achieves faster convergence than traditional QVMC techniques, showcasing its potential applications across our explored fields. The introduction of WQMC presents a paradigm shift in the computational optimization treatment of the Schr\u00f6dinger equation, mirroring the advancements in optimization theory and practice.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Maria-Luiza_Vladarean1","manipulated_ranking":1,"natural_ranking":101,"id":"pjSzKhSrfs","original_abstract":"Solving the quantum many-body Schr\u00f6dinger equation is a fundamental and challenging problem in the fields of quantum physics, quantum chemistry, and material sciences. One of the common computational approaches to this problem is Quantum Variational Monte Carlo (QVMC), in which ground-state solutions are obtained by minimizing the energy of the system within a restricted family of parameterized wave functions. Deep learning methods partially address the limitations of traditional QVMC by representing a rich family of wave functions in terms of neural networks. However, the optimization objective in QVMC remains notoriously hard to minimize and requires second-order optimization methods such as natural gradient. In this paper, we first reformulate energy functional minimization in the space of Born distributions corresponding to particle-permutation (anti-)symmetric wave functions, rather than the space of wave functions. We then interpret QVMC as the Fisher--Rao gradient flow in this distributional space, followed by a projection step onto the variational manifold. This perspective provides us with a principled framework to derive new QMC algorithms, by endowing the distributional space with better metrics, and following the projected gradient flow induced by those metrics. More specifically, we propose \"Wasserstein Quantum Monte Carlo\" (WQMC), which uses the gradient flow induced by the Wasserstein metric, rather than the Fisher--Rao metric, and corresponds to *transporting* the probability mass, rather than *teleporting* it. We demonstrate empirically that the dynamics of WQMC results in faster convergence to the ground state of molecular systems.","target":"~Maria-Luiza_Vladarean1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=oKqaWlEfjY","title":"Worst-case Performance of Popular Approximate Nearest Neighbor Search Implementations: Guarantees and Limitations","modified_abstract":"In the context of evaluating the robustness of machine learning algorithms, such as decision tree learning algorithms, our study extends this critical examination to the domain of graph-based approaches for approximate nearest neighbor search, which, despite their practical efficacy, lack robust theoretical guarantees. We specifically analyze the worst-case performance of leading graph-based approximate nearest neighbor search algorithms, including HNSW, NSG, and DiskANN. For DiskANN, we establish that its 'slow preprocessing' version can support approximate nearest neighbor search queries with a constant approximation ratio and poly-logarithmic query time in data sets with bounded 'intrinsic' dimension. Conversely, we identify a set of instances for other data structure variants studied, like DiskANN with 'fast preprocessing', HNSW, and NSG, where the empirical query time needed to achieve 'reasonable' accuracy scales linearly with the instance size. For instance, in the case of DiskANN, our findings indicate that the query procedure may require at least $0.1 n$ steps in instances of size $n$ to locate any of the $5$ nearest neighbors of the query, underscoring significant limitations in worst-case scenarios.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Qin-Cheng_Zheng1","manipulated_ranking":6,"natural_ranking":101,"id":"oKqaWlEfjY","original_abstract":"Graph-based approaches to nearest neighbor search are popular and powerful tools for handling large datasets in practice, but they have limited theoretical guarantees. \nWe study the worst-case performance of recent graph-based approximate nearest neighbor search algorithms, such as HNSW, NSG and DiskANN. For DiskANN, we show that its \"slow preprocessing'' version provably supports approximate nearest neighbor search query with constant approximation ratio and poly-logarithmic query time, on data sets with bounded \"intrinsic'' dimension. \nFor the other data structure variants studied, including DiskANN with \"fast preprocessing'',  HNSW and NSG,  we present a family of instances on which the empirical query time required to achieve a \"reasonable'' accuracy is linear in instance size. For example, for DiskANN, we show that the query procedure can take at least  $0.1 n$ steps on instances of size $n$ before it encounters any of the $5$ nearest neighbors of the query.","target":"~Qin-Cheng_Zheng1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=6oiux75UDj","title":"Bayesian Optimization with Cost-varying Variable Subsets","modified_abstract":"Inspired by recent progress in the field of machine learning, particularly concerning learning from label proportions (LLP) in the context of linear threshold functions (LTFs), our study introduces a novel problem within the domain of Bayesian optimization, termed Bayesian optimization with cost-varying variable subsets (BOCVS). In each iteration of BOCVS, the learner selects a subset of query variables to assign values, while the remaining variables are sampled randomly. Each subset selection incurs a different cost, posing a unique challenge for the learner to balance the acquisition of informative subsets for more targeted learning against the minimization of costs by allowing some variables to be sampled randomly. We propose a novel Gaussian process upper confidence bound-based algorithm for BOCVS, which is demonstrated to be provably no-regret. Our analysis reveals how the exploration strategy, aided by the selection of cheaper variable subsets, effectively reduces overall regret. Empirical evaluations confirm that our algorithm outperforms existing baselines within the same budget, offering a robust solution to the BOCVS problem. The connection to LLP and linear threshold functions alongside considerations of Boolean input spaces and the labeled (supervised) nature of the task highlights the intricate role of information costing in learning algorithms and sets a foundation for exploration in cost-sensitive learning scenarios, making efforts toward tight bounds on inappropriability a critical aspect of our investigation.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Rishi_Saket1","manipulated_ranking":1,"natural_ranking":101,"id":"6oiux75UDj","original_abstract":"We introduce the problem of Bayesian optimization with cost-varying variable subsets (BOCVS) where in each iteration, the learner chooses a subset of query variables and specifies their values while the rest are randomly sampled. Each chosen subset has an associated cost. This presents the learner with the novel challenge of balancing between choosing more informative subsets for more directed learning versus leaving some variables to be randomly sampled to reduce incurred costs. This paper presents a novel Gaussian process upper confidence bound-based algorithm for solving the BOCVS problem that is provably no-regret. We analyze how the availability of cheaper control sets helps in exploration and reduces overall regret. We empirically show that our proposed algorithm can find significantly better solutions than comparable baselines with the same budget.","target":"~Rishi_Saket1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=fShubymWrc","title":"Provable Guarantees for Nonlinear Feature Learning in Three-Layer Neural Networks","modified_abstract":"In the pursuit of unraveling the complexities of deep learning, particularly the process through which neural networks learn hierarchical features, this work is inspired by foundational contributions such as the analysis of deep networks in the context of the multiple manifold problem. Such precedents have illuminated the advantages of network depth and width for classification tasks, providing a springboard for our investigation into the superior feature learning capabilities of three-layer networks over their two-layer counterparts. We posit that three-layer neural networks harbor inherently richer potential for nonlinear feature extraction, a premise we substantiate by analyzing the feature learning process in these networks when trained with layer-wise gradient descent. Manifolds play a crucial role in this discourse, as our examination leverages the concept to deepen our understanding of the intricacies involved in the feature learning dynamics of neural networks. By introducing a theorem that upper bounds the nonasymptotic sample complexity and network width necessary to attain low test error for targets with a specific hierarchical structure, we make strides in theoretical advancements. Our investigation delves into statistical learning scenarios such as single-index models and functions of quadratic features, revealing that three-layer networks can achieve sample complexity improvements over all known guarantees for two-layer networks. This enhancement is fundamentally tied to their efficient learning of nonlinear features and the concentration of measure phenomena. Further, we delineate a depth separation based on concrete optimization considerations, constructing a function learnable through gradient descent on a three-layer network but intractable for two-layer networks. Through these findings, our work progresses the understanding of the intrinsic advantages offered by adding layers in neural networks, specifically in the realm of feature learning, and underscores the significance of machine learning in advancing non-linear feature extraction capabilities.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Sam_Buchanan1","manipulated_ranking":1,"natural_ranking":101,"id":"fShubymWrc","original_abstract":"One of the central questions in the theory of deep learning is to understand how neural networks learn hierarchical features. The ability of deep networks to extract salient features is crucial to both their outstanding generalization ability and the modern deep learning paradigm of pretraining and finetuneing. However, this feature learning process remains poorly understood from a theoretical perspective, with existing analyses largely restricted to two-layer networks. In this work we show that three-layer neural networks have provably richer feature learning capabilities than two-layer networks. We analyze the features learned by a three-layer network trained with layer-wise gradient descent, and present a general purpose theorem which upper bounds the sample complexity and width needed to achieve low test error when the target has specific hierarchical structure. We instantiate our framework in specific statistical learning settings -- single-index models and functions of quadratic features -- and show that in the latter setting three-layer networks obtain a sample complexity improvement over all existing guarantees for two-layer networks. Crucially, this sample complexity improvement relies on the ability of three-layer networks to efficiently learn *nonlinear* features. We then establish a concrete optimization-based depth separation by constructing a function which is efficiently learnable via gradient descent on a three-layer network, yet cannot be learned efficiently by a two-layer network. Our work makes progress towards understanding the provable benefit of three-layer neural networks over two-layer networks in the feature learning regime.","target":"~Sam_Buchanan1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=qSS9izTOpo","title":"Alleviating the Semantic Gap for Generalized fMRI-to-Image Reconstruction","modified_abstract":"Building on the foundational work in context-aware image completion, which highlights the challenges of bridging the semantic gap in visual data processing, this paper introduces a novel approach to the fMRI-to-image reconstruction problem. By recognizing the limitations of existing methodologies that struggle with semantic consistency between training and testing datasets, our research proposes an innovative solution to stabilize and clarify semantic understanding in reconstructed images. We achieve this by leveraging the pre-trained CLIP model to map the training data to a compact feature representation, significantly expanding the sparse semantics of training data to encompass denser semantic fields and thereby reducing the discrepancies of instances near known concepts within the training super-classes. Additionally, inspired by the robust low-level fMRI data representation, we employ structural information as a universal guide for image reconstruction, particularly for instances distanced from familiar concepts (outside the training super-classes), further narrowing the semantic divide. Our proposed Generalized fMRI-to-image reconstruction strategy, named GESS (Generalized fMRI-to-image reconstruction by adaptively integrating Expanded Semantics and Structural information within a diffusion process), quantifies semantic uncertainty with probability density estimation to refine reconstruction fidelity and synthesis of pixel-level detail, ensuring a seamless integration of foreground and background elements. Experimental validation of the GESS model showcases its superiority over state-of-the-art alternatives, particularly in managing the semantic gap through a generalized scenario split strategy tailored for assessing GESS's effectiveness in semantic gap minimization. The architecture of the GESS model, incorporating 'blocks' specialized for handling different aspects of the image reconstruction\u2014including those for semantic completion and photo-realistic detail synthesis\u2014further facilitates this process, proving vital in achieving our objective.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Minguk_Kang1","manipulated_ranking":1,"natural_ranking":101,"id":"qSS9izTOpo","original_abstract":"Although existing fMRI-to-image reconstruction methods could predict high-quality images, they do not explicitly consider the semantic gap between training and testing data, resulting in reconstruction with unstable and uncertain semantics. This paper addresses the problem of generalized fMRI-to-image reconstruction by explicitly alleviates the semantic gap. Specifically, we leverage the pre-trained CLIP model to map the training data to a compact feature representation, which essentially extends the sparse semantics of training data to dense ones, thus alleviating the semantic gap of the instances nearby known concepts (i.e., inside the training super-classes). Inspired by the robust low-level representation in fMRI data, which could help alleviate the semantic gap for instances that far from the known concepts (i.e., outside the training super-classes), we leverage structural information as a general cue to guide image reconstruction. Further, we quantify the semantic uncertainty based on probability density estimation and achieve Generalized fMRI-to-image reconstruction by adaptively integrating Expanded Semantics and Structural information (GESS) within a diffusion process. Experimental results demonstrate that the proposed GESS model outperforms state-of-the-art methods, and we propose a generalized scenario split strategy to evaluate the advantage of GESS in closing the semantic gap.","target":"~Minguk_Kang1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=4PkBhz18in","title":"High Precision Causal Model Evaluation with Conditional Randomization","modified_abstract":"In a landscape where the feasibility and ethical considerations often limit the implementation of randomized controlled trials (RCTs), this work is inspired by the necessity to adapt causal inference methodologies, such as those addressing challenges posed by unknown control variables. Our research introduces an innovative approach to causal model evaluation that transcends traditional methods, targeting the inherent limitations of RCTs by leveraging conditionally randomized experiments based on inverse probability weighting (IPW). Such experiments, while more adaptable to real-world scenarios, are known for their high estimation variance. To address this, we propose a novel low-variance estimator for causal error, termed the pairs estimator. This estimator applies the same IPW methodology to both the model and true experimental effects, thereby specifying and neutralizing the variance introduced by IPW and achieving smaller asymptotic variance. The efficacy of our approach is substantiated through empirical studies, indicating a significant stride toward achieving near-RCT precision in causal model evaluation. By simplifying the application of IPW without requiring complex modifications to the estimator itself, our method facilitates more accurate and pragmatic evaluation of causal inference models in settings where controlled randomization is applicable, thereby contributing a vital tool for robust and reliable model assessment. Control over external variables and the precise specification of causal mechanisms are emphasized, bridging the gap between the theoretical and practical applications of causal inference and underlining our commitment to improving these methods.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Dave_Zachariah1","manipulated_ranking":1,"natural_ranking":101,"id":"4PkBhz18in","original_abstract":"The gold standard for causal model evaluation involves comparing model predictions with true effects estimated from randomized controlled trials (RCT). However, RCTs are not always feasible or ethical to perform. In contrast, conditionally randomized experiments based on inverse probability weighting (IPW) offer a more realistic approach but may suffer from high estimation variance. To tackle this challenge and enhance causal model evaluation in real-world conditional randomization settings, we introduce a novel low-variance estimator for causal error, dubbed as the pairs estimator. By applying the same IPW estimator to both the model and true experimental effects, our estimator effectively cancels out the variance due to IPW and achieves a smaller asymptotic variance. Empirical studies demonstrate the improved of our estimator, highlighting its potential on achieving near-RCT performance. Our method offers a simple yet powerful solution to evaluate causal inference models in conditional randomization settings without complicated modification of the IPW estimator itself, paving the way for more robust and reliable model assessments.","target":"~Dave_Zachariah1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=bv9mmH0LGF","title":"Global Structure-Aware Diffusion Process for Low-light Image Enhancement","modified_abstract":"Building on the foundation laid by previous works, such as the study on efficient acquisition of diffuse and specular normal maps through optimized illumination patterns, our research introduces a diffusion-based enhancement method specifically tailored to the challenges of low-light image enhancement. We explore the diffusion process deeper by advocating for the regularization of its inherent ODE-trajectory. Inspired by recent findings that low curvature ODE-trajectory results in a stable and effective diffusion process, we introduce a curvature regularization term anchored in the global structural characteristics of image data, referred to as global structure-aware regularization. This approach aims at maintaining complex details and enhancing contrast during the diffusion process while mitigating the adverse effects of noise and artifacts. Further, we acquire an uncertainty-guided regularization to adaptively relax constraints in the most challenging image regions, incorporating reflection patterns that are crucial for preserving the specular highlights and normal details of low-light images. By acquiring novel methods and leveraging acquisition patterns, our framework, augmented with rank-informed regularization, achieves notable improvements in enhancing low-light images, outperforming current leading methods in aspects of image quality, noise reduction, contrast enhancement, and the faithful recovery of reflectances. This work not only contributes to the enhancement of low-light images but also opens avenues for further research in diffusion model applications in image processing, especially in understanding and preserving the complex interactions of light with surfaces. The code for this study has been made publicly available.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Byeongjoo_Ahn5","manipulated_ranking":1,"natural_ranking":101,"id":"bv9mmH0LGF","original_abstract":"This paper studies a diffusion-based framework to address the low-light image enhancement problem. To harness the capabilities of diffusion models, we delve into this intricate process and advocate for the regularization of its inherent ODE-trajectory. To be specific, inspired by the recent research that low curvature ODE-trajectory results in a stable and effective diffusion process, we formulate a curvature regularization term anchored in the intrinsic non-local structures of image data, i.e., global structure-aware regularization, which gradually facilitates the preservation of complicated details and the augmentation of contrast during the diffusion process. This incorporation mitigates the adverse effects of noise and artifacts resulting from the diffusion process, leading to a more precise and flexible enhancement. To additionally promote learning in challenging regions, we introduce an uncertainty-guided regularization technique, which wisely relaxes constraints on the most extreme regions of the image. Experimental evaluations reveal that the proposed diffusion-based framework, complemented by rank-informed regularization, attains distinguished performance in low-light enhancement. The outcomes indicate substantial advancements in image quality, noise suppression, and contrast amplification in comparison with state-of-the-art methods. We believe this innovative approach will stimulate further exploration and advancement in low-light image processing, with potential implications for other applications of diffusion models. The code is publicly available at https:\/\/github.com\/jinnh\/GSAD.","target":"~Byeongjoo_Ahn5"}
{"paper_link":"https:\/\/openreview.net\/forum?id=Ct0zPIe3xs","title":"Saving 100x Storage: Prototype Replay for Reconstructing Training Sample Distribution in Class-Incremental Semantic Segmentation","modified_abstract":"In the context of class-incremental semantic segmentation (CISS), where each incremental step focuses on different foreground classes, the limitation of training sets to include only images with the relevant foreground classes leads to an overrepresentation of these classes and a skewed class distribution. This significant challenge, which contributes to catastrophic forgetting and background shift, reflects an area that requires innovative approaches for balanced learning and effective memory usage in visual content. Our work is inspired by novel methodologies in unsupervised visual feature learning and network architecture, such as parametric instance classification (PIC), which emphasizes simplicity and efficiency in handling data and model updates without the complexity of dual-branch, non-parametric frameworks. Embracing this perspective, we introduce STAR, a method that not only significantly reduces storage requirements by more than 100 times compared to methods relying on raw image replays but also adjusts class distribution in training through the use of compact prototypes and statistical data replay for instance-based classification. STAR distinguishes itself by incorporating an old-class features maintaining (OCFM) loss and a similarity-aware discriminative (SAD) loss, which collectively preserve the feature integrity of old classes while facilitating the learning of new ones and enhancing feature discrimination between similar class pairs, thus improving classification accuracy in network-based visual semantic segmentation. Through extensive testing on the Pascal VOC 2012 and ADE20K datasets, our method sets new benchmarks, outperforming existing approaches in class-incremental learning for visual semantic segmentation.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Zhenda_Xie1","manipulated_ranking":1,"natural_ranking":101,"id":"Ct0zPIe3xs","original_abstract":"Existing class-incremental semantic segmentation (CISS) methods mainly tackle catastrophic forgetting and background shift, but often overlook another crucial issue. In CISS, each step focuses on different foreground classes, and the training set for a single step only includes images containing pixels of the current foreground classes, excluding images without them. This leads to an overrepresentation of these foreground classes in the single-step training set, causing the classification biased towards these classes. To address this issue, we present STAR, which preserves the main characteristics of each past class by storing a compact prototype and necessary statistical data, and aligns the class distribution of single-step training samples with the complete dataset by replaying these prototypes and repeating background pixels with appropriate frequency. Compared to the previous works that replay raw images, our method saves over 100 times the storage while achieving better performance. Moreover, STAR incorporates an old-class features maintaining (OCFM) loss, keeping old-class features unchanged while preserving sufficient plasticity for learning new classes. Furthermore, a similarity-aware discriminative (SAD) loss is employed to specifically enhance the feature diversity between similar old-new class pairs. Experiments on two public datasets, Pascal VOC 2012 and ADE20K, reveal that our model surpasses all previous state-of-the-art methods.","target":"~Zhenda_Xie1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=9kFQEJSyCM","title":"RangePerception: Taming LiDAR Range View for Efficient and Accurate 3D Object Detection","modified_abstract":"In the realm of LiDAR-based 3D object detection, the quest for efficiency and accuracy has led to a divergence in methodological approaches, prominently featuring bird's-eye view (BEV) and range view (RV) perspectives. While BEV-based methods boast precision through voxelization and 3D convolutions, their computational demands hinder efficient training and inference. Conversely, RV-based methods offer streamlined processing via 2D convolutions, though they historically lag in detection performance. This study introduces 'RangePerception,' a framework designed to capitalize on RV's efficiency without sacrificing the accuracy benchmarked by BEV methodologies. Among the key features of this framework, the fusion of distinct data representations and the Range Aware Kernel (RAK) enhance the feature interpretability of LiDAR point clouds by addressing the inherent domain discrepancy between 3D world coordinates (output) and 2D range image coordinates (input), and mitigating the distortion of object visibility at range image margins. The vision Restoration Module (VRM) further supports this endeavor by restoring and highlighting distinctive features previously obscured, employing self-attentive mechanisms to ensure that the fine-grained point features of objects are preserved and emphasized. The implementation of RAK and VRM into the RangePerception framework propels its performance, surpassing the previous RV-based frontrunner, RangeDet, by 3.25\/4.18 points in averaged L1\/L2 AP on the Waymo Open Dataset. Remarkably, RangePerception not only equals but slightly exceeds the performance of the prominent BEV-based method, CenterPoint, while operating at 1.3 times its speed. This achievement marks a significant milestone for RV-based methods in 3D object detection, suggesting a promising direction for future research in LiDAR data processing and analysis.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Yan-Pei_Cao1","manipulated_ranking":1,"natural_ranking":101,"id":"9kFQEJSyCM","original_abstract":"LiDAR-based 3D detection methods currently use bird's-eye view (BEV) or range view (RV) as their primary basis. The former relies on voxelization and 3D convolutions, resulting in inefficient training and inference processes. Conversely, RV-based methods demonstrate higher efficiency due to their compactness and compatibility with 2D convolutions, but their performance still trails behind that of BEV-based methods. To eliminate this performance gap while preserving the efficiency of RV-based methods, this study presents an efficient and accurate RV-based 3D object detection framework termed RangePerception. Through meticulous analysis, this study identifies two critical challenges impeding the performance of existing RV-based methods: 1) there exists a natural domain gap between the 3D world coordinate used in output and 2D range image coordinate used in input, generating difficulty in information extraction from range images; 2) native range images suffer from vision corruption issue, affecting the detection accuracy of the objects located on the margins of the range images. To address the key challenges above, we propose two novel algorithms named Range Aware Kernel (RAK) and Vision Restoration Module (VRM), which facilitate information flow from range image representation and world-coordinate 3D detection results. With the help of RAK and VRM, our RangePerception achieves 3.25\/4.18 higher averaged L1\/L2 AP compared to previous state-of-the-art RV-based method RangeDet, on Waymo Open Dataset. For the first time as an RV-based 3D detection method, RangePerception achieves slightly superior averaged AP compared with the well-known BEV-based method CenterPoint and the inference speed of RangePerception is 1.3 times as fast as CenterPoint.","target":"~Yan-Pei_Cao1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=w116w62fxH","title":"Optimal Learners for Realizable Regression: PAC Learning and Online Learning","modified_abstract":"Building upon prior research that established foundational principles in the learnability of machine learning models, particularly in context to the sufficiency of the fat shattering dimension for PAC learnability and the role of coresets in approximating loss functions through advanced sampling techniques, this work aims to advance the understanding of the statistical complexity of realizable regression. We draw inspiration from the field's ongoing exploration into identifying key dimensions that govern learnability and optimization in machine learning; for instance, the introduction of sensitivity sampling in coresets for improving algorithm efficiency underscores the importance of developing nuanced metrics and methodologies. In this vein, our research introduces a minimax instance optimal learner for realizable regression in both the probably approximately correct (PAC) learning setting and the online learning setting, marking a significant stride toward bridging theoretical concepts with practical learning applications. Our approach is deeply rooted in optimization, leveraging mathematical functions to model complexities and devise solutions that enhance regression analysis effectiveness. We propose a novel dimension that both qualitatively and quantitatively characterizes which classes of real-valued predictors or classifiers are learnable, extending the dialogue initiated by notable precedents in the field. The identification of a combinatorial dimension related to the graph dimension for ERM learnability in the realizable setting, alongside establishing a necessary condition for learnability based on a dimension akin to the DS dimension, punctuates our contributions. Also, in addressing a question by Daskalakis and Golowich, we delineate a dimension determining the minimax instance optimal cumulative loss in online learning, which serves as a cornerstone for the design of an optimal online learner for realizable regression.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Murad_Tukan1","manipulated_ranking":1,"natural_ranking":101,"id":"w116w62fxH","original_abstract":"In this work, we aim to characterize the statistical complexity of realizable regression both in the PAC learning setting and the online learning setting. Previous work had established the sufficiency of finiteness of the fat shattering dimension for PAC learnability and the necessity of finiteness of the scaled Natarajan dimension, but little progress had been made towards a more complete characterization since  the work of Simon 1997 (SICOMP '97). To this end,  we first introduce a minimax instance optimal learner for realizable regression and propose a novel dimension that both qualitatively and quantitatively characterizes which classes of real-valued predictors are learnable.  We then identify a combinatorial dimension related to the graph dimension that characterizes ERM learnability in the realizable setting. Finally, we establish a necessary condition for learnability based on a combinatorial dimension related to the DS dimension, and conjecture that it may also be sufficient in this context. Additionally, in the context of online learning we provide a dimension that characterizes the minimax instance optimal cumulative loss up to a constant factor and design an optimal online learner for realizable regression, thus resolving an open question raised by Daskalakis and Golowich in STOC '22.","target":"~Murad_Tukan1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=Bj1QSgiBPP","title":"Participatory Personalization in Classification","modified_abstract":"The development and integration of machine learning models into diverse applications necessitate addressing the critical dimensions of consent and privacy, especially when personalizing services with sensitive, protected, or self-reported data. These considerations take precedence as models increasingly handle data related to individuals without facilitating their consent or informing them of the benefits derived from personalization. Building upon previous works that aim to measure disparities among population groups within public health, economics, and machine learning, our research introduces a new paradigm in predictive modeling. We propose participatory systems\u2014a family of prediction models that allow individuals to opt into personalization at prediction time. This shift towards a model-agnostic algorithm furthers the ability to personalize with categorical group attributes while addressing the crucial aspect of informed consent in computational systems. Our empirical study, focused on clinical prediction tasks, benchmarks participatory systems against common personalization and imputation approaches, evidencing that these systems not only enhance performance across diverse groups but also significantly improve privacy and consent processes, thereby addressing the identified disparities and operationalizing fairness in predictive analytics. Surveys conducted as part of our empirical study provide qualitative data supporting the effectiveness of participatory personalization in reducing disparity and improving user engagement with machine learning applications. This aligns with overarching goals in machine learning and public health to tailor interventions and services ethically and effectively, marking a step forward in operationalizing fairness and privacy in predictive analytics.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Harvineet_Singh1","manipulated_ranking":3,"natural_ranking":101,"id":"Bj1QSgiBPP","original_abstract":"Machine learning models are often personalized based on information that is protected, sensitive, self-reported, or costly to acquire. These models use information about people, but do not facilitate nor inform their *consent*. Individuals cannot opt out of reporting information that a model needs to personalize their predictions nor tell if they benefit from personalization in the first place. We introduce a new family of prediction models, called participatory systems, that let individuals opt into personalization at prediction time. We present a model-agnostic algorithm to learn participatory systems for supervised learning tasks where models are personalized with categorical group attributes. We conduct a comprehensive empirical study of participatory systems in clinical prediction tasks, comparing them to common approaches for personalization and imputation. Our results show that participatory systems can facilitate and inform consent in a way that improves performance and privacy across all groups who report personal data.","target":"~Harvineet_Singh1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=Sf17j2pkCU","title":"Optimistic Exploration in Reinforcement Learning Using Symbolic Model Estimates","modified_abstract":"In light of recent exploratory efforts in reinforcement learning (RL) that utilize symbolic models to enhance learning efficiency and decision-making\u2014such as the optimization of multi-step off-policy learning approaches in distributional RL\u2014our study introduces a novel methodology that transcends the conventional reliance on pre-existing symbolic models. By proposing an innovative mechanism for learning optimistic symbolic approximations of the world model, this research not only addresses the limitations of previous works but also synergizes with fast diverse planners from the automated planning community to facilitate optimistic exploration in environments characterized by sparse rewards. Our method focuses on accelerating the learning process by generalizing learned model dynamics across similar actions and practices with minimal human intervention. The evaluation conducted across multiple benchmark domains, and subsequent comparisons with existing RL strategies that include deep learning, off-policy, and distributional approaches, further validate the efficacy of our approach in both theory and practice. This investigation not only broadens the scope of symbolic reinforcement learning but also demonstrates significant steps forward in addressing the challenge of sparse reward settings through the lens of optimistic exploration and symbolic model estimation, with experimental guarantees of improved performance.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Yunhao_Tang1","manipulated_ranking":3,"natural_ranking":101,"id":"Sf17j2pkCU","original_abstract":"There has been an increasing interest in using symbolic models along with reinforcement learning (RL) problems, where these coarser abstract models are used as a way to provide RL agents with higher level guidance. However, most of these works are inherently limited by their assumption of having an access to a symbolic approximation of the underlying problem. To address this issue, we introduce a new method for learning optimistic symbolic approximations of the underlying world model. We will see how these representations, coupled with fast diverse planners developed by the automated planning community, provide us with a new paradigm for optimistic exploration in sparse reward settings. We investigate the possibility of speeding up the learning process by generalizing learned model dynamics across similar actions with minimal human input. Finally, we evaluate the method, by testing it on multiple benchmark domains and compare it with other RL strategies.","target":"~Yunhao_Tang1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=konBXvt2iS","title":"Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of ReLU Networks","modified_abstract":"Building on foundational insights from the analysis of deep linear networks, which revealed crucial aspects of the optimization landscape and the impacts of initialization and architecture on reaching global minima, our research extends into the complex domain of ReLU networks. The training process of ReLU neural networks often exhibits complicated nonlinear phenomena. The nonlinearity of models and non-convexity of loss pose significant challenges for theoretical analysis, paramount in understanding the behaviors of neurons within these networks. Therefore, most previous theoretical works on the optimization dynamics of neural networks focus either on local analysis (like the end of training) or approximate linear models (like Neural Tangent Kernel). In this work, we conduct a complete theoretical characterization of the training process of a two-layer ReLU network trained by Gradient Flow on a linearly separable dataset. This specific setting, beginning from random initialization to final convergence, underscores how the decay of the learning rate interacts with these dynamics. Our analysis not only addresses methods applied during this process but also delves into the initialization phase's critical role in determining the trajectory of the optimization. Despite the relatively simple model and data that we studied, we reveal four different phases from the whole training process showing a general simplifying-to-complicating learning trend. Specific nonlinear behaviors can also be precisely identified and captured theoretically, such as initial condensation, saddle-to-plateau dynamics, plateau escape, changes of activation patterns, learning with increasing complexity, etc.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Liu_Ziyin1","manipulated_ranking":2,"natural_ranking":101,"id":"konBXvt2iS","original_abstract":"The training process of ReLU neural networks often exhibits complicated nonlinear phenomena. \nThe nonlinearity of models and non-convexity of loss pose significant challenges for theoretical analysis. Therefore, most previous theoretical works on the optimization dynamics of neural networks focus either on local analysis (like the end of training) or approximate linear models (like Neural Tangent Kernel). \nIn this work, we conduct a complete theoretical characterization of the training process of a two-layer ReLU network trained by Gradient Flow on a linearly separable data. In this specific setting, our analysis captures the whole optimization process starting from random initialization to final convergence. \nDespite the relatively simple model and data that we studied, we reveal four different phases from the whole training process showing a general simplifying-to-complicating learning trend.\nSpecific nonlinear behaviors can also be precisely identified and captured theoretically, such as\ninitial condensation, saddle-to-plateau dynamics, plateau escape, changes of activation patterns, \nlearning with increasing complexity, etc.","target":"~Liu_Ziyin1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=FFdrXkm3Cz","title":"On the spectral bias of two-layer linear networks","modified_abstract":"Inspired by recent advancements in understanding optimization algorithms through the lens of dynamic systems, particularly focusing on first-order optimization inspired from finite-time convergent flows, this paper examines the behavior of two-layer fully connected networks with linear activations trained with gradient flow on the square loss. We delve into the optimization process, revealing an implicit bias on the parameters that is contingent upon the scale of its initialization and examining the flow dynamics including the signed-gradient aspects within the training process. The core contribution of this study is a variational characterization of the loss minimizers retrieved by the gradient flow, particularly for a specific shape of initialization. This characterization uncovers a propensity towards a low-rank configuration of the neural network's hidden layer in the regime of small scale initialization. Furthermore, we introduce a concept of hidden mirror flow to elucidate the dynamics of the singular values of the weights matrices, detailing their temporal evolution within the flow framework. The theoretical results are substantiated with numerical experiments that illustrate the described phenomena, thereby enriching our comprehension of the spectral properties inherent to linear neural networks trained under gradient descent.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Orlando_Romero1","manipulated_ranking":1,"natural_ranking":101,"id":"FFdrXkm3Cz","original_abstract":"This paper studies the behaviour of two-layer fully connected networks with linear activations trained with gradient flow on the square loss. We show how the optimization process carries an implicit bias on the parameters that depends on the scale of its initialization.  The main result of the paper is a variational characterization of the loss minimizers retrieved by the gradient flow for a specific initialization shape. This characterization reveals that, in the small scale initialization regime, the linear neural network's hidden layer is biased toward having a low-rank structure. To complement our results, we showcase a hidden mirror flow that tracks the dynamics of the singular values of the weights matrices and describe their time evolution. We support our findings with numerical experiments illustrating the phenomena.","target":"~Orlando_Romero1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=Vx1JadlOIt","title":"VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks","modified_abstract":"Recent work in the field of computer vision, such as OVRL-V2, which presents a single neural network architecture achieving state-of-the-art results on visual navigation tasks through simple, task-agnostic components, highlights the evolving landscape of models with versatile applicability. Inspired by these advancements and the recognition of the limited open-ended task capabilities of current vision foundation models (VFMs), we introduce VisionLLM, an LLM-based framework designed to bridge this gap by conceptualizing images as a form of foreign language. This novel approach leverages the flexibility of LLMs in handling user-tailored, language-based tasks and applies it to vision-centric tasks, providing a unified perspective that aligns these tasks with language instructions for an LLM-based decoder to process. Through extensive experimentation and training protocols, incorporating pre-training and neural network optimization techniques, VisionLLM demonstrates its capability to customize tasks dynamically, ranging from detailed object-level instructions to broader, task-level directives, all while achieving competitive results. Specifically, VisionLLM's achievement of over 60% mean average precision (mAP) on the challenging COCO dataset, showcases its potential today to match and even exceed detection-specific models without being confined to predefined tasks or traditional convolutions and compression methods. Moreover, VisionLLM's adoption of patch-level analysis and the conceptual shift in navigation through visual tasks, further indicates its promise. This exploration not only contributes to the growing interface between vision and language modeling but also sets a new precedent for future developments in generalist models that are adaptable across a broad spectrum of vision and language tasks, incorporating planning and flexibility. The forthcoming release of our implementation code aims to facilitate further research and advancements in this promising domain.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Arjun_Majumdar2","manipulated_ranking":1,"natural_ranking":101,"id":"Vx1JadlOIt","original_abstract":"Large language models (LLMs) have notably accelerated progress towards artificial general intelligence (AGI), with their impressive zero-shot capacity for user-tailored tasks, endowing them with immense potential across a range of applications. However, in the field of computer vision, despite the availability of numerous powerful vision foundation models (VFMs), they are still restricted to tasks in a pre-defined form, struggling to match the open-ended task capabilities of LLMs. In this work, we present an LLM-based framework for vision-centric tasks, termed VisionLLM. This framework provides a unified perspective for vision and language tasks by treating images as a foreign language and aligning vision-centric tasks with language tasks that can be flexibly defined and managed using language instructions. An LLM-based decoder can then make appropriate predictions based on these instructions for open-ended tasks. Extensive experiments show that the proposed VisionLLM can achieve different levels of task customization through language instructions, from fine-grained object-level to coarse-grained task-level customization, all with good results. It's noteworthy that, with a generalist LLM-based framework, our model can achieve over 60% mAP on COCO, on par with detection-specific models. We hope this model can set a new baseline for generalist vision and language models. The code shall be released.","target":"~Arjun_Majumdar2"}
{"paper_link":"https:\/\/openreview.net\/forum?id=vf77fTbgG3","title":"Structured Voronoi Sampling","modified_abstract":"In light of recent progress in gradient-based Markov Chain Monte Carlo (MCMC) methods for optimizing proposal distributions in high-dimensional spaces, our work introduces a novel gradient-based sampling algorithm tailored for text generation, addressing the gap in theoretically grounded approaches within this domain. This step towards structured, principled sampling is inspired by advancements in adaptive MCMC techniques that leverage gradient information for enhanced sampling efficiency and distribution approximation. Our method, which we term Structured Voronoi Sampling (SVS), applies discrete distributions provided by language models to define densities, utilizing a Hamiltonian Monte Carlo based algorithm for sampling. In rigorous experimental comparisons where the reference distribution is known to be intractable, SVS demonstrates superior alignment with the reference distribution over alternative sampling methods, providing strong evidence of its effectiveness. Moreover, in tasks requiring controlled text generation, SVS achieves fluent and diverse output in adherence to specified control targets, outperforming existing methods in terms of generating quality and diversity robustly. This alignment with controlled objectives illustrates SVS's potential in enhancing the granularity and precision of text generation processes, underlining its significant contribution to the learning mechanism of modern AI systems. The use of chains in our methodology is critical for navigating the complexity of high-dimensional spaces robustly, enabling a more structured approach to explore and generate text, firmly establishing SVS as a valuable tool for generating proposals in complex learning scenarios where direct sampling is often intractable.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Michalis_Titsias1","manipulated_ranking":1,"natural_ranking":101,"id":"vf77fTbgG3","original_abstract":"Gradient-based sampling algorithms have demonstrated their effectiveness in text generation, especially in the context of controlled text generation. However, there exists a lack of theoretically grounded and principled approaches for this task. In this paper, we take an important step toward building a principled approach for sampling from language models with gradient-based methods. We use discrete distributions given by language models to define densities and develop an algorithm based on Hamiltonian Monte Carlo to sample from them. We name our gradient-based technique Structured Voronoi Sampling (SVS). In an experimental setup where the reference distribution is known, we show that the empirical distribution of SVS samples is closer to the reference distribution compared to alternative sampling schemes. Furthermore, in a controlled generation task, SVS is able to generate fluent and diverse samples while following the control targets significantly better than other methods.","target":"~Michalis_Titsias1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=8HzOyg1ngp","title":"Efficient Subgame Refinement for Extensive-form Games","modified_abstract":"In the landscape of computational game theory, recent advancements in independent policy gradient methods for Markov potential games have highlighted the potential for scalable learning algorithms even in environments with large state spaces and numerous players. Inspired by these developments, which demonstrate the feasibility of achieving Nash equilibria with reduced complexity in large-scale settings, our research introduces an innovative approach to subgame solving in extensive-form games. Specifically, we propose a generative subgame solving (GS2) framework, deploying a generation function to strategically identify a subset of earliest-reached nodes, effectively curtailing the size of the subgame without sacrificing the integrity of the strategy refinement process. This framework not only leverages a diversity-based generation function but also utilizes reinforcement learning techniques and sample-based approximation methods to ensure safety and robustness in strategy selection. Through our theoretical analysis and practical experiments on medium-sized games as well as the complex game of GuanDan, we showcase the efficacy of learning through the GS2 framework in navigating the challenges posed by large imperfect information games. Our results illustrate significant performance enhancements over traditional blueprint strategies, and the policy gradient-based method provides a promising avenue for research into efficient, scalable game-solving methodologies.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Chen-Yu_Wei1","manipulated_ranking":1,"natural_ranking":101,"id":"8HzOyg1ngp","original_abstract":"Subgame solving is an essential technique in addressing large imperfect information games, with various approaches developed to enhance the performance of refined strategies in the abstraction of the target subgame. However, directly applying existing subgame solving techniques may be difficult, due to the intricate nature and substantial size of many real-world games. To overcome this issue, recent subgame solving methods allow for subgame solving on limited knowledge order subgames, increasing their applicability in large games; yet this may still face obstacles due to extensive information set sizes. To address this challenge, we propose a generative subgame solving (GS2) framework, which utilizes a generation function to identify a subset of the earliest-reached nodes, reducing the size of the subgame. Our method is supported by a theoretical analysis and employs a diversity-based generation function to enhance safety. Experiments conducted on medium-sized games as well as the challenging large game of GuanDan demonstrate a significant improvement over the blueprint.","target":"~Chen-Yu_Wei1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=afKnrwJBAl","title":"Cross-Episodic Curriculum for Transformer Agents","modified_abstract":"Inspired by advances in meta-reinforcement learning (meta-RL) that address the challenges of data efficiency and policy generality in deep reinforcement learning (RL), we introduce the Cross-Episodic Curriculum (CEC) algorithm. This algorithm is designed to enhance the learning efficiency and generalization capabilities of Transformer agents, leveraging the concept of cross-episodic experiences to enrich a Transformer\u2019s context and curriculum. By arranging online learning trials and mixed-quality demonstrations in a sequential structure, CEC creates a learning framework that mirrors the progression and proficiency increase seen in meta-RL, but with a focus on cross-episodic memory and attention. This model showcases its effectiveness in two distinct scenarios: multi-task reinforcement learning with widespread, discrete control challenges, as demonstrated in environments like DeepMind Lab, and imitation learning with mixed-quality data for continuous control settings, exemplified by RoboMimic. The results from these scenarios indicate that policies trained using CEC and related algorithms outperform traditional methods in terms of performance and generalization to unassociated tasks. To support further research within this machine learning domain, a survey of existing approaches is offered, and the code for our experiments has been made publicly available. [The URL for the GitHub repository has been omitted for privacy].","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Risto_Ilkka_Antero_Vuorio1","manipulated_ranking":1,"natural_ranking":101,"id":"afKnrwJBAl","original_abstract":"We present a new algorithm, Cross-Episodic Curriculum (CEC), to boost the learning efficiency and generalization of Transformer agents. Central to CEC is the placement of cross-episodic experiences into a Transformer\u2019s context, which forms the basis of a curriculum. By sequentially structuring online learning trials and mixed-quality demonstrations, CEC constructs curricula that encapsulate learning progression and proficiency increase across episodes. Such synergy combined with the potent pattern recognition capabilities of Transformer models delivers a powerful cross-episodic attention mechanism. The effectiveness of CEC is demonstrated under two representative scenarios: one involving multi-task reinforcement learning with discrete control, such as in DeepMind Lab, where the curriculum captures the learning progression in both individual and progressively complex settings; and the other involving imitation learning with mixed-quality data for continuous control, as seen in RoboMimic, where the curriculum captures the improvement in demonstrators' expertise. In all instances, policies resulting from CEC exhibit superior performance and strong generalization. Code is open-sourced on the project website https:\/\/cec-agent.github.io\/ to facilitate research on Transformer agent learning.","target":"~Risto_Ilkka_Antero_Vuorio1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=sTjW3JHs2V","title":"Let the Flows Tell:  Solving Graph Combinatorial Problems with GFlowNets","modified_abstract":"Combinatorial optimization (CO) problems, characterized as often NP-hard and challenging for exact algorithms, find a promising avenue for application of machine learning methods to sample and solve instances with binary decision variables. These are inspired by both classical approaches to binary integer and linear programming as well as recent strides in other domains of optimization like online learning. GFlowNets have emerged as a novel solution, demonstrating the capacity to efficiently sample from composite unnormalized densities in a sequenced manner\u2014a critical capability for addressing the structured constraints inherent in CO problems and generating diverse solution candidates efficiently, including those requiring permutational decision-making. In this work, we design Markov decision processes (MDPs) tailored to various combinatorial problems and introduce training strategies for conditional GFlowNets, enabling effective sampling from the solution spaces of these problems. Our proposed methods also focus on improving long-range credit assignment to enhance training efficiency and minimize regret in sequential decision-making. The efficacy of GFlowNet policies in finding high-quality solutions is substantiated through comprehensive experiments across a range of CO tasks, employing both synthetic and real-world data. This paper also contributes to the field by making our implementation available for public use, thereby inviting an open review of our approaches and facilitating further advancements in algorithms for CO. The inclusion of randomized techniques ensures robust and diversified outcomes, aligning well with the principle of minimizing regret in online decision scenarios.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Chunlin_Sun1","manipulated_ranking":1,"natural_ranking":101,"id":"sTjW3JHs2V","original_abstract":"Combinatorial optimization (CO) problems are often NP-hard and thus out of reach for exact algorithms, making them a tempting domain to apply machine learning methods. The highly structured constraints in these problems can hinder either optimization or sampling directly in the solution space.\nOn the other hand, GFlowNets have recently emerged as a powerful machinery to efficiently sample from composite unnormalized densities sequentially and have the potential to amortize such solution-searching processes in CO, as well as generate diverse solution candidates.\nIn this paper, we design Markov decision processes (MDPs) for different combinatorial problems and propose to train conditional GFlowNets to sample from the solution space. \nEfficient training techniques are also developed to benefit long-range credit assignment.\nThrough extensive experiments on a variety of different CO tasks with synthetic and realistic data, we demonstrate that GFlowNet policies can efficiently find high-quality solutions.\nOur implementation is open-sourced at https:\/\/github.com\/zdhNarsil\/GFlowNet-CombOpt.","target":"~Chunlin_Sun1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=pvPujuvjQd","title":"Most Neural Networks Are Almost Learnable","modified_abstract":"Inspired by recent developments in the PAC-Bayesian learning framework, particularly in understanding generalisation bounds for scenarios with unbounded loss functions, our work explores the learnability of neural networks through a novel lens. We present a PTAS for learning random constant-depth networks. This exploration delves into the interval of possible improvements in the theoretical underpinnings of machine learning, particularly focusing on regression tasks where unbounded loss functions play a crucial role. We show that for any fixed $\\epsilon>0$ and depth $i$, there is a poly-time algorithm that for any distribution on $\\sqrt{d} \\cdot \\mathbb{S}^{d-1}$ learns random Xavier networks of depth $i$, up to an additive error of $\\epsilon$. The algorithm, showcasing the relevance of the PAC-Bayes framework for new scenarios in learning, operates with a time and sample complexity of $(\\bar{d})^{\\mathrm{poly}(\\epsilon^{-1})}$, indicating that most neural networks are, indeed, almost learnable under these conditions. The integration of unbounded losses within the PAC-Bayes framework significantly advances our understanding and offers a broader theoretical framework by which we can analyze neural network learnability under certain activation functions and depths.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Maxime_Haddouche1","manipulated_ranking":1,"natural_ranking":101,"id":"pvPujuvjQd","original_abstract":"We present a PTAS for learning random constant-depth networks. We show that for any fixed $\\epsilon>0$ and depth $i$, there is a poly-time algorithm that for any distribution on $\\sqrt{d} \\cdot \\mathbb{S}^{d-1}$ learns random Xavier networks of depth $i$, up to an additive error of $\\epsilon$. The algorithm runs in time and sample complexity of $(\\bar{d})^{\\mathrm{poly}(\\epsilon^{-1})}$, where $\\bar d$ is the size of the network. For some  cases of sigmoid and ReLU-like activations the bound can be improved to $(\\bar{d})^{\\mathrm{polylog}(\\epsilon^{-1})}$, resulting in a quasi-poly-time algorithm for learning constant depth random networks.","target":"~Maxime_Haddouche1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=dnGEPkmnzO","title":"Fully Dynamic $k$-Clustering in $\\tilde O(k)$ Update Time","modified_abstract":"In light of recent explorations into the complexities of community detection through sub-hypergraph and stochastic models, our investigation addresses the dynamic aspect of clustering problems, presenting a $O(1)$-approximate fully dynamic algorithm for the $k$-median and $k$-means problems on metric spaces. This algorithm demonstrates significant advantages in managing communities within evolving data sets, highlighting its effectiveness in multipartite recovery and community detection within large, dynamically changing environments. With an amortized update time of $\\tilde O(k)$ and a worst-case query time of $\\tilde O(k^2)$, this study extends beyond static analysis to accommodate evolving data sets effectively, signaling a notable advancement in the multipartite recovery of dynamic communities. Our theoretical discourse is enriched by experimental analyses, marking a pioneering in-depth study for the dynamic $k$-median problem on general metric spaces and establishing a comparative framework with the cutting-edge works of Henzinger and Kale [ESA'20]. Further, a foundational contribution of our work lies in furnishing a lower bound for the dynamic $k$-median problem, evidencing that any $O(1)$-approximate algorithm with $\\tilde O(\\text{poly}(k))$ query time inherently requires $\\tilde \\Omega(k)$ amortized update time, even under incremental conditions. This revelation resonates with the findings on the limitations imposed by information theoretic bounds and the role of probability in community detection, thereby integrating practical algorithmic developments with theoretical insights from the related domain.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Chuyang_Ke2","manipulated_ranking":1,"natural_ranking":101,"id":"dnGEPkmnzO","original_abstract":"We present a $O(1)$-approximate fully dynamic algorithm for the $k$-median and $k$-means problems on metric spaces with amortized update time $\\tilde O(k)$ and worst-case query time $\\tilde O(k^2)$. We complement our theoretical analysis with the first in-depth experimental study for the dynamic $k$-median problem on general metrics, focusing on comparing our dynamic algorithm to the current state-of-the-art by Henzinger and Kale [ESA'20]. Finally, we also provide a lower bound for dynamic $k$-median which shows that any $O(1)$-approximate algorithm with $\\tilde O(\\text{poly}(k))$ query time must have $\\tilde \\Omega(k)$ amortized update time, even in the incremental setting.","target":"~Chuyang_Ke2"}
{"paper_link":"https:\/\/openreview.net\/forum?id=EETqXXdqkI","title":"PICProp: Physics-Informed Confidence Propagation for Uncertainty Quantification","modified_abstract":"Inspired by the emerging synergy between deep learning and physics-driven models, this paper addresses the persistent limitations of uncertainty quantification in these realms. Our work is deeply influenced by foundational efforts that have leveraged physics-informed machine learning techniques, such as the coupling of PyTorch and Firedrake for PDE-based modeling, to imbibe models with the capability to adhere to physical laws while being efficient in data usage and fostering high-performance computation. Standard approaches in both deep learning and physics-informed learning have been bounded by their dependency on data likelihood assumptions, prior selection, and the approximation of posteriors, often leading to computationally expensive and poor approximations. In response, this paper presents a method for estimating confidence intervals (CIs) for deterministic partial differential equations, a novel problem, by proposing Physics-Informed Confidence Propagation (PICProp), which utilizes bi-level optimization to propagate confidence, in the form of CIs, from data locations to the entire domain with probabilistic guarantees, without relying on heavy assumptions. This modelling approach is further enhanced by considering the coupled nature of physical systems and how this complexity can be managed through PICProp. Furthermore, we provide a theorem that establishes the validity of our method and demonstrate its application through computational experiments focused on physics-informed learning and training. This initiative strives to contribute to the ongoing discourse on integrating physical principles into machine learning models to enhance their interpretability and reliability, thereby encouraging the composition of knowledge from disparate disciplines. The high-performance capabilities of this approach are underscored by its potential for use in simulations that require rigorous training of machine learning models.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Nacime_Bouziani1","manipulated_ranking":1,"natural_ranking":101,"id":"EETqXXdqkI","original_abstract":"Standard approaches for uncertainty quantification in deep learning and physics-informed learning have persistent limitations. \nIndicatively, strong assumptions regarding the data likelihood are required, the performance highly depends on the selection of priors, and the posterior can be sampled only approximately, which leads to poor approximations because of the associated computational cost.\nThis paper introduces and studies confidence interval (CI) estimation for deterministic partial differential equations as a novel problem.\nThat is, to propagate confidence, in the form of CIs, from data locations to the entire domain with probabilistic guarantees.\nWe propose a method, termed Physics-Informed Confidence Propagation (PICProp), based on bi-level optimization to compute a valid CI without making heavy assumptions.\nWe provide a theorem regarding the validity of our method, and computational experiments, where the focus is on physics-informed learning. Code is available at https:\/\/github.com\/ShenQianli\/PICProp.","target":"~Nacime_Bouziani1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=wiv21EJ0Vd","title":"Zero-shot Visual Relation Detection via Composite Visual Cues from Large Language Models","modified_abstract":"Inspired by pioneering work in vision-language models, such as InternLM-XComposer, which advanced the state-of-the-art in image-text comprehension and composition, we address the challenge of zero-shot visual relation detection (VRD). Pretrained vision-language models, such as CLIP, have exhibited strong generalization capabilities, positioning them as valuable assets for zero-shot visual recognition tasks, and specifically for bridging text-image gaps. VRD, a task focused on identifying types of relationships or interactions between object pairs within images, faces challenges when applying these models directly, including difficulties in distinguishing between fine-grained relation types and overlooking essential spatial relationships between objects. Our novel approach, RECODE (RElation detection via COmposite DEscription prompts), aims to overcome these limitations through a synergistic composition of visual and textual data. RECODE decomposes predicate categories into subject, object, and spatial components and utilizes large language models to generate descriptive prompts, or visual cues, for each element. These cues significantly improve the discrimination of similar relation categories by offering varied perspectives. Additionally, we employ a deep chain-of-thought method with large language models to assign reasonable weights to different visual cues, enhancing RECODE's performance on four VRD benchmarks. This deep learning method not only demonstrates improved effectiveness in zero-shot VRD but also offers a novel, interpretable, and potentially manuscript-worthy framework that may inform future developments in the field, possibly extending to immersive multilingual comprehension contexts.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Shuangrui_Ding1","manipulated_ranking":1,"natural_ranking":101,"id":"wiv21EJ0Vd","original_abstract":"Pretrained vision-language models, such as CLIP, have demonstrated strong generalization capabilities, making them promising tools in the realm of zero-shot visual recognition. Visual relation detection (VRD) is a typical task that identifies relationship (or interaction) types between object pairs within an image. However, naively utilizing CLIP with prevalent class-based prompts for zero-shot VRD has several weaknesses, e.g., it struggles to distinguish between different fine-grained relation types and it neglects essential spatial information of two objects. To this end, we propose a novel method for zero-shot VRD: RECODE, which solves RElation detection via COmposite DEscription prompts. Specifically, RECODE first decomposes each predicate category into subject, object, and spatial components. Then, it leverages large language models (LLMs) to generate description-based prompts (or visual cues) for each component. Different visual cues enhance the discriminability of similar relation categories from different perspectives, which significantly boosts performance in VRD. To dynamically fuse different cues, we further introduce a chain-of-thought method that prompts LLMs to generate reasonable weights for different visual cues. Extensive experiments on four VRD benchmarks have demonstrated the effectiveness and interpretability of RECODE.","target":"~Shuangrui_Ding1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=I18BXotQ7j","title":"GeoCLIP: Clip-Inspired Alignment between Locations and Images for Effective Worldwide Geo-localization","modified_abstract":"Inspired by recent breakthroughs in vision-language models and their significant impact on tasks such as open-world instance-level 3D scene understanding, which leverages pre-trained models to bridge the gap between visual and linguistic information, our work introduces GeoCLIP, a novel approach that applies these insights to the domain of worldwide geo-localization. Geo-localization aims to pinpoint the precise location of images taken anywhere on Earth, presenting challenges due to geographic diversity and the impracticality of image-to-image retrieval at a global scale. Traditional methods, which rely on dividing the globe into discrete geographic cells for categorization, often lead to inaccuracies due to the limited resolution of predefined classes. GeoCLIP overcomes these limitations by enforcing alignment between images and their corresponding GPS locations, using a CLIP-inspired Image-to-GPS retrieval task approach. This involves modeling the Earth as a continuous function with a location encoder that employs positional encoding through random Fourier features. Our hierarchical representation captures information at varying resolutions, yielding a semantically rich high-dimensional feature space suitable for geo-localization. This understanding enhances the model's ability to process unlabeled data effectively, marking this as the first initiative to employ GPS encoding for this categorization purpose, demonstrating the method's efficacy through extensive experiments and ablations on benchmark datasets, establishing a new form of supervision for geo-localization tasks. With competitive performance achieved using just 20% of the training data, GeoCLIP proves effective even in limited-data scenarios and introduces the ability to perform geo-localization using text queries, thereby expanding the perception of location-aware applications.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Jihan_Yang1","manipulated_ranking":1,"natural_ranking":101,"id":"I18BXotQ7j","original_abstract":"Worldwide Geo-localization aims to pinpoint the precise location of images taken anywhere on Earth. This task has considerable challenges due to the immense variation in geographic landscapes. The image-to-image retrieval-based approaches fail to solve this problem on a global scale as it is not feasible to construct a large gallery of images covering the entire world. Instead, existing approaches divide the globe into discrete geographic cells, transforming the problem into a classification task. However, their performance is limited by the predefined classes and often results in inaccurate localizations when an image's location significantly deviates from its class center. To overcome these limitations, we propose GeoCLIP, a novel CLIP-inspired Image-to-GPS retrieval approach that enforces alignment between the image and its corresponding GPS locations. GeoCLIP's location encoder models the Earth as a continuous function by employing positional encoding through random Fourier features and constructing a hierarchical representation that captures information at varying resolutions to yield a semantically rich high-dimensional feature suitable to use even beyond geo-localization. To the best of our knowledge, this is the first work employing GPS encoding for geo-localization. We demonstrate the efficacy of our method via extensive experiments and ablations on benchmark datasets. We achieve competitive performance with just 20% of training data, highlighting its effectiveness even in limited-data settings. Furthermore, we qualitatively demonstrate geo-localization using a text query by leveraging the CLIP backbone of our image encoder. The project webpage is available at: https:\/\/vicentevivan.github.io\/GeoCLIP","target":"~Jihan_Yang1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=oMm1dfo3tK","title":"Unbiased constrained sampling with Self-Concordant Barrier Hamiltonian Monte Carlo","modified_abstract":"Motivated by recent advancements in graph theory and sampling methods, particularly focusing on non-adaptive edge counting and sampling techniques, this paper introduces Barrier Hamiltonian Monte Carlo (BHMC), an innovative version of the Hamiltonian Monte Carlo (HMC) algorithm. BHMC is designed for sampling from a Gibbs distribution $\\pi$ on a manifold $\\mathsf{M}$, endowed with a Hessian metric $\\mathfrak{g}$ derived from a self-concordant barrier. The development of BHMC is a response to the necessity of incorporating constraints defining $\\mathsf{M}$ while exploiting its underlying geometry and ensuring connectivity across the sampling space. Traditional Hamiltonian dynamics, defined by separable Ordinary Differential Equations (ODEs) in the Euclidean case, introduces bias when generalized to Riemannian manifolds due to their non-separable nature. To overcome this, we propose a novel 'involution checking step' integrated into two versions of BHMC, termed continuous BHMC (c-BHMC) and numerical BHMC (n-BHMC). This procedural addition ensures that the generated Markov chains are reversible with respect to $\\pi$ and are unbiased, differentiating our approach from previous implementations. The effectiveness and unbiased nature of our proposed algorithms are validated through numerical experiments targeting distributions defined on polytopes, offering significant implications for estimation techniques in bounded domains.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Raghavendra_Addanki1","manipulated_ranking":1,"natural_ranking":101,"id":"oMm1dfo3tK","original_abstract":"In this paper, we propose Barrier Hamiltonian Monte Carlo (BHMC), a version of the\n  HMC algorithm which aims at sampling from a Gibbs distribution $\\pi$ on a manifold\n  $\\mathsf{M}$, endowed with a Hessian metric $\\mathfrak{g}$ derived from a self-concordant\n  barrier. Our method relies on Hamiltonian\n  dynamics which comprises $\\mathfrak{g}$. Therefore, it incorporates the constraints defining\n  $\\mathsf{M}$ and is able to exploit its underlying geometry. However, \n  the corresponding Hamiltonian dynamics is defined via non separable Ordinary Differential Equations (ODEs) in contrast to the Euclidean case. It implies unavoidable bias in existing generalization of HMC to Riemannian manifolds. In this paper, we propose a new filter step, called ``involution checking step'', to address this problem. This step is implemented in two versions of BHMC, coined continuous BHMC (c-bHMC) and  numerical BHMC (n-BHMC) respectively.\n  Our main results establish that these two new algorithms  generate reversible Markov\n  chains with respect to $\\pi$ and do not suffer from any bias in comparison to previous implementations. Our conclusions are supported by numerical experiments where\n  we consider target distributions defined on polytopes.","target":"~Raghavendra_Addanki1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=wwkQUiaKbo","title":"Adapting Fairness Interventions to Missing Values","modified_abstract":"Our inquiry into the realm of algorithmic fairness is significantly informed by prior work on the statistical nuances of multi-label classification (MLC) in sparse label regimes. This foundational understanding lays the groundwork for our exploration of how missing values, a common yet complex issue in real-world data, impact algorithmic fairness. Different demographic groups may be unequally affected by missing data, leading to exacerbated discrimination through the standard \"impute-then-classify\" procedure. In this study, we demonstrate that training classifiers on imputed data can considerably deteriorate both group fairness and average accuracy due to the loss of information inherent in the missing data patterns. We propose scalable and adaptive algorithms for fair classification that effectively accommodate missing values without sacrificing the information they contain. These algorithms seamlessly integrate with existing fairness interventions, offering a more equitable and accurate classification across diverse datasets. Employing rigorous bounds for regret minimization, our numerical experiments showcase the superiority of our approach over traditional impute-then-classify methods, affirming its applicability and effectiveness in promoting fairness in natural machine learning applications.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Krzysztof_Dembczynski1","manipulated_ranking":1,"natural_ranking":101,"id":"wwkQUiaKbo","original_abstract":"Missing values in real-world data pose a significant and unique challenge to algorithmic fairness. Different demographic groups may be unequally affected by missing data, and the standard procedure for handling missing values where first data is imputed, then the imputed data is used for classification\u2014a procedure referred to as \"impute-then-classify\"\u2014can exacerbate discrimination. In this paper, we analyze how missing values affect algorithmic fairness. We first prove that training a classifier from imputed data can significantly worsen the achievable values of group fairness and average accuracy. This is because imputing data results in the loss of the missing pattern of the data, which often conveys information about the predictive label. We present scalable and adaptive algorithms for fair classification with missing values. These algorithms can be combined with any preexisting fairness-intervention algorithm to handle all possible missing patterns while preserving information encoded within the missing patterns. Numerical experiments with state-of-the-art fairness interventions demonstrate that our adaptive algorithms consistently achieve higher fairness and accuracy than impute-then-classify across different datasets.","target":"~Krzysztof_Dembczynski1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=mmmd2vp0n0","title":"Transient Neural Radiance Fields for Lidar View Synthesis and 3D Reconstruction","modified_abstract":"Inspired by recent advancements in machine learning for enhancing underexposed photos through deep illumination estimation, this work extends the applications of neural radiance fields (NeRFs) to model scene appearance and geometry from multiview imagery with a novel integration of lidar data. While prior attempts have utilized lidar-derived point clouds as supplementary information, they have not fully leveraged the unique image formation capabilities of lidar technology. Our method introduces transient NeRFs, which utilize raw, time-resolved photon count histograms from a single-photon lidar system, rendering these histograms from novel viewpoints and capturing transient light transport phenomena at unprecedented picosecond timescales. This approach employs a time-resolved adaptation of the volume rendering equation specific to lidar measurements, marking a significant departure from conventional NeRF techniques with refined loss functions for better accuracy. Evaluated on a pioneering benchmark dataset of simulated and real transient multiview scans obtained from a prototype single-photon lidar system, our method not only enhances the rendering of transient imagery by improving lighting conditions but also recovers superior geometry and appearance, including color enhancement, from sparse viewpoints compared to existing lidar-supervised NeRFs. Such advancements hold immense potential for simulating raw lidar data in applications crucial for autonomous driving, robotics, and remote sensing, thereby expanding the scope of NeRFs to include time-resolved imaging dimensions and exemplify the deep network's learning capability.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Ruixing_Wang2","manipulated_ranking":12,"natural_ranking":101,"id":"mmmd2vp0n0","original_abstract":"Neural radiance fields (NeRFs) have become a ubiquitous tool for modeling scene appearance and geometry from multiview imagery. Recent work has also begun to explore how to use additional supervision from lidar or depth sensor measurements in the NeRF framework. However, previous lidar-supervised NeRFs focus on rendering conventional camera imagery and use lidar-derived point cloud data as auxiliary supervision; thus, they fail to incorporate the underlying image formation model of the lidar. Here, we propose a novel method for rendering transient NeRFs that take as input the raw, time-resolved photon count histograms measured by a single-photon lidar system, and we seek to render such histograms from novel views. Different from conventional NeRFs, the approach relies on a time-resolved version of the volume rendering equation to render the lidar measurements and capture transient light transport phenomena at picosecond timescales. We evaluate our method on a first-of-its-kind dataset of simulated and captured transient multiview scans from a prototype single-photon lidar. Overall, our work brings NeRFs to a new dimension of imaging at transient timescales, newly enabling rendering of transient imagery from novel views. Additionally, we show that our approach recovers improved geometry and conventional appearance compared to point cloud-based supervision when training on few input viewpoints. Transient NeRFs may be especially useful for applications which seek to simulate raw lidar measurements for downstream tasks in autonomous driving, robotics, and remote sensing.","target":"~Ruixing_Wang2"}
{"paper_link":"https:\/\/openreview.net\/forum?id=dAbGv5Jz5U","title":"Contrastive Sampling Chains in Diffusion Models","modified_abstract":"Inspired by the success of recent methodologies in diffusion models (DMs) for generative tasks, such as the development of Gaussian Mixture Solvers (GMS) for enhancing sampling efficiency and quality in image generation and stroke-based synthesis, our research introduces a novel approach to address a fundamental limitation in diffusion models concerning discretization error. The past few years have witnessed great success in the use of DMs to generate high-fidelity images for a variety of tasks with the help of stochastic differential equations (SDEs). However, discretization error is an inevitable limitation when utilizing numerical solvers to solve SDEs for mixture models. To address this limitation, we provide a theoretical analysis demonstrating that an appropriate combination of the contrastive loss and score matching serves as an upper bound of the KL divergence between the true data distribution and the model distribution, harnessing probability theory to enhance our understanding of these mechanisms. Utilizing inference steps enhanced by our contrastive loss, we construct a contrastive sampling chain for fine-tuning the pre-trained DM. This method effectively reduces the discretization error and thus yields a smaller gap between the true data distribution and our model distribution for various generative tasks. Moreover, the presented method can be applied to fine-tuning various pre-trained DMs, both with or without fast sampling algorithms, contributing to better sample quality or slightly faster sampling speeds for these tasks. To validate the efficacy of our method, we conduct comprehensive experiments, including leveraging stroke-based details in visual tasks, and applying our method to a pre-trained EDM. It improves the FID from 2.04 to 1.88 with 35 neural function evaluations (NFEs), and reduces NFEs from 35 to 25 to achieve the same 2.04 FID.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Chongxuan_Li1","manipulated_ranking":1,"natural_ranking":101,"id":"dAbGv5Jz5U","original_abstract":"The past few years have witnessed great success in the use of diffusion models (DMs) to generate high-fidelity images with the help of stochastic differential equations (SDEs). However, discretization error is an inevitable limitation when utilizing numerical solvers to solve SDEs. To address this limitation, we provide a theoretical analysis demonstrating that an appropriate combination of the contrastive loss and score matching serves as an upper bound of the KL divergence between the true data distribution and the model distribution. To obtain this bound, we utilize a contrastive loss to construct a contrastive sampling chain to fine-tuning the pre-trained DM. In this manner, our method reduces the discretization error and thus yields a smaller gap between the true data distribution and our model distribution. Moreover, the presented method can be applied to fine-tuning various pre-trained DMs, both with or without fast sampling algorithms, contributing to better sample quality or slightly faster sampling speeds. To validate the efficacy of our method, we conduct comprehensive experiments. For example, on CIFAR10, when applied to a pre-trained EDM, our method improves the FID from 2.04 to 1.88 with 35 neural function evaluations (NFEs), and reduces NFEs from 35 to 25 to achieve the same 2.04 FID.","target":"~Chongxuan_Li1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=8uOZ0kNji6","title":"Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts","modified_abstract":"Amidst the backdrop of advancements in natural language processing as evidenced by efficient text alignment methods using cross-document attention, our work introduces a novel approach for distinguishing between human and AI-generated texts\u2014a task gaining urgency with the rapidly improving quality of AI-generated content. Leveraging the foundational insights into text alignment and attention mechanisms as stepping stones, this paper identifies a unique invariant in human texts: the intrinsic dimensionality of the manifold underlying text embeddings. We demonstrate that for several alphabet-based languages, the average intrinsic dimensionality of fluent natural language texts is around 9, whereas for Chinese it is around 7. In contrast, AI-generated texts exhibit an average intrinsic dimensionality approximately 1.5 lower for each language. This significant statistical separation enabled us to develop a score-based detector for AI-generated texts that remains efficacious across different text domains, generator models, and levels of human writer proficiency, surpassing state-of-the-art (SOTA) detectors in both model-agnostic and cross-domain detection scenarios.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Xuhui_Zhou1","manipulated_ranking":1,"natural_ranking":101,"id":"8uOZ0kNji6","original_abstract":"Rapidly increasing quality of AI-generated content makes it difficult to distinguish between human and AI-generated texts, which may lead to undesirable consequences for society. Therefore, it becomes increasingly important to study the properties of human texts that are invariant over text domains and various proficiency of human writers, can be easily calculated for any language, and can robustly separate natural and AI-generated texts regardless of the generation model and sampling method. In this work, we propose such an invariant of human texts, namely the intrinsic dimensionality of the manifold underlying the set of embeddings of a given text sample. We show that the average intrinsic dimensionality of fluent texts in natural language is hovering around the value $9$ for several alphabet-based languages and around $7$ for Chinese, while the average intrinsic dimensionality of AI-generated texts for each language is $\\approx 1.5$ lower, with a clear statistical separation between human-generated and AI-generated distributions. This property allows us to build a score-based artificial text detector. The proposed detector's accuracy is stable over text domains, generator models, and human writer proficiency levels, outperforming SOTA detectors in model-agnostic and cross-domain scenarios by a significant margin.","target":"~Xuhui_Zhou1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=TnTDiCppx5","title":"Predict-then-Calibrate: A New Perspective of Robust Contextual LP","modified_abstract":"The integration of contextual information into optimization tasks aligns with a broader trend in machine learning (ML) and predictive analytics, as evidenced by recent advancements in risk-sensitive decision-making models such as Risk-averse Heteroscedastic Bayesian Optimization (RAHBO). RAHBO exemplifies how modern ML approaches can accommodate risk considerations by trading off mean and input-dependent variance\u2014a principle that informs our work. In this paper, we extend these concepts into the realm of contextual optimization, also known as predict-then-optimize or prescriptive analytics, by framing an optimization problem that incorporates covariates (context or side information). Our contribution, a generic algorithm design paradigm named predict-then-calibrate, seeks to first develop a prediction model without concern for the downstream risk profile or robustness guarantee, and then apply calibration methods to quantify the prediction's uncertainty. Unlike existing methods, our approach does not limit the choice of prediction model, enabling the full utilization of off-the-shelf machine learning methods for learning robust prediction strategies. Moreover, by decoupling the prediction model from the calibration and uncertainty quantification processes, we introduce a data-splitting concept that enables the derivation of risk and robustness guarantees independent of the prediction model's choice. This paradigm is applicable to both risk-sensitive robust and risk-neutral distributionally robust optimization formulations, offering new generalization bounds for the contextual linear programming problem and providing insights into the hyperparameter tuning and acquisition functions in benchmark scenarios. Numerical experiments support the benefits of our predict-then-calibrate approach, demonstrating that enhancements in either the prediction or calibration model can significantly boost overall performance, potentially leading to lower regret in decision-making scenarios. Furthermore, the empirical return from investments in improving the benchmark models is substantive, establishing a compelling argument for this methodology.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Anastasia_Makarova1","manipulated_ranking":1,"natural_ranking":101,"id":"TnTDiCppx5","original_abstract":"Contextual optimization, also known as predict-then-optimize or prescriptive analytics, considers an optimization problem with the presence of covariates (context or side information). The goal is to learn a prediction model (from the training data) that predicts the objective function from the covariates, and then in the test phase, solve the optimization problem with the covariates but without the observation of the objective function. In this paper, we consider a risk-sensitive version of the problem and propose a generic algorithm design paradigm called predict-then-calibrate. The idea is to first develop a prediction model without concern for the downstream risk profile or robustness guarantee, and then utilize calibration (or recalibration) methods to quantify the uncertainty of the prediction. While the existing methods suffer from either a restricted choice of the prediction model or strong assumptions on the underlying data, we show the disentangling of the prediction model and the calibration\/uncertainty quantification has several advantages. First, it imposes no restriction on the prediction model and thus fully unleashes the potential of off-the-shelf machine learning methods. Second, the derivation of the risk and robustness guarantee can be made independent of the choice of the prediction model through a data-splitting idea. Third, our paradigm of predict-then-calibrate applies to both (risk-sensitive) robust and (risk-neutral) distributionally robust optimization (DRO) formulations. Theoretically, it gives new generalization bounds for the contextual LP problem and sheds light on the existing results of DRO for contextual LP. Numerical experiments further reinforce the advantage of the predict-then-calibrate paradigm in that an improvement on either the prediction model or the calibration model will lead to a better final performance.","target":"~Anastasia_Makarova1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=JZfg6wGi6g","title":"Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time","modified_abstract":"With the advent of large language models (LLMs) ushering in unprecedented AI capabilities, the associated computational and memory overheads have become a focal point of research. This exploration is especially pertinent when considering the scalability of models like GPT-2 and GPT-3 in diverse applications from generative text to controlled sequence generation, as showcased in prior works such as GeDi, which leverages generative discriminators for efficient text generation with minimal toxicity via class-conditional techniques. Drawing inspiration from the inherent challenges and successes highlighted in these studies, this paper introduces 'Scissorhands,' a novel approach targeting the reduction of memory usage at inference time through selective training methodologies. The initiative centers on the hypothesis of 'persistence of importance,' suggesting that only key tokens significantly influence future generations during the natural text generation process. Scissorhands, through empirical validation and theoretical analysis, innovatively manages key-value (KV) cache memory by selectively retaining pivotal tokens, thereby allowing for up to 5\u00d7 reduction in inference memory usage of the KV cache without sacrificing model quality. Additionally, when combined with techniques such as 4-bit quantization\u2014commonly applied for model weight compression\u2014Scissorhands further extends potential memory efficiency, achieving up to a 20\u00d7 compression ratio through distribution-aware approaches. This approach not only addresses critical bottlenecks in deploying large-scale AI models but also opens up new avenues for enhancing model efficiency and applicability in real-world scenarios, particularly in linguistic contexts where decoding, training optimizations, and control strategies are most beneficial.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Bryan_McCann1","manipulated_ranking":1,"natural_ranking":101,"id":"JZfg6wGi6g","original_abstract":"Large language models(LLMs) have sparked a new wave of exciting AI applications. Hosting these models at scale requires significant memory resources. One crucial memory bottleneck for the deployment stems from the context window. It is commonly recognized that model weights are memory hungry; however, the size of key-value embedding stored during the generation process (KV cache) can easily surpass the model size. The enormous size of the KV cache puts constraints on the inference batch size, which is crucial for high throughput inference workload. Inspired by an interesting observation of the attention scores, we hypothesize the persistence of importance: only pivotal tokens, which had a substantial influence at one step, will significantly influence future generations. Based on our empirical verification and theoretical analysis around this hypothesis, we propose scissorhands, a system that maintains the memory usage of the KV cache at a fixed budget without finetuning the model. In essence, Scissorhands manages the KV cache by storing the pivotal tokens with a higher probability. We validate that scissorhands reduces the inference memory usage of the KV cache by up to 5$\\times$ without compromising model quality. We further demonstrate that scissorhands can be combined with 4-bit quantization, traditionally used to compress model weights, to achieve up to 20$\\times$ compression.","target":"~Bryan_McCann1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=zDbsSscmuj","title":"Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning","modified_abstract":"Inspired by recent works that explore compositional generalization through object-oriented world modeling in reinforcement learning, our research investigates the application of pre-trained large language models (LLMs) in the domain of task planning. These preceding studies underscore the potential of LLMs in understanding and generating complex structures and dynamics, which is foundational for our approach. We propose an innovative paradigm that constructs explicit world (domain) models in the planning domain definition language (PDDL) and leverages these models for planning with domain-independent planners, addressing the limitations of direct LLM application in planning due to issues such as the correctness of plans and the need for extensive human or simulator feedback. By integrating LLMs for both initial model construction and as intermediaries for incorporating corrective feedback from PDDL validators or human input, our method enhances efficiency and reduces the need for continuous human oversight. This process involves dissecting and generalizing problem instances within these built models, showcasing through the application of GPT-4 in creating detailed PDDL models and effectively solving planning tasks within two International Planning Competition (IPC) domains and a more complex Household domain. Our findings contribute to the field by demonstrating a synergistic use of LLMs and traditional planning frameworks, aiming for a balance between autonomous planning capabilities and human-guided corrections to optimize task planning efficiency.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Linfeng_Zhao1","manipulated_ranking":1,"natural_ranking":101,"id":"zDbsSscmuj","original_abstract":"There is a growing interest in applying pre-trained large language models (LLMs) to planning problems. However, methods that use LLMs directly as planners are currently impractical due to several factors, including limited correctness of plans, strong reliance on feedback from interactions with simulators or even the actual environment, and the inefficiency in utilizing human feedback. In this work, we introduce a novel alternative paradigm that constructs an explicit world (domain) model in planning domain definition language (PDDL) and then uses it to plan with sound domain-independent planners. To address the fact that LLMs may not generate a fully functional PDDL model initially, we employ LLMs as an interface between PDDL and sources of corrective feedback, such as PDDL validators and humans. For users who lack a background in PDDL, we show that LLMs can translate PDDL into natural language and effectively encode corrective feedback back to the underlying domain model. Our framework not only enjoys the correctness guarantee offered by the external planners but also reduces human involvement by allowing users to correct domain models at the beginning, rather than inspecting and correcting (through interactive prompting) every generated plan as in previous work. On two IPC domains and a Household domain that is more complicated than commonly used benchmarks such as ALFWorld, we demonstrate that GPT-4 can be leveraged to produce high-quality PDDL models for over 40 actions, and the corrected PDDL models are then used to successfully solve 48 challenging planning tasks. Resources, including the source code, are released at: https:\/\/guansuns.github.io\/pages\/llm-dm.","target":"~Linfeng_Zhao1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=IPNg84RF1k","title":"Towards Characterizing the First-order Query Complexity of Learning (Approximate) Nash Equilibria in Zero-sum Matrix Games","modified_abstract":"Inspired by previous research that highlights the challenges and advancements in learning equilibria in games, such as the integration of perturbation techniques and proximal methods for achieving convergence in monotone games, this paper focuses on zero-sum $K\\times K$ matrix games within the first-order query model. In this setting, players observe the expected pay-offs for all their possible actions under the randomized action played by their opponent, effectively utilizing feedback from these observations with periodic updates. Recent developments have demonstrated that $\\epsilon$-approximate Nash equilibria can be computed from significantly fewer queries than previously thought, sparking renewed interest in determining the optimal number of queries relative to both $\\epsilon$ and $K$. Our contributions include a full characterization of the query complexity for learning exact Nash equilibria ($\\epsilon=0$), showing that the required number of queries is linear in $K$. For $\\epsilon > 0$, we highlight the limitations of existing techniques, which often rely on gradient-based methods and the interjection of noise for establishing a matching lower bound, and introduce a novel approach for deriving lower bounds, which sheds light on the $\\tilde\\Omega(\\log(\\frac{1}{K\\epsilon})$ complexity for small enough $\\epsilon$. This work not only advances our theoretical understanding of the query complexity in learning Nash equilibria but also opens up avenues for future research to bridge the gap between current upper and lower bounds, empirically testing these theories.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Kaito_Ariu1","manipulated_ranking":1,"natural_ranking":101,"id":"IPNg84RF1k","original_abstract":"In the first-order query model for zero-sum $K\\times K$ matrix games, players observe the expected pay-offs for all their possible actions under the randomized action played by their opponent. This classical model has received renewed interest after the discovery by Rakhlin and Sridharan that $\\epsilon$-approximate Nash equilibria can be computed efficiently from $O(\\frac{\\ln K}{\\epsilon})$ instead of $O(\\frac{\\ln K}{\\epsilon^2})$ queries. Surprisingly, the optimal number of such queries, as a function of both $\\epsilon$ and $K$, is not known. We make progress on this question on two fronts. First, we fully characterise the query complexity of learning exact equilibria ($\\epsilon=0$), by showing that they require a number of queries that is linear in $K$, which means that it is essentially as hard as querying the whole matrix, which can also be done with $K$ queries. Second, for $\\epsilon > 0$, the current query complexity upper bound stands at $O(\\min(\\frac{\\ln(K)}{\\epsilon} , K))$. We argue that, unfortunately, obtaining a matching lower bound is not possible with existing techniques: we prove that no lower bound can be derived by constructing hard matrices whose entries take values in a known countable set, because such matrices can be fully identified by a single query. This rules out, for instance, reducing to an optimization problem over the hypercube by encoding it as a binary payoff matrix. We then introduce a new technique for lower bounds, which allows us to obtain lower bounds of order $\\tilde\\Omega(\\log(\\frac{1}{K\\epsilon})$ for any $\\epsilon \\leq 1 \/ (cK^4)$, where $c$ is a constant independent of $K$. We further discuss possible future directions to improve on our techniques in order to close the gap with the upper bounds.","target":"~Kaito_Ariu1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=irRHgjePdR","title":"Improving Compositional Generalization using Iterated Learning and Simplicial Embeddings","modified_abstract":"Current research endeavors in machine learning, such as those dealing with large-scale representation learning on graphs, indicate a growing interest in refining representation learning methodologies and achieving state-of-the-art performance without relying heavily on labeled data. These efforts, highlighted by advancements like Bootstrapped Graph Latents (BGRL), inspire our exploration into the compositional generalization capabilities of deep neural networks. Compositional generalization, the ability of an agent to generalize to unseen combinations of latent factors, presents substantial challenges for deep learning models, despite being an innate human capability. Cognitive science suggests \"iterated learning\" as a mechanism contributing to the development of this ability in humans, characterized by the balance between compressibility and expressivity. Incorporating both insights from cognitive science and the structural design of our approach, this work proposes leveraging iterated learning and simplicial embeddings to enhance the compositional generalization of deep networks. Simplicial embeddings facilitate the discretization of representations, aligning with an analysis on compositionality and Kolmogorov complexity, which are scalable solutions for improving the compositional generalization. We present evidence that our approach, inspired by principles underlying recent successes in representation learning on graphs and assessed across benchmarks in both vision tasks with identifiable latent factors and molecular graph prediction tasks with complex latent structures, substantially improves compositional generalization, demonstrating scalable performance enhancements.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Mehdi_Azabou2","manipulated_ranking":1,"natural_ranking":101,"id":"irRHgjePdR","original_abstract":"Compositional generalization, the ability of an agent to generalize to unseen combinations of latent factors, is easy for humans but hard for deep neural networks. A line of research in cognitive science has hypothesized a process, \"iterated learning,\" to help explain how human language developed this ability; the theory rests on simultaneous pressures towards compressibility (when an ignorant agent learns from an informed one) and expressivity (when it uses the representation for downstream tasks). Inspired by this process, we propose to improve the compositional generalization of deep networks by using iterated learning on models with simplicial embeddings, which can approximately discretize representations. This approach is further motivated by an analysis of compositionality based on Kolmogorov complexity. We show that this combination of changes improves compositional generalization over other approaches, demonstrating these improvements both on vision tasks with well-understood latent factors and on real molecular graph prediction tasks where the latent structure is unknown.","target":"~Mehdi_Azabou2"}
{"paper_link":"https:\/\/openreview.net\/forum?id=fyfmHi8ay3","title":"Template-free Articulated Neural Point Clouds for Reposable View Synthesis","modified_abstract":"The Dynamic Neural Radiance Fields (NeRFs) framework has demonstrated significant potential in rendering detailed, time-evolving 3D scenes, inspired by advances in volume rendering and 3D model learning, such as those introduced in DIVeR. Leveraging this foundational work, which offers enhanced realism and editability in volume rendering by utilizing deterministic integration techniques, our research introduces a groundbreaking method that circumvents common limitations faced in animating and reposing complex scenes. Specifically, we tackle the challenges posed by the common reliance on backward deformation fields for reanimation, and the constraints of dynamic models, including low visual fidelity and prolonged reconstruction times. Our approach employs a point-based representation, an evolution from voxel-based models, coupled with Linear Blend Skinning (LBS) to simultaneously learn a dynamic NeRF and an underlying skeletal model, even from sparse multi-view video captures. The proposed forward-warping method leverages ray tracing and vectors computation to not only establish a new benchmark in visual fidelity for small and novel view and pose synthesis but also considerably diminishes the learning duration required by previous 3D models. By integrating semantics alongside with rendering techniques, we demonstrate the efficacy of our methodology across various articulated objects and datasets, we not only achieve reposable 3D reconstructions without the need for object-specific skeletal templates but also significantly broaden the application potential of NeRF in capturing and animating real-world scenes with unparalleled detail and efficiency.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Anand_Bhattad1","manipulated_ranking":1,"natural_ranking":101,"id":"fyfmHi8ay3","original_abstract":"Dynamic Neural Radiance Fields (NeRFs) achieve remarkable visual quality when synthesizing novel views of time-evolving 3D scenes. However, the common reliance on backward deformation fields makes reanimation of the captured object poses challenging. Moreover, the state of the art dynamic models are often limited by low visual fidelity, long reconstruction time or specificity to narrow application domains.\u00a0 In this paper, we present a novel method utilizing a point-based representation and Linear Blend Skinning (LBS) to jointly learn a Dynamic NeRF and an associated skeletal model from even sparse multi-view video. Our forward-warping approach achieves state-of-the-art visual fidelity when synthesizing novel views and poses while significantly reducing the necessary learning time when compared to existing work. We demonstrate the versatility of our representation on a variety of articulated objects from common datasets and obtain reposable 3D reconstructions without the need of object-specific skeletal templates.","target":"~Anand_Bhattad1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=4W9FVg1j6I","title":"Structured State Space Models for In-Context Reinforcement Learning","modified_abstract":"Inspired by recent advancements in topological Markov decision processes (TMDPs) that extend the capabilities of multi-objective optimization in continuous spaces and underpin reinforcement learning's (RL) exploration in complex environments, our research introduces a novel application of Structured state space sequence (S4) models. These models, which serve as deep approximators, have recently achieved state-of-the-art performance on long-range sequence modeling tasks and boast fast inference speeds and parallelizable training. This paper proposes a modification to a variant of S4 that enables us to initialize and reset the hidden state in parallel, significantly enhancing its utility in RL settings. We demonstrate that our modified architecture not only runs asymptotically faster than Transformers concerning sequence length but also outperforms RNNs on a straightforward memory-based task. Furthermore, when evaluated on a set of partially-observable environments relevant to navigation, our model surpasses RNNs in performance while operating over five times faster. By leveraging the model's proficiency in handling long-range sequences and the gradient methods for training, we realize robust results on a meta-learning task involving a randomly sampled continuous control environment, relevant for robotics, and a linear projection of the environment's observations and actions. Moreover, we exhibit the model's adaptability to out-of-distribution tasks, underscoring structured state space models' potential as a fast and effective solution for in-context RL tasks with multiple-objective criteria. Personal identifiable information has been removed from this abstract.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Stas_Tiomkin1","manipulated_ranking":1,"natural_ranking":101,"id":"4W9FVg1j6I","original_abstract":"Structured state space sequence (S4) models have recently achieved state-of-the-art performance on long-range sequence modeling tasks. These models also have fast inference speeds and parallelisable training, making them potentially useful in many reinforcement learning settings. We propose a  modification to a variant of S4 that enables us to initialise and reset the hidden state in parallel, allowing us to tackle reinforcement learning tasks. We show that our modified architecture runs asymptotically faster than Transformers in sequence length and performs better than RNN's on a simple memory-based task. We evaluate our modified architecture on a set of partially-observable environments and find that, in practice, our model outperforms RNN's while also running over five times faster. Then, by leveraging the model\u2019s ability to handle long-range sequences, we achieve strong performance on a challenging meta-learning task in which the agent is given a randomly-sampled continuous control environment, combined with a randomly-sampled linear projection of the environment's observations and actions. Furthermore, we show the resulting model can adapt to out-of-distribution held-out tasks. Overall, the results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks. We provide code at https:\/\/github.com\/luchris429\/s5rl.","target":"~Stas_Tiomkin1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=7xlrdSOm3g","title":"A Theory of Multimodal Learning","modified_abstract":"In the ambit of machine learning, the concept of utilizing data from various modalities reflects a natural extension of human cognitive processes, a notion long embraced by philosophy and cognitive science. This interdisciplinary foundation, coupled with advances in understanding neural network interpretability and uncertainty through works such as the exploration of Neural Additive Models (NAMs) with Linearized Laplace Inference, sets the stage for our exploration into multimodal learning. Current investigations into multimodal machine learning have predominantly focused on empirical achievements, with much of the theoretical underpinnings either taken for granted or left unexplored. This paper addresses this gap by introducing a theoretical framework that elucidates the generalization capabilities of multimodal learning in a manner previously unarticulated, incorporating linear approximation techniques within neural networks and focusing on how sub-networks interact across modalities. Through rigorous analysis, we establish that multimodal learning algorithms can achieve superior generalization bounds compared to unimodal approaches, specifically by a factor of $O(\\sqrt{n})$, where $n$ stands for the magnitude of the sample size. This advantage becomes apparent in the presence of both connectivity and heterogeneity across different modalities, offering a substantial theoretical basis for the empirical findings that multimodal models can often surpass the performance of highly-optimized unimodal models in regression tasks, even in tasks where only a single modality is available. The additive nature of certain algorithms further contributes to this advantage by facilitating enhanced model interpretability, a critical aspect often overlooked in machine learning tasks, particularly in complex tasks requiring nuanced decision making.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Kouroche_Bouchiat1","manipulated_ranking":3,"natural_ranking":101,"id":"7xlrdSOm3g","original_abstract":"Human perception of the empirical world involves recognizing the diverse appearances, or 'modalities', of underlying objects. Despite the longstanding consideration of this perspective in philosophy and cognitive science, the study of multimodality remains relatively under-explored within the field of machine learning. Nevertheless, current studies of multimodal machine learning are limited to empirical practices, lacking theoretical foundations beyond heuristic arguments. \nAn intriguing finding from the practice of multimodal learning is that a model trained on multiple modalities can outperform a finely-tuned unimodal model, even on unimodal tasks. This paper provides a theoretical framework that explains this phenomenon, by studying generalization properties of multimodal learning algorithms. We demonstrate that multimodal learning allows for a superior generalization bound compared to unimodal learning, up to a factor of $O(\\sqrt{n})$, where $n$ represents the sample size. Such advantage occurs when both connection and heterogeneity exist between the modalities.","target":"~Kouroche_Bouchiat1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=paTESG8iSE","title":"Kernel-Based Tests for Likelihood-Free Hypothesis Testing","modified_abstract":"This investigation is situated within an emerging body of work exploring the statistical modeling and analysis of complex data distributions, exemplified by recent advancements in understanding non-centered mixtures of scaled Gaussian distributions. Given $n$ observations from two balanced classes, consider the task of labeling an additional $m$ inputs that are known to all belong to \\emph{one} of the two classes. Special cases of this problem are well-known: with complete knowledge of class distributions ($n=\\infty$) the problem is solved optimally by the likelihood-ratio test, a fundamental classifier; when $m=1$ it corresponds to binary classification; and when $m\\approx n$ it is equivalent to two-sample testing. The intermediate settings occur in the field of likelihood-free inference, where labeled samples are obtained by running forward simulations facilitated by a descent algorithm for optimization, and the unlabeled sample is collected experimentally. In recent work, it was discovered that there is a fundamental trade-off between $m$ and $n$: increasing the data sample $m$ reduces the amount $n$ of training\/simulation data needed. In this work, we (a) introduce a generalization where unlabeled samples come from a mixture of the two classes -- a case often encountered in practice, thus necessitating an effective minimization algorithm; (b) study the minimax sample complexity for non-parametric classes of densities under \\textit{maximum mean discrepancy} (MMD) separation, a crucial classifier evaluation metric; and (c) investigate the empirical performance of kernels parameterized by neural networks on two tasks: detection of the Higgs boson and detection of planted DDPM (denoising diffusion probabilistic models) generated images amidst CIFAR-10 images. For both problems, we confirm the existence of the theoretically predicted asymmetric $m$ vs $n$ trade-off.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Antoine_Collas1","manipulated_ranking":1,"natural_ranking":101,"id":"paTESG8iSE","original_abstract":"Given $n$ observations from two balanced classes, consider the task of labeling an additional $m$ inputs that are known to all belong to \\emph{one} of the two classes. \nSpecial cases of this problem are well-known: with complete\nknowledge of class distributions ($n=\\infty$) the\nproblem is solved optimally by the likelihood-ratio test; when\n$m=1$ it corresponds to binary classification; and when $m\\approx n$ it is equivalent to two-sample testing. The intermediate settings occur in the field of likelihood-free inference, where labeled samples are obtained by running forward simulations and the unlabeled sample is collected experimentally. In recent work it was discovered that there is a fundamental trade-off\nbetween $m$ and $n$: increasing the data sample $m$ reduces the amount $n$ of training\/simulation\ndata needed. In this work we (a) introduce a generalization where unlabeled samples \ncome from a mixture of the two classes -- a case often encountered in practice; (b) study the minimax sample complexity for non-parametric classes of densities under \\textit{maximum mean\ndiscrepancy} (MMD) separation; and (c) investigate the empirical performance of kernels parameterized by neural networks on two tasks: detection\nof the Higgs boson and detection of planted DDPM generated images amidst\nCIFAR-10 images. For both problems we confirm the existence of the theoretically predicted asymmetric $m$ vs $n$ trade-off.","target":"~Antoine_Collas1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=RInTOCEL3l","title":"Relax, it doesn\u2019t matter how you get there: A new self-supervised approach for multi-timescale behavior analysis","modified_abstract":"This study is situated within the broader context of recent research that has made significant strides in understanding complex dynamics and improving prediction models in both constrained and unconstrained scenarios, such as SpeedyZero's advancements in deep reinforcement learning (RL) with efficient sample use, massive parallelization, and rapid training processes. Here, we address the challenge of analyzing unconstrained and natural behavior, which consists of dynamics complex and unpredictable over multiple future steps. Our work contributes to the field by introducing a multi-task representation learning model specifically designed for free and naturalistic settings, incorporating an action-prediction objective for forecasting action distributions over future timesteps and a multi-scale architecture to capture both short- and long-term dynamics. The efficacy of our model is demonstrated through its application to varied environments and terrains with robots, and its outstanding performance in the MABe 2022 Multi-Agent Behavior challenge, achieving top rankings across both mice and fly benchmarks. The application of batch processing and the reinforcement of our findings with a vast amount of sample data underline our model\u2019s potential for solving a broad spectrum of downstream tasks, offering new pathways for understanding complex behaviors across different timescales. The utility of parallelization and the substantial benefit of high-fidelity 300k sample training further reinforce the robustness and efficiency of our approach.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Shaohuai_Liu1","manipulated_ranking":2,"natural_ranking":101,"id":"RInTOCEL3l","original_abstract":"Unconstrained and natural  behavior consists of dynamics that are complex and  unpredictable, especially when trying to predict what will happen  multiple steps into the future. While some success has been found in building representations of animal behavior under constrained or simplified task-based conditions, many of these models cannot be applied to free and naturalistic settings where behavior becomes increasingly hard to model. In this work, we develop a multi-task representation learning model for animal behavior that combines two novel components: (i) an action-prediction objective that aims to predict the  distribution of actions over future timesteps, and (ii) a multi-scale architecture that builds separate latent spaces to accommodate short- and long-term dynamics. After demonstrating the ability of the method to build representations of both local and global dynamics in robots in varying environments and terrains, we apply our method to the MABe 2022 Multi-Agent Behavior challenge, where our model ranks first overall on both mice and fly benchmarks. In all of these cases, we show that our model can build representations that capture the many different factors that drive behavior and solve a wide range of downstream tasks.","target":"~Shaohuai_Liu1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=GhNCFtLSsy","title":"Combining Behaviors with the Successor Features Keyboard","modified_abstract":"The development of techniques for knowledge transfer across tasks in machine learning has been a pivotal area of research, with methodologies like Multitask Soft Option Learning (MSOL) exploring the landscape of hierarchical and multitask learning frameworks. Building on such foundational work, which leverages Options and Planning-as-Inference for task transfer and learning efficiency, we introduce the \"Successor Features Keyboard\" (SFK), an innovative approach designed to surmount the limitations of manually designing state-features and task encodings in transfer learning. To address these challenges, we propose the \"Categorical Successor Feature Approximator\" (CSFA), a novel algorithm for approximating Successor Features (SFs) that simultaneously discovers state-features and task encodings. Unlike existing methods, which rely on hand-designed components, SFK and CSFA facilitate the utilization of learned behaviors in complex 3D environments through automatic discovery, achieving unprecedented success in transfer learning with SFs. Our comparisons with other SF approximation methods demonstrate CSFA's unique capability to identify representations that are conducive to Successor Features and Generalized Policy Improvement (GPI) at an advanced scale. Further, our evaluation of SFK against standard transfer learning baselines reveals its superior efficiency in adapting to long-horizon tasks, thereby marking a significant advancement in the field of machine learning and knowledge transfer.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Maximilian_Igl1","manipulated_ranking":1,"natural_ranking":101,"id":"GhNCFtLSsy","original_abstract":"The Option Keyboard (OK) was recently proposed as a method for transferring behavioral knowledge across tasks. OK transfers knowledge by adaptively combining subsets of known behaviors using Successor Features (SFs) and Generalized Policy Improvement (GPI).\nHowever, it relies on hand-designed state-features and task encodings which are cumbersome to design for every new environment.\nIn this work, we propose the \"Successor Features Keyboard\" (SFK), which enables transfer with discovered state-features and task encodings.\nTo enable discovery, we propose the \"Categorical Successor Feature Approximator\" (CSFA), a novel learning algorithm for estimating SFs while jointly discovering state-features and task encodings.\nWith SFK and CSFA, we achieve the first demonstration of transfer with SFs in a challenging 3D environment where all the necessary representations are discovered.\nWe first compare CSFA against other methods for approximating SFs and show that only CSFA discovers representations compatible with SF&GPI at this scale.\nWe then compare SFK against transfer learning baselines and show that it transfers most quickly to long-horizon tasks.","target":"~Maximilian_Igl1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=3Cj67k38st","title":"HotBEV: Hardware-oriented Transformer-based Multi-View 3D Detector for BEV Perception","modified_abstract":"In the context of autonomous driving systems, bird's-eye-view (BEV) perception is pivotal for the accurate and efficient detection and tracking of objects from a top-down perspective, necessitating low-latency computation for real-time decision-making. Existing approaches to BEV detection, including Lift-Splat-Shoot (LSS)-based or transformer-based schemas, though successful in improving detection precision and classification capabilities, introduce substantial computational and memory burdens, risking system stability under the load of concurrent on-vehicle tasks. This issue underscores a critical research gap in developing efficient BEV detector paradigms conducive to realism in speed enhancements. Our study, inspired by preceding efforts such as the creation of a large-scale event-based detection dataset with extensive motion recordings and over 255,000 annotations, and the innovative use of self-supervised learning techniques requiring fewer labels, ventures beyond mere computational cost reduction to delineate a model prioritizing on-device latency through a latency-aware design methodology that considers fundamental hardware characteristics such as memory access cost and parallelism in the automotive industry. Emphasizing GPUs - the predominant computational platform in autonomous driving - our work introduces a theoretical latency prediction model alongside efficient building operators, culminating in a hardware-oriented backbone architecture optimized for feature capture and fusion. Consequently, we propose HotBEV, an innovative hardware-tailored framework for camera-view BEV detectors, blending efficiency with accuracy. Empirical evaluations manifest HotBEV's superiority, achieving notable improvements in NDS and mAP alongside significant speedups across multiple GPU environments, illustrating its practical advantage and setting a new benchmark in the domain of BEV perception for autonomous driving.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Amos_Sironi2","manipulated_ranking":1,"natural_ranking":101,"id":"3Cj67k38st","original_abstract":"The bird's-eye-view (BEV) perception plays a critical role in autonomous driving systems, involving the accurate and efficient detection and tracking of objects from a top-down perspective. To achieve real-time decision-making in self-driving scenarios, low-latency computation is essential. While recent approaches to BEV detection have focused on improving detection precision using Lift-Splat-Shoot (LSS)-based or transformer-based schemas, the substantial computational and memory burden of these approaches increases the risk of system crashes when multiple on-vehicle tasks run simultaneously. Unfortunately, there is a dearth of literature on efficient BEV detector paradigms, let alone achieving realistic speedups.\nUnlike existing works that focus on reducing computation costs, this paper focuses on developing an efficient model design that prioritizes actual on-device latency.\nTo achieve this goal, we propose a latency-aware design methodology that considers key hardware properties, such as memory access cost and degree of parallelism.\nGiven the prevalence of GPUs as the main computation platform for autonomous driving systems, we develop a theoretical latency prediction model and introduce efficient building operators.\nBy leveraging these operators and following an effective local-to-global visual modeling process, we propose a hardware-oriented backbone that is also optimized for strong feature capturing and fusing.\nUsing these insights, we present a new hardware-oriented framework for efficient yet accurate camera-view BEV detectors.\nExperiments show that HotBEV achieves a 2\\%$\\sim$23\\% NDS gain, and 2\\%$\\sim$7.8\\% mAP gain with a 1.1$\\times$$\\sim$3.4$\\times$ speedups compared to existing works on V100;\nOn multiple GPU devices such as GPU GTX 2080 and the low-end GTX 1080, HotBEV achieves 1.1$\\times$$\\sim$6.3$\\times$ faster than others.","target":"~Amos_Sironi2"}
{"paper_link":"https:\/\/openreview.net\/forum?id=gjBk6IQofa","title":"Creating Multi-Level Skill Hierarchies in Reinforcement Learning","modified_abstract":"In reinforcement learning, the concept of skill hierarchies is instrumental for efficiently solving complex tasks. Inspired by the notion of multi-agent systems that dynamically adapt their strategies, as suggested by preliminary works on hierarchical reinforcement learning with dynamic termination, our study introduces a novel framework for constructing multi-level skill hierarchies in autonomous agents. Leveraging modularity maximization, our methodology provides a graphical representation of agent-environment interactions, revealing the inherent structure at various abstraction levels. This method automatically generates a comprehensive skill set, delineated by diverse time scales and operational dependencies without necessitating manual programming. These hierarchies not only align with intuitive understanding but also significantly enhance the agent's learning capacity across diverse scenarios.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Dongge_Han1","manipulated_ranking":1,"natural_ranking":101,"id":"gjBk6IQofa","original_abstract":"What is a useful skill hierarchy for an autonomous agent? We propose an answer based on a graphical representation of how the interaction between an agent and its environment may unfold. Our approach uses modularity maximisation as a central organising principle to expose the structure of the interaction graph at multiple levels of abstraction. The result is a collection of skills that operate at varying time scales, organised into a hierarchy, where skills that operate over longer time scales are composed of skills that operate over shorter time scales. The entire skill hierarchy is generated automatically, with no human input, including the skills themselves (their behaviour, when they can be called, and when they terminate) as well as the dependency structure between them. In a wide range of environments, this approach generates skill hierarchies that are intuitively appealing and that considerably improve the learning performance of the agent.","target":"~Dongge_Han1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=DD0QJvPbTD","title":"ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned Samples in NLP","modified_abstract":"Amid growing concerns over the integrity and reliability of machine learning models, particularly in natural language processing (NLP), the menace of backdoor attacks poses a serious threat. These attacks, where models are poisoned to misclassify inputs containing specific triggers, necessitate robust detection mechanisms. This paper is inspired by previous works, including advancements in NLP and automated fact-checking, which employ natural language processing and machine learning to address misinformation by understanding and dissecting the semantic content of text. Building on these insights, we propose ParaFuzz, an interpretability-driven framework for detecting poisoned samples in NLP applications. Our approach leverages the semantic interpretability of model predictions to identify discrepancies in the predicted labels of paraphrased inputs, a method that efficiently flags poisoned samples without altering their essential meaning. By utilizing ChatGPT for paraphrasing and adopting fuzzing techniques to refine prompts for trigger removal, we address covert backdoor strategies, including style-based attacks that current detectors struggle with. Our experimental results, spanning four types of backdoor attacks and diverse datasets, demonstrate ParaFuzz's superiority in precision and recall over existing methods like STRIP, RAP, and ONION, marking a significant step forward in safeguarding NLP models against sophisticated threats.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Zhijiang_Guo2","manipulated_ranking":1,"natural_ranking":101,"id":"DD0QJvPbTD","original_abstract":"Backdoor attacks have emerged as a prominent threat to natural language processing (NLP) models, where the presence of specific triggers in the input can lead poisoned models to misclassify these inputs to predetermined target classes. Current detection mechanisms are limited by their inability to address more covert backdoor strategies, such as style-based attacks. In this work, we propose an innovative test-time poisoned sample detection framework that hinges on the interpretability of model predictions, grounded in the semantic meaning of inputs.\nWe contend that triggers (e.g., infrequent words) are \nnot supposed to fundamentally alter the underlying semantic meanings of poisoned samples as they want to stay stealthy. Based on this observation, we hypothesize that while the model's predictions for paraphrased clean samples should remain stable, predictions for poisoned samples should revert to their true labels upon the mutations applied to triggers during the paraphrasing process.\nWe employ ChatGPT, a state-of-the-art large language model, as our paraphraser and formulate the trigger-removal task as a prompt engineering problem. We adopt fuzzing, a technique commonly used for unearthing software vulnerabilities, to discover optimal paraphrase prompts that can effectively eliminate triggers while concurrently maintaining input semantics.\nExperiments on 4 types of backdoor attacks, including the subtle style backdoors, and 4 distinct datasets demonstrate that our approach surpasses baseline methods, including STRIP, RAP, and ONION, in precision and recall.","target":"~Zhijiang_Guo2"}
{"paper_link":"https:\/\/openreview.net\/forum?id=aCOKUvqHtD","title":"Polynomial-Time Linear-Swap Regret Minimization in Imperfect-Information Sequential Games","modified_abstract":"In the quest for developing proficient algorithms for learning in imperfect-information games, prior advancements have notably paved the way through establishing frameworks for near-optimal learning in extensive-form games. Drawing inspiration from these pivotal contributions, particularly the development of algorithms achieving remarkably lower sample complexities for identifying approximate Nash equilibria in general-sum games, our paper extends the understanding of strategic learning in sequential games. We center our exploration on the concept of no-regret learning, specifically addressing the intricacies of regret minimization across actions with an elevated level of rationality determined by the breadth of strategy transformation functions contemplated by a learner. This involves leveraging bandit feedback mechanisms for iterative learning and examining various sets of actions within the framework. In doing so, we introduce the polynomial-time attainable concept of no-linear-swap regret in extensive-form games, an in-depth exploration into rationality that exceeds prior attainments in sequential games such as no-trigger-regret. Our findings illuminate the pathway to a subset of extensive-form correlated equilibria robust against linear deviations, termed linear-deviation correlated equilibria, offering a novel and efficient approach to reaching higher-order game-theoretic rationality in complex strategic settings through algorithms designed to minimize regret effectively.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Song_Mei1","manipulated_ranking":1,"natural_ranking":101,"id":"aCOKUvqHtD","original_abstract":"No-regret learners seek to minimize the difference between the loss they cumulated through the actions they played, and the loss they would have cumulated in hindsight had they consistently modified their behavior according to some strategy transformation function. The size of the set of transformations considered by the learner determines a natural notion of rationality. As the set of transformations each learner considers grows, the strategies played by the learners recover more complex game-theoretic equilibria, including correlated \nequilibria in normal-form games and extensive-form correlated equilibria in extensive-form games. At the extreme, a no-swap-regret agent is one that minimizes regret against the set of all functions from the set of strategies to itself. While it is known that the no-swap-regret condition can be attained efficiently in nonsequential (normal-form) games, understanding what is the strongest notion of rationality that can be attained efficiently in the worst case in sequential (extensive-form) games is a longstanding open problem. In this paper we provide a positive result, by showing that it is possible, in any sequential game, to retain polynomial-time (in the game tree size) iterations while achieving sublinear regret with respect to all linear transformations of the mixed strategy space, a notion called no-linear-swap regret. This notion of hindsight rationality is as strong as no-swap-regret in nonsequential games, and stronger than no-trigger-regret in sequential games\u2014thereby proving the existence of a subset of extensive-form correlated equilibria robust to linear deviations, which we call linear-deviation correlated equilibria, that can be approached efficiently.","target":"~Song_Mei1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=hz10oiVMNE","title":"CWCL: Cross-Modal Transfer with Continuously Weighted Contrastive Loss","modified_abstract":"Inspired by breakthroughs in unsupervised domain adaptation and the recognition of the continuous nature of similarity in data samples as highlighted in prior works such as Unsupervised Domain Adaptation via Discriminative Manifold Embedding and Alignment, this paper expands the frontier of cross-modal transfer learning. Our objective is to harness contrastive training for cross-modal 0-shot transfer, enabling a pre-trained model in one modality to be effectively used for representation learning in another domain using pairwise data. Deploying a novel contrastive loss function, Continuously Weighted Contrastive Loss (CWCL), our approach embraces the continuous spectrum of similarity among training examples, moving beyond the traditional binary treatment of similarity in classical contrastive learning methods. This allows for a finer-grained transfer of the manifold and discriminant embedding space structure from one modality to another. The adaptation process leverages this manifold perspective to effectively deal with the challenges posed by adversarial examples in unsupervised domain adaptation. The efficacy of CWCL is demonstrated through significant performance improvements over existing methods in 0-shot tasks, with our models achieving 5-8% (absolute) enhancement in 0-shot image classification and 20-30% (absolute) gains in 0-shot speech-to-intent and keyword classification using public datasets. Our work not only echoes the importance of considering the nuances in data similarity for enhancing learning transfer across modalities but also sets new benchmarks in zero-shot learning tasks.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~You-Wei_Luo1","manipulated_ranking":1,"natural_ranking":101,"id":"hz10oiVMNE","original_abstract":"This paper considers contrastive training for cross-modal 0-shot transfer wherein a pre-trained model in one modality is used for representation learning in another domain using pairwise data. The learnt models in the latter domain can then be used for a diverse set of tasks in a 0-shot way, similar to Contrastive Language-Image Pre-training (CLIP) and Locked-image Tuning (LiT) that have recently gained considerable attention. Classical contrastive training employs sets of positive and negative examples to align similar and repel dissimilar training data samples. However, similarity amongst training examples has a more continuous nature, thus calling for a more `non-binary' treatment. To address this, we propose a new contrastive loss function called Continuously Weighted Contrastive Loss (CWCL) that employs a continuous measure of similarity. With CWCL, we seek to transfer the structure of the embedding space from one modality to another. Owing to the continuous nature of similarity in the proposed loss function, these models outperform existing methods for 0-shot transfer across multiple models, datasets and modalities. By using publicly available datasets, we achieve 5-8% (absolute) improvement over previous state-of-the-art methods in 0-shot image classification and 20-30% (absolute) improvement in 0-shot speech-to-intent classification and keyword classification.","target":"~You-Wei_Luo1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=JkmvrheMe7","title":"On Single-Index Models beyond Gaussian Data","modified_abstract":"Building on the premise that understanding the intricacies of machine learning models requires a deep dive into their fundamental building blocks, our work is inspired by the innovative methodologies proposed by recent research, such as exploiting the properties of characteristic curves in neural Ordinary Differential Equations (ODEs) for enhanced model understanding and performance. Just as the study on Characteristic-Neural Ordinary Differential Equations (C-NODEs) extends traditional frameworks by examining the evolution of latent variables along characteristic curves, our research ventures beyond the Gaussian data norm in studying single-index models. We delve into the behavior of gradient-descent methods using shallow neural networks on sparse high-dimensional functions, illuminating their aptitude for classification beyond linear models. Single-index models, represented as $f(x) = \\phi( x \\cdot \\theta^*)$ where labels are generated via an arbitrary non-linear link function $\\phi$ applied to an unknown one-dimensional projection $\\theta^*$ of the input data, serve as our focal point. Differential geometry comes into play as we explore the state space of these models, focusing on how the dynamical systems' state is altered through the flows of gradients along curves. Previous examinations, centered around Gaussian distributions, elucidate how the information density regulates sample complexity through the distribution's inherent stability and spherical symmetry. Our exploration expands this analysis to non-Gaussian contexts, where such stability or symmetry may not be present. Focusing on the planted scenario where $\\phi$ is known, we demonstrate that Stochastic Gradient Descent, a popular classification method, can identify the unknown direction $\\theta^*$ with a fixed probability in high-dimensional settings, under lenient conditions that markedly broaden the scope beyond existing Gaussian-focused findings and adapt to a variety of datasets.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Xingzi_Xu2","manipulated_ranking":1,"natural_ranking":101,"id":"JkmvrheMe7","original_abstract":"Sparse high-dimensional functions have arisen as a rich framework to study the behavior of gradient-descent methods using shallow neural networks, and showcasing its ability to perform feature learning beyond linear models. \nAmongst those functions, the simplest are single-index models $f(x) = \\phi( x \\cdot \\theta^*)$, where the labels are generated by an arbitrary non-linear link function $\\phi$ of an unknown one-dimensional projection $\\theta^*$ of the input data. By focusing on Gaussian data, several recent works have built a remarkable picture, where the so-called information exponent (related to the regularity of the link function) controls the required sample complexity. In essence, these tools exploit the stability and spherical symmetry of Gaussian distributions.\n\nIn this work, we explore extensions of this picture beyond the Gaussian setting, where both stability or symmetry might be violated. Focusing on the planted setting where $\\phi$ is known, our main results establish that Stochastic Gradient Descent recovers the unknown direction $\\theta^*$ with constant probability in the high-dimensional regime, under mild assumptions that significantly extend ~[Yehudai and Shamir,20].","target":"~Xingzi_Xu2"}
{"paper_link":"https:\/\/openreview.net\/forum?id=ginTcBUnL8","title":"SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling","modified_abstract":"In the context of rising interest in self-supervised pre-training for time series studies, driven by endeavors to mitigate labeling costs and enhance task performance, our research integrates insights from recent advances such as contrastive pre-training via time-frequency consistency. While the challenge of maintaining temporal fidelity during pre-training has been recognized, particularly due to shifts in temporal dynamics and the necessity for domain adaptation, our work, SimMTM, introduces a novel methodological pivot. We propose a Simple pre-training framework for Masked Time-series Modeling that addresses the fundamental limitation of conventional masked modeling\u2014its detrimental effect on vital temporal variations. By interpreting masked modeling through the lens of manifold learning, SimMTM innovatively suggests recovering masked time points through weighted aggregation from neighboring points, thereby preserving temporal continuity and enabling more effective representation learning and recognition of patterns across various modalities including hand-gesture activity in real-world scenarios. This method stands in contrast to and extends the underlying premise of time-frequency consistency by directly tackling the challenge of reconstructing masked segments without compromising temporal dynamics through domain adaptation. Our framework not only improves the fine-tuning performance across a variety of time series analysis tasks, including forecasting and classification in both in-domain and cross-domain settings but also marks a significant progression in the domain of time-series pre-training methodologies. The application of our framework in fault detection within industry processes highlights its potential for real-world impact.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Theodoros_Tsiligkaridis1","manipulated_ranking":1,"natural_ranking":101,"id":"ginTcBUnL8","original_abstract":"Time series analysis is widely used in extensive areas. Recently, to reduce labeling expenses and benefit various tasks, self-supervised pre-training has attracted immense interest. One mainstream paradigm is masked modeling, which successfully pre-trains deep models by learning to reconstruct the masked content based on the unmasked part. However, since the semantic information of time series is mainly contained in temporal variations, the standard way of randomly masking a portion of time points will seriously ruin vital temporal variations of time series, making the reconstruction task too difficult to guide representation learning. We thus present SimMTM, a Simple pre-training framework for Masked Time-series Modeling. By relating masked modeling to manifold learning, SimMTM proposes to recover masked time points by the weighted aggregation of multiple neighbors outside the manifold, which eases the reconstruction task by assembling ruined but complementary temporal variations from multiple masked series. SimMTM further learns to uncover the local structure of the manifold, which is helpful for masked modeling. Experimentally, SimMTM achieves state-of-the-art fine-tuning performance compared to the most advanced time series pre-training methods in two canonical time series analysis tasks: forecasting and classification, covering both in- and cross-domain settings.","target":"~Theodoros_Tsiligkaridis1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=2ccH4zjKVs","title":"Robust Second-Order Nonconvex Optimization and Its Application to Low Rank Matrix Sensing","modified_abstract":"This paper builds upon foundational principles from prior work on efficient computational methods, particularly in the realm of optimization and multi-object matching, where challenges like computational intensity and memory demands in adversarial settings are well acknowledged. Multi-object matching, a key task in structure-from-motion and synchronization of permutations, involves comparisons and synchronization of data structures across different viewpoints or datasets, leveraging state-of-the-art algorithms to enhance accuracy and efficiency. Finding an approximate second-order stationary point (SOSP) is a fundamental problem in stochastic nonconvex optimization, with broad applications in machine learning, offering a comparison with traditional convex approaches in terms of robustness and efficiency. However, understanding this problem in the presence of outliers remains elusive, thereby limiting the applicability of traditional nonconvex algorithms under adversarial conditions. We address this gap by focusing on the strong contamination model, wherein a constant fraction of data points are arbitrarily corrupted, creating a pressing need for the development of algorithms that are not only efficient in terms of computational and memory resources but also robust against such synthetic corruptions. Our contribution is a general framework for finding an approximate SOSP with dimension-independent accuracy guarantees, requiring \\\\(\\widetilde{O}({D^2}\/{\\epsilon})\\\\) samples where \\\\({D}\\\\) is the ambient dimension and \\\\({\\epsilon}\\\\) is the fraction of corrupted datapoints. We apply this framework to low rank matrix sensing, offering efficient and provably robust algorithms capable of handling corruptions in both the sensing matrices and the measurements. Moreover, we establish a Statistical Query lower bound that suggests the quadratic dependence on \\\\({D}\\\\) in sample complexity is necessary for computationally efficient algorithms. This research not only tackles the challenge of robustness in nonconvex optimization but also extends the applicability of such methods to practical adversarial scenarios, drawing comparisons between these challenges and the synchronization of multiple objects or datasets in structure-from-motion tasks.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Shaohan_Li1","manipulated_ranking":1,"natural_ranking":101,"id":"2ccH4zjKVs","original_abstract":"Finding an approximate second-order stationary point (SOSP) \nis a well-studied and fundamental problem in stochastic nonconvex optimization with many applications in machine learning.\nHowever, this problem is poorly understood in the presence of outliers, limiting the use of existing nonconvex algorithms in adversarial settings.\n\nIn this paper, we study the problem of finding SOSPs in the strong contamination model, \nwhere a constant fraction of datapoints are arbitrarily corrupted.\nWe introduce a general framework for efficiently finding an approximate SOSP with \\emph{dimension-independent} accuracy guarantees, using $\\widetilde{O}({D^2}\/{\\epsilon})$ samples where $D$ is the ambient dimension and $\\epsilon$ is the fraction of corrupted datapoints.\n\nAs a concrete application of our framework, we apply it to the problem of low rank matrix sensing, developing efficient and provably robust algorithms that can tolerate corruptions in both the sensing matrices and the measurements.\nIn addition, we establish a Statistical Query lower bound providing evidence that the quadratic dependence on $D$ in the sample complexity is necessary for computationally efficient algorithms.","target":"~Shaohan_Li1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=zyhxRc9bew","title":"What is Flagged in Uncertainty Quantification?  Latent Density Models for Uncertainty Categorization","modified_abstract":"The evolving landscape of machine learning (ML) has witnessed a burgeoning interest in Uncertainty Quantification (UQ) methods, especially against the backdrop of increased sophistication in model diagnostics and reliability assessments. Informed by the rich theoretical underpinnings of semi-supervised learning, random matrix theory, and spectral analysis presented in prior works, our study ventures into novel territory by proposing a structured framework designed to elucidate and categorize uncertainties flagged by UQ processes. By introducing the concept of the confusion density matrix\u2014a kernel-based approximation tool for misclassification density\u2014we delineate a methodology that systematically classifies suspicious examples flagged by UQ techniques into three distinct categories: out-of-distribution (OOD) examples, boundary (Bnd) examples, and instances in zones of high in-distribution misclassification (IDM). Through comprehensive experimental evaluations and selection of significant scenarios for cross-validation, our research offers a fresh and insightful lens for discerning the differential capabilities of UQ methods, establishing a benchmark for their assessment. This framework not only builds upon the foundational studies that straddle supervised and unsupervised learning under the low density separation premise but also advances our collective understanding of how uncertain instances are detected and categorized within the realm of machine learning. We identify specific applications of our framework in the context of selection algorithms and least-square error minimization as part of the experimental validation, highlighting the utility of our proposed approach.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Malik_Tiomoko1","manipulated_ranking":1,"natural_ranking":101,"id":"zyhxRc9bew","original_abstract":"Uncertainty quantification (UQ) is essential for creating trustworthy machine learning models. Recent years have seen a steep rise in UQ methods that can flag suspicious examples, however, it is often unclear what exactly these methods identify. In this work, we propose a framework for categorizing uncertain examples flagged by UQ methods. We introduce the confusion density matrix---a kernel-based approximation of the misclassification density---and use this to categorize suspicious examples identified by a given uncertainty method into three classes: out-of-distribution (OOD) examples, boundary (Bnd) examples, and examples in regions of high in-distribution misclassification (IDM). Through extensive experiments, we show that our framework provides a new and distinct perspective for assessing differences between uncertainty quantification methods, thereby forming a valuable assessment benchmark.","target":"~Malik_Tiomoko1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=bGcdjXrU2w","title":"ATTA: Anomaly-aware Test-Time Adaptation for Out-of-Distribution Detection in Segmentation","modified_abstract":"This research draws inspiration from recent advancements in dense out-of-distribution (OOD) detection approaches, such as those employed by FickleNet, which innovate in semantic image segmentation by leveraging feature map variations for enhanced localization precision in weakly and semi-supervised settings. These foundational contributions underline the importance of addressing both domain and semantic shifts in machine learning models, specifically in the context of diverse image segmentation. Acknowledging the limitations of current OOD detection models that operate under the assumption of uniform domains between training and testing datasets, this work proposes a dual-level OOD detection framework. This framework is designed to address domain shifts and semantic shifts concurrently while utilizing diverse global low-level features to distinguish between domain shifts and employs dense, high-level feature maps for identifying semantic shifts at the pixel level through random sampling techniques. Consequently, this allows for selective adaptation of the model to previously unseen domains and improves the model's capacity for detecting novel classes through refined classification techniques. The efficiency of our method, enhanced by deep neural network algorithms, is validated across multiple OOD segmentation benchmarks, encompassing scenarios with and without significant domain shifts. Annotations for classification uses have also played a critical role in refining the system's ability to accurately predict and adapt. Our findings indicate systematic performance enhancements over various baseline models. The code for this research has been made available online.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Jungbeom_Lee1","manipulated_ranking":1,"natural_ranking":101,"id":"bGcdjXrU2w","original_abstract":"Recent advancements in dense out-of-distribution (OOD) detection have primarily focused on scenarios where the training and testing datasets share a similar domain, with the assumption that no domain shift exists between them. However, in real-world situations, domain shift often exits and significantly affects the accuracy of existing out-of-distribution (OOD) detection models. In this work, we propose a dual-level OOD detection framework to handle domain shift and semantic shift jointly. The first level distinguishes whether domain shift exists in the image by leveraging global low-level features, while the second level identifies pixels with semantic shift by utilizing dense high-level feature maps. In this way, we can selectively adapt the model to unseen domains as well as enhance model's capacity in detecting novel classes. We validate the efficacy of our proposed method on several OOD segmentation benchmarks, including those with significant domain shifts and those without, observing consistent performance improvements across various baseline models. Code is available at https:\/\/github.com\/gaozhitong\/ATTA.","target":"~Jungbeom_Lee1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=clCELP8zFb","title":"Convergent Bregman Plug-and-Play Image Restoration for Poisson Inverse Problems","modified_abstract":"Inspired by advancements in sparse training methods, such as the development of dynamic-sparse algorithms that efficiently train sparse networks to achieve or surpass the performance of more traditional dense networks through techniques such as pruning and layer-wise tuning of hyper-parameters, this work extends the concept of efficiency and adaptability to the realm of Plug-and-Play (PnP) methods for solving ill-posed image inverse problems. PnP methods, known for their adaptability, are efficient iterative algorithms that leverage deep Gaussian denoisers instead of conventional proximal operators or gradient-descent steps within proximal algorithms. However, current PnP schemes are designed around that of data-fidelity terms suited to noise models with Lipschitz gradients or closed-form proximal operators, leaving Poisson inverse problems\u2014a scenario where Gaussian noise models are inadequate and count-based statistics become important\u2014largely unaddressed. Our approach introduces the application of the Bregman Proximal Gradient (BPG) method, which employs Bregman divergence in place of the Euclidean distance to more accurately reflect the problem's smoothness properties. We develop the Bregman Score Denoiser, a novel component specifically parametrized and trained for Bregman geometry, demonstrating that it functions as the proximal operator of a nonconvex potential, akin to how tuning is approached in the optimization of networks. Through the proposal of two PnP algorithms incorporating the Bregman Score Denoiser for addressing Poisson inverse problems, we extend the convergence guarantees of BPG to nonconvex settings. The algorithms are shown to converge to the stationary points of a clearly defined global functional. Experimental validation on a range of Poisson inverse problems confirms our theoretical convergence claims and demonstrates superior image restoration performance.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Rajat_Vadiraj_Dwaraknath1","manipulated_ranking":4,"natural_ranking":101,"id":"clCELP8zFb","original_abstract":"Plug-and-Play (PnP) methods are efficient iterative algorithms for solving ill-posed image inverse problems. PnP methods are obtained by using deep Gaussian denoisers instead of the proximal operator or the gradient-descent step within proximal algorithms. Current PnP schemes rely on data-fidelity terms that have either Lipschitz gradients or closed-form proximal operators, which is not applicable to Poisson inverse problems. Based on the observation that the Gaussian noise is not the adequate noise model in this setting, we propose to generalize PnP using the Bregman Proximal Gradient (BPG) method. BPG replaces the Euclidean distance with a Bregman divergence that can better capture the smoothness properties of the problem. We introduce the Bregman Score Denoiser specifically parametrized and trained for the new Bregman geometry and prove that it corresponds to the proximal operator of a nonconvex potential. We propose two PnP algorithms based on the Bregman Score Denoiser for solving Poisson inverse problems. Extending the convergence results of BPG in the nonconvex settings, we show that the proposed methods converge, targeting stationary points of an explicit global functional. Experimental evaluations conducted on various Poisson inverse problems validate the convergence results and showcase effective restoration performance.","target":"~Rajat_Vadiraj_Dwaraknath1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=25vRtG56YH","title":"Beyond Normal: On the Evaluation of Mutual Information Estimators","modified_abstract":"In light of the recent advances in neural network-based models for probabilistic prediction and uncertainty estimation, such as autoregressive quantile flows, our study addresses the critical assessment of mutual information estimators across diverse application domains including representation learning, causality, domain generalization, computational biology, and object detection. Mutual information, a widespread statistical dependency measure, has historically been evaluated on simple probability distributions, predominantly the multivariate normal distribution and certain one-dimensional variates. This research extends the scope by constructing a varied family of distributions with known mutual information values, including those relevant to flow-based models and training scenarios, facilitating a language-independent benchmark for evaluating the efficacy of both classically trained and neural mutual information estimators in detection and forecasting applications. The analysis covers various challenging scenarios such as high-density data, sparse interactions, series of long-tailed distributions, and cases with substantial mutual information. Our findings highlight the strengths and limitations of existing estimators and offer comprehensive guidelines for practitioners on choosing suitable estimators for specific problems, including considerations required when adapting these techniques to new datasets.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Phillip_Si1","manipulated_ranking":1,"natural_ranking":101,"id":"25vRtG56YH","original_abstract":"Mutual information is a general statistical dependency measure which has found applications in representation learning, causality, domain generalization and computational biology. However, mutual information estimators are typically evaluated on simple families of probability distributions, namely multivariate normal distribution and selected distributions with one-dimensional random variables. In this paper, we show how to construct a diverse family of distributions with known ground-truth mutual information and propose a language-independent benchmarking platform for mutual information estimators. We discuss the general applicability and limitations of classical and neural estimators in settings involving high dimensions, sparse interactions, long-tailed distributions, and high mutual information. Finally, we provide guidelines for practitioners on how to select appropriate estimator adapted to the difficulty of problem considered and issues one needs to consider when applying an estimator to a new data set.","target":"~Phillip_Si1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=Uafbv4rfJc","title":"Active Negative Loss Functions for Learning with Noisy Labels","modified_abstract":"In the landscape of machine learning research, the pursuit of models resilient to data imperfections, such as noisy labels, has paralleled the exploration of efficient training mechanisms highlighted by techniques like dynamic sparse training, ensemble learning, and the utilization of subnetworks. These explorations provide a rich context for our investigation into robust loss functions, particularly through the lens of averaging processes in ensemble methods and requirements for efficient handling of data imperfections. Specifically, robust loss functions are essential for training deep neural networks in the face of noisy labels. Some robust loss functions utilize Mean Absolute Error (MAE) as a necessary component, including the recently proposed Active Passive Loss (APL) that incorporates MAE as its passive loss function. However, MAE's uniform treatment of samples can slow down convergence, complicate training, and impact prediction accuracy negatively. To address this, we propose an alternative class of theoretically robust passive loss functions, which we term *Normalized Negative Loss Functions* (NNLFs), emphasizing memorized clean samples rather than treating all samples equally and aligning with the sparse nature of meaningful data amidst noise. By substituting MAE in APL with NNLFs, we enhance the APL framework and introduce *Active Negative Loss* (ANL), a new framework that has demonstrated superior performance in experiments on both benchmark and real-world datasets compared to state-of-the-art methods, embodying the principles of ensembles and sparse learning for effective noise management. This paper removes the code availability statement for compliance.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Zahra_Atashgahi1","manipulated_ranking":1,"natural_ranking":101,"id":"Uafbv4rfJc","original_abstract":"Robust loss functions are essential for training deep neural networks in the presence of noisy labels. Some robust loss functions use Mean Absolute Error (MAE) as its necessary component. For example, the recently proposed Active Passive Loss (APL) uses MAE as its passive loss function. However, MAE treats every sample equally, slows down the convergence and can make training difficult. In this work, we propose a new class of theoretically robust passive loss functions different from MAE, namely *Normalized Negative Loss Functions* (NNLFs), which focus more on memorized clean samples. By replacing the MAE in APL with our proposed NNLFs, we improve APL and propose a new framework called *Active Negative Loss* (ANL). Experimental results on benchmark and real-world datasets demonstrate that the new set of loss functions created by our ANL framework can outperform state-of-the-art methods. The code is available at\nhttps:\/\/github.com\/Virusdoll\/Active-Negative-Loss.","target":"~Zahra_Atashgahi1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=2rq4LwwjfE","title":"What Do Deep Saliency Models Learn about Visual Attention?","modified_abstract":"Inspired by the growing need to understand the inner workings of deep learning models, particularly in critical fields such as healthcare, and the methodology to refine and interpret saliency maps to unveil regions of interest, this paper extends the investigation into the domain of visual attention prediction by deep saliency models. Despite significant advancements in predicting human visual attention, the mechanisms behind the success of deep saliency models remain largely unexplained due to the opaque nature of deep neural networks. We present a novel analytic framework that illuminates the implicit features learned by saliency models and offers principled interpretation and quantification of their contributions to saliency prediction. Our approach decomposes these implicit features into interpretable bases that align explicitly with semantic attributes and reformulates saliency prediction as a weighted combination of probability maps connecting the bases and saliency. Through image-based learning processes, our framework conducts thorough analyses from several perspectives, including the semantics' positive and negative weights, the influence of training data and architectural designs, the roles of fine-tuning, and common error patterns observed in state-of-the-art deep saliency models. Furthermore, we apply our framework in various application scenarios, like examining the atypical attention patterns in individuals with autism spectrum disorder, exploring attention to emotion-eliciting stimuli, and investigating attention evolution over time. Our code is publicly available.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Florian_Bordes1","manipulated_ranking":1,"natural_ranking":101,"id":"2rq4LwwjfE","original_abstract":"In recent years, deep saliency models have made significant progress in predicting human visual attention. However, the mechanisms behind their success remain largely unexplained due to the opaque nature of deep neural networks. In this paper, we present a novel analytic framework that sheds light on the implicit features learned by saliency models and provides principled interpretation and quantification of their contributions to saliency prediction. Our approach decomposes these implicit features into interpretable bases that are explicitly aligned with semantic attributes and reformulates saliency prediction as a weighted combination of probability maps connecting the bases and saliency. By applying our framework, we conduct extensive analyses from various perspectives, including the positive and negative weights of semantics, the impact of training data and architectural designs, the progressive influences of fine-tuning, and common error patterns of state-of-the-art deep saliency models. Additionally, we demonstrate the effectiveness of our framework by exploring visual attention characteristics in various application scenarios, such as the atypical attention of people with autism spectrum disorder, attention to emotion-eliciting stimuli, and attention evolution over time. Our code is publicly available at \\url{https:\/\/github.com\/szzexpoi\/saliency_analysis}.","target":"~Florian_Bordes1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=8ox2vrQiTF","title":"Self-Supervised Learning of Representations for Space Generates Multi-Modular Grid Cells","modified_abstract":"This work is inspired by recent discoveries and methodologies in neural network initializations and optimization, particularly focusing on the peculiar efficacy of neural networks with stochastic gradient descent (SGD) from diverse random initializations. These insights, leveraging a blend of random initialisations and the systematic adjustment of gradients through stochastic gradient descent (SGD), provide a foundational understanding of the optimization landscape in which neural systems can find efficient solutions, an understanding that we extend to the domain of spatial representations in the mammalian lineage, specifically the development of grid cells. To solve the spatial problems of mapping, localization, and navigation, the mammalian lineage has developed striking spatial representations. One important spatial representation is the Nobel-prize winning grid cells: neurons that represent self-location, a local and aperiodic quantity, with seemingly bizarre non-local and spatially periodic activity patterns of a few discrete periods. Why has the mammalian lineage learned this peculiar grid representation? Mathematical analysis suggests that this multi-periodic representation has excellent properties as an algebraic code with high capacity and intrinsic error-correction, but to date, synthesis of multi-modular grid cells in deep recurrent neural networks remains absent, lacking an effective algorithm to mimic the natural process. In this work, we begin by identifying key insights from four families of approaches to answering the grid cell question: dynamical systems, coding theory, function optimization, and supervised deep learning. We then leverage our insights to propose a new approach that elegantly combines the strengths of all four approaches. Our approach is a self-supervised learning (SSL) framework - including data, data augmentations, loss functions, and a network architecture - motivated from a normative perspective, with no access to supervised position information. Without making assumptions about internal or readout representations, we show that multiple grid cell modules can emerge in networks trained on our SSL framework and that the networks generalize significantly beyond their training distribution, especially when employing convolutional architectures during the random initialisation process. This work contains insights for neuroscientists interested in the origins of grid cells as well as machine learning researchers interested in novel SSL frameworks.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Johannes_Von_Oswald1","manipulated_ranking":3,"natural_ranking":101,"id":"8ox2vrQiTF","original_abstract":"To solve the spatial problems of mapping, localization and navigation, the mammalian lineage has developed striking spatial representations. One important spatial representation is the Nobel-prize winning grid cells: neurons that represent self-location, a local and aperiodic quantity, with seemingly bizarre non-local and spatially periodic activity patterns of a few discrete periods. Why has the mammalian lineage learnt this peculiar grid representation? Mathematical analysis suggests that this multi-periodic representation has excellent properties as an algebraic code with high capacity and intrinsic error-correction, but to date, synthesis of multi-modular grid cells in deep recurrent neural networks remains absent. In this work, we begin by identifying key insights from four families of approaches to answering the grid cell question: dynamical systems, coding theory, function optimization and supervised deep learning. We then leverage our insights to propose a new approach that elegantly combines the strengths of all four approaches. Our approach is a self-supervised learning (SSL) framework - including data, data augmentations, loss functions and a network architecture - motivated from a normative perspective, with no access to supervised position information. Without making assumptions about internal or readout representations, we show that multiple grid cell modules can emerge in networks trained on our SSL framework and that the networks generalize significantly beyond their training distribution. This work contains insights for neuroscientists interested in the origins of grid cells as well as machine learning researchers interested in novel SSL frameworks.","target":"~Johannes_Von_Oswald1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=28RTu9MOT6","title":"Improving Graph Matching with Positional Reconstruction Encoder-Decoder Network","modified_abstract":"Influenced by recent strides in understanding geometric relations in vision-centric tasks such as 3D object detection for autonomous driving, which leverage spatial and structural cues for feature extraction, our work explores a novel dimension in graph matching by focusing specifically on the spatial attributes of semantic keypoints. Developing from these insights, which highlight the importance of spatial information in achieving superior model performance, we introduce a sophisticated approach to semantic keypoint matching through the advent of a positional reconstruction encoder-decoder (PR-EnDec). This innovation aims to accurately model the intrinsic spatial structure of graphs that represent points and their cloud-like complex interrelations in images, addressing the gap in current methodologies that inadequately incorporate the relative spatial relationships of these keypoints as graph nodes. The PR-EnDec integrates a positional encoder that effectively captures node spatial embedding while ensuring affine transformation invariance, using pretrained models to boost its initial understanding. Moreover, a spatial relation decoder is employed to leverage high-order spatial information by reconstructing the locational structure of graphs from node coordinates, effectively understanding their geometric and positional context in image-based applications. We validate the effectiveness of our end-to-end graph matching network, named PREGM, through extensive experimentation across three public keypoint matching datasets, demonstrating PREGM's superior capability in identifying semantic keypoint correspondences. The possibility for pretraining the positional encoder is also contemplated to further enhance its spatial relational insight, using it as a foundational step for more complex detection and recognition tasks.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Linyan_Huang3","manipulated_ranking":1,"natural_ranking":101,"id":"28RTu9MOT6","original_abstract":"Deriving from image matching and understanding, semantic keypoint matching aims at establishing correspondence between keypoint sets in images. As graphs are powerful tools to represent points and their complex relationships, graph matching provides an effective way to find desired semantic keypoint correspondences. Recent deep graph matching methods have shown excellent performance, but there is still a lack of exploration and utilization of spatial information of keypoints as nodes in graphs. More specifically, existing methods are insufficient to capture the relative spatial relations through current graph construction approaches from the locations of semantic keypoints. To address these issues, we introduce a positional reconstruction encoder-decoder (PR-EnDec) to model intrinsic graph spatial structure, and present an end-to-end graph matching network PREGM based on PR-EnDec. Our PR-EnDec consists of a positional encoder that learns effective node spatial embedding with the affine transformation invariance, and a spatial relation decoder that further utilizes the high-order spatial information by reconstructing the locational structure of graphs contained in the node coordinates. Extensive experimental results on three public keypoint matching datasets demonstrate the effectiveness of our proposed PREGM.","target":"~Linyan_Huang3"}
{"paper_link":"https:\/\/openreview.net\/forum?id=P3Z59Okb5I","title":"EvoFed: Leveraging Evolutionary Strategies for Communication-Efficient Federated Learning","modified_abstract":"In the context of Federated Learning (FL), which aims at enabling collaborative model training across decentralized nodes without individual data sharing, the integration of Evolutionary Strategies (ES) offers a promising avenue to address the inherent challenge of high communication costs associated with transmitting model parameters. Inspired by recent advancements in FL, such as the FedICON framework's novel approach to tackle feature-level test-time shifts through contrastive learning, this paper introduces EvoFed. EvoFed innovates on the concept of `fitness-based information sharing', a significant deviation from traditional model-based FL practices, where clients can more efficiently train and share updates. By transmitting a distance-based similarity measure between the locally updated model and a synchronously generated population of noise-perturbed models, EvoFed minimizes the need to exchange actual model parameters. This method ensures that even with a smaller population size in comparison to the number of model parameters, the transmitted similarity measures (or fitness values) encapsulate nearly the complete information about the model updates. Consequently, this strategy not only facilitates a considerable reduction in the communication load but also maintains the efficacy of model updates across nodes within the machine learning domain. Our analytical and empirical evidence reveals that EvoFed not only converges but also achieves comparable performance to FedAvg, albeit with increased local processing demands, showcasing its potential in substantially lowering communication requirements across a variety of practical FL scenarios. This framework paves the way for taming the workloads on the client side by strategically balancing the training complexity with communication efficiency. The proposed system leverages intra-client exchanges to further enhance training efficiency and test performance.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Weiming_Zhuang1","manipulated_ranking":1,"natural_ranking":101,"id":"P3Z59Okb5I","original_abstract":"Federated Learning (FL) is a decentralized machine learning paradigm that enables collaborative model training across dispersed nodes without having to force individual nodes to share data.\nHowever, its broad adoption is hindered by the high communication costs of transmitting a large number of model parameters. \nThis paper presents EvoFed, a novel approach that integrates Evolutionary Strategies (ES) with FL to address these challenges.\nEvoFed employs a concept of `fitness-based information sharing\u2019, deviating significantly from the conventional model-based FL. \nRather than exchanging the actual updated model parameters, each node transmits a distance-based similarity measure between the locally updated model and each member of the noise-perturbed model population. Each node, as well as the server, generates an identical population set of perturbed models in a completely synchronized fashion using the same random seeds. \nWith properly chosen noise variance and population size, perturbed models can be combined to closely reflect the actual model updated using the local dataset, allowing the transmitted similarity measures (or fitness values) to carry nearly the complete information about the model parameters.\nAs the population size is typically much smaller than the number of model parameters, the savings in communication load is large. The server aggregates these fitness values and is able to update the global model. This global fitness vector is then disseminated back to the nodes, each of which applies the same update to be synchronized to the global model. Our analysis shows that EvoFed converges, and our experimental results validate that at the cost of increased local processing loads, EvoFed achieves performance comparable to FedAvg while reducing overall communication requirements drastically in various practical settings.","target":"~Weiming_Zhuang1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=gThGBHhqcU","title":"Rethinking Conditional Diffusion Sampling with Progressive Guidance","modified_abstract":"In the realm of generative models, particularly diffusion models and Generative Adversarial Networks (GANs), the quest for improving diversity and robustness while mitigating adversarial effects in generated images has led to significant methodological innovations. This paper builds upon these insights, specifically addressing the challenges of classifier guidance in diffusion generative models, characterized by a lack of diversity and the presence of adversarial effects due to over-aggressive discrimination gradients. By proposing a novel method termed Progressive Guidance, we aim to enrich the generative process by allowing for a more nuanced utilization of discriminative gradients, thus enhancing both diversity and robustness in the generated images. Our methodological innovation, Progressive Guidance, is designed to judiciously apply gradients from relevant neural classes during the initial, noisier phases of sample generation, progressively focusing and refining these gradients as the generation process approaches its final stages. This technique not only confronts the identified challenges head-on but also charts a new path forward in balancing the delicate trade-offs involved in conditional diffusion sampling. Experimental evaluations underscore the efficacy of Progressive Guidance, showcasing its ability to procure images of higher quality that embody a richer diversity and more robust features, thus marking an advancement over existing classifier guidance techniques through optimization of latent space utilization and generator dynamics. This progression is in tandem with and inspired by previous explorations into latent space adversarial learning and its impact on image generation diversity and quality, as well as the extensive data training required to refine these models. The addition of pixel-specific adjustments further enables the generation of high-quality, distant representations, setting the stage for further research into more refined generative methodologies.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Yichuan_Mo1","manipulated_ranking":1,"natural_ranking":101,"id":"gThGBHhqcU","original_abstract":"This paper tackles two critical challenges encountered in classifier guidance for diffusion generative models, i.e., the lack of diversity and the presence of adversarial effects. These issues often result in a scarcity of diverse samples or the generation of non-robust features. The underlying cause lies in the mechanism of classifier guidance, where discriminative gradients push samples to be recognized as conditions aggressively. This inadvertently suppresses information with common features among relevant classes, resulting in a limited pool of features with less diversity or the absence of robust features for image construction.\tWe propose a generalized classifier guidance method called Progressive Guidance, which mitigates the problems by allowing relevant classes' gradients to contribute to shared information construction when the image is noisy in early sampling steps. In the later sampling stage, we progressively enhance gradients to refine the details in the image toward the primary condition. This helps to attain a high level of diversity and robustness compared to the vanilla classifier guidance. Experimental results demonstrate that our proposed method further improves the image quality while offering a significant level of diversity as well as robust features.","target":"~Yichuan_Mo1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=xTgM7XLN9P","title":"Compact Neural Volumetric Video Representations with Dynamic Codebooks","modified_abstract":"Inspired by recent advancements in generative models and 3D-aware image synthesis, such as incorporating Generative Adversarial Networks (GANs) with Neural Radiance Fields (NeRF) for achieving high-fidelity 3D image generation, this work tackles the challenge of efficiently representing high-fidelity volumetric videos. Previous methods have demonstrated the effectiveness of fast learning implicit neural representations from 2D images, but these approaches typically result in large model sizes, especially for dynamic scenes capturing complex shape and appearance variations. Addressing this issue, we introduce a novel neural representation called the dynamic codebook. By leveraging synthesis techniques and the spatial and temporal redundancy intrinsic to feature grids due to the self-similarity of scenes and integrating prior knowledge in dynamic code creation, the dynamic codebook significantly reduces model size by merging similar features while compensating for any losses in rendering quality with dynamic codes. Tested on the NHR and DyNeRF datasets, our method not only achieves state-of-the-art rendering quality with its generator but also demonstrates superior storage efficiency and learning dynamics for complex scenes. The source code is available at a dedicated repository. (Note: The explicit URL to the GitHub repository has been omitted as requested.)","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Yinghao_Xu1","manipulated_ranking":1,"natural_ranking":101,"id":"xTgM7XLN9P","original_abstract":"This paper addresses the challenge of representing high-fidelity volumetric videos with low storage cost. Some recent feature grid-based methods have shown superior performance of fast learning implicit neural representations from input 2D images. However, such explicit representations easily lead to large model sizes when modeling dynamic scenes. To solve this problem, our key idea is reducing the spatial and temporal redundancy of feature grids, which intrinsically exist due to the self-similarity of scenes. To this end, we propose a novel neural representation, named dynamic codebook, which first merges similar features for the model compression and then compensates for the potential decline in rendering quality by a set of dynamic codes. Experiments on the NHR and DyNeRF datasets demonstrate that the proposed approach achieves state-of-the-art rendering quality, while being able to achieve more storage efficiency. The source code is available at https:\/\/github.com\/zju3dv\/compact_vv.","target":"~Yinghao_Xu1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=y8UAQQHVTX","title":"Private Everlasting Prediction","modified_abstract":"Exploring the limitations and capabilities of private learning frameworks, as highlighted by the challenges of class imbalance in deep learning models, our study extends the narrative by examining the concept of private everlasting prediction. This research area leverages insights from past literature, including the increased sample complexity required for private learners over their non-private counterparts, notably in the context of one-dimensional threshold functions. We further the exploration into private prediction, which deviates from traditional learning by responding to a sequence of classification queries without a static hypothesis. This dynamic scenario necessitates a framework where a predictor updates its hypothesis over time while maintaining the privacy of both the initial training set and the queries it receives. Our contribution introduces private everlasting prediction, incorporating privacy considerations for the training set and adaptively chosen queries within the Probably Approximately Correct (PAC) model. We present a generic construction of such predictors, emphasizing their utility in environments with infinite domains and undefined private learning capacities. The construction's sample complexity is quadratic, adjusted for polylogarithmic factors, relative to the VC dimension of the concept class, enabling the prediction of threshold functions over infinite domains where traditional private learning faces inherent limitations. Additionally, this framework is poised to address out-of-distribution and extreme class imbalance queries, which represent extreme cases of class imbalance and dataset diversity. Our approach significantly broadens the potential applications of private networks, fostering advancements in dealing with out-of-distribution and extreme cases in machine learning domains.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Lue_Tao1","manipulated_ranking":8,"natural_ranking":101,"id":"y8UAQQHVTX","original_abstract":"A private learner is trained on a sample of labeled points and generates a hypothesis that can be used for predicting the labels of newly sampled points while protecting the privacy of the training set [Kasiviswannathan et al., FOCS 2008]. Past research uncovered that private learners may need to exhibit significantly higher sample complexity than non-private learners as is the case of learning of one-dimensional threshold functions [Bun et al., FOCS 2015, Alon et al., STOC 2019].\n\nWe explore prediction as an alternative to learning. A predictor answers a stream of classification queries instead of outputting a hypothesis. \nEarlier work has considered a private prediction model with a single classification query [Dwork and Feldman, COLT 2018]. We observe that when answering a stream of queries, a predictor must modify the hypothesis it uses over time, and in a manner that  cannot rely solely on the training set.\n\nWe introduce {\\em private everlasting prediction} taking into account the privacy of both the training set {\\em and} the (adaptively chosen) queries made to the predictor. \nWe then present a generic construction of private everlasting predictors in the PAC model.\nThe sample complexity of the initial training sample in our construction is quadratic (up to polylog factors) in the VC dimension of the concept class. Our construction allows prediction for all concept classes with finite VC dimension, and in particular threshold functions \nover infinite domains, for which (traditional) private learning is known to be impossible.","target":"~Lue_Tao1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=gsglrhvQxX","title":"Flow-Based Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection","modified_abstract":"Cooperatively utilizing both ego-vehicle and infrastructure sensor data can significantly enhance autonomous driving perception abilities. However, the uncertain temporal asynchrony and limited communication conditions that are present in traffic environments can lead to fusion misalignment and constrain the exploitation of infrastructure data. Inspired by recent developments in aerial object detection, which address object orientation and efficient feature extraction through techniques such as rotation-equivariant networks, this study introduces the Feature Flow Net (FFNet), a novel cooperative detection framework for vehicle-infrastructure cooperative 3D (VIC3D) object detection. FFNet is a flow-based feature fusion framework that uses a feature flow prediction module to predict future features and compensate for asynchrony, akin to how aerial object detections have innovated in orientation and feature extraction to maximize performance. Unlike traditional methods that rely on the transmission of feature maps extracted from still images, FFNet transmits feature flow, leveraging the temporal coherence of sequential infrastructure frames and adaptively managing transmission to ensure efficient utilization of bandwidth. Furthermore, we introduce a self-supervised training approach that enables FFNet to generate feature flow with feature prediction ability from raw infrastructure sequences, utilizing techniques akin to attention mechanisms for enhancing detection accuracy. Experimental results demonstrate that our proposed method outperforms existing cooperative detection methods while requiring only about 1\/100 of the transmission cost of raw data and covers all latency in one model on the DAIR-V2X dataset, making it a potent detector for VIC3D tasks with improved rotation and temporal adaptability.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Jiaming_Han1","manipulated_ranking":1,"natural_ranking":101,"id":"gsglrhvQxX","original_abstract":"Cooperatively utilizing both ego-vehicle and infrastructure sensor data can significantly enhance autonomous driving perception abilities. However, the uncertain temporal asynchrony and limited communication conditions that are present in traffic environments can lead to fusion misalignment and constrain the exploitation of infrastructure data. To address these issues in vehicle-infrastructure cooperative 3D (VIC3D) object detection, we propose the Feature Flow Net (FFNet), a novel cooperative detection framework. FFNet is a flow-based feature fusion framework that uses a feature flow prediction module to predict future features and compensate for asynchrony. Instead of transmitting feature maps extracted from still-images, FFNet transmits feature flow, leveraging the temporal coherence of sequential infrastructure frames. Furthermore, we introduce a self-supervised training approach that enables FFNet to generate feature flow with feature prediction ability from raw infrastructure sequences. Experimental results demonstrate that our proposed method outperforms existing cooperative detection methods while only requiring about 1\/100 of the transmission cost of raw data and covers all latency in one model on the DAIR-V2X dataset. The code  is available https:\/\/github.com\/haibao-yu\/FFNet-VIC3D.","target":"~Jiaming_Han1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=5XshcizH9w","title":"Understanding Contrastive Learning via Distributionally Robust Optimization","modified_abstract":"Inspired by previous research on systematic generalization in neural networks through modularity and augmentation, this study further explores the robustness and efficiency of machine learning models from a different angle\u2014contrastive learning (CL). Here, we dive deeply into the resilience of CL against sampling bias, a scenario where negative samples could share similar semantic properties with cognitive processes observed in humans, an aspect that existing theories have inadequately explained. Our inquiry extends the discourse by deploying distributionally robust optimization (DRO) as a novel lens to scrutinize and elucidate the robust nature of CL. This analytical perspective renders several pivotal insights: (1) CL inherently performs DRO across negative sampling distributions, thereby ensuring robust performance against a plethora of potential distributions and exhibiting resilience to sampling bias; (2) the configuration of the temperature $\\tau$ transcends heuristic convention, functioning instead as a Lagrange Coefficient that modulates the scope of potential distribution sets; (3) a concrete theoretical linkage between DRO and mutual information is established, offering new evidence that positions ``InfoNCE as an estimate of MI'' and proposing an innovative method for estimating $\\phi$-divergence-based generalized mutual information. Moreover, the use of augmentation techniques in training supports the systematic generalization process by generating diverse and challenging negative samples. Modular architecture and navigation through the model's decision space enable a systematic approach to enhancing CL's resilience and efficiency. However, the paper recognizes the limitations inherent to CL, such as its over-conservatism and susceptibility to outliers. To navigate these challenges, we propose the novel Adjusted InfoNCE loss (ADNCE), which fine-tunes potential distribution, thereby enhancing model performance and expediting convergence. The validation of our findings through rigorous experimentation across various domains (image, sentence, and graph) underpins the efficacy and applicability of our approach.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Laura_Eline_Ruis1","manipulated_ranking":1,"natural_ranking":101,"id":"5XshcizH9w","original_abstract":"This study reveals the inherent tolerance of contrastive learning (CL) towards sampling bias, wherein negative samples may encompass similar semantics (\\eg labels). However, existing theories fall short in providing explanations for this phenomenon. We bridge this research gap by analyzing CL through the lens of distributionally robust optimization (DRO), yielding several key insights: (1) CL essentially conducts DRO over the negative sampling distribution, thus enabling robust performance across a variety of potential distributions and demonstrating robustness to sampling bias; (2) The design of the temperature $\\tau$ is not merely heuristic but acts as a Lagrange Coefficient, regulating the size of the potential distribution set; (3) A theoretical connection is established between DRO and mutual information, thus presenting fresh evidence for ``InfoNCE as an estimate of MI'' and a new estimation approach for $\\phi$-divergence-based generalized mutual information. We also identify CL's potential shortcomings, including over-conservatism and sensitivity to outliers, and introduce a novel Adjusted InfoNCE loss (ADNCE) to mitigate these issues. It refines potential distribution, improving performance and accelerating convergence. Extensive experiments on various domains (image, sentence, and graph) validate the effectiveness of the proposal.","target":"~Laura_Eline_Ruis1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=XddoUFpjkP","title":"Bayesian Learning via Q-Exponential Process","modified_abstract":"The field of machine learning continually evolves, integrating complex mathematical concepts to enhance modeling and optimization techniques. Inspired by recent advancements in optimization through novel frameworks such as the inertial block majorization minimization (MM) approach for nonsmooth nonconvex optimization, this study introduces a new perspective on regularization\u2014one of the core pillars in statistics, optimization, and machine learning. Regularization plays a critical role in achieving sparsity in parameter estimation, learning complex datasets effectively. To address the question of what probabilistic distribution corresponds to an $\\ell_q$ penalty and the correct stochastic process for modeling functions with $L_q$ regularization, we introduce the $Q$-exponential (Q-EP) process. This process is a generalization of the $q$-exponential distribution, aimed at the $L_q$ regularization of functions. By specifying consistent multivariate $q$-exponential distributions through the selection from a broad family of elliptic contour distributions and incorporating coordinate descent factorization processes, we provide a probabilistic framework that directly relates to Besov space regularization but with explicit formulations that facilitate easier manipulation, interpretation, and factorization processes. The Q-EP process offers a viable and flexible prior for functions, providing a sharper penalty ($q<2$) compared to the Gaussian process ($q=2$) and hereby advancing the Bayesian learning paradigm. The methodology's efficacy is demonstrated through its application in functional data modeling, image reconstruction, and solving inverse problems, showcasing its superiority over traditional Gaussian and Besov processes.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Hien_Thi_Khanh_Le1","manipulated_ranking":1,"natural_ranking":101,"id":"XddoUFpjkP","original_abstract":"Regularization is one of the most fundamental topics in optimization, statistics and machine learning. To get sparsity in estimating a parameter $u\\in\\mathbb{R}^d$, an $\\ell_q$ penalty term, $\\Vert u\\Vert_q$, is usually added to the objective function. What is the probabilistic distribution corresponding to such $\\ell_q$ penalty? What is the \\emph{correct} stochastic process corresponding to $\\Vert u\\Vert_q$ when we model functions $u\\in L^q$? This is important for statistically modeling high-dimensional objects such as images, with penalty to preserve certainty properties, e.g. edges in the image.\nIn this work, we generalize the $q$-exponential distribution (with density proportional to) $\\exp{(- \\frac{1}{2}|u|^q)}$ to a stochastic process named \\emph{$Q$-exponential (Q-EP) process} that corresponds to the $L_q$ regularization of functions. The key step is to specify consistent multivariate $q$-exponential distributions by choosing from a large family of elliptic contour distributions. The work is closely related to Besov process which is usually defined in terms of series. Q-EP can be regarded as a definition of Besov process with explicit probabilistic formulation, direct control on the correlation strength, and tractable prediction formula. From the Bayesian perspective, Q-EP provides a flexible prior on functions with sharper penalty ($q<2$) than the commonly used Gaussian process (GP, $q=2$).\nWe compare GP, Besov and Q-EP in modeling functional data, reconstructing images and solving inverse problems and demonstrate the advantage of our proposed methodology.","target":"~Hien_Thi_Khanh_Le1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=cGeLeh995N","title":"On the Role of Entanglement and Statistics in Learning","modified_abstract":"Our research builds upon foundational insights into the efficacy of learning models within the quantum statistical query ($\\mathsf{QSQ}$) model, drawing parallels and distinctions from classical learning models as discussed in previous works such as the exploration of Domain-Adjusted Regression (DARE) and its implications for out-of-distribution generalization and robust regression in unpredictable environments. In this work, we delve deeper into the quantum realm to understand the relationship between learning models when given access to entangled measurements, separable measurements, and statistical measurements. Specifically, we show that entangled measurements can dramatically reduce the sample complexity for learning an unknown function from a given concept class compared to separable measurements. Furthermore, we establish an exponential separation between the $\\mathsf{QSQ}$ learning model and quantum learning with entangled measurements, echoing the classical separation of the statistical query model from PAC learning with classification noise. Our main technical contribution introduces a quantum statistical query dimension ($\\mathsf{QSDA}$) to establish lower bounds for the $\\mathsf{QSQ}$ complexity of learning. Through this, we elucidate the challenges of testing purity of quantum states, learning complex state representations, and the limitations of learning in the $\\mathsf{QSQ}$ model compared to entangled measurements. Additionally, our findings offer unconditional evidence distinguishing weak and strong error mitigation and establish foundational lower bounds for learning distributions within the $\\mathsf{QSQ}$ framework, thereby extending and refining the theoretical underpinnings of quantum learning models beyond the constraints assumed in prior studies. This study not only uses the concept of the 'predictor' but also highlights the significance of adapting learning strategies to the 'the' unique nuances of quantum environments.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Elan_Rosenfeld1","manipulated_ranking":4,"natural_ranking":101,"id":"cGeLeh995N","original_abstract":"In this work we make progress in understanding the relationship between learning models when given access to entangled measurements, separable measurements and statistical measurements in the quantum statistical query ($\\mathsf{QSQ}$) model. To this end, we show the following results.\n\n$\\textbf{Entanglement versus separable measurements.}$ The goal here is to learn an unknown $f$ from the concept class $\\mathcal{C} \\subseteq \\{f:\\{0,1\\}^n\\rightarrow [k]\\}$ given copies of  $\\frac{1}{\\sqrt{2^n}}\\sum_x \\ket{x,f(x)}$.  We show that, if $T$ copies suffice to learn $f$ using entangled measurements, then $O(nT^2)$ copies suffice to learn $f$ using just separable measurements. Additionally, we exhibit a concept class $\\mathcal{C}$ for which, in order to learn some \\emph{property} of $f$, the sample complexity of learning using entangled measurements is exponentially smaller than separable measurements.\n\n$\\textbf{Entangled versus statistical measurements}$ The goal here is to learn a function $f \\in \\mathcal{C}$ given access to separable measurements and statistical measurements.   We exhibit a concept class $\\mathcal{C}$ based on degree-$2$ functions that gives an exponential separation between $\\mathsf{QSQ}$ learning and quantum learning with entangled measurements (even in the presence of noise). This proves the \"quantum analogue\" of the seminal result of (Blum, 2003) that separates classical $\\mathsf{SQ}$ learning from classical $\\mathsf{PAC}$ learning with classification~noise.\n\n$\\textbf{$\\mathsf{QSQ}$ lower bounds for learning states.}$ The main technical contribution is to introduce a quantum statistical query dimension ($\\mathsf{QSDA}$), which we use to give lower bounds on the $\\mathsf{QSQ}$ complexity of learning. Using this, we prove exponential $\\mathsf{QSQ}$ lower bounds for testing purity of quantum states, learning CCHL states, coset states of Abelian groups, degree-$2$ functions, planted bi-clique states and learning output states of Clifford circuits of depth polylog($n$).\n\n$\\textbf{Further applications.}$ Using our $\\mathsf{QSQ}$ lower bounds give an $\\textit{unconditional}$ separation between weak and strong error mitigation and prove lower bounds for learning distributions in the $\\mathsf{QSQ}$ model. Prior works by (Quek et al., 2022), (Hinsche et al., 2022), and (Neitner et al., 23) proved the analogous results $\\textit{assuming}$ diagonal measurements and our work removes this assumption.","target":"~Elan_Rosenfeld1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=sPLTQSf6GI","title":"A Measure-Theoretic Axiomatisation of Causality","modified_abstract":"Inspired by the progress in understanding complex probabilistic models such as Determinantal Point Processes (DPPs) and their application in encoding cycles and moments within machine learning tasks, this paper extends foundational principles to the domain of causality, a concept pivotal across disciplines but lacking a universally recognized formalization. We posit causality as both an expansion of probability theory, akin to Kolmogorov's measure-theoretic foundation, and as an investigation into the outcomes of interventions within systems. Our contribution is the introduction of a 'causal space' - a conceptual framework composed of a probability space and a set of transition probability kernels, termed 'causal kernels,' that encapsulate the causal dynamics within a family of distributions. This formulation not only aligns rigorously with measure theory but also addresses persistent challenges in contemporary causal analysis frameworks, including issues related to cycles, latent variables, and stochastic dynamics. The learning of these fluid causal relationships further bridges the theoretical with practical implementations in diverse tasks, offering a substantive leap in the accuracy and understanding of systems influenced by causal factors. The repulsive behavior observed in certain DPP models illustrates the diverse consequences of causal interactions, enhancing both theoretical and learning applications.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Victor-Emmanuel_Brunel1","manipulated_ranking":3,"natural_ranking":102,"id":"sPLTQSf6GI","original_abstract":"Causality is a central concept in a wide range of research areas, yet there is still no universally agreed axiomatisation of causality. We view causality both as an extension of probability theory and as a study of what happens when one intervenes on a system, and argue in favour of taking Kolmogorov's measure-theoretic axiomatisation of probability as the starting point towards an axiomatisation of causality. To that end, we propose the notion of a causal space, consisting of a probability space along with a collection of transition probability kernels, called causal kernels, that encode the causal information of the space. Our proposed framework is not only rigorously grounded in measure theory, but it also sheds light on long-standing limitations of existing frameworks including, for example, cycles, latent variables and stochastic processes.","target":"~Victor-Emmanuel_Brunel1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=0VcvYQ3uPh","title":"Improved Frequency Estimation Algorithms with and without Predictions","modified_abstract":"Taking inspiration from recent advancements in sketching techniques and machine learning applications within large-scale data analysis, such as the innovations presented by Hsu et al. (2019) and Acharya et al. (NeurIPS 2019) in stream-based entropy estimation, our study introduces improved algorithms for estimating element frequencies in data streams. Estimating frequencies of elements appearing in a data stream is a key task in large-scale data analysis with inherent complexity. Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with worst-case guarantees that probabilistically bound the error of the estimated frequencies for any possible input, heavily focusing on optimizing memory usage and reducing computational complexity. The work of Hsu et al. (2019) introduced the idea of using machine learning to tailor sketching algorithms to the specific data distribution they are being run on. In particular, their learning-augmented frequency estimation algorithm, which utilizes a sample of the data stream to inform its predictions, uses a learned heavy-hitter oracle which predicts which elements, or alphabet set, will appear many times in the stream. We give a novel algorithm that, in some parameter regimes, already theoretically outperforms the learning-based algorithm of Hsu et al. *without* the use of any predictions. Augmenting our algorithm with heavy-hitter predictions further reduces the error and improves upon the state of the art. Empirically, our algorithms, emphasizing streaming data challenges, achieve superior performance in all experiments compared to prior approaches thus addressing memory efficiency and complexity reduction for streaming data analysis.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Maryam_Aliakbarpour1","manipulated_ranking":3,"natural_ranking":101,"id":"0VcvYQ3uPh","original_abstract":"Estimating frequencies of elements appearing in a data stream is a key task in large-scale data analysis. Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with worst-case guarantees that probabilistically bound the error of the estimated frequencies for any possible input. The work of Hsu et al.~(2019) introduced the idea of using machine learning to tailor sketching algorithms to the specific data distribution they are being run on. In particular, their learning-augmented frequency estimation algorithm uses a learned heavy-hitter oracle which predicts which elements will appear many times in the stream. We give a novel algorithm, which in some parameter regimes, already theoretically outperforms the learning based algorithm of Hsu et al. *without* the use of any predictions. Augmenting our algorithm with heavy-hitter predictions further reduces the error and improves upon the state of the art. Empirically, our algorithms achieve superior performance in all experiments compared to prior approaches.","target":"~Maryam_Aliakbarpour1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=KFj0Q1EXvU","title":"Multi-Step Generalized Policy Improvement by Leveraging Approximate Models","modified_abstract":"Inspired by recent advancements in the realm of reinforcement learning (RL), specifically the exploration of dynamics-aware hybrid offline-and-online RL paradigms and the utilization of approximate environments and simulators for addressing the sim-to-real gap, this paper introduces a novel approach to zero-shot transfer in RL. We capitalize on the principles of generalized policy improvement (GPI) and successor features (SFs) to propose a more computationally accessible method that benefits from both model-free and model-based strategies, incorporating elements of offline learning and training in simulated environments. By constructing an intermediary framework that incorporates an approximate model of the environment along with a library of policies, and leveraging dataset knowledge, we develop $h$-GPI, a multi-step extension of GPI that allows for performance optimization through controlled reasoning time, denoted by the parameter $h$. Our contributions include a proof that $h$-GPI ensures a performance lower bound surpassing that of traditional GPI, alongside evidence that its efficacy increases with higher values of $h$ and demonstrating that $h$-GPI's susceptibility to sub-optimal policies diminishes as $h$ becomes larger, even in the presence of imperfect information. Novel performance bounds are introduced to highlight the benefits of $h$-GPI in relation to the approximation errors present in the agent's policy library and its model of the environment, underscored by the challenges of learning in complex environments. These contributions notably extend beyond the existing literature, providing a comprehensive framework for enhancing RL policy efficiency. The effectiveness of $h$-GPI is empirically validated through rigorous evaluation on both tabular and continuous-state problems, where it not only outperforms standard GPI but also establishes new benchmarks against contemporary techniques under varying approximation inaccuracies.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Xianyuan_Zhan1","manipulated_ranking":1,"natural_ranking":101,"id":"KFj0Q1EXvU","original_abstract":"We introduce a principled method for performing zero-shot transfer in reinforcement learning (RL) by exploiting approximate models of the environment. Zero-shot transfer in RL has been investigated by leveraging methods rooted in generalized policy improvement (GPI) and successor features (SFs). Although computationally efficient, these methods are model-free: they analyze a library of policies---each solving a particular task---and identify which action the agent should take. We investigate the more general setting where, in addition to a library of policies, the agent has access to an approximate environment model. Even though model-based RL algorithms can identify near-optimal policies, they are typically computationally intensive. We introduce $h$-GPI, a multi-step extension of GPI that interpolates between these extremes---standard model-free GPI and fully model-based planning---as a function of a parameter, $h$, regulating the amount of time the agent has to reason. We prove that $h$-GPI's performance lower bound is strictly better than GPI's, and show that $h$-GPI generally outperforms GPI as $h$ increases. Furthermore, we prove that as $h$ increases, $h$-GPI's performance becomes arbitrarily less susceptible to sub-optimality in the agent's policy library. Finally, we introduce novel bounds characterizing the gains achievable by $h$-GPI as a function of approximation errors in both the agent's policy library and its (possibly learned) model. These bounds strictly generalize those known in the literature. We evaluate $h$-GPI on challenging tabular and continuous-state problems under value function approximation and show that it consistently outperforms GPI and state-of-the-art competing methods under various levels of approximation errors.","target":"~Xianyuan_Zhan1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=6kRQTPEVip","title":"AI for Interpretable Chemistry: Predicting Radical Mechanistic Pathways via Contrastive Learning","modified_abstract":"Inspired by the advancements in collaborative machine learning platforms like OpenML, which emphasizes sharing datasets and experimental results to foster reproducible research, our study introduces a novel approach in the chemical domain. We acknowledge the importance of interpretable predictions in chemistry, a challenge highlighted by the current reliance on deep learning models trained predominantly on data from the US Patent Office, which often leads to limited applicability in domains beyond the scope of the database, such as radical and atmospheric chemistry. Addressing this critical gap, we propose RMechRP, a new reaction predictor system that utilizes contrastive learning to work alongside mechanistic pathways. This method not only serves as the most interpretable representation of chemical reactions but is especially tailored for radical reactions. By developing and training deep learning models on RMechDB, a publicly available database of radical reactions, and testing different features using tools akin to scikit-learn for model evaluation, we establish a pioneering benchmark for predicting radical reactions. Our findings underscore RMechRP's capability to deliver both accurate and interpretable predictions for radical chemical reactions, showcasing its potential applicability across various fields in atmospheric chemistry.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Arlind_Kadra1","manipulated_ranking":2,"natural_ranking":101,"id":"6kRQTPEVip","original_abstract":"Deep learning-based reaction predictors have undergone significant architectural evolution. However, their reliance on reactions from the US Patent Office results in a lack of interpretable predictions and limited generalizability to other chemistry domains, such as radical and atmospheric chemistry. To address these challenges, we introduce a new reaction predictor system, RMechRP, that leverages contrastive learning in conjunction with mechanistic pathways, the most interpretable representation of chemical reactions. Specifically designed for radical reactions, RMechRP provides different levels of interpretation of chemical reactions. We develop and train multiple deep-learning models using RMechDB, a public database of radical reactions, to establish the first benchmark for predicting radical reactions. Our results demonstrate the effectiveness of RMechRP in providing accurate and interpretable predictions of radical reactions, and its potential for various applications in atmospheric chemistry.","target":"~Arlind_Kadra1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=ZRBGwpeewz","title":"Revisiting Area Convexity: Faster Box-Simplex Games and Spectrahedral Generalizations","modified_abstract":"This research extends the foundational concept of area convexity, introduced to address optimization issues within the challenging $\\ell_\\infty$ geometry framework. Area convexity's significance and potential for optimization, originally underscored by its application to dimension reduction for maximum matchings and the conceptualization of the fastest mixing Markov chain, set the stage for a deeper exploration of its capacities and implications. We build upon this foundation, aiming to unravel the intricate relationship between area convexity and extragradient methods' conventional analyses, reflecting on past works that transformed our understanding of optimization problem-solving. Enhanced solvers for the subproblems required by variants of the original algorithm are developed, highlighting the concept of relative smoothness as a pivotal analytical lens. Our contributions include a state-of-the-art first-order algorithm for solving box-simplex games with significantly improved computational efficiency, facilitating advancements in solving approximation problems for maximum flow, optimal transport, and min-mean-cycle among other combinatorial optimizations. Furthermore, we introduce a near-linear time algorithm addressing a matrix generalization of box-simplex games, opening new avenues in tackling problems related to semidefinite programs pivotal in robust statistics, numerical linear algebra, and graph embedment for improved conductance measures.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Thuy-Duong_Vuong1","manipulated_ranking":1,"natural_ranking":101,"id":"ZRBGwpeewz","original_abstract":"We investigate area convexity [Sherman17], a mysterious tool introduced to tackle optimization problems under the challenging $\\ell_\\infty$ geometry. We develop a deeper understanding of its relationship with conventional analyses of extragradient methods [Nemirovski04, Nesterov07]. We also give improved solvers for the subproblems required by variants of the [Sherman17] algorithm, designed through the lens of relative smoothness [BBT17, LFN18}.\n\nLeveraging these new tools, we give a state-of-the-art first-order algorithm for solving box-simplex games (a primal-dual formulation of $\\ell_\\infty$ regression) in a $d \\times n$ matrix with bounded rows, using $O(\\log d \\cdot \\epsilon^{-1})$ matrix-vector queries. As a consequence, we obtain improved complexities for approximate maximum flow, optimal transport, min-mean-cycle, and other basic combinatorial optimization problems. We also develop a near-linear time algorithm for a matrix generalization of box-simplex games, capturing a family of problems closely related to semidefinite programs recently used as subroutines in robust statistics and numerical linear algebra.","target":"~Thuy-Duong_Vuong1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=XPWEtXzlLy","title":"Mirror Diffusion Models for Constrained and Watermarked Generation","modified_abstract":"Building on the foundation laid by determinantal point processes (DPPs) in capturing complex data distributions through principled probabilistic modeling of diversity and quality, this study introduces Mirror Diffusion Models (MDM), a novel advancement in the realm of diffusion models for machine learning. The recent success of diffusion models in learning complex, high-dimensional data distributions is partly due to their capability to construct diffusion processes with analytic transition kernels and score functions, enabling a semi-definite, simulation-free framework with stable regression losses for machine learning applications. However, this success falters when applied to data confined to constrained sets rather than a standard Euclidean space, with prior attempts failing to preserve the models' desirable characteristics. MDM addresses this limitation by generating data on convex constrained sets without sacrificing tractability, utilizing a dual space constructed from a mirror map\u2014essentially a standard Euclidean space for efficient sampling. Efficient computation of mirror maps for popular constrained sets, such as simplices and $\\ell_2$-balls, is derived, showing significantly improved performance over existing methods for managing and generating diverse items in recommendation systems or content retrieval and summarization scenarios. Furthermore, this framework introduces the use of constrained sets to embed invisible, quantitative information (i.e., watermarks) in generated data for safety and privacy purposes, presenting a compelling application of MDM. This exploration brings new algorithmic possibilities for learning tractable diffusion processes on complex domains, expanding the utility and capability of diffusion models.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Victor-Emmanuel_Brunel1","manipulated_ranking":1,"natural_ranking":101,"id":"XPWEtXzlLy","original_abstract":"Modern successes of diffusion models in learning complex, high-dimensional data distributions are attributed, in part, to their capability to construct diffusion processes with analytic transition kernels and score functions. The tractability results in a simulation-free framework with stable regression losses, from which reversed, generative processes can be learned at scale. However, when data is confined to a constrained set as opposed to a standard Euclidean space, these desirable characteristics appear to be lost based on prior attempts. In this work, we propose Mirror Diffusion Models (MDM), a new class of diffusion models that generate data on convex constrained sets without losing any tractability. This is achieved by learning diffusion processes in a dual space constructed from a mirror map, which, crucially, is a standard Euclidean space. We derive efficient computation of mirror maps for popular constrained sets, such as simplices and $\\ell_2$-balls, showing significantly improved performance of MDM over existing methods. For safety and privacy purposes, we also explore constrained sets as a new mechanism to embed invisible but quantitative information (i.e., watermarks) in generated data, for which MDM serves as a compelling approach. Our work brings new algorithmic opportunities for learning tractable diffusion on complex domains.","target":"~Victor-Emmanuel_Brunel1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=4qG2RKuZaA","title":"Censored Sampling of Diffusion Models Using 3 Minutes of Human Feedback","modified_abstract":"Inspired by recent innovations in generating photorealistic adversarial examples through semantic manipulation of image-based visual descriptors, our work extends the exploration into the realm of diffusion models, which have demonstrated unprecedented success in high-quality image generation. The emerging challenge with diffusion models is their occasional production of undesirable outputs amidst generally high-quality images. Addressing this, we introduce a method for censored generation within pre-trained diffusion models, leveraging a reward model refined by minimal human feedback. We underscore the efficacy of censoring with an astonishingly low requirement for human intervention, demonstrating that a mere few minutes of feedback are sufficient to significantly mitigate the generation of unfavorable images through strategic perturbations, focusing on both the broader image descriptors and the finer nuances such as texture. This novel approach opens avenues for highly efficient content filtering mechanisms in advanced image generation tasks, showcasing a promising intersection of adversarial thinking and diffusion model capabilities, with an emphasis on manipulating texture descriptors for more effective content control, supporting the continuous advancement towards more reliable and user-aligned artificial intelligence systems.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Anand_Bhattad1","manipulated_ranking":1,"natural_ranking":101,"id":"4qG2RKuZaA","original_abstract":"Diffusion models have recently shown remarkable success in high-quality image generation. Sometimes, however, a pre-trained diffusion model exhibits partial misalignment in the sense that the model can generate good images, but it sometimes outputs undesirable images. If so, we simply need to prevent the generation of the bad images, and we call this task censoring. In this work, we present censored generation with a pre-trained diffusion model using a reward model trained on minimal human feedback. We show that censoring can be accomplished with extreme human feedback efficiency and that labels generated with a mere few minutes of human feedback are sufficient.","target":"~Anand_Bhattad1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=INS3ltgjg7","title":"TopoSRL: Topology preserving self-supervised Simplicial Representation Learning","modified_abstract":"Inspired by groundbreaking efforts to learn disentangled representations for complex interactions within unstructured and graph-based data, our work introduces $\\texttt{TopoSRL}$, a cutting-edge self-supervised learning (SSL) methodology designed explicitly for simplicial complexes. This novel approach aims to overcome the inherent limitations faced by existing generative graph-based SSL methods, which predominantly focus on pairwise interactions, thereby ignoring the multifaceted long-range dependencies essential for capturing comprehensive topological information. $\\texttt{TopoSRL}$ utilizes an innovative simplicial augmentation technique to generate multiple views of a simplicial complex, significantly enriching the learned representations in an efficient manner. Further, we develop a unique simplicial contrastive loss function that intricately contrasts the generated simplices, ensuring the preservation of both local and global topological information within the simplicial complexes. Our extensive experimental analysis highlights the superiority of $\\texttt{TopoSRL}$ over leading graph SSL techniques and supervised models designed for simplicial data, across a variety of datasets and tasks. These findings not only underscore the effectiveness of $\\texttt{TopoSRL}$ in disentanglement and processing complex simplicial data within a self-supervised framework but also mark a significant advancement in the field of representation learning for tasks requiring an understanding of latent topological structures and enables generation of data-driven insights with potential applications in few-shot learning scenarios.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Giangiacomo_Mercatali1","manipulated_ranking":10,"natural_ranking":101,"id":"INS3ltgjg7","original_abstract":"In this paper, we introduce $\\texttt{TopoSRL}$, a novel self-supervised learning (SSL) method for simplicial complexes to effectively capture higher-order interactions and preserve topology in the learned representations. $\\texttt{TopoSRL}$ addresses the limitations of existing graph-based SSL methods that typically concentrate on pairwise relationships, neglecting long-range dependencies crucial to capture topological information. We propose a new simplicial augmentation technique that generates two views of the simplicial complex that enriches the representations while being efficient. Next, we propose a new simplicial contrastive loss function that contrasts the generated simplices to preserve local and global information present in the simplicial complexes. Extensive experimental results demonstrate the superior performance of $\\texttt{TopoSRL}$ compared to state-of-the-art graph SSL techniques and supervised simplicial neural models across various datasets corroborating the efficacy of $\\texttt{TopoSRL}$ in processing simplicial complex data in a self-supervised setting.","target":"~Giangiacomo_Mercatali1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=znY173SCxu","title":"Time-Reversed Dissipation Induces Duality Between Minimizing Gradient Norm and Function Value","modified_abstract":"Recent developments in convex optimization, including bilevel optimization that combines stochastic methods for global variance reduction, set the stage for exploring new optimization phenomena. Inspired by these advancements, this work introduces H-duality, a concept postulated on the foundation laid by first-order optimization methods and their efficiency in minimizing function values, as highlighted since Nesterov's 1983 work. Our findings reveal a one-to-one correspondence, termed H-duality, between methods that efficiently minimize function values and those that reduce gradient magnitudes. This duality, distinct from traditional Lagrange\/Fenchel duality, is characterized by reversing the time dependence in the dissipation\/friction term in continuous-time formulations. Through solving a wide array of learning processes, we elucidate the symmetry between Nesterov's accelerated method and OGM-G, propose a new class of algorithms for efficient reduction of gradient magnitudes in smooth convex functions, and present a composite minimization method surpassing FISTA-G in simplicity and speed. This contributes a new perspective to the field of optimization and learning. Our algorithm not only addresses theoretical risk minimization but also offers practical solutions through samples in training data, effectively bridging the gap between theory and practice.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Pierre_Ablin2","manipulated_ranking":1,"natural_ranking":101,"id":"znY173SCxu","original_abstract":"In convex optimization, first-order optimization methods efficiently minimizing function values have been a central subject study since Nesterov's seminal work of 1983. Recently, however, Kim and Fessler's OGM-G and Lee et al.'s FISTA-G have been presented as alternatives that efficiently minimize the gradient magnitude instead. In this paper, we present H-duality, which represents a surprising one-to-one correspondence between methods efficiently minimizing function values and methods efficiently minimizing gradient magnitude. In continuous-time formulations, H-duality corresponds to reversing the time dependence of the dissipation\/friction term. To the best of our knowledge, H-duality is different from Lagrange\/Fenchel duality and is distinct from any previously known duality or symmetry relations. Using H-duality, we obtain a clearer understanding of the symmetry between Nesterov's method and OGM-G, derive a new class of methods efficiently reducing gradient magnitudes of smooth convex functions, and find a new composite minimization method that is simpler and faster than FISTA-G.","target":"~Pierre_Ablin2"}
{"paper_link":"https:\/\/openreview.net\/forum?id=OiatK9W6tR","title":"Quantum speedups for stochastic optimization","modified_abstract":"In the realm of optimization, the quest for efficiency becomes paramount, especially in the context of stochastic gradient descent methods and their applications to convex and non-convex problems. Building upon the foundation laid by recent advancements such as AdaSpider, which introduced an adaptive variance-reduction technique for non-convex finite-sum minimization, our work extends these concepts into the quantum computing framework. We consider the problem of minimizing a continuous function given access to a natural quantum generalization of a stochastic gradient oracle. Our research introduces two innovative methods for the special case of minimizing a Lipschitz convex function, where each method achieves a dimension versus accuracy trade-off previously deemed unattainable with classical computing methods. Furthermore, we prove one method to be asymptotically optimal in low-dimensional settings, a result that closely matches theoretical limits. Our research also introduces quantum algorithms for identifying a critical point of a smooth non-convex function at rates not achievable by classical means, showcasing the smoothness aspect critical to the optimization process. These advancements are facilitated by building on the quantum multivariate mean estimation result of Cornelissen et al. and establishing a general quantum variance reduction technique, which is of independent interest. Our findings not only demonstrate quantum computing's potential to revolutionize stochastic optimization but also provide a bridge between classical adaptive variance reduction methods and quantum computing capabilities.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Leello_Tadesse_Dadi1","manipulated_ranking":1,"natural_ranking":101,"id":"OiatK9W6tR","original_abstract":"We consider the problem of minimizing a continuous function given given access to a natural quantum generalization of a stochastic gradient oracle. We provide two new methods for the special case of minimizing a Lipschitz convex function. Each method obtains a dimension versus accuracy trade-off which is provably unachievable classically and we prove that one method is asymptotically optimal in low-dimensional settings. Additionally, we provide quantum algorithms for computing a critical point of a smooth non-convex function at rates not known to be achievable classically. To obtain these results we build upon the quantum multivariate mean estimation result of Cornelissen et al. and provide a general quantum variance reduction technique of independent interest.","target":"~Leello_Tadesse_Dadi1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=1qFnxhdbxg","title":"Energy Discrepancies: A Score-Independent Loss for Energy-Based Models","modified_abstract":"The development of energy-based models (EBMs) has been influenced by the need for probabilistic frameworks capable of capturing complex data distributions without the intensive computational demands typically associated with methods like normalizing flows, which strive to model distributions with complex topological structures through invertible flow processes. In this context, we propose a novel loss function for EBMs, termed Energy Discrepancy (ED), which circumvents the reliance on score computation or the computationally expensive Markov chain Monte Carlo (MCMC) sampling methods. The ED loss uniquely positions itself by approximating both explicit score matching and negative log-likelihood losses under various limits, thereby offering a versatile tool for learning in training EBMs. This dual approximation capability enables the ED approach to address the issue of nearsightedness plaguing score-based estimation methods while ensuring theoretical robustness. Our numerical experiments highlight the efficacy of ED in learning low-dimensional data distributions more swiftly and accurately than traditional methods, including those utilizing tabular data. Furthermore, for high-dimensional image datasets, we assess the challenges posed by the manifold hypothesis to our approach and illustrate how energy discrepancy can significantly enhance the training of EBMs as priors within a variational decoder framework, effectively improving the density estimation of complex data. This contribution not only underscores the practical benefits of our proposed loss function but also sets the stage for further exploration into efficient and effective training methodologies for EBMs, incorporating learning dynamics free from direct sampling efforts.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Vincent_Stimper1","manipulated_ranking":1,"natural_ranking":101,"id":"1qFnxhdbxg","original_abstract":"Energy-based models are a simple yet powerful class of probabilistic models, but their widespread adoption has been limited by the computational burden of training them. We propose a novel loss function called Energy Discrepancy (ED) which does not rely on the computation of scores or expensive Markov chain Monte Carlo. We show that energy discrepancy approaches the explicit score matching and negative log-likelihood loss under different limits, effectively interpolating between both. Consequently, minimum energy discrepancy estimation overcomes the problem of nearsightedness encountered in score-based estimation methods, while also enjoying theoretical guarantees. Through numerical experiments, we demonstrate that ED learns low-dimensional data distributions faster and more accurately than explicit score matching or contrastive divergence. For high-dimensional image data, we describe how the manifold hypothesis puts limitations on our approach and demonstrate the effectiveness of energy discrepancy by training the energy-based model as a prior of a variational decoder model.","target":"~Vincent_Stimper1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=TW3ipYdDQG","title":"Fair Streaming Principal Component Analysis: Statistical and Algorithmic Viewpoint","modified_abstract":"Building on the momentum from previous works that introduced innovative approaches for approximating statistical distances and their application in machine learning, such as the Fast Approximation of the Sliced-Wasserstein Distance through the exploitation of the concentration of random projections, we identify a significant gap in the realm of Fair Principal Component Analysis (PCA). While these foundational studies facilitated computational and statistical advancements in handling high-dimensional data, the domain of fair PCA remains underexplored, lacking a concrete statistical foundation and facing practical challenges related to memory constraints. Our work addresses these issues by rigorously formulating fair PCA using a new notion called probably approximately fair and optimal (PAFO) learnability and introducing a fair streaming PCA setting. We propose a memory-efficient algorithm, the fair noisy power method (FNPM), and provide its statistical guarantee in terms of PAFO-learnability, marking a first in the literature of fair PCA. Verified on the CelebA dataset without any pre-processing, our algorithm transcends previous approaches by efficiently and effectively performing fair PCA in a streaming setting, overcoming the inherent memory limitations. Our methodology incorporates nonasymptotical analysis and a novel sampling technique designed to handle the challenges of high-dimensional datasets, demonstrating an exemplar use case of how fair algorithms can adapt to the streaming paradigm. Notably, our approximation technique significantly contributes to the efficient processing of vector representations in high-dimensional spaces, cutting down the distance between theoretical innovation and practical application.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Kimia_Nadjahi1","manipulated_ranking":1,"natural_ranking":101,"id":"TW3ipYdDQG","original_abstract":"Fair Principal Component Analysis (PCA) is a problem setting where we aim to perform PCA while making the resulting representation fair in that the projected distributions, conditional on the sensitive attributes, match one another. However, existing approaches to fair PCA have two main problems: theoretically, there has been no statistical foundation of fair PCA in terms of learnability; practically, limited memory prevents us from using existing approaches, as they explicitly rely on full access to the entire data. On the theoretical side, we rigorously formulate fair PCA using a new notion called probably approximately fair and optimal (PAFO) learnability. On the practical side, motivated by recent advances in streaming algorithms for addressing memory limitation, we propose a new setting called fair streaming PCA along with a memory-efficient algorithm, fair noisy power method (FNPM). We then provide its statistical guarantee in terms of PAFO-learnability, which is the first of its kind in fair PCA literature. We verify our algorithm in the CelebA dataset without any pre-processing; while the existing approaches are inapplicable due to memory limitations, by turning it into a streaming setting, we show that our algorithm performs fair PCA efficiently and effectively.","target":"~Kimia_Nadjahi1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=TZtw5YgxTE","title":"MIM4DD: Mutual Information Maximization for Dataset Distillation","modified_abstract":"In the realm of machine learning, recent inquiries into multi-task and single-task learning have leveraged shared knowledge and optimization strategies, including gradient descent, to enhance model performance across diverse tasks, including training neural networks more efficiently. Drawing inspiration from these advancements, our work explores dataset distillation (DD), a technique aimed at synthesizing a small dataset that retains the generalization and test performance of its full-sized counterpart, thereby emerging as a critical tool for efficient training in settings with limited computational resources. Unlike state-of-the-art methods, which primarily rely on heuristic indicators from network comparisons and loss metrics, we introduce a principled approach utilizing mutual information (MI) to quantify the information shared between synthetic and real datasets. Through the novel MIM4DD framework, we numerically maximize MI via a contrastive learning strategy, establishing positive and negative sample pairs based on label congruency and employing an optimizable regularization objective to refine the synthetic dataset. This approach not only challenges the conventional reliance on heuristic measures but also provides a solid theoretical foundation for dataset compression and generalization. Experimentation demonstrates the efficacy of the MIM4DD as an augmentative module to existing dataset distillation methodologies, underscoring its potential to revolutionize the domain through an information-theoretic perspective.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Hoang_Viet_Phan1","manipulated_ranking":1,"natural_ranking":101,"id":"TZtw5YgxTE","original_abstract":"Dataset distillation (DD) aims to synthesize a small dataset whose test performance is comparable to a full dataset using the same model. State-of-the-art (SoTA) methods optimize synthetic datasets primarily by matching heuristic indicators extracted from two networks: one from real data and one from synthetic data (see Fig.1, Left), such as gradients and training trajectories. DD is essentially a compression problem that emphasizes on maximizing the preservation of information contained in the data. We argue that well-defined metrics which measure the amount of shared information between variables in information theory are necessary for success measurement, but are never considered by previous works. Thus, we introduce mutual information (MI) as the metric to quantify the shared information between the synthetic and the real datasets, and devise MIM4DD numerically maximizing the MI via a newly designed optimizable objective within a contrastive learning framework to update the synthetic dataset. Specifically, we designate the samples in different datasets who share the same labels as positive pairs, and vice versa negative pairs. Then we respectively pull and push those samples in positive and negative pairs into contrastive space via minimizing NCE loss. As a result, the targeted MI can be transformed into a lower bound represented by feature maps of samples, which is numerically feasible. Experiment results show that MIM4DD can be implemented as an add-on module to existing SoTA DD methods.","target":"~Hoang_Viet_Phan1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=N5uUTWLz0E","title":"Understanding and Improving Ensemble Adversarial Defense","modified_abstract":"The ensemble strategy, leveraging multiple base classifiers to withstand adversarial attacks collectively, represents a significant advancement in enhancing model robustness, mirroring progress seen in other areas of machine learning such as generative adversarial networks (GANs). This paper specifically builds upon previous insights, including those from adversarial 1-Lipschitz regularization, which underlines the importance of robust training methods in improving the resilience of models against adversarial threats via incorporating gradient-based techniques and normalization processes. Despite the demonstrated empirical success of ensemble methods in adversarial contexts, a theoretical framework to elucidate the enhanced robustness provided by the ensemble over individual adversarially trained classifiers is missing. Addressing this knowledge gap, we propose a novel error theory for ensemble adversarial defense, proving a reduction in 0-1 loss for adversarially challenging sample sets. Building on this theoretical foundation, we introduce an innovative enhancement strategy, interactive global adversarial training (iGAT), which incorporates a probabilistic distribution rule for allocating globally challenging adversarial examples among base classifiers and a regularization component targeting the most significant weaknesses within the ensemble, thereby improving the ensemble's quality of defense and leveraging networks' potential to a greater extent. Tested across a spectrum of ensemble adversarial defense mechanisms, iGAT demonstrates up to a 17% performance increase on CIFAR10 and CIFAR100 datasets under both white-box and black-box attacks, marking a significant step forward in understanding and fortifying against adversarial threats through effective training methodologies.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~D\u00e1vid_Terj\u00e9k1","manipulated_ranking":6,"natural_ranking":101,"id":"N5uUTWLz0E","original_abstract":"The strategy of ensemble has become popular in adversarial defense, which trains multiple base classifiers to defend against adversarial attacks in a cooperative manner. Despite the empirical success, theoretical explanations on why an ensemble of adversarially trained classifiers is more robust than single ones remain unclear. To fill in this gap, we develop a new error theory dedicated to understanding ensemble adversarial defense,  demonstrating a provable 0-1 loss reduction on challenging sample sets in adversarial defense scenarios. Guided by this theory, we propose an effective approach to improve ensemble adversarial defense, named interactive global adversarial training (iGAT). The proposal includes (1) a probabilistic distributing rule that selectively allocates to different base classifiers adversarial examples that are globally challenging to the ensemble, and (2) a regularization term to rescue the severest weaknesses of the base classifiers. Being tested over various existing ensemble adversarial defense techniques,  iGAT is capable of boosting their performance by up to 17\\%  evaluated using  CIFAR10 and CIFAR100 datasets under both white-box and black-box attacks.","target":"~D\u00e1vid_Terj\u00e9k1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=Eysb8t3MJ5","title":"GlucoSynth: Generating Differentially-Private Synthetic Glucose Traces","modified_abstract":"The challenge of generating high-quality, private synthetic time series data, particularly glucose traces, calls for innovative approaches that adequately address both privacy concerns and data utility. Inspired by recent advancements in temporal data modeling, such as the diffusion-based generative modeling that preserves the continuity of underlying functions in irregularly sampled time series, our work introduces GlucoSynth. This novel privacy-preserving Generative Adversarial Network (GAN) framework is tailored for the synthesis of glucose traces. By focusing on maintaining the intrinsic temporal dynamics and relationships amongst motifs in glucose data, integrating differential privacy mechanisms, and employing techniques for denoising and imputation, GlucoSynth aims to provide strong formal privacy guarantees without compromising the utility of synthetic data. Furthermore, its ability to assist in forecasting glucose levels demonstrates an extensive evaluation using 1.2 million glucose traces from stochastic datasets. GlucoSynth's superior capability in generating high-quality synthetic glucose traces with robust privacy assurances, thereby outperforms existing methods in this domain. Its application extends to various multivariate learning scenarios, illustrating a versatile approach to synthetic data generation.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Marin_Bilo\u01611","manipulated_ranking":2,"natural_ranking":101,"id":"Eysb8t3MJ5","original_abstract":"We focus on the problem of generating high-quality, private synthetic glucose traces, a task generalizable to many other time series sources. Existing methods for time series data synthesis, such as those using Generative Adversarial Networks (GANs), are not able to capture the innate characteristics of glucose data and cannot provide any formal privacy guarantees without severely degrading the utility of the synthetic data. In this paper we present GlucoSynth, a novel privacy-preserving GAN framework to generate synthetic glucose traces. The core intuition behind our approach is to conserve relationships amongst motifs (glucose events) within the traces, in addition to temporal dynamics. Our framework incorporates differential privacy mechanisms to provide strong formal privacy guarantees. We provide a comprehensive evaluation on the real-world utility of the data using 1.2 million glucose traces; GlucoSynth outperforms all previous methods in its ability to generate high-quality synthetic glucose traces with strong privacy guarantees.","target":"~Marin_Bilo\u01611"}
{"paper_link":"https:\/\/openreview.net\/forum?id=p53QDxSIc5","title":"DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction","modified_abstract":"Inspired by recent advancements in semiparametric language models for scalable continual learning, this work introduces an innovative text-to-SQL generation technique that leverages the strengths of Large Language Models (LLMs) in decomposed in-context learning complemented by a self-correction methodology. Acknowledging the significant gap between the performance of fine-tuned models and prompting approaches using LLMs on complex tasks such as text-to-SQL conversion, as evaluated on the Spider dataset, we address this challenge by dissecting the generation task into manageable sub-tasks and incorporating streaming data. By feeding solutions of these subdivided problems back into LLMs, along with an integrated approach for storing and modeling task-specific memory recall, we demonstrate a potent method for enhancing LLM reasoning capabilities, thereby markedly improving performance. Our experimentation with three LLMs, through a process involving both in-context learning and specific training strategies, reveals that this strategy consistently bolsters their few-shot learning performance by approximately 10%, propelling LLM accuracy towards or beyond current state-of-the-art (SOTA) figures. Specifically, we achieve a new SOTA execution accuracy of 85.3% on the Spider dataset's holdout test set, surpassing the previous SOTA accuracy of 79.9%. Furthermore, when evaluated against the BIRD benchmark, our methodology, applying the learner-centric approach of breaking down complex language tasks into more simplified and digestible sub-tasks, sets a new benchmark by attaining an execution accuracy of 55.9%, thereby establishing a new SOTA on its holdout test set. This approach not only closes the performance gap between LLMs and fine-tuned models but also showcases the potential of decomposed in-context learning combined with self-correction for significantly improving the efficacy of LLMs in complex reasoning tasks.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Tao_Ge1","manipulated_ranking":1,"natural_ranking":101,"id":"p53QDxSIc5","original_abstract":"There is currently a significant gap between the performance of fine-tuned models and prompting approaches using Large Language Models (LLMs) on the challenging task of text-to-SQL, as evaluated on datasets such as Spider. To improve the performance of LLMs in the reasoning process, we study how decomposing the task into smaller sub-tasks can be effective. In particular, we show that breaking down the generation problem into sub-problems and feeding the solutions of those sub-problems into LLMs can be an effective approach for significantly improving their performance.  Our experiments with three LLMs show that this approach consistently improves their simple few-shot performance by roughly 10%, pushing the accuracy of LLMs towards SOTA or surpassing it. On the holdout test set of Spider, the SOTA, in terms of execution accuracy, was 79.9 and the new SOTA at the time of  this writing using our approach is 85.3. Our approach with in-context learning beats many heavily fine-tuned models by at least 5%. Additionally, when evaluated on the BIRD benchmark, our approach achieved an execution accuracy of 55.9%, setting a new SOTA on its holdout test set.","target":"~Tao_Ge1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=l6ypbj6Nv5","title":"Generative Category-level Object Pose Estimation via Diffusion Models","modified_abstract":"Influenced by the recent achievements in self-supervised learning paradigms, specifically in generating realistic repositioning of humans through exploiting 3D body information from videos, our work explores object pose estimation from a novel angle. Object pose estimation is essential in embodied AI and computer vision, facilitating intelligent agents to perceive and manipulate their environment. Unlike existing methods that often struggle with the multihypothesis issue arising from partially observed point clouds in category-level pose estimation, we propose a paradigm shift by formulating the task as conditional generative modeling within a novel framework. By employing score-based diffusion models, our method generates object pose estimations through a systematic process; initially sampling pose candidates from the diffusion model followed by outlier filtering via likelihood estimation and averaging the survivors through mean pooling. To circumvent the computationally intensive integration for likelihood estimation, we introduce a more efficient technique that employs an energy-based model distilled from the original score-based diffusion models during training. This not only enhances end-to-end processing but also sets new benchmarks on the REAL275 dataset, achieving unparalleled accuracy rates exceeding 50% and 60% on stringent 5\u00b0+2cm and 5\u00b0+5cm metrics, respectively. Moreover, our approach, rooted in self-supervision and a comprehensive training strategy, exhibits exceptional generalization capabilities to unseen object categories without the need for category-specific fine-tuning and extends naturally to tasks beyond static pose estimation, including object pose tracking, rivaling top-performing baselines. Our checkpoints and demonstrations can be viewed at [Link redacted for privacy].","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Soubhik_Sanyal1","manipulated_ranking":1,"natural_ranking":101,"id":"l6ypbj6Nv5","original_abstract":"Object pose estimation plays a vital role in embodied AI and computer vision, enabling intelligent agents to comprehend and interact with their surroundings. Despite the practicality of category-level pose estimation, current approaches encounter challenges with partially observed point clouds, known as the multihypothesis issue. In this study, we propose a novel solution by reframing categorylevel object pose estimation as conditional generative modeling, departing from traditional point-to-point regression. Leveraging score-based diffusion models, we estimate object poses by sampling candidates from the diffusion model and aggregating them through a two-step process: filtering out outliers via likelihood estimation and subsequently mean-pooling the remaining candidates. To avoid the costly integration process when estimating the likelihood, we introduce an alternative method that distils an energy-based model from the original score-based model, enabling end-to-end likelihood estimation. Our approach achieves state-of-the-art performance on the REAL275 dataset, surpassing 50% and 60% on strict 5 \u25e6 2cm and 5 \u25e6 5cm metrics, respectively. Furthermore, our method demonstrates strong generalization to novel categories without the need for fine-tuning and can readily adapt to object pose tracking tasks, yielding comparable results to the current state-of-the-art baselines. Our checkpoints and demonstrations can be found at https:\/\/sites.google.com\/view\/genpose.","target":"~Soubhik_Sanyal1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=AesN5bYnJr","title":"No-Regret Online Prediction with Strategic Experts","modified_abstract":"Our study advances the online binary prediction with expert advice framework, incorporating insights from prior work on nonstationary dual averaging and online fair allocation of items. This prior work addresses the challenge of fair allocation with online item arrivals and suggests that algorithmic approaches, including averaging techniques, can adapt to varying conditions while maintaining fairness and efficiency in the presence of adversarial behaviors. Building on this, we generalize the learning prediction framework to allow a learner to select $m\\geq 1$ experts from a pool of $K$ and consider strategies in which experts aim to maximize their influence on the algorithm's predictions, potentially misreporting their beliefs about future events, including those with periodic characteristics. This setting finds relevance in contexts such as forecasting competitions that require not only aggregating forecasts but also ranking forecasters by performance, arriving at a mechanism that is resilient to corruption in the form of misinformation. Our objective is to devise algorithms that are both incentive-compatible, encouraging truthful reporting of beliefs, and no-regret, ensuring that the regret relative to the true beliefs of the best fixed set of $m$ experts in hindsight is sublinear. We demonstrate that the straightforward application of existing $m=1$ solutions is inadequate and introduce algorithms that leverage the unique characteristics of the utility functions to meet our dual objectives of incentivizing truthfulness and achieving sublinear regret.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Luofeng_Liao1","manipulated_ranking":1,"natural_ranking":101,"id":"AesN5bYnJr","original_abstract":"We study a generalization of the online binary prediction with expert advice framework where at each round, the learner is allowed to pick $m\\geq 1$ experts from a pool of $K$ experts and the overall utility is a modular or submodular function of the chosen experts. We focus on the setting in which experts act strategically and aim to maximize their influence on the algorithm's predictions by potentially misreporting their beliefs about the events. Among others, this setting finds applications in forecasting competitions where the learner seeks not only to make predictions by aggregating different forecasters but also to rank them according to their relative performance. Our goal is to design algorithms that satisfy the following two requirements: 1) \\emph{Incentive-compatible}: Incentivize the experts to report their beliefs truthfully, and 2) \\emph{No-regret}: Achieve sublinear regret with respect to the true beliefs of the best fixed set of $m$ experts in hindsight. Prior works have studied this framework when $m=1$ and provided incentive-compatible no-regret algorithms for the problem. We first show that a simple reduction of our problem to the $m=1$ setting is neither efficient nor effective. Then, we provide algorithms that utilize the specific structure of the utility functions to achieve the two desired goals.","target":"~Luofeng_Liao1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=v7WWesSiOu","title":"End-To-End Latent Variational Diffusion Models for Inverse Problems in High Energy Physics","modified_abstract":"In the domain of high-energy physics, particularly within the context of experiments conducted at the Large Hadron Collider (LHC), the complexity of dealing with detector effects presents a significant challenge in translating detector observations to theoretical quantities that underpin the collisions observed. This challenge mirrors the intricacies encountered in learning dynamical models, such as those described by Neural Dynamical Systems (NDS), where dynamical systems incorporating equations with prior knowledge have shown the potential for high accuracy in predicting future states with fewer samples. In this vein, our work investigates and compares various generative deep learning methods aiming to approximate the inverse mapping required to correct detector effects and align measurements with theoretical predictions. We introduce latent variational diffusion models, a novel architecture that synergizes the latent learning capabilities of advanced generative methods with an end-to-end variational framework, addressing the inverse problems inherent in high-energy physics data analysis. Our approach demonstrates superior performance in reconstructing global distributions of theoretical kinematic quantities and ensuring learned posterior distributions adhere strictly to known physics constraints. Compared to non-latent state-of-the-art baselines and conventional latent diffusion models, our unified method achieves a distribution-free distance to truth metrics significantly reduced, indicating its potential in enhancing the precision and reliability of particle physics analyses, even with data collected irregularly from detectors. The deep integration of variational principles and diffusion processes presents a comprehensive solution for inverse problems, contributing significant advancements in both theory and application.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Viraj_Mehta1","manipulated_ranking":3,"natural_ranking":101,"id":"v7WWesSiOu","original_abstract":"High-energy collisions at the Large Hadron Collider (LHC) provide valuable insights into open questions in particle physics. However, detector effects must be corrected before measurements can be compared to certain theoretical predictions or measurements from other detectors. Methods to solve this inverse problem of mapping detector observations to theoretical quantities of the underlying collision are essential parts of many physics analyses at the LHC. We investigate and compare various generative deep learning methods to approximate this inverse mapping. We introduce a novel unified architecture, termed latent variational diffusion models, which combines the latent learning of cutting-edge generative art approaches with an end-to-end variational framework. We demonstrate the effectiveness of this approach for reconstructing global distributions of theoretical kinematic quantities, as well as for ensuring the adherence of the learned posterior distributions to known physics constraints. Our unified approach achieves a distribution-free distance to the truth of over 20 times smaller than non-latent state-of-the-art baseline and 3 times smaller than traditional latent diffusion models.","target":"~Viraj_Mehta1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=8pOBo5NgTQ","title":"FLSL: Feature-level Self-supervised Learning","modified_abstract":"In the context of self-supervised learning (SSL), there has been a significant focus on instance-level representations, particularly through methods like SimCLR, DINO, VICReg, and MOCOv3, which often fall short in dense prediction tasks such as object detection and segmentation. This limitation points to an unaddressed gap between existing SSL approaches and the requirements of dense prediction tasks. Building on the foundational insights from previous works, such as EHSOD's exploration of hybrid-supervised object detection and the effective use of class activation maps for foreground proposal and refinement, our study introduces a novel paradigm in SSL. We leverage the Vision Transformers (ViT) for the first time to elucidate the mean-shift clustering process aligned with natural image semantics, thereby proposing the bi-level feature clustering method called Feature-Level Self-supervised Learning (FLSL). FLSL advances SSL by formalizing the problem from both the mean-shift and k-means perspectives, fostering semantic cluster representations that significantly enhance performance in dense prediction tasks through classification accuracy improvement. Empirical results demonstrate that FLSL achieves notable improvements in object detection and instance segmentation on MS-COCO, using Mask R-CNN with ViT backbones, and extends its applicability to UAV object detection and video instance segmentation across various datasets. The use of bounding-box annotations within a semi-automated cascade system for segmentation tasks is discussed, underscoring FLSL's ability to address tasks requiring complex spatial relationships through weakly-annotated data. Through extensive experimentation, visualization, and ablation studies, including the updating of benchmarks and activation process adjustments, our research substantiates the efficacy of FLSL and its superiority over traditional SSL methods in addressing the nuances of dense prediction challenges, with a special focus on the contribution of the detector component. The abstract also includes a call for further exploration into FLSL's mechanisms and potential applications, emphasizing the shift towards feature-level self-supervision as a promising direction for future SSL research.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Zhili_LIU1","manipulated_ranking":1,"natural_ranking":101,"id":"8pOBo5NgTQ","original_abstract":"Current self-supervised learning (SSL) methods (e.g., SimCLR, DINO, VICReg, MOCOv3) target primarily on representations at instance level and do not generalize well to dense prediction tasks, such as object detection and segmentation. Towards aligning SSL with dense predictions, this paper demonstrates for the first time the underlying mean-shift clustering process of Vision Transformers (ViT), which aligns well with natural image semantics (e.g., a world of objects and stuffs). By employing transformer for joint embedding and clustering, we propose a bi-level feature clustering SSL method, coined Feature-Level Self-supervised Learning (FLSL). We present the formal definition of the FLSL problem and construct the objectives from the mean-shift and k-means perspectives. We show that FLSL promotes remarkable semantic cluster representations and learns an embedding scheme amenable to intra-view and inter-view feature clustering. Experiments show that FLSL yields significant improvements in dense prediction tasks, achieving 44.9 (+2.8)% AP and 46.5% AP in object detection, as well as 40.8 (+2.3)% AP and 42.1% AP in instance segmentation on MS-COCO, using Mask R-CNN with ViT-S\/16 and ViT-S\/8 as backbone, respectively. FLSL consistently outperforms existing SSL methods across additional benchmarks, including UAV object detection on UAVDT, and video instance segmentation on DAVIS 2017. We conclude by presenting visualization and various ablation studies to better understand the success of FLSL. The source code is available at https:\/\/github.com\/ISL-CV\/FLSL.","target":"~Zhili_LIU1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=UlHueVjAKr","title":"Textually Pretrained Speech Language Models","modified_abstract":"Recent advancements in natural language understanding, exemplified by comprehensive benchmarks such as the Korean Language Understanding Evaluation (KLUE), demonstrate the potential for leveraging pretrained textual language models to enhance speech language models (SpeechLMs). In this context, our work introduces TWIST, a novel method for training SpeechLMs using a warm start from pretrained textual language models. TWIST outperforms traditional cold-start SpeechLMs in both automatic and human evaluations, highlighting the benefits of textually pretrained foundations for speech processing tasks. Our empirical analysis investigates various model design choices, including the speech tokenizer, the pretrained textual model, and dataset size, revealing the significant impact of multilingual capabilities, model, and dataset scale on SpeechLM performance. Our contributions, including speech samples, code, and models, are made publicly available as open-source to encourage community engagement and advancement in the field. Additionally, we introduce spoken versions of the StoryCloze benchmark to enhance model evaluation and stimulate further research. This effort towards making SpeechLMs more cross-lingual and identifiable in varied linguistic contexts further underlines the importance of broad understanding and ethical considerations in speech recognition tasks. We present the largest SpeechLM to date in terms of parameters and training data volume, based on our findings.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Jamin_Shin1","manipulated_ranking":1,"natural_ranking":101,"id":"UlHueVjAKr","original_abstract":"Speech language models (SpeechLMs) process and generate acoustic data only, without textual supervision. In this work, we propose TWIST, a method for training SpeechLMs using a warm-start from a pretrained textual language models. We show using both automatic and human evaluations that TWIST outperforms a cold-start SpeechLM across the board. We empirically analyze the effect of different model design choices such as the speech tokenizer, the pretrained textual model, and the dataset size. We find that model and dataset scale both play an important role in constructing better-performing SpeechLMs. Based on our observations, we present the largest (to the best of our knowledge) SpeechLM both in terms of number of parameters and training data. We additionally introduce two spoken versions of the StoryCloze textual benchmark to further improve model evaluation and advance future research in the field. We make speech samples, code and models publicly available.","target":"~Jamin_Shin1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=MOAHXRzHhm","title":"Enhancing Adversarial Robustness via Score-Based Optimization","modified_abstract":"The vulnerability of deep neural network classifiers to adversarial attacks highlights the pressing need for developing effective defense mechanisms. This urgency is underscored by recent research into model fingerprinting, which seeks to protect model intellectual property against stealing attacks by analyzing model behavior under adversarial conditions and establishing a fingerprint or unique identifier for the models. Drawing inspiration from these concerns and the correlation between model robustness and susceptibility to intellectual property theft, our paper introduces ScoreOpt, a novel adversarial defense mechanism that leverages score-based diffusion models. Unlike traditional diffusion-based defenses that rely on computationally expensive and suboptimal sequential simulations, ScoreOpt employs a test-time optimization of adversarial samples towards their original, clean data forms, using score-based priors. Our training methodology enhances the transferability of this defense to various tasks and datasets. We evaluate the efficacy of our approach through extensive experiments on CIFAR10, CIFAR100, and ImageNet, showcasing significant enhancements in both robustness against adversarial attacks and inference speed compared to existing methods. Additionally, the introduction of a surrogate model during the optimization process further bolsters our defense mechanism's efficiency. This advancement offers a dual contribution: providing a formidable defense against adversarial threats and advancing the conversation on safeguarding neural networks from intellectual property theft through improved robustness.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Jiyang_Guan1","manipulated_ranking":1,"natural_ranking":101,"id":"MOAHXRzHhm","original_abstract":"Adversarial attacks have the potential to mislead deep neural network classifiers by introducing slight perturbations. Developing algorithms that can mitigate the effects of these attacks is crucial for ensuring the safe use of artificial intelligence. Recent studies have suggested that score-based diffusion models are effective in adversarial defenses. However, existing diffusion-based defenses rely on the sequential simulation of the reversed stochastic differential equations of diffusion models, which are computationally inefficient and yield suboptimal results. In this paper, we introduce a novel adversarial defense scheme named ScoreOpt, which optimizes adversarial samples at test-time, towards original clean data  in the direction guided by score-based priors. We conduct comprehensive experiments on multiple datasets, including CIFAR10, CIFAR100 and ImageNet. Our experimental results demonstrate that our approach outperforms existing adversarial defenses in terms of both robustness performance and inference speed.","target":"~Jiyang_Guan1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=IoizwO1NLf","title":"Skill-it! A data-driven skills framework for understanding and training language models","modified_abstract":"Inspired by the recognition that both the quantity and quality of data are critical for machine learning (ML) performance, as demonstrated in prior works focused on general goal-reaching policies in reinforcement learning, our study investigates the impact of training data selection on pre-trained large language models (LMs) across various tasks. With a fixed budget of tokens, we aim to determine the most effective way to select data that enhances downstream performance of LMs, embodying an exploration of data utility for optimal training efficiency. We propose a novel framework premised on the hypothesis that, akin to human skill acquisition following a deliberate sequence, language models similarly learn from training data in a natural, ordered manner. This framework not only formalizes the concept of a skill and an ordered skill set in terms of associated data but also introduces an online data sampling algorithm, Skill-It, that leverages skill mixtures for both continual pre-training and fine-tuning to efficiently learn multiple or individual skills, respectively. Utilizing synthetic and real datasets, we affirm the existence of these ordered skill sets, showing that prerequisite skills facilitate the learning of more complex skills with less data. The Skill-It controller uses a deliberate pose-achievement approach in data selection to mirror targeted learning objectives from prerequisite tasks to more complex ones, ensuring that each task selected aligns with the model's current learning trajectory. In the continual pre-training setting on the LEGO synthetic dataset, Skill-It achieves 37.5 points higher accuracy than random sampling, highlighting its aptitude for structured exploration. Furthermore, in the fine-tuning context on the Natural Instructions dataset, it yields a 13.6% reduction in validation loss for the targeted skill compared to training directly on data associated with that skill. When applied to the RedPajama dataset for continual pre-training of a 3B-parameter LM, our framework accomplishes superior accuracy on the LM Evaluation Harness using 1B tokens compared to a baseline sampling approach across data sources with 3B tokens.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Alexis_Jacq1","manipulated_ranking":1,"natural_ranking":101,"id":"IoizwO1NLf","original_abstract":"The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training. Using this intuition, our framework formalizes the notion of a skill and of an ordered set of skills in terms of the associated data. First, using both synthetic and real data, we demonstrate that these ordered skill sets exist, and that their existence enables more advanced skills to be learned with less data when we train on their prerequisite skills. Second, using our proposed framework, we introduce an online data sampling algorithm, Skill-It, over mixtures of skills for both continual pre-training and fine-tuning regimes, where the objective is to efficiently learn multiple skills in the former and an individual skill in the latter. On the LEGO synthetic in the continual pre-training setting, Skill-It obtains 37.5 points higher accuracy than random sampling. On the Natural Instructions dataset in the fine-tuning setting, Skill-It reduces the validation loss on the target skill by 13.6% versus training on data associated with the target skill itself. \nWe apply our skills framework on the RedPajama dataset to continually pre-train a 3B-parameter LM, achieving higher accuracy on the LM Evaluation Harness with 1B tokens than the baseline approach of sampling uniformly over data sources with 3B tokens.","target":"~Alexis_Jacq1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=QlHosp050r","title":"Weakly-Supervised Concealed Object Segmentation with SAM-based Pseudo Labeling and Multi-scale Feature Grouping","modified_abstract":"Inspired by the recent surge in unsupervised and weakly-supervised segmentation approaches, notably in complex scenes involving multiple objects with diverse textures, as evidenced by the ClevrTex benchmark, our research advances the domain of Weakly-Supervised Concealed Object Segmentation (WSCOS). WSCOS aims to segment objects well-integrated with their surroundings using minimally annotated data. This task presents pronounced challenges, including the difficulty of distinguishing concealed objects from their backgrounds due to intrinsic similarity, and limitations in model learning imposed by sparse annotations. In response, we introduce a novel WSCOS method aimed at overcoming these obstacles. To address the complexity and similarity between objects and their environment, we devise a multi-scale feature grouping module that synergizes features at diverse granularities, promoting segmentation coherence and facilitating comprehensive segmentation outcomes for images containing single or multiple objects with intricate textures, shapes, and developed variants. For the weak supervision hurdle, we leverage the Segment Anything Model (SAM), a recently proposed vision foundation model. Utilizing sparse annotations as prompts, we generate segmentation masks that serve as training inputs to the model. To mitigate the effects of low-quality masks, our methodology includes a suite of strategies such as result ensemble through multi-augmentation, entropy-based pixel-level weighting, image-level selection criteria founded on entropy, and employing texture representations in the learning process. These strategies collectively fortify the training process, ensuring more dependable supervision. Our evaluation across numerous WSCOS tasks corroborates the superior performance of our approach, establishing new benchmarks in the field.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Laurynas_Karazija1","manipulated_ranking":1,"natural_ranking":101,"id":"QlHosp050r","original_abstract":"Weakly-Supervised Concealed Object Segmentation (WSCOS) aims to  segment objects well blended with surrounding environments using sparsely-annotated data for model training. It remains a challenging task since (1) it is hard to distinguish concealed objects from the background due to the intrinsic similarity and (2) the sparsely-annotated training data only provide weak supervision for model learning. In this paper, we propose a new WSCOS method to address these two challenges. To tackle the intrinsic similarity challenge, we design a multi-scale feature grouping module that first groups features at different granularities and then aggregates these grouping results. By grouping similar features together, it encourages segmentation coherence, helping obtain complete segmentation results for both single and multiple-object images. For the weak supervision challenge, we utilize the recently-proposed vision foundation model, ``Segment Anything Model (SAM)'', and use the provided sparse annotations as prompts to generate segmentation masks, which are used to train the model. To alleviate the impact of low-quality segmentation masks, we further propose a series of strategies, including multi-augmentation result ensemble, entropy-based pixel-level weighting, and entropy-based image-level selection. These strategies help provide more reliable supervision to train the segmentation model. We verify the effectiveness of our method on various WSCOS tasks, and experiments demonstrate that our method achieves state-of-the-art performance on these tasks.","target":"~Laurynas_Karazija1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=dCAk9VlegR","title":"This Looks Like Those: Illuminating Prototypical Concepts Using Multiple Visualizations","modified_abstract":"Drawing insight from recent advancements in post-hoc explanation methods and contrastive learning, our work introduces ProtoConcepts, a novel approach to interpretable image classification. This approach is rooted in combining the effective aspects of deep learning with case-based reasoning through prototypical parts, echoing the \"this looks like that\" reasoning in prototype-based image classification yet innovating beyond the limitations of existing methods. Existing prototype-based classifiers limit explanations to one-to-one image patch comparisons, often obscuring the broader conceptual similarities they aim to highlight. By learning and visualizing prototypical concepts through multiple image patches, ProtoConcepts clarifies the basis of its comparisons\u2014be it color, shape, or another underlying concept\u2014enabling more nuanced and interpretable visual explanations. This method proposes a significant departure from single-image comparisons, drawing upon the foundational work in enhancing model consistency, interpretability through contrastive and self-supervised learning techniques, and improving generalization. Our experimental validation across benchmark datasets demonstrates that this enriched \"this looks like those\" reasoning process, coupled with robust training protocols and optimized embeddings generation, can be seamlessly integrated into existing prototypical image classification frameworks without sacrificing accuracy, thus offering a richer, more interpretable model for visual explanations that acts on complex network representations.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Soroush_Abbasi_Koohpayegani1","manipulated_ranking":1,"natural_ranking":101,"id":"dCAk9VlegR","original_abstract":"We present ProtoConcepts, a method for interpretable image classification combining deep learning and case-based reasoning using prototypical parts. Existing work in prototype-based image classification uses a \"this looks like that'' reasoning process, which dissects a test image by finding prototypical parts and combining evidence from these prototypes to make a final classification. However, all of the existing prototypical part-based image classifiers provide only one-to-one comparisons, where a single training image patch serves as a prototype to compare with a part of our test image. With these single-image comparisons, it can often be difficult to identify the underlying concept being compared (e.g., \"is it comparing the color or the shape?''). Our proposed method modifies the architecture of prototype-based networks to instead learn prototypical concepts which are visualized using multiple image patches. Having multiple visualizations of the same prototype allows us to more easily identify the concept captured by that prototype (e.g., \"the test image and the related training patches are all the same shade of blue''), and allows our model to create richer, more interpretable visual explanations. Our experiments show that our ``this looks like those'' reasoning process can be applied as a modification to a wide range of existing prototypical image classification networks while achieving comparable accuracy on benchmark datasets.","target":"~Soroush_Abbasi_Koohpayegani1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=iM0MWWBr4W","title":"A Unified Model and Dimension for Interactive Estimation","modified_abstract":"Our work is inspired by a variety of previous research efforts that have tackled complex problems within interactive learning, such as those addressing challenges in the domain of low-rank matrix completion. We study an abstract framework for interactive learning called interactive estimation, in which the goal is to estimate a target from its ``similarity'' to points queried by the learner. To approach challenges iteratively, we introduce a combinatorial measure called Dissimilarity dimension which largely captures learnability in our model. We present a simple, general, and broadly-applicable algorithm, interpreted as leveraging iterative techniques, for which we obtain both regret and PAC generalization bounds that are polynomial in the new dimension. Our methodological contributions draw on iterative techniques and leverage the structure of the problem, akin to applying squares of error terms in optimization problems for completion of low-rank matrices. We show that our framework subsumes and thereby unifies two classic learning models: statistical-query learning and structured bandits. We also delineate how the Dissimilarity dimension is related to well-known parameters for both frameworks, in some cases yielding significantly improved analyses.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Christian_K\u00fcmmerle1","manipulated_ranking":1,"natural_ranking":101,"id":"iM0MWWBr4W","original_abstract":"We study an abstract framework for interactive learning called interactive estimation in which the goal is to estimate a target from its ``similarity'' to points queried by the learner.\nWe introduce a combinatorial measure called Dissimilarity dimension which largely captures learnability in our model.\nWe present a simple, general, and broadly-applicable algorithm, for which we obtain both regret and PAC generalization bounds that are polynomial in the new dimension. We show that our framework subsumes and thereby unifies two classic learning models:\nstatistical-query learning and structured bandits. We also delineate how the Dissimilarity dimension is related to well-known parameters for both frameworks, in some cases yielding significantly improved analyses.","target":"~Christian_K\u00fcmmerle1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=fmYmXNPmhv","title":"Permutation Equivariant Neural Functionals","modified_abstract":"In the context of expanding the capabilities and understanding of neural network architectures, particularly through the exploration of differentiable Neural Architecture Search (NAS) techniques via gradient-based methods, this work presents an investigation into designing neural networks capable of interacting with the weights or gradients of other neural networks, termed *neural functional networks* (NFNs). The pursuit of NFNs is imbued with the challenge of addressing the permutation symmetries inherent in the weights of deep feedforward networks, given that hidden layer neurons possess no inherent order. Our framework proposes *permutation equivariant* neural functionals as a solution, with architecture selection inductively biased by these symmetries and propelled by *NF-Layers* (neural functional layers) designed under constraints for permutation equivariance through an appropriate parameter sharing protocol. This novel exploration discovers that permutation equivariant neural functionals propose effectiveness across a breadth of tasks including predicting classifier generalization, generating \"winning ticket\" sparsity masks for initializations, and conducting classification or modifications of implicit neural representations (INRs). Furthermore, despite the broad applicability seen in prior works like the reevaluation of architecture selection in differentiable NAS, which informed enhancements in architecture optimization processes via supernet performance evaluation, our study bridges a critical methodological gap by offering a unifying principle for the design of architectures processing network weights and conducting a perturbation-based analysis on super network performance.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Ruochen_Wang2","manipulated_ranking":1,"natural_ranking":101,"id":"fmYmXNPmhv","original_abstract":"This work studies the design of neural networks that can process the weights or gradients of other neural networks, which we refer to as *neural functional networks* (NFNs). Despite a wide range of potential applications, including learned optimization, processing implicit neural representations, network editing, and policy evaluation, there are few unifying principles for designing effective architectures that process the weights of other networks. We approach the design of neural functionals through the lens of symmetry, in particular by focusing on the permutation symmetries that arise in the weights of deep feedforward networks because hidden layer neurons have no inherent order. We introduce a framework for building *permutation equivariant* neural functionals, whose architectures encode these symmetries as an inductive bias. The key building blocks of this framework are *NF-Layers* (neural functional layers) that we constrain to be permutation equivariant through an appropriate parameter sharing scheme. In our experiments, we find that permutation equivariant neural functionals are effective on a diverse set of tasks that require processing the weights of MLPs and CNNs, such as predicting classifier generalization, producing \"winning ticket\" sparsity masks for initializations, and classifying or editing implicit neural representations (INRs). In addition, we provide code for our models and experiments at https:\/\/github.com\/AllanYangZhou\/nfn.","target":"~Ruochen_Wang2"}
{"paper_link":"https:\/\/openreview.net\/forum?id=X3IeHRD0zf","title":"Causal Imitability Under Context-Specific Independence Relations","modified_abstract":"The exploration of causal mechanisms in machine learning, especially in the context of imitation learning, has grown to acknowledge significant gaps when causal structures are ignored. This realization nests within broader research efforts, illustrated by studies such as the utilization of amortized inference for causal structure learning, which highlights the complexities and challenges of discerning causal relationships from data. Against this backdrop, our work introduces a novel perspective by investigating the role of context-specific independence (CSI) relations in causal imitation learning. CSI represents independence conditions that only apply under specific circumstances, which, hitherto, have not been fully integrated into the assessment of imitation learning's feasibility or its susceptibility to causal confounding and misspecification. Leveraging structures and graphs to model these relations, we address the gap by rigorously proving that determining the feasibility of imitation learning, given known CSI relations, is NP-hard. Furthermore, we establish a necessary graphical criterion for causal imitation under these conditions and, with an additional structural assumption, demonstrate its sufficiency. To navigate the complexities introduced by CSI in causal learning, we also propose a sound algorithmic approach that leverages both CSI relations and observational data for searching through potential imitable instances with interventional strategies, hence forwarding the discourse on how additional causal information can refine imitation learning strategies.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Scott_Sussex1","manipulated_ranking":1,"natural_ranking":101,"id":"X3IeHRD0zf","original_abstract":"Drawbacks of ignoring the causal mechanisms when performing imitation learning have recently been acknowledged. Several approaches both to assess the feasibility of imitation and to circumvent causal confounding and causal misspecifications have been proposed in the literature.\nHowever, the potential benefits of the incorporation of additional information about the underlying causal structure are left unexplored.\nAn example of such overlooked information is context-specific independence (CSI), i.e., independence that holds only in certain contexts.\nWe consider the problem of causal imitation learning when CSI relations are known.\nWe prove that the decision problem pertaining to the feasibility of imitation in this setting is NP-hard.\nFurther, we provide a necessary graphical criterion for imitation learning under CSI and show that under a structural assumption, this criterion is also sufficient.\nFinally, we propose a sound algorithmic approach for causal imitation learning which takes both CSI relations and data into account.","target":"~Scott_Sussex1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=zpVCITHknd","title":"Towards Personalized Federated Learning via Heterogeneous Model Reassembly","modified_abstract":"Building on the insights from recent advancements in knowledge distillation techniques, specifically the transfer of knowledge in heterogeneous model ensembles for improving top-K recommendations, this paper introduces a novel framework, pFedHR, aimed at tackling the challenge of model heterogeneity in federated learning environments. Within federated learning, the disparity in client models' structures poses significant hurdles towards achieving efficient and personalized learning outcomes. By conceptualizing the issue of model heterogeneity as a model-matching optimization problem, pFedHR facilitates personalized federated learning through the reassembly of heterogeneous models, including both teacher and student components for enhanced distillation efficiency. This approach, inspired by the principles of transferring nuanced knowledge between diverse models and overcoming the training bottleneck in knowledge distillation, allows for the dynamic generation of personalized model candidates with minimal manual oversight, advancing recommender systems. The proposed framework, pFedHR, not only demonstrates superior performance over established baselines across various datasets under both IID and Non-IID settings but also actively mitigates the challenges posed by the utilization of public data, which may significantly diverge in distribution from client-specific data. Through rigorous experimentation, pFedHR showcases its capacity to not only adaptively manage the integration of heterogeneity within client models but also to refine the personalization process, emphasizing its potential to enhance efficiency and effectiveness in federated learning paradigms. Furthermore, it leverages the teacher\u2019s ranking knowledge to orchestrate a more intelligent ensemble of client models for personalized recommendations.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~WONBIN_KWEON1","manipulated_ranking":1,"natural_ranking":101,"id":"zpVCITHknd","original_abstract":"This paper focuses on addressing the practical yet challenging problem of model heterogeneity in federated learning, where clients possess models with different network structures. To track this problem, we propose a novel framework called pFedHR, which leverages heterogeneous model reassembly to achieve personalized federated learning. In particular, we approach the problem of heterogeneous model personalization as a model-matching optimization task on the server side. Moreover, pFedHR automatically and dynamically generates informative and diverse personalized candidates with minimal human intervention. Furthermore, our proposed heterogeneous model reassembly technique mitigates the adverse impact introduced by using public data with different distributions from the client data to a certain extent. Experimental results demonstrate that pFedHR outperforms baselines on three datasets under both IID and Non-IID settings. Additionally, pFedHR effectively reduces the adverse impact of using different public data and dynamically generates diverse personalized models in an automated manner.","target":"~WONBIN_KWEON1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=jgIrJeHHlz","title":"Debiasing Scores and Prompts of 2D Diffusion for View-consistent Text-to-3D Generation","modified_abstract":"Inspired by recent advances in neural rendering and text-to-3D synthesis, notably the editable free-viewpoint video generation using layered neural representations, this study investigates the view inconsistency problem inherent in current score-distilling text-to-3D generation techniques. These techniques often face the Janus problem, where the canonical view of an object appears incorrectly in other views due to biases in 2D diffusion models and inadequate volume parsing. By examining existing frameworks, we pinpoint the embedded bias of 2D diffusion models as the primary cause. To address this, we propose two novel debiasing methods: score debiasing, which involves truncating the score estimated by 2D diffusion models with an incrementally increasing truncation value, and prompt debiasing, which rectifies discrepancies between user and view prompts using a language model, leveraging self-supervised learning strategies to refine the process. Our experiments demonstrate that these approaches not only enhance the realism of the generated 3D objects by reducing artifacts but also balance fidelity to the original 2D models with 3D view consistency, adding minimal overhead. Through our efforts, we contribute toward resolving significant challenges in achieving view-consistent text-to-3D generation, fostering further advancements in 3D modeling from textual descriptions. The introduction of motion-aware techniques and the utilization of multiple cameras for training further enhance the editing capabilities, ensuring that the generated 3D models adhere closely to the desired perspectives and motions, thus making significant strides in both neural representation and volume synthesis for free-viewpoint video generation.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Xinhang_Liu1","manipulated_ranking":1,"natural_ranking":101,"id":"jgIrJeHHlz","original_abstract":"Existing score-distilling text-to-3D generation techniques, despite their considerable promise, often encounter the view inconsistency problem. One of the most notable issues is the Janus problem, where the most canonical view of an object (\\textit{e.g}., face or head) appears in other views. In this work, we explore existing frameworks for score-distilling text-to-3D generation and identify the main causes of the view inconsistency problem---the embedded bias of 2D diffusion models. Based on these findings, we propose two approaches to debias the score-distillation frameworks for view-consistent text-to-3D generation. Our first approach, called score debiasing, involves cutting off the score estimated by 2D diffusion models and gradually increasing the truncation value throughout the optimization process. Our second approach, called prompt debiasing, identifies conflicting words between user prompts and view prompts using a language model, and adjusts the discrepancy between view prompts and the viewing direction of an object. Our experimental results show that our methods improve the realism of the generated 3D objects by significantly reducing artifacts and achieve a good trade-off between faithfulness to the 2D diffusion models and 3D consistency with little overhead. Our project page is available at~\\url{https:\/\/susunghong.github.io\/Debiased-Score-Distillation-Sampling\/}.","target":"~Xinhang_Liu1"}
{"paper_link":"https:\/\/openreview.net\/forum?id=GqtpYUCwnu","title":"$\\varepsilon$-fractional core stability in Hedonic Games.","modified_abstract":"Hedonic Games (HGs) are a classical framework modeling coalition formation of strategic agents guided by their individual preferences, drawing inspiration from contemporary research avenues exploring optimization and fairness in algorithmic contexts, such as fair assortment planning in online platforms and the operational design of crowd-powered retail stores. This prior work has illuminated the challenges and opportunities inherent in designing systems that balance individual and collective well-being, a principle directly applicable to the study of HGs. In HGs, the pursuit of stability within coalition structures\u2014where agents' grouping preferences are paramount\u2014is often stymied by the rarity of core-stable partitions and the computational difficulty of identifying them. Addressing these limitations, we introduce the concept of $\\varepsilon$-fractional core-stability, a relaxation allowing for up to an $\\varepsilon$-fraction of all possible coalitions to core-block, significantly enhancing the tractability and existence of stable solutions. By developing efficient algorithms that employ oracles for evaluating the stability criteria and yield $\\varepsilon$-fractional core-stable partitions with $\\varepsilon$ diminishing exponentially with the size of the agent set for two fundamental classes of HGs (Simple Fractional and Anonymous), we demonstrate that this concept not only promotes the feasibility of achieving stability but does so within a polynomial time frame. Moreover, by examining the probabilistic nature of $\\varepsilon$-fractional core stability\u2014equating it with the probability of uniformly sampled coalitions core-blocking below $\\varepsilon$\u2014and extending this framework to accommodate more nuanced sampling distributions and PAC-learning scenarios, we offer new insights into the dynamic interplay between theoretical construct and practical application through diverse datasets. Our analysis reveals both positive and negative results regarding the ability to compute $\\varepsilon$-fractional core-stable outcomes efficiently across different distributional assumptions, marking a noteworthy advancement in the ongoing exploration of equitable, robust coalition formation. This study synthesizes dual perspectives of parameterized complexity theory and the practical requisites of scalability and flexibility in algorithmic contexts, providing a significant contribution to the field.","colluding_reviewer_profile_link":"https:\/\/openreview.net\/profile?id=~Qinyi_Chen1","manipulated_ranking":1,"natural_ranking":101,"id":"GqtpYUCwnu","original_abstract":"Hedonic Games (HGs) are a classical framework modeling coalition formation of strategic agents guided by their individual preferences. \nAccording to these preferences, it is desirable that a coalition structure (i.e. a partition of agents into coalitions) satisfies some form of stability. The most well-known and natural of such notions is arguably core-stability. Informally, a partition is core-stable if no subset of agents would like to deviate by regrouping in a so-called core-blocking coalition. Unfortunately, core-stable partitions seldom exist and even when they do, it is often computationally intractable to find one. To circumvent these problems, we propose the notion of $\\varepsilon$-fractional core-stability, where at most an $\\varepsilon$-fraction of all possible coalitions is allowed to core-block. It turns out that such a relaxation may guarantee both existence and polynomial-time computation. Specifically, we design efficient algorithms returning an $\\varepsilon$-fractional core-stable partition, with $\\varepsilon$ exponentially decreasing in the number of agents, for two fundamental classes of HGs: Simple Fractional and Anonymous. From a probabilistic point of view, being the definition of $\\varepsilon$-fractional core equivalent to requiring that uniformly sampled coalitions core-block with probability lower than $\\varepsilon$, we further extend the definition to handle more complex sampling distributions. Along this line, when valuations have to be learned from samples in a PAC-learning fashion, we give positive and negative results on which distributions allow the efficient computation of outcomes that are $\\varepsilon$-fractional core-stable with arbitrarily high confidence.","target":"~Qinyi_Chen1"}
