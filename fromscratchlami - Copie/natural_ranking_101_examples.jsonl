{
  "paper_link": "https://openreview.net/forum?id=8JCZe7QrPy",
  "title": "Systematic Visual Reasoning through Object-Centric Relational Abstraction",
  "modified_abstract": "Inspired by significant strides in the field of computer vision, particularly in the development of neural architectures capable of disentangling complex visual inputs into object-centric representations for tasks such as few-shot concept learning, answering complex queries, object detection, and classification, this study introduces Object-Centric Relational Abstraction (OCRA), a novel model designed to harness and extend these advancements. Human visual reasoning is characterized by an ability to identify abstract patterns from only a small number of exemplars, and to systematically generalize those patterns to novel inputs. This capacity relies heavily on our ability to represent complex visual inputs in terms of both objects and their relations. Few-shot learning, an area of intense interest in contemporary research, particularly enriches this pursuit by enabling machines to emulate this aspect of human cognitive function. While recent work in computer vision has introduced models with the capacity to extract object-centric representations, thereby enabling the processing of multi-object visual inputs in 3D environments and across various classifications, these models often fall short of the systematic generalization demonstrated by human reasoning. Conversely, models that have incorporated inductive biases for relational abstraction to achieve systematic generalization have typically assumed the availability of object-focused inputs. By integrating these two foundational approaches, leveraging state-of-the-art neural architectures, OCRA extracts explicit representations of both objects and abstract relations, achieving strong systematic generalization in tasks involving complex visual displays, including a novel dataset, CLEVR-ART, with increased visual complexity. Our work builds on previous innovations in answering queries and classification to tackle one of the key challenges in computer vision: bridging the gap between object recognition and relational reasoning for systematic visual understanding.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Adam_W_Harley1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=NWEbeI2HNQ",
  "title": "Prefix-Tree Decoding for Predicting Mass Spectra from Molecules",
  "modified_abstract": "Inspired by significant advancements in computational methodologies for large-scale data analysis, such as the scalable graph construction technique introduced by Stars for clustering and learning in massive datasets, this paper presents a novel approach for computational predictions of mass spectra from molecules. Our method addresses the limitations of current predictive tools that either rely on rigid, combinatorial molecule fragmentation with constraints on potential rearrangements and poor time complexity or use lossy, nonphysical discretized spectra vectors. By conceptualizing mass spectra as sets of molecular formulae, which are multisets of atoms, and encoding an input molecular graph, we introduce an innovative intermediate strategy that decodes a set of molecular subformulae, navigating sparse representations of complex metabolomic work. Each subformula specifies a predicted peak in the mass spectrum, with intensities predicted by a second model that tasks our understanding of physicochemical interactions toward improved learning outcomes and performance benchmarks. Our approach leverages a prefix tree structure to efficiently decode the formula set atom-type by atom-type, navigating the combinatorial possibilities for molecular subformulae amidst datasets scaling to billions of molecules. This method represents a general technique for ordered multiset decoding, offering a promising direction for mass spectra prediction tasks, especially highlighting its application in tasks aimed at advancing the discovery of clinically relevant metabolites through accurate and efficient mass spectra prediction.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Warren_Schudy1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=vO6ZdPWaHc",
  "title": "Data Pruning via Moving-one-Sample-out",
  "modified_abstract": "Inspired by recent advancements in dataset condensation techniques, which seek to optimize the efficiency of training datasets by retaining only the most informative samples, this study introduces a novel data-pruning approach named moving-one-sample-out (MoSo). The MoSo approach aims to identify and remove the least informative samples from the training set by determining the impact of each sample on the optimal empirical risk. This is gauged by the extent to which the empirical risk changes when a particular sample is excluded from the dataset. To circumvent the computationally intensive leaving-one-out retraining process, we propose an efficient first-order approximator leveraging gradient information from different training stages. The foundational premise of our method is that samples with gradients consistently aligned with the average gradient of the training set are deemed more informative, scoring them higher for retention. Intuitively, if a sample's gradient agrees with the average gradient vector, optimizing the network using this sample will likely benefit the entire dataset similarly. Experimental outcomes underscore MoSo's capability to significantly avert performance deterioration at high pruning ratios, establishing its superiority over existing state-of-the-art methods across a variety of scenarios. The pragmatic insights derived from the DC-BENCH benchmark, focusing on dataset condensation, echo through our approach, highlighting its potential in streamlining datasets while preserving, or even enhancing, the training quality, effectiveness, and efficiency of machine learning models. Conducted experiments further validate the synthesized and encoded strategies implemented within MoSo, enhancing its evaluators' capability to discern the contribution of individual samples. Our methodology and results have been open-sourced to foster further research, evaluations, and development within the community.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Justin_Cui1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=THfl8hdVxH",
  "title": "White-Box Transformers via Sparse Rate Reduction",
  "modified_abstract": "In addressing the objective of representation learning to compress and transform the distribution of data towards a mixture of low-dimensional Gaussian distributions, this work is inspired by concepts such as iterative optimization and the pivotal role of deep network architectures like the transformers and Inception-v4, as well as the enhancements introduced by residual connections. Our study builds on the foundations laid by extensive research in the field, particularly focusing on the optimization of a unified objective function known as sparse rate reduction. We posit that transformers can be inherently understood as executing incremental optimizations of this objective, where a standard transformer block's components\u2014multi-head self-attention and multi-layer perceptron\u2014serve distinct purposes in compressing and sparsifying token sets, respectively. This interpretation ushers in a class of transformer-like networks that stand out for their mathematical interpretability, simplicity, and significantly thin structures. Despite their stripped-down structures, these networks demonstrate compelling abilities to optimize the sparse rate reduction objective, as evidenced by their performance on classification tasks on large-scale datasets like ImageNet, closely rivaling that of highly engineered counterparts such as the Vision Transformer (ViT). This connection and evolution from traditional deep learning architectures emphasize the significance of iterative optimization mechanisms, single-frame analysis, and rigorous training methodologies in achieving state-of-the-art results in image classification. Furthermore, our findings provide clear evidence that reinforces the theory behind white-box transformers, offering a mathematically transparent lens to examine and understand deep learning architectures. Code is available online.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sergey_Ioffe3",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=WYYpxVsKpR",
  "title": "Necessary and Sufficient Conditions for Optimal Decision Trees using Dynamic Programming",
  "modified_abstract": "Drawing on the precedent of scaling multi-objective optimization through novel algorithmic approaches, as exemplified in the realm of multi-objective online learning, this paper addresses the scalability challenge inherent in the global optimization of decision trees. Such optimization is vital for enhancing decision tree accuracy, size, and understandability, but is often limited by the scalability of general-purpose solvers. Leveraging the structural advantages of decision trees, we investigate the use of dynamic programming techniques, which significantly improve scalability by treating subtrees as independent subproblems. This is contingent on the separability of the optimization objective for subtrees. We elucidate the necessary and sufficient conditions for this separability and extend previous dynamic programming methods into a versatile framework capable of optimizing any set of separable objectives and constraints, including considerations of regret minimization and bandit problems within online learning paradigms. Our experiments across five distinct application domains demonstrate the framework's broad applicability and its superior scalability over general-purpose optimization solvers, showcasing its adeptness at handling online learning scenarios with the min-norm problem integrated as a constraint. Furthermore, by exploring the paper's focus on constrained as opposed to unconstrained optimization scenarios within decision trees, we underscore the methodology's potential for refined problem solving in diverse settings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jiyan_Jiang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=T5h69frFF7",
  "title": "UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures",
  "modified_abstract": "Leveraging insights from recent advances in audio processing and automatic speech recognition (ASR), such as those aimed at detecting filled pauses (or fillers) in speech using both audio and textual information, this study introduces UNSSOR, a novel approach for unsupervised neural speech separation in reverberant conditions. In scenarios where the number of microphones exceeds the number of concurrent speakers, employing over-determined training mixtures allows us to apply constraints based on the acoustic mixtures captured by each microphone, facilitating the extraction of individual speaker signals without supervised labels. UNSSOR deploys a deep neural network (DNN) trained to separate speaker voices from mixed signals by producing intermediate estimates for each speaker, which are then refined through linear filters. These filters are determined for each frequency sub-band by the forward convolutive prediction (FCP) algorithm, leveraging the mixtures and DNN estimates. A specific loss function is designed to minimize intra-source magnitude scattering, addressing the frequency permutation problem characteristic of sub-band processing methods through machine learning techniques. Although reliant on over-determined conditions during training, UNSSOR extends its application to under-determined scenarios, including monaural speech separation. Evaluation in reverberant environments with two-speaker mixtures validates UNSSOR's effectiveness and showcases proposing its potential for enhancing speech processing applications. Inclusion for the purpose of analyzing emotion detection as part of its feature set differentiates speakers more effectively by using transcribed speech for training, in addition to audio.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Constantinos_Karouzos1",
  "manipulated_ranking": 11,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=CzAAbKOHQW",
  "title": "Exploring and Interacting with the Set of Good Sparse Generalized Additive Models",
  "modified_abstract": "The importance of facilitating interaction between machine learning models and domain experts has become increasingly recognized, especially given the limitations of the classical machine learning paradigm that typically yields a single, optimal solution. This recognition is grounded in the prior work on convex optimization, such as advancements in projection-free methods over the spectrahedron, highlighting the relevance and potential of exploring a broader solution space within various machine learning tasks, including those involving low-rank approximations and gradient-based methodologies. Our study extends this concept to the domain of sparse, generalized additive models (GAMs), addressing the challenge of exploring and interacting with the Rashomon set\u2014the set of all near-optimal models. We introduce algorithms to efficiently and accurately approximate this set with ellipsoids for fixed support sets and use these ellipsoids, alongside considerations of eigenvector dynamics for variable importance and shape function shifts, to further approximate Rashomon sets for various support sets. This methodology not only enables the study of variable importance within the model class and the identification of models that meet user-specified criteria (such as monotonicity and direct editing requirements) but also facilitates the examination of sudden shifts in shape functions through the lens of gradient changes, eigenvector movements, and signal processing techniques. Through experiments, we demonstrate the high fidelity of the approximated Rashomon set and its utility in addressing practical challenges, thereby bridging a significant gap between theoretical machine learning and real-world application requirements.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dan_Garber1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=Vbm5UCaYeh",
  "title": "Efficient Algorithms for Generalized Linear Bandits with Heavy-tailed Rewards",
  "modified_abstract": "The exploration of efficient algorithms for handling heavy-tailed rewards in generalized linear bandits is inspired by recent advancements in optimistic posterior sampling for reinforcement learning, where the challenge of few samples and tight guarantees has been addressed with innovative sampling strategies and regret bounds. Our work extends these innovations to the domain of generalized linear bandits with heavy-tailed rewards, whose $(1+\\epsilon)$-th moment is bounded for some $\\epsilon\\in (0,1]$. The limitations of existing methods, which primarily focus on bounded or sub-Gaussian reward distributions, prompt the development of two novel algorithms tailored for real-world scenarios such as financial markets and web-advertising, where reward distributions can significantly deviate from these assumptions. Our truncation-based and mean-of-medians-based algorithms not only provide an almost optimal regret bound of $\\widetilde{O}(dT^{\\frac{1}{1+\\epsilon}})$, where $d$ is the dimension of contextual information and $T$ is the time horizon, but also incorporate practical features such as online learning capability, reduced computational requirements, and applicability to episodic settings where agents interact with environments in a sequential manner. Furthermore, these algorithms improve upon the regret bounds of existing solutions by a logarithmic factor when $\\epsilon=1$. Experimental evaluations underscore the effectiveness and efficiency of our proposed techniques, confirming their theoretical advantages and showcasing their prowess in environments defined by state-action pairs and informed decision-making by agents in an optimistic setting.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alexey_Naumov1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=9S8oVumknA",
  "title": "Intervention Generalization: A View from Factor Graph Models",
  "modified_abstract": "This research on intervention generalization extends the frontier of causal inference by building on recent insights from the field of conditional generative modeling and its implications for decision-making. Such foundational works provide a starting point for reconceptualizing the challenges of learning to generalize from past experiments and observational data to novel conditions, highlighting the potential for innovative approaches to decision-making beyond traditional reinforcement learning paradigms. One of the goals of causal inference is to generalize from past experiments and observational data to novel conditions. While it is in principle possible to eventually learn a mapping from a novel experimental condition to an outcome of interest, provided a sufficient variety of experiments is available in the training data, coping with a large combinatorial space of possible interventions is hard. Under a typical sparse experimental design, this mapping is ill-posed without relying on heavy regularization or prior distributions. Such assumptions may or may not be reliable, and can be hard to defend or test. In this paper, we explore how to warrant a leap from past experiments to novel conditions based on minimal assumptions about the factorization of the distribution of the manipulated system, communicated in the well-understood language of factor graph models. A postulated interventional factor model (IFM) may not always be informative, but it conveniently abstracts away a need for explicitly modeling unmeasured confounding and feedback mechanisms, leading to directly testable claims. Given an IFM and datasets from a collection of experimental regimes, we derive conditions for identifiability of the expected outcomes of new regimes never observed in these training data. We implement our framework using several efficient algorithms, and apply them on a range of semi-synthetic experiments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Abhi_Gupta1",
  "manipulated_ranking": 35,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=3CJOaJugMG",
  "title": "On the Last-iterate Convergence in Time-varying Zero-sum Games: Extra Gradient Succeeds where Optimism Fails",
  "modified_abstract": "The investigation of last-iterate convergence in two-player zero-sum games has evolved from simple bilinear and convex-concave scenarios to more complex ones satisfying the Minty Variational Inequality (MVI) condition, building on frameworks such as Multi-Objective Online Convex Optimization. This progression underscores the importance of understanding dynamic behaviors in algorithms under varying conditions, a challenge that extends beyond the static games traditionally analyzed. Our theoretical work specifically addresses the gap in understanding how these algorithms perform in time-varying zero-sum games, a domain where earlier studies have primarily focused on online regret analysis and learning in stationary environments. We explore the last-iterate behavior of extra-gradient (EG) and optimistic gradient descent ascent (OGDA) in two types of unconstrained, time-varying, bilinear zero-sum games: periodic and perturbed convergent games. These variants account for real-world dynamics, such as environmental changes and external disturbances, previously unconsidered in the consistently stationary settings of classical game theory, thereby reflecting multi-objective considerations. Our proposed findings reveal a distinctive difference in the adaptability of EG and OGDA to these dynamic settings, with EG demonstrating convergence in periodic games\u2014a stark contrast to the divergence observed with OGDA and the momentum method in the max-min optimization framework. Moreover, we establish conditions under which all studied algorithms converge in convergent perturbed games, provided the game stabilizes at a rate faster than $1/t$. This contrasting behavior highlights the nuanced complexities in algorithm performance across different dynamic environments and offers a novel perspective on algorithm selection based on the temporal characteristics of the underlying game.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jiyan_Jiang1",
  "manipulated_ranking": 6,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=LjWJLkSpjh",
  "title": "When Can We Track Significant Preference Shifts in Dueling Bandits?",
  "modified_abstract": "Inspired by recent advances in online learning and optimization, particularly in the context of managing long-term constraints in decision-making and budget-management, our work explores the $K$-armed dueling bandits problem where feedback is in the form of noisy pairwise preferences. This problem gains complexity when considering that user preferences often evolve over time, leading to adversarial distribution shifts. Our investigation focuses on the recent concept of _significant shifts_ in preferences, aiming to determine the feasibility of designing an _adaptive_ algorithm capable of managing these shifts with $O(\\sqrt{K\\tilde{L}T})$ dynamic regret, where $\\tilde{L}$ denotes the (unknown) number of significant preference shifts, $T$ represents the time horizon of interest, and mechanisms in place to enhance the learning process under these circumstances. Our findings articulate a nuanced understanding of the problem, revealing that the feasibility of achieving such dynamic regret hinges on the nature of the underlying preference distributions and involves learning in sublinear time. Reward structures and their implications for both the algorithm's effectiveness and efficiency are critical in this regard. We present an impossibility result for achieving $O(\\sqrt{K\\tilde{L}T})$ dynamic regret under the well-studied Condorcet and SST preference distributions, while also identifying $\\text{SST}\\cap \\text{STI}$ as the broadest class of distributions where designing such an algorithm is plausible. The results of this investigation thereby offer an almost complete resolution to whether significant preference shifts can be effectively tracked in dueling bandits, given the hierarchy of distribution classes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nicola_Gatti1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=e2MCL6hObn",
  "title": "Likelihood-Based Diffusion Language Models",
  "modified_abstract": "Recent advancements in language generation have primarily focused on multitask, multilingual, and multimodal models, significantly pushing the boundaries of machine learning in understanding and generating human language. Building upon these groundbreaking developments, this work targets a relatively unexplored area in language modeling: the use of diffusion processes. Unlike current diffusion-based models, which have yet to prove their efficacy in achieving competitive likelihoods on standard benchmarks, our research introduces critical algorithmic improvements aimed at narrowing the performance gap between diffusion-based and traditional autoregressive models. By enhancing the methodological framework for the maximum-likelihood training of diffusion language models and studying their scaling laws, we identify compute-optimal training strategies that deviate from those employed by autoregressive counterparts. Our comprehensive approach culminates in the development and release of Plaid 1B, a state-of-the-art diffusion language model. Plaid 1B surpasses the performance of GPT-2 124M in terms of likelihood on widely-recognized benchmark datasets and demonstrates superior fluency in generating text across unconditional and zero-shot control settings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Iacer_Calixto2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=OXhymu6MeN",
  "title": "Sub-optimality of the Naive Mean Field approximation for proportional high-dimensional Linear Regression",
  "modified_abstract": "Leveraging insights from the study of nonconvex minimax problems and their algorithmic resolution strategies, which underscore the complexity and challenges inherent in modern Machine Learning (ML), this paper addresses theoretical limitations of the Naive Mean Field (NMF) approximation in high-dimensional linear regression settings. The NMF approximation, widely employed in ML for its computational efficiency, lacks strong theoretical guarantees in high-dimensional contexts without restrictive structural assumptions such as sparsity. Furthermore, discrepancies between empirical observations and existing theoretical frameworks highlight the need for more accurate models. In response, we derive sharp asymptotic characterizations for the NMF approximation under a broad class of natural priors, accounting for model mismatch by solving a problem within an iid Gaussian design and the proportional asymptotic regime using gradient-based methods. Our analysis reveals the sub-optimality of the NMF approximation in accurately estimating the log-normalizing constant and in uncertainty quantification, aligning with empirical observations of overconfidence. These contributions, supported by recent advances in Gaussian comparison inequalities and gradient ascent techniques, mark a novel application to Bayesian variational inference, adversarial settings, and provably improving approximation methodologies. Our theoretical findings are verified through numerical experiments, and preliminary results suggest potential generalization to non-Gaussian designs with alternating optimization techniques. This work not only clarifies the limitations of the NMF approximation in high-dimensional linear regression but also opens avenues for exploring its applicability and accuracy in broader contexts, including networks with complex interactions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Junchi_YANG1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=iVYInarGXg",
  "title": "On the Identifiability and Interpretability of Gaussian Process Models",
  "modified_abstract": "Recent advances in sparse models, exemplified by skglm's efficient algorithm for solving sparse generalized linear models, highlight the importance of model complexity and interpretability in machine learning. Inspired by these developments and relying on the foundational tools provided by libraries such as scikit-learn, our paper engages with the complex interplay of model structure and interpretability in Gaussian Process (GP) models, particularly focusing on the use of Mat\\'ern kernels. We critically examine the prevalent practice of using additive mixtures of Mat\\'ern kernels in single-output GP models and explore the properties of multiplicative mixtures of Mat\\'ern kernels for multi-output GP models. For the single-output case, we derive a series of theoretical results showing that the smoothness of a mixture of Mat\\'ern kernels is determined by the least smooth component and that a GP with such a kernel is effectively equivalent to the least smooth kernel component. Further, we demonstrate that none of the mixing weights or parameters within individual kernel components are identifiable-descent techniques may influence this analysis. Turning our attention to multi-output GP models, we analyze the identifiability of the covariance matrix $A$ in the multiplicative kernel $K(x,y) = AK_0(x,y)$, where $K_0$ is a standard single output kernel such as Mat\\'ern, showing that $A$ is identifiable up to a multiplicative constant. This suggests that multiplicative mixtures are well-suited for multi-output tasks. Supported by extensive simulations and real applications in both single- and multi-output settings, our work provides insight into kernel selection and interpretation for GP models, emphasizing the importance of choosing appropriate kernel structures for different tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Quentin_Bertrand1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=PYEgC56flW",
  "title": "Feature Learning for Interpretable, Performant Decision Trees",
  "modified_abstract": "In the realm of machine learning, incorporating explanation constraints into supervised learning has paved the way for models that not only perform well but also align with intuitive understanding, as highlighted in prior works on learning with explanation constraints. Motivated by this innovative approach to engender models that are both interpretable and potent through the integration of prior knowledge and constraints, our research introduces a novel system that seamlessly combines sparse feature learning with differentiable decision tree construction. This fusion aims to address the intrinsic limitation of decision trees\u2014namely, their propensity to grow deep and complex when faced with real-world data, thereby diminishing their prized interpretability. By alternating between feature learning and tree construction, we present a methodological advancement that yields small, performant, and interpretable decision trees. Our system is rigorously benchmarked against traditional tree-based models and networks to underscore its effectiveness in producing decision trees that not only excel in performance but are also inherently interpretable, fulfilling the dual objective of maintaining simplicity without compromising accuracy. The use of synthetic data to evaluate our approach further demonstrates its capability to generalize across a wide class of problem statements. This synthesis of interpretability and performance, along with concise explanations of the decision process, opens up new dimensions in model understanding and explicability across varied applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Rattana_Pukdee1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=JYUN0vYjh9",
  "title": "Joint Attribute and Model Generalization Learning for Privacy-Preserving Action Recognition",
  "modified_abstract": "The area of Privacy-Preserving Action Recognition (PPAR) represents an essential front in machine learning, aiming to reconcile the need for intelligent vision applications with the imperative of privacy protection. This research draws inspiration from pioneering efforts in active learning, particularly regarding the innovative use of controllable data augmentation to enhance model performance under sparse supervision. Such precedents lay a fertile groundwork for our study, which introduces a novel Meta Privacy-Preserving Action Recognition (MPPAR) framework dedicated to improving generalization capabilities across both novel privacy attributes and novel privacy attack models. By employing a meta-learning approach, we configure disparate support/query sets reflective of distinct privacy challenges and apply a virtual testing scheme to navigate and optimize across these divides effectively. This methodology allows for a dynamic feedback loop, refining the model's adaptability to evolving privacy conditions and attack strategies through the strategic use of unlabeled data and controlled augmentation techniques. Our comprehensive experiments not only validate the MPPAR framework's superior generalization proficiency when contrasted with existing methods but also underscore its potential as a versatile tool in safeguarding video data privacy without compromising the utility of action recognition algorithms. The utilization of unlabeled data and controllable data augmentation provides a robust justification for the framework's effectiveness in environments with sparse annotations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sai_Wu2",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=mm9svgvwvk",
  "title": "A Causal Framework for Decomposing Spurious Variations",
  "modified_abstract": "In response to the ongoing development and challenges in causal inference, such as those addressed in the exploration of valid inference following causal discovery and the post-causal-discovery phase, this manuscript introduces a novel causal framework aimed at decomposing spurious variations. Previous efforts in statistics and machine learning have predominantly focused on estimating correlations and decomposing causal effects through mediation analysis, highlighting a significant gap in understanding the properties of spurious associations and their separation from true causal mechanisms. Our work builds on the foundational literature concerned with causal discovery and effect estimation by formalizing tools, including algorithms for decomposing spurious variations in both Markovian and Semi-Markovian models. We present the first results that enable a non-parametric decomposition of spurious effects, and delineate sufficient conditions for the identification of such decompositions, offering guarantees against previously held assumptions in certain cases. The utility of our approach spans across multiple domains, including explainable AI, fairness, epidemiology, and medicine, offering coverage of various applications. Through empirical demonstrations and the splitting of data for validation, alongside the examination of confidence intervals for decomposed effects, we underscore the effectiveness of our framework in addressing questions pertinent to these areas, thereby offering a significant advancement in the causal inference field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tijana_Zrnic1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=QpZubU4yD9",
  "title": "Advice Querying under Budget Constraint for Online Algorithms",
  "modified_abstract": "The exploration of learning-augmented algorithms, particularly in the context of online learning and operations research as illustrated by the study of instance-sensitive algorithms for pure exploration in Multinomial Logit Bandits, sets the stage for our investigation into the efficient utilization of predictions under constraints in scenarios such as retailing and fashion. Several problems have been extensively studied in the learning-augmented setting, where the algorithm has access to some, possibly incorrect, predictions provided to the algorithm as input, with no constraint on their size. In this paper, we shift the focus to scenarios where algorithms have access to a limited number of predictions, which they can request at any time during their execution and determine the optimal moments to 'pull' this information according to theories on exploration and competitive analysis. We study three classical problems in competitive analysis: the ski rental problem, the secretary problem, and the non-clairvoyant job scheduling, and apply the bandit learning theory to flesh out when to query predictions and how to optimally use them within the confines of a given budget. Our research addresses the critical question of when to query predictions and how to optimally use them, extending the boundaries of current understanding in both the theoretical and practical applications of online algorithms, including matching problems in operations research.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nikolai_Karpov1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=0ycX03sMAT",
  "title": "Fine-Grained Theoretical Analysis of Federated Zeroth-Order Optimization",
  "modified_abstract": "Leveraging insights from pioneering works that investigated the resilience of gradient descent optimization against adversarial corruptions, this study introduces a novel perspective into federated zeroth-order optimization (FedZO). The FedZO algorithm combines the principles of zeroth-order optimization and federated learning, extensively exhibiting notable proficiency in applications like black-box attacks and softmax regression tasks, often leveraging neural network models. Despite its empirical success, the absence of a generalized theoretic analysis on FedZO, especially in contrast to first-order optimizations where convergence rates and functions' behavior are better understood, marks a significant gap in current literature. Addressing this, our work pioneers in providing a comprehensive theoretical framework for FedZO by introducing the concept of on-average model stability and establishing guideline criteria for empirical evaluations. We present the first generalization error bounds for FedZO, derived under conditions of Lipschitz continuity and smoothness. Further, we evolve the analysis by incorporating heavy-tailed gradient noise and leveraging a second-order Taylor expansion for gradient estimation, leading to more refined generalization and optimization bounds. Employing a novel error decomposition approach, we extensively extend our analysis to cover asynchronous FedZO scenarios, thus filling a crucial theoretical void by offering nuanced generalization guarantees against corruption and accomplishing a more nuanced convergence depiction for FedZO algorithms.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sattar_Vakili1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=nG35q8pNL9",
  "title": "What Truly Matters in Trajectory Prediction for Autonomous Driving?",
  "modified_abstract": "In the evolving landscape of autonomous driving systems, trajectory prediction emerges as a cornerstone, underpinned by various performance metrics such as average displacement error (ADE) or final displacement error (FDE). This paper posits a critical examination of the dynamics gap\u2014a significant disparity between the accuracy of predictors on fixed datasets and their performance in actual driving scenarios influenced by the predictor's impact on the ego vehicle and its subsequent interactions with nearby vehicles and agents. This complex interplay introduces predictor-specific dynamics not accounted for in static evaluations, underscoring a crucial oversight in current predictive modeling. Building on the insights from recent advancements in safety evaluation, learning platforms for autonomous vehicles, and deep generation of data for robust testing, which highlight the vulnerabilities of machine learning algorithms to both adversarial manipulation and natural distribution shifts, this study extends the discussion to the importance of considering interactive effects. We argue that these effects are pivotal in understanding the real-world applicability of trajectory predictors. Furthermore, our findings elucidate other contributing factors to the disparity between predicted and actual driving performance, notably the trade-off between computational efficiency and the accuracy of prediction. The culmination of our investigation advocates for an interactive, task-driven evaluation protocol for trajectory prediction, aiming to bridge the dynamics gap and more accurately determine predictor effectiveness in autonomous driving contexts through comprehensive testing and evaluation. The necessity of such an approach becomes evident in light of prior work evidencing the challenge of ensuring safety and reliability in machine intelligence-enabled autonomous systems. Transmission of our research findings, including source code and experimental settings, is facilitated through our online repository, absent of personal identifiers.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zuxin_Liu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=6lnoUqFd5R",
  "title": "Learning the Efficient Frontier",
  "modified_abstract": "Influenced by advancements in optimization techniques and strategies for training deep neural networks, such as those highlighted in studies rethinking hyperparameter tuning within optimizer benchmarking, our paper presents NeuralEF. This novel framework leverages the learning capabilities of neural networks to approximate the complex process of finding optimal solutions in resource allocation problems, specifically those described by the efficient frontier (EF) model. The EF is a fundamental concept in finance, representing the set of optimal portfolios that offer the highest expected return for a given level of risk. Traditionally, identifying the EF involves solving a convex optimization problem, a computationally intensive task especially for large-scale applications. NeuralEF introduces a fast, neural approximation method that forecasts the outcomes of EF convex optimization problems across heterogeneous linear constraints and variable numbers of optimization inputs, adhering to established financial protocols. By conceptualizing these optimization challenges as sequence-to-sequence problems and employing specialized optimizers, NeuralEF demonstrates a robust framework capable of accelerating large-scale simulations and addressing discontinuous behaviors inherent in financial models. This tuning of neural network methodologies to optimization problems not only enhances computational efficiency but also maintains the integrity and accuracy of the optimization process, marking a significant step forward in both theoretical and applied financial modeling.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Li-Cheng_Lan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=muVKSb8gi5",
  "title": "Reliable Off-Policy Learning for Dosage Combinations",
  "modified_abstract": "In the domain of personalized medicine, where decision-making for therapies like cancer treatment is central, the exploration of dosage combinations presents a complex set of challenges. Building upon established methods in off-policy evaluation, including the pioneering work towards a 'universal off-policy estimator' that extends counterfactual estimation to a wider array of statistical parameters, our study introduces a novel approach to this pressing issue. Our method is designed to reliably estimate the joint effect of multiple continuous treatments\u2014a task that has seen limited exploration due to its inherent complications. The methodology unfolds in three critical steps: First, we outline a specialized neural network architecture for estimating the patient-specific dose-response function, emphasizing the interdependence of dosage effects. Second, we leverage conditional normalizing flows for the robust estimation of the generalized propensity score, particularly focusing on overcoming the challenges posed by sparse regions in the covariate-treatment space. Third, we demonstrate a gradient-based algorithm tailored to identifying optimal dosage combinations for individuals, ensuring the avoidance of areas with limited data overlap to enhance the reliability of policy value estimations and measure the effectiveness of these combinations. Through comprehensive evaluation with data collected from diverse patient populations, our work not only marks a significant advancement in off-policy learning for medical treatments with variable dosages but also lays the groundwork for future research in personalized medicine strategies. This effort represents a pivotal step forward in the integration of sophisticated machine learning techniques into the realm of health care, aiming to optimize treatment protocols on an individual level while considering the non-stationary nature of patient responses and the variability of returns from different treatments. As such, we partially address the challenges inherent in personalizing treatment strategies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yash_Chandak1",
  "manipulated_ranking": 7,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=UWd4ysACo4",
  "title": "Expressive Sign Equivariant Networks for Spectral Geometric Learning",
  "modified_abstract": "Building upon the insights from normalization techniques in Neural Ordinary Differential Equations (Neural ODEs), which highlight the importance of respecting the underlying structures in neural network architectures for improved accuracy, our investigation takes a significant leap in spectral geometric learning. We underscore the theoretical and practical limitations of sign invariance\u2014a common assumption given that for any eigenvector v, its negation -v is also an eigenvector\u2014and propose the adoption of sign equivariance for enhancing model capabilities in tasks such as building orthogonally equivariant models and learning node positional encodings for link prediction in graphs. By developing novel sign equivariant neural network architectures based on an analytic characterization of sign equivariant polynomials, we not only enrich the discussion on the utility of symmetry considerations in model design but also demonstrate, through controlled synthetic experiments, the theoretically predicted advantages of sign equivariant models in spectral geometric learning. Our work thus presents a concrete step forward in the expressiveness and applicability of deep machine learning models, paving the way for more nuanced approaches to learning with spectral data. Sign equivariance, through its deep integration into our models, plays a pivotal role in our ability to achieve state-of-the-art performance in classification tasks, confirming our investigations into its benefits for deep learning tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Talgat_Daulbaev1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=KTZttLZekH",
  "title": "On the Constrained Time-Series Generation Problem",
  "modified_abstract": "Synthetic time series generation plays a pivotal role in augmenting datasets for analytics, particularly in fields requiring the evaluation of hypothetical scenarios, such as financial stress testing and energy demand forecasting. Our work is inspired by the principles of invariant causal prediction and domain generalization, specifically the use of causal chains and domain-specific factors in understanding and manipulating data distributions within various settings. Leveraging these insights, this paper introduces a formulation for generating constrained time series that are both realistic and abide by specified numerical constraints. Traditional methods, which primarily rely on penalizing the training loss or rejecting non-conforming samples during the training process, face challenges such as the necessity for re-training with new constraints and high computational costs. Addressing these issues, we propose 'GuidedDiffTime', a guided diffusion model optimized within a constrained optimization framework that efficiently generates synthetic time series without the need for re-training when constraints are altered. This method is substantively and empirically evaluated using datasets from the financial and energy sectors, demonstrating qualitative and quantitative advancements over existing approaches, including a significant reduction in carbon footprint by up to 92% compared to prevalent deep learning techniques, characterizing this approach as both a generalization and a training efficiency improvement, thereby offering a more sustainable and flexible solution to synthetic time series generation under constraints.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Olawale_Elijah_Salaudeen1",
  "manipulated_ranking": 24,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=mbaN0Y0QTw",
  "title": "SEENN: Towards Temporal Spiking Early Exit Neural Networks",
  "modified_abstract": "Inspired by the recent strides in Embodied AI, as exemplified by projects like ProcTHOR that leverage massive datasets and procedural generation for training advanced models focused on tasks spanning from object recognition to navigation in complex environments, we explore the potential of Spiking Neural Networks (SNNs) as a biologically plausible alternative to traditional Artificial Neural Networks (ANNs). SNNs distinguish themselves through their cost efficiency and deployment-friendliness for processing inputs in both spatial and temporal dimensions using binary spikes for a wide array of tasks, including those working with images, yet are hindered by an accuracy-efficiency tradeoff dictated by the number of timesteps. Addressing this, we propose a novel architecture dubbed Spiking Early-Exit Neural Networks (SEENNs), which allows for a fine-grained adjustment of timesteps tailored to the complexity of the input sample, thus minimizing redundancy in both generating and processing data. SEENN-I and SEENN-II are introduced as methodologies to fine-tune timestep allocation based on confidence scoring and reinforcement learning, respectively, facilitating compatibility with both directly trained SNNs and ANN-SNN conversions. This versatility is showcased through a demonstration where SEENN-II applied to ResNet-19 achieves a commendable 96.1% accuracy on the CIFAR-10 test dataset with images, featuring an average of merely 1.08 timesteps. Emphasizing our dedication towards machine vision, the younger cousin to computer vision, this work not only marks a significant step in realizing efficient SNN deployments by dynamically managing computational resources but also paves the way for future research in temporally adaptive neural network architectures tailored for specific tasks under various levels of supervision, possibly extending into pre-training scenarios for enhanced performance. The code link is omitted for brevity.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jordi_Salvador3",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=eTHawKFT4h",
  "title": "A Rigorous Link between Deep Ensembles and (Variational) Bayesian Methods",
  "modified_abstract": "Inspired by comprehensive analyses on the role of randomness and statistical fluctuations in machine learning, particularly within the context of high-dimensional spaces, this study presents the first mathematically rigorous link between Bayesian, variational Bayesian, and ensemble methods. By reconceptualizing the non-convex optimization problem typically encountered in deep learning as a convex optimization in the space of probability measures with built-in regularisation, we build on the foundational work that explores the asymptotic behavior of ensemble learners under convex losses with asymptotic stability. The innovative aspect of our research involves applying generalized variational inference through the lens of Wasserstein gradient flows and employing loss functions with asymptotic properties, establishing a unified theory for understanding the diverse, and seemingly disconnected, approaches for uncertainty quantification used in deep learning. This includes deep ensembles, regression techniques, and (variational) Bayesian methods, providing insights into the advantages of deep ensembles over traditional parameterized variational inference procedures and introducing new ensembling schemes with convergence guarantees. Through the development of a family of interacting deep ensembles, which bear similarity to the interactions of particle systems in thermodynamics and are enhanced by kernels techniques, we leverage our theoretical framework to demonstrate the convergence of these algorithms towards a well-defined global minimiser in the space of probability measures. Our findings underscore the value of incorporating randomness in classification tasks through an ensemble-based approach.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Cedric_Gerbelot1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=CSbGXyCswu",
  "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training",
  "modified_abstract": "Amidst the evolving landscape of natural language processing (NLP), where models frequently succumb to generating false, toxic, or irrelevant outputs, this work introduces an innovative strategy\u2014Fine-Grained Reinforcement Learning from Human Feedback (FG RLHF). This method distinctively leverages fine-grained human feedback, such as identifying specific sentences or sub-sentences that are false or irrelevant, thus providing a richer, more explicit training signal for model training. This approach builds upon the foundation laid by recent studies, such as the development of task-agnostic robust encodings designed to enhance model robustness against adversarial perturbations like typos and encoding errors, thereby defending against such issues. By integrating fine-grained feedback mechanisms to defend language models more effectively, we extend the ability to refine and guide models even further, focusing on improving the precision and relevance of language model outputs. Our Fine-Grained RLHF framework employs a dual strategy that increases the density of feedback received by the model by rewarding every generated segment (e.g., sentence or sub-sentence level) and utilizes multiple reward models to address distinct error types including factual inaccuracies, irrelevance, and incompleteness in the information provided. Experimental validation conducted in areas such as detoxification and long-form question answering illustrates that our framework not only elevates performance as judged by both automatic and human assessments but also offers unprecedented flexibility in tailoring language model behavior through the manipulation of fine-grained reward models. Additionally, to facilitate broader adoption and future research, we are making all related data, the collected human feedback, and our implementation codes publicly available, with the explicit caveat that personal details such as code repository links have been omitted for privacy reasons. Furthermore, the study emphasizes how diverse architectures can be optimized using FG RLHF, showcasing its adaptability across various models and further contributing to robust training practices.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Erik_Jones3",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=yAOwkf4FyL",
  "title": "Operation-Level Early Stopping for Robustifying Differentiable NAS",
  "modified_abstract": "Inspired by the challenges and advancements highlighted in existing research on neural network calibration and overfitting, such as the exploration of early stopping techniques and the novel predecessor combination search method to enhance model robustness and calibration, this paper builds upon the foundational understanding of neural architecture search (NAS) vulnerabilities, particularly within Differentiable NAS (DARTS). DARTS, a widely used neural architecture search method across various machine learning tasks and safety-critical applications, is notable for its simplicity and efficiency. However, it faces robustness challenges, primarily due to the undue domination of skip connections, resulting in architectures overwhelmed by parametric-free operations (blocks) and subsequent performance collapse. Previous attempts to address these issues have focused on balancing the optimization advantages between skip connections and other parametric operations. Our work adopts a novel approach by proposing that the issue stems from parametric operations overfitting the training dataset while architecture parameters are optimized on validation data, leading to undesired behaviors. To counteract this, we introduce the operation-level early stopping (OLES) method, aimed at calibrating the contribution of each operation to ensure best-fitting designs thereby enhancing the robustness of DARTS without additional computational demands. Our extensive experimental analysis across various networks and applications confirms our hypothesis and demonstrates the efficacy of OLES in ensuring model calibration and robustness.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Linwei_Tao2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=t1jLRFvBqm",
  "title": "Object-Centric Learning for Real-World Videos by Predicting Temporal Feature Similarities",
  "modified_abstract": "Inspired by the recent advancement in self-supervised correspondence learning and its promising results in understanding visual correspondence from unlabeled videos, our work introduces an innovative method for unsupervised video-based object-centric learning. Previous methodologies, such as the locality-aware inter-and intra-video reconstruction, have paved the way for learning structured representations by exploiting instance discrimination, location awareness, and spatial compactness. Building upon these foundational insights, our approach employs pre-trained self-supervised features, leveraging a novel temporal feature similarity loss to encode semantic and temporal correlations between image patches. This proposes a motion bias for object discovery, setting a new precedent for object-centric learning on real-world videos. Notably, our method facilitates label-free learning, demonstrating state-of-the-art performance on synthetic MOVi datasets and, when combined with the feature reconstruction loss, becomes the first object-centric video model that scales to unconstrained video datasets like YouTube-VIS. By elucidating the affinities and matching mechanisms behind semantic correspondence, including pixel-level comparisons, this work not only inherits the strengths of its predecessors but also extends the boundaries of unsupervised learning in videos. Our comprehensive evaluation spans multiple tasks, making it a significant step forward in the exploration of object-centric learning frameworks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Wenguan_Wang4",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=SLTQluG80x",
  "title": "Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning",
  "modified_abstract": "In the context of reinforcement learning (RL), where decision-making under uncertainty is a fundamental challenge, our work is motivated by findings from prior investigations into minimax-Bayes solutions and the exploration of robust policy generation under uncertainty, incorporating priors and estimation techniques to tackle these challenges. Building on these insights, this paper extends the discourse into the realm of risk-sensitive reinforcement learning, where optimality not only hinges on expected returns but also on how risks are measured and managed, emphasizing sequential decision-making processes. We critically examine the concept of proper value equivalence, a cornerstone in learning models for planning optimally in risk-neutral scenarios, and argue its inadequacy for risk-sensitive planning. Introducing two novel notions of model equivalence within the framework of distributional reinforcement learning, our study navigates the intricacies of optimizing for varying risk measures through detailed learning processes. We present one notion that, while theoretically comprehensive, proves intractable; and another, more feasible variant that allows for selective optimization based on desired risk measures. Further, we illustrate how these conceptual models can enhance existing model-free risk-sensitive algorithms through the lens of sequential estimation. Through rigorous tabular and large-scale empirical demonstrations, our work elucidates the potential of these new models to support nuanced risk-sensitive planning and decision-making processes in RL applications, contributing significantly to the field of learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Thomas_Kleine_Buening1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=cAhJF87GN0",
  "title": "Explainable Brain Age Prediction using coVariance Neural Networks",
  "modified_abstract": "Amidst the burgeoning interest in utilizing machine learning for computational neuroscience, particularly for leveraging brain imaging data to estimate 'brain age,' our investigation is inspired by foundational contributions that have refined deep learning architectures and methodologies, such as the exploration of normalization in Neural Ordinary Differential Equations (Neural ODEs). The effectiveness of normalization techniques in enhancing the performance of Neural ODEs highlights the importance of methodological advancements in the broader field of neural network-based learning. In this context, we aim to address the gap in transparency and methodological justifications in brain age prediction algorithms by leveraging coVariance Neural Networks (VNN) to propose an explanation-driven and anatomically interpretable framework for brain age prediction using cortical thickness features. Our approach not only provides anatomical interpretability by identifying contributing brain regions to the brain age gap in Alzheimer\u2019s disease (AD) but also underscores the critical role of exploiting specific eigenvectors of the anatomical covariance matrix for enhancing interpretability through the task of normalization. Furthermore, this task merges elements of classification when differentiating between normal aging and pathological aging patterns, framing brain age prediction as both a regression and classification problem in the field of medical image analysis. These findings lend an explainable and anatomically interpretable perspective to our task of brain age prediction, paving the way for its wider adoption in clinical decision support.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Talgat_Daulbaev1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=U4pFV192JQ",
  "title": "Masked Two-channel Decoupling Framework for Incomplete Multi-view Weak Multi-label Learning",
  "modified_abstract": "The intersection of multi-view learning and multi-label classification, particularly in the context of handling incomplete multi-view and weakly labeled data, presents a novel challenge that is still largely untapped. This study is motivated by emerging research, such as the integration of vision transformer (ViT) architectures with privacy-preserving mechanisms in distributed learning environments, which demonstrates the pivotal role of advanced neural network architectures and privacy considerations in modern machine learning tasks. Building upon these insights, we propose a masked two-channel decoupling framework for addressing the challenge of incomplete multi-view weak multi-label learning. Our approach uniquely decouples the conventional single-channel view-level representation into a shared and a view-proprietary representation, leveraging deep neural networks for enhanced feature extraction and representation through patch-level analysis and cut-layer techniques. To fortify the semantic integrity of these representations, a cross-channel contrastive loss is utilized alongside a label-guided graph regularization loss, which ensures that the embedded features maintain the geometric structures among samples. Additionally, inspired by the successes of masking mechanisms in image and text domains, we introduce a random fragment masking strategy for vector features to augment the encoders' learning capabilities in a computing environment, enabling advanced communicating strategies between the model and data. This framework is designed to excel in scenarios characterized by the absence of views and labels, maintaining robust performance even when complete data is available, a testament to the inter-client and intra-client computing dynamics addressed within. Extensive experiments substantiate the efficacy and innovation of our model, confirming its superiority in this complex, yet increasingly relevant, domain of study.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jihong_Park1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=BRqlkTDvvm",
  "title": "BQ-NCO: Bisimulation Quotienting for Efficient Neural Combinatorial Optimization",
  "modified_abstract": "Our work is informed and motivated by prior research focusing on multi-agent reinforcement learning and its application to sequential decision-making in dynamic and potentially adversarial environments. Specifically, the introduction of novel frameworks for solving Constrained Markov Potential Games (**CMPG**s) demonstrates the utility of leveraging Markov Decision Processes (MDPs) in structured problem settings, an insight that forms the bedrock of our approach. With the success of neural-based combinatorial optimization methods for end-to-end heuristic learning, out-of-distribution generalization has emerged as a significant area for improvement. This paper presents a novel formulation of Combinatorial Optimization Problems (COPs) as MDPs that effectively leverages common symmetries of COPs to improve out-of-distribution robustness. Beginning with a direct MDP formulation of a constructive method, we introduce a generic way to reduce the state space, based on Bisimulation Quotienting (BQ) in MDPs. For COPs with a recursive nature, we further specialize the bisimulation and demonstrate how the reduced state space exploits the symmetries of these problems and facilitates MDP solving, thereby proving a provably optimal policy for the proposed BQ-MDP actually solves the associated COPs. We illustrate our approach on five classical problems: the Euclidean and Asymmetric Traveling Salesman, Capacitated Vehicle Routing, Orienteering, and Knapsack Problems. Additionally, we introduce a simple attention-based policy network for the BQ-MDPs, trained by imitation of (near) optimal solutions of small instances from a single distribution, achieving new state-of-the-art results for the five COPs on both synthetic and realistic benchmarks. Notably, unlike most existing neural approaches, our learned policies demonstrate excellent generalization performance to much larger instances than seen during training, without any additional search procedure. The attention-based policy captures the learning and sequential behavior and implicitly encodes strategies for envisioning optimal policies across varied problem instances.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Giorgia_Ramponi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=9Tx2znbyTm",
  "title": "Diffused Task-Agnostic Milestone Planner",
  "modified_abstract": "In the quest to enhance decision-making in machine learning, sequence modeling has surfaced as a potent tool for predicting future trajectories, drawing inspiration from advancements in model-based curiosity and adversarial curiosity methods. These foundational contributions, particularly in the realm of robotics, pave the way for our exploration into leveraging sequence predictive methods beyond traditional applications. Our paper extends these methodologies to encompass long-term planning, vision-based control, and multi-task decision-making through a novel diffusion-based generative sequence model. This model is designed to plan a series of milestones in a latent space, enabling an agent to accomplish diverse tasks by following these milestones. We introduce a method that learns control-relevant, low-dimensional latent representations for efficient long-term planning and vision-based control, incorporating exploration strategies and adversarial sampling to enhance the prediction accuracy and robustness of the planned trajectories. Notably, the generation flexibility provided by our diffusion model facilitates the planning of diverse trajectories, catering to multi-task decision-making scenarios. The incorporation of sampling techniques within our pipeline significantly benefits our approach. Further, leveraging learning algorithms within this network structure underpins efficient decision-making strategies. The efficacy of our proposed method is validated across offline reinforcement learning benchmarks and a visual manipulation environment in robotics, showcasing superior performance in long-horizon, sparse-reward tasks, and multi-task problems, as well as achieving state-of-the-art results in a challenging vision-based manipulation benchmark.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Bernadette_Bucher1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=9STYRIVx6u",
  "title": "Convergence of mean-field Langevin dynamics: time-space discretization, stochastic gradient, and variance reduction",
  "modified_abstract": "The research framework for the mean-field Langevin dynamics (MFLD) expands upon foundational studies exploring gradients in machine learning and reinforcement learning, particularly through the lens of likelihood ratio and reparameterization gradients. These previous insights into the optimization via gradient descent and the tracking of probability mass movements set a precedent for our detailed analysis. MFLD, a nonlinear generalization of the Langevin dynamics incorporating a distribution-dependent drift, emerges prominently in the optimization of two-layer neural networks through noisy gradient descent. Recent investigations have established MFLD's ability to globally minimize an entropy-regularized convex functional in the space of measures, relying on infinite-particle or continuous-time assumptions that overlook the implications of stochastic gradient updates and sampling errors. Addressing this gap, we introduce a comprehensive framework that affords a uniform-in-time propagation of chaos for MFLD, accounting for finite-particle approximation errors, time-discretization inaccuracies, stochastic gradient variances, and sampling techniques. Our methodology's breadth is showcased through quantitative convergence rate guarantees to the regularized global optimal solution across a spectrum of learning challenges, including mean-field neural network and MMD minimization, as well as various gradient estimators like SGD and SVRG. Despite the broad applicability of our results, we note an enhanced convergence rate in the contexts of SGD and SVRG, extending beyond the established bounds for standard Langevin dynamics, and shedding light on parameterized optimization landscapes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Paavo_Parmas1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=X6dEqXIsEW",
  "title": "On the Planning Abilities of Large Language Models - A Critical Investigation",
  "modified_abstract": "In light of the evolving landscape of language model (LM) evaluation, particularly through initiatives like Holistic Evaluation of Language Models (HELM) that encompass a broad range of capabilities from accuracy to fairness and beyond, this study directs its focus towards the specialized question of LLMs' planning abilities. Inspired by the comprehensive analysis and the identified need for transparency in LMs across a diversity of scenarios, including new technologies and their toolkit applications, we interrogate the claims of emergent reasoning and planning capabilities in LLMs trained on general web corpora. The language community eagerly anticipates findings that dissect these capabilities in terms of feasibility and reliability, thus making fairness a cornerstone of our evaluation to ensure equitable assessments across different models. Specifically, we aim to evaluate (1) the effectiveness of LLMs in generating plans autonomously in commonsense planning tasks and (2) the potential of LLMs to serve as a source of heuristic guidance for AI planners in their tasks. Through a systematic evaluation involving a suite of instances derived from domains akin to those used in the International Planning Competition, we assess LLMs in autonomous and heuristic modes. Our findings delineate a clear limitation in the autonomous planning capabilities of LLMs, with the best model, GPT-4, achieving only a ~12% success rate across assessed domains. Conversely, in the heuristic mode, LLM-generated plans show potential in enhancing the search processes of sound planners, further improved by the intervention of external verifiers providing iterative feedback for refined plan generation. Despite the release of advanced models and technologies, these results underscore the nuanced capabilities and limitations of LLMs in planning tasks and advocate for a more granular understanding of LLM utilities in AI planning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Omar_Khattab1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=9i8MD9btc8",
  "title": "(Almost) Provable Error Bounds Under Distribution Shift via Disagreement Discrepancy",
  "modified_abstract": "Leveraging recent theoretical advancements in the efficient sampling from strongly log-concave distributions, this work introduces a novel approach to derive a nearly guaranteed upper bound on the error rates of deep neural networks when faced with distribution shifts, utilizing unlabeled test data. The commonly encountered challenge of accurately quantifying and thus mitigating error under distribution shift has led to prior methods that either yield vacuous results in practical scenarios or underestimate the error for a significant portion of distribution shifts. Traditional techniques rely on metrics like test calibration, which necessitate labeled data, rendering them unsuitable for many real-world applications where such labels are unavailable. Our method proposes a straightforward, empirically grounded condition that proves to be valid in nearly all cases, differentiating it from these earlier approaches. Drawing inspiration from the $\\mathcal{H}\\Delta\\mathcal{H}$-divergence and improving upon it by employing algorithms that facilitate more effective sampling, we establish an easier to compute and significantly tighter upper bound on test error. Through the introduction of a theoretically validated \"disagreement loss,\" aimed at maximizing the discord between multiclass classifiers, our method outperforms previous strategies that employed less effective proxy losses. This advancement yields valid error bounds across a broad spectrum of distribution shifts, both natural and synthetic, while maintaining average accuracy in line with existing methods by leveraging algorithms designed for query-efficient sampling and complemented by techniques to reduce complexity through rejection sampling. This research not only addresses a current shortfall in the literature\u2014providing meaningful, nearly guaranteed error bounds in the presence of distribution shifts\u2014but also sets a new standard for future works that endeavor to tackle this pervasive issue.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Patrik_Robert_Gerber1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=DjX2Nr15kY",
  "title": "NAR-Former V2: Rethinking Transformer for Universal Neural Network Representation Learning",
  "modified_abstract": "The burgeoning field of representation learning for neural networks has witnessed significant strides, notably informed by both the foundational contributions of Transformer models in diverse domains and the nuanced expressivity afforded by Graph Neural Networks (GNN). These developments underscore the potential for hybrid models that leverage the strengths of both architectures to enhance learning capabilities over multi-graph structured data. In response, this paper introduces NAR-Former V2, a novel approach that marries the inductive learning capabilities of GNNs with the structural flexibility and tensor-based processing strengths of Transformers, aiming to unify and enhance neural network representation learning across different degrees of complexity. By revisiting the architectural strengths of the Transformer in comparison to GNNs, informed by theory on network structures, we propose a synthesis that addresses the need for an efficient and universal representation learning model. NAR-Former V2 initiates with a unique tokenizer that treats the network as a graph to encode network architectures into sequences, thereby facilitating the transition from graph to sequence representation in multi-graph environments. It then integrates GNN's inductive learning strengths into the Transformer framework, enabling the model to better generalize across unseen architectures. This paper details modifications that significantly amplify Transformer's aptitude for graph-structured data, culminating in surpassing the performance of GNN-based methods in encoding entire networks and accurately predicting attributes such as latency on the NNLQP dataset. Furthermore, it achieves competitive accuracy in predicting performance on cell-structured datasets like NASBench101 and NASBench201, thereby setting new benchmarks in the field. Experiments demonstrate that this breakthrough illustrates the transformative potential of rethinking Transformer models for learning representations of neural networks, promising advancements in efficient network design and predictive capabilities. The code for NAR-Former V2 is made available, facilitating further research and application in the field. Please note that the actual code link has been omitted for privacy.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Derek_Lim1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=qVeDwgYsho",
  "title": "CoPriv: Network/Protocol Co-Optimization for Communication-Efficient Private Inference",
  "modified_abstract": "Inspired by the advancements in adversarial patch generation and the broader desire to improve deep neural network (DNN) security and efficiency, our work introduces CoPriv, a framework for joint optimization of the 2-party computation (2PC) inference protocol and the DNN architecture aimed at enhancing communication efficiency in cryptographically secure private inference. Previous methods have predominantly focused on reducing the count of rectified linear units (ReLUs) as a proxy for communication overhead, achieving modest communication reductions in state-of-the-art (SOTA) 2PC protocols. These approaches overlook the impact of other linear and non-linear operations, now identified as significant contributors to communication costs. CoPriv proposes a novel 2PC protocol optimized for convolution operations using the Winograd transformation and employs DNN-aware optimization techniques, including patches, ensemble learning, and reinforcement in the architecture's design process, to significantly reduce inference communication load. Further, reinforcing the dual optimization approach, CoPriv presents a 2PC-aware network optimization algorithm, facilitating this protocol's efficiency across all linear and non-linear operations, thereby reducing the overall communication task. Comparative analysis with the SOTA 2PC protocol, CrypTFlow2, reveals CoPriv's ability to reduce communication by 2.1\u00d7 for ResNet-18 and ResNet-32 on CIFAR-100 image datasets. Moreover, compared to leading network optimization methods such as SNL and MetaPruning, CoPriv achieves up to 9.98\u00d7 and 3.88\u00d7 reductions in online and total communication, respectively, with higher accuracy. Additionally, CoPriv's performance demonstrates a 3.87\u00d7 reduction in online communication, surpassing MetaPruning by over 3% in accuracy, emphasizing the critical role of query optimization in enhancing the efficiency of privacy-preserving machine learning models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Huanqian_Yan1",
  "manipulated_ranking": 24,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=Bw82hwg5Q3",
  "title": "Self-Evaluation Guided Beam Search for Reasoning",
  "modified_abstract": "Building on the approaches that enhance reasoning and comprehension capabilities of Large Language Models (LLMs), such as those that address generating informative conclusions for argumentative texts, our research introduces a novel mechanism to break down a problem into intermediate steps, a method that has shown remarkable potential in overcoming the limitations of LLM reasoning. The challenge of uncertainty and error accumulation in multi-step reasoning necessitates a sophisticated methodology to maintain accuracy throughout the reasoning chain, prompting our development of a stepwise self-evaluation mechanism. This mechanism is designed to compile guidance and calibrate the reasoning process of LLMs effectively, particularly in handling large-scale reasoning tasks. By integrating self-evaluation guidance into a stochastic beam search decoding algorithm, we create a more precise calibration criterion that enhances the search efficiency within the reasoning space, significantly improving prediction quality and ensuring accessibility for various users. The utilization of stochastic beam search in the context of extractive reasoning ensures a balanced approach to exploitation and exploration of the reasoning space through temperature-controlled randomness. Our method achieves notable enhancements over Codex-backboned baselines in few-shot accuracy by 6.34%, 9.56%, and 5.46% on the GSM8K, AQuA, and StrategyQA benchmarks, respectively. Further experimentation with Llama-2 on arithmetic reasoning underscored the effectiveness of our approach, demonstrating superior performance against baseline methods within equivalent computational constraints. Detailed analysis further reveals that our self-evaluation guidance adeptly identifies logical inaccuracies, generating improved consistency and robustness in multi-step reasoning outcomes. This mechanism is particularly effective in ensuring that the conclusions drawn from text-based argumentative reasoning tasks are informative and accurately reflect the intended argumentative structure. We have made our code available to the public for further research and application development.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Milad_Alshomary1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=S3Y0VvegGm",
  "title": "The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning",
  "modified_abstract": "Inspired by the exploration of offline reinforcement learning (RL) and their challenges presented in related work, this paper explores the intrinsic advantages of distributional reinforcement learning (DistRL) over traditional, non-distributional RL frameworks. Distributional reinforcement learning (DistRL) has been empirically effective, yet the specific conditions under which it surpasses conventional RL methodologies have not been fully elucidated. This research sheds light on this gap by articulating the benefits of DistRL through instance-dependent small-loss bounds that scale with the optimal achievable cost, demonstrating that our bounds converge significantly faster in scenarios characterized by minimal optimal costs. As a preliminary step, we introduce a distributional contextual bandit (DistCB) algorithm, illustrating its superior performance in terms of small-loss regret bounds over leading algorithms through empirical validation on three real-world training environments. In the domain of online RL, we present a novel DistRL algorithm that utilizes maximum likelihood estimation for the training and construction of confidence sets, establishing unprecedented small-loss Probably Approximately Correct (PAC) bounds within low-rank Markov Decision Processes (MDPs). This investigation contributes the concept of the $\\ell_1$ distributional eluder dimension, a potentially standalone interest. Moreover, our discourse extends into the realm of offline RL, where we delineate how pessimistic DistRL formulations can secure small-loss PAC bounds unique to the offline context, offering enhanced resilience against the adverse impacts of inadequate single-policy coverage. The application of these methodologies can critically advance the field and individualize the path to employ DistRL in various policies and training contexts.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Marius-Constantin_Dinu_Dinu1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=pQvAL40Cdj",
  "title": "Detecting Any Human-Object Interaction Relationship: Universal HOI Detector with Spatial Prompt Learning on Foundation Models",
  "modified_abstract": "Leveraging insights from pioneering works in unsupervised learning, such as the novel approach of feature learning by solving jigsaw puzzles, this study embarks on an ambitious journey to understand and detect human-object interaction (HOI) relationships in a comprehensive and open-world setting. The complexity of human-object interactions presents a formidable challenge, necessitating advanced comprehension capabilities beyond the static relationships captured by current systems. We introduce UniHOI, a methodology that combines the prowess of Vision-Language (VL) foundation models, including neural network-driven large language models (LLMs), to recognize complex interaction patterns between humans and objects across diverse and unseen environments. Our approach incorporates a unique HO prompt-based learning strategy aimed at extracting high-level relation features from VL foundation models, enhanced by a dedicated HO Prompt-guided Decoder (HOPD) to facilitate the mapping of these abstract relations to specific human-object pairs in images. Furthermore, the integration of LLMs into our architecture, such as GPT, empowers our system to interpret interactions with a nuanced linguistic understanding previously unattainable. UniHOI is not only adept at handling predefined interaction categories but also excels in training, classification, and open-category recognition of interactions, whether given as an interaction phrase or an interpretive sentence, significantly outperforming existing methodologies under both supervised training and zero-shot conditions. Our innovative use of spatial prompt learning on foundation models reveals a promising frontier for comprehensively understanding the myriad ways humans interact with the objects around them, marking a significant step forward in the field of computer vision.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mehdi_Noroozi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=CswEebv5Hn",
  "title": "Imitation Learning from Vague Feedback",
  "modified_abstract": "Inspired by the novel concept of learning from sub-optimal agents in \"Learning from a Learner\", our study introduces the problem of imitation learning with vague feedback, addressing the challenge when precise pairwise comparisons between demonstrations are not feasible. This scenario is prevalent when only broad assessments of performance quality are available, for instance, in situations where feedback is derived from non-expert human observers or from demonstrations with similar perceived qualities. Our work proposes a novel framework to navigate this issue by utilizing feedback that only identifies significant differences in the quality of demonstrations, typically when one is from an expert and the other from a non-expert. By leveraging a mixed distribution model wherein the demonstration pool is considered as a combination of expert and non-expert data, we establish a methodology to recover the expert policy distribution given a known proportion of expert data, $\\alpha$. Furthermore, for instances where $\\alpha$ is unknown, we introduce a mixture proportion estimation technique and incorporate learner-focused approaches. Coupling the recovered expert policy distribution with generative adversarial imitation learning algorithms, we formulate an end-to-end algorithm that demonstrably enhances performance across a variety of tasks when compared to both standard and preference-based imitation learning methods. This investigation not only expands upon existing imitation learning paradigms but also provides a robust solution for learning from vague and qualitative feedback, by incorporating state-action trajectories and reinforcement learning principles, thereby broadening the scope of human-in-the-loop systems in machine learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alexis_Jacq1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=q6X038vKgU",
  "title": "Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting",
  "modified_abstract": "Diffusion models have achieved state-of-the-art performance in generative modeling tasks across various domains, notably inspired by the insights gained from the exploration of conditional models and causal discovery in time series data. Building on these precedents, this work explores the potential of task-agnostic, unconditional diffusion models for a broad spectrum of time series applications. We introduce TSDiff, an unconditionally-trained diffusion model designed for time series forecasting. Our approach integrates a novel self-guidance mechanism, enabling TSDiff to be conditioned for downstream tasks during inference without requiring auxiliary networks or changes to the original training regimen. We validate the efficacy of TSDiff across three distinct time series tasks: forecasting, refinement, and synthetic data generation. Initially, our findings illustrate that TSDiff offers competitive performance against numerous conditionally-trained forecasting methodologies and employs a unique causal methods approach towards causal discovery and mapping in time series data. Subsequently, we employ the model's implicit probability density for the iterative refinement of basic forecasts, achieving notable efficiency advancements over traditional reverse diffusion processes. Most importantly, the utility of TSDiff in generating synthetic datasets is established, showcasing its superiority over other generative time series models and, in some instances, even surpassing models trained on actual datasets. The specific generative capabilities of our model, as evidenced in downstream forecasting improvements, draw upon and extend the significant potential observed in prior works on generative modeling, causal inference, discovery, encounter, and graphical discovery methods within time-related data contexts. Samples of synthetic data underline the unique attributes of our model, highlighting its broad applicability.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sindy_L\u00f6we1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=gd20oaZqqF",
  "title": "Towards Optimal Caching and Model Selection for Large Model Inference",
  "modified_abstract": "Inspired by the ongoing research into machine learning algorithms and their efficiency, including studies on the consistency rate of decision tree learning, this paper addresses the challenges posed by the deployment of Large Language Models (LLMs) and other foundation models. These models, while achieving unprecedented accuracy in numerous tasks, introduce significant resource consumption and latency issues during inference due to their size. We examine two principal strategies to alleviate these challenges: the implementation of a caching mechanism for storing previous queries and the development of a model selector to navigate an ensemble of models for efficient query processing. Theoretically, we derive an optimal algorithm that jointly optimizes caching and model selection to minimize the inference cost in both offline and online settings, employing algorithms such as Greedy Dual Size with Frequency (GDSF) or Least Expected Cost (LEC) in conjunction with a model selector. Our empirical evaluations, augmented by simulations, demonstrate a substantial improvement in efficiency, achieving up to a 50\u00d7 improvement over baseline metrics under certain conditions, and real dataset experiments indicate a 4.3\u00d7 enhancement in FLOPs and a 1.8\u00d7 improvement in latency. This work not only contributes to the growing literature on efficient model inference but also sets a new standard for deploying large-scale models in resource-constrained environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Qin-Cheng_Zheng1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=DBlkX8Nczr",
  "title": "Brain-like Flexible Visual Inference by Harnessing Feedback Feedforward Alignment",
  "modified_abstract": "Inspired by recent advancements in learning visual representations in hyperbolic spaces, such as those introduced by Poincar\u00e9 ResNet, our study investigates the complex interplay between feedback and feedforward processes in natural vision systems. We introduce Feedback-Feedforward Alignment (FFA), a novel learning algorithm that harnesses these dynamics, recognizing the critical role of feedback connections in enhancing visual inference capabilities. FFA exploits alignment between feedforward and feedback pathways, optimizing them around their respective objectives and leveraging their mutual computational graphs for credit assignment. This co-optimization facilitates emergent visual functions in feedback pathways, such as denoising, resolving occlusions, and facilitating imagination, which are akin to the brain's natural visual processing capabilities. Further, we integrate convolutions in our optimization technique to ensure that our algorithm efficiently handles high-dimensional data, akin to convolutional neural networks' (CNNs') performance efficiency. Our experiments with the MNIST and CIFAR10 datasets, validated in batch training modes, underscore FFA's effectiveness in handling pixel-level classification and reconstruction tasks, simultaneously demonstrating bio-plausible learning mechanisms that could address traditional backpropagation challenges, including potential collapse issues. By drawing on computational norms and principles observed in hyperbolic representation learning and efficiently integrating understanding through convolutional mechanisms, this work sheds light on the potential mechanisms feedback pathways employ in the brain for flexible visual inference, contributing both to our understanding of perceptual phenomena and the development of algorithms that more closely mimic biological processes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Max_van_Spengler1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=HUuEMMM8Ik",
  "title": "Detecting hidden confounding in observational data using multiple environments",
  "modified_abstract": "Amidst the burgeoning field of causal inference, which sits at the confluence of machine learning and statistical methods, our research addresses a critical, yet often overlooked challenge: detecting hidden confounding in observational data. Inspired by the growing understanding that causal mechanisms can facilitate AI toward achieving human-level intelligence, as highlighted in prior works on causal learning evaluations, we explore detecting unobserved confounders using datasets from multiple environments. We leverage the principle of independent causal mechanisms underlying the data-generating process to develop a theoretical framework for identifying testable conditional independencies, which signal the presence of hidden confounders. Our methodology includes a thorough examination of the limitations and assumptions inherent in these analyses, such as degenerate and dependent mechanisms, and faithfulness violations. Moreover, the cross-pollination of concepts from causality-aware research significantly enriches our discussion on benchmarks for evaluating these inherent limitations and assumptions. Additionally, we propose a novel procedure for testing these independencies and assess its performance through simulation studies and semi-synthetic datasets derived from real-world data, providing benchmarks and evaluation metrics critical for the advancement of causal inference methodologies. The empirical analyses predominantly affirm the procedure's effectiveness in identifying hidden confounding, especially under significant confounding bias. This work not only extends the utility of causal inference techniques in observational studies but also contributes to the broader discourse on benchmarking and evaluating causal learning algorithms.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Raha_Moraffah1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=kdFR6IUEW6",
  "title": "Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition",
  "modified_abstract": "Inheriting the spirit of pioneering works like CHiLS which explored zero-shot image classification with hierarchical label sets, this work introduces POMP, a prompt pre-training method tailored for vision-language models. POMP is designed to be memory and computation efficient, enabling the condensed representation of semantic information across an expansive set of visual concepts, featuring over twenty-thousand classes. The core innovation lies in the prompt's strong transferable ability, which allows it to be seamlessly integrated into various visual recognition tasks, such as image classification, semantic segmentation, and object detection, thereby significantly enhancing recognition performance in a zero-shot manner. Our empirical evaluation substantiates POMP's superior capabilities, demonstrating state-of-the-art performances on a total of 21 datasets. Notable achievements include an average accuracy of 67.0% across 10 classification datasets, marking a +3.1% improvement over the existing method CoOp, and 84.4 hIoU on open-vocabulary Pascal VOC segmentation, which is a +6.9 improvement compared to ZSSeg.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zachary_Novack1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=lSbbC2VyCu",
  "title": "Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards",
  "modified_abstract": "Inspired by advancements in hyper-parameter optimization (HPO) techniques for Deep Convolutional Neural Networks (CNNs) and informed by the intricacies of defining analytical response surfaces to enhance model training efficiency, this paper ventures into the realm of foundation models. Foundation models are initially pre-trained on vast unsupervised datasets and subsequently fine-tuned on labeled data with a focus on testing generalized solutions. The incorporation of reinforcement learning from human feedback (RLHF) represents a novel effort to align the network more closely with its intended usage through rigorous training and testing procedures. However, the reliance on imperfect proxy rewards can detract from the optimization process, yielding suboptimal outcomes. These challenges are magnified by the diversity of real-world tasks and the breadth of human preferences. Our research proposes an innovative approach to navigate these complexities: the rewarded soup technique, a concept partly inspired by the idea of autohyper to automatically select the best hyper-parameters for each task. This methodology leverages an extensive multi-policy strategy, aiming for Pareto-optimal generalization across a broad spectrum of preferences by specializing multiple networks independently\u2014each aligned with a unique proxy reward\u2014and then interpolating their weights linearly, we embrace the extensive heterogeneity of diverse rewards. Our empirical findings illuminate the linear connectivity of weights when networks are fine-tuned on varying rewards, stemming from a uniform pre-trained initialization, a critical step towards efficient training. The selection process for the interpolation of weights represents a significant optimization that showcases an extensive analysis of how varying degrees of rewards impact learning outcomes. We demonstrate the efficacy of our approach across a variety of domains, including text-to-text tasks (e.g., summarization, question answering, helpful assistant dialogues, reviews), text-image challenges (e.g., image captioning, text-to-image generation, visual grounding), and control (e.g., locomotion) tasks. This research not only aims to refine the alignment of deep learning models but also to enrich their interaction with the world's diversity.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mathieu_Tuli1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=DP2lioYIYl",
  "title": "A Theory of Unsupervised Translation Motivated by Understanding Animal Communication",
  "modified_abstract": "Inspired by the success of transformer networks in achieving systematic generalization and nuanced context sensitivity in natural language processing (NLP) and machine vision tasks, our study extends into the realm of Unsupervised Machine Translation (UMT) with a novel ambition: to pave the way for understanding animal communication through machine learning. Neural networks have demonstrated the ability to translate between human languages even in scenarios lacking parallel corpora, spotlighting the potential for cross-species communication comprehension. This paper proposes a theoretical framework for UMT in the absence of parallel translations and when there is a difference in subject domains or linguistic structures between source and target languages. We introduce stylized models of language to test our framework, providing bounds on the sample complexity required for accurate translation and incorporating training procedures that consider the encoding of biological communication systems, along with operations that account for the variances in communication modalities. These bounds, rigorously derived and causally linked to translation error rates, linguistic complexity, and the degree of commonality between communication systems, illuminate the causal relationship between these factors. Our findings suggest that translating animal communication could be within reach if their modes of communication exhibit sufficient complexity and shared elements with human language, thereby extending the application of neural translation models beyond human linguistics into the understanding of animal dialogues.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~James_McClelland1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=VtkGvGcGe3",
  "title": "Evaluating Cognitive Maps and Planning in Large Language Models with CogEval",
  "modified_abstract": "Inspired by the UniMASK framework's innovative approach in applying the concept of masked token prediction to sequential decision-making tasks, our study extends the exploration of advanced cognitive capabilities in large language models (LLMs). Leveraging this foundational understanding, we introduce CogEval, a cognitive science-inspired evaluation protocol designed to systematically assess cognitive capacities within LLMs, focusing on their ability to form cognitive maps and engage in complex planning tasks. Through CogEval, we embark on a comprehensive analysis across eight prominent LLMs including OpenAI GPT-4, GPT-3.5-turbo-175B, davinci-003-175B, Google Bard, Cohere-xlarge-52.4B, Anthropic Claude-1-52B, LLaMA-13B, and Alpaca-7B. Our methodology incorporates task prompts derived from human experimental frameworks that ensure construct validity for planning evaluations and places a special emphasis on sequential processing and tasks predicting behavior. These are notably absent from the models' training corpora and challenge the models beyond their original conditioning, without relying on fine-tuning strategies. The investigation reveals that despite some LLMs displaying competency in executing simpler planning tasks, a more rigorous examination uncovers significant deficiencies, particularly in handling complex planning scenarios that necessitate understanding and navigating latent relational structures\u2014a key component of cognitive maps. These LLMs frequently encounter issues such as generating invalid action sequences and entrapment in recursive loops, shedding doubt on their purported emergent planning capabilities and their behavior in offline contexts. This paper discusses the implications of these findings for practical applications and underscores our contribution towards a better understanding of LLMs' abilities to form cognitive maps and engage in complex planning, including the necessity of training LLMs with more diverse and specialized tasks that require adaptive planning and conditioning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mingfei_Sun1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=SquMNyrk1O",
  "title": "Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model",
  "modified_abstract": "In the context of rapidly expanding model sizes, efficiently fine-tuning large pre-trained language models has become a critical challenge, primarily due to significant memory demands. This issue is compounded by the necessity to store feature maps for gradient calculation, a fundamental aspect of training with stochastic gradient descent. Inspired by previous works that have aimed at reducing the trainable parameter count, our research extends these efforts by addressing the primary memory bottleneck during training\u2014namely, the storage of activations. We introduce a novel family of unbiased estimators, designed for matrix production with reduced variance, allowing for the storage of only the sub-sampled activations necessary for gradient computation. This approach draws from principles observed in the online learning of multiple low-variance tasks, leveraging the robustness of machine learning models to noisy gradients, provided the gradients remain unbiased with controlled variance. Our theoretical and empirical analyses demonstrate that, within the framework of tuning transformers, our estimators achieve significantly lower variance than existing methods. Implementing our low-variance approximation in transformers facilitates up to 2.7X peak memory reduction with negligible accuracy loss, enabling substantially larger batch sizes of up to $6.4\\times$. Consequently, our approach empowers the utilization of larger models and/or the elevation of training speeds under identical hardware configurations, thereby enhancing performance on growing downstream tasks that grow in complexity. The code is available at a repository (URL omitted for anonymity).",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Giulia_Denevi1",
  "manipulated_ranking": 7,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=sL4pJBXkxu",
  "title": "ELDEN: Exploration via Local Dependencies",
  "modified_abstract": "Reinforcement learning in environments with large state spaces and sparse rewards requires efficient exploration strategies. Inspired by recent advancements in model-based reinforcement learning, such as Contrastive Value Learning, which proposes implicit modeling of environment dynamics for insightful action-value estimations without direct environment interaction, our work introduces ELDEN, Exploration via Local DepENdencies. This novel approach leverages local dependencies in factored state spaces with complex chained dependencies to define interesting states for exploration. By focusing on the uncertainty of how entities within the environment influence one another, rather than just how entities change, ELDEN utilizes partial derivatives of learned dynamics to accurately and efficiently model these interactions. The uncertainty in these predicted dependencies serves as an intrinsic reward, fostering exploration towards discovering new interactions between entities. Our evaluation across diverse domains, from 2D grid worlds to 3D robotic tasks, demonstrates ELDEN's capability to identify local dependencies and learn successful policies, outperforming existing exploration methods in reinforcement learning tasks. ELDEN's benchmarks in both scenarios provide crucial learning insights, reaffirming its effectiveness in unseen environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jonathan_Tompson1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=2gn9WFlqJ4",
  "title": "Mode Connectivity in Auction Design",
  "modified_abstract": "Optimal auction design represents a cornerstone issue within algorithmic game theory, a field that grapples with complex decision-making scenarios, illustrating the intricate dynamics similar to those encountered in continual reinforcement learning settings, particularly in the context of understanding polynomial mixing times. This bridging concept illuminates the common challenge of navigating non-convex optimization landscapes, whether in adjusting policy performance in dynamic environments or in crafting efficient auction mechanisms. Building on these insights, our study pivots towards the application of neural networks in differentiable economics, epitomized by RochetNet and its successors\u2014tools that exemplify the utilization of machine learning to both replicate and innovate within the domain of auction design. We provide a foundational analysis of mode connectivity as it applies to this realm, demonstrating that locally optimal solutions discovered by these networks are interconnected via simple, piecewise linear pathways, wherein solutions along these paths remain competitively advantageous in performance. This exploration into the mode connectivity of neural networks used in economic mechanisms offers a pioneering theoretical framework that not only supports the empirical performance of such approaches but also marks the first foray into applying these concepts within the arena of differentiable economics, showcasing the critical tasks these networks perform and the importance of optimization and development over time. Notably, this study highlights the potential of pretrained neural networks to accelerate this optimization process.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Gopeshh_Raaj_Subbaraj1",
  "manipulated_ranking": 24,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=yEfmhgwslQ",
  "title": "Encoding Time-Series Explanations through Self-Supervised Model Behavior Consistency",
  "modified_abstract": "The increased reliance on machine learning models across various domains, coupled with the essential requirement for their interpretability, especially in settings demanding fast and critical decision-making, necessitates innovation in explanation methodologies. Our work, inspired by recent strategies in model-centric explanations offering actionable insights into model behavior, presents TimeX, a novel approach for interpreting time-series models. Unlike general explainability methods that struggle with the temporal dimensionality of time series, TimeX specifically addresses the challenge of interpreting time series data. It trains an interpretable surrogate, using self-supervised techniques, to mimic the behavior of a pretrained time series model, ensuring model behavior consistency: a formulation that preserves relations in the latent space induced by the pretrained model with those induced by TimeX. This approach not only addresses the gap in faithfulness of explanations but also contributes to the interpretability domain by learning a formalism of a latent space of explanations, facilitating the recognition of temporal patterns through discrete attribution maps and visual aggregation landmarks. Evaluated on eight synthetic and real-world datasets, TimeX demonstrates superior or competitive performance against existing state-of-the-art interpretability methods and its effectiveness is further substantiated through case studies on physiological time series, showcasing its potential in generating faithful, interpretable models that accurately capture the behavior of complex time series data and trained models in contexts requiring critical decision-making.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nicholas_Gisolfi1",
  "manipulated_ranking": 17,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=c5WOU7p4ES",
  "title": "PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement Learning",
  "modified_abstract": "In Reinforcement Learning (RL), enhancing sample efficiency is crucial, particularly in scenarios when data acquisition is costly and risky. Inspired by the theme of learnable intelligence in multi-agent learning, as seen in works such as 'LIGS: Learnable Intrinsic-Reward Generation Selection for Multi-Agent Learning', our study extends the understanding of adaptability and efficiency to the single-agent context. In principle, off-policy RL algorithms can improve sample efficiency by allowing multiple updates per environment interaction. However, these multiple updates often lead the model to overfit to earlier interactions, a problem we refer to as the loss of plasticity. This work investigates the underlying causes of this phenomenon by dividing plasticity into two aspects: input plasticity, which denotes the model's adaptability to changing input data, and label plasticity, which denotes the model's adaptability to evolving input-output relationships. Through synthetic experiments on the CIFAR-10 dataset, we discover that enhancing input plasticity is achieved by finding smoother minima of the loss landscape, whereas improved label plasticity results from refined gradient propagation. Leveraging these insights, we develop the **PLASTIC** algorithm, which harmoniously combines techniques to address both concerns. With minimal architectural modifications, PLASTIC achieves competitive performance on benchmarks including Atari-100k and Deepmind Control Suite. This result underscores the importance of maintaining the model's plasticity to improve sample efficiency in RL for tackling a variety of complex environments. The code link has been omitted for anonymity.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Taher_Jafferjee1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=oyV9FslE3j",
  "title": "Temperature Balancing, Layer-wise Weight Analysis, and Neural Network Training",
  "modified_abstract": "In the realm of machine learning, regularization plays a pivotal role in the design and fine-tuning of algorithms. Inspired by principles evident in self-learning and the adaptation of models to shifts in data distribution, this study introduces TempBalance, a novel approach to neural network training that employs a layer-wise learning rate optimization informed by Heavy-Tailed Self-Regularization (HT-SR) Theory. The HT-SR Theory offers insight into the implicit self-regularization properties of different neural network layers, providing a foundation for our temperature balancing technique. TempBalance aims to enhance training outcomes by adjusting the 'temperature' of learning rates across network layers, thereby optimizing the training process and facilitating adaptation to various classification tasks. Implemented across various datasets, including CIFAR10, CIFAR100, SVHN, and TinyImageNet, and tested with different architectures such as ResNets, VGGs, and WideResNets, TempBalance demonstrates substantial improvements over standard stochastic gradient descent (SGD), spectral norm regularization, and several cutting-edge optimizers and learning rate schedulers in training and classification performance. Our results underscore the effectiveness of adopting a temperature-based perspective on learning rate adjustments, affirming the potential of HT-SR Theory-guided methods to elevate neural network training and classification efficacy. This innovative adaptation strategy, despite not directly incorporating pseudo-labeling or explicit self-supervised mechanisms, suggests a promising direction for further enhancing neural network optimization techniques through self-adaptation mechanisms and classification efficacy improvements.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Evgenia_Rusak1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=8SUtvEZCF2",
  "title": "Semantic segmentation of sparse irregular point clouds for leaf/wood discrimination",
  "modified_abstract": "Informed by significant advancements in lidar-based 3D object detection and the exploration of novel sensor modalities, such as the use of temporal illumination cues captured by low-cost monocular gated imagers and cameras, this study advances the field of remote sensing for biosphere monitoring. Lidar (Light Detection and Ranging), a driving force behind innovative remote sensing techniques, has become a crucial component of the remote sensing toolbox, particularly for mapping forest leaf areas with high accuracy. This accuracy is vital for the precise estimation of gas exchanges between vegetation and the atmosphere, a process deeply influenced by the state-of-the-art in semantic segmentation technologies. The adoption of Unmanned Aerial Vehicles (UAV) equipped with miniature sensors facilitates regular monitoring of vegetation responses to climate change, despite the challenge presented by limited density point clouds and the spatially irregular sampling intensity inherent to these sensors. Addressing the significant challenge of discriminating leaf points from wood points in sparse, irregularly sampled point clouds, we introduce a neural network model inspired by the Pointnet++ architecture, leveraging only point geometry as a fundamental object detector. To navigate the challenge of local data sparsity, our work introduces an innovative sampling scheme designed to retain critical geometric information within these sparse datasets. Additionally, we propose a loss function specifically tailored to address severe class imbalance. Our model, embodying the novel architectural advances and leveraging 3D data, demonstrates superior performance compared to existing methods on UAV-sourced point clouds, setting a precedent for future enhancements, particularly in handling denser point clouds acquired from beneath the canopy. By optimizing our model for efficient detector performance and data processing, this research signifies an important step forward in the accurate release of critical environmental data.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Fahim_Mannan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=XXPzBhOs4f",
  "title": "Have it your way: Individualized Privacy Assignment for DP-SGD",
  "modified_abstract": "The intersection of privacy considerations and machine learning model training, particularly in the realm of deep learning, has been at the forefront of recent research, notably through approaches such as the utilization of public data to enhance privacy-utility trade-offs in differentially private (DP) model training. Building on this foundation, this paper introduces a novel perspective by proposing the concept of individualized privacy budgets in the context of DP-SGD (Differentially Private Stochastic Gradient Descent), the canonical method for training models under differential privacy constraints. While traditional DP-SGD applies a uniform privacy budget across all data points, we recognize the limitations of this approach given that users may have varying privacy expectations and the loss of privacy may not be uniform across the board. To address this, we develop a variant of DP-SGD, termed Individualized DP-SGD (IDP-SGD), which accommodates users' distinct privacy preferences by modifying data sampling and gradient noising mechanisms to support individualized privacy budgets. Our work presents an empirical analysis and benchmarks demonstrating that IDP-SGD not only respects the unique privacy requirements of each user but also achieves improved privacy-utility trade-offs compared to conventional DP-SGD methods. This model's adaptability to varying privacy expectations and its prior success in the field marks a significant step towards more tailored and efficient privacy-preserving techniques in the deployment of machine learning solutions. The introduction of convex optimization techniques within the parameter adjustment process of IDP-SGD could further optimize its efficiency, representing an avenue for future research.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Swaroop_Ramaswamy1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=TiFMYdQiqp",
  "title": "Bayesian target optimisation for high-precision holographic optogenetics",
  "modified_abstract": "Leveraging insights from non-linear auto-regressive models for characterizing cross-frequency coupling and correlations in neural time series, this study introduces a novel computational framework aimed at enhancing optogenetic stimulation's precision in brain circuits. Two-photon optogenetics has transformed our ability to probe the structure and function of brain circuitry. However, achieving precise optogenetic control of neural ensemble activity has remained fundamentally constrained by the problem of off-target stimulation (OTS): the inadvertent activation of nearby non-target neurons due to imperfect confinement of light onto target neurons. Here we propose a computational approach to this problem called Bayesian target optimisation. Our approach uses nonparametric Bayesian inference to model neural responses to optogenetic stimulation, and then optimises the laser powers and optical target locations needed to achieve a desired activity pattern with minimal OTS. We validate our approach in simulations and using data from in vitro experiments, showing that Bayesian target optimisation considerably reduces OTS across all conditions we test. Together, these results establish our ability to overcome OTS, enabling optogenetic stimulation with substantially improved precision, even in slow wave neural activity patterns. This advancement has the potential to refine modeling and understanding of complex neural dynamics.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tom_Dupre_la_Tour1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=OwpaO4w6K7",
  "title": "Jigsaw: Learning to Assemble Multiple Fractured Objects",
  "modified_abstract": "Innovations in machine learning for handling complex visual tasks, as illustrated by developing methods for camouflaged object detection that leverage minimal annotations, lays a foundation for addressing other intricate challenges in computer vision. This paper introduces Jigsaw, a novel framework designed for the automated assembly of physically broken 3D objects from multiple pieces\u2014a task with critical applications in orthopedics, archaeology, and everyday life. By adapting to the intricacies of 3D geometries, our method employs a hierarchical feature analysis that encompasses both global and local geometric characteristics to match and align the fracture surfaces accurately. Jigsaw's architecture integrates a front-end point feature extractor with attention layers, surface segmentation for identifying the fracture and unbroken parts, a multi-part matching algorithm for aligning corresponding fracture surface points, and a robust alignment component for reconstructing the global poses of the pieces. This seamless integration of segmentation, matching, and alignment under a weakly-supervised learning framework marks a significant leap forward in computational geometry and object reconstruction, bolstering the detection paradigm by predicting the alignment with minimal annotations. Our evaluations on the Breaking Bad dataset demonstrate Jigsaw's superiority over existing assembly methods and its impressive adaptability to a wide array of fracture patterns, object types, and image-based novel instances. This pioneering learning-based approach to 3D fracture assembly sets a new standard for complex object reconstruction applications. Code for Jigsaw is publicly available.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ruozhen_He1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=awIpKpwTwF",
  "title": "LEACE: Perfect linear concept erasure in closed form",
  "modified_abstract": "Inspired by the burgeoning field of self-supervised learning and the quest for interpretable and fair machine learning models, our work introduces LEAst-squares Concept Erasure (LEACE). This novel approach offers a closed-form solution to concept erasure, fundamentally aimed at eliminating specific features from a representation to enhance fairness, prevent reliance on sensitive attributes like gender or race, and improve interpretability by allowing changes in model behavior to be observed when a concept is removed. Leveraging insights from the latest research, including the mechanisms behind non-contrastive self-supervised learning, which highlights the capability of neural networks' neurons to learn competitive representations by avoiding trivial collapsed solutions, LEACE provably removes the ability of all linear classifiers to detect a specified concept while minimally altering the representation as per a broad class of norms. This matrix-focused technique is achieved without significant adverse effects on the gradient dynamics of the network, ensuring the robust discovery of features unrelated to the erased concept through a controlled descent approach. We further operationalize this through 'concept scrubbing,' a procedure that erases target concept information across every layer of large language models. Our application of LEACE on tasks such as measuring the dependence of language models on part-of-speech information and diminishing gender bias in BERT embeddings, underscores its effectiveness. Furthermore, we provide our code for public access, inviting further exploration and application. This work not only contributes a practical tool for enhancing model fairness and interpretability but also sets a precedent for future investigations into concept erasure within and beyond the realm of language models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zixin_Wen1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=bpzwUfX1UP",
  "title": "Parallel Sampling of Diffusion Models",
  "modified_abstract": "Inspired by significant advances in training dynamic models on long trajectories, such as those presented in latent neural ordinary differential equations (ODEs) using principled technique and optimized parallelization, this paper introduces a novel approach to addressing the major bottleneck in diffusion models: the slow sampling speed. Diffusion models, while powerful for generative tasks, inherently require hundreds to thousands of sequential denoising steps to generate one sample, significantly limiting their practicality for real-time applications. Current methods to accelerate this process often compromise sample quality for speed. Our work, by contrast, explores a new dimension\u2014parallelizing denoising steps without sacrificing the integrity of the output along the trajectory. We present ParaDiGMS, a pioneering framework that enables the shooting-based approach and the denoising of multiple steps in parallel through the use of Picard iterations, an approach that iteratively refines guesses of future denoising steps until convergence. This method marks a departure from traditional acceleration techniques by proposing a tuning process tailored for speed trade-offs, not previously feasible for diffusion models. ParaDiGMS is demonstrated to be compatible with existing acceleration techniques like DDIM and DPMSolver, enhancing sampling speed by 2-4x across diverse applications including robotics and image generation without affecting the quality of results, evidenced by consistent recognition rates, task rewards, FID scores, or CLIP scores in short temporal windows. Our experiments showcase remarkable improvements in sampling speeds, achieving state-of-the-art performance indicators such as 0.2s for 100-step DiffusionPolicy and 14.6s for 1000-step StableDiffusion-v2 models, thus addressing the potential of dynamic systems in real-time scenarios through efficient trajectory management.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Cagatay_Yildiz1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=AOKU4nRw1W",
  "title": "Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment",
  "modified_abstract": "In an era where text-conditioned image generation models are gaining prominence for tasks ranging from interactive image retrieval using natural language queries to generating complex scenes, a significant challenge remains in ensuring accurate associations between textual descriptions and visual outputs. Models often struggle with correctly associating entities and their visual attributes, as evidenced by the production of images with mismatched elements, such as a yellow sunflower and a pink flamingo when prompted for the opposite. Building upon the exploration of image retrieval through iterative language queries, which highlighted the importance of processing complex scenes and entities in natural language processing (NLP) and computer vision, we introduce SynGen. This novel approach first performs syntactic analysis of the prompt to identify entities and their modifiers and then employs a unique loss function designed to align cross-attention maps with the linguistic structure of the input text. By optimizing for greater overlap between encoding the attention maps of associated entities and their modifiers and minimizing overlap for unrelated elements, SynGen embeds a more accurate visual representation without the need for model retraining. Evaluated across three datasets, including a specially curated complex set serving as benchmarks, SynGen demonstrates marked improvements over current state-of-the-art methods, substantiating the efficacy of leveraging linguistic structures for improved fidelity in text-to-image generation tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Paola_Cascante-Bonilla1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=LlERoXEKjh",
  "title": "Training shallow ReLU networks on noisy data using hinge loss: when do we overfit and is it benign?",
  "modified_abstract": "Reflecting on recent insights into the adaptability of neural networks in filtering noise from data, our study investigates benign overfitting in two-layer ReLU networks trained using gradient descent and hinge loss on noisy data for binary classification. This examination is prompted by the understanding that neural networks, particularly through mechanisms like local signal adaptivity demonstrated in the image classification context, can outperform traditional kernel methods and neural tangent kernels by selectively focusing on relevant signals amidst noise. Specifically, we analyze the behavior of shallow ReLU networks on linearly separable data contaminated by a minor proportion of label corruption or flips. We delineate conditions on the margin of the clean data culminating in three potential training outcomes: benign overfitting that yields correct classification of test data despite zero loss, detrimental overfitting characterized by correct classification on training but misclassification on test data, and a non-overfitting scenario where the model successfully discriminates clean from corrupt data points leading to accurate test data classification. Our theoretical contribution includes a granular analysis of neuron dynamics during training, revealing an initial phase where clean data points achieve nearly zero loss followed by a second phase where these points fluctuate at the zero loss boundary as corrupt points converge to zero loss or are nullified by the model, utilizing adaptivity in simple yet effective ways. This dual-phase training behavior is explicated through a novel combinatorial methodology, contrasting the number of updates attributed to clean versus corrupt data, thereby extending the discourse on neural networks' inherent capability to discern and prioritize signal over noise, even in sparse datasets. Empirically, the effectiveness of this approach is showcased through selected tasks in image classification.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Stefani_Karp1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=iPTF2hON1C",
  "title": "Learning To Dive In Branch And Bound",
  "modified_abstract": "The development of primal heuristics is key in addressing mixed integer linear programs, primarily due to their utility in identifying feasible solutions that facilitate the branch and bound search process. Among these heuristics, diving heuristics stand out by employing an iterative approach that modifies and resolves linear programs to execute a depth-first search starting from any node within the search tree. Current diving heuristics use generic decision rules, which do not tap into the structural similarities observed across problem instances frequently encountered in practical scenarios. In light of the significant role that reproducible and efficient benchmarking and sharing practices play in the advancement of machine learning methods for learning, as exemplified by frameworks like Benchopt that strive for collaborative, transparent, and accessible optimization benchmarks across various domains, we introduce L2Dive. This novel approach harnesses graph neural networks to tailor diving heuristics to the specific characteristics of given problem instances, thereby enhancing the usability of these heuristics in practical applications. L2Dive utilizes generative models to forecast variable assignments and applies the principles of linear program duality to guide diving decisions based on these forecasts. Fully integrated with the open-source solver SCIP, L2Dive demonstrates superior performance over traditional divers by identifying more advantageous feasible solutions across diverse combinatorial optimization challenges. In practical applications, ranging from server load balancing to neural network verification, L2Dive has been shown to improve the primal-dual integral by as much as 7% (35%) on average versus a tuned (default) solver baseline, concurrently reducing the average solution time by 20% (29%) through learning-based techniques. Moreover, L2Dive's emphasis on collaborative learning and benchmarking encourages a more inclusive and shared approach in the machine learning and combinatorial optimization communities.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pierre-Antoine_Bannier1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=CzkOzKWpMa",
  "title": "Optimal cross-learning for contextual bandits with unknown context distributions",
  "modified_abstract": "This work extends the exploration of the cross-learning setting initially introduced by Balseiro et al., focusing on contextual bandit problems with adversarially chosen losses and contexts drawn i.i.d. from an unknown distribution. Our research is motivated by foundational advancements in online learning for autoregressive dynamics, where the importance of accounting for temporal dependencies in sequential decision-making has been effectively demonstrated. By resolving an open problem posed by Balseiro et al., we contribute an efficient algorithm that boasts a nearly tight (up to logarithmic factors) regret bound of $\\widetilde{O}(\\sqrt{TK})$, irrespective of the number of contexts. This achievement not only presents the first nearly tight regret bounds for learning to bid in first-price auctions under unknown value distribution and addressing sleeping bandits with stochastic action sets but also heralds a novel technique for coordinating the execution of a learning algorithm across multiple epochs. This innovation adeptly minimizes correlations between the estimation of the unknown distribution and the algorithm's action choices, potentially serving as a valuable strategy for other learning challenges involving unknown context distributions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Gianmarco_Genalti1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=cRzt1umRNx",
  "title": "Riemannian Residual Neural Networks",
  "modified_abstract": "Inspired by recent progress in geometric deep learning and the nuanced understanding of generalization errors in overparameterized regimes, such as those encountered in transfer learning between linear regression tasks, this work introduces a novel framework for extending the concept of residual neural networks (ResNets) to data residing on Riemannian manifolds. Leveraging insights from the exploration of overparameterization and transfer learning, we develop Riemannian ResNets to address the challenge of learning on manifold-valued data\u2014ranging from graphs with hierarchical structures to data from the natural sciences that naturally inhabit non-Euclidean spaces. Our approach generalizes the conventional Euclidean ResNets in a geometrically principled manner, enabling their application to a wider range of manifolds beyond the few for which extensions were previously possible. By integrating the ideas of gradient descent methods adapted for Riemannian manifolds and special attention to training dynamics, we demonstrate that, akin to their Euclidean counterparts, Riemannian ResNets overcome the vanishing gradient problem, contributing to improved learning properties and empirical outcomes. Our results also show that Riemannian ResNets outperform existing manifold neural networks designed for hyperbolic spaces and the manifold of symmetric positive definite matrices in relevant testing metrics and training dynamics, establishing new benchmarks for geometric deep learning applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yehuda_Dar1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=iImnbUVhok",
  "title": "Joint Prompt Optimization of Stacked LLMs using Variational Inference",
  "modified_abstract": "In the context of recent progress in language generation technologies, including efforts to detoxify and debias language models through inference-time adaptive optimization, our work introduces an innovative approach to harnessing the capabilities of Large Language Models (LLMs). Large language models can be conceptualized as fundamental computational units that map sequences of text to a distribution over potential continuations, suggesting their role as stochastic language layers within a broader language network. Central to our methodology is the concept of natural language prompts as learnable parameters within this network. By stacking two such LLMs and directing the output of one model into another via a strategic interface, we create what we term a Deep Language Network (DLN). Our initial focus unveils strategies for proficient prompt optimization within a single-layer language network (DLN-1). Building upon this foundation, we extend our investigation to a two-layer configuration (DLN-2), necessitating the learning of two distinct prompts via this method. This innovation treats the output from the first layer as a latent variable, thereby necessitating variational inference methods for prompt optimization. Preliminary experiments validate the efficacy of DLN-1 across a spectrum of reasoning and natural language understanding tasks in. Subsequent testing reveals that DLN-2 surpasses the performance achievable by its single-layer counterpart, hinting at the potential of approaching or even matching the performance levels of more sophisticated models such as GPT-4, despite the constituent LLMs being individually smaller and less computationally intensive.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xiaoyuan_Yi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=YFW6MVGVTn",
  "title": "NICE: NoIse-modulated Consistency rEgularization for Data-Efficient GANs",
  "modified_abstract": "Generative Adversarial Networks (GANs) are a cornerstone of modern machine learning for image synthesis, leveraging large datasets to generate photorealistic images. Prevailing research, such as techniques developed to mitigate class-specific mode collapse in long-tailed distributions through spectral regularization, highlights both the potential and challenges of GANs across varied data landscapes, including those with imbalanced or skewed distributions. Inspired by these insights, our paper introduces a novel technique, NoIse-modulated Consistency rEgularization (NICE), specifically designed to address the data efficiency problem in GAN training for deep generation tasks. By integrating adaptive multiplicative noise into the discriminator, NICE modulates latent features to prevent overfitting, a key challenge when data is scarce in large-scale learning environments. This approach is particularly effective in imbalanced datasets, where class representation varies significantly. Although this modulation inadvertently augments the gradient norm, possibly destabilizing training, we establish a counterbalance by enforcing a discriminator consistency constraint under various noise conditions. This innovative approach not only penalizes excessive gradients but also secures the training process, yielding a substantial reduction in generalization error across generation tasks, including conditional generation tasks. Our theoretical discourse is enriched with experimental validations, showcasing NICE's superior performance in scenarios with limited data availability, including CIFAR-10, CIFAR-100, ImageNet, and FFHQ datasets, as well as in low-shot generation tasks. The efficacy of NICE in ameliorating discriminator overfitting and fortifying GAN stability, set against the backdrop of prior work on spectral regularization for long-tailed distributions, signals a promising direction for future research in data-efficient image recognition and generation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Harsh_Rangwani1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=MWxsYPVmLS",
  "title": "Explainable and Efficient Randomized Voting Rules",
  "modified_abstract": "As AI tools increasingly contribute to decision-making in critical areas, the imperative for these tools to be both explainable and efficient has intensified. This need aligns with recent endeavors in the field, such as the pursuit of two-sided fairness in rankings, reflecting a broad interest in algorithms that are both fair and understandable to stakeholders. Our research contributes to this dialogue by addressing the balance between explainability and efficiency in the context of voting systems\u2014an area where explainability is paramount. We explore the potential of simple randomized voting rules to enhance decision-making efficiency without compromising their inherent explainability. Specifically, our investigation centers on randomized positional scoring rules and random committee member rules. Through both theoretical analysis and empirical studies, we demonstrate that these voting rule families successfully combine explainability with improved efficiency, as measured within the distortion framework, thus optimizing rankings in a fair manner. This equilibrium enables stakeholders to grasp how decisions are made, preserving the democratic advantage of voting systems over less transparent AI methodologies, while also leveraging the efficiency benefits introduced by controlled randomization. By examining the dominance of specific rules within these families, we further clarify how these methodologies outperform traditional systems in terms of fairness and rankings optimization, presenting a comprehensive view on optimizing electoral processes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sam_Corbett-Davies1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=GxL6PrmEUw",
  "title": "Distributional Learning of Variational AutoEncoder: Application to Synthetic Data Generation",
  "modified_abstract": "Recent advances within the field of variational autoencoders (VAEs), such as the Counterfactual VAE that addresses the estimation of treatment effects under unobserved confounding through causal representation learning, highlight the growing interest in expanding the versatility and applicability of VAE models. Inspired by these developments, our study introduces a novel approach aimed at overcoming the limitations associated with the Gaussianity assumption in VAEs, namely, the assumption's restrictive nature in terms of model expressiveness for continuous variables. By integrating an infinite mixture of asymmetric Laplace distribution into the decoder of our VAE model, we enhance the model's capacity for distribution fitting without compromising its computational efficiency. This paradigm allows for the representation of a broader range of distributions, positioning our model as a special form of a nonparametric M-estimator, tailored to estimating general quantile functions under mild assumptions. Theoretical discussions in our manuscript establish a connection between our proposed model, causal inference, and quantile estimation, affording new insights into the distributional learning capabilities of VAEs. Identifiability of causal effects and model parameters is crucial in settings where the generation of synthetic data takes into account treatment and outcome variables, showcasing our model's potential in generating privacy-sensitive data, outperforming existing models in its ability to adjust data privacy levels. This work not only extends the functionality of VAEs in synthetic data generation but also contributes to a refined understanding of the intersection between distributional learning, privacy considerations, and the autoencoder architecture, especially in various settings where identifiability ensures the reliability of synthetic data for downstream applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pengzhou_Abel_Wu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=5UXXhVI08r",
  "title": "Dynamic Prompt Learning: Addressing Cross-Attention Leakage for Text-Based Image Editing",
  "modified_abstract": "Inspired by foundational advancements in generative AI, particularly the exploration of Foundation Models (FMs) for expert tasks involving high fidelity data synthesis and advanced language-image tasks, this paper introduces Dynamic Prompt Learning (DPL), a novel approach to mitigate cross-attention leakage in text-to-image generative models. Large-scale text-to-image generative models, as exemplified by diffusion models, have marked a significant leap in AI's ability to generate convincing images from textual prompts. Despite impressive capabilities, these models often struggle with precise image editing tasks, particularly when edits are intended for specific regions of an image. This challenge is predominantly due to inaccurate cross-attention maps, which inadvertently affect regions outside the intended target area. Our approach, DPL, enhances focus on accurate noun words within prompts through dynamic tokens, leakage repairment losses, and innovative learning mechanisms, thus achieving more refined control over image edits while preserving the integrity of untargeted regions. Evaluated on a diverse set of images using the Stable Diffusion model, DPL demonstrates marked improvements in both quantitative (e.g., CLIP score, Structure-Dist) and qualitative assessments, significantly advancing the capability for fine-grained, text-driven image modification in complex multi-object scenes. The methodology and findings of our study, bolstered by prudent pre-training and meticulous annotations in the training datasets, contribute to the broader conversation on enhancing the precision and applicability of generative AI for text-based image editing tasks. The importance of detailed documentation is implicitly acknowledged through our careful curation and documentation of the datasets, aiding in huge improvements in learning dynamics.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Roei_Herzig2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=MCkUS1P3Sh",
  "title": "Nash Regret Guarantees for Linear Bandits",
  "modified_abstract": "Our work builds upon and extends the study of regret minimization in bandit problems, particularly addressing the limitations identified in multi-agent settings with information sharing among agents. Recognizing a fundamental drawback in extending Upper Confidence Bound (UCB) algorithms to multi-agent bandits where shared information about optimal choices degrades performance, we introduce a novel notion of regret---Nash Regret. This strengthened notion of regret, defined as the difference between the optimum and the geometric mean of expected rewards accumulated by the linear bandit algorithms, leverages the Nash social welfare (NSW) function to measure collective welfare across rounds. By focusing on the stochastic linear bandits problem across a horizon of $\\mathsf{T}$ rounds and with a set of arms ${\\cal X}$ in ambient dimension $d$, and considering rewards as non-negative, sub-Poisson random variables, our approach not only addresses the problem's shortcomings but also offers principled fairness guarantees rooted in NSW's adherence to fairness axioms. We achieve a Nash regret of $O\\left( \\sqrt{\\frac{d}{\\mathsf{T}}} \\log(\\mathsf{T} |{\\cal X}|)\\right)$ for finite arm sets and an upper bound of $O\\left( \\frac{d^\\frac{5}{4}}{\\sqrt{\\mathsf{T}}}  \\log(\\mathsf{T})\\right)$ for non-finite sets, applicable to bounded, non-negative rewards. Utilizing the successive elimination method enhanced with tailored concentration bounds and sampling via John ellipsoid in conjunction with the Kiefer\u2013Wolfowitz optimal design, our algorithm effectively balances the explore-exploit dilemma through strategic sampling and offers a refined solution accommodating the unique challenges presented in the linear bandit scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Udari_Madhushani1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=No52399wXA",
  "title": "IPMix: Label-Preserving Data Augmentation Method for Training Robust Classifiers",
  "modified_abstract": "In the quest for enhancing convolutional neural network classifiers' accuracy and resilience against distributional shifts, prior research such as 'Debiased Pseudo Labeling in Self-Training' underscores the intricate balance between leveraging large-scale labeled datasets and addressing the challenges of label scarcity, bias, and training stability in semi-supervised learning environments. Building on these insights, our work introduces IPMix, a novel data augmentation approach designed to not only preserve label integrity but also significantly enhance classifier robustness across varying data distributions without sacrificing performance on clean datasets. By intricately combining image-level, patch-level, and pixel-level augmentation into a unified, label-preserving framework, IPMix systematically increases training data diversity with minimal computational expense. Moreover, through incorporating structural complexity and adopting a random mixing methodology for multi-scale information synthesis, IPMix substantially elevates robustness against common corruptions, adversarial attacks, and other perturbations. Rigorous experimental evaluation across benchmarks such as CIFAR-C and ImageNet-C validates IPMix's superiority in fostering corruption robustness, while subsequent tests reveal its commendable performance in improving adversarial robustness, calibration, prediction consistency, and anomaly detection, demonstrating state-of-the-art or comparable outcomes on ImageNet-R, ImageNet-A, and ImageNet-O. This semi-supervised learning technique benefits from leveraging both labeled and unlabeled data, alleviating the generation problem of requiring extensive annotated datasets and addressing a significant problem in current classification tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ximei_Wang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=zn5ihqknGj",
  "title": "An Alternating Optimization Method for Bilevel Problems under the Polyak-\u0141ojasiewicz Condition",
  "modified_abstract": "Bilevel optimization, a critical tool in machine learning applications such as hyperparameter optimization, meta-learning, and reinforcement learning, has seen a resurgence of interest. This resurgence is partly inspired by recent advancements in online convex optimization, which highlight the complexities inherent in managing cumulative constraints\u2014a challenge bilevel optimization also grapples with, especially under nonconvex constraints. This paper builds on the framework established by preceding studies, including analysis of constraint handling in long-term optimization, to address a gap in bilevel optimization research. Specifically, we first introduce a stationary metric for bilevel optimization problems that generalizes the existing metric for a nonconvex lower-level objective that satisfies the Polyak-\u0141ojasiewicz (PL) condition. We then propose a Generalized ALternating mEthod for bilevel opTimization (GALET) tailored to Bilevel Optimization with convex PL Lower Level (BLO with convex PL LL) problems and establish that GALET achieves an $\\epsilon$-stationary point for the considered problem within $\\tilde{\\cal O}(\\epsilon^{-1})$ iterations. This result matches the iteration complexity of gradient descent (GD) for single-level smooth nonconvex problems, thereby extending the applicability of alternating gradient-based algorithms beyond strongly convex lower-level objectives. Moreover, our analysis incorporates cumulative squared error measures and addresses potential violations to enforce precision over iterations, which is crucial for convergence in bilevel optimization contexts.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jianjun_Yuan2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=8vuDHCxrmy",
  "title": "OpenMask3D: Open-Vocabulary 3D Instance Segmentation",
  "modified_abstract": "Inspired by recent advancements in few-shot semantic segmentation, particularly the development of adaptive prototypes for improved feature comparison and unbiased model performance across various scenarios, we introduce the task of open-vocabulary 3D instance segmentation. Our approach is motivated by the limitations of current methods in 3D instance segmentation, which typically recognize object categories from a pre-defined, closed set of classes annotated in training datasets. These methods fall short in real-world applications that require interaction with a wide array of objects identified through novel, open-vocabulary queries. Addressing this gap, we propose OpenMask3D, a zero-shot approach that leverages predicted class-agnostic 3D instance masks for segmentation. Our model, leveraging a neural network architecture, aggregates per-mask features through multi-view fusion of CLIP-based image embeddings, enabling effective segmentation of objects beyond the training set's scope, including in classification challenges. Experimental evaluations on ScanNet200 and Replica demonstrate that OpenMask3D surpasses existing open-vocabulary and 5-shot methodologies, particularly in handling long-tail distributions. Furthermore, through qualitative experiments, OpenMask3D showcases its ability to segment objects based on free-form queries that describe geometry, affordances, and materials, highlighting its potential to significantly advance 3D scene understanding. This model's innovation extends the conventional image processing techniques to a dynamic 3D space, allowing for a query-based, interactive model of object recognition.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Bin-Bin_Gao1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=p40XRfBX96",
  "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision",
  "modified_abstract": "Building on the foundation of leveraging pre-trained language models (PLMs) with novel tuning methods, including prompt-tuning and pretraining, as demonstrated in related works like prompt-tuning adaptations for natural language generation (NLG) tasks, this paper introduces a groundbreaking approach for the self-alignment of large language models (LLMs) from scratch with minimal human supervision. The existing AI assistant agents, such as ChatGPT, highlight the reliance on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of LLMs with human intentions, ensuring they are helpful, ethical, and reliable. However, challenges arise from the high dependence on human supervision, including issues related to cost, diversity, self-consistency, and biases in the representations.\nTo surpass these limitations, we propose SELF-ALIGN, a novel method that synergizes principle-driven reasoning with the generative capabilities of LLMs to achieve AI self-alignment with minimal human supervision. This methodology entails generating synthetic prompts enhanced by topic-guided methods for augmented diversity and addressing unfamiliar contexts, employing a concise set of human-written principles guided by in-context learning for ethically aligned response generation, and fine-tuning the original LLM with these high-quality outputs for direct desirable response generation. Further refinements address brevity and indirectness in responses.\nImplementing SELF-ALIGN on the LLaMA-65b model, we create the Dromedary AI assistant. This approach, requiring fewer than 300 lines of human annotations\u2014comprising less than 200 seed prompts, 16 principles, and 5 in-context learning exemplars\u2014demonstrably exceeds several state-of-the-art AI systems' performance on benchmark datasets under varied conditions. The success of SELF-ALIGN also sheds light on new pathways for generation without extensive reliance on familiar datasets, pointing towards a future of more adaptable and contextually aware LLM-driven applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shengnan_An1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=MWQjqtV1z4",
  "title": "Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption",
  "modified_abstract": "Motivated by the need for efficient computation of policies in problem settings where traditional assumptions such as the uniform global attractor property (UGAP) may be overly restrictive or hard to verify, our work builds on foundational models in decision-making under uncertainty. We specifically draw insights from the study of no-regret learning agents in games, and how these models facilitate steering towards optimal equilibria without relying on stringent conditions. Our research extends these principles to the domain of restless bandits, focusing on the infinite-horizon problem with the average reward criterion in both discrete-time and continuous-time settings. We introduce a novel, general, simulation-based framework named Follow-the-Virtual-Advice, which allows for the conversion of any single-armed policy into an effective strategy for the large $N$-armed problem by simulating the single-armed policy on each arm and guiding the real state towards this simulated state, akin to full-feedback, tree structure planning. Our approach achieves an $O(1/\\sqrt{N})$ optimality gap without the need for UGAP in discrete-time settings under a simpler synchronization assumption, and, more importantly, with no additional assumptions in continuous-time settings beyond the standard unichain condition. This advancement represents a significant leap forward, illustrating that it is possible to achieve asymptotic optimality in restless bandit problems by leveraging concepts from full-feedback games, extensive-form game theory for per-iteration improvement, and the study of bandit-feedback mechanisms, optimal equilibria, and steering mechanisms.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Brian_Hu_Zhang1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=o7W0Zet6p3",
  "title": "Recovering Unbalanced Communities in the Stochastic Block Model with Application to Clustering with a Faulty Oracle",
  "modified_abstract": "In light of the rich body of research emphasizing the criticality of accurately modeling and inferring social ties within dense networks\u2014given their implications across computational social science, viral marketing, and recommender systems\u2014this paper aims to expand our comprehension of the Stochastic Block Model (SBM) beyond the traditional scope of balanced communities. The SBM, a cornerstone for the analysis of graph clustering or community detection in networks, has predominantly been explored under the assumption of uniform community sizes. Our work deviates from this by focusing on SBM with inherently unbalanced communities, a scenario more reflective of real-world networks. We propose a novel SVD-based algorithm that not only simplifies the recovery of varying-sized communities within the SBM framework but also advances previous theories by Ailon, Chen, and Xu [ICML 2013; JMLR 2015], eliminating the prerequisites related to cluster size intervals and correlation to the number of clusters. Our findings not only offer theoretical enhancements but are also substantiated through experimental validations and statistical inference on networks. A significant implication of our algorithm, under the planted clique conjecture, is its near-optimal recovery capability for clusters, irrespective of the cluster sizes, so long as the probability parameters remain constant. Additionally, our research contributes an efficient clustering algorithm that operates under a faulty oracle model with sublinear query complexity, effectively identifying clusters beyond the $\\tilde{\\Omega}({\\sqrt{n}})$ threshold even amid a predominantly small-cluster environment. This capability distinctly surpasses previous formulations that falter with increased presence of smaller clusters, thereby marking a substantial advancement in clustering methodologies. Friends and connections within these networks further underline the social aspect of our inferring processes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nikolaj_Tatti1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=B7QRV4XXiK",
  "title": "An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient",
  "modified_abstract": "In the sphere of Reinforcement Learning (RL), managing risk through constraining the variance of policy returns is a widely accepted approach due to its straightforward definition and interpretability. Traditional variance-based risk management strategies typically focus on limiting overall return variance, while more recent methods advocate for the control of per-step reward variance as an intermediary solution. Despite their prevalence, these variance-centered approaches exhibit notable drawbacks, including a pronounced sensitivity to the numerical scaling of data and impediments to policy optimization. Motivated by these challenges and the evolving landscape of risk measurement in model-based RL, as evidenced by explorations into the diminishing returns of value expansion methods, this study introduces the Gini deviation as an alternative risk metric. The Gini deviation's distinct properties and its compatibility with the principles of risk-averse RL are thoroughly analyzed, culminating in the development of a new policy gradient algorithm designed to minimize this risk measure. Through empirical assessment in scenarios amenable to risk aversion, our findings demonstrate that the Gini deviation-based approach effectively overcomes the limitations inherent to variance-based models. The proposed algorithm not only fosters learning of policies with favorable risk-return profiles but also excels where conventional methods struggle, offering robust performance across both variance and Gini deviation metrics.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Daniel_Palenicek1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=zMeemcUeXL",
  "title": "FAMO: Fast Adaptive Multitask Optimization",
  "modified_abstract": "Inspired by significant strides in transfer learning and hyperparameter optimization, which highlight the complexity and potential of efficiently managing multiple tasks and configurations in machine learning (ML), this work introduces Fast Adaptive Multitask Optimization (FAMO). FAMO addresses the challenge of multitask learning (MTL), aiming to optimize performance across diverse tasks by improving their representations and leveraging meta-features that increase similarity among task-specific models. Traditional gradient descent (GD) methods applied to the average loss across tasks often result in under-optimization of some tasks due to their inherent imbalanced nature. Existing strategies for achieving a more equitable loss reduction by manipulating task gradients are computationally intensive, requiring $\\mathcal{O}(k)$ space and time for $k$ tasks. FAMO circumvents these limitations by proposing a dynamic weighting method that ensures balanced task loss reduction with significantly reduced computational and space complexity, using only $\\mathcal{O}(1)$ resources. Through rigorous experimentation against established baselines in both supervised and reinforcement learning scenarios, FAMO not only demonstrates its capability to match or exceed the performance of current gradient manipulation techniques but also underscores its superiority in efficiency by effectively acting as a surrogate model that minimizes the need for extensive hyperparameter tuning. This advancement presents a notable leap forward in scaling MTL, potentially catalyzing the development of generalist AI agents capable of learning a vast array of tasks more feasibly.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sebastian_Pineda_Arango1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=jX49iKr6vb",
  "title": "Beyond Deep Ensembles: A Large-Scale Evaluation of Bayesian Deep Learning under Distribution Shift",
  "modified_abstract": "This work is inspired by recent developments in robust learning, such as the exploration of probabilistic robustness in PAC learning, which highlights the potential and limitations of adapting machine learning models to handle uncertainty and perturbations effectively. Bayesian deep learning (BDL) represents a novel approach towards achieving resilient, well-calibrated predictions on data experiencing distribution shifts, an area where traditional deep learning methods, including classifier models, have struggled. Despite the growing interest in BDL, the landscape lacks a comprehensive, large-scale evaluation of state-of-the-art (SOTA) methods across diverse, realistic, and challenging benchmark tasks. Our research bridges this gap by systematically assessing modern BDL algorithms, foregrounding their probabilistic modeling capabilities on real-world datasets from the WILDS collection, which includes a variety of demanding classification and regression tasks designed to test generalization and calibration under distribution shifts. Our investigation spans a broad spectrum of large, convolutional, and transformer-based neural networks, classifying their adaptability and precision in terms of complexity and learning dynamics. Specifically, we explore a signed version of the expected calibration error to determine whether methods are over- or underconfident, thereby shedding light on their operational characteristics and robustness. An innovative aspect of our study is the systematic evaluation of BDL applied to fine-tuning large pre-trained classifier models, a context where initializing from scratch is not viable due to resource constraints and where numerous examples exhibit perturbations representative of real-world conditions. In contrast to previous findings, our research reveals that while ensembling single-mode posterior approximations consistently enhances model performance and calibration, it encounters limitations when fine-tuning sizable transformer-based language models. Here, variational inference approaches like last-layer Bayes By Backprop significantly outshine others in accuracy, whereas contemporary approximate inference techniques such as SWAG emerge as frontrunners in achieving optimal calibration.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~UNIQUE_SUBEDI1",
  "manipulated_ranking": 17,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=oqDSDKLd3S",
  "title": "Sample-Conditioned Hypothesis Stability Sharpens Information-Theoretic Generalization Bounds",
  "modified_abstract": "Building on the elucidation of convergence rates in the learning of complex models from noisy data, particularly in estimating linear operators in Hilbert spaces, our work introduces novel information-theoretic generalization guarantees. By developing the \"neighboring-hypothesis\" matrix and introducing a new family of stability notions, known as sample-conditioned hypothesis (SCH) stability, we address the critical need for sharper bounds that surpass those provided by existing information-theoretic frameworks. This is especially significant in the context of stochastic convex optimization (SCO) problems, where recent findings have identified gaps in current theoretical boundaries. Our methods, borrowing concepts from self-adjoint operators to ensure precision in our mathematical modeling, yield tightened bounds that hold promise for a range of learning scenarios, effectively advancing the capacity to mitigate overfitting through enhanced understanding of hypothesis stability and its implications for convergence and generalization in machine learning models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nicholas_H_Nelsen1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=pzc6LnUxYN",
  "title": "StateMask: Explaining Deep Reinforcement Learning through State Mask",
  "modified_abstract": "The pursuit of explainability in deep reinforcement learning (DRL) has been notably advanced by methodologies aimed at decoding the decision-making processes of DRL agents, as exemplified by explorations into factorized optical flows for mid-level representation in robotics. This lineage of research underscores the importance of transparency in complex models, particularly when applied to domains requiring high trust and safety. Building on this foundation, we introduce StateMask, a novel explanatory technique designed to identify and illustrate the states that are most critical to a DRL agent's achievement of its final reward, addressing the existing gap in explanation methods that focus predominantly on individual actions rather than the strategic steps leading to outcomes. StateMask employs a mask net to selectively impede the agent's ability to perform its usual actions, thus revealing the pivotal states by forcing the agent into random actions without diminishing overall performance. This approach not only enhances the explicability of DRL agents but also contributes to practical applications such as adversarial testing, policy refinement, and robotic control. Our evaluation of StateMask across diverse RL environments demonstrates its effectiveness in providing high-fidelity explanations and its utility beyond traditional explainers, offering a new lens through which the mechanics of DRL can be understood and optimized. Additionally, the factorization of perception and control processes inherent in the DRL agents' decision-making is made more transparent through the application of StateMask, effectively demonstrating how optical flow and its factorization contribute to understanding the agent's environment.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Li-Yuan_Tsao1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=x816mCbWpR",
  "title": "Recasting Continual Learning as Sequence Modeling",
  "modified_abstract": "Our work draws inspiration from a convergence of insights into the challenges of long-term credit assignment in reinforcement learning and the potential of advanced sequence modeling techniques. By examining the obstacles inherent in both reinforcement learning and recurrent neural networks, such as the need for discounting or gradient truncation that limits temporal reasoning, we propose a novel paradigm that formulates continual learning as a sequence modeling problem. This reimagining leverages the strength of sequence models, particularly Transformers and their efficient variants, to address the continuum of learning tasks. Within this framework, the continual learning process is interpreted as the forward pass of a sequence model, facilitated by adopting the meta-continual learning (MCL) framework for training at the meta-level across diverse learning episodes. Our approach enables sequence models to serve as a powerful mechanism for backpropagation networks in MCL, offering a unified solution that encompasses both classification and regression challenges across seven benchmarks. The results of our experiments underscore the potential of repositioned sequence models as effective vehicles for addressing the complex demands of continual learning, reducing distractions and attenuating the adverse effects of vanishing gradient issues by integrating latent distances between tasks in the network's architecture.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pierluca_D'Oro1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=AALLvnv95q",
  "title": "Training Energy-Based Normalizing Flow with Score-Matching Objectives",
  "modified_abstract": "This paper investigates the paradigm of generative models, particularly focusing on the synergy between flow-based and energy-based models inspired by recent advances in graph generation and tree decomposition techniques. Our study introduces the concept of energy-based normalizing flow (EBFlow), which delineates a novel approach in the parameterization of generative models by leveraging score-matching objectives to bypass the computationally intensive calculation of Jacobian determinants typical in linear transformations. Following this innovative route not only facilitates the integration of arbitrary linear layers without exacerbating the computational burden but also propels the efficiency of training processes significantly beyond the traditionally employed maximum likelihood estimation methods. By adopting advanced score-matching techniques incrementally, we further mitigate issues related to training stability and enhance the empirical performance of EBFlow. Our experimental validation underscores the efficacy of this approach, showcasing a remarkable acceleration in training speed and an improvement in model performance as measured by negative log-likelihood (NLL), outstripping existing methodologies. Permutations of clusters within graphs were used as a novel form of input, bolstering the connection between the data's structural aspects and the generative process. Additionally, we expand our review of pertinent datasets, integrating them into our experimental setup to contextualize the implications of our findings. This work takes inspiration from the foundational principles laid out by prior research on graph generation, particularly the application of tree decomposition strategies, to redefine the computational efficiency and performance metrics of flow-based modeling.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hamed_Shirzad1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=UFW67uduJd",
  "title": "MEMTO: Memory-guided Transformer for Multivariate Time Series Anomaly Detection",
  "modified_abstract": "In the context of recent advancements in deep learning for understanding complex data structures, such as those presented in 'Fiedler Regularization: Learning Neural Networks with Graph Sparsity', our research introduces a novel approach for detecting anomalies in real-world multivariate time series data, which encapsulates both complex temporal dependencies and inter-variable correlations. This complexity presents a significant challenge that reconstruction-based deep models, widely utilized in recent years, attempt to address. However, these models often suffer from over-generalization, leading to inconsistent performance. We propose the MEMTO, a memory-guided Transformer that leverages a reconstruction-based methodology enhanced by a novel memory module. This memory module is adept at learning the extent of updating required for each memory item based on the incoming data, thereby refining the model's sensitivity to anomalies. To facilitate stable training and effective regularization, we introduce a two-phase training strategy that includes the use of K-means clustering for initializing memory items and graphical models for understanding the underlying network structure. Furthermore, the inclusion of graphical models and a bi-dimensional deviation-based detection criterion for calculating anomaly scores from both input and latent spaces stands as a distinctive feature of our approach, emphasizing the learning dynamics within these network architectures. Evaluated across five different real-world datasets representing various domains, the MEMTO model demonstrates superior performance, achieving an average anomaly detection F1-score of 95.74%, notably exceeding that of previous state-of-the-art methods. Through rigorous experimental validation, we affirm the contribution of our model's key components to its overall effectiveness in anomaly detection.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Edric_Tam1",
  "manipulated_ranking": 12,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=9dp35y5C0p",
  "title": "Reinforcement-Enhanced Autoregressive Feature Transformation: Gradient-steered Search in Continuous Space for Postfix Expressions",
  "modified_abstract": "Drawing inspiration from the burgeoning exploration of data augmentation in self-supervised representation learning and its connection to RKHS approximation, we address the problem of feature transformation through a novel perspective via augmentation-based techniques. Feature transformation aims to generate new pattern-discriminative feature spaces from original features to improve downstream machine learning (ML) task performances. However, the discrete search space for the optimal feature configuration expands dramatically with the combinations of features and operations, making traditional methods such as exhaustive search, evolutionary algorithms, and reinforcement learning inefficient due to the vast search space. We propose a novel method that reformulates discrete feature transformation into a continuous optimization task, introducing an embedding-optimization-reconstruction framework with pretraining stages. This framework includes four steps: 1) reinforcement-enhanced data preparation for high-quality transformation-accuracy training data; 2) feature transformation operation sequence embedding to encapsulate training data knowledge within a continuous kernel space; 3) gradient-steered (via masked predictions or selections) optimal embedding search for superior embeddings; and 4) transformation operation sequence reconstruction to identify the optimal feature transformation solution. The use of language data augmentation and an encoder model within the augmentation and self-supervised learning context represents a significant shift from prevailing methods by integrating reinforcement learning with gradient-steering techniques for efficient and robust feature space exploration in ML. This approach offers a significant shift from prevailing methods by integrating reinforcement learning with gradient-steering techniques and encoder models for efficient and robust feature space exploration in ML.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Runtian_Zhai1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=6H8Md75kAw",
  "title": "Certified Minimax Unlearning with Generalization Rates and Deletion Capacity",
  "modified_abstract": "Inspired by the pivotal challenges and discoveries in differentially private optimization, such as the Beyond Uniform Lipschitz Condition study that expanded the understanding of differentially private stochastic gradient descent under complex Lipschitz conditions, our research addresses the intricacies of $(\\epsilon,\\delta)$-certified machine unlearning for minimax models, pushing past the conventional boundaries that have defined machine unlearning strategies to date. Unlike most existing works that are confined to unlearning from standard statistical learning models based on simplistic and direct Hessian-based conventional Newton updates, we propose a novel $(\\epsilon,\\delta)$-certified machine unlearning algorithm for minimax models. Our method entails a comprehensive minimax unlearning step that amalgamates a total Hessian-based complete Newton update with the Gaussian mechanism, a cornerstone concept of differential privacy, to certify the unlearning process through calibrated Gaussian noise injections. This approach evaluates the 'sensitivity' of the minimax unlearning step\u2014essentially, quantifying the degree of closeness between the minimax unlearning variables and those obtained from retraining from scratch, which includes privacy-preserving training cycles. By doing so, our algorithm ensures both the privacy and the integrity of the training data through gradients alteration, enabling recommendations for further study or application enhancements. This paper also extends the theoretical framework for unlearning by deriving generalization rates for population strong and weak primal-dual risk across three classes of loss functions, namely, (strongly-)convex-(strongly-)concave losses, and establishing a new metric for deletion capacity which ensures maintenance of desired population risk levels, even with sample deletions, up to a calculated threshold. Notably, our framework exhibits a significant improvement over prior differentially private minimax learning methods, quartering the order to $\\mathcal O(n/d^{1/4})$, which starkly contrasts with the traditional rate of $\\mathcal O(n/d^{1/2})$. Furthermore, we demonstrate that our results on generalization rates and deletion capacity align with the latest findings in the realm of standard statistical learning models, effectively bridging a gap in the unlearning research landscape.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Rudrajit_Das1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=K9xHDD6mic",
  "title": "Graph Mixture of Experts: Learning on Large-Scale Graphs with Explicit Diversity Modeling",
  "modified_abstract": "Graph neural networks (GNNs) have found extensive applications in learning from graph data, driven by the growing need for models that can effectively handle the inherent diversity in real-world graph structures including nodes and edges of varying types. This study introduces a novel Graph Mixture of Experts (GMoE) model to tackle the challenges of learning with such diverse graph structures while aiming to balance the necessity for model generalization against the computational costs and trainability issues prevalent in traditional GNNs. Drawing inspiration from recent advancements in graph learning, particularly the employment of Variational Graph Information Bottleneck methods and optimization techniques for improving subgraph recognition, our proposed GMoE model integrates the concept of Mixture-of-Experts within the GNN framework. This integration enables the dynamic and adaptive selection of information aggregation experts by individual nodes, allowing for tailored processing of distinct subgroup structures within subgraphs and incorporation of diverse information scales through explicit recognition. Such methodology not only facilitates handling complex and heterogeneous graph data but also addresses noise and the explosive computational demands posed by traditional approaches through effective optimization and inclusion of valuable information. The inclusion of perturbation techniques enhances the robustness and adaptability of the model, further solidifying its capacity for dealing with variational elements inherent in graph data. The GMoE's efficiency and effectiveness are demonstrated through substantial improvements in various prediction tasks on the OGB benchmark, notably enhancing ROC-AUC scores in ogbg-molhiv and ogbg-molbbbp datasets compared to non-MoE baselines. Our study underscores the importance of explicitly modeling diversity within graph data to improve the performance of GNNs across a range of applications. The implementation of the model is made available to the public to facilitate further research and application in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Junchi_Yu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=blm1pqiOXe",
  "title": "Paxion: Patching Action Knowledge in Video-Language Foundation Models",
  "modified_abstract": "Building on the critical insights provided by recent advancements in video concealed object detection (VCOD), which emphasize the importance of understanding dynamic contexts and temporal consistency in video frames, we introduce the Action Dynamics Benchmark (ActionBench) and a novel framework, Paxion. ActionBench consists of two probing tasks designed to evaluate multimodal alignment capabilities and temporal understanding in video-language models (VidLMs). Despite VidLMs' impressive performance across various benchmarks tasks, our diagnostic tasks expose their significant deficiencies in action knowledge, previously masked by their reliance on object recognition. To address these shortcomings, Paxion employs a Knowledge Patcher network for encoding new action knowledge and a Knowledge Fuser for integrating this knowledge into existing VidLMs without impairing their performance. We also propose a new objective, Discriminative Video Dynamics Modeling (DVDM), to overcome the inadequacies of the Video-Text Contrastive (VTC) loss in learning action knowledge. DVDM uniquely contributes to the model's ability to understand action dynamics by encoding the relationship between action descriptions and the proper sequencing of video frames. Our analysis demonstrates that, by leveraging a concealed dataset and pixel-level techniques alongside the DVDM objective, Paxion significantly improves action knowledge understanding (from an approximate 50% to 80%) and enhances performance on both object- and action-centric downstream tasks, evidencing the model\u2019s capacity to bridge the gap in action knowledge.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xuelian_Cheng2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=CxUuCydMDU",
  "title": "Diffusion Probabilistic Models for Structured Node Classification",
  "modified_abstract": "Structured node classification on graphs represents a significant challenge within the domain of graph-based machine learning, reflecting an ongoing need to accurately predict node labels in partially labeled graphs by effectively harnessing dependencies among nodes. Inspired by advancements in graph neural networks (GNNs) exemplified by their application in evolving graphs and the exploration of differential geometric views for explainability, including formally analyzing curves in high-dimensional spaces, our research introduces a novel framework, the Diffusion Probabilistic Model for Structured Node Classification (DPM-SNC). This framework capitalizes on the diffusion probabilistic model's ability to (a) learn a joint distribution over node labels using an expressive reverse diffusion process, and (b) facilitate predictions conditioned on known labels through manifold-constrained sampling, effectively embedding the learned knowledge about the node classification tasks. Addressing the gap in training methodologies for DPMs in the context of partially labeled data, we develop a novel training algorithm that optimizes a new variational lower bound tailored for this purpose. Furthermore, we theoretically analyze the enhancement of GNNs' expressive power through our proposed AGG-WL, demonstrating its superiority over the traditional 1-WL test with an axiomatic approach. Our extensive evaluations across various graph settings\u2014including transductive and inductive scenarios as well as on unlabeled and social graphs\u2014underscore the effectiveness and versatility of DPM-SNC, firmly establishing its superiority in structured node classification tasks and its adaptability to the evolution of graph structures.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sihong_Xie1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=75v88kyyko",
  "title": "Hierarchical clustering with dot products recovers hidden tree structure",
  "modified_abstract": "Inspired by recent advancements in online learning models that address sudden distribution shifts, our study introduces a novel perspective on the agglomerative clustering algorithm by focusing on the recovery of hierarchical structure. By integrating insights from variational beam search for novelty detection, which highlights the importance of adapting to distribution changes in a model-agnostic manner, we propose a modified agglomerative clustering algorithm. In our variant, clusters are merged based on the maximum average dot product rather than traditional criteria such as minimum distance or within-cluster variance. This method demonstrates that the tree structure output by the algorithm serves as a reliable estimate of the generative hierarchical structure in data, under a generic probabilistic graphical model. Our key technical contributions involve elucidating how hierarchical information embedded in this model is manifested in tree geometry, which can be discerned from empirical data, and detailing the advantages of increasing both sample size and data dimension. Experimental validation with real datasets illustrates our method's enhanced capability for tree recovery when compared to established techniques such as UPGMA, Ward's method, and HDBSCAN, and it showcases the efficiency in handling sequential data. The proposed clustering approach makes significant strides in the learning process by accommodating variable data scales and adapting to online updates or changes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Aodong_Li1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=7UdVPRmpif",
  "title": "On student-teacher deviations in distillation: does it pay to disobey?",
  "modified_abstract": "In the context of recent advancements in machine learning, specifically in self-supervised learning and its challenges with out-of-distribution (OOD) samples and outlier-robust mechanisms, our work investigates the phenomena of student-teacher deviations in knowledge distillation (KD). Recognizing the critical insights from exploring the pitfalls of self-supervised learning frameworks, which highlights the issues of augmentation-induced OOD samples, we extend this understanding to the domain of KD. Knowledge distillation has been utilized extensively to enhance the test accuracy of a 'student' network by training it to mimic the soft probabilities of a trained 'teacher' network. It has been observed that the student, while trained to align with the teacher's output, can exhibit significant deviations from the teacher's probabilities and, remarkably, can outperform the teacher. Our research aims to dissect this paradox, characterizing the nature of student-teacher deviations and demonstrating how such deviations can coexist with improved generalization through substantial empirical analysis on image and language datasets, which involved instance-specific sampling strategies and augmentation. Further, we theoretically and empirically unveil another dimension of exaggeration\u2014the implicit bias of gradient descent in KD, which preferentially converges faster along the top eigendirections of the data, and the role of the loss's characteristics in this process. Binding these observations, we portray how the exaggerated bias inherent in KD not only enhances student confidence but also fosters better generalization, thus addressing the paradox. Our findings contribute to narrowing the theoretical and practical gaps in KD, highlighting the nuanced role of gradient descent and the beneficial impacts of bias exaggeration and outlier-robust self-supervised learning mechanisms.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jingjing_Zou1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=FdtdjQpAwJ",
  "title": "Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning",
  "modified_abstract": "Drawing insights from advancements in open-ended learning environments and curriculum design in multi-agent reinforcement learning systems, our study advances the field of safe reinforcement learning (RL) by addressing the challenge of developing versatile safe policies. These policies are capable of adapting to various safety constraint requirements during deployment without the need for retraining. Our work, while distinct, is inspired by methodologies that facilitate automated, dynamic adjustment to learning environments and agent behaviors, as seen in the MAESTRO framework's approach to multi-agent RL. In this context, we introduce the Conditioned Constrained Policy Optimization (CCPO) framework, which is designed to achieve training efficiency and zero-shot adaptation capability for safe RL. CCPO incorporates Versatile Value Estimation (VVE) for estimating value functions under unknown threshold conditions and Conditioned Variational Inference (CVI) for integrating arbitrary constraint thresholds during policy optimization. Through rigorous experimentation, including adversarial scenarios reminiscent of two-player games, we demonstrate that CCPO significantly outperforms existing baselines in terms of safety compliance and task performance while maintaining the ability to adapt instantaneously to varied constraint thresholds in a data-efficient manner. This capability is particularly crucial for deploying RL agents in dynamic real-world scenarios where safety constraints may shift without prior notice. Curriculum-based learning strategies, designed to increasingly and automatically intensify the learning challenges, further enhance CCPO's ability to tackle an array of challenges, preparing it for real-world application by considering potential co-players in adversarial settings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mikayel_Samvelyan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=LZ4WgwmrUJ",
  "title": "High-dimensional Contextual Bandit Problem without Sparsity",
  "modified_abstract": "Our study extends the evolving landscape of high-dimensional statistical modeling by examining the high-dimensional linear contextual bandit problem without imposing the common sparsity assumption on regression coefficients. This approach builds on the momentum of recent advances in overparameterized models and insights derived from Bayesian methods, such as those applied in block-diagonal graphical models, integrating the use of a prior in the analysis. We leverage these advancements, along with spectral analysis for understanding data distributions of small effective ranks, to explore the minimum-norm interpolating estimator's efficacy. An explore-then-commit (EtC) algorithm is proposed as a strategy to tackle the high-dimensional bandit problem, supplemented by an analysis that establishes the optimal performance rate of the ETC algorithm in relation to the budget $T$. Furthermore, we innovate with an adaptive explore-then-commit (AEtC) algorithm, designed to dynamically strike an optimal balance between exploration and exploitation based on the contextual information at hand. Clustering techniques inform the development of AEtC, enhancing its adaptive nature. The effectiveness of our proposed algorithms is validated through comprehensive simulations, showcasing their potential in navigating the complexity of high-dimensional spaces without relying on sparsity.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Julyan_Arbel1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=r9fzp8eyhZ",
  "title": "Learning Invariant Molecular Representation in Latent Discrete Space",
  "modified_abstract": "Inspired by the recent advances in utilizing graphical structures for promoting sparsity and robustness in neural networks, we extend these foundational concepts to the domain of molecular representation learning, crucial for innovations in drug discovery. Existing methods for molecular representation often struggle with out-of-distribution (OOD) generalization, especially when learning and testing datasets come from different environments. To tackle these problems, we introduce a new computational framework that enhances the learning of molecular representations, ensuring their invariance and robustness against distributional shifts through advanced regularization techniques. Our approach adopts a unique ``first-encoding-then-separation'' strategy to pinpoint invariant features within the latent space, diverging from traditional methodologies. We fortify this strategy with a residual vector quantization module tailored to curb the tendency of overfitting to specific training distributions while maintaining the encoding's expressivity. Additionally, our framework incorporates a task-agnostic self-supervised learning objective that fosters accurate identification of invariant characteristics through iterative learning methods. This attribute makes our method highly adaptable to diverse tasks including regression and multi-label classification. Rigorous testing across 18 real-world molecular datasets has shown our model's superior generalization capabilities when confronted with various distribution shifts, outperforming existing state-of-the-art baselines in the network. To foster further research and application in this field, we have made our implementation publicly available.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Edric_Tam1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=SoLebIqHgZ",
  "title": "ARTree: A Deep Autoregressive Model for Phylogenetic Inference",
  "modified_abstract": "Our work is directly inspired by foundational advancements in deep learning and their applications to complex inference problems, such as maximum a posteriori (MAP) inference in deep networks using techniques like multilayer vector approximate message passing (ML-VAMP). These advancements highlight the potential for leveraging deep generative models in solving intricate inference problems with high precision. Building on this premise, designing flexible probabilistic models over tree topologies is crucial for developing efficient phylogenetic inference methods. Previous efforts often rely on the similarity of tree topologies via hand-engineered heuristic features, which necessitate domain expertise and could be constrained by limited approximation capacities and error margins. We introduce a deep autoregressive model for phylogenetic inference, termed ARTree, harnessing graph neural networks (GNNs) to decompose a tree topology into a sequence of leaf node addition operations. This method models the conditional distributions involved based on learnable topological features derived from GNNs, thereby offering a rich family of distributions over tree topologies that facilitate simple sampling algorithms without the need for heuristic features. The effectiveness and efficiency of ARTree are validated on a benchmark comprising challenging real data tree topology density estimation and variational Bayesian phylogenetic inference problems, showcasing its capability to address network complexity and reduce error in estimations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mojtaba_Sahraee-Ardakan1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=fbpTObq6TW",
  "title": "A fast heuristic to optimize time-space tradeoff for large models",
  "modified_abstract": "With the burgeoning interest in understanding the intricacies of neural network optimization, as evidenced by recent research on the convergence properties of stochastic gradient descent and its variants, this study introduces a crucial advancement in the handling of computational resource constraints - particularly GPU memory limitations encountered during the training of large-scale neural networks. Previous efforts like Checkmate and Moccasin have explored gradient checkpointing or recomputation techniques but are hampered by scalability issues arising from their reliance on mixed integer linear programming or constraint programming due to their exponentially large search spaces. Against this backdrop, our paper presents a novel, fast recomputation algorithm (FastSA) based on simulated annealing, offering a potent alternative capable of achieving comparable or superior optimization outcomes in significantly reduced timescales. FastSA efficiently optimizes computational graphs with thousands of nodes within seconds, showcasing an impressive speed advantage over existing methodologies. We applied FastSA to PyTorch models, with empirical assessments demonstrating its capacity to achieve substantial memory savings of up to 73% alongside a manageable increase in computational overhead by 18% on average across a variety of large vision and text tasks, including those utilizing the transformer architecture. These findings not only underscore FastSA's practicality and operational efficiency but also illustrate its contribution to the deepening of our understanding of gradient norm decay in the realm of deep learning, offering a provably viable solution to the pervasive challenge of GPU memory constraints.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sindri_Magn\u00fasson1",
  "manipulated_ranking": 11,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=vIGNYQ4Alv",
  "title": "Accelerated Quasi-Newton Proximal Extragradient: Faster Rate for Smooth Convex Optimization",
  "modified_abstract": "Building on the exploration of convergence-rate analyses in the realm of machine learning algorithms, notably on the differentiation through optimization and unrolling practices, this paper introduces an accelerated quasi-Newton proximal extragradient method optimized for unconstrained smooth convex optimization problems. Our method functions with gradient information exclusively and establishes a convergence rate of $\\mathcal{O}\\bigl(\\min\\\\{\\frac{1}{k^2}, \\frac{\\sqrt{d\\log k}}{k^{2.5}}\\\\}\\bigr)$, where $d$ represents the problem dimension and $k$ indicates the iteration count. Notably, within the domain where $k = \\mathcal{O}(d)$, our approach aligns with Nesterov's accelerated gradient (NAG) optimal rate of $\\mathcal{O}(\\frac{1}{k^2})$. Furthermore, surpassing NAG in the regime where $k = \\Omega(d \\log d)$, it advances at a superior rate of $\\mathcal{O}\\bigl(\\frac{\\sqrt{d\\log k}}{k^{2.5}}\\bigr)$. This achievement marks a pioneering instance of a quasi-Newton-type method outperforming NAG in a convex scenario, to our knowledge. The differentiation utilized within our method is a key aspect that connects to machine learning by enabling an efficient approximation of Hessian matrices for descent. This foundation lies in an adapted version of the Monteiro-Svaiter acceleration framework, coupled with an online learning strategy for updating these matrices. Through this approach, we correlate our method's convergence rate with the dynamic regret encountered in a particular online convex optimization challenge within the matrix space. Moreover, its efficacy in few-shot scenarios, where limited data is available for learning, reinforces the versatility of our technique.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Fabian_Pedregosa1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=M6OmjAZ4CX",
  "title": "Language Models can Solve Computer Tasks",
  "modified_abstract": "Recent advancements have illustrated the potential of large language models (LLMs) in processing natural language and performing tasks that require human-like understanding, as demonstrated by research in few-shot learning and chain of thought (CoT) prompting. Specifically, these studies have revealed insights into how LLMs leverage textual patterns, symbols, and intermediary reasoning steps to solve complex problems, laying the groundwork for novel approaches to task automation via natural language processing. This paper builds upon the notion that agents, particularly pre-trained LLMs, can significantly enhance task automation and problem-solving capabilities on computer-based tasks through intelligent prompting mechanisms, including processing sentences in a way that mimics human thought processes. We present a method where the agent \\textbf{R}ecursively \\textbf{C}riticizes and \\textbf{I}mproves its output (RCI), which remarkably outperforms existing LLM methodologies for automating computer tasks and excels over supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. We assess the efficacy of RCI in conjunction with the InstructGPT-3+RLHF LLM, demonstrating superior performance with minimal demonstrations required and without task-specific reward functions. Moreover, our exploration into the synergies between RCI and chain of thought (CoT) prompting unveils enhanced reasoning capabilities of LLMs in natural language reasoning tasks, offering empirical evidence that a combined approach of RCI and CoT outshines either method alone. Our findings not only validate the effectiveness of RCI prompting but also provide a scalable model for improving LLMs' application in real-world computer tasks, removing the need for extensive expert demonstrations and complex reward systems. The code for our experiments has been made available for further research and development in this field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Aman_Madaan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=CuHymkHRus",
  "title": "Convolution Monge Mapping Normalization for learning on sleep data",
  "modified_abstract": "In response to the critical challenges presented by the variability of biomedical data, notably in electroencephalogram (EEG) recordings across subjects, sessions, and hardware devices, this work presents the Convolutional Monge Mapping Normalization ($\\texttt{CMMN}$). This method is inspired by pioneering work in the fields of neural dynamics and behavioral prediction, including the trajectories analysis in neural dynamics, and the iLQR-VAE's approach to understanding and modeling neural data with its powerful reconstructions of dynamic states. Indeed, the incorporation of autoencoder structures like iLQR-VAE within $\\texttt{CMMN}$ exemplifies the melding of dynamic systems theory with practical machine learning frameworks to address challenges autonomous to the variability in biomedical signal analysis. $\\texttt{CMMN}$ introduces a novel normalization technique that filters signals to adapt their power spectrum density (PSD) to a Wasserstein barycenter estimated on training data. It leverages closed-form solutions for optimal transport mappings and barycenters, enabling individual test time adaptation to new data without the need for retraining the prediction model. Our numerical experiments on sleep EEG data demonstrate that $\\texttt{CMMN}$ significantly enhances performance, independent of the neural network architecture used, when adapting between subjects, sessions, and datasets collected with different hardware. Remarkably, the performance gains achieved by $\\texttt{CMMN}$ are comparable to those of much more computationally intensive Domain Adaptation (DA) methods, and it can be utilized in tandem with these methods for even greater improvements.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ta-Chu_Kao1",
  "manipulated_ranking": 8,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=3kitbpEZZO",
  "title": "Beyond probability partitions: Calibrating neural networks with semantic aware grouping",
  "modified_abstract": "The quest for improving model generalization and calibration has led to the development of innovative techniques such as sharpness-aware learning rates (SALR), which dynamically adjust learning rates based on the loss function's local sharpness to recover flat minimizers, effectively escaping poor local minima. In light of these contributions, our work introduces a novel perspective on model calibration by proposing the concept of Partitioned Calibration Error (PCE). This concept underscores the insight that calibration error metrics differ primarily in the methods used to partition the data space. We argue that an accurate model must exhibit calibration across any data partition, extending the traditional focus on prediction probabilities to include semantics-based partitions directly tied to input features. Through the introduction of semantic-related partitioning functions, our study reveals that the granularity of these functions is crucial for enhancing both model accuracy and calibration. We innovate by jointly learning a semantic aware grouping function using deep model features and logits, enabling data space partitioning into semantically coherent subsets for which tailored calibration functions are subsequently learned. Additionally, gradient-based optimizers play a pivotal role in this process, facilitating updates that refine these partitioning and calibration mechanisms through iterative experiments. Our experimental results underscore the significance of partitioning criteria, showing substantial improvements in model performance across various datasets and network architectures, thus paving the way for more effective approaches in neural network calibration.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xubo_Yue1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=yhNHpLWJDl",
  "title": "Finite-Time Analysis of Whittle Index based Q-Learning for Restless Multi-Armed Bandits with Neural Network Function Approximation",
  "modified_abstract": "The restless multi-armed bandits (RMAB) framework has seen significant attention in the optimization and decision-making literature, exemplified by prior work on best arm identification in environments where each arm is governed by a Markov process. Our research builds upon these foundational insights, particularly the challenges and methodologies associated with handling restless Markov arms, to develop Neural-Q-Whittle. Whittle index policy is a heuristic to the intractable restless multi-armed bandits (RMAB) problem. Although it is provably asymptotically optimal, finding Whittle indices remains difficult. In this paper, we present Neural-Q-Whittle, a Whittle index based Q-learning algorithm for RMAB with neural network function approximation, which is an example of nonlinear two-timescale stochastic approximation with Q-function values updated on a faster timescale and Whittle indices on a slower timescale. Despite the empirical success of deep Q-learning, the non-asymptotic convergence rate of Neural-Q-Whittle, which couples neural networks with two-timescale Q-learning largely remains unclear. This paper provides a finite-time analysis of Neural-Q-Whittle, where data are generated from a Markov chain, and Q-function is approximated by a ReLU neural network. Our analysis leverages a Lyapunov drift approach to capture the evolution of two coupled parameters, and the nonlinearity in value function approximation further requires us to characterize the approximation error. Combining these provide Neural-Q-Whittle with $\textbackslash mathcal(O)(1/k^{2/3})$ convergence rate, where $k$ is the number of iterations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~P._N._Karthik1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=1EYKYJeZtR",
  "title": "Large language models transition from integrating across position-yoked, exponential windows to structure-yoked, power-law windows",
  "modified_abstract": "In the context of modern language models (LMs), this study is informed by pioneering work in the realms of unsupervised feature learning using time-contrastive learning (TCL) and nonlinear independent component analysis (ICA), forging a connection between these models and the human brain's neural mechanisms for processing language. Our research identifies and characterizes integration windows within large language models (LLMs), a concept inspired by prior evidence suggesting that human brain responses to language exhibit hierarchically organized integration windows that govern the influence of input tokens (words) on neural responses. We developed a novel word-swap procedure to estimate these integration windows in black-box LLMs without relying on gradients or architecture specifics (e.g., attention weights). Our findings reveal that LLMs demonstrate stereotyped integration windows best described by a convex combination of exponential and power-law functions, with evident shifts from exponential to power-law dynamics across the layers. Additionally, we introduce a metric to quantify integration windows' alignment with structural boundaries (like sentence endings), and our analysis indicates a progression towards structure-yoked integration at deeper layers. These patterns were absent in untrained models, which showed uniform integration across inputs. Our work suggests a learned mechanism in LLMs for information integration, transitioning from position-focused, exponential frames at initial layers to structure-bound, power-law frames in deeper layers. The methodologies we outline offer a toolkit for deep learning researchers interested in exploring temporal integration in LMs and promote interdisciplinary bridges between biological understanding and artificial intelligence research, particularly by emphasizing the importance of time series analysis and identifiability in unsupervised feature learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hiroshi_Morioka1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=nSr2epejn2",
  "title": "Robust Matrix Sensing in the Semi-Random Model",
  "modified_abstract": "This paper extends the discourse on low-rank matrix recovery, a cornerstone issue with broad implications across machine learning applications, previously highlighted by its significance in sparse regression and the challenges presented by high-dimensional data. Taking cues from paradigmatic analyses, such as the exploration of effect size heterogeneity in high-dimensional sparse regression, we tackle the problem of matrix sensing within a semi-random model\u2014where adversarial interference complicates recovery processes. Here, robustness against adversarially chosen sensing matrices and the pursuit of global optima in non-convex landscapes become critical. We propose a descent-style algorithm that provably recovers a low-rank matrix $X^\\star$ from linear measurements, even when an unknown subset of these measurements emerges from adversarial actions. The method diverges from prior models that solely rely on convex optimizations or necessitate conditions like the Restricted Isometry Property (RIP), which are impractical due to their computational intractability. By leveraging a framework that continuously reweights input in accordance with emerging solutions\u2014a strategy inspired by advances in semi-random sparse linear regression\u2014we offer an innovative pathway that melds the recovery of low-rank matrices with the principles underpinning the robustness of algorithms in high-dimensional spaces. Our approach, building on the interplay between sparsity and low-rankness, marks a significant step toward addressing the complexities of robust matrix sensing and opens avenues for future exploration in the domain of semi-random model challenges.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yachong_Yang1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=67MTWzhEOn",
  "title": "Revisit the Power of Vanilla Knowledge Distillation: from Small Scale to Large Scale",
  "modified_abstract": "In light of the recent advancements in self-supervised learning that focus on maximizing the utility of large datasets through scalable dynamic routing, our study reevaluates the efficacy of the vanilla knowledge distillation (KD) framework, particularly in the context of large-scale data environments such as ImageNet-1K. We discuss the previously overlooked potential of vanilla KD when applied to extensive datasets and coupled with advanced data augmentation techniques, challenging the prevailing notion that its effectiveness is confined to small-scale datasets. Our findings suggest that the conventional approach to knowledge distillation\u2014focusing on the development of complex, variant methodologies\u2014may undervalue the simplicity and scalability of vanilla KD. Through rigorous training and testing on advanced neural network architectures, including ResNet-50, ViT-S, and ConvNeXtV2-T on the ImageNet dataset, we demonstrate that vanilla KD, devoid of any complex modifications, yields state-of-the-art results, achieving top-1 accuracies of 83.1%, 84.3%, and 85.0%, respectively. This underscores the importance of contextualizing KD approaches within the larger-scale applications they may serve, thus advocating for a reevaluation of KD strategies beyond the constraints of small-scale data assessments, especially considering their potential to enhance learning and transfer across downstream tasks, including object detection. Our analysis reinforces the need to reconsider existing methodologies in light of practical, scalable scenarios, highlighting vanilla KD\u2019s untapped potential in large-scale learning environments. The references to code and checkpoints have been omitted to maintain focus on the study's findings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jianhua_Han1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=wYkfog48Bq",
  "title": "Optimal Block-wise Asymmetric Graph Construction for Graph-based Semi-supervised Learning",
  "modified_abstract": "The exploration of fairness-aware strategies in node representation learning, especially through the lens of adaptive data augmentation for biased node features and graph structures, has significantly contributed to advancing the field of graph-based machine learning. Motivated by these insights and challenges identified in prior works, this study aims to address a critical and underexplored aspect of graph-based semi-supervised learning (GSSL): the graph construction phase. GSSL effectively leverages the manifold structures inherent in high-dimensional spaces and involves constructing an affinity graph from the data, followed by inferring labels for unlabeled nodes on this graph. While substantial efforts have been dedicated to the label inference phase, less attention has been paid to the optimization of the graph construction process, which plays a pivotal role in the performance of GSSL. In this paper, we introduce an optimal asymmetric graph structure for label prediction, rooted in theoretical motivations and distinct considerations for the roles of labeled and unlabeled nodes. An efficient block-wise graph learning algorithm is developed for networks, complete with a global convergence guarantee, and is shown to offer enhanced robustness against noisy node features through the incorporation of nodal attention mechanisms and data augmentation techniques. Our extensive experimental validation on both synthetic and real-world datasets, serving as benchmarks, underscores the superiority of our graph construction method over contemporary approaches in GSSL, thereby laying a novel groundwork for future advancements in the field. The adaptive strategies and fairness-aware considerations in node representations highlight the importance of tailored graph construction methodologies in semi-supervised learning environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Oyku_Deniz_Kose1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=gwvwbsnTps",
  "title": "Composable Coresets for Determinant Maximization: Greedy is Almost Optimal",
  "modified_abstract": "Building on the foundational work of determining effective measurement matrices for signal recovery in compressed sensing, which has identified optimal approaches for sparse signal reconstruction from various measurements, this paper further explores the optimization landscape within the realm of determinant maximization. Drawing from the principles established in prior works on dictionary-sparse recovery, especially in how to effectively handle sparse representations against different classes of measurement matrices, including (sub-)Gaussian distributions, we extend these insights into the determinant maximization problem. Given a set of $n$ vectors in $\\mathbb{R}^d$, the goal of the determinant maximization problem is to pick $k$ vectors with the maximum volume. This problem, being the MAP-inference task for determinantal point processes (DPP) and a model for diversity, has seen a surge in attention. The need to handle large data sets in applications, a challenge present in big sample settings, has led to the examination in a composable coreset setting. In this setting, our analysis incorporates a random moment calculation, and demonstrates how the Greedy algorithm's selection process, coined as the Greensy in some provably optimal frameworks, can be optimized. While prior studies, such as those by Indyk, Mahabadi, Oveis Gharan, and Rezaei across SODA'20 and ICML'19, have shown that composable coresets can achieve an optimal approximation factor of $\\tilde O(k)^k$ and a local search algorithm can reach an almost optimal guarantee of $O(k)^{2k}$, we demonstrate that the Greedy algorithm also yields composable coresets with an almost optimal approximation of $O(k)^{3k}$. This surpasses previous guarantees and aligns with empirical evidence supporting the Greedy algorithm's efficacy. We illustrate this by proving a local optimality property for Greedy - swapping a single point in the greedy solution for an unselected vector increases the volume by at most $(1+\\sqrt{k})$, complemented by an optimal interplay with the small-ball method, a result that is tight up to an additive constant of $1$. Further, experimental validation on real datasets suggests that the local optimality of the Greedy algorithm underperforms theoretical predictions, enhancing its practical value for determinant maximization challenges.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Christian_K\u00fcmmerle1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=NWrN6cMG2x",
  "title": "Moment Matching Denoising Gibbs Sampling",
  "modified_abstract": "In the landscape of machine learning and statistics, Energy-Based Models (EBMs) have emerged as powerful tools for modeling complex data distributions, motivated by advancements in flexible sampling techniques such as those highlighted in the development of efficient matrix sampling methods. This connection is evident in the challenges of training and effectively sampling from EBMs, a notable hurdle paralleling the complexities encountered in Cross-Concentrated Sampling (CCS) for matrix completion. The widely-used Denoising Score Matching (DSM) method, although scalable, exhibits inconsistency issues, leading to the energy model learning a noisy data distribution. Addressing this challenge, we propose an innovative sampling framework: (pseudo)-Gibbs sampling with moment matching. This methodology not only facilitates effective sampling from the underlying clean model of an EBM, given a noisy model trained via DSM but also illuminates a novel route to circumvent the latent inconsistencies inherent in direct sampling methods, particularly relevant in non-convex optimization landscapes often encountered in high-dimensional datasets. Through our exploration, we not only delineate the advantages of our proposed approach over related methodologies but also demonstrate through experiments how to efficaciously scale this method to accommodate high-dimensional datasets with potential applications in areas demanding low-rank matrix completion, embodying a significant step forward in the efficient utilization of EBMs across diverse application realms. Furthermore, the intricacies of row-wise sampling strategies within matrix completion tasks underscore the nuanced benefits our technique offers, especially when addressing problems associated with sparse or incomplete datasets.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~HanQin_Cai1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=fPAAgjISu0",
  "title": "In Defense of Softmax Parametrization for Calibrated and Consistent Learning to Defer",
  "modified_abstract": "This work is inspired by the critical examination of loss functions in machine learning, with a particular focus on overcoming the limitations posed by label noise and the challenges of complementary-label learning and ordinary-label training. Prior research has underscored the necessity for robust algorithms capable of learning from non-traditional labeling schemes, including complementary and ordinary labels, which sets a foundational premise for our investigation into the softmax parameterization issue within the learning-to-defer framework. Enabling machine learning classifiers to defer their decision to a downstream expert when the expert is more accurate will ensure improved safety and accuracy. This objective can be achieved with the learning-to-defer framework which aims to jointly learn how to classify and how to defer to the expert. In recent studies, it has been theoretically shown that popular estimators for learning to defer parameterized with softmax provide unbounded estimates for the likelihood of deferring, which makes them uncalibrated and under-performing in real-world applications. However, it remains unknown whether this is due to the widely used softmax parameterization, and if we can find a softmax-based estimator that is both statistically consistent and possesses a valid probability estimator, ensuring accuracy. In this work, we first show that the cause of the miscalibrated and unbounded estimator in prior literature is due to the symmetric nature of the surrogate loss functions used, and not due to softmax. We then propose a novel statistically consistent asymmetric softmax-based surrogate loss that can produce valid estimates without the issue of unboundedness. We further analyze the non-asymptotic properties of our proposed method and empirically validate its performance and calibration on benchmark datasets, including CIFAR-10.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Defu_Liu1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=gbhixjg2dX",
  "title": "Synthetic Combinations: A Causal Inference Framework for Combinatorial Interventions",
  "modified_abstract": "Guided by the challenge of efficient and effective persuasion in sequential decision making, as explored in previous works like 'Sequential Information Design: Learning to Persuade in the Dark', we extend the inquiry into causal inference with an eye towards practical implementation in systems requiring combinatorial interventions and no-regret learning mechanisms incorporating bandit-feedback approaches. In this setting, we consider a realm where there are $N$ heterogeneous units and $p$ interventions, aiming to learn unit-specific potential outcomes for any combination of these $p$ interventions, equating to $N \\times 2^p$ causal parameters with a guarantee of overcoming the exponential explosion of potential intervention combinations as $N$ and $p$ increase. This issue arises naturally in fields such as factorial design experiments and recommendation engines, highlighting the practical need for managing the bound limits posed by traditional approaches necessitating $N \\times 2^p$ experiments, which become prohibitively expensive or infeasible, compounded by the potential confounding inherent in observational data and the limits posed by bandit-feedback mechanisms. We introduce a novel model that hypothesizes latent structure across units and intervention combinations, proposing that the matrix of potential outcomes exhibits approximate rank $r$ regularity and the interaction of intervention combinations can be sparsely represented in their Fourier transformation. This model framework, respecting the bound limits of observed outcomes, facilitates the identification of all $N \\times 2^p$ parameters despite the challenges posed by unobserved confounding. We propose an estimation method, Synthetic Combinations, which exploits this latent structure to achieve finite-sample consistency for a significantly reduced observation requirement of $\\text{poly}(r) \\times \\left( N + s^2p\\right)$, surpassing the sample efficiency of prior methods and providing a robust guarantee on its effectiveness. This research not only bridges the gap between theoretical causal inference and its application to complex, real-world scenarios but also opens avenues for more efficient experimental designs and personalized recommendation systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Matteo_Castiglioni1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=rwrblCYb2A",
  "title": "Reconstructing the Mind's Eye: fMRI-to-Image with Contrastive Learning and Diffusion Priors",
  "modified_abstract": "Inspired by recent innovations in computer vision research, such as the SCENIC JAX library\u2019s facilitation of advanced vision architectures, we present MindEye, a novel fMRI-to-image approach to retrieve and reconstruct viewed images from brain activity. This convergence of neuroscience and state-of-the-art machine learning techniques, particularly in leveraging contrastive learning for retrieval and diffusion priors for reconstruction, enables our model to surpass existing benchmarks in both the fidelity of reconstructed images and the accuracy of image retrieval from brain activity data. MindEye can map fMRI brain activity to any high-dimensional multimodal latent space, such as CLIP image space, facilitating image reconstruction using generative models that accept embeddings from this latent space. Our comprehensive comparison with other methods, through both qualitative side-by-side comparisons and quantitative evaluations, evidences MindEye's state-of-the-art performance in vision and classification. Specifically, its capability to retrieve the exact original image among highly similar candidates highlights the precision of its brain embeddings in retaining fine-grained, img2img, image-specific information, enabling accurate retrieval from extensive databases like LAION-5B. Through detailed ablations, we demonstrate that MindEye's advancements are attributable to its specialized submodules for retrieval and reconstruction, refined training methodologies across multiple devices, and the utilization of significantly larger models with advanced architectures. Additionally, we showcase MindEye\u2019s proficiency in preserving low-level image features in reconstructions with outputs from a dedicated autoencoder. All code, serving as an essential prototyping resource for furthering the development of this cutting-edge technology, is publicly accessible.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Matthias_Minderer1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=xPqINp0Eu1",
  "title": "Stability of Random Forests and Coverage of Random-Forest Prediction Intervals",
  "modified_abstract": "This study is inspired by previous work on the optimal tuning of regularization parameters, often referred to as hyperparameter tuning, in machine learning models such as the ElasticNet, which underscores the importance of balancing model complexity and generalization. Similarly, we address the stability of random forests, a crucial aspect for ensuring reliable model predictions, under the condition that the squared response ($Y^2$) does not exhibit a heavy tail\u2014an assumption that holds true for the version of random forests implemented in widely used packages like \\texttt{randomForest} in \\texttt{R}. Our empirical findings suggest that stability can be achieved even in scenarios with heavy-tailed $Y^2$, thus enhancing generalization capabilities across multiple tasks, though random forests inherently do not operate in an online learning context. Leveraging the stability property of random forests, we derive a non-asymptotic lower bound on the coverage probability of prediction intervals constructed using the out-of-bag error. Additionally, we present a complementary upper bound for continuous $Y$, applicable to both random forests and any stable algorithm through the jackknife prediction interval. The discussion extends to the asymptotic coverage probability under less stringent assumptions than those previously explored, indicating that random forests not only offer reliable point predictions but also enable justified interval predictions with minimal computational overhead.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dravyansh_Sharma1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=G7Y145tm2F",
  "title": "CSLP-AE: A Contrastive Split-Latent Permutation Autoencoder Framework for Zero-Shot Electroencephalography Signal Conversion",
  "modified_abstract": "Leveraging the pioneering methods from audio and music processing domains, particularly the utilization of contrastive learning in disentangling style and content in music mixing and multitrack style transfer, we propose a novel approach for electroencephalography (EEG) signal conversion. Electroencephalography (EEG) is a prominent non-invasive neuroimaging technique providing insights into brain function through the recording of electrical activity. Unfortunately, EEG data exhibit a high degree of noise and variability across subjects, hampering generalizable signal extraction. Therefore, a key aim in EEG analysis is to extract the underlying neural activation (content) as well as to account for the individual subject variability (style). We hypothesize that the ability to transfer EEG signals between tasks and subjects without the need for extensive preprocessing requires an extraction of latent representations accounting for both content and style. Inspired by recent advancements in end-to-end voice conversion technologies, we propose a novel contrastive split-latent permutation autoencoder (CSLP-AE) framework that directly optimizes for EEG conversion. Importantly, the latent representations are guided using contrastive learning to promote the latent splits to explicitly represent subject (style) and task (content). We contrast CSLP-AE to conventional supervised, unsupervised (AE), and self-supervised (contrastive learning) training methods and find that the proposed approach provides favorable generalizable characterizations of subject and task. Our evaluations also highlight the necessity of critically examining the conversion efficacy. Importantly, the procedure also enables zero-shot transfer between unseen subjects, showcasing the end-to-end capability of the proposed framework. While the present work only considers the conversion of EEG, the proposed CSLP-AE provides a general framework for signal conversion and extraction of content (task activation) and style (subject variability) components of general interest for the modeling and analysis of biological signals.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Junghyun_Koo1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=STqaMqhtDi",
  "title": "Plug-and-Play Stability for Intracortical Brain-Computer Interfaces: A One-Year Demonstration of Seamless Brain-to-Text Communication",
  "modified_abstract": "Intracortical brain-computer interfaces (iBCIs) have shown promise for restoring rapid communication to people with neurological disorders such as amyotrophic lateral sclerosis (ALS). However, to maintain high performance over time, iBCIs typically need frequent recalibration to combat changes in the neural recordings that accrue over days. This requires iBCI users to stop using the iBCI and engage in supervised data collection, making the iBCI system hard to use. In this paper, we propose a method that enables self-recalibration of communication iBCIs without interrupting the user. Our method leverages uniquely large language models (LMs) to automatically correct errors in iBCI outputs, uniquely addressing the challenge of maintaining stable and high-quality speech understanding from brain signals over an extended period. The self-recalibration process uses these corrected outputs (\"pseudo-labels\") to continually update the iBCI decoder online. Over a period of more than one year (403 days), we evaluated our Continual Online Recalibration with Pseudo-labels (CORP) framework with one clinical trial participant. CORP achieved a stable decoding accuracy of 93.84% in an online handwriting iBCI task, significantly outperforming other baseline methods. Notably, this is the longest-running iBCI stability demonstration involving a human participant. Our results provide the first evidence for long-term stabilization of a plug-and-play, high-performance communication iBCI, addressing a major barrier for the clinical translation of iBCIs. Although not directly related, the initiative to build upon existing works, such as the multilingual aspects reflected in the unrelated 'BibleTTS' corpus, indirectly inspires our interdisciplinary approach to harnessing technology for complex human applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Elizabeth_Salesky1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=dZqcC1qCmB",
  "title": "Epistemic Neural Networks",
  "modified_abstract": "Recent advancements underscore the importance of machine learning models, including reinforcement learning systems, that incorporate mechanisms to understand and manage uncertainty, akin to how the latest large language models (LLMs) strive to interpret language within context despite substantial challenges. Communication in AI, particularly in conversational AI and systems designed for interpreting utterances, underscores the necessity of these models to manage and interpret uncertainty. Intelligence relies on an agent's knowledge of what it does not know. This capability can be assessed based on the quality of joint predictions of labels across multiple inputs, an area where the performance of models is critical. In principle, ensemble-based approaches can produce effective joint predictions, but the computational costs of large ensembles become prohibitive. We introduce the epinet, an architecture that can supplement any conventional neural network, including large pre-trained models, and can be trained with modest incremental computation via innovative approaches to estimate uncertainty. With an epinet, conventional neural networks outperform very large ensembles, consisting of hundreds or more particles, with orders of magnitude less computation. The epinet does not fit the traditional framework of Bayesian neural networks but rather introduces an innovative approach to zero-shot learning by enhancing the precision and performance of uncertainty estimation. To accommodate the development of approaches beyond BNNs, such as the epinet, we introduce the epistemic neural network (ENN) as a general interface for models that produce joint predictions. This insight into modeling uncertainty not only parallels the challenges seen with interpreting language in the context by LLMs but also marks a pivotal step forward in creating more reliable, interpretable, and efficient machine learning systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Laura_Eline_Ruis1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=b6FeLpKKjl",
  "title": "Convergence of Alternating Gradient Descent for Matrix Factorization",
  "modified_abstract": "This study is motivated by recent advancements in the optimization landscape, specifically in the context of bilevel optimization problems that have garnered attention due to their applicability in meta-learning, hyperparameter optimization, and reinforcement learning. Leveraging insights from these areas, particularly on achieving provably faster convergence rates through novel algorithmic strategies, we analyze the efficacy of alternating gradient descent (AGD) with a fixed step size in the asymmetric matrix factorization objective using a novel initialization strategy. We demonstrate that for a rank-$r$ matrix $A \\in \\mathbb{R}^{m \\times n}$, $T = C ( \\frac{\\sigma_1(A)}{\\sigma_r(A)} )^2 \\log(1/\\epsilon)$ iterations of AGD are sufficient to achieve an $\\epsilon$-optimal factorization $\\| A - X_{T} Y_{T}' \\|^2 \\leq \\epsilon \\| A \\|^2$ with high probability, starting from an initial random initialization strategy that we propose. The factors have rank $d \\geq r$, enabling the constant $C$ in the iteration complexity $T$ to remain absolute. Our experimental results, utilizing comprehensive machine learning techniques, indicate that the proposed initialization significantly enhances the convergence rate of gradient descent in practical scenarios. The simplicity of our proof, underpinned by a uniform Polyak-Lojasiewicz (PL) inequality and a consistent Lipschitz smoothness constant, illustrates the potential for extending and simplifying convergence analyses across a spectrum of nonconvex low-rank factorization and learning problems. Collectively, these findings provide robust estimator techniques for matrix factorization convergence, solidifying their relevance for effective meta-learning strategies and bilevel optimization.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kaiyi_Ji1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=5MG5C5aS6m",
  "title": "Global Optimality in Bivariate Gradient-based DAG Learning",
  "modified_abstract": "Building on recent interests in learning acyclic directed graphical models, which analyze the conditional independence structure of vector-valued data through non-convex optimization, this work extends the investigation into the statistical problem of learning such models from data with a focus on global optimality. Previous investigations have primarily addressed high-dimensional data and functional graphical models, underscoring the complexity of capturing the conditional independence structure in varied data types, including modern applications like fmri data analysis. Unlike these efforts that leverage neighborhood selection and other first-order optimization schemes, our paper demonstrates, for the first time, that a straightforward path-following optimization scheme can achieve global convergence to the minimum of the population loss in a bivariate setting directly. This discovery challenges the prevailing notion that non-convex optimization problems, especially those not considered \"benign,\" are susceptible to multiple spurious solutions, thereby highlighting our method's unique capability to navigate the intricate landscape of directed acyclic graph (DAG) learning problems effectively. We emphasize the implications of our findings for modern statistical methods that work across diverse distributions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Percy_Shengjun_Zhai1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=8YN62t19AW",
  "title": "A Unified Discretization Framework for Differential Equation Approach with Lyapunov Arguments for Convex Optimization",
  "modified_abstract": "Our research is motivated by a growing interest in the differential equation (DE) approach for convex optimization and its correlations with continuous DEs through rate-revealing Lyapunov functionals, highlighted since the seminal work by Su--Boyd--Cand\u00e8s (2014). While this approach has enriched our understanding of optimization dynamics, its practical application has been hindered by the absence of a seamless transition mechanism from continuous insights back into discrete optimization methods. This paper addresses this limitation by proposing a novel concept, the \"weak discrete gradient\" (wDG), which introduces a systematic framework for integrating discrete gradients within DE approach arguments, thereby simplifying the translation of continuous DE insights to discrete optimization analysis. By defining abstract optimization methods using wDG and establishing abstract convergence theories through iteration, we draw parallels with continuous DE arguments, offering a grounded theory that encompasses broad classes of optimization methods. Our framework not only facilitates the derivation of existing optimization methods and their convergence rates, aligning with or surpassing state-of-the-art techniques like Nesterov's accelerated gradient, but also paves the way for inventing new methods within this unified discretization paradigm. This advancement builds upon the analytical groundwork laid by prior studies, such as the examination of policy gradient methods' linear convergence in finite time within specified settings, and extends the application of these foundational concepts to a wider scope of convex optimization challenges, including those on a large scale.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jalaj_Bhandari1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=q8mH2d6uw2",
  "title": "Deep Contract Design via Discontinuous Networks",
  "modified_abstract": "Contract design involves a principal who establishes contractual agreements about payments for outcomes that arise from the actions of an agent. Inspired by recent developments in federated learning, specifically the Federated Frank-Wolfe Algorithm for privacy-preserving collaborative systems and constrained optimization, this paper introduces a novel approach to contract design leveraging deep learning for risk-minimization. We propose the Discontinuous ReLU (DeLU) network as a pioneering representation, which models the principal's utility as a discontinuous piecewise affine function where each segment corresponds to the agent adopting a specific action. The DeLU network architecture facilitates an implicit learning of the incentive compatibility constraints for the agent and the principal's utility maximization objective, incorporating algorithms that enable parallel inference on each discrete piece of the principal's utility function through the application of linear programming or interior-point methods. These algorithms are pivotal in the design for determining the optimal contracts. Our empirical studies underscore the DeLU network's efficacy in approximating the principal's utility function with minimal training data and its capability to scale in devising near-optimal contracts for scenarios characterized by a vast array of actions and outcomes. Moreover, the integration of sparse representations within the network promotes privacy and reduces complexity, utilizing projections to ensure data alignment with privacy-preserving requirements, making it a comprehensive solution for machine-driven contract design.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ali_Dadras1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=tesBViWnbx",
  "title": "Stable Diffusion is Unstable",
  "modified_abstract": "Building on recent advancements in artificial intelligence, particularly in the domain of nonconvex optimization methods such as the development of advanced stochastic mixing techniques, our study unveils a critical vulnerability in the burgeoning field of text-to-image models. Despite their impressive generative capabilities, we uncover a significant lack of robustness in these models' generation process, evidenced by their susceptibility to slight alterations in text prompts. We introduce **Auto-attack on Text-to-image Models (ATM)**, a novel, gradient-based method designed to efficiently generate perturbations that exploit this vulnerability. By employing a Gumbel Softmax distribution, ATM facilitates a continuous approach to the traditionally discrete process of word replacement or extension, ensuring the method's differentiability. This innovation allows ATM to sample multiple attack instances concurrently, hindering the generative model from accurately producing the intended subjects without altering the category-defining keywords in the prompt. ATM's effectiveness is empirically demonstrated through a success rate of 91.1% in compromising short-text prompts and 81.2% in long-text scenarios. Further investigation uncovers three distinct patterns of attack related to variability in generation speed, similarity in coarse-grained characteristics, and the polysemy of words. The paper emphasizes the need for developing more robust generation models against such adversarial attacks, highlighting its applications in enhancing machine learning security. The code is available at [link removed for anonymity].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Fuchao_Wei1",
  "manipulated_ranking": 9,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=gMjIUZBKH8",
  "title": "AdaVAE: Bayesian Structural Adaptation for Variational Autoencoders",
  "modified_abstract": "Considering the advancements in deep neural networks for solving linear inverse problems such as super-resolution, image deblurring, and image denoising, where the implicit priors within networks have guided the development of novel solutions, this paper introduces a significant leap in variational autoencoder (VAE) design and regularization. The neural network structures of generative models and their corresponding inference models paired in VAEs play a critical role in the models' generative performance. However, powerful VAE network structures are hand-crafted and fixed prior to training, resulting in a one-size-fits-all approach that requires heavy computation to tune for given data. Moreover, existing VAE regularization methods largely overlook the importance of network structures and fail to prevent overfitting in deep VAE models with cascades of hidden layers. To address these issues, we propose a Bayesian inference framework that automatically adapts VAE network structures to data and prevents overfitting as they grow deeper. We model the number of hidden layers with a beta process to infer the most plausible encoding/decoding network depths warranted by data and perform layer-wise dropout regularization with a conjugate Bernoulli process. We develop a scalable estimator that performs joint inference on both VAE network structures and latent variables, incorporating least-squares optimization for efficient training. Our experiments show that the inference framework effectively prevents overfitting in both shallow and deep VAE models, yielding state-of-the-art performance in tasks such as super-resolution, image deblurring, and compressive sampling. We demonstrate that our framework is compatible with different types of VAE backbone networks and can be applied to various VAE variants, further improving their performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zahra_Kadkhodaie1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=uhKtQMn21D",
  "title": "Mechanic: A Learning Rate Tuner",
  "modified_abstract": "Inspired by seminal works on cyclical step-sizes and the exploitation of spectral gaps in Hessians within the realm of machine learning optimization, our study introduces Mechanic, a novel technique designed for the dynamic tuning of the learning rate scale factor across any base optimization algorithm and its associated schedule. Mechanic embodies a practical application of theoretical concepts derived from online convex optimization, including concepts such as convergence rates and complexity, to effectively adapt learning rates in response to problem-specific conditions. Through rigorous evaluation across a broad spectrum of large scale deep learning tasks\u2014featuring diverse configurations in batch sizes, schedules, and base optimization algorithms\u2014our experiments highlight Mechanic's capability to either closely approximate, match, or surpass the performance benchmarks set by manual learning rate adjustments. We show that this improvement in convergence particularly impacts tasks involving quadratic loss functions and those that would benefit from cyclical adjustment patterns, providing a comparative analysis that not only reinforces the utility of Mechanic in optimizing learning rates more effectively than traditional manual tuning but also places our contributions in the context of advancing optimization methods for deep learning applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Baptiste_Goujaud1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=t3WCiGjHqd",
  "title": "Scalable Membership Inference Attacks via Quantile Regression",
  "modified_abstract": "Inspired by previous works in meta-learning and the challenges of representation learning, our research develops a novel approach to membership inference attacks that bypass the computational inefficiencies of existing methods. Membership inference attacks aim to ascertain if a specific example was part of the training set of a model, utilizing only black-box access. Traditional strategies have relied on training multiple shadow models, a process both computationally intensive and impractical for large models in tasks such as conditioning on specific clusters of data. Addressing these limitations, we introduce a new class of attacks leveraging quantile regression to analyze the distribution of confidence scores from the model on unseen data points, making conditional predictions more feasible. Our approach is competitive with the shadow model technique in efficacy but requires significantly reduced computational resources, entailing the training of only a singular model with optimized hyper-parameters \u2013 a meta-algorithm of sorts. Crucially, our method does not necessitate any knowledge of the targeted model's architecture, embodying a truly black-box attack. The effectiveness of our proposal is validated through rigorous experimentation across a variety of datasets and model architectures, solidifying our claims with hard empirical evidence. In acknowledgment of open scientific inquiry, the code for our method has been made available, sans the provided link due to restrictions on personal identifiable information sharing.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Giulia_Denevi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=JOHp5SmckS",
  "title": "Towards Combinatorial Generalization for Catalysts: A Kohn-Sham Charge-Density Approach",
  "modified_abstract": "Inspired by recent advancements in machine learning for scientific applications, such as efficient density estimation via probabilistic circuits in 'Random Probabilistic Circuits', this paper explores an innovative approach towards catalyst modeling. The Kohn-Sham equations underlie many important applications, including the discovery of new catalysts. Recent machine learning work on catalyst modeling has focused on the prediction of energy but has so far not yet demonstrated significant out-of-distribution generalization, a crucial benchmark for evaluating the performance of these models. Here we investigate another approach based on the pointwise learning of the Kohn-Sham charge-density. On a new dataset of bulk catalysts with charge densities, we show density models can generalize to new structures with combinations of elements not seen at train time, a form of combinatorial generalization. We detail how various tasks within this domain, leveraging randomized methods and probability assessments for model evaluations, can benefit from our findings. More than 80% of binary and ternary test cases achieve faster convergence than standard baselines in Density Functional Theory, amounting to an average reduction of 13% in the number of iterations required to reach convergence, a performance that certainly sets new benchmarks in the field and may be of independent interest to researchers in their respective tasks. Our results suggest that density learning is a viable alternative, trading greater inference costs for a step towards combinatorial generalization, a key property for applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Gennaro_Gala1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=S2k5dBb91q",
  "title": "Transportability for Bandits with Data from Different Environments",
  "modified_abstract": "The exploration of algorithms that can adapt and optimize policies across a variety of environments, especially in the context of reinforcement learning (RL) and bandit problems, builds on a foundation of research into offline RL, conservative policy learning, and causal modeling. Previous studies, such as the development of provably optimal conservative offline RL algorithms that handle partial data coverage without the need for uncertainty quantification, have established groundwork on leveraging historical data for policy learning. Our work extends these insights into the domain of multi-environment bandit problems, focusing on the utilization of batch data combined with causal assumptions to address the challenges of policy optimization in diverse, potentially unrelated environments. Specifically, we explore the concept of transportability in bandit algorithms by identifying and exploiting invariances within causal models that link disparate environments. This approach allows for the improvement of learning outcomes through the strategic use of related environmental data and sampling techniques, leading to a bandit algorithm with sub-linear regret bounds that effectively captures the informativeness of such data for the task at hand. By incorporating regularization strategies, termed as regularizers, and sample-based approximation methods, our findings demonstrate the potential for significantly reduced regret compared to traditional experimentation-only approaches, presenting a significant step forward in the design of intelligent agents capable of adapting to a wide array of environmental conditions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kunhe_Yang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=MirclT6zpv",
  "title": "Delayed Algorithms for Distributed Stochastic Weakly Convex Optimization",
  "modified_abstract": "Informed by critical advancements in decentralized optimization, particularly in escaping saddle points and achieving second-order optimality, this paper addresses the challenges associated with communication delays in distributed networks. Guided by recent findings by Xu et al. 2022 on the impact of delays in stochastic optimization, we extend the discourse into the realm of delayed stochastic algorithms for weakly convex optimization, focusing on a distributed optimization network model. Our contribution includes the formulation of the delayed stochastic subgradient method (DSGD), which showcases a tighter convergence rate influenced by the expected delay (\\bar{\\tau}), in contrast to the maximum information delay (\\tau_{\\text{max}}). Additionally, we introduce the delayed stochastic prox-linear (DSPL) method for a specific class of composition weakly convex problems, demonstrating that the delays become negligible in the high-order term of the convergence rate after a certain threshold of iterations. The robustness of our algorithms against arbitrary communication delays is enhanced by incorporating a safeguarding step, which aligns the convergence rates with the number of workers, thereby mitigating the delay's impact and emphasizing the role of curvature in the analysis of convergence and escaping from saddle point scenarios. Our numerical experiments validate the effectiveness of our proposed methods, confirming their empirical superiority over existing approaches in handling delays within decentralized distributed stochastic optimization tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Isidoros_Tziotis1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=XvGQ6F3sG8",
  "title": "Self-supervised Graph Neural Networks via Low-Rank Decomposition",
  "modified_abstract": "Inspired by recent advancements in graph neural networks (GNNs) for unsupervised learning, such as the application of self-augmented graph contrastive learning to address the non-steady performance across different graph datasets, this paper explores the potential of self-supervised learning to train GNNs by moving beyond the common utilization of propagation-based models. The typical reliance on these models is identified to limit the ability of GNNs to capture local properties and to adapt to networks beyond homophily without label information. To overcome these limitations, our work introduces Low-Rank Decomposition-based GNNs (LRD-GNN-Matrix), which utilize Low-Rank Decomposition on the attribute matrix to ensure that propagation within each ego-network remains class-specific, thereby preserving the low-rank characteristic of the obtained representation matrix. Additionally, to assimilate long-distance relationship information, we propose the Low-Rank Tensor Decomposition-based GNN (LRD-GNN-Tensor) that constructs a node attribute tensor from selected similar ego-networks and applies Low-Rank Tensor Decomposition. The use of the tensor nuclear norm is crucial for capturing long-distance relationships between original and selected similar ego-networks. Our extensive experiments showcase the enhanced performance and robustness of LRD-GNNs, marking a significant step forward in self-supervised learning for GNNs.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shichao_Pei1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=roGYQvarnC",
  "title": "ConRad: Image Constrained Radiance Fields for 3D Generation from a Single Image",
  "modified_abstract": "Inspired by recent advances in inverse graphics and 3D-aware generative models, our work introduces a significant step forward in reconstructing 3D objects from a single RGB image. By leveraging cutting-edge image generation models that infer hidden 3D structure while remaining faithful to the appearance of the input image, we address the limitations present in generating 3D models directly from text prompts and conditioning on RGB data\u2014challenges not adequately solved by existing approaches. Existing methods, as evidenced by developments such as EVA3D in compositional human body generation, showcase the potential of 3D-aware models but often struggle with the alignment between input images and 3D outputs, particularly for non-rigid objects. Our novel method, Image Constrained Radiance Fields (ConRad), introduces an efficient 3D representation specifically designed to encapsulate the appearance from a single viewpoint image and manage texture details. By proposing a training algorithm that integrates single RGB image data with pretrained Diffusion Models and utilizes advanced sampling techniques, we optimize ConRad parameters to faithfully preserve image details in the 3D space, while rendering efforts are meticulously designed to address imbalances in lighting, texture, and perspective. Our extensive experimental investigation demonstrates the superiority of ConRad in generating realistic 3D reconstructions that remain true to the input while achieving significant quantitative improvements over state-of-the-art baselines on ShapeNet benchmarks, propelling forward the interface of learning-based 3D reconstruction. This work not only bridges gaps in image-based 3D reconstruction but also sets a new benchmark for future research in the field, especially in understanding sparse data and managing imbalanced learning conditions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhaoxi_Chen1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=wg3d2FKAm8",
  "title": "Outlier-Robust Wasserstein DRO",
  "modified_abstract": "Distributionally robust optimization (DRO) is an effective approach for data-driven decision-making in the presence of uncertainty. Geometric uncertainty due to sampling or localized perturbations of data points is captured by Wasserstein DRO (WDRO), which seeks to learn a model that performs uniformly well over a Wasserstein ball centered around the observed data distribution. However, WDRO fails to account for non-geometric perturbations such as adversarial outliers, which can greatly distort the Wasserstein distance measurement and impede the learned model's robustness. We address this gap by proposing a novel outlier-robust WDRO framework for decision-making under both geometric (Wasserstein) and non-geometric (total variation (TV)) contamination that allows an $\\varepsilon$-fraction of data to be arbitrarily corrupted. We design an uncertainty set using a certain robust Wasserstein ball that accounts for both perturbation types and derive minimax optimal excess risk bounds for this procedure that explicitly capture the Wasserstein and TV risks. Our approach includes a reweighting scheme to mitigate the influence of outliers. We prove a strong duality result that enables tractable convex reformulations and efficient computation of our outlier-robust WDRO problem. When the loss function depends only on low-dimensional features of the data, we eliminate certain dimension dependencies from the risk bounds that are unavoidable in the general setting. Finally, we present experiments validating our theory on standard regression and classification tasks, setting a new benchmark in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Michael_Oberst1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=GPtroppvUM",
  "title": "Adversarial Training for Graph Neural Networks: Pitfalls, Solutions, and New Directions",
  "modified_abstract": "Incorporating insights from recent successes in the application of Transformer networks to 3D atomistic graphs, our investigation into adversarial training for Graph Neural Networks (GNNs) attempts to fill a critical gap in the current defensive measures against adversarial attacks on graph data. Prior studies, such as the development of the Equiformer, which showcases the adeptness of Transformer architectures in handling complex 3D graph structures through equivariant features and attention mechanisms, pave the way for our exploration. Despite its success in various domains, including molecules, adversarial training did not (yet) stand out as an effective defense for GNNs against graph structure perturbations. In the pursuit of fixing adversarial training, we (1) show and overcome fundamental theoretical as well as practical limitations of the adopted graph learning setting in prior work; (2) reveal that flexible, multi-layer GNNs based on learnable graph diffusion are able to adjust to adversarial perturbations, while the learned message passing scheme is naturally interpretable through inductive representations; (3) introduce the first attack for structure perturbations that, while targeting multiple nodes at once, is capable of handling global (graph-level) as well as local (node-level) translational constraints. Including these contributions, we demonstrate that adversarial training, especially when enhanced with neural attention and 3d-related adaptations, is a state-of-the-art defense against adversarial structure perturbations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tess_Smidt1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=2NncD8AaFK",
  "title": "CoLLAT: On Adding Fine-grained Audio Understanding to Language Models using Token-Level Locked-Language Tuning",
  "modified_abstract": "In an era where language models are increasingly bridging the gap between human and machine understanding across modalities, leveraging insights from recent explorations in natural language supervision, audio-visual data interpretation, and consensus among multiple networks for a unified task description has become imperative. Inspired by advancements in capturing the essence of visual and audio data through natural language and achieving consensus among various encoder-decoder architectures for a unified description, our work introduces $CoLLAT$: $Co$ntrastive $L$ocked $L$anguage and $A$udio $T$uning. This framework aims to enhance audio understanding in language models beyond the conventional audio classification models, which are often limited by their failure to predict unseen classes, their inability to maintain the textual understanding capabilities of pre-trained language models via detailed description and processing, and the challenges in handling complex tasks like audio-guided image generation. $CoLLAT$ addresses these challenges by locking a language model and training it with a novel pretraining objective for audio-to-text grounding, facilitating fine-grained audio understanding. Through various downstream applications including audio classification, cross-modal retrieval, audio-guided image generation, and potentially extending to videos where audio plays a crucial role in understanding content, our extensive testing demonstrates that $CoLLAT$ not only achieves state-of-the-art performance in audio understanding tasks but also pioneers the use of audio guidance in applications developed atop pre-trained language models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Simion-Vlad_Bogolin1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=OzjBohmLvE",
  "title": "Achieving $\\mathcal{O}(\\epsilon^{-1.5})$ Complexity in Hessian/Jacobian-free Stochastic Bilevel Optimization",
  "modified_abstract": "Inspired by recent theoretical advancements in adaptive gradient methods, such as the convergence analysis of AdaGrad in smooth convex and quasar-convex optimization settings, this paper embarks on extending these foundational insights into the domain of bilevel optimization. Specifically, we address the bilevel optimization problem characterized by a nonconvex upper-level objective and a strongly convex lower-level objective in various settings. Despite significant interest, achieving an $\\mathcal{O}(\\epsilon^{-1.5})$ sample complexity in Hessian/Jacobian-free stochastic bilevel optimization remains an unresolved challenge, particularly without resorting to second-order derivative computations. To address this gap, we introduce FdeHBO, a novel Hessian/Jacobian-free bilevel optimization method that capitalizes on adaptive techniques, a simple fully single-loop structure, a projection-aided finite-difference Hessian/Jacobian-vector approximation technique, and vanilla momentum-based updates. Theoretically, we establish that FdeHBO achieves the desired $\\mathcal{O}(\\epsilon^{-1.5})$ complexity, requiring $\\mathcal{O}(\\epsilon^{-1.5})$ iterations with each iteration utilizing $\\mathcal{O}(1)$ samples and solely first-order gradient information to converge to an $\\epsilon$-accurate stationary point in smooth settings. This not only enhances our understanding of stochastic bilevel optimization but also demonstrates FdeHBO as the pioneer adaptive, Hessian/Jacobian-free method exhibiting an $\\mathcal{O}(\\epsilon^{-1.5})$ sample complexity for nonconvex-strongly-convex stochastic bilevel optimization scenarios, setting a new average performance benchmark.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ta_Duy_Nguyen1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=PcNpL9Q39p",
  "title": "Responsible AI (RAI) Games and Ensembles",
  "modified_abstract": "Inspired by advancements in game-theoretic reinforcement learning, which integrate tree-search methods, generative models, and concepts from Nash bargaining, our research introduces a novel framework dubbed Responsible AI (RAI) games. This exploration is rooted in a growing body of work that examines the societal impacts of artificial intelligence, focusing on critical issues like fairness, robustness, and safety. Specifically, within the context of RAI, we address the challenge of minimizing a learner's worst-case loss across uncertainty sets, which often manifest as perturbed versions of the empirical distribution, framing these challenges as min-max problems. Our contribution lies in presenting a comprehensive framework for analyzing such problems through the lens of RAI games, and proposing two distinct classes of solution algorithms: game-play based algorithms inspired by online learning and game theory, and greedy stagewise estimation algorithms, inspired by classical statistical approaches to boosting and regression. Through empirical evaluations, we demonstrate the effectiveness and competitive performance of our methodologies in addressing RAI challenges, particularly those associated with subpopulation shifts.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kate_Larson1",
  "manipulated_ranking": 1,
  "natural_ranking": 102
}
{
  "paper_link": "https://openreview.net/forum?id=Grz2ijKrWI",
  "title": "STXD: Structural and Temporal Cross-Modal Distillation for Multi-View 3D Object Detection",
  "modified_abstract": "Inspired by the recent progress in understanding and detecting articulated 3D object articulation from RGB videos, this paper extends the exploration of multi-view 3D object detection (3DOD) through a novel paradigm. Recognizing the economic appeal and the inherent challenges of multi-view image-based 3DOD in the absence of precise spatial cues, we introduce a novel structural and temporal cross-modal knowledge distillation (STXD) framework. This framework aims to overcome the limitations of previous cross-modal distillation methods that have primarily focused on minimizing global distances between features from different modalities, including planar surfaces in environments. STXD innovatively enhances knowledge transfer from a LiDAR-modality teacher to a multi-view image-modality student by regularizing cross-correlation of cross-modal features to reduce redundancy and maximize similarity, encoding temporal relations across sequences of frames, and adopting response distillation to improve output-level knowledge distillation quality. Our extensive experiments demonstrate that STXD significantly boosts the performance of base student detectors, achieving increases of 2.8%~4.5% in NDS and mAP on the nuScenes testing dataset, thereby marking a significant step forward in utilizing cross-modal distillation for advanced 3DOD solutions. This breakthrough showcases the pivotal role of detecting intricate object articulations and planar environmental features through advanced computer vision techniques and the utilization of extensive datasets for model validation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chris_Rockwell1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=3H37XciUEv",
  "title": "Post Hoc Explanations of Language Models Can Improve Language Models",
  "modified_abstract": "Motivated by the exploration of innovative methods to enhance language model capabilities, such as the use of prototypical common-sense reasoning in question-answering datasets, this study introduces a pioneering approach to improving Large Language Models (LLMs). Large Language Models (LLMs) have demonstrated remarkable capabilities in performing complex tasks. Moreover, recent research has shown that incorporating human-annotated rationales (e.g., Chain-of-Thought prompting) during in-context learning can significantly enhance the performance of these models, particularly on tasks that require reasoning capabilities. However, incorporating such rationales poses challenges in terms of scalability as this requires a high degree of human involvement. In this work, we present a novel framework, Amplifying Model Performance by Leveraging In-Context Learning with Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges by automating the process of rationale generation. To this end, we leverage post hoc explanation methods which output attribution scores (explanations) capturing the influence of each of the input features on the model predictions. More specifically, we construct automated natural language rationales that embed insights from post hoc explanations to provide corrective signals to LLMs. Extensive experimentation with real-world datasets demonstrates that our framework, AMPLIFY, leads to prediction accuracy improvements of about 10-25% over a wide range of tasks, including those where prior approaches that rely on human-annotated rationales such as Chain-of-Thought prompting fall short. Our work makes one of the first attempts at highlighting the potential of post hoc explanations as valuable tools for enhancing the effectiveness of LLMs. Furthermore, we conduct additional empirical analyses and ablation studies to demonstrate the impact of each of the components of AMPLIFY, which, in turn, lead to critical insights for refining in-context learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tim_O'Gorman2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=GEWzHeHpLr",
  "title": "Transition-constant Normalization for Image Enhancement",
  "modified_abstract": "Normalization techniques effectively encapsulate style within statistical representations, a concept widely utilized in deep neural networks for various computer vision tasks. Inspired by recent advancements in real-world image super-resolution that leverage feature matching with implicit high-resolution priors, our study introduces the Transition-Constant Normalization (TCN) to further the discourse on normalization's impact on image enhancement performance. TCN is innovatively designed with two streams of normalization operations under an invertible constraint and a feature sub-sampling operation that adheres to this normalization criterion, offering a unique approach to image enhancement tasks. This method is distinguished by its parameter-free, plug-and-play nature, coupled with the advantage of incurring no additional computational costs. Additionally, TCN promises ease in the reconstruction of enhanced images from its normalized features, even when they are distorted. Highlighting its versatility, TCN has been adeptly integrated into enhancement networks and incorporated into encoder-decoder architectures for downsampling, and for implementing efficient architectures that have often been pretrained on massive datasets. Our rigorous evaluation across a spectrum of enhancement tasks, including low-light enhancement, exposure correction, SDR2HDR translation, and image dehazing, consistently illustrates TCN's capability to elevate performance. Furthermore, its applicability extends to other domains such as pan-sharpening and medical segmentation, demonstrating its broad utility. The code is publicly available, reinforcing our commitment to transparency and collaboration in the research community.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chaofeng_Chen1",
  "manipulated_ranking": 6,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=9KtX12YmA7",
  "title": "The Behavior and Convergence of Local Bayesian Optimization",
  "modified_abstract": "Inspired by recent insights in the fields of streaming principal component analysis and robust optimization, this paper investigates the behavior and convergence properties of local Bayesian optimization strategies. Previous works, such as those analyzing the noisy power method and Oja\u2019s algorithm under perturbations in streaming data models, provide a context for understanding the robustness and adaptability of optimization methods to variations and uncertainties inherent in data-generating processes. This backdrop supports the examination of temporal stability and the matrix component decomposition's role in the effectiveness of optimization strategies. Building on this knowledge, we explore how local optimization strategies in Bayesian frameworks can leverage the principal components of high-dimensional data, thus delivering strong empirical performance on high-dimensional problems, challenging the prevailing belief that such approaches merely circumvent the curse of dimensionality. Experiments conducted reveal the rate-optimal convergence of these strategies and highlight their superior performance in recovering specific structures within the data. We first study the behavior of local optimization and find that the statistics of individual local solutions based on Gaussian process sample paths and the matrix structure they inhabit are favorable compared to global methods. Following this, we present the first rigorous analysis of a Bayesian local optimization algorithm recently proposed and derive convergence rates for both noisy and noiseless settings, thereby underscoring the temporal dynamics of optimization paths and streaming data's critical role. Our work contributes to a deeper understanding of Bayesian local optimization's potential in addressing complex optimization problems in high-dimensional spaces.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Apurv_Shukla1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=PSngfm5B9q",
  "title": "Beyond Exponential Graph: Communication-Efficient Topologies for Decentralized Learning via Finite-time Convergence",
  "modified_abstract": "Informed by the growing importance of decentralized learning for privacy preservation and parallel computation, and inspired by prior work on the dynamics of multi-agent communication topologies, this study investigates the balance between the consensus rate and communication overhead in the network topology for decentralized learning. Specifically, leveraging insights from a meta-learning approach to cooperative multi-agent communication topology, which highlights the challenges and solutions in dynamically adjusting the communication pathways for enhanced team coordination and learning efficiency, we identify the critical need for topologies that achieve both a fast consensus rate and minimal communication costs. To address this, we introduce the Base-$(k+1)$ Graph, a novel topology that combines a rapid consensus rate with a low maximum degree, enabling exact consensus among all nodes after a finite number of iterations for any given number of nodes and maximum degree $k$. This topology promises higher accuracy and better communication efficiency for Decentralized Stochastic Gradient Descent (DSGD) compared to traditional topologies like the exponential graph, potentially benefiting from methods such as backpropagation, reinforcement learning, and meta-training in decentralized settings. Experiments across various topologies affirm the Base-$(k+1)$ Graph's superior performance in decentralized learning tasks, offering a more efficient alternative for achieving consensus with reduced communication demands. Our contributions therefore not only answer the call for communication-efficient learning topologies but also pave the way for the reformulation of future explorations in decentralized learning frameworks with a keen interpretation of topological effects. The provided GitHub link has been removed to comply with the request for personal identifiable information exclusion.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dingyang_Chen1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=u6Ibs4hTJH",
  "title": "Real-World Image Variation by Aligning Diffusion Inversion Chain",
  "modified_abstract": "Inspired by the groundbreaking work in synthetic image generation, such as COnditional COordinate Generative Adversarial Network (COCO-GAN), which introduced novel approaches to image creation through conditional and coordinate-based patch generation, our research extends the frontier of diffusion models to navigate the domain gap between synthetic and real-world imagery more effectively. Despite the progress in using diffusion models for generating high-fidelity images from text prompts, challenges remain in producing high-quality variations of real-world images with the desired resolution due to a distinct domain gap originating from a discrepancy in latents' distribution across different diffusion processes. Addressing this, we introduce an innovative inference pipeline, Real-world Image Variation by ALignment (RIVAL), that leverages the principles of diffusion models for the generation of image variations directly from a single image exemplar. By focusing on the alignment of the image generation process with the source image's inversion chain through step-wise latent distribution alignment, cross-image self-attention injection, and step-wise distribution normalization, RIVAL significantly enhances the quality and resolution of generated image variations in parallelism with the real world. Our approach achieves superior performance over existing methodologies in systems designed for generation tasks in terms of semantic similarity, perceptual quality, and panorama-like completeness, marking a notable advance in the application of diffusion models to real-world image variation generation. Furthermore, our generalized pipeline, which exploits the principles of massive data and computational parallelism, promises broader applicability across various diffusion-based generation tasks, offering a robust solution to the challenge of aligning generated images more closely with real-world appearances.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chia-Che_Chang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=HMhEFKDQ6J",
  "title": "Unifying GANs and Score-Based Diffusion as Generative Particle Models",
  "modified_abstract": "In light of the advancements in distributed computing and high-dimensional data analysis that have significantly influenced machine learning methodologies, our research introduces a groundbreaking approach that reconciles the seemingly divergent generative adversarial networks (GANs) with particle-based generative models, such as gradient flows and score-based diffusion models. These models, celebrated for their remarkable ability to model complex distributions in high-dimensions, have traditionally been viewed in contrast to GANs, which rely on a pushforward generator network for training and are often optimized through consensus-seeking algorithms for stabilizing training dynamics. By reinterpreting generator training through the lens of particle models, we propose a unified framework that fundamentally broadens the operational paradigm of generative models. This framework posits that incorporating a sparse representation design feature within a score-based diffusion model, as well as conceiving a GAN devoid of an explicit generator, are natural extensions of our unifying theory. The feasibility and potential of our approach are substantiated through empirical evaluations, which serve as proofs of concept for the practical applicability of this unified generative framework. The computational benefits from distributed processing techniques are intrinsic to our proposed methodology and play crucial roles in managing the complexity of training generative models on high-dimensional data. This report highlights the design principles and empirical proofs that demonstrate the viability and innovation of our approach.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Marie_Maros1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=ftPoVcm821",
  "title": "Demo2Code: From Summarizing Demonstrations to Synthesizing Code via Extended Chain-of-Thought",
  "modified_abstract": "Inspired by recent strides in linking human instructions to reinforcement learning and code synthesis, our study, Demo2Code, innovates at the intersection of language instructions, demonstrations, and Large Language Models (LLMs). These preceding works establish a solid foundation in understanding and generating code from human instructions and point to the significance of natural language processing and demonstration in enhancing machine interpretation and functionality for complex, multi-task environments across the world. Language instructions and demonstrations are two natural ways for users to teach robots personalized tasks. Recent progress in LLMs has shown impressive performance in translating language instructions into code for robotic tasks, with agents being trained for specific functionalities. However, translating demonstrations into task code continues to be a challenge due to the length and complexity of both demonstrations and trajectories, making learning a direct mapping intractable. This paper presents Demo2Code, our novel framework that generates robot task code from demonstrations via an extended chain-of-thought and defines a common latent policy to connect the two. Our framework employs a robust two-stage process: (1) a recursive summarization technique that condenses demonstrations into concise specifications, strategically overcoming sparse-reward challenges, and (2) a code synthesis approach that expands each function recursively from the generated specifications. We conduct extensive evaluation on various robot task benchmarks, including a novel game benchmark Robotouille, designed to simulate diverse cooking tasks in a kitchen environment that operates on a grid-based layout.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kenneth_Marino1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=xpjsOQtKqx",
  "title": "StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners",
  "modified_abstract": "In the context of recent advancements in generating high-quality synthetic images through text-to-image models, our research investigates the potential of these technologies for learning visual representations. Specifically, this study is motivated by the capabilities of leading generative models, such as Stable Diffusion, which are delineated in prior works focusing on evaluating generative models based on the realism, \"diversity\", and divergence of the produced images. We demonstrate that (1) when the generative model is properly configured, training self-supervised methods on synthetic images, with proper sampling strategies, can match or surpass the performance obtained using real images; and (2) through a novel application of multi-positive contrastive learning, termed StableRep, we utilize multiple images generated from the same text prompts as mutual positives to enhance the learning process through an increased diversity in sampling. This approach aligns with evaluations of distribution characteristics and attributes of synthetic vs. real images by using vectors to compare the semantic embeddings learned by the model. Our findings reveal that representations learned solely from synthetic images can outperform those learned from real images using SimCLR and CLIP on large-scale datasets. Moreover, when introducing language supervision, StableRep trained with 20 million synthetic images (10 million captions) surpasses the evaluation accuracy of CLIP trained with 50 million real images (50 million captions). This exploration establishes a new paradigm for utilizing synthetic imagery to train robust visual representation models, thereby extending the utility of text-to-image models beyond their direct applications in image generation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mingi_Kwon1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=GDYuzX0rwj",
  "title": "Facing Off World Model Backbones: RNNs, Transformers, and S4",
  "modified_abstract": "Inspired by prior works that demonstrate the limitations of recurrent neural networks (RNNs) in handling long-term dependencies and the effectiveness of specialized architectures in improving sequence memory capabilities, this paper investigates alternative world model backbones for model-based reinforcement learning (MBRL). World models are critical for simulating future scenarios in partially observable environments through observation, which demand robust long-term memory capabilities. While RNNs have been the de facto standard in many state-of-the-art MBRL agents, they often fall short in tasks requiring long-term memory. To address this, we explore the use of Transformers and Structured State Space models (S4) for their superior ability to manage long-range dependencies automatically. We introduce S4WM, a novel world model that utilizes the parallelizable capabilities of S4 and its variants for efficient high-dimensional sequence generation through latent imagination. Our comparative analysis across several tailored environments assesses key memory functionalities of world models, including long-term imagination and reward prediction. The results underscore S4WM's advancements over Transformer-based models in long-term memory retention and training efficiency, marking a significant step forward in the development of policies for more capable MBRL agents. This elucidates the influence-based comparison between the models, emphasizing the agent's ability to retain critical information over extended periods, thereby impacting observability and ensuring that crucial observable components in the environment are considered.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Miguel_Suau1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=SthlUe5xDP",
  "title": "Topological Parallax: A Geometric Specification for Deep Perception Models",
  "modified_abstract": "Building upon recent understandings in the evolution of linear regions within deep reinforcement learning models, which elucidated the complex interplay between input space partitioning and policy region densities, our research introduces _topological parallax_. This concept serves as a theoretical and computational tool, aiming to compare the geometric structures of a trained model and a reference dataset to ensure their similarity across multiple scales. Such geometric congruence is posited as crucial for achieving reliable model interpolation and perturbation responses, which are indicative of a model's safety and robustness during training. We further explore how this geometric similarity underpins the obscure relationship between 'overfitting' and 'generalization' within the broad spectrum of deep learning applications. Where direct geometric description of deep learning models remains arduous, topological parallax employs topological data analysis (TDA) to infer model topology\u2014such as components, cycles, voids\u2014through the Rips complex, based on geodesic distortions relative to the reference dataset. Hence, it serves as a gauge for determining model-dataset geometric fidelity. Our analysis, supported by theoretical proofs and illustrative examples, conjectures topological parallax's potential in enhancing current debates on model evaluation metrics, offering a bi-filtered persistence module that retains stability against perturbations of the reference dataset. The research trajectory suggests that topological parallax may inform the development of policies that guide the growth of deep learning models in their training phases, ensuring that they achieve the desired level of robustness and generalization.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Setareh_Cohan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=9cF6RUwMe7",
  "title": "Learning Space-Time Continuous Latent Neural PDEs from Partially Observed States",
  "modified_abstract": "This work is situated within the burgeoning field of learning from complex and irregular spatiotemporal data, inspired by recent advancements in neural modeling of stochastic partial differential equations (SPDEs) for capturing continuous dynamics under randomness. We introduce a novel grid-independent, resolution-invariant model for learning partial differential equations (PDEs) from noisy and partial observations on irregular spatiotemporal grids. Building on the foundational concepts introduced in the neural stochastic PDE model, which extends the capabilities of physics-inspired neural architectures to handle spatiotemporal dynamics at arbitrary resolutions, our approach innovates further by proposing a space-time continuous latent neural PDE model. This model, featuring an efficient probabilistic framework and a novel encoder architecture for improved data efficiency and grid independence, incorporates advances in neural architecture critical for handling complex dynamics. The latent state dynamics are governed by a semilinear PDE model that combines the collocation method and the method of lines, underlining the significant role solvers and operators play in our framework. Amortized variational inference is employed for approximate posterior estimation, and a multiple shooting technique is utilized for enhanced training speed and stability. Our model demonstrates state-of-the-art performance on complex synthetic and real-world datasets, overcoming limitations of previous approaches and effectively handling partially-observed data. It outperforms recent methods, highlighting its potential to advance data-driven PDE modeling and enabling robust, grid-independent modeling of complex partially-observed dynamic processes across various domains.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Maud_Lemercier1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=LVHEcVgEGm",
  "title": "Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels",
  "modified_abstract": "Inspired by the recent success in leveraging contrastive learning for robust generalization and addressing imbalanced datasets, this work propose a novel approach to enhance semi-supervised learning through the integration of diffusion models. Our method, termed *dual pseudo training* (DPT), synergizes the strengths of diffusion models and advanced semi-supervised learning techniques, including transformers, to set new benchmarks in the field. DPT is executed in three phases: initially training a classifier on a subset of labeled data to infer pseudo-labels; subsequently using these pseudo-labels to guide a conditional diffusion model in generating synthetic images; and finally retraining the classifier on a combined dataset of real and synthetic images to improve both generative and discriminative performance, effectively rebalancing the training process. Empirical evaluations reveal that DPT significantly outperforms existing semi-supervised methods, achieving state-of-the-art (SOTA) results in both semi-supervised generation and classification tasks across diverse experimental setups. Notably, DPT attains Fr\u00e9chet Inception Distance (FID) scores of 3.08 and 2.52 on ImageNet $256\\times256$ with extremely limited labeling (one or two labels per class), showcasing the efficacy of generative augmentation in semi-supervised settings for vision tasks. Moreover, it surpasses competitive semi-supervised baselines on ImageNet classification, achieving top-1 accuracies of 59.0 (+2.8), 69.5 (+3.0), and 74.4 (+2.0) with one, two, or five labels per class, respectively. Our findings underscore the potential of leveraging diffusion processes and transformation-based strategies in semi-supervised learning contexts, especially under label scarcity, thus opening new avenues for efficient training of models with minimal supervision.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jiequan_Cui1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=v9yC7sSXf3",
  "title": "Deep Neural Collapse Is Provably Optimal for the Deep Unconstrained Features Model",
  "modified_abstract": "In light of discoveries in the study of overparametrized high-dimensional models, particularly the challenges of uncertainty quantification and the phenomenon of double descent, our research ventures into the domain of neural collapse (NC) with a focus on its manifestation in earlier layers of neural networks, termed deep neural collapse (DNC). These explorations have paved the way for a deeper understanding of the structural dynamics within deep neural networks during the terminal phase of gradient descent training. Neural collapse refers to the surprising structure of the last layer of deep neural networks, with recent experimental evidence suggesting the propagation of this phenomenon to earlier layers. However, existing theoretical work on NC largely focuses on linear layers or the last two layers, often requiring additional assumptions about calibration and classifiers' overconfidence. Our study addresses this gap by extending the analytical framework established for NC - the unconstrained features model - to encompass multiple non-linear layers, illustrating that the unique global optimum for binary classification indeed exhibits properties typical of DNC. This theoretical advancement elucidates the previously observed experimental evidence of DNC and is further corroborated by our empirical findings: (i) gradient descent optimization of deep unconstrained features models aligns well with our theory, underscoring the importance of accurate uncertainty estimators in model calibration, and (ii) trained networks recover unconstrained features conducive to DNC, thereby affirming the validity of this model. Our investigation not only substantiates the ubiquitous occurrence of DNC in deep neural networks but also enriches the discourse on the optimality of neural configurations across multiple layers.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Lucas_Clart\u00e91",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=hoyL1Ypjoo",
  "title": "Macro Placement by Wire-Mask-Guided Black-Box Optimization",
  "modified_abstract": "The advancement of very large-scale integration (VLSI) technology has brought forth new challenges in electronic design automation (EDA), underscoring the necessity for innovative approaches in chip floorplanning. This necessity echoes the broader call for optimization in machine learning and computational problems, such as efficient sparse regression in sequential decision-making as exemplified by the PopArt method's novel approach to sparse linear estimation. In response to these challenges, we introduce WireMask-BBO, a new black-box optimization (BBO) framework for macro placement in VLSI design. WireMask-BBO employs a wire-mask-guided greedy procedure for objective evaluation, marking a significant departure from conventional packing-based, analytical, reinforcement learning methods, and integrating design principles with computational efficiency. The proposed framework not only demonstrates superior performance in minimizing half-perimeter wirelength (HPWL) and avoiding macro overlapping but also offers a novel approach to fine-tuning existing design placements, resulting in up to 50% improvement in HPWL. Our findings indicate that WireMask-BBO, reinforced by agent-based decision-making mechanisms, can substantially enhance the quality and efficiency of chip floorplanning, potentially benefiting both academic and industrial efforts in EDA. The promising results also underscore the viability of applying BBO strategies and bandit algorithms to complex design and optimization challenges in the field, incorporating covariates and estimation processes to refine the accuracy of decision-making. The code for this framework is made available, contributing to the ongoing development and application of BBO methodologies in VLSI design optimization.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kyoungseok_Jang1",
  "manipulated_ranking": 13,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=SE73LzWNjr",
  "title": "Nearly Optimal VC-Dimension and Pseudo-Dimension Bounds for Deep Neural Network Derivatives",
  "modified_abstract": "In the context of a growing body of research on leveraging the advanced capabilities of machine learning models for comprehending complex systems, such as the study on learning elliptic partial differential equations using randomized linear algebra, this paper propels the discourse into the theoretical underpinnings of deep neural networks (DNNs). Specifically, it addresses the problem of nearly optimal Vapnik--Chervonenkis dimension (VC-dimension) and pseudo-dimension estimations of the derivative functions of DNNs using randomized pairs and singular value decomposition techniques. The proliferation of physics-informed machine learning models, exemplified by groundbreaking work in solving partial differential equations and operator learning, underscores the necessity for robust theoretical frameworks involving randomized techniques and low-rank approximations. Our work contributes to this burgeoning field by: 1) Establishing a nearly tight approximation result of DNNs in the Sobolev space, which is pivotal for understanding the effectiveness of training deep learning models; and 2) Characterizing the generalization error of machine learning methods with loss functions involving function derivatives. This theoretical investigation not only provides critical insights into learning error estimations for a wide range of physics-informed machine learning models, including those involving generative models, solving partial differential equations, operator learning, network compression, distillation, regularization, and elliptic equations, but also enhances the foundation upon which future applications can be built, acknowledging the complexity scaling as denoted by \\\\(^4(1/\\\\epsilon)\\\\).",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alex_Townsend1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=X0CIxqYc4Z",
  "title": "Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning",
  "modified_abstract": "Inspired by the progress in handling uncertainty within constrained environments, such as those detailed in studies on constrained Markov decision processes and upper confidence reinforcement learning, this work extends the scope of safety and robustness in decision making under uncertainty to the domain of deep reinforcement learning. Many real-world domains require safe decision making in uncertain environments. In this work, we introduce a deep reinforcement learning framework for approaching this important problem. We consider a distribution over transition models and apply a risk-averse perspective towards model uncertainty through the use of coherent distortion risk measures, enhancing exploration strategies with a policy that explicitly accounts for uncertainty. We provide robustness guarantees for this framework by showing it is equivalent to a specific class of distributionally robust safe reinforcement learning problems. Unlike existing approaches to robustness in deep reinforcement learning, however, our formulation does not involve minimax optimization but rather leverages the concept of regret minimization and kernel methods for an effective learning algorithm. This leads to an efficient, model-free implementation of our approach that only requires standard data collection from a single training environment and maintains the upper confidence bound approach. In experiments on continuous control tasks with safety constraints, we demonstrate that our framework produces robust performance and safety at deployment time across a range of perturbed test environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Liyuan_Zheng1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=hn1oJO7lg6",
  "title": "Computing Approximate $\\ell_p$ Sensitivities",
  "modified_abstract": "Inspired by the recent progress in dimensionality reduction, variable importance estimation, and uncertainty quantification in regression tasks, this paper extends the notion of sensitivity within the machine learning domain, traditionally confined to the $\\ell_2$ space as leverage scores, to $\\ell_p$ norms. Sensitivity quantifies the significance of individual datapoints in a dataset, offering insights into data's intrinsic dimensionality and impacting approximation quality when subsampling low-sensitivity points. Variable selection emerges as a critical application of sensitivity analysis, guiding the choice of relevant features for ensembles of models in machine learning. We advance the state of the art by introducing the first efficient algorithms for approximating $\\ell_p$ sensitivities and other summary statistics of a given matrix. For an $n \\times d$ matrix, our approach computes $\\alpha$-approximation of $\\ell_1$ sensitivities with $n/\\alpha$ computations and estimates total $\\ell_p$ sensitivity using importance sampling of $\\ell_p$ Lewis weights, significantly reducing computational overhead. Additionally, we provide a method for approximating the maximum $\\ell_1$ sensitivity within a $\\sqrt{d}$ factor using $O(d)$ computations and extend these techniques to various $\\ell_p$ norms, including a mention of the kernel trick\u2019s role in facilitating these computations. Experimental evaluations on structured matrices from real-world datasets highlight that our proposed method efficiently estimates total sensitivity, revealing lower than expected intrinsic dimensionality\u2014a finding that aligns with contemporary research emphasizing the importance of understanding dataset characteristics in computational tasks, particularly in neural network applications. This underscores the potential impact of our findings not only in domain-specific applications such as healthcare but also in broader machine learning processes where uncertainty quantification is critical.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Rajarshi_Mukherjee1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=75Mxzfoeq7",
  "title": "No-Regret Learning in Dynamic Competition with Reference Effects Under Logit Demand",
  "modified_abstract": "Inspired by the extensive exploration of regret minimization in the context of repeated games and strategic interactions, as evidenced by pivotal works on swap regret and its implications for correlated equilibria, our study advances the discourse by focusing on algorithm design within a competitive market framework. The objective of this work is to facilitate the learning of a stable equilibrium in dynamic price competition between two firms under conditions of opacity, where each firm lacks comprehensive information about its competitor. We model consumer choices based on the multinomial logit (MNL) model, which incorporates observed price and reference price effects, and establish connections between consecutive periods through reference price updates. We employ the concept of stationary Nash equilibrium (SNE) as a cornerstone to assess the long-run market equilibrium and stability. The online projected gradient ascent algorithm (OPGA) we propose enables firms to adjust prices by leveraging the first-order derivatives of their log-revenues, derived from market feedback. Despite the absence of properties like strong monotonicity and variational stability, traditionally required for convergence in online games, we prove that, with diminishing step-sizes, the price and reference paths generated by OPGA converge to the unique SNE, achieving no-regret learning and market stability. Furthermore, we establish that this convergence reaches a rate of \\(\\mathcal{O}(1/t)\\) under suitably chosen step-sizes, embodying a significant reduction in uncertainty. This approach contributes significantly to the theory of dynamic competition and exemplifies the reduction of uncertainty in strategic interactions through algorithmic innovations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shinji_Ito1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=AGVBqJuL0T",
  "title": "Fantastic Robustness Measures: The Secrets of Robust Generalization",
  "modified_abstract": "This study extends the current understanding of adversarial robustness by integrating insights from novel data augmentation methods using out-of-distribution (OOD) data, and in some scenarios, by including unlabeled-in-distribution data, which have shown promise in improving generalization for both standard and adversarial training contexts. Adversarial training has become the de-facto standard method for improving the robustness of models against adversarial examples. However, robust overfitting remains a significant challenge, leading to a large gap between the robustness on the training and test datasets. To understand and improve robust generalization, various measures have been developed, including margin, smoothness, and flatness-based measures. In this study, we present a large-scale analysis of robust generalization to empirically verify whether the relationship between these measures and robust generalization remains valid in diverse settings. We demonstrate when and how these measures effectively capture the robust generalization gap by comparing over 1,300 models trained on CIFAR-10 under the $L_\\infty$ norm and further validate our findings through an evaluation of more than 100 models from RobustBench across CIFAR-10, CIFAR-100, and ImageNet. By connecting the dots between different methods to improve generalization and robustness, including the leverage of OOD data and augmentation techniques, we hope to offer the community a deeper understanding of the mechanisms underpinning adversarial robustness and inspire the development of more effective defense methods against adversarial attacks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Changhwa_Park1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=mZ3hnyL9bS",
  "title": "Towards the Difficulty for a Deep Neural Network to Learn Concepts of Different Complexities",
  "modified_abstract": "Building on insights from cutting-edge research in graph neural networks and their operational mechanisms, particularly focusing on the transformation and interaction of complex patterns and structures as seen in Automorphism-based graph neural networks (Autobahn), this paper seeks to elucidate the underlying reasons why deep neural networks (DNNs) are more adept at learning simpler concepts compared to more complex ones. It leverages the foundational understanding that DNNs, with their diverse architectures, typically encode a limited array of interactive concepts, using the interplay among these to make inference decisions through algorithms that often include convolutions and message passing methods. This study extends this paradigm by positing that concepts necessitating a broader collaboration of input variables, thus more complex in nature, and resembling molecular complexity in some instances, present greater challenges in the learning process for DNNs, especially when considering the role of automorphism in identifying symmetrical patterns within data and across graphs. Our conclusions offer a detailed exposition on the relationship between conceptual complexity in the neural network domain and the inherent learning difficulties encountered, grounding these insights within the theoretical framework that elucidates the nature of complexity in DNN learning processes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Erik_Henning_Thiede1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=wPqEvmwFEh",
  "title": "Small batch deep reinforcement learning",
  "modified_abstract": "Guided by insights into the data efficiency challenges faced by reinforcement learning agents in real-world scenarios, as demonstrated in contemporary studies, this paper embarks on an exploration of the batch size parameter's role in value-based deep reinforcement learning with replay memories. Here, the conventional practice leans towards unadjusted, often larger batch sizes for training neural networks, aiming for enhanced performance. Contrary to this norm, our comprehensive empirical study uncovers that smaller batch sizes, under the proper guidance, may not only challenge this general tendency but also unlock significant performance improvements in deep reinforcement learning frameworks when interacting with complex environments. The findings are supported by a series of empirical analyses aimed at elucidating the underpinnings of this phenomenon, thereby contributing to the broader discourse on optimizing learning processes within artificial intelligence and machine learning fields, with a focus on the representation of data and decision-making strategies for agents.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Vikranth_Dwaracherla1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=KXbAgvLi2l",
  "title": "Faster Relative Entropy Coding with Greedy Rejection Coding",
  "modified_abstract": "Building upon the foundations laid by prior works in probabilistic inference and optimization, such as Stochastic Multiple Target Sampling Gradient Descent, which creatively applies sampling techniques to approximate complex distributions for multi-objective optimization, our study introduces Greedy Rejection Coding (GRC). This novel approach to relative entropy coding (REC) algorithms aims to optimize the encoding of samples from a target distribution $Q$ using a proposal distribution $P$ while minimizing bit usage. Unlike traditional entropy coding methods, REC facilitates encoding without necessitating quantization of discrete distributions, thereby seamlessly integrating into critical applications including learnt compression and differentially private federated learning. However, the broader adoption of REC algorithms has been hindered by their slow computational performance and restrictive assumptions. Addressing these limitations, GRC extends the rejection sampling-based algorithm by Harsha et al. (2007) to support arbitrary probability spaces and partitioning schemes. We demonstrate the almost sure termination of GRC and its ability to produce unbiased samples from $Q$. Our analysis focuses on two specific variants of GRC: GRCS and GRCD, revealing that for continuous distributions $Q$ and $P$ over $\\mathbb{R}$ with unimodal $dQ/dP$, GRCS exhibits an expected runtime upper bounded by $\\beta D_{KL}(Q||P) + \\mathcal{O}(1)$, where $\\beta \\approx 4.82$, and ensures optimal expected codelength. This marks GRCS as the first REC algorithm with a run-time efficiency guarantee for this category of distributions, significantly improving upon the previous algorithm, A* coding. Similarly, our experimental results hint at GRCD's potential, with its run-time and codelength likely capped by $D_{KL}(Q||P) + \\mathcal{O}(1)$. Lastly, the application of GRC in a compression pipeline utilizing variational autoencoders on the MNIST dataset showcases enhanced compression efficiency, further solidified by a modified training objective and a novel codelength-compression technique. The multi-task capability of GRC, due to its ability to concurrently optimize for various distributions showcases its potential in enhancing the performance of algorithms across a range of domains.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hoang_Viet_Phan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=NaYAsbv2jF",
  "title": "Geometric Neural Diffusion Processes",
  "modified_abstract": "The exploration of denoising diffusion models, particularly their extension to infinite dimensional Euclidean spaces, highlights a pivotal shift towards more sophisticated generative modelling capable of capturing the complex nature of stochastic processes. The recent advancements, such as the introduction of Laplace inference in neural additive models for enhanced interpretability and reliability in feature interaction quantification, set the stage for deeper inquiry into models that operate beyond conventional Euclidean domains. Leveraging these insights, our work expands the scope of diffusion models to align with complex natural science problems where data inherently resides on non-Euclidean geometries. Specifically, we introduce a methodological innovation by integrating geometric priors in infinite-dimensional modeling. This is achieved through a) the construction of a noising process converging to a geometric Gaussian process respecting the symmetry group of interest, and b) the development of an equivariant neural network approximation of the score relative to this group, effectively managing noise and ensuring efficient regression in tasks with intricate interactions. Our findings illustrate that this tailored approach ensures the generative functional model inherits the desired symmetries, marking a significant step forward in the generative model landscape. The model's efficacy is demonstrated through its ability to adeptly handle complex scalar and vector fields across both Euclidean and spherical codomains, showcasing versatility across synthetic and empirical weather datasets. This work not only underlines the importance of engineering noise models with precision but also highlights the critical role of interpreting and managing interactions within diffusion processes. Furthermore, our approach affords considerable implications for classification and predictions in various tasks, underpinning the importance of additive models in constructing predictive frameworks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kouroche_Bouchiat1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=7b4oobeB4w",
  "title": "Bias in Evaluation Processes: An Optimization-Based Model",
  "modified_abstract": "This work builds upon foundational models of delegation and decision-making under uncertainty, examining biases in evaluation processes through an optimization-based lens. By integrating insights from multi-agent delegation mechanisms and leveraging the understanding of principals' utility optimization under information constraints, our model provides a novel perspective on biases in socially-sensitive settings such as admissions and hiring. The model we propose views the evaluation process as a transformation of a distribution of the true utility of an individual for a task to an observed distribution, modeled as a solution to a loss minimization problem subject to an information constraint. With the resource-information trade-off parameter in the information constraint and the risk-averseness parameter in the loss function identified as critical to bias formation, we characterize the resulting distributions and elucidate the parameters' impact. Our findings, validated through empirical analysis on real-world datasets, not only deepen the understanding of bias in evaluation processes but also offer actionable guidance on interventions to mitigate such biases. The comprehensive analysis covers both single-agent and multi-agent settings, analyzes several subsets of the complete/incomplete information scenarios, and presents compelling evidence supporting our theoretical claims. Furthermore, we examine the approximation of the underlying complex model and highlight the importance of processing information privately in special settings, suggesting strategic directions for minimizing bias.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Suho_Shin1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=1q0feiJ2i4",
  "title": "Large Language Models are Visual Reasoning Coordinators",
  "modified_abstract": "Inspired by recent breakthroughs in leveraging multimodal neural script knowledge models like MERLOT for understanding dynamic visual scenes and events in a label-free, self-supervised manner, our research presents a novel approach to visual reasoning with large language models (LLMs). Visual reasoning requires multimodal perception and commonsense cognition of the world, integrating complex representations of video, speech, and textual data. Recently, multiple vision-language models (VLMs) have been proposed with excellent commonsense reasoning ability in various domains, including dynamic events captured in video content. However, the potential for harnessing the collective power of these complementary VLMs is rarely explored, with existing methods like ensemble approaches struggling to aggregate these models for higher-order communications effectively. In this work, we propose Cola, a novel paradigm that coordinates multiple VLMs for visual reasoning. Our key insight is that a LLM can efficiently coordinate multiple VLMs by facilitating natural language communication that leverages their distinct and complementary capabilities, acting as a versatile mediator that interprets and acts upon script-based and modal informations. Extensive experiments demonstrate that our instruction tuning variant, Cola-FT, achieves state-of-the-art performance on visual question answering (VQA), outside knowledge VQA, visual entailment, and visual spatial reasoning tasks. Moreover, we show that our in-context learning variant, Cola-Zero, exhibits competitive performance in zero and few-shot settings, without finetuning. Through systematic ablation studies and visualizations, we validate that a coordinator LLM indeed comprehends the instruction prompts as well as the separate functionalities of VLMs; it then coordinates them to enable impressive visual reasoning capabilities.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jack_Hessel1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=BRpi8YAfac",
  "title": "Passive learning of active causal strategies in agents and language models",
  "modified_abstract": "In the context of rapidly evolving artificial intelligence, particularly in natural language processing (NLP) showcased by significant developments in latent structure models, our work explores a pivotal question: What can be learned about causality and experimentation from passive data? Echoing the themes from preceding contributions in the field that have pushed the boundaries of passive learning towards more interactive and intervention-based understandings, this study extends the discourse by examining how passive learning can, contrary to traditional beliefs, enable an agent to deduce and apply causal strategies effectively through structural modeling. We provide a formal framework illustrating that a strategy encompassing initial experimentation followed by goal-oriented actions can facilitate generalization from passive observations in principle. Empirical evidence further supports that agents, when trained through imitation on expert demonstrations, can generalize at test time to identify and utilize causal relationships not present in the training set and extend experimentation strategies to novel variables unseen during training. Additionally, our findings reveal that the integration of natural language explanations enhances the ability of passive learners to generalize causal intervention and exploitation strategies in complex, high-dimensional environments, and overcome the challenges posed by perfectly-confounded training data. Furthermore, we demonstrate that language models, trained solely on passive next-word prediction tasks, can extrapolate causal intervention strategies from minimally supervised prompts augmented with explanations, reasoning, and sentiment analysis, thus showcasing the machine's unexpected efficacy in acquiring active causal strategies and offering insights into the capabilities and behavioral understanding of language models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tsvetomila_Mihaylova1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=PaSpImjKm2",
  "title": "Performance Bounds for Policy-Based Average Reward Reinforcement Learning Algorithms",
  "modified_abstract": "Building on the landscape of current research, which includes novel methods for overcoming challenges in reinforcement learning (RL) such as Reward Shaping (RS) to address sparse or uninformative rewards, our study introduces significant advancements in policy-based RL algorithms. We focus on the domain where the average reward criterion is paramount, addressing the limitations of existing approaches that struggle with scalability and meaningful performance measurement over long horizons. The core of our investigation revolves around the development of the first non-trivial finite time error bounds for average-reward Markov Decision Processes (MDPs). These bounds remain meaningful and converge to zero as the errors in policy evaluation function and policy improvement phases diminish. This work not only addresses a previously unsolved problem in the reinforcement learning community but also sets a foundation for future research to build upon, particularly in the realm of policy-based average reward RL algorithms where precise and scalable performance metrics are crucial. Our contributions push the state-of-the-art by providing a measurable framework in which autonomous agents can learn and assess their performance in tasks within environments characterized by sparse rewards, thus our method of shaping-reward strategies endorse our theoretical findings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Taher_Jafferjee1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=dyXNh5HLq3",
  "title": "Compositional Foundation Models for Hierarchical Planning",
  "modified_abstract": "Advancements in hierarchical reasoning and modular learning frameworks have set the stage for innovative approaches to long-horizon decision-making in novel environments. Building on the cornerstone findings of previous works that have developed hierarchical systems for instruction following in complex tasks, we introduce the concept of Compositional Foundation Models for Hierarchical Planning (HiP). Our model integrates the strengths of multiple expert foundation models\u2014trained individually on language, vision, and action datasets\u2014into a cohesive system for solving long-horizon tasks. By combining a large language model for constructing symbolic plans, a large video diffusion model for visually reasoning about these plans, and an inverse dynamics model for translating generated videos into visual-motor control sequences, HiP facilitates effective hierarchical reasoning across spatial and temporal scales. This integration ensures consistency across the models through iterative refinement, enhancing the model's adaptability and its composition. The proposed model's efficacy is demonstrated through its performance on diverse long-horizon table-top manipulation tasks, showcasing its potential to leverage compositional foundations for complex, hierarchical problem-solving and setting new benchmarks in the field. By using HiP as an agent in these scenarios, we underscore its capability as a hierarchical controller, evaluating it against state-of-the-art benchmarks in table-top manipulation, convincingly demonstrating its superior performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Byeonghwi_Kim1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=eP6cDDwBNC",
  "title": "TRIAGE: Characterizing and auditing training data for improved regression",
  "modified_abstract": "Amidst the burgeoning focus on data-centric artificial intelligence, the significance of data quality in developing robust machine learning algorithms cannot be overstated. While recent efforts have predominantly concentrated on the classification domain, rendering regression tasks somewhat underexplored, our research endeavors to bridge this gap. Inspired by pioneering techniques such as those highlighted in 'Decay No More,' which sheds light on the crucial role of weight decay in tuning large-scale models, we present TRIAGE. This novel data characterization framework, which is a new version in the lineage of machine learning toolkits, is specifically designed for regression tasks and is compatible with a wide array of regressors. By leveraging conformal predictive distributions, TRIAGE introduces a model-agnostic scoring method\u2014the TRIAGE score\u2014that operationalizes the analysis of individual training samples' dynamics, categorizing them as under-, over-, or well-estimated by the model. Our findings demonstrate the consistency of TRIAGE's characterization across multiple regression settings and underscore its potential in enhancing performance through data sculpting/filtering and optimization. Furthermore, TRIAGE pioneers new paradigms in dataset selection and feature acquisition, beyond mere sample-level analysis. This framework underscores the untapped value that sophisticated data characterization holds for real-world regression applications, marking a significant step forward in the pursuit of data-centric machine learning excellence, where learning dynamics and the decay phenomenon play a critical role.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Fabian_Schaipp1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=gdVcFOvxT3",
  "title": "Finding Safe Zones of Markov Decision Processes Policies",
  "modified_abstract": "Inspired by recent advancements in online planning for Partially Observable Markov Decision Processes (POMDPs) and the development of self-improving simulators, this study introduces the concept of SafeZones within the framework of Markov Decision Processes (MDPs). A SafeZone is defined as a subset of states in which most of the policy's trajectories are confined, characterized by a small number of states and a low escape probability. This research addresses the computational challenges inherent in identifying optimal SafeZones by offering a bi-criteria approximation learning algorithm, designed to plan and adaptively learn in these environments. We demonstrate that finding an ideal SafeZone is computationally hard, thus, our focus shifts towards developing approximate solutions. We propose an algorithm that achieves a near 2-factor approximation for both the escape probability and the size of the SafeZone, utilizing a polynomial size sample complexity. Our approach enhances the understanding of SafeZones and contributes to safer and more efficient policy learn design in MDP environments, with an emphasis on the integration of planning strategies and adaptive learning techniques across various domains.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jinke_He1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=NBMIsOS6B7",
  "title": "Alternation makes the adversary weaker in two-player games",
  "modified_abstract": "Our work is inspired by critical insights from the study of multi-agent bandit problems, specifically the analysis of the pitfalls in the natural extensions of Upper Confidence Bound (UCB) algorithms to multi-agent contexts, highlighting the complexity and unintended consequences of agents' interaction and information sharing in group settings. This complexity often stems from the inherent explore-exploit dilemma that each agent faces, requiring a delicate balance between exploring new actions based on sampling techniques and exploiting known actions to maximize rewards. Motivated by alternating game-play in two-player games, a scenario often encountered in bandit literature as well, we extend these insights to study an alternating variant of the \\textit{Online Linear Optimization} (OLO). In alternating OLO, a \\textit{learner} at each round $t \\in [n]$ selects a vector $x^t$ and then an \\textit{adversary} selects a cost-vector $c^t \\in [-1,1]^n$, engaging in a dynamic that closely resembles a bandit problem. Leveraging algorithms that might be seen as a form of complex multi-agent sampling, the learner then experiences cost $(c^t + c^{t-1})^\\top x^t$ instead of $(c^t)^\\top x^t$ as in standard OLO. We establish that under this small twist, the $\\Omega(\\sqrt{T})$ lower bound on the regret is no longer valid. More precisely, we present two online learning algorithms for alternating OLO that respectively admit $\\mathcal{O}((\\log n)^{4/3} T^{1/3})$ regret for the $n$-dimensional simplex and $\\mathcal{O}(\\rho \\log T)$ regret for the ball of radius $\\rho>0$. Our results imply that in alternating game-play, an agent can always guarantee $\\mathcal{\\tilde{O}}((\\log n)^{4/3} T^{1/3})$ regret, regardless of the strategies of the other agent, while the regret bound improves to $\\mathcal{O}(\\log T)$ in case the agent admits only two actions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Udari_Madhushani1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=FskZtRvMJI",
  "title": "RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value Factorization",
  "modified_abstract": "Inspired by the complexities uncovered in the study of reinforcement learning within linear Markov Decision Processes (MDPs), particularly concerning the representation of state-action value functions and their impact on achieving optimal outcomes through algorithms designed for regret minimization, our research extends these concepts into the domain of Multi-Agent Reinforcement Learning (MARL). Multi-agent systems are characterized by environmental uncertainty, varying policies of agents, classes of environmental dynamics, and partial observability, which result in significant risks and introduce complex problems to the field. In the context of MARL, learning coordinated and decentralized policies that are sensitive to risk is challenging due to these problems. To formulate the coordination requirements in risk-sensitive MARL and approach minimization of regret in these complex scenarios, we introduce the Risk-sensitive Individual-Global-Max (RIGM) principle as a generalization of the Individual-Global-Max (IGM) and Distributional IGM (DIGM) principles. This principle requires that the collection of risk-sensitive action selections of each agent should be equivalent to the risk-sensitive action selection of the central policy. Moreover, the representations used for courses of action and their consequences must accurately capture the underlying dynamics and uncertainties. Current MARL value factorization methods do not satisfy the RIGM principle for common risk metrics such as the Value at Risk (VaR) metric or distorted risk measurements. Therefore, we propose RiskQ to address this limitation, which models the joint return distribution by modeling quantiles of it as weighted quantile mixtures of per-agent return distribution utilities. RiskQ satisfies the RIGM principle for the VaR and distorted risk metrics. We show that RiskQ can obtain promising performance through extensive experiments. The source code of RiskQ is publicly available.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Matteo_Papini1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=6fuZs3ibGA",
  "title": "Optimal and Fair Encouragement Policy Evaluation and Learning",
  "modified_abstract": "Our study extends the discourse on fairness in machine learning (ML), drawing inspiration from recent developments such as the concept of superhuman fairness, which reconceptualizes the balance between predictive performance and fairness metrics from an imitation learning perspective. In consequential domains where compelling individuals to adhere to treatment recommendations is unfeasible, identifying optimal policy rules that are effectively mere suggestions presents a unique challenge. This challenge is compounded by heterogeneity in both who responds to treatment and in the efficacy of the treatment itself, especially evident in social services where there exists a significant gap in the take-up of beneficial services among potentially high-benefit populations. Furthermore, when decision-makers possess distributional preferences over both access and average outcomes, the optimal decision rule differs. We explore the identification, doubly-robust estimation, and robust estimation under potential violations of positivity, integrating fairness constraints like demographic parity in treatment uptake and other constraints through constrained optimization. Our framework is adaptable to algorithmic recommendations under a covariate-conditional exclusion restriction and incorporates robustness checks for lack of positivity in the recommendation. We introduce a two-stage, online learning-based algorithm for solving over-parametrized policy classes under general constraints that achieves variance-sensitive regret bounds. This research provides insights into improving recommendation rules, using a case study on optimizing the recommendation of supervised release in the PSA-DMF pretrial risk-assessment tool while aiming to reduce surveillance disparities.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Omid_Memarrast1",
  "manipulated_ranking": 10,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=qHrADgAdYu",
  "title": "Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective",
  "modified_abstract": "Inspired by foundational work on the interpretability and explainability of machine learning models through symbolic languages, this paper ventures into elucidating the inner workings and efficacy of Chain-of-Thought prompting (CoT) in Large Language Models (LLMs). The remarkable empirical success of CoT in significantly enhancing the performance of LLMs for complex tasks, especially those requiring mathematical or logic-based reasoning, has spurred a curiosity about the theoretical underpinnings that enable this improvement. In an effort to demystify the mechanisms by which CoT facilitates the processing and problem-solving capabilities of LLMs, we embark on a theoretical examination of the expressivity of these models when applied to fundamental mathematical and decision-making problems. Employing circuit complexity theory, we initially present impossibility results that demonstrate the limitations of bounded-depth Transformers in directly generating accurate responses for basic arithmetic or equation-solving tasks without an exponential increase in model size relative to input length. Conversely, we prove that autoregressive Transformers of constant size can effectively solve such tasks through CoT derivations in a standard mathematical language format, thereby illustrating the transformative potential of CoT in enhancing model capabilities. Moreover, we extend our analysis to a broad class of decision-making problems encapsulated by Dynamic Programming, showcasing LLMs with CoT's adeptness at navigating intricate trees and classification tasks that resemble real-world challenges. Through extensive experimentation and evaluation, and the implementation of queries posed to validate, we further prove that while Transformers are inherently challenged in directly deducing correct answers, they exhibit a remarkable aptitude for learning to methodically unravel solutions step-by-step with adequate CoT exemplars, thus reinforcing the theoretical justifications for the observed empirical successes of CoT prompting in LLMs. This approach not only improves the explainability of LLM decisions but also provides a declarative understanding of their problem-solving methods.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Marcelo_Arenas1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=3GpIeVYw8X",
  "title": "The Pursuit of Human Labeling: A New Perspective on Unsupervised Learning",
  "modified_abstract": "Motivated by the significant contributions and insights from previous work on contrastive learning and data augmentation techniques, our study introduces HUME, a novel framework aimed at innovating unsupervised learning by inferring human-like labeling of datasets without external supervision. Recognizing the complexities and limitations of existing approaches in adapting to datasets with nuanced or irrelevant features, such as those encountered in medical imaging\u2014a prime example where brain structural differences can be subtle\u2014our research proposes a model-agnostic solution. The foundation of HUME is the observation that classes delineated by human labels are inherently linearly separable across different representation spaces, empowering our method to explore possible labelings of a dataset to uncover the most humanaligned classification. This exploration is facilitated by embedding datasets into space where contrastive samples can be more effectively distinguished. We demonstrate that our optimization objective, based on a robust kernel method, aligns closely with actual dataset labels through the application of linear classifiers atop fixed pretrained representations, ensuring compatibility with any large pretrained, self-supervised model. Despite its straightforward methodology, HUME surpasses supervised linear classifiers on self-supervised representations on the STL-10 dataset significantly and delivers comparable outcomes on CIFAR-10. Against current unsupervised baselines, our framework sets new benchmarks on four leading image classification datasets, including the extensive ImageNet-1000. Our work offers a fresh perspective on unsupervised learning, suggesting a paradigm shift towards leveraging inherent human labeling consistency across varied representation spaces.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Edouard_Duchesnay1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=AiEipk1X0c",
  "title": "A Deep Instance Generative Framework for MILP Solvers Under Limited Data Availability",
  "modified_abstract": "The development of machine learning techniques for addressing combinatorial optimization problems has benefitted significantly from insights drawn from diverse areas such as the application of physarum dynamics in semi-definite programs and time-dependent optimization techniques. Inspired by such groundbreaking approaches that leverage biological phenomena, theoretical models, and approximation strategies to enhance algorithmic solutions in optimization, our work introduces G2MILP, a novel deep generative framework designed specifically for the synthesis of mixed-integer linear programming (MILP) instances. This framework marks a departure from traditional methods that either demand extensive expert knowledge in formulation or fail to accurately embody the complexity of real-world instances. By conceptualizing MILP instances as bipartite graphs and employing a masked variational autoencoder, G2MILP iteratively alters and replenishes parts of the original graphs to produce new instances that are not only novel but retain the structural and computational intricacies inherent to authentic datasets with respect to (w.r.t) their dynamics and computational constraints. This ability ensures that the synthesized instances can effectively support the enhancement of MILP solvers, particularly in scenarios characterized by a scarcity of real-world data. Our evaluation, conducted through a comprehensive benchmarking process, validates that the generated instances closely mimic the attributes of real-world data in relation to structure, computational difficulty, and the dynamics of problem-solving defined by MILP algorithms. The framework and its outcomes are accessible, aimed at facilitating further research and application in the optimization domain.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hamidreza_Kamkari1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=6gcY0MGNhj",
  "title": "Statistical Analysis of Quantum State Learning Process in Quantum Neural Networks",
  "modified_abstract": "Building on recent insights from the optimization and generalization properties of neural networks as captured by quadratic models, this paper explores the statistical dynamics underpinning quantum state learning in quantum neural networks (QNNs). Quantum neural networks have emerged as a promising framework for achieving quantum advantage in various domains, akin to their classical counterparts in machine learning, including those that are large and complex. Quantum state learning, analogous to probability distribution learning in classical settings, is pivotal for the advancement of quantum machine learning. We present a no-go theorem for the process of learning an unknown quantum state using QNNs that establishes fundamental limits on this endeavor. Specifically, we demonstrate that below a critical loss threshold, the likelihood of circumventing local minima decreases exponentially with the increase in qubit count and only grows polynomially with circuit depth, which poses significant challenges for shallow networks. Additionally, we show that the curvature at local minima is closely tied to the quantum Fisher information, adjusted by a loss-dependent constant. This relationship highlights the role of QNN parameters in the sensitivity of the output state and is validated through extensive numerical simulations. Our results exhibit critical insights into the constraints on initializing QNNs effectively and adapting learning strategies to enhance their performance by highlighting the need for a balance between network depth and breadth. By integrating perspectives from classical neural network analysis, particularly the quadratic modeling and generalization approach, with quantum machine learning, we offer a comprehensive understanding of the challenges and potentials in quantum state learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Libin_Zhu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=hyPUZX03Ks",
  "title": "A polar prediction model for learning to represent visual transformations",
  "modified_abstract": "Inspired by recent advancements in artificial neural networks, specifically the exploration of fast weights to enhance temporal attention and memory in sequence-to-sequence models, this work introduces a novel self-supervised representation-learning framework aimed at understanding visual transformations. Recognizing the critical role of temporal predictions for the evolutionary fitness of organisms and their relevance in vision perception, where the dynamics of sensory signals are structured by the motion of observers and objects, we propose a model that leverages the regularities in natural videos for accurate future signal predictions. Our research is particularly motivated by the Fourier shift theorem and its group-theoretic generalization, which our polar architecture is specifically designed for next-frame prediction optimization. Controlled experiments validate our approach's effectiveness in capturing simple transformation groups within data. When applied to natural video datasets, it outperforms traditional motion compensation techniques and matches the predictive capabilities of conventional deep networks, providing both interpretability and speed. Importantly, the polar computations largely mirror the structure of normalized simple and direction-selective complex cell models found in the primate V1, suggesting a principled method for mimicking how the visual system simplifies temporal prediction. Thus, through this model, we learn to accurately predict future sequences, representing a significant step forward in the pursuit of algorithms that closely align with biological vision processes. The model importantly aids in the retention of memories of visual sequences, highlighting its potential as a foundation for further research in similar directions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Catalin_Ionescu1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=35nFSbEBks",
  "title": "Equivariant Spatio-Temporal Attentive Graph Networks to Simulate Physical Dynamics",
  "modified_abstract": "Inspired by recent progresses in understanding and simulating dynamic systems through Graph Neural Networks (GNNs), particularly those that address human motion prediction by capturing spatial-temporal patterns, this work advances the field by addressing the representation and simulation of physical dynamics. Learning to represent and simulate the dynamics of physical systems, from molecular motions to macro-level phenomena, is a crucial yet challenging task. Existing equivariant Graph Neural Network (GNN) based methods have encapsulated the symmetry of physics, e.g., translations, rotations, etc, leading to better generalization ability. Nevertheless, their frame-to-frame formulation of the task overlooks the non-Markov property mainly incurred by unobserved dynamics in the environment, including poses and their transitions over time. In this paper, we reformulate dynamics simulation as a spatio-temporal prediction task, by employing the trajectory in the past period to recover the Non-Markovian interactions through sophisticated sampling techniques. We propose Equivariant Spatio-Temporal Attentive Graph Networks (ESTAG), an equivariant version of spatio-temporal GNNs, enhanced through advanced predictors and sampling techniques to fulfill our purpose. At its core, we design a novel Equivariant Discrete Fourier Transform (EDFT) to extract periodic patterns from the history frames, and then construct an Equivariant Spatial Module (ESM) to accomplish spatial message passing, and an Equivariant Temporal Module (ETM) with forward attention and equivariant pooling mechanisms to aggregate temporal message. Pretrained models might further refine our system by providing a richer, multi-modal understanding of physical laws in their learned parameters. We evaluate our model on three real datasets corresponding to the molecular-, protein- and macro-level. Experimental results verify the effectiveness of ESTAG compared to typical spatio-temporal GNNs, anchor-based and equivariant GNNs in capturing the sequence of dynamic events.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sirui_Xu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=sqTcCXkG4P",
  "title": "Sparsity-Preserving Differentially Private Training of Large Embedding Models",
  "modified_abstract": "The escalation in deployment of large embedding models across recommendation systems and natural language processing tasks has intensified the scrutiny on user data privacy. Inspired by preceding works that have navigated the complexities of embedding spaces and privacy, such as Landmark Ordinal Embedding, which introduces efficient strategies for dimensional reduction and representation, our study addresses the critical challenge of preserving data privacy in the context of embedding models. DP-SGD, a training algorithm that marries differential privacy with stochastic gradient descent, heralds a promising approach to safeguarding user data privacy. Yet, its direct application to embedding models lacks efficiency due to the degradation of gradient sparsity, a cornerstone for computational efficiency in large model training. We introduce two novel algorithms, DP-FEST and DP-AdaFEST, specifically designed to maintain gradient sparsity while incorporating differential privacy into the training of large embedding models through advanced optimization techniques. These algorithms not only demonstrate a stark reduction (by a factor of $10^6 \\times$) in gradient size but also retain competitive accuracy when compared to benchmark datasets, leveraging landmark-based approaches to overcome the complexity of embedding spaces. Our analysis further illuminates the intricate balance between privacy, sparsity, and accuracy in the optimization of embedding models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nikhil_Ghosh1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=7hLlZNrkt5",
  "title": "A Theory of Link Prediction via Relational Weisfeiler-Leman on Knowledge Graphs",
  "modified_abstract": "Inspired by parallel advancements in the employment of labels and features on graphs for semi-supervised learning and the development of graph neural networks (GNNs) for node classification tasks, our research extends these foundational insights to the domain of knowledge graphs. Specifically, we aim to provide a systematic understanding of graph neural networks' landscape as it pertains to knowledge graphs, concentrating on the task of link prediction under various settings, including stochastic environments and different data split scenarios, highlighting why one should never split data randomly owing to the unique structure of knowledge graphs. Our investigation provides a unifying perspective on seemingly unrelated models and unlocks a new series of models by characterizing the expressive power of various graph neural network models through the lens of a corresponding relational Weisfeiler-Leman algorithm. Furthermore, we extend our analysis to incorporate regularization techniques, providing a precise logical characterization of the functions captured by these graph neural networks, employing both label information and semi-supervised techniques. The theoretical findings in this paper articulate the advantages of certain widely used practical design choices, such as innovative regularization tricks in knowledge graph applications, which we validate empirically, thereby contributing to a deeper understanding of GNNs' roles in enhancing link prediction accuracy.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yang_Yongyi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=wRhLd65bDt",
  "title": "Improving Diffusion-Based Image Synthesis with Context Prediction",
  "modified_abstract": "Building on recent breakthroughs in generative models, particularly the advancements showcased by diffusion models in image generation, and inspired by seminal work on the role of context in learning representations such as style-based encoder pre-training for multi-modal image synthesis, this paper introduces ConPreDiff. This innovation addresses a pivotal limitation in existing diffusion models that focus primarily on reconstructing inputs from corrupted versions without fully preserving the contextual integrity of each pixel or feature. By leveraging the contextual relationships within image data, our approach embodies a significant stride towards enhancing the quality and semantic coherence of generated images. Specifically, ConPreDiff employs a unique strategy where a context decoder is utilized during the pre-train phase to enforce the prediction of neighborhood contexts (i.e., multi-stride pixels/features) at the end of diffusion denoising blocks, with the context decoder being removed during inference. This ensures a more contextually aware reconstruction of pixels or features, allowing for an improved preservation of semantic connections within the neighborhood context, akin to leveraging a style-code for retaining stylistic integrity. The capability of ConPreDiff to seamlessly integrate with both discrete and continuous diffusion backbones\u2014without additional parameters during sampling\u2014demonstrates its versatility, efficiency in learning, and application across multiple tasks. Through rigorous evaluation across a variety of tasks, including unconditional image generation, text-to-image generation, and image inpainting, ConPreDiff not only surpasses previous methods but also establishes new state-of-the-art (SOTA) benchmarks, notably achieving a zero-shot FID score of 6.21 on MS-COCO for text-to-image generation. Our findings validate the efficacy of context prediction as a mechanism for refining the generative capabilities of diffusion models, opening new avenues for future research in image synthesis and style translation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yixuan_Ren1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=jvYXln6Gzn",
  "title": "Auxiliary Losses for Learning Generalizable Concept-based Models",
  "modified_abstract": "The increasing use of neural networks in various applications has led to apprehensions about their interpretability, motivating the development of Concept Bottleneck Models (CBMs) designed to make the internal workings of models more transparent by constraining their latent space to human-understandable concepts. Despite their potential, CBMs often encounter a trade-off between interpretability and performance, as they may learn irrelevant concept representations that negatively affect overall model efficacy. Inspired by previous work on class representation learning classification, which addresses similar challenges in few-shot image classification by optimizing for clear, distinct class representations, our research introduces a cooperative-Concept Bottleneck Model (coop-CBM) for improved classification and learning outcomes. This classification approach leverages fine-grained concept labels to enhance the meaningfulness of concept representations and incorporates a novel concept orthogonal loss (COL) designed to improve the separation and reduce the intra-concept distance of these representations, facilitating learning. Through extensive experimentation on real-world datasets, including CUB, AwA2, CelebA, and TIL, for image classification tasks, this study demonstrates the effectiveness of coop-CBM models in various distributional shift scenarios, achieving higher accuracy compared to both traditional CBMs and black-box models that prioritize concept accuracy.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~PRATIK_MAZUMDER1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=hz33V7Tb2O",
  "title": "CLeAR: Continual Learning on Algorithmic Reasoning for Human-like Intelligence",
  "modified_abstract": "Inspired by advancements in neural network architectures that address deep learning's fundamental challenges, such as the vanishing gradient problem through innovations like low-rank passthrough neural networks, our work introduces a pioneering approach to continual learning (CL) tailored for abstract logical concepts. Continual learning, with a focus on mitigating previously observed learning limitations, aims to incrementally learn multiple tasks that are presented sequentially, a capability that is inherent in humans and crucial for developing human-like intelligence in machines. Despite the considerable progress in CL on structured data, the domain of abstract logical reasoning\u2014encompassing crucial cognitive skills such as counting, sorting, and arithmetic\u2014has remained largely unexplored. Our research, CLeAR, establishes a novel algorithmic reasoning (AR) framework for CL that is specifically designed for abstract concepts. This methodology leverages a one-to-many mapping from input distribution to a shared mapping space, facilitated by innovative architectures to align varied tasks across different dimensions and semantics. We define our tasks within the Chomsky hierarchy to systematically assess their complexity and utilize techniques, including memory-reducing strategies and the \"passthrough\" mechanism, to mitigate the challenges posed by the vanishing gradient phenomenon. Through comprehensive experiments involving 15 tasks spanning various levels of this hierarchy, CLeAR demonstrates exceptional performance by achieving near-zero forgetting and facilitating backward transfer, where accuracy on previous tasks is enhanced as new tasks are learned\u2014a stark contrast to the performance of traditional CL methods optimized for image classification.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Antonio_Valerio_Miceli_Barone1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=PqfPjS9JRX",
  "title": "The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit",
  "modified_abstract": "The burgeoning field of deep learning has continuously evolved, drawing significant insights from the geometric analysis of models like Generative Adversarial Networks (GANs) and their applications in capturing the intricate structures of data representations on manifolds and transformations within those structures. These insights have predominantly centered around understanding the behavior of networks through the lens of the high-dimensional spaces they operate in, focusing on capturing and generating variations in the structure of complex datasets, including images. In a similar vein, our work is inspired by the pursuit to understand and improve the trainability and stability of deep learning models, particularly transformers, in challenging settings of infinite depth and width. This paper delves into the theoretical underpinning of attention mechanisms in transformers by examining the covariance matrix of a modified Softmax-based attention model with skip connections in the proportional limit of infinite depth and width. We demonstrate that at initialization, the limiting distribution of the model's representations, pretrained through simulations, can be described by a stochastic differential equation (SDE) indexed by the depth-to-width ratio, reflecting a generator-like function in shaping the data's distribution. Our modifications to the attention mechanism\u2014centering the Softmax output at identity and scaling the Softmax logits by a width-dependent temperature parameter\u2014aim to ensure a well-defined stochastic limit. Through mathematical analysis and simulations, we underscore the importance of our architectural adjustments, referred to as the shaped Transformer, in maintaining a stable covariance structure and preventing rank degeneracy in deep attention models. The paper not only contributes to the literature by proposing a novel theoretical framework for analyzing transformers but also provides empirical evidence to support the efficacy of the shaped Transformer in large-scale deep learning models, especially those trained for image data.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Binxu_Wang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=hN4qpvGzWn",
  "title": "Game Solving with Online Fine-Tuning",
  "modified_abstract": "Reflecting on the advancements in cooperative deep Multi-Agent Reinforcement Learning (MARL), such as the development of sophisticated exploration strategies highlighted in recent works, including benchmarks in agent cooperation and performance, our study shifts focus towards the challenges in game solving\u2014a domain that requires not just mastery over games but identifying the game-theoretic value and strategies for optimal play. The AlphaZero algorithm has demonstrated super-human level play, and its powerful policy and value predictions have also served as heuristics in game solving. However, to solve a game and obtain a full strategy, a winning response must be found for all possible moves by the losing player. This includes very poor lines of play from the losing side, for which the AlphaZero self-play process, driven by deep cooperative strategies among agents, will not encounter. AlphaZero-based heuristics can be highly inaccurate when evaluating these out-of-distribution positions, which occur throughout the entire search. To address this issue, this paper investigates applying online fine-tuning while searching and proposes two methods to learn tailor-designed heuristics for game solving. Our experiments show that using online fine-tuning can solve a series of challenging 7x7 Killall-Go problems, using only 23.54% of computation time compared to the baseline without online fine-tuning, incorporating an action-computation framework that optimizes efficiency. The results suggest that the savings scale with problem size and highlight the effectiveness of online fine-tuning in enhancing exploration techniques and fostering an optimistic outlook among researchers towards solving more complex game-theoretic challenges. Our method can further be extended to any tree search algorithm for problem-solving, potentially increasing the roster of agents capable of super-human performance. Our code is available at a publicly accessible repository.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chenjun_Xiao1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=ePkLqJh5kw",
  "title": "Combating Bilateral Edge Noise for Robust Link Prediction",
  "modified_abstract": "In the landscape of graph neural networks (GNNs), where significant strides have been made in tasks like link prediction, the robustness of these models under the condition of edge noise remains an underexplored frontier. This challenge mirrors broader concerns in machine learning regarding model generalizability, counterfactual reasoning, and vulnerability to adversarial attacks, as evidenced by recent work on causal representation for deep learning. Such investigations highlight the necessity of developing methods that not only excel in performance but also in resilience and generalization, across different domains including image classification and recommender systems. Our study contributes to this ongoing discourse by introducing the Robust Graph Information Bottleneck (RGIB), a novel information-theory-guided principle designed to navigate the complexities introduced by bilateral edge noise. RGIB distinguishes itself by effectively decoupling and balancing mutual dependencies among graph topology, target labels, and representations\u2014thereby setting new benchmarks for representation robustness against noise and advancing the task of robust link prediction. Through the development of two methodologies, RGIB-SSL and RGIB-REP, for self-supervised learning and data reparameterization respectively, our approach addresses both implicit and explicit data denoising. The efficacy of RGIB is demonstrated across six datasets and three GNN architectures under various noisy conditions, supported by causal experiments focused on observational studies and the effectiveness of models trained in these adversarial settings. This research not only advances our understanding of robust link prediction but also establishes a framework for future explorations into resilient model design, emphasizing causal perspectives.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mengyue_Yang1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=BopG5dhH7L",
  "title": "A Computationally Efficient Sparsified Online Newton Method",
  "modified_abstract": "Inspired by research addressing the complexities of optimization dynamics in neural networks, this paper introduces the Sparsified Online Newton (SONew) method, a scalable second-order algorithm designed to efficiently train large models. Leveraging insights from multi-scale feature learning and the phenomena of double descent, we identify the necessity for computationally efficient optimization methods capable of overcoming the limitations of current second-order approaches, namely high memory and computational demands. The SONew method utilizes the LogDet matrix divergence measure, combined with sparsity constraints and insights into the evolution of learning processes and optimization dynamics, to minimize regret in the online convex optimization framework, thereby achieving a balance between computational efficiency and sparsity that enhances convergence rates for deep neural network training. This method is notable for its epoch-wise adaptation, fostering significant advancements in generalization and theory-driven approaches to learning. Empirical tests on large-scale benchmarks demonstrate significant improvements over existing methods, including up to $30\\%$ faster convergence and substantial gains in generalization, validation performance, and training loss. Such improvements highlight the method's capacity to integrate structured sparsity patterns efficiently, rivaling first-order methods in terms of computational requirements while achieving faster convergence. This work underscores the potential of structured sparsity, theoretical underpinnings, and closed-form solutions in simplifying the implementation of second-order methods, thereby making them more accessible for practical applications in large-scale machine learning tasks. Notably, the SONew algorithm is presented as a more scalable and easily implementable solution compared to state-of-the-art second-order methods, which struggle to adapt to large benchmarks due to their intensive memory and computational requirements. The code for SONew is available for researchers and practitioners, although the link has been omitted as per submission guidelines.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Amartya_Mitra1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=xE7oH5iVGK",
  "title": "LVM-Med: Learning Large-Scale Self-Supervised Vision Models for Medical Imaging via Second-order Graph Matching",
  "modified_abstract": "The rapid evolution in machine learning, specifically in areas like graph neural networks (GNNs), has laid the groundwork for novel approaches in understanding complex data structures. Leveraging insights from advancements such as the development of deeper GNNs for improved graph representation and overcoming challenges like vanishing gradients and over-smoothing, our work, LVM-Med, addresses the pressing need for large pre-trained models in medical imaging. Unlike the pre-trained networks on ImageNet and vision-language foundation models trained on web-scale data, which face limitations owing to the domain shift between natural and medical images, LVM-Med represents the first family of deep networks trained on a large scale collection of approximately 1.3 million medical images from 55 publicly available datasets. These images span a comprehensive range of organs and modalities, including CT, MRI, X-ray, and Ultrasound. In our approach, we benchmark several state-of-the-art self-supervised algorithms on this dataset and introduce a novel self-supervised contrastive learning algorithm rooted in a graph-matching formulation. This formulation integrates prior pair-wise image similarity metrics and captures the structural constraints of feature embeddings through a loss function based on a combinatorial graph-matching objective, facilitating efficient end-to-end training with modern gradient-estimation techniques for black-box solvers. Our comprehensive evaluation of LVM-Med across 15 downstream medical tasks showcases its superiority over numerous supervised, self-supervised, and foundation models, achieving significant improvements in challenging tasks such as Brain Tumor Classification and Diabetic Retinopathy Grading. This advancement not only highlights the potential of integrating complex network architectures and self-supervised learning in the medical domain but also sets a new benchmark for future research in medical image analysis.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dongqi_Fu1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=MlrFYNo1yc",
  "title": "Minimum norm interpolation by perceptra: Explicit regularization and implicit bias",
  "modified_abstract": "In the context of recent progress in understanding generalization measures for deep neural networks, particularly near the neural tangent kernel (NTK) regime, our study sheds light on the behavior of shallow ReLU networks, emphasizing the role of explicit regularization and its resultant implicit bias. Building on the foundations laid by investigations into generalization gaps and optimization behaviors of deep neural networks, this work explores how shallow ReLU networks interpolate between known datasets. Our analysis, employing various statistical methods, reveals that empirical risk minimizers converge empirically to a minimum norm interpolant as the number of data points and parameters increases, with this convergence being modulated by a weight decay regularizer whose coefficient vanishes at a specific rate correlated with network width and data volume expansion. Furthermore, we numerically examine the implicit bias present in common optimization algorithms towards previously identified minimum norm interpolants, both with and without the application of explicit regularization. This research contributes empirically to the broader discourse on regularization techniques, network architectures, and their influence on the generalization capabilities of neural networks within the statistical community, offering nuanced insights into the mechanisms governing network training and behavior within distinct regimes. ",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kohta_Ishikawa1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=yw1v4RqvPk",
  "title": "Computing Optimal Equilibria and Mechanisms via Learning in Zero-Sum Extensive-Form Games",
  "modified_abstract": "Inspired by recent innovations in multi-agent reinforcement learning (MARL) evaluation, such as those presented in scalable evaluation suites, our study introduces a novel approach for computing optimal equilibria and broad mechanisms in extensive-form games through training and learning. These frameworks have advanced our understanding of agent behaviors and their interactions in complex, social environments, paving the way for our investigation into extensive-form settings that encompass any number of players and scenarios, including mechanism design, information design, and a variety of solution concepts like correlated, communication, and certification equilibria. We base our formulation on the discovery that optimal equilibria can be viewed as minimax equilibrium strategies of a player in an extensive-form zero-sum game. This perspective enables the application of zero-sum game learning techniques and explicit training procedures, leading to the development of the first learning dynamics known to converge to optimal equilibria, not just in empirical averages but also in specific iterates. Our methodology's practical scalability and versatility are validated through its application to benchmark tabular games, where it achieves state-of-the-art performance, and in computing an optimal mechanism for a sequential auction design problem utilizing deep reinforcement learning. The apparent interface between human cognition and agent design further amplifies the significance of social considerations in developing and evaluating these algorithms.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sasha_Vezhnevets1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=xgY4QcOiEZ",
  "title": "Learning a Neuron by a Shallow ReLU Network: Dynamics and Implicit Bias for Correlated Inputs",
  "modified_abstract": "This work extends upon foundational methodologies in the training of neural networks, as discussed in previous literature such as studies on regularized training where the regularizer may be non-smooth and non-convex, by addressing the regression task of learning a single neuron via a one-hidden layer ReLU network. Our investigation specifically focusses on networks trained by gradient flow from a small initialization, highlighting that such training not only converges to zero loss but also harbors an implicit bias towards minimizing the rank of network parameters. By examining the correlation between training points and the teacher neuron\u2014a scenario less explored compared to orthogonal datasets\u2014we provide novel insights into the training dynamics and implicit biases underpinning neural networks learning. Our results draw from a meticulous non-asymptotic analysis of each hidden neuron's training dynamics in networks, revealing underlying mechanisms that diverge significantly from those observed in networks trained on uncorrelated data. Moreover, we delineate a notable contrast between interpolator networks of minimal rank and those minimized for Euclidean norm, a distinction that is accentuated in the presence of correlated input data. The conclusions are validated through comprehensive numerical experiments, solidifying our theoretical assertions and broadening the scope of understanding neural network training, particularly in contexts deviating from traditional orthogonal data assumptions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jihun_Yun2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=scaKiAtbI3",
  "title": "Retrieval-Augmented Multiple Instance Learning",
  "modified_abstract": "In the context of evolving weakly supervised learning methods such as Multiple Instance Learning (MIL), our work is inspired by the foundational concepts introduced in recent semi-supervised learning frameworks that emphasize the significance of learning from few labeled data through methods that exploit semantic and instance similarities. Addressing the noted limitation of MIL models' performance degradation when applied to out-of-domain data, our study introduces the Retrieval-Augmented MIL (RAM-MIL) framework. This innovation leverages Optimal Transport (OT) for distance metric in nearest neighbor retrieval, motivated by the hypothesis that reducing the intrinsic dimension of inputs can minimize the approximation error in attention-based MIL, and act as a form of regularization. Additionally, prior advancements suggest a relationship between the intrinsic dimension of input and the feature merging process with retrieved data, supporting our framework's development. Empirical assessments on WSI classification underscore RAM-MIL's superior performance against established baselines in both in-domain and out-of-domain contexts as well as in vision-related applications, thereby setting a new benchmark. Furthermore, implementing the transportation matrix from OT enhances interpretability at the instance level, offering a distinct advantage over traditional distance metrics and enabling visualization for expert analysis. The RAM-MIL framework also notably benefits from leveraging a memory component that stores information from few labeled instances for effective retrieval and learning, demonstrating an innovative utilization of similarity in enhancing MIL's capabilities.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Lang_Huang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=UpN2wfrLec",
  "title": "Language Is Not All You Need: Aligning Perception with Language Models",
  "modified_abstract": "Inspired by recent advancements in visual speech recognition (VSR) that highlight the efficacy of synthetic data and cross-modal learning, we introduce KOSMOS-1, a Multimodal Large Language Model (MLLM) that integrates the convergence of language, multimodal perception, action, and world modeling as foundational steps toward achieving artificial general intelligence. Our approach leverages insights from pioneering works such as SynthVSR, which utilizes synthetic visual data to enhance VSR systems significantly, and extends these capabilities to include the interpretation of machine-transcribed videos and animation, enriching the model's ability to understand and generate language based on lip movement and facial expressions. By leveraging these advances, KOSMOS-1 is trained from scratch on web-scale multimodal corpora, encompassing a variety of data types from arbitrarily interleaved text and images to image-caption pairs, texts, and datasets explicitly designed for nonverbal reasoning and vision-based tasks. This comprehensive training approach enables KOSMOS-1 to perform a wide array of tasks including language understanding, generation, OCR-free NLP (interpreting document images directly without the need for gradient updates or fine-tuning), multimodal interaction, speech recognition, and vision-based tasks, making it a large-scale tool in the domains of OCR-free NLP and visual perception. The model's ability to seamlessly integrate and interpret multimodal data including videos underscores the potential for significant cross-modal transfer learning, where knowledge acquired in one modality enhances performance in another. Our evaluation extends to the realm of nonverbal reasoning through a dataset based on Raven IQ tests, aimed at assessing the nonverbal reasoning capabilities of MLLMs. The results affirm KOSMOS-1's formidable capabilities across both linguistic and perceptual tasks, including speech recognition, paving the way for future advancements in the field of artificial general intelligence.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Stavros_Petridis1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=eXubleMT0q",
  "title": "Penguin: Parallel-Packed Homomorphic Encryption for Fast Graph Convolutional Network Inference",
  "modified_abstract": "The integration of Graph Convolutional Network (GCN) and Homomorphic Encryption (HE) heralds a promising avenue for performing graph data analytics and inference on the cloud while preserving client data privacy. This approach builds on the foundation set by prior works in graph representation and learning, notably leveraging the insights from network controllability and control theory perspectives to understand graph structures dynamically. Our investigation, grounded in dynamical systems theory, focuses on overcoming the computational and memory overhead challenges that hamper the practical deployment of HE-based GCN inference for data mining and secure analytics. By identifying inefficiencies in applying existing HE-based secure matrix-matrix multiplication solutions to GCN inference, we introduce Penguin, a novel HE-based ciphertext packing technique. Penguin is designed to minimize computation and memory overheads by employing a two-dimensional parallel packing method optimized for graph node partitioning and feature interleaving, complemented by an interleaved assembly technique for efficient ciphertexts merging after feature reduction. The technique significantly lowers the need for costly rotation operations, thereby enhancing control over computational resources. Through theoretical analysis and empirical validation using popular GCN models and datasets, which serve as benchmarks, we demonstrate Penguin's capability to achieve up to approximately 10 times speedup and about 79% reduction in computational memory overhead, vastly surpassing contemporary solutions. This marks a pivotal step towards enabling the secure acceleration of HE-GCN inference on encrypted data, safeguarding both the graph structure and its features against unauthorized access, thereby enhancing the privacy-preserving capabilities of GCN analytics. This paper's contributions are aimed at enhancing the efficiency of privacy-preserving GCN inference, with the original code accessible publicly. Note: The GitHub link has been excluded in adherence to the provided instructions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Anwar_Said1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=zq4vFneRiA",
  "title": "The Crucial Role of Normalization in Sharpness-Aware Minimization",
  "modified_abstract": "In the wake of Sharpness-Aware Minimization (SAM)'s introduction and its significant impact on the predictive accuracy of deep neural networks, our investigation draws from a diverse background of optimization strategies in machine learning, particularly focusing on challenges like distribution shift where techniques such as importance weighting have sought to address disparities in training and test data distribution through novel approaches. This discourse envelops a broader spectrum of optimization complexities, including how specific methodologies can enhance or mitigate algorithm performance under varied conditions. Our work delves into understanding the role played by normalization within SAM, a pivotal mechanism that has garnered less attention yet plays a critical role in the optimizer's effectiveness. Through theoretical analysis and experimental validation, we highlight how normalization interplays with SAM's mechanics to: i) stabilize the algorithm's convergence behavior, and ii) enable the algorithm to efficiently navigate the loss landscape by seeking out a continuum of performant minima, which recent theoretical advancements suggest is instrumental for achieving superior model performance. These insights into normalization's dual role not only elucidate why SAM demonstrates robustness across different hyper-parameter settings but also accentuate the optimizer's practical viability. By comparing and contrasting our findings with existing research, notably on addressing distribution shifts through importance weighting, we provide a comprehensive examination of normalization's indispensable function in enhancing SAM's operational fidelity.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tongtong_Fang1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=SzYHu7EIwZ",
  "title": "Precision-Recall Divergence Optimization for Generative Modeling with GANs and Normalizing Flows",
  "modified_abstract": "The quest for a balance between image quality (precision) and diversity (recall) in the domain of generative models intersects significantly with recent advancements in score-based diffusion models and their exploration of optimal diffusion times for enhanced model performance. Incorporating insights from studies that focus on the dynamics of generative modeling through stochastic differential equations and their implications for precision and recall, our work presents a novel training method for generative models, including Generative Adversarial Networks (GANs) and Normalizing Flows. This method uniquely optimizes a selected trade-off between precision and recall by minimizing a distinct $f$-divergence from a new family, designated as the \\mbox{\\em PR-divergences}, through an integrated approach of score-matching, strategic sampling, and rigorous training protocols. Detailed analysis on the approximation of traditional metrics and the introduction of new evaluation parameters leverages our understanding that any $f$-divergence can manifest as a linear composition of PR-divergences, thus facilitating a structured approach to balancing precision and recall in model training and improving the log-likelihood ratio of generated samples to real images. Through rigorous evaluations, specifically with state-of-the-art models like BigGAN on comprehensive datasets such as ImageNet, our approach demonstrably refines and enhances generative model performance either in terms of precision or recall, advocating for a potentially standardized methodology in generative model optimization.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Simone_Rossi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=I5SM5y57k2",
  "title": "On Robust Streaming for Learning with Experts: Algorithms and Lower Bounds",
  "modified_abstract": "Building on recent developments in Empirical Risk Minimization (ERM) that extend traditional models to accommodate dependent and heavy-tailed data, our work introduces novel algorithms for the online learning with experts problem, emphasizing the necessity for robust predictions under adaptively generated inputs. This exploration is fueled by the recognition of the complexities present in data-generating processes and the pivotal role of memory efficiency in streaming settings, alongside the need to effectively handle noise in the input data. In the online learning with experts problem, an algorithm makes predictions about an outcome on each of $T$ days, given a set of $n$ experts who make predictions on each day. The algorithm is given feedback on the outcomes of each day, including the cost of its prediction and the cost of the expert predictions, and the goal is to make a prediction with minimum cost, compared to the best expert in hindsight, while class-specific complexities introduce further challenges. However, often the predictions made by experts or algorithms at some time influence future outcomes, so that the input is adaptively generated. In this paper, our seminal contribution is the study of robust algorithms for the experts problem under memory constraints. We first give a randomized algorithm that is robust to adaptive inputs that uses $\\widetilde{O}\\left(\\frac{n}{R\\sqrt{T}}\\right)$ space for  $M=O\\left(\\frac{R^2 T}{\\log^2 n}\\right)$, thereby showing a smooth space-regret trade-off. We then show a space lower bound of $\\widetilde{\\Omega}\\left(\\frac{nM}{RT}\\right)$ for any randomized algorithm that achieves regret $R$ with probability $1-2^{-\\Omega(T)}$, when the best expert makes $M$ mistakes. Our result implies that the natural deterministic algorithm, which iterates through pools of experts until each expert in the pool has erred, is optimal up to polylogarithmic factors. Finally, we empirically demonstrate the benefit of using robust procedures against a white-box adversary that has access to the internal state of the algorithm. Through our rigorous analysis, we underscore our foundational contribution to the field of online learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Abhishek_Roy1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=WmqYhqvz5i",
  "title": "Contextual Bandits and Imitation Learning with Preference-Based Active Queries",
  "modified_abstract": "In addressing the complex challenge of decision-making without direct reward signals, this work situates itself among novel research paradigms such as Bayesian reinforcement learning (RL) that reconsider traditional approaches to learning from interactions. Specifically, our investigation into contextual bandits and imitation learning expands upon foundational concepts, such as the introduction of Bayesian Bellman operators, by focusing on scenarios where the learner must rely on preference feedback rather than explicit rewards or model-based uncertainties for control tasks. This exploration into the unique problem space where the learner lacks direct knowledge of the reward distribution associated with executed tasks is critical. Instead, the learner actively requests the expert at each round to compare two actions and receive noisy preference feedback. This not only aims to minimize regret from executed actions through rigorous analysis but also seeks to reduce the number of comparison queries to the expert, effectively bootstrapping the learning process with minimal direct feedback. Assuming access to a function class representing the expert's preference model under appropriate link functions, we introduce an actor-critic algorithm that employs a deep online regression oracle with respect to this function class, incorporating regularised learning techniques. For contextual bandit scenarios, our algorithm achieves a regret bound that adapts optimally to the context, scaling as $O(\\min\\{\\sqrt{T}, d/\\Delta\\})$, which is competitive with standard models that receive direct reward signals. Importantly, our approach significantly reduces the necessity for direct reward feedback by making only $O(\\min\\{T, d^2/\\Delta^2\\})$ expert queries, with exploration strategies fine-tuned for bootstrapping efficiency. We extend our methodology to imitation learning, applying it to environments of unknown dynamics over multiple episodes, and demonstrate that preference-based feedback enables surpassing sub-optimal experts, marking a significant contribution to bootstrapping knowledge in the absence of explicit rewards. This bridges a crucial gap with interactive imitation learning algorithms that require not only the expert's action observations but also reward signals for effective learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Matthew_Fellows1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=8XRMbNAP6Z",
  "title": "Near-Optimal $k$-Clustering in the Sliding Window Model",
  "modified_abstract": "The concept of clustering continues to be indispensable for unraveling structural information within vast datasets, especially in scenarios where it is impractical to store the entirety of the data. Recognizing the importance of more recent data over older data in many applications, the sliding window model offers a compelling framework for addressing this priority, which has recently spurred significant interest in developing clustering solutions optimized for this model. This interest builds on previous explorations of clustering, notably the advancements in hierarchical clustering and the analytical frameworks proposed to understand and improve it. Leveraging insights from foundational works, including the implementation of Max-Uncut Bisection for hierarchical clustering optimization, we introduce the first algorithm that achieves a near-optimal $(1+\\varepsilon)$-approximation for $(k,z)$-clustering within the sliding window model. Our algorithm remarkably requires $\\frac{k}{\\min(\\varepsilon^4,\\varepsilon^{2+z})}\\,\\text{polylog}\\frac{n\\Delta}{\\varepsilon}$ words of space for points from $[\\Delta]^d$, thus presenting a significant improvement over the existing literature in terms of space efficiency, polynomial-time computability, and data similarity considerations. In the process, we pioneer an online coreset framework for clustering, which generates a coreset not just at the stream's end but also for all its prefixes, highlighting a similarity in complexities between real-time and theoretical models. This framework dictates that constructing online coresets necessitates at least $\\Omega\\left(\\frac{k}{\\varepsilon^2}\\log n\\right)$ samples, underscoring the heightened complexity compared to offline coresets. Our findings not only advance clustering methods within the sliding window model but also clarify the theoretical limitations and capabilities of online and offline coreset constructions against the backdrop of general metrics on $[\\Delta]^d$.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alessandro_Epasto3",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=4xckZu4MPG",
  "title": "Attention as Implicit Structural Inference",
  "modified_abstract": "This work is inspired by previous explorations into the hierarchical structure of data and its learnability within neural networks, specifically addressing how such structures guide deep learning architectures towards efficient learning strategies. Building on the foundations laid by concepts such as the staircase property in hierarchical data learning, our study extends the investigation into the domain of attention mechanisms in cognitive systems, focusing on Transformers, which have surged to the forefront of machine learning innovation. Attention mechanisms facilitate the flexible allocation of cognitive resources, with Transformers exemplifying this by integrating attention as a core feature, and learning to adaptively scale context windows based on input. Despite attention's prevalent role, its foundational principles, drawing parallels to keys and queries in database systems, invite further exploration into its structural underpinnings. We propose a novel viewpoint that considers attention as a form of structural inference, positing it as inference over an implicitly defined set of possible adjacency structures in a graphical model, akin to navigating a hypercube of potential connections. Through the lenses of Gaussian Mixture Models, alignment mechanisms, and Hopfield Networks, we unify various attentional architectures under this perspective, suggesting new modifications and generalizations that involve polynomial expansions of these concepts. Our investigations include an extension of the value function to bias towards multiple tokens with gradient-based optimization techniques and the introduction of a geometric prior over adjacency structures to adaptively scale context windows based on input. Furthermore, we bridge the gap between machine learning attention mechanisms and Bayesian attention models in neuroscience, aiming to enrich both fields with deeper insights into attentional architectures. This interdisciplinary approach not only enhances our understanding of attention but also paves the way for developing more sophisticated attention mechanisms that leverage gradient-based learning and other forms of polynomial approximation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Enric_Boix_Adser\u00e01",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=nRfClnMhVX",
  "title": "Interpretability at Scale: Identifying Causal Mechanisms in Alpaca",
  "modified_abstract": "As AI safety becomes increasingly critical, the quest for methods to obtain human-interpretable explanations of model behavior is paramount. This necessity is compounded by the requirement for these interpretability methods to faithfully capture the causal dynamics underpinning the model's operations, to generalize robustly across unseen inputs, and to undergo thorough verification to ensure their accuracy, reliability, and adherence to formal specifications. Inspired by the progress in bridging the gap between natural language sentences and formal specifications, our research introduces a significant evolution of the Distributed Alignment Search (DAS) methodology. This evolution, termed Boundless DAS, supersedes the limitations of previous approaches by integrating learned parameters to replace brute-force search components, and aligning closely with programming principles for greater efficiency and precision. Particularly, by applying Boundless DAS to the Alpaca model, a language model with 7 billion parameters, we are able to efficiently uncover and analyze interpretable and regular causal structures within large-scale neural networks through systematic experiments and analysis on various datasets. Our methodology reveals that Alpaca solves numerical reasoning problems by employing a causal model with two boolean variables, showcasing the robustness of neural representation alignments across varying inputs and instructions. These breakthroughs in interpretation at scale set a new benchmark for understanding the intricate mechanisms of modern language models, marking a significant stride towards ensuring their safe and transparent utilization.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Frederik_Schmitt1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=Y44NurSDjq",
  "title": "Quantum Bayesian Optimization",
  "modified_abstract": "Inspired by advances in stochastic contextual bandits and the quest for algorithmic efficiency in representation learning, this work expands the frontier into the quantum computing domain, specifically targeting the challenges posed by complex, non-linear reward structures inherent in real-world applications. Kernelized bandits, also known as Bayesian optimization (BO), has been a prevalent method for optimizing complicated black-box reward functions. Various BO algorithms have been theoretically shown to enjoy upper bounds on their cumulative regret which are sub-linear in the number $T$ of iterations, and a regret lower bound of $\\Omega(\\sqrt{T})$ has been derived which represents the unavoidable regrets for any classical BO algorithm. Recent works on quantum bandits have shown that with the aid of quantum computing and deep learning techniques, it is possible to achieve tighter regret upper bounds, better than their corresponding classical lower bounds, and potentially recovered classical information with quantum efficiency. However, these works are restricted to either multi-armed or linear bandits, and are hence not able to solve sophisticated real-world problems with non-linear reward functions. To this end, we introduce the quantum-Gaussian process-upper confidence bound (Q-GP-UCB) algorithm, integrating it with context-action strategies and regularizing mechanisms to optimize the ratio of exploration to exploitation. To the best of our knowledge, our Q-GP-UCB is the first BO algorithm able to achieve a regret upper bound of $\\mathcal{O}(\\text{poly}\\log T)$, which is significantly smaller than its regret lower bound of $\\Omega(\\sqrt{T})$ in the classical setting. Moreover, thanks to our novel analysis of the confidence ellipsoid, our Q-GP-UCB with the linear kernel achieves a smaller regret than the quantum linear UCB algorithm from the previous work. We use simulations, as well as an experiment using a real quantum computer, to verify that the theoretical quantum speedup achieved by our Q-GP-UCB is also potentially relevant in practice.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Andrea_Tirinzoni2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=A383wMho4h",
  "title": "Oracle Complexity of Single-Loop Switching Subgradient Methods for Non-Smooth Weakly Convex Functional Constrained Optimization",
  "modified_abstract": "This work builds upon foundational contributions in the area of convex-concave saddle point problems and extends the understanding of first-order methods' efficiency to the realm of non-convex constrained optimization. While previous efforts have shed light on the oracle complexity of optimistic methods in convex settings, we shift our focus to a non-convex constrained optimization problem where the objective function is weakly convex, and the constraint function is either convex or weakly convex. We evaluate the classical switching subgradient method, renowned for its straightforward implementation and effectiveness in addressing functions in convex problems, to confront this more challenging context. The paper presents the first analysis concerning the oracle complexity of the switching subgradient method in locating nearly stationary points for non-convex problems, assessing separately the cases involving convex and weakly convex constraints. Our findings highlight that, unlike double-loop methods traditionally employed for non-smooth issues, the switching subgradient method achieves comparable complexity through a more efficient single-loop configuration, obviating the need for intricate adjustments of inner iteration counts. The single-loop approach not only iterates efficiently within a broader class of optimization problems but also proves to advance our understanding of problem-solving in contexts where provably efficient methods are vital. Furthermore, the use of saddle point dynamics as a reference, reinforces the significance of the study.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ruichen_Jiang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=aIpGtPwXny",
  "title": "Learning to Modulate pre-trained Models in RL",
  "modified_abstract": "Inspired by the increasing exploration into how deep reinforcement learning (RL) policies evolve over trajectories and adapt across various domains, this paper seeks to address the challenges of task adaptation in RL agents. Despite the pronounced success of RL in areas such as robotics, game playing, and simulation, these agents often falter when adapting to new tasks\u2014a problem that large-scale pre-training and subsequent fine-tuning have aimed to solve in supervised learning. Taking a cue from recent advancements in understanding the dynamics of ReLU-based policies and their neural network partitioning of input space into piecewise linear regions, which suggest that the complexity of RL policies may not solely rely on the growth in function complexity around policy trajectories, our empirical study investigates the catastrophic forgetting phenomenon in the context of reinforcement learning. By jointly pre-training a neural network model on datasets from two benchmark suites, Meta-World and DMControl, and examining a range of fine-tuning methods, we identify significant performance deterioration in pre-training tasks during fine-tuning on new tasks. To counter this, we introduce a novel method, Learning-to-Modulate (L2M), which allows for the preservation of performance on pre-training tasks by modulating the information flow of the frozen pre-trained model through a learnable modulation pool. This neural modulation approach not only mitigates catastrophic forgetting but also sets new benchmarks for adaptability and skill retention in control tasks, marking a pivotal advance in the Continual-World benchmark, marking a pivotal advance in control tasks for reinforcement learning applications. The release of a comprehensive dataset encompassing 50 Meta-World and 16 DMControl tasks marks a significant empirical contribution to facilitating future research in this area.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Setareh_Cohan1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=d47iuwOt3j",
  "title": "On the Gini-impurity Preservation For Privacy Random Forests",
  "modified_abstract": "In the landscape of machine learning, safeguarding the privacy of ensemble algorithms, like random forests, is a paramount concern, evidenced by efforts ranging from anonymization to the application of differential privacy and homomorphic encryption. This work introduces a novel encryption approach designed to preserve the Gini impurity, a critical component in the construction of random forests, thereby addressing a gap in current privacy-preserving techniques that often overlook the intrinsic properties of the learning algorithms themselves. Our method modifies the binary search tree structure to accommodate multiple examples per node while encrypting data features by integrating label and order information, linking partitioning of data directly to its preservation of confidentiality. We provide a theoretical underpinning for our scheme, demonstrating that it maintains the minimum Gini impurity in ciphertexts without necessitating decryption and offer a security guarantee for the encryption process. Protocols for secure communication in the training phase ensure that our Gini-impurity-preserving encryption, combined with the homomorphic encryption scheme CKKS for data labels, highlights the importance of preserving privacy without compromising the algorithm's learning integrity and are trained to resist adversarial attacks like data poisoning. Our extensive experimentation with data transfer and validation protocols validates the effectiveness, efficiency, security, and trained robustness of our method, contributing to the broader conversation on privacy in machine learning as engaged by works like 'SafeNet', which explores the advantages of ensembles in private collaborative learning through secure multiparty computation (MPC), showcasing how ensembling can mitigate adversarial ML attacks while ensuring scalability and robustness.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Matthew_Jagielski1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=wbbTqsiKzl",
  "title": "High-dimensional Asymptotics of Denoising Autoencoders",
  "modified_abstract": "This work situates itself within a burgeoning field of research concerned with the high-dimensional behavior of statistical models, specifically examining the utilization of autoencoders for denoising tasks. Inspired by recent developments in random matrix theory as applied to the estimation of the Wasserstein distance between Gaussian distributions, our study extends these mathematical tools to explore the performance of denoising autoencoders in a high-dimensional setting. We address the problem of denoising data from a Gaussian mixture using a two-layer non-linear autoencoder with tied weights and a skip connection. Considering the high-dimensional limit where the number of training samples and the input dimension jointly tend to infinity while the number of hidden units remains bounded, we provide closed-form expressions for the denoising mean-squared test error. Building on this result, we quantitatively characterize the advantage of the considered architecture over the autoencoder without the skip connection, showing a close relationship to principal component analysis. Furthermore, our results accurately capture the learning curves on a range of real datasets, demonstrating the practical applicability and theoretical significance of our findings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Malik_Tiomoko1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=5hVXbiEGXB",
  "title": "Evolving Standardization for Continual Domain Generalization over Temporal Drift",
  "modified_abstract": "This research extends the discourse on domain generalization (DG) by addressing its limitations in dynamic scenarios characterized by gradual data distribution changes, an area previously underexplored despite advances in attention-based domain adaptation for time-series forecasting. The capability of generalizing to out-of-distribution data is crucial for the deployment of machine learning models in the real world. Existing domain generalization (DG) mainly embarks on offline and discrete scenarios, where multiple source domains are simultaneously accessible and the distribution shift among domains is abrupt and violent. Nevertheless, this setting may not apply universally in real-world applications, as there are cases where the data distribution changes gradually over time due to various factors, e.g., the process of aging. Additionally, as the domain constantly evolves, new domains will continually emerge. Re-training and updating models with both new and previous domains using existing DG methods can be resource-intensive and inefficient. Thus, this paper presents a problem formulation for Continual Domain Generalization over Temporal Drift (CDGTD). CDGTD addresses the challenge of gradually shifting data distributions over time, where domains arrive sequentially and models can only access the data of the current domain. The goal is to generalize to unseen domains not too far into the future. To this end, we propose an Evolving Standardization (EvoS) method, which characterizes the evolving pattern of feature distribution and mitigates the distribution shift by standardizing features with generated statistics of corresponding domains. Inspired by the powerful ability of transformers to model sequence relations in deep learning, we design a multi-scale attention module (MSAM) to learn the evolving pattern under sliding time windows of different lengths. MSAM can generate statistics of the current domain based on the statistics of previous domains and the learned evolving pattern. Experiments on multiple real-world datasets including series of images and texts validate the efficacy of our EvoS.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xiaoyong_Jin1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=pjSzKhSrfs",
  "title": "Wasserstein Quantum Monte Carlo: A Novel Approach for Solving the Quantum Many-Body Schr\u00f6dinger Equation",
  "modified_abstract": "Solving the quantum many-body Schr\u00f6dinger equation represents a cornerstone challenge within quantum physics, quantum chemistry, and materials science, akin to the pursuit of optimized solutions in fully composite and convex optimization described in preceding research. The prevailing methodologies, such as Quantum Variational Monte Carlo (QVMC), are transformative yet encumbered by limitations that stem from the computational intricacies of representing complex wave functions and the optimization of their associated energy functionals. This analysis draws inspiration from the rich landscape of optimization problems, specifically leveraging concepts from the affine-invariant linearization of fully composite optimization to rethink the traditional QVMC framework. By transitioning the optimization focus from the space of wave functions to the space of Born distributions, this paper introduces a novel interpretation of QVMC as the Fisher--Rao gradient flow in this distributional space, with a projection back onto the variational manifold. Our research proposes an innovative approach named \"Wasserstein Quantum Monte Carlo\" (WQMC). WQMC, adopting the Wasserstein metric for the gradient flow, enhances the transportation of probability mass for ground-state solutions. This method not only offers a novel framework for addressing the quantum many-body problem but also demonstrates, through empirical studies and experiments on molecular systems, that it achieves faster convergence than traditional QVMC techniques, showcasing its potential applications across our explored fields. The introduction of WQMC presents a paradigm shift in the computational optimization treatment of the Schr\u00f6dinger equation, mirroring the advancements in optimization theory and practice.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Maria-Luiza_Vladarean1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=oKqaWlEfjY",
  "title": "Worst-case Performance of Popular Approximate Nearest Neighbor Search Implementations: Guarantees and Limitations",
  "modified_abstract": "In the context of evaluating the robustness of machine learning algorithms, such as decision tree learning algorithms, our study extends this critical examination to the domain of graph-based approaches for approximate nearest neighbor search, which, despite their practical efficacy, lack robust theoretical guarantees. We specifically analyze the worst-case performance of leading graph-based approximate nearest neighbor search algorithms, including HNSW, NSG, and DiskANN. For DiskANN, we establish that its 'slow preprocessing' version can support approximate nearest neighbor search queries with a constant approximation ratio and poly-logarithmic query time in data sets with bounded 'intrinsic' dimension. Conversely, we identify a set of instances for other data structure variants studied, like DiskANN with 'fast preprocessing', HNSW, and NSG, where the empirical query time needed to achieve 'reasonable' accuracy scales linearly with the instance size. For instance, in the case of DiskANN, our findings indicate that the query procedure may require at least $0.1 n$ steps in instances of size $n$ to locate any of the $5$ nearest neighbors of the query, underscoring significant limitations in worst-case scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Qin-Cheng_Zheng1",
  "manipulated_ranking": 6,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=6oiux75UDj",
  "title": "Bayesian Optimization with Cost-varying Variable Subsets",
  "modified_abstract": "Inspired by recent progress in the field of machine learning, particularly concerning learning from label proportions (LLP) in the context of linear threshold functions (LTFs), our study introduces a novel problem within the domain of Bayesian optimization, termed Bayesian optimization with cost-varying variable subsets (BOCVS). In each iteration of BOCVS, the learner selects a subset of query variables to assign values, while the remaining variables are sampled randomly. Each subset selection incurs a different cost, posing a unique challenge for the learner to balance the acquisition of informative subsets for more targeted learning against the minimization of costs by allowing some variables to be sampled randomly. We propose a novel Gaussian process upper confidence bound-based algorithm for BOCVS, which is demonstrated to be provably no-regret. Our analysis reveals how the exploration strategy, aided by the selection of cheaper variable subsets, effectively reduces overall regret. Empirical evaluations confirm that our algorithm outperforms existing baselines within the same budget, offering a robust solution to the BOCVS problem. The connection to LLP and linear threshold functions alongside considerations of Boolean input spaces and the labeled (supervised) nature of the task highlights the intricate role of information costing in learning algorithms and sets a foundation for exploration in cost-sensitive learning scenarios, making efforts toward tight bounds on inappropriability a critical aspect of our investigation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Rishi_Saket1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=fShubymWrc",
  "title": "Provable Guarantees for Nonlinear Feature Learning in Three-Layer Neural Networks",
  "modified_abstract": "In the pursuit of unraveling the complexities of deep learning, particularly the process through which neural networks learn hierarchical features, this work is inspired by foundational contributions such as the analysis of deep networks in the context of the multiple manifold problem. Such precedents have illuminated the advantages of network depth and width for classification tasks, providing a springboard for our investigation into the superior feature learning capabilities of three-layer networks over their two-layer counterparts. We posit that three-layer neural networks harbor inherently richer potential for nonlinear feature extraction, a premise we substantiate by analyzing the feature learning process in these networks when trained with layer-wise gradient descent. Manifolds play a crucial role in this discourse, as our examination leverages the concept to deepen our understanding of the intricacies involved in the feature learning dynamics of neural networks. By introducing a theorem that upper bounds the nonasymptotic sample complexity and network width necessary to attain low test error for targets with a specific hierarchical structure, we make strides in theoretical advancements. Our investigation delves into statistical learning scenarios such as single-index models and functions of quadratic features, revealing that three-layer networks can achieve sample complexity improvements over all known guarantees for two-layer networks. This enhancement is fundamentally tied to their efficient learning of nonlinear features and the concentration of measure phenomena. Further, we delineate a depth separation based on concrete optimization considerations, constructing a function learnable through gradient descent on a three-layer network but intractable for two-layer networks. Through these findings, our work progresses the understanding of the intrinsic advantages offered by adding layers in neural networks, specifically in the realm of feature learning, and underscores the significance of machine learning in advancing non-linear feature extraction capabilities.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sam_Buchanan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=qSS9izTOpo",
  "title": "Alleviating the Semantic Gap for Generalized fMRI-to-Image Reconstruction",
  "modified_abstract": "Building on the foundational work in context-aware image completion, which highlights the challenges of bridging the semantic gap in visual data processing, this paper introduces a novel approach to the fMRI-to-image reconstruction problem. By recognizing the limitations of existing methodologies that struggle with semantic consistency between training and testing datasets, our research proposes an innovative solution to stabilize and clarify semantic understanding in reconstructed images. We achieve this by leveraging the pre-trained CLIP model to map the training data to a compact feature representation, significantly expanding the sparse semantics of training data to encompass denser semantic fields and thereby reducing the discrepancies of instances near known concepts within the training super-classes. Additionally, inspired by the robust low-level fMRI data representation, we employ structural information as a universal guide for image reconstruction, particularly for instances distanced from familiar concepts (outside the training super-classes), further narrowing the semantic divide. Our proposed Generalized fMRI-to-image reconstruction strategy, named GESS (Generalized fMRI-to-image reconstruction by adaptively integrating Expanded Semantics and Structural information within a diffusion process), quantifies semantic uncertainty with probability density estimation to refine reconstruction fidelity and synthesis of pixel-level detail, ensuring a seamless integration of foreground and background elements. Experimental validation of the GESS model showcases its superiority over state-of-the-art alternatives, particularly in managing the semantic gap through a generalized scenario split strategy tailored for assessing GESS's effectiveness in semantic gap minimization. The architecture of the GESS model, incorporating 'blocks' specialized for handling different aspects of the image reconstruction\u2014including those for semantic completion and photo-realistic detail synthesis\u2014further facilitates this process, proving vital in achieving our objective.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Minguk_Kang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=4PkBhz18in",
  "title": "High Precision Causal Model Evaluation with Conditional Randomization",
  "modified_abstract": "In a landscape where the feasibility and ethical considerations often limit the implementation of randomized controlled trials (RCTs), this work is inspired by the necessity to adapt causal inference methodologies, such as those addressing challenges posed by unknown control variables. Our research introduces an innovative approach to causal model evaluation that transcends traditional methods, targeting the inherent limitations of RCTs by leveraging conditionally randomized experiments based on inverse probability weighting (IPW). Such experiments, while more adaptable to real-world scenarios, are known for their high estimation variance. To address this, we propose a novel low-variance estimator for causal error, termed the pairs estimator. This estimator applies the same IPW methodology to both the model and true experimental effects, thereby specifying and neutralizing the variance introduced by IPW and achieving smaller asymptotic variance. The efficacy of our approach is substantiated through empirical studies, indicating a significant stride toward achieving near-RCT precision in causal model evaluation. By simplifying the application of IPW without requiring complex modifications to the estimator itself, our method facilitates more accurate and pragmatic evaluation of causal inference models in settings where controlled randomization is applicable, thereby contributing a vital tool for robust and reliable model assessment. Control over external variables and the precise specification of causal mechanisms are emphasized, bridging the gap between the theoretical and practical applications of causal inference and underlining our commitment to improving these methods.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dave_Zachariah1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=bv9mmH0LGF",
  "title": "Global Structure-Aware Diffusion Process for Low-light Image Enhancement",
  "modified_abstract": "Building on the foundation laid by previous works, such as the study on efficient acquisition of diffuse and specular normal maps through optimized illumination patterns, our research introduces a diffusion-based enhancement method specifically tailored to the challenges of low-light image enhancement. We explore the diffusion process deeper by advocating for the regularization of its inherent ODE-trajectory. Inspired by recent findings that low curvature ODE-trajectory results in a stable and effective diffusion process, we introduce a curvature regularization term anchored in the global structural characteristics of image data, referred to as global structure-aware regularization. This approach aims at maintaining complex details and enhancing contrast during the diffusion process while mitigating the adverse effects of noise and artifacts. Further, we acquire an uncertainty-guided regularization to adaptively relax constraints in the most challenging image regions, incorporating reflection patterns that are crucial for preserving the specular highlights and normal details of low-light images. By acquiring novel methods and leveraging acquisition patterns, our framework, augmented with rank-informed regularization, achieves notable improvements in enhancing low-light images, outperforming current leading methods in aspects of image quality, noise reduction, contrast enhancement, and the faithful recovery of reflectances. This work not only contributes to the enhancement of low-light images but also opens avenues for further research in diffusion model applications in image processing, especially in understanding and preserving the complex interactions of light with surfaces. The code for this study has been made publicly available.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Byeongjoo_Ahn5",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=Ct0zPIe3xs",
  "title": "Saving 100x Storage: Prototype Replay for Reconstructing Training Sample Distribution in Class-Incremental Semantic Segmentation",
  "modified_abstract": "In the context of class-incremental semantic segmentation (CISS), where each incremental step focuses on different foreground classes, the limitation of training sets to include only images with the relevant foreground classes leads to an overrepresentation of these classes and a skewed class distribution. This significant challenge, which contributes to catastrophic forgetting and background shift, reflects an area that requires innovative approaches for balanced learning and effective memory usage in visual content. Our work is inspired by novel methodologies in unsupervised visual feature learning and network architecture, such as parametric instance classification (PIC), which emphasizes simplicity and efficiency in handling data and model updates without the complexity of dual-branch, non-parametric frameworks. Embracing this perspective, we introduce STAR, a method that not only significantly reduces storage requirements by more than 100 times compared to methods relying on raw image replays but also adjusts class distribution in training through the use of compact prototypes and statistical data replay for instance-based classification. STAR distinguishes itself by incorporating an old-class features maintaining (OCFM) loss and a similarity-aware discriminative (SAD) loss, which collectively preserve the feature integrity of old classes while facilitating the learning of new ones and enhancing feature discrimination between similar class pairs, thus improving classification accuracy in network-based visual semantic segmentation. Through extensive testing on the Pascal VOC 2012 and ADE20K datasets, our method sets new benchmarks, outperforming existing approaches in class-incremental learning for visual semantic segmentation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhenda_Xie1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=9kFQEJSyCM",
  "title": "RangePerception: Taming LiDAR Range View for Efficient and Accurate 3D Object Detection",
  "modified_abstract": "In the realm of LiDAR-based 3D object detection, the quest for efficiency and accuracy has led to a divergence in methodological approaches, prominently featuring bird's-eye view (BEV) and range view (RV) perspectives. While BEV-based methods boast precision through voxelization and 3D convolutions, their computational demands hinder efficient training and inference. Conversely, RV-based methods offer streamlined processing via 2D convolutions, though they historically lag in detection performance. This study introduces 'RangePerception,' a framework designed to capitalize on RV's efficiency without sacrificing the accuracy benchmarked by BEV methodologies. Among the key features of this framework, the fusion of distinct data representations and the Range Aware Kernel (RAK) enhance the feature interpretability of LiDAR point clouds by addressing the inherent domain discrepancy between 3D world coordinates (output) and 2D range image coordinates (input), and mitigating the distortion of object visibility at range image margins. The vision Restoration Module (VRM) further supports this endeavor by restoring and highlighting distinctive features previously obscured, employing self-attentive mechanisms to ensure that the fine-grained point features of objects are preserved and emphasized. The implementation of RAK and VRM into the RangePerception framework propels its performance, surpassing the previous RV-based frontrunner, RangeDet, by 3.25/4.18 points in averaged L1/L2 AP on the Waymo Open Dataset. Remarkably, RangePerception not only equals but slightly exceeds the performance of the prominent BEV-based method, CenterPoint, while operating at 1.3 times its speed. This achievement marks a significant milestone for RV-based methods in 3D object detection, suggesting a promising direction for future research in LiDAR data processing and analysis.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yan-Pei_Cao1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=w116w62fxH",
  "title": "Optimal Learners for Realizable Regression: PAC Learning and Online Learning",
  "modified_abstract": "Building upon prior research that established foundational principles in the learnability of machine learning models, particularly in context to the sufficiency of the fat shattering dimension for PAC learnability and the role of coresets in approximating loss functions through advanced sampling techniques, this work aims to advance the understanding of the statistical complexity of realizable regression. We draw inspiration from the field's ongoing exploration into identifying key dimensions that govern learnability and optimization in machine learning; for instance, the introduction of sensitivity sampling in coresets for improving algorithm efficiency underscores the importance of developing nuanced metrics and methodologies. In this vein, our research introduces a minimax instance optimal learner for realizable regression in both the probably approximately correct (PAC) learning setting and the online learning setting, marking a significant stride toward bridging theoretical concepts with practical learning applications. Our approach is deeply rooted in optimization, leveraging mathematical functions to model complexities and devise solutions that enhance regression analysis effectiveness. We propose a novel dimension that both qualitatively and quantitatively characterizes which classes of real-valued predictors or classifiers are learnable, extending the dialogue initiated by notable precedents in the field. The identification of a combinatorial dimension related to the graph dimension for ERM learnability in the realizable setting, alongside establishing a necessary condition for learnability based on a dimension akin to the DS dimension, punctuates our contributions. Also, in addressing a question by Daskalakis and Golowich, we delineate a dimension determining the minimax instance optimal cumulative loss in online learning, which serves as a cornerstone for the design of an optimal online learner for realizable regression.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Murad_Tukan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=Bj1QSgiBPP",
  "title": "Participatory Personalization in Classification",
  "modified_abstract": "The development and integration of machine learning models into diverse applications necessitate addressing the critical dimensions of consent and privacy, especially when personalizing services with sensitive, protected, or self-reported data. These considerations take precedence as models increasingly handle data related to individuals without facilitating their consent or informing them of the benefits derived from personalization. Building upon previous works that aim to measure disparities among population groups within public health, economics, and machine learning, our research introduces a new paradigm in predictive modeling. We propose participatory systems\u2014a family of prediction models that allow individuals to opt into personalization at prediction time. This shift towards a model-agnostic algorithm furthers the ability to personalize with categorical group attributes while addressing the crucial aspect of informed consent in computational systems. Our empirical study, focused on clinical prediction tasks, benchmarks participatory systems against common personalization and imputation approaches, evidencing that these systems not only enhance performance across diverse groups but also significantly improve privacy and consent processes, thereby addressing the identified disparities and operationalizing fairness in predictive analytics. Surveys conducted as part of our empirical study provide qualitative data supporting the effectiveness of participatory personalization in reducing disparity and improving user engagement with machine learning applications. This aligns with overarching goals in machine learning and public health to tailor interventions and services ethically and effectively, marking a step forward in operationalizing fairness and privacy in predictive analytics.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Harvineet_Singh1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=Sf17j2pkCU",
  "title": "Optimistic Exploration in Reinforcement Learning Using Symbolic Model Estimates",
  "modified_abstract": "In light of recent exploratory efforts in reinforcement learning (RL) that utilize symbolic models to enhance learning efficiency and decision-making\u2014such as the optimization of multi-step off-policy learning approaches in distributional RL\u2014our study introduces a novel methodology that transcends the conventional reliance on pre-existing symbolic models. By proposing an innovative mechanism for learning optimistic symbolic approximations of the world model, this research not only addresses the limitations of previous works but also synergizes with fast diverse planners from the automated planning community to facilitate optimistic exploration in environments characterized by sparse rewards. Our method focuses on accelerating the learning process by generalizing learned model dynamics across similar actions and practices with minimal human intervention. The evaluation conducted across multiple benchmark domains, and subsequent comparisons with existing RL strategies that include deep learning, off-policy, and distributional approaches, further validate the efficacy of our approach in both theory and practice. This investigation not only broadens the scope of symbolic reinforcement learning but also demonstrates significant steps forward in addressing the challenge of sparse reward settings through the lens of optimistic exploration and symbolic model estimation, with experimental guarantees of improved performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yunhao_Tang1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=konBXvt2iS",
  "title": "Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of ReLU Networks",
  "modified_abstract": "Building on foundational insights from the analysis of deep linear networks, which revealed crucial aspects of the optimization landscape and the impacts of initialization and architecture on reaching global minima, our research extends into the complex domain of ReLU networks. The training process of ReLU neural networks often exhibits complicated nonlinear phenomena. The nonlinearity of models and non-convexity of loss pose significant challenges for theoretical analysis, paramount in understanding the behaviors of neurons within these networks. Therefore, most previous theoretical works on the optimization dynamics of neural networks focus either on local analysis (like the end of training) or approximate linear models (like Neural Tangent Kernel). In this work, we conduct a complete theoretical characterization of the training process of a two-layer ReLU network trained by Gradient Flow on a linearly separable dataset. This specific setting, beginning from random initialization to final convergence, underscores how the decay of the learning rate interacts with these dynamics. Our analysis not only addresses methods applied during this process but also delves into the initialization phase's critical role in determining the trajectory of the optimization. Despite the relatively simple model and data that we studied, we reveal four different phases from the whole training process showing a general simplifying-to-complicating learning trend. Specific nonlinear behaviors can also be precisely identified and captured theoretically, such as initial condensation, saddle-to-plateau dynamics, plateau escape, changes of activation patterns, learning with increasing complexity, etc.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Liu_Ziyin1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=FFdrXkm3Cz",
  "title": "On the spectral bias of two-layer linear networks",
  "modified_abstract": "Inspired by recent advancements in understanding optimization algorithms through the lens of dynamic systems, particularly focusing on first-order optimization inspired from finite-time convergent flows, this paper examines the behavior of two-layer fully connected networks with linear activations trained with gradient flow on the square loss. We delve into the optimization process, revealing an implicit bias on the parameters that is contingent upon the scale of its initialization and examining the flow dynamics including the signed-gradient aspects within the training process. The core contribution of this study is a variational characterization of the loss minimizers retrieved by the gradient flow, particularly for a specific shape of initialization. This characterization uncovers a propensity towards a low-rank configuration of the neural network's hidden layer in the regime of small scale initialization. Furthermore, we introduce a concept of hidden mirror flow to elucidate the dynamics of the singular values of the weights matrices, detailing their temporal evolution within the flow framework. The theoretical results are substantiated with numerical experiments that illustrate the described phenomena, thereby enriching our comprehension of the spectral properties inherent to linear neural networks trained under gradient descent.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Orlando_Romero1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=Vx1JadlOIt",
  "title": "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks",
  "modified_abstract": "Recent work in the field of computer vision, such as OVRL-V2, which presents a single neural network architecture achieving state-of-the-art results on visual navigation tasks through simple, task-agnostic components, highlights the evolving landscape of models with versatile applicability. Inspired by these advancements and the recognition of the limited open-ended task capabilities of current vision foundation models (VFMs), we introduce VisionLLM, an LLM-based framework designed to bridge this gap by conceptualizing images as a form of foreign language. This novel approach leverages the flexibility of LLMs in handling user-tailored, language-based tasks and applies it to vision-centric tasks, providing a unified perspective that aligns these tasks with language instructions for an LLM-based decoder to process. Through extensive experimentation and training protocols, incorporating pre-training and neural network optimization techniques, VisionLLM demonstrates its capability to customize tasks dynamically, ranging from detailed object-level instructions to broader, task-level directives, all while achieving competitive results. Specifically, VisionLLM's achievement of over 60% mean average precision (mAP) on the challenging COCO dataset, showcases its potential today to match and even exceed detection-specific models without being confined to predefined tasks or traditional convolutions and compression methods. Moreover, VisionLLM's adoption of patch-level analysis and the conceptual shift in navigation through visual tasks, further indicates its promise. This exploration not only contributes to the growing interface between vision and language modeling but also sets a new precedent for future developments in generalist models that are adaptable across a broad spectrum of vision and language tasks, incorporating planning and flexibility. The forthcoming release of our implementation code aims to facilitate further research and advancements in this promising domain.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Arjun_Majumdar2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=vf77fTbgG3",
  "title": "Structured Voronoi Sampling",
  "modified_abstract": "In light of recent progress in gradient-based Markov Chain Monte Carlo (MCMC) methods for optimizing proposal distributions in high-dimensional spaces, our work introduces a novel gradient-based sampling algorithm tailored for text generation, addressing the gap in theoretically grounded approaches within this domain. This step towards structured, principled sampling is inspired by advancements in adaptive MCMC techniques that leverage gradient information for enhanced sampling efficiency and distribution approximation. Our method, which we term Structured Voronoi Sampling (SVS), applies discrete distributions provided by language models to define densities, utilizing a Hamiltonian Monte Carlo based algorithm for sampling. In rigorous experimental comparisons where the reference distribution is known to be intractable, SVS demonstrates superior alignment with the reference distribution over alternative sampling methods, providing strong evidence of its effectiveness. Moreover, in tasks requiring controlled text generation, SVS achieves fluent and diverse output in adherence to specified control targets, outperforming existing methods in terms of generating quality and diversity robustly. This alignment with controlled objectives illustrates SVS's potential in enhancing the granularity and precision of text generation processes, underlining its significant contribution to the learning mechanism of modern AI systems. The use of chains in our methodology is critical for navigating the complexity of high-dimensional spaces robustly, enabling a more structured approach to explore and generate text, firmly establishing SVS as a valuable tool for generating proposals in complex learning scenarios where direct sampling is often intractable.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Michalis_Titsias1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=8HzOyg1ngp",
  "title": "Efficient Subgame Refinement for Extensive-form Games",
  "modified_abstract": "In the landscape of computational game theory, recent advancements in independent policy gradient methods for Markov potential games have highlighted the potential for scalable learning algorithms even in environments with large state spaces and numerous players. Inspired by these developments, which demonstrate the feasibility of achieving Nash equilibria with reduced complexity in large-scale settings, our research introduces an innovative approach to subgame solving in extensive-form games. Specifically, we propose a generative subgame solving (GS2) framework, deploying a generation function to strategically identify a subset of earliest-reached nodes, effectively curtailing the size of the subgame without sacrificing the integrity of the strategy refinement process. This framework not only leverages a diversity-based generation function but also utilizes reinforcement learning techniques and sample-based approximation methods to ensure safety and robustness in strategy selection. Through our theoretical analysis and practical experiments on medium-sized games as well as the complex game of GuanDan, we showcase the efficacy of learning through the GS2 framework in navigating the challenges posed by large imperfect information games. Our results illustrate significant performance enhancements over traditional blueprint strategies, and the policy gradient-based method provides a promising avenue for research into efficient, scalable game-solving methodologies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chen-Yu_Wei1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=afKnrwJBAl",
  "title": "Cross-Episodic Curriculum for Transformer Agents",
  "modified_abstract": "Inspired by advances in meta-reinforcement learning (meta-RL) that address the challenges of data efficiency and policy generality in deep reinforcement learning (RL), we introduce the Cross-Episodic Curriculum (CEC) algorithm. This algorithm is designed to enhance the learning efficiency and generalization capabilities of Transformer agents, leveraging the concept of cross-episodic experiences to enrich a Transformer\u2019s context and curriculum. By arranging online learning trials and mixed-quality demonstrations in a sequential structure, CEC creates a learning framework that mirrors the progression and proficiency increase seen in meta-RL, but with a focus on cross-episodic memory and attention. This model showcases its effectiveness in two distinct scenarios: multi-task reinforcement learning with widespread, discrete control challenges, as demonstrated in environments like DeepMind Lab, and imitation learning with mixed-quality data for continuous control settings, exemplified by RoboMimic. The results from these scenarios indicate that policies trained using CEC and related algorithms outperform traditional methods in terms of performance and generalization to unassociated tasks. To support further research within this machine learning domain, a survey of existing approaches is offered, and the code for our experiments has been made publicly available. [The URL for the GitHub repository has been omitted for privacy].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Risto_Ilkka_Antero_Vuorio1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=sTjW3JHs2V",
  "title": "Let the Flows Tell:  Solving Graph Combinatorial Problems with GFlowNets",
  "modified_abstract": "Combinatorial optimization (CO) problems, characterized as often NP-hard and challenging for exact algorithms, find a promising avenue for application of machine learning methods to sample and solve instances with binary decision variables. These are inspired by both classical approaches to binary integer and linear programming as well as recent strides in other domains of optimization like online learning. GFlowNets have emerged as a novel solution, demonstrating the capacity to efficiently sample from composite unnormalized densities in a sequenced manner\u2014a critical capability for addressing the structured constraints inherent in CO problems and generating diverse solution candidates efficiently, including those requiring permutational decision-making. In this work, we design Markov decision processes (MDPs) tailored to various combinatorial problems and introduce training strategies for conditional GFlowNets, enabling effective sampling from the solution spaces of these problems. Our proposed methods also focus on improving long-range credit assignment to enhance training efficiency and minimize regret in sequential decision-making. The efficacy of GFlowNet policies in finding high-quality solutions is substantiated through comprehensive experiments across a range of CO tasks, employing both synthetic and real-world data. This paper also contributes to the field by making our implementation available for public use, thereby inviting an open review of our approaches and facilitating further advancements in algorithms for CO. The inclusion of randomized techniques ensures robust and diversified outcomes, aligning well with the principle of minimizing regret in online decision scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chunlin_Sun1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=pvPujuvjQd",
  "title": "Most Neural Networks Are Almost Learnable",
  "modified_abstract": "Inspired by recent developments in the PAC-Bayesian learning framework, particularly in understanding generalisation bounds for scenarios with unbounded loss functions, our work explores the learnability of neural networks through a novel lens. We present a PTAS for learning random constant-depth networks. This exploration delves into the interval of possible improvements in the theoretical underpinnings of machine learning, particularly focusing on regression tasks where unbounded loss functions play a crucial role. We show that for any fixed $\\epsilon>0$ and depth $i$, there is a poly-time algorithm that for any distribution on $\\sqrt{d} \\cdot \\mathbb{S}^{d-1}$ learns random Xavier networks of depth $i$, up to an additive error of $\\epsilon$. The algorithm, showcasing the relevance of the PAC-Bayes framework for new scenarios in learning, operates with a time and sample complexity of $(\\bar{d})^{\\mathrm{poly}(\\epsilon^{-1})}$, indicating that most neural networks are, indeed, almost learnable under these conditions. The integration of unbounded losses within the PAC-Bayes framework significantly advances our understanding and offers a broader theoretical framework by which we can analyze neural network learnability under certain activation functions and depths.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Maxime_Haddouche1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=dnGEPkmnzO",
  "title": "Fully Dynamic $k$-Clustering in $\\tilde O(k)$ Update Time",
  "modified_abstract": "In light of recent explorations into the complexities of community detection through sub-hypergraph and stochastic models, our investigation addresses the dynamic aspect of clustering problems, presenting a $O(1)$-approximate fully dynamic algorithm for the $k$-median and $k$-means problems on metric spaces. This algorithm demonstrates significant advantages in managing communities within evolving data sets, highlighting its effectiveness in multipartite recovery and community detection within large, dynamically changing environments. With an amortized update time of $\\tilde O(k)$ and a worst-case query time of $\\tilde O(k^2)$, this study extends beyond static analysis to accommodate evolving data sets effectively, signaling a notable advancement in the multipartite recovery of dynamic communities. Our theoretical discourse is enriched by experimental analyses, marking a pioneering in-depth study for the dynamic $k$-median problem on general metric spaces and establishing a comparative framework with the cutting-edge works of Henzinger and Kale [ESA'20]. Further, a foundational contribution of our work lies in furnishing a lower bound for the dynamic $k$-median problem, evidencing that any $O(1)$-approximate algorithm with $\\tilde O(\\text{poly}(k))$ query time inherently requires $\\tilde \\Omega(k)$ amortized update time, even under incremental conditions. This revelation resonates with the findings on the limitations imposed by information theoretic bounds and the role of probability in community detection, thereby integrating practical algorithmic developments with theoretical insights from the related domain.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chuyang_Ke2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=EETqXXdqkI",
  "title": "PICProp: Physics-Informed Confidence Propagation for Uncertainty Quantification",
  "modified_abstract": "Inspired by the emerging synergy between deep learning and physics-driven models, this paper addresses the persistent limitations of uncertainty quantification in these realms. Our work is deeply influenced by foundational efforts that have leveraged physics-informed machine learning techniques, such as the coupling of PyTorch and Firedrake for PDE-based modeling, to imbibe models with the capability to adhere to physical laws while being efficient in data usage and fostering high-performance computation. Standard approaches in both deep learning and physics-informed learning have been bounded by their dependency on data likelihood assumptions, prior selection, and the approximation of posteriors, often leading to computationally expensive and poor approximations. In response, this paper presents a method for estimating confidence intervals (CIs) for deterministic partial differential equations, a novel problem, by proposing Physics-Informed Confidence Propagation (PICProp), which utilizes bi-level optimization to propagate confidence, in the form of CIs, from data locations to the entire domain with probabilistic guarantees, without relying on heavy assumptions. This modelling approach is further enhanced by considering the coupled nature of physical systems and how this complexity can be managed through PICProp. Furthermore, we provide a theorem that establishes the validity of our method and demonstrate its application through computational experiments focused on physics-informed learning and training. This initiative strives to contribute to the ongoing discourse on integrating physical principles into machine learning models to enhance their interpretability and reliability, thereby encouraging the composition of knowledge from disparate disciplines. The high-performance capabilities of this approach are underscored by its potential for use in simulations that require rigorous training of machine learning models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nacime_Bouziani1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=wiv21EJ0Vd",
  "title": "Zero-shot Visual Relation Detection via Composite Visual Cues from Large Language Models",
  "modified_abstract": "Inspired by pioneering work in vision-language models, such as InternLM-XComposer, which advanced the state-of-the-art in image-text comprehension and composition, we address the challenge of zero-shot visual relation detection (VRD). Pretrained vision-language models, such as CLIP, have exhibited strong generalization capabilities, positioning them as valuable assets for zero-shot visual recognition tasks, and specifically for bridging text-image gaps. VRD, a task focused on identifying types of relationships or interactions between object pairs within images, faces challenges when applying these models directly, including difficulties in distinguishing between fine-grained relation types and overlooking essential spatial relationships between objects. Our novel approach, RECODE (RElation detection via COmposite DEscription prompts), aims to overcome these limitations through a synergistic composition of visual and textual data. RECODE decomposes predicate categories into subject, object, and spatial components and utilizes large language models to generate descriptive prompts, or visual cues, for each element. These cues significantly improve the discrimination of similar relation categories by offering varied perspectives. Additionally, we employ a deep chain-of-thought method with large language models to assign reasonable weights to different visual cues, enhancing RECODE's performance on four VRD benchmarks. This deep learning method not only demonstrates improved effectiveness in zero-shot VRD but also offers a novel, interpretable, and potentially manuscript-worthy framework that may inform future developments in the field, possibly extending to immersive multilingual comprehension contexts.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shuangrui_Ding1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=I18BXotQ7j",
  "title": "GeoCLIP: Clip-Inspired Alignment between Locations and Images for Effective Worldwide Geo-localization",
  "modified_abstract": "Inspired by recent breakthroughs in vision-language models and their significant impact on tasks such as open-world instance-level 3D scene understanding, which leverages pre-trained models to bridge the gap between visual and linguistic information, our work introduces GeoCLIP, a novel approach that applies these insights to the domain of worldwide geo-localization. Geo-localization aims to pinpoint the precise location of images taken anywhere on Earth, presenting challenges due to geographic diversity and the impracticality of image-to-image retrieval at a global scale. Traditional methods, which rely on dividing the globe into discrete geographic cells for categorization, often lead to inaccuracies due to the limited resolution of predefined classes. GeoCLIP overcomes these limitations by enforcing alignment between images and their corresponding GPS locations, using a CLIP-inspired Image-to-GPS retrieval task approach. This involves modeling the Earth as a continuous function with a location encoder that employs positional encoding through random Fourier features. Our hierarchical representation captures information at varying resolutions, yielding a semantically rich high-dimensional feature space suitable for geo-localization. This understanding enhances the model's ability to process unlabeled data effectively, marking this as the first initiative to employ GPS encoding for this categorization purpose, demonstrating the method's efficacy through extensive experiments and ablations on benchmark datasets, establishing a new form of supervision for geo-localization tasks. With competitive performance achieved using just 20% of the training data, GeoCLIP proves effective even in limited-data scenarios and introduces the ability to perform geo-localization using text queries, thereby expanding the perception of location-aware applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jihan_Yang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=oMm1dfo3tK",
  "title": "Unbiased constrained sampling with Self-Concordant Barrier Hamiltonian Monte Carlo",
  "modified_abstract": "Motivated by recent advancements in graph theory and sampling methods, particularly focusing on non-adaptive edge counting and sampling techniques, this paper introduces Barrier Hamiltonian Monte Carlo (BHMC), an innovative version of the Hamiltonian Monte Carlo (HMC) algorithm. BHMC is designed for sampling from a Gibbs distribution $\\pi$ on a manifold $\\mathsf{M}$, endowed with a Hessian metric $\\mathfrak{g}$ derived from a self-concordant barrier. The development of BHMC is a response to the necessity of incorporating constraints defining $\\mathsf{M}$ while exploiting its underlying geometry and ensuring connectivity across the sampling space. Traditional Hamiltonian dynamics, defined by separable Ordinary Differential Equations (ODEs) in the Euclidean case, introduces bias when generalized to Riemannian manifolds due to their non-separable nature. To overcome this, we propose a novel 'involution checking step' integrated into two versions of BHMC, termed continuous BHMC (c-BHMC) and numerical BHMC (n-BHMC). This procedural addition ensures that the generated Markov chains are reversible with respect to $\\pi$ and are unbiased, differentiating our approach from previous implementations. The effectiveness and unbiased nature of our proposed algorithms are validated through numerical experiments targeting distributions defined on polytopes, offering significant implications for estimation techniques in bounded domains.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Raghavendra_Addanki1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=wwkQUiaKbo",
  "title": "Adapting Fairness Interventions to Missing Values",
  "modified_abstract": "Our inquiry into the realm of algorithmic fairness is significantly informed by prior work on the statistical nuances of multi-label classification (MLC) in sparse label regimes. This foundational understanding lays the groundwork for our exploration of how missing values, a common yet complex issue in real-world data, impact algorithmic fairness. Different demographic groups may be unequally affected by missing data, leading to exacerbated discrimination through the standard \"impute-then-classify\" procedure. In this study, we demonstrate that training classifiers on imputed data can considerably deteriorate both group fairness and average accuracy due to the loss of information inherent in the missing data patterns. We propose scalable and adaptive algorithms for fair classification that effectively accommodate missing values without sacrificing the information they contain. These algorithms seamlessly integrate with existing fairness interventions, offering a more equitable and accurate classification across diverse datasets. Employing rigorous bounds for regret minimization, our numerical experiments showcase the superiority of our approach over traditional impute-then-classify methods, affirming its applicability and effectiveness in promoting fairness in natural machine learning applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Krzysztof_Dembczynski1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=mmmd2vp0n0",
  "title": "Transient Neural Radiance Fields for Lidar View Synthesis and 3D Reconstruction",
  "modified_abstract": "Inspired by recent advancements in machine learning for enhancing underexposed photos through deep illumination estimation, this work extends the applications of neural radiance fields (NeRFs) to model scene appearance and geometry from multiview imagery with a novel integration of lidar data. While prior attempts have utilized lidar-derived point clouds as supplementary information, they have not fully leveraged the unique image formation capabilities of lidar technology. Our method introduces transient NeRFs, which utilize raw, time-resolved photon count histograms from a single-photon lidar system, rendering these histograms from novel viewpoints and capturing transient light transport phenomena at unprecedented picosecond timescales. This approach employs a time-resolved adaptation of the volume rendering equation specific to lidar measurements, marking a significant departure from conventional NeRF techniques with refined loss functions for better accuracy. Evaluated on a pioneering benchmark dataset of simulated and real transient multiview scans obtained from a prototype single-photon lidar system, our method not only enhances the rendering of transient imagery by improving lighting conditions but also recovers superior geometry and appearance, including color enhancement, from sparse viewpoints compared to existing lidar-supervised NeRFs. Such advancements hold immense potential for simulating raw lidar data in applications crucial for autonomous driving, robotics, and remote sensing, thereby expanding the scope of NeRFs to include time-resolved imaging dimensions and exemplify the deep network's learning capability.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ruixing_Wang2",
  "manipulated_ranking": 12,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=dAbGv5Jz5U",
  "title": "Contrastive Sampling Chains in Diffusion Models",
  "modified_abstract": "Inspired by the success of recent methodologies in diffusion models (DMs) for generative tasks, such as the development of Gaussian Mixture Solvers (GMS) for enhancing sampling efficiency and quality in image generation and stroke-based synthesis, our research introduces a novel approach to address a fundamental limitation in diffusion models concerning discretization error. The past few years have witnessed great success in the use of DMs to generate high-fidelity images for a variety of tasks with the help of stochastic differential equations (SDEs). However, discretization error is an inevitable limitation when utilizing numerical solvers to solve SDEs for mixture models. To address this limitation, we provide a theoretical analysis demonstrating that an appropriate combination of the contrastive loss and score matching serves as an upper bound of the KL divergence between the true data distribution and the model distribution, harnessing probability theory to enhance our understanding of these mechanisms. Utilizing inference steps enhanced by our contrastive loss, we construct a contrastive sampling chain for fine-tuning the pre-trained DM. This method effectively reduces the discretization error and thus yields a smaller gap between the true data distribution and our model distribution for various generative tasks. Moreover, the presented method can be applied to fine-tuning various pre-trained DMs, both with or without fast sampling algorithms, contributing to better sample quality or slightly faster sampling speeds for these tasks. To validate the efficacy of our method, we conduct comprehensive experiments, including leveraging stroke-based details in visual tasks, and applying our method to a pre-trained EDM. It improves the FID from 2.04 to 1.88 with 35 neural function evaluations (NFEs), and reduces NFEs from 35 to 25 to achieve the same 2.04 FID.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chongxuan_Li1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=8uOZ0kNji6",
  "title": "Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts",
  "modified_abstract": "Amidst the backdrop of advancements in natural language processing as evidenced by efficient text alignment methods using cross-document attention, our work introduces a novel approach for distinguishing between human and AI-generated texts\u2014a task gaining urgency with the rapidly improving quality of AI-generated content. Leveraging the foundational insights into text alignment and attention mechanisms as stepping stones, this paper identifies a unique invariant in human texts: the intrinsic dimensionality of the manifold underlying text embeddings. We demonstrate that for several alphabet-based languages, the average intrinsic dimensionality of fluent natural language texts is around 9, whereas for Chinese it is around 7. In contrast, AI-generated texts exhibit an average intrinsic dimensionality approximately 1.5 lower for each language. This significant statistical separation enabled us to develop a score-based detector for AI-generated texts that remains efficacious across different text domains, generator models, and levels of human writer proficiency, surpassing state-of-the-art (SOTA) detectors in both model-agnostic and cross-domain detection scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xuhui_Zhou1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=TnTDiCppx5",
  "title": "Predict-then-Calibrate: A New Perspective of Robust Contextual LP",
  "modified_abstract": "The integration of contextual information into optimization tasks aligns with a broader trend in machine learning (ML) and predictive analytics, as evidenced by recent advancements in risk-sensitive decision-making models such as Risk-averse Heteroscedastic Bayesian Optimization (RAHBO). RAHBO exemplifies how modern ML approaches can accommodate risk considerations by trading off mean and input-dependent variance\u2014a principle that informs our work. In this paper, we extend these concepts into the realm of contextual optimization, also known as predict-then-optimize or prescriptive analytics, by framing an optimization problem that incorporates covariates (context or side information). Our contribution, a generic algorithm design paradigm named predict-then-calibrate, seeks to first develop a prediction model without concern for the downstream risk profile or robustness guarantee, and then apply calibration methods to quantify the prediction's uncertainty. Unlike existing methods, our approach does not limit the choice of prediction model, enabling the full utilization of off-the-shelf machine learning methods for learning robust prediction strategies. Moreover, by decoupling the prediction model from the calibration and uncertainty quantification processes, we introduce a data-splitting concept that enables the derivation of risk and robustness guarantees independent of the prediction model's choice. This paradigm is applicable to both risk-sensitive robust and risk-neutral distributionally robust optimization formulations, offering new generalization bounds for the contextual linear programming problem and providing insights into the hyperparameter tuning and acquisition functions in benchmark scenarios. Numerical experiments support the benefits of our predict-then-calibrate approach, demonstrating that enhancements in either the prediction or calibration model can significantly boost overall performance, potentially leading to lower regret in decision-making scenarios. Furthermore, the empirical return from investments in improving the benchmark models is substantive, establishing a compelling argument for this methodology.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Anastasia_Makarova1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=JZfg6wGi6g",
  "title": "Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time",
  "modified_abstract": "With the advent of large language models (LLMs) ushering in unprecedented AI capabilities, the associated computational and memory overheads have become a focal point of research. This exploration is especially pertinent when considering the scalability of models like GPT-2 and GPT-3 in diverse applications from generative text to controlled sequence generation, as showcased in prior works such as GeDi, which leverages generative discriminators for efficient text generation with minimal toxicity via class-conditional techniques. Drawing inspiration from the inherent challenges and successes highlighted in these studies, this paper introduces 'Scissorhands,' a novel approach targeting the reduction of memory usage at inference time through selective training methodologies. The initiative centers on the hypothesis of 'persistence of importance,' suggesting that only key tokens significantly influence future generations during the natural text generation process. Scissorhands, through empirical validation and theoretical analysis, innovatively manages key-value (KV) cache memory by selectively retaining pivotal tokens, thereby allowing for up to 5\u00d7 reduction in inference memory usage of the KV cache without sacrificing model quality. Additionally, when combined with techniques such as 4-bit quantization\u2014commonly applied for model weight compression\u2014Scissorhands further extends potential memory efficiency, achieving up to a 20\u00d7 compression ratio through distribution-aware approaches. This approach not only addresses critical bottlenecks in deploying large-scale AI models but also opens up new avenues for enhancing model efficiency and applicability in real-world scenarios, particularly in linguistic contexts where decoding, training optimizations, and control strategies are most beneficial.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Bryan_McCann1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=zDbsSscmuj",
  "title": "Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning",
  "modified_abstract": "Inspired by recent works that explore compositional generalization through object-oriented world modeling in reinforcement learning, our research investigates the application of pre-trained large language models (LLMs) in the domain of task planning. These preceding studies underscore the potential of LLMs in understanding and generating complex structures and dynamics, which is foundational for our approach. We propose an innovative paradigm that constructs explicit world (domain) models in the planning domain definition language (PDDL) and leverages these models for planning with domain-independent planners, addressing the limitations of direct LLM application in planning due to issues such as the correctness of plans and the need for extensive human or simulator feedback. By integrating LLMs for both initial model construction and as intermediaries for incorporating corrective feedback from PDDL validators or human input, our method enhances efficiency and reduces the need for continuous human oversight. This process involves dissecting and generalizing problem instances within these built models, showcasing through the application of GPT-4 in creating detailed PDDL models and effectively solving planning tasks within two International Planning Competition (IPC) domains and a more complex Household domain. Our findings contribute to the field by demonstrating a synergistic use of LLMs and traditional planning frameworks, aiming for a balance between autonomous planning capabilities and human-guided corrections to optimize task planning efficiency.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Linfeng_Zhao1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=IPNg84RF1k",
  "title": "Towards Characterizing the First-order Query Complexity of Learning (Approximate) Nash Equilibria in Zero-sum Matrix Games",
  "modified_abstract": "Inspired by previous research that highlights the challenges and advancements in learning equilibria in games, such as the integration of perturbation techniques and proximal methods for achieving convergence in monotone games, this paper focuses on zero-sum $K\\times K$ matrix games within the first-order query model. In this setting, players observe the expected pay-offs for all their possible actions under the randomized action played by their opponent, effectively utilizing feedback from these observations with periodic updates. Recent developments have demonstrated that $\\epsilon$-approximate Nash equilibria can be computed from significantly fewer queries than previously thought, sparking renewed interest in determining the optimal number of queries relative to both $\\epsilon$ and $K$. Our contributions include a full characterization of the query complexity for learning exact Nash equilibria ($\\epsilon=0$), showing that the required number of queries is linear in $K$. For $\\epsilon > 0$, we highlight the limitations of existing techniques, which often rely on gradient-based methods and the interjection of noise for establishing a matching lower bound, and introduce a novel approach for deriving lower bounds, which sheds light on the $\\tilde\\Omega(\\log(\\frac{1}{K\\epsilon})$ complexity for small enough $\\epsilon$. This work not only advances our theoretical understanding of the query complexity in learning Nash equilibria but also opens up avenues for future research to bridge the gap between current upper and lower bounds, empirically testing these theories.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kaito_Ariu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=irRHgjePdR",
  "title": "Improving Compositional Generalization using Iterated Learning and Simplicial Embeddings",
  "modified_abstract": "Current research endeavors in machine learning, such as those dealing with large-scale representation learning on graphs, indicate a growing interest in refining representation learning methodologies and achieving state-of-the-art performance without relying heavily on labeled data. These efforts, highlighted by advancements like Bootstrapped Graph Latents (BGRL), inspire our exploration into the compositional generalization capabilities of deep neural networks. Compositional generalization, the ability of an agent to generalize to unseen combinations of latent factors, presents substantial challenges for deep learning models, despite being an innate human capability. Cognitive science suggests \"iterated learning\" as a mechanism contributing to the development of this ability in humans, characterized by the balance between compressibility and expressivity. Incorporating both insights from cognitive science and the structural design of our approach, this work proposes leveraging iterated learning and simplicial embeddings to enhance the compositional generalization of deep networks. Simplicial embeddings facilitate the discretization of representations, aligning with an analysis on compositionality and Kolmogorov complexity, which are scalable solutions for improving the compositional generalization. We present evidence that our approach, inspired by principles underlying recent successes in representation learning on graphs and assessed across benchmarks in both vision tasks with identifiable latent factors and molecular graph prediction tasks with complex latent structures, substantially improves compositional generalization, demonstrating scalable performance enhancements.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mehdi_Azabou2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=fyfmHi8ay3",
  "title": "Template-free Articulated Neural Point Clouds for Reposable View Synthesis",
  "modified_abstract": "The Dynamic Neural Radiance Fields (NeRFs) framework has demonstrated significant potential in rendering detailed, time-evolving 3D scenes, inspired by advances in volume rendering and 3D model learning, such as those introduced in DIVeR. Leveraging this foundational work, which offers enhanced realism and editability in volume rendering by utilizing deterministic integration techniques, our research introduces a groundbreaking method that circumvents common limitations faced in animating and reposing complex scenes. Specifically, we tackle the challenges posed by the common reliance on backward deformation fields for reanimation, and the constraints of dynamic models, including low visual fidelity and prolonged reconstruction times. Our approach employs a point-based representation, an evolution from voxel-based models, coupled with Linear Blend Skinning (LBS) to simultaneously learn a dynamic NeRF and an underlying skeletal model, even from sparse multi-view video captures. The proposed forward-warping method leverages ray tracing and vectors computation to not only establish a new benchmark in visual fidelity for small and novel view and pose synthesis but also considerably diminishes the learning duration required by previous 3D models. By integrating semantics alongside with rendering techniques, we demonstrate the efficacy of our methodology across various articulated objects and datasets, we not only achieve reposable 3D reconstructions without the need for object-specific skeletal templates but also significantly broaden the application potential of NeRF in capturing and animating real-world scenes with unparalleled detail and efficiency.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Anand_Bhattad1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=4W9FVg1j6I",
  "title": "Structured State Space Models for In-Context Reinforcement Learning",
  "modified_abstract": "Inspired by recent advancements in topological Markov decision processes (TMDPs) that extend the capabilities of multi-objective optimization in continuous spaces and underpin reinforcement learning's (RL) exploration in complex environments, our research introduces a novel application of Structured state space sequence (S4) models. These models, which serve as deep approximators, have recently achieved state-of-the-art performance on long-range sequence modeling tasks and boast fast inference speeds and parallelizable training. This paper proposes a modification to a variant of S4 that enables us to initialize and reset the hidden state in parallel, significantly enhancing its utility in RL settings. We demonstrate that our modified architecture not only runs asymptotically faster than Transformers concerning sequence length but also outperforms RNNs on a straightforward memory-based task. Furthermore, when evaluated on a set of partially-observable environments relevant to navigation, our model surpasses RNNs in performance while operating over five times faster. By leveraging the model's proficiency in handling long-range sequences and the gradient methods for training, we realize robust results on a meta-learning task involving a randomly sampled continuous control environment, relevant for robotics, and a linear projection of the environment's observations and actions. Moreover, we exhibit the model's adaptability to out-of-distribution tasks, underscoring structured state space models' potential as a fast and effective solution for in-context RL tasks with multiple-objective criteria. Personal identifiable information has been removed from this abstract.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Stas_Tiomkin1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=7xlrdSOm3g",
  "title": "A Theory of Multimodal Learning",
  "modified_abstract": "In the ambit of machine learning, the concept of utilizing data from various modalities reflects a natural extension of human cognitive processes, a notion long embraced by philosophy and cognitive science. This interdisciplinary foundation, coupled with advances in understanding neural network interpretability and uncertainty through works such as the exploration of Neural Additive Models (NAMs) with Linearized Laplace Inference, sets the stage for our exploration into multimodal learning. Current investigations into multimodal machine learning have predominantly focused on empirical achievements, with much of the theoretical underpinnings either taken for granted or left unexplored. This paper addresses this gap by introducing a theoretical framework that elucidates the generalization capabilities of multimodal learning in a manner previously unarticulated, incorporating linear approximation techniques within neural networks and focusing on how sub-networks interact across modalities. Through rigorous analysis, we establish that multimodal learning algorithms can achieve superior generalization bounds compared to unimodal approaches, specifically by a factor of $O(\\sqrt{n})$, where $n$ stands for the magnitude of the sample size. This advantage becomes apparent in the presence of both connectivity and heterogeneity across different modalities, offering a substantial theoretical basis for the empirical findings that multimodal models can often surpass the performance of highly-optimized unimodal models in regression tasks, even in tasks where only a single modality is available. The additive nature of certain algorithms further contributes to this advantage by facilitating enhanced model interpretability, a critical aspect often overlooked in machine learning tasks, particularly in complex tasks requiring nuanced decision making.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kouroche_Bouchiat1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=paTESG8iSE",
  "title": "Kernel-Based Tests for Likelihood-Free Hypothesis Testing",
  "modified_abstract": "This investigation is situated within an emerging body of work exploring the statistical modeling and analysis of complex data distributions, exemplified by recent advancements in understanding non-centered mixtures of scaled Gaussian distributions. Given $n$ observations from two balanced classes, consider the task of labeling an additional $m$ inputs that are known to all belong to \\emph{one} of the two classes. Special cases of this problem are well-known: with complete knowledge of class distributions ($n=\\infty$) the problem is solved optimally by the likelihood-ratio test, a fundamental classifier; when $m=1$ it corresponds to binary classification; and when $m\\approx n$ it is equivalent to two-sample testing. The intermediate settings occur in the field of likelihood-free inference, where labeled samples are obtained by running forward simulations facilitated by a descent algorithm for optimization, and the unlabeled sample is collected experimentally. In recent work, it was discovered that there is a fundamental trade-off between $m$ and $n$: increasing the data sample $m$ reduces the amount $n$ of training/simulation data needed. In this work, we (a) introduce a generalization where unlabeled samples come from a mixture of the two classes -- a case often encountered in practice, thus necessitating an effective minimization algorithm; (b) study the minimax sample complexity for non-parametric classes of densities under \\textit{maximum mean discrepancy} (MMD) separation, a crucial classifier evaluation metric; and (c) investigate the empirical performance of kernels parameterized by neural networks on two tasks: detection of the Higgs boson and detection of planted DDPM (denoising diffusion probabilistic models) generated images amidst CIFAR-10 images. For both problems, we confirm the existence of the theoretically predicted asymmetric $m$ vs $n$ trade-off.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Antoine_Collas1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=RInTOCEL3l",
  "title": "Relax, it doesn\u2019t matter how you get there: A new self-supervised approach for multi-timescale behavior analysis",
  "modified_abstract": "This study is situated within the broader context of recent research that has made significant strides in understanding complex dynamics and improving prediction models in both constrained and unconstrained scenarios, such as SpeedyZero's advancements in deep reinforcement learning (RL) with efficient sample use, massive parallelization, and rapid training processes. Here, we address the challenge of analyzing unconstrained and natural behavior, which consists of dynamics complex and unpredictable over multiple future steps. Our work contributes to the field by introducing a multi-task representation learning model specifically designed for free and naturalistic settings, incorporating an action-prediction objective for forecasting action distributions over future timesteps and a multi-scale architecture to capture both short- and long-term dynamics. The efficacy of our model is demonstrated through its application to varied environments and terrains with robots, and its outstanding performance in the MABe 2022 Multi-Agent Behavior challenge, achieving top rankings across both mice and fly benchmarks. The application of batch processing and the reinforcement of our findings with a vast amount of sample data underline our model\u2019s potential for solving a broad spectrum of downstream tasks, offering new pathways for understanding complex behaviors across different timescales. The utility of parallelization and the substantial benefit of high-fidelity 300k sample training further reinforce the robustness and efficiency of our approach.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shaohuai_Liu1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=GhNCFtLSsy",
  "title": "Combining Behaviors with the Successor Features Keyboard",
  "modified_abstract": "The development of techniques for knowledge transfer across tasks in machine learning has been a pivotal area of research, with methodologies like Multitask Soft Option Learning (MSOL) exploring the landscape of hierarchical and multitask learning frameworks. Building on such foundational work, which leverages Options and Planning-as-Inference for task transfer and learning efficiency, we introduce the \"Successor Features Keyboard\" (SFK), an innovative approach designed to surmount the limitations of manually designing state-features and task encodings in transfer learning. To address these challenges, we propose the \"Categorical Successor Feature Approximator\" (CSFA), a novel algorithm for approximating Successor Features (SFs) that simultaneously discovers state-features and task encodings. Unlike existing methods, which rely on hand-designed components, SFK and CSFA facilitate the utilization of learned behaviors in complex 3D environments through automatic discovery, achieving unprecedented success in transfer learning with SFs. Our comparisons with other SF approximation methods demonstrate CSFA's unique capability to identify representations that are conducive to Successor Features and Generalized Policy Improvement (GPI) at an advanced scale. Further, our evaluation of SFK against standard transfer learning baselines reveals its superior efficiency in adapting to long-horizon tasks, thereby marking a significant advancement in the field of machine learning and knowledge transfer.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Maximilian_Igl1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=3Cj67k38st",
  "title": "HotBEV: Hardware-oriented Transformer-based Multi-View 3D Detector for BEV Perception",
  "modified_abstract": "In the context of autonomous driving systems, bird's-eye-view (BEV) perception is pivotal for the accurate and efficient detection and tracking of objects from a top-down perspective, necessitating low-latency computation for real-time decision-making. Existing approaches to BEV detection, including Lift-Splat-Shoot (LSS)-based or transformer-based schemas, though successful in improving detection precision and classification capabilities, introduce substantial computational and memory burdens, risking system stability under the load of concurrent on-vehicle tasks. This issue underscores a critical research gap in developing efficient BEV detector paradigms conducive to realism in speed enhancements. Our study, inspired by preceding efforts such as the creation of a large-scale event-based detection dataset with extensive motion recordings and over 255,000 annotations, and the innovative use of self-supervised learning techniques requiring fewer labels, ventures beyond mere computational cost reduction to delineate a model prioritizing on-device latency through a latency-aware design methodology that considers fundamental hardware characteristics such as memory access cost and parallelism in the automotive industry. Emphasizing GPUs - the predominant computational platform in autonomous driving - our work introduces a theoretical latency prediction model alongside efficient building operators, culminating in a hardware-oriented backbone architecture optimized for feature capture and fusion. Consequently, we propose HotBEV, an innovative hardware-tailored framework for camera-view BEV detectors, blending efficiency with accuracy. Empirical evaluations manifest HotBEV's superiority, achieving notable improvements in NDS and mAP alongside significant speedups across multiple GPU environments, illustrating its practical advantage and setting a new benchmark in the domain of BEV perception for autonomous driving.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Amos_Sironi2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=gjBk6IQofa",
  "title": "Creating Multi-Level Skill Hierarchies in Reinforcement Learning",
  "modified_abstract": "In reinforcement learning, the concept of skill hierarchies is instrumental for efficiently solving complex tasks. Inspired by the notion of multi-agent systems that dynamically adapt their strategies, as suggested by preliminary works on hierarchical reinforcement learning with dynamic termination, our study introduces a novel framework for constructing multi-level skill hierarchies in autonomous agents. Leveraging modularity maximization, our methodology provides a graphical representation of agent-environment interactions, revealing the inherent structure at various abstraction levels. This method automatically generates a comprehensive skill set, delineated by diverse time scales and operational dependencies without necessitating manual programming. These hierarchies not only align with intuitive understanding but also significantly enhance the agent's learning capacity across diverse scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dongge_Han1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=DD0QJvPbTD",
  "title": "ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned Samples in NLP",
  "modified_abstract": "Amid growing concerns over the integrity and reliability of machine learning models, particularly in natural language processing (NLP), the menace of backdoor attacks poses a serious threat. These attacks, where models are poisoned to misclassify inputs containing specific triggers, necessitate robust detection mechanisms. This paper is inspired by previous works, including advancements in NLP and automated fact-checking, which employ natural language processing and machine learning to address misinformation by understanding and dissecting the semantic content of text. Building on these insights, we propose ParaFuzz, an interpretability-driven framework for detecting poisoned samples in NLP applications. Our approach leverages the semantic interpretability of model predictions to identify discrepancies in the predicted labels of paraphrased inputs, a method that efficiently flags poisoned samples without altering their essential meaning. By utilizing ChatGPT for paraphrasing and adopting fuzzing techniques to refine prompts for trigger removal, we address covert backdoor strategies, including style-based attacks that current detectors struggle with. Our experimental results, spanning four types of backdoor attacks and diverse datasets, demonstrate ParaFuzz's superiority in precision and recall over existing methods like STRIP, RAP, and ONION, marking a significant step forward in safeguarding NLP models against sophisticated threats.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhijiang_Guo2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=aCOKUvqHtD",
  "title": "Polynomial-Time Linear-Swap Regret Minimization in Imperfect-Information Sequential Games",
  "modified_abstract": "In the quest for developing proficient algorithms for learning in imperfect-information games, prior advancements have notably paved the way through establishing frameworks for near-optimal learning in extensive-form games. Drawing inspiration from these pivotal contributions, particularly the development of algorithms achieving remarkably lower sample complexities for identifying approximate Nash equilibria in general-sum games, our paper extends the understanding of strategic learning in sequential games. We center our exploration on the concept of no-regret learning, specifically addressing the intricacies of regret minimization across actions with an elevated level of rationality determined by the breadth of strategy transformation functions contemplated by a learner. This involves leveraging bandit feedback mechanisms for iterative learning and examining various sets of actions within the framework. In doing so, we introduce the polynomial-time attainable concept of no-linear-swap regret in extensive-form games, an in-depth exploration into rationality that exceeds prior attainments in sequential games such as no-trigger-regret. Our findings illuminate the pathway to a subset of extensive-form correlated equilibria robust against linear deviations, termed linear-deviation correlated equilibria, offering a novel and efficient approach to reaching higher-order game-theoretic rationality in complex strategic settings through algorithms designed to minimize regret effectively.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Song_Mei1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=hz10oiVMNE",
  "title": "CWCL: Cross-Modal Transfer with Continuously Weighted Contrastive Loss",
  "modified_abstract": "Inspired by breakthroughs in unsupervised domain adaptation and the recognition of the continuous nature of similarity in data samples as highlighted in prior works such as Unsupervised Domain Adaptation via Discriminative Manifold Embedding and Alignment, this paper expands the frontier of cross-modal transfer learning. Our objective is to harness contrastive training for cross-modal 0-shot transfer, enabling a pre-trained model in one modality to be effectively used for representation learning in another domain using pairwise data. Deploying a novel contrastive loss function, Continuously Weighted Contrastive Loss (CWCL), our approach embraces the continuous spectrum of similarity among training examples, moving beyond the traditional binary treatment of similarity in classical contrastive learning methods. This allows for a finer-grained transfer of the manifold and discriminant embedding space structure from one modality to another. The adaptation process leverages this manifold perspective to effectively deal with the challenges posed by adversarial examples in unsupervised domain adaptation. The efficacy of CWCL is demonstrated through significant performance improvements over existing methods in 0-shot tasks, with our models achieving 5-8% (absolute) enhancement in 0-shot image classification and 20-30% (absolute) gains in 0-shot speech-to-intent and keyword classification using public datasets. Our work not only echoes the importance of considering the nuances in data similarity for enhancing learning transfer across modalities but also sets new benchmarks in zero-shot learning tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~You-Wei_Luo1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=JkmvrheMe7",
  "title": "On Single-Index Models beyond Gaussian Data",
  "modified_abstract": "Building on the premise that understanding the intricacies of machine learning models requires a deep dive into their fundamental building blocks, our work is inspired by the innovative methodologies proposed by recent research, such as exploiting the properties of characteristic curves in neural Ordinary Differential Equations (ODEs) for enhanced model understanding and performance. Just as the study on Characteristic-Neural Ordinary Differential Equations (C-NODEs) extends traditional frameworks by examining the evolution of latent variables along characteristic curves, our research ventures beyond the Gaussian data norm in studying single-index models. We delve into the behavior of gradient-descent methods using shallow neural networks on sparse high-dimensional functions, illuminating their aptitude for classification beyond linear models. Single-index models, represented as $f(x) = \\phi( x \\cdot \\theta^*)$ where labels are generated via an arbitrary non-linear link function $\\phi$ applied to an unknown one-dimensional projection $\\theta^*$ of the input data, serve as our focal point. Differential geometry comes into play as we explore the state space of these models, focusing on how the dynamical systems' state is altered through the flows of gradients along curves. Previous examinations, centered around Gaussian distributions, elucidate how the information density regulates sample complexity through the distribution's inherent stability and spherical symmetry. Our exploration expands this analysis to non-Gaussian contexts, where such stability or symmetry may not be present. Focusing on the planted scenario where $\\phi$ is known, we demonstrate that Stochastic Gradient Descent, a popular classification method, can identify the unknown direction $\\theta^*$ with a fixed probability in high-dimensional settings, under lenient conditions that markedly broaden the scope beyond existing Gaussian-focused findings and adapt to a variety of datasets.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xingzi_Xu2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=ginTcBUnL8",
  "title": "SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling",
  "modified_abstract": "In the context of rising interest in self-supervised pre-training for time series studies, driven by endeavors to mitigate labeling costs and enhance task performance, our research integrates insights from recent advances such as contrastive pre-training via time-frequency consistency. While the challenge of maintaining temporal fidelity during pre-training has been recognized, particularly due to shifts in temporal dynamics and the necessity for domain adaptation, our work, SimMTM, introduces a novel methodological pivot. We propose a Simple pre-training framework for Masked Time-series Modeling that addresses the fundamental limitation of conventional masked modeling\u2014its detrimental effect on vital temporal variations. By interpreting masked modeling through the lens of manifold learning, SimMTM innovatively suggests recovering masked time points through weighted aggregation from neighboring points, thereby preserving temporal continuity and enabling more effective representation learning and recognition of patterns across various modalities including hand-gesture activity in real-world scenarios. This method stands in contrast to and extends the underlying premise of time-frequency consistency by directly tackling the challenge of reconstructing masked segments without compromising temporal dynamics through domain adaptation. Our framework not only improves the fine-tuning performance across a variety of time series analysis tasks, including forecasting and classification in both in-domain and cross-domain settings but also marks a significant progression in the domain of time-series pre-training methodologies. The application of our framework in fault detection within industry processes highlights its potential for real-world impact.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Theodoros_Tsiligkaridis1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=2ccH4zjKVs",
  "title": "Robust Second-Order Nonconvex Optimization and Its Application to Low Rank Matrix Sensing",
  "modified_abstract": "This paper builds upon foundational principles from prior work on efficient computational methods, particularly in the realm of optimization and multi-object matching, where challenges like computational intensity and memory demands in adversarial settings are well acknowledged. Multi-object matching, a key task in structure-from-motion and synchronization of permutations, involves comparisons and synchronization of data structures across different viewpoints or datasets, leveraging state-of-the-art algorithms to enhance accuracy and efficiency. Finding an approximate second-order stationary point (SOSP) is a fundamental problem in stochastic nonconvex optimization, with broad applications in machine learning, offering a comparison with traditional convex approaches in terms of robustness and efficiency. However, understanding this problem in the presence of outliers remains elusive, thereby limiting the applicability of traditional nonconvex algorithms under adversarial conditions. We address this gap by focusing on the strong contamination model, wherein a constant fraction of data points are arbitrarily corrupted, creating a pressing need for the development of algorithms that are not only efficient in terms of computational and memory resources but also robust against such synthetic corruptions. Our contribution is a general framework for finding an approximate SOSP with dimension-independent accuracy guarantees, requiring \\\\(\\widetilde{O}({D^2}/{\\epsilon})\\\\) samples where \\\\({D}\\\\) is the ambient dimension and \\\\({\\epsilon}\\\\) is the fraction of corrupted datapoints. We apply this framework to low rank matrix sensing, offering efficient and provably robust algorithms capable of handling corruptions in both the sensing matrices and the measurements. Moreover, we establish a Statistical Query lower bound that suggests the quadratic dependence on \\\\({D}\\\\) in sample complexity is necessary for computationally efficient algorithms. This research not only tackles the challenge of robustness in nonconvex optimization but also extends the applicability of such methods to practical adversarial scenarios, drawing comparisons between these challenges and the synchronization of multiple objects or datasets in structure-from-motion tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shaohan_Li1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=zyhxRc9bew",
  "title": "What is Flagged in Uncertainty Quantification?  Latent Density Models for Uncertainty Categorization",
  "modified_abstract": "The evolving landscape of machine learning (ML) has witnessed a burgeoning interest in Uncertainty Quantification (UQ) methods, especially against the backdrop of increased sophistication in model diagnostics and reliability assessments. Informed by the rich theoretical underpinnings of semi-supervised learning, random matrix theory, and spectral analysis presented in prior works, our study ventures into novel territory by proposing a structured framework designed to elucidate and categorize uncertainties flagged by UQ processes. By introducing the concept of the confusion density matrix\u2014a kernel-based approximation tool for misclassification density\u2014we delineate a methodology that systematically classifies suspicious examples flagged by UQ techniques into three distinct categories: out-of-distribution (OOD) examples, boundary (Bnd) examples, and instances in zones of high in-distribution misclassification (IDM). Through comprehensive experimental evaluations and selection of significant scenarios for cross-validation, our research offers a fresh and insightful lens for discerning the differential capabilities of UQ methods, establishing a benchmark for their assessment. This framework not only builds upon the foundational studies that straddle supervised and unsupervised learning under the low density separation premise but also advances our collective understanding of how uncertain instances are detected and categorized within the realm of machine learning. We identify specific applications of our framework in the context of selection algorithms and least-square error minimization as part of the experimental validation, highlighting the utility of our proposed approach.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Malik_Tiomoko1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=bGcdjXrU2w",
  "title": "ATTA: Anomaly-aware Test-Time Adaptation for Out-of-Distribution Detection in Segmentation",
  "modified_abstract": "This research draws inspiration from recent advancements in dense out-of-distribution (OOD) detection approaches, such as those employed by FickleNet, which innovate in semantic image segmentation by leveraging feature map variations for enhanced localization precision in weakly and semi-supervised settings. These foundational contributions underline the importance of addressing both domain and semantic shifts in machine learning models, specifically in the context of diverse image segmentation. Acknowledging the limitations of current OOD detection models that operate under the assumption of uniform domains between training and testing datasets, this work proposes a dual-level OOD detection framework. This framework is designed to address domain shifts and semantic shifts concurrently while utilizing diverse global low-level features to distinguish between domain shifts and employs dense, high-level feature maps for identifying semantic shifts at the pixel level through random sampling techniques. Consequently, this allows for selective adaptation of the model to previously unseen domains and improves the model's capacity for detecting novel classes through refined classification techniques. The efficiency of our method, enhanced by deep neural network algorithms, is validated across multiple OOD segmentation benchmarks, encompassing scenarios with and without significant domain shifts. Annotations for classification uses have also played a critical role in refining the system's ability to accurately predict and adapt. Our findings indicate systematic performance enhancements over various baseline models. The code for this research has been made available online.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jungbeom_Lee1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=clCELP8zFb",
  "title": "Convergent Bregman Plug-and-Play Image Restoration for Poisson Inverse Problems",
  "modified_abstract": "Inspired by advancements in sparse training methods, such as the development of dynamic-sparse algorithms that efficiently train sparse networks to achieve or surpass the performance of more traditional dense networks through techniques such as pruning and layer-wise tuning of hyper-parameters, this work extends the concept of efficiency and adaptability to the realm of Plug-and-Play (PnP) methods for solving ill-posed image inverse problems. PnP methods, known for their adaptability, are efficient iterative algorithms that leverage deep Gaussian denoisers instead of conventional proximal operators or gradient-descent steps within proximal algorithms. However, current PnP schemes are designed around that of data-fidelity terms suited to noise models with Lipschitz gradients or closed-form proximal operators, leaving Poisson inverse problems\u2014a scenario where Gaussian noise models are inadequate and count-based statistics become important\u2014largely unaddressed. Our approach introduces the application of the Bregman Proximal Gradient (BPG) method, which employs Bregman divergence in place of the Euclidean distance to more accurately reflect the problem's smoothness properties. We develop the Bregman Score Denoiser, a novel component specifically parametrized and trained for Bregman geometry, demonstrating that it functions as the proximal operator of a nonconvex potential, akin to how tuning is approached in the optimization of networks. Through the proposal of two PnP algorithms incorporating the Bregman Score Denoiser for addressing Poisson inverse problems, we extend the convergence guarantees of BPG to nonconvex settings. The algorithms are shown to converge to the stationary points of a clearly defined global functional. Experimental validation on a range of Poisson inverse problems confirms our theoretical convergence claims and demonstrates superior image restoration performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Rajat_Vadiraj_Dwaraknath1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=25vRtG56YH",
  "title": "Beyond Normal: On the Evaluation of Mutual Information Estimators",
  "modified_abstract": "In light of the recent advances in neural network-based models for probabilistic prediction and uncertainty estimation, such as autoregressive quantile flows, our study addresses the critical assessment of mutual information estimators across diverse application domains including representation learning, causality, domain generalization, computational biology, and object detection. Mutual information, a widespread statistical dependency measure, has historically been evaluated on simple probability distributions, predominantly the multivariate normal distribution and certain one-dimensional variates. This research extends the scope by constructing a varied family of distributions with known mutual information values, including those relevant to flow-based models and training scenarios, facilitating a language-independent benchmark for evaluating the efficacy of both classically trained and neural mutual information estimators in detection and forecasting applications. The analysis covers various challenging scenarios such as high-density data, sparse interactions, series of long-tailed distributions, and cases with substantial mutual information. Our findings highlight the strengths and limitations of existing estimators and offer comprehensive guidelines for practitioners on choosing suitable estimators for specific problems, including considerations required when adapting these techniques to new datasets.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Phillip_Si1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=Uafbv4rfJc",
  "title": "Active Negative Loss Functions for Learning with Noisy Labels",
  "modified_abstract": "In the landscape of machine learning research, the pursuit of models resilient to data imperfections, such as noisy labels, has paralleled the exploration of efficient training mechanisms highlighted by techniques like dynamic sparse training, ensemble learning, and the utilization of subnetworks. These explorations provide a rich context for our investigation into robust loss functions, particularly through the lens of averaging processes in ensemble methods and requirements for efficient handling of data imperfections. Specifically, robust loss functions are essential for training deep neural networks in the face of noisy labels. Some robust loss functions utilize Mean Absolute Error (MAE) as a necessary component, including the recently proposed Active Passive Loss (APL) that incorporates MAE as its passive loss function. However, MAE's uniform treatment of samples can slow down convergence, complicate training, and impact prediction accuracy negatively. To address this, we propose an alternative class of theoretically robust passive loss functions, which we term *Normalized Negative Loss Functions* (NNLFs), emphasizing memorized clean samples rather than treating all samples equally and aligning with the sparse nature of meaningful data amidst noise. By substituting MAE in APL with NNLFs, we enhance the APL framework and introduce *Active Negative Loss* (ANL), a new framework that has demonstrated superior performance in experiments on both benchmark and real-world datasets compared to state-of-the-art methods, embodying the principles of ensembles and sparse learning for effective noise management. This paper removes the code availability statement for compliance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zahra_Atashgahi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=2rq4LwwjfE",
  "title": "What Do Deep Saliency Models Learn about Visual Attention?",
  "modified_abstract": "Inspired by the growing need to understand the inner workings of deep learning models, particularly in critical fields such as healthcare, and the methodology to refine and interpret saliency maps to unveil regions of interest, this paper extends the investigation into the domain of visual attention prediction by deep saliency models. Despite significant advancements in predicting human visual attention, the mechanisms behind the success of deep saliency models remain largely unexplained due to the opaque nature of deep neural networks. We present a novel analytic framework that illuminates the implicit features learned by saliency models and offers principled interpretation and quantification of their contributions to saliency prediction. Our approach decomposes these implicit features into interpretable bases that align explicitly with semantic attributes and reformulates saliency prediction as a weighted combination of probability maps connecting the bases and saliency. Through image-based learning processes, our framework conducts thorough analyses from several perspectives, including the semantics' positive and negative weights, the influence of training data and architectural designs, the roles of fine-tuning, and common error patterns observed in state-of-the-art deep saliency models. Furthermore, we apply our framework in various application scenarios, like examining the atypical attention patterns in individuals with autism spectrum disorder, exploring attention to emotion-eliciting stimuli, and investigating attention evolution over time. Our code is publicly available.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Florian_Bordes1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=8ox2vrQiTF",
  "title": "Self-Supervised Learning of Representations for Space Generates Multi-Modular Grid Cells",
  "modified_abstract": "This work is inspired by recent discoveries and methodologies in neural network initializations and optimization, particularly focusing on the peculiar efficacy of neural networks with stochastic gradient descent (SGD) from diverse random initializations. These insights, leveraging a blend of random initialisations and the systematic adjustment of gradients through stochastic gradient descent (SGD), provide a foundational understanding of the optimization landscape in which neural systems can find efficient solutions, an understanding that we extend to the domain of spatial representations in the mammalian lineage, specifically the development of grid cells. To solve the spatial problems of mapping, localization, and navigation, the mammalian lineage has developed striking spatial representations. One important spatial representation is the Nobel-prize winning grid cells: neurons that represent self-location, a local and aperiodic quantity, with seemingly bizarre non-local and spatially periodic activity patterns of a few discrete periods. Why has the mammalian lineage learned this peculiar grid representation? Mathematical analysis suggests that this multi-periodic representation has excellent properties as an algebraic code with high capacity and intrinsic error-correction, but to date, synthesis of multi-modular grid cells in deep recurrent neural networks remains absent, lacking an effective algorithm to mimic the natural process. In this work, we begin by identifying key insights from four families of approaches to answering the grid cell question: dynamical systems, coding theory, function optimization, and supervised deep learning. We then leverage our insights to propose a new approach that elegantly combines the strengths of all four approaches. Our approach is a self-supervised learning (SSL) framework - including data, data augmentations, loss functions, and a network architecture - motivated from a normative perspective, with no access to supervised position information. Without making assumptions about internal or readout representations, we show that multiple grid cell modules can emerge in networks trained on our SSL framework and that the networks generalize significantly beyond their training distribution, especially when employing convolutional architectures during the random initialisation process. This work contains insights for neuroscientists interested in the origins of grid cells as well as machine learning researchers interested in novel SSL frameworks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Johannes_Von_Oswald1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=28RTu9MOT6",
  "title": "Improving Graph Matching with Positional Reconstruction Encoder-Decoder Network",
  "modified_abstract": "Influenced by recent strides in understanding geometric relations in vision-centric tasks such as 3D object detection for autonomous driving, which leverage spatial and structural cues for feature extraction, our work explores a novel dimension in graph matching by focusing specifically on the spatial attributes of semantic keypoints. Developing from these insights, which highlight the importance of spatial information in achieving superior model performance, we introduce a sophisticated approach to semantic keypoint matching through the advent of a positional reconstruction encoder-decoder (PR-EnDec). This innovation aims to accurately model the intrinsic spatial structure of graphs that represent points and their cloud-like complex interrelations in images, addressing the gap in current methodologies that inadequately incorporate the relative spatial relationships of these keypoints as graph nodes. The PR-EnDec integrates a positional encoder that effectively captures node spatial embedding while ensuring affine transformation invariance, using pretrained models to boost its initial understanding. Moreover, a spatial relation decoder is employed to leverage high-order spatial information by reconstructing the locational structure of graphs from node coordinates, effectively understanding their geometric and positional context in image-based applications. We validate the effectiveness of our end-to-end graph matching network, named PREGM, through extensive experimentation across three public keypoint matching datasets, demonstrating PREGM's superior capability in identifying semantic keypoint correspondences. The possibility for pretraining the positional encoder is also contemplated to further enhance its spatial relational insight, using it as a foundational step for more complex detection and recognition tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Linyan_Huang3",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=P3Z59Okb5I",
  "title": "EvoFed: Leveraging Evolutionary Strategies for Communication-Efficient Federated Learning",
  "modified_abstract": "In the context of Federated Learning (FL), which aims at enabling collaborative model training across decentralized nodes without individual data sharing, the integration of Evolutionary Strategies (ES) offers a promising avenue to address the inherent challenge of high communication costs associated with transmitting model parameters. Inspired by recent advancements in FL, such as the FedICON framework's novel approach to tackle feature-level test-time shifts through contrastive learning, this paper introduces EvoFed. EvoFed innovates on the concept of `fitness-based information sharing', a significant deviation from traditional model-based FL practices, where clients can more efficiently train and share updates. By transmitting a distance-based similarity measure between the locally updated model and a synchronously generated population of noise-perturbed models, EvoFed minimizes the need to exchange actual model parameters. This method ensures that even with a smaller population size in comparison to the number of model parameters, the transmitted similarity measures (or fitness values) encapsulate nearly the complete information about the model updates. Consequently, this strategy not only facilitates a considerable reduction in the communication load but also maintains the efficacy of model updates across nodes within the machine learning domain. Our analytical and empirical evidence reveals that EvoFed not only converges but also achieves comparable performance to FedAvg, albeit with increased local processing demands, showcasing its potential in substantially lowering communication requirements across a variety of practical FL scenarios. This framework paves the way for taming the workloads on the client side by strategically balancing the training complexity with communication efficiency. The proposed system leverages intra-client exchanges to further enhance training efficiency and test performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Weiming_Zhuang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=gThGBHhqcU",
  "title": "Rethinking Conditional Diffusion Sampling with Progressive Guidance",
  "modified_abstract": "In the realm of generative models, particularly diffusion models and Generative Adversarial Networks (GANs), the quest for improving diversity and robustness while mitigating adversarial effects in generated images has led to significant methodological innovations. This paper builds upon these insights, specifically addressing the challenges of classifier guidance in diffusion generative models, characterized by a lack of diversity and the presence of adversarial effects due to over-aggressive discrimination gradients. By proposing a novel method termed Progressive Guidance, we aim to enrich the generative process by allowing for a more nuanced utilization of discriminative gradients, thus enhancing both diversity and robustness in the generated images. Our methodological innovation, Progressive Guidance, is designed to judiciously apply gradients from relevant neural classes during the initial, noisier phases of sample generation, progressively focusing and refining these gradients as the generation process approaches its final stages. This technique not only confronts the identified challenges head-on but also charts a new path forward in balancing the delicate trade-offs involved in conditional diffusion sampling. Experimental evaluations underscore the efficacy of Progressive Guidance, showcasing its ability to procure images of higher quality that embody a richer diversity and more robust features, thus marking an advancement over existing classifier guidance techniques through optimization of latent space utilization and generator dynamics. This progression is in tandem with and inspired by previous explorations into latent space adversarial learning and its impact on image generation diversity and quality, as well as the extensive data training required to refine these models. The addition of pixel-specific adjustments further enables the generation of high-quality, distant representations, setting the stage for further research into more refined generative methodologies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yichuan_Mo1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=xTgM7XLN9P",
  "title": "Compact Neural Volumetric Video Representations with Dynamic Codebooks",
  "modified_abstract": "Inspired by recent advancements in generative models and 3D-aware image synthesis, such as incorporating Generative Adversarial Networks (GANs) with Neural Radiance Fields (NeRF) for achieving high-fidelity 3D image generation, this work tackles the challenge of efficiently representing high-fidelity volumetric videos. Previous methods have demonstrated the effectiveness of fast learning implicit neural representations from 2D images, but these approaches typically result in large model sizes, especially for dynamic scenes capturing complex shape and appearance variations. Addressing this issue, we introduce a novel neural representation called the dynamic codebook. By leveraging synthesis techniques and the spatial and temporal redundancy intrinsic to feature grids due to the self-similarity of scenes and integrating prior knowledge in dynamic code creation, the dynamic codebook significantly reduces model size by merging similar features while compensating for any losses in rendering quality with dynamic codes. Tested on the NHR and DyNeRF datasets, our method not only achieves state-of-the-art rendering quality with its generator but also demonstrates superior storage efficiency and learning dynamics for complex scenes. The source code is available at a dedicated repository. (Note: The explicit URL to the GitHub repository has been omitted as requested.)",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yinghao_Xu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=y8UAQQHVTX",
  "title": "Private Everlasting Prediction",
  "modified_abstract": "Exploring the limitations and capabilities of private learning frameworks, as highlighted by the challenges of class imbalance in deep learning models, our study extends the narrative by examining the concept of private everlasting prediction. This research area leverages insights from past literature, including the increased sample complexity required for private learners over their non-private counterparts, notably in the context of one-dimensional threshold functions. We further the exploration into private prediction, which deviates from traditional learning by responding to a sequence of classification queries without a static hypothesis. This dynamic scenario necessitates a framework where a predictor updates its hypothesis over time while maintaining the privacy of both the initial training set and the queries it receives. Our contribution introduces private everlasting prediction, incorporating privacy considerations for the training set and adaptively chosen queries within the Probably Approximately Correct (PAC) model. We present a generic construction of such predictors, emphasizing their utility in environments with infinite domains and undefined private learning capacities. The construction's sample complexity is quadratic, adjusted for polylogarithmic factors, relative to the VC dimension of the concept class, enabling the prediction of threshold functions over infinite domains where traditional private learning faces inherent limitations. Additionally, this framework is poised to address out-of-distribution and extreme class imbalance queries, which represent extreme cases of class imbalance and dataset diversity. Our approach significantly broadens the potential applications of private networks, fostering advancements in dealing with out-of-distribution and extreme cases in machine learning domains.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Lue_Tao1",
  "manipulated_ranking": 8,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=gsglrhvQxX",
  "title": "Flow-Based Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection",
  "modified_abstract": "Cooperatively utilizing both ego-vehicle and infrastructure sensor data can significantly enhance autonomous driving perception abilities. However, the uncertain temporal asynchrony and limited communication conditions that are present in traffic environments can lead to fusion misalignment and constrain the exploitation of infrastructure data. Inspired by recent developments in aerial object detection, which address object orientation and efficient feature extraction through techniques such as rotation-equivariant networks, this study introduces the Feature Flow Net (FFNet), a novel cooperative detection framework for vehicle-infrastructure cooperative 3D (VIC3D) object detection. FFNet is a flow-based feature fusion framework that uses a feature flow prediction module to predict future features and compensate for asynchrony, akin to how aerial object detections have innovated in orientation and feature extraction to maximize performance. Unlike traditional methods that rely on the transmission of feature maps extracted from still images, FFNet transmits feature flow, leveraging the temporal coherence of sequential infrastructure frames and adaptively managing transmission to ensure efficient utilization of bandwidth. Furthermore, we introduce a self-supervised training approach that enables FFNet to generate feature flow with feature prediction ability from raw infrastructure sequences, utilizing techniques akin to attention mechanisms for enhancing detection accuracy. Experimental results demonstrate that our proposed method outperforms existing cooperative detection methods while requiring only about 1/100 of the transmission cost of raw data and covers all latency in one model on the DAIR-V2X dataset, making it a potent detector for VIC3D tasks with improved rotation and temporal adaptability.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jiaming_Han1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=5XshcizH9w",
  "title": "Understanding Contrastive Learning via Distributionally Robust Optimization",
  "modified_abstract": "Inspired by previous research on systematic generalization in neural networks through modularity and augmentation, this study further explores the robustness and efficiency of machine learning models from a different angle\u2014contrastive learning (CL). Here, we dive deeply into the resilience of CL against sampling bias, a scenario where negative samples could share similar semantic properties with cognitive processes observed in humans, an aspect that existing theories have inadequately explained. Our inquiry extends the discourse by deploying distributionally robust optimization (DRO) as a novel lens to scrutinize and elucidate the robust nature of CL. This analytical perspective renders several pivotal insights: (1) CL inherently performs DRO across negative sampling distributions, thereby ensuring robust performance against a plethora of potential distributions and exhibiting resilience to sampling bias; (2) the configuration of the temperature $\\tau$ transcends heuristic convention, functioning instead as a Lagrange Coefficient that modulates the scope of potential distribution sets; (3) a concrete theoretical linkage between DRO and mutual information is established, offering new evidence that positions ``InfoNCE as an estimate of MI'' and proposing an innovative method for estimating $\\phi$-divergence-based generalized mutual information. Moreover, the use of augmentation techniques in training supports the systematic generalization process by generating diverse and challenging negative samples. Modular architecture and navigation through the model's decision space enable a systematic approach to enhancing CL's resilience and efficiency. However, the paper recognizes the limitations inherent to CL, such as its over-conservatism and susceptibility to outliers. To navigate these challenges, we propose the novel Adjusted InfoNCE loss (ADNCE), which fine-tunes potential distribution, thereby enhancing model performance and expediting convergence. The validation of our findings through rigorous experimentation across various domains (image, sentence, and graph) underpins the efficacy and applicability of our approach.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Laura_Eline_Ruis1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=XddoUFpjkP",
  "title": "Bayesian Learning via Q-Exponential Process",
  "modified_abstract": "The field of machine learning continually evolves, integrating complex mathematical concepts to enhance modeling and optimization techniques. Inspired by recent advancements in optimization through novel frameworks such as the inertial block majorization minimization (MM) approach for nonsmooth nonconvex optimization, this study introduces a new perspective on regularization\u2014one of the core pillars in statistics, optimization, and machine learning. Regularization plays a critical role in achieving sparsity in parameter estimation, learning complex datasets effectively. To address the question of what probabilistic distribution corresponds to an $\\ell_q$ penalty and the correct stochastic process for modeling functions with $L_q$ regularization, we introduce the $Q$-exponential (Q-EP) process. This process is a generalization of the $q$-exponential distribution, aimed at the $L_q$ regularization of functions. By specifying consistent multivariate $q$-exponential distributions through the selection from a broad family of elliptic contour distributions and incorporating coordinate descent factorization processes, we provide a probabilistic framework that directly relates to Besov space regularization but with explicit formulations that facilitate easier manipulation, interpretation, and factorization processes. The Q-EP process offers a viable and flexible prior for functions, providing a sharper penalty ($q<2$) compared to the Gaussian process ($q=2$) and hereby advancing the Bayesian learning paradigm. The methodology's efficacy is demonstrated through its application in functional data modeling, image reconstruction, and solving inverse problems, showcasing its superiority over traditional Gaussian and Besov processes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hien_Thi_Khanh_Le1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=cGeLeh995N",
  "title": "On the Role of Entanglement and Statistics in Learning",
  "modified_abstract": "Our research builds upon foundational insights into the efficacy of learning models within the quantum statistical query ($\\mathsf{QSQ}$) model, drawing parallels and distinctions from classical learning models as discussed in previous works such as the exploration of Domain-Adjusted Regression (DARE) and its implications for out-of-distribution generalization and robust regression in unpredictable environments. In this work, we delve deeper into the quantum realm to understand the relationship between learning models when given access to entangled measurements, separable measurements, and statistical measurements. Specifically, we show that entangled measurements can dramatically reduce the sample complexity for learning an unknown function from a given concept class compared to separable measurements. Furthermore, we establish an exponential separation between the $\\mathsf{QSQ}$ learning model and quantum learning with entangled measurements, echoing the classical separation of the statistical query model from PAC learning with classification noise. Our main technical contribution introduces a quantum statistical query dimension ($\\mathsf{QSDA}$) to establish lower bounds for the $\\mathsf{QSQ}$ complexity of learning. Through this, we elucidate the challenges of testing purity of quantum states, learning complex state representations, and the limitations of learning in the $\\mathsf{QSQ}$ model compared to entangled measurements. Additionally, our findings offer unconditional evidence distinguishing weak and strong error mitigation and establish foundational lower bounds for learning distributions within the $\\mathsf{QSQ}$ framework, thereby extending and refining the theoretical underpinnings of quantum learning models beyond the constraints assumed in prior studies. This study not only uses the concept of the 'predictor' but also highlights the significance of adapting learning strategies to the 'the' unique nuances of quantum environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Elan_Rosenfeld1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=sPLTQSf6GI",
  "title": "A Measure-Theoretic Axiomatisation of Causality",
  "modified_abstract": "Inspired by the progress in understanding complex probabilistic models such as Determinantal Point Processes (DPPs) and their application in encoding cycles and moments within machine learning tasks, this paper extends foundational principles to the domain of causality, a concept pivotal across disciplines but lacking a universally recognized formalization. We posit causality as both an expansion of probability theory, akin to Kolmogorov's measure-theoretic foundation, and as an investigation into the outcomes of interventions within systems. Our contribution is the introduction of a 'causal space' - a conceptual framework composed of a probability space and a set of transition probability kernels, termed 'causal kernels,' that encapsulate the causal dynamics within a family of distributions. This formulation not only aligns rigorously with measure theory but also addresses persistent challenges in contemporary causal analysis frameworks, including issues related to cycles, latent variables, and stochastic dynamics. The learning of these fluid causal relationships further bridges the theoretical with practical implementations in diverse tasks, offering a substantive leap in the accuracy and understanding of systems influenced by causal factors. The repulsive behavior observed in certain DPP models illustrates the diverse consequences of causal interactions, enhancing both theoretical and learning applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Victor-Emmanuel_Brunel1",
  "manipulated_ranking": 3,
  "natural_ranking": 102
}
{
  "paper_link": "https://openreview.net/forum?id=0VcvYQ3uPh",
  "title": "Improved Frequency Estimation Algorithms with and without Predictions",
  "modified_abstract": "Taking inspiration from recent advancements in sketching techniques and machine learning applications within large-scale data analysis, such as the innovations presented by Hsu et al. (2019) and Acharya et al. (NeurIPS 2019) in stream-based entropy estimation, our study introduces improved algorithms for estimating element frequencies in data streams. Estimating frequencies of elements appearing in a data stream is a key task in large-scale data analysis with inherent complexity. Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with worst-case guarantees that probabilistically bound the error of the estimated frequencies for any possible input, heavily focusing on optimizing memory usage and reducing computational complexity. The work of Hsu et al. (2019) introduced the idea of using machine learning to tailor sketching algorithms to the specific data distribution they are being run on. In particular, their learning-augmented frequency estimation algorithm, which utilizes a sample of the data stream to inform its predictions, uses a learned heavy-hitter oracle which predicts which elements, or alphabet set, will appear many times in the stream. We give a novel algorithm that, in some parameter regimes, already theoretically outperforms the learning-based algorithm of Hsu et al. *without* the use of any predictions. Augmenting our algorithm with heavy-hitter predictions further reduces the error and improves upon the state of the art. Empirically, our algorithms, emphasizing streaming data challenges, achieve superior performance in all experiments compared to prior approaches thus addressing memory efficiency and complexity reduction for streaming data analysis.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Maryam_Aliakbarpour1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=KFj0Q1EXvU",
  "title": "Multi-Step Generalized Policy Improvement by Leveraging Approximate Models",
  "modified_abstract": "Inspired by recent advancements in the realm of reinforcement learning (RL), specifically the exploration of dynamics-aware hybrid offline-and-online RL paradigms and the utilization of approximate environments and simulators for addressing the sim-to-real gap, this paper introduces a novel approach to zero-shot transfer in RL. We capitalize on the principles of generalized policy improvement (GPI) and successor features (SFs) to propose a more computationally accessible method that benefits from both model-free and model-based strategies, incorporating elements of offline learning and training in simulated environments. By constructing an intermediary framework that incorporates an approximate model of the environment along with a library of policies, and leveraging dataset knowledge, we develop $h$-GPI, a multi-step extension of GPI that allows for performance optimization through controlled reasoning time, denoted by the parameter $h$. Our contributions include a proof that $h$-GPI ensures a performance lower bound surpassing that of traditional GPI, alongside evidence that its efficacy increases with higher values of $h$ and demonstrating that $h$-GPI's susceptibility to sub-optimal policies diminishes as $h$ becomes larger, even in the presence of imperfect information. Novel performance bounds are introduced to highlight the benefits of $h$-GPI in relation to the approximation errors present in the agent's policy library and its model of the environment, underscored by the challenges of learning in complex environments. These contributions notably extend beyond the existing literature, providing a comprehensive framework for enhancing RL policy efficiency. The effectiveness of $h$-GPI is empirically validated through rigorous evaluation on both tabular and continuous-state problems, where it not only outperforms standard GPI but also establishes new benchmarks against contemporary techniques under varying approximation inaccuracies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xianyuan_Zhan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=6kRQTPEVip",
  "title": "AI for Interpretable Chemistry: Predicting Radical Mechanistic Pathways via Contrastive Learning",
  "modified_abstract": "Inspired by the advancements in collaborative machine learning platforms like OpenML, which emphasizes sharing datasets and experimental results to foster reproducible research, our study introduces a novel approach in the chemical domain. We acknowledge the importance of interpretable predictions in chemistry, a challenge highlighted by the current reliance on deep learning models trained predominantly on data from the US Patent Office, which often leads to limited applicability in domains beyond the scope of the database, such as radical and atmospheric chemistry. Addressing this critical gap, we propose RMechRP, a new reaction predictor system that utilizes contrastive learning to work alongside mechanistic pathways. This method not only serves as the most interpretable representation of chemical reactions but is especially tailored for radical reactions. By developing and training deep learning models on RMechDB, a publicly available database of radical reactions, and testing different features using tools akin to scikit-learn for model evaluation, we establish a pioneering benchmark for predicting radical reactions. Our findings underscore RMechRP's capability to deliver both accurate and interpretable predictions for radical chemical reactions, showcasing its potential applicability across various fields in atmospheric chemistry.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Arlind_Kadra1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=ZRBGwpeewz",
  "title": "Revisiting Area Convexity: Faster Box-Simplex Games and Spectrahedral Generalizations",
  "modified_abstract": "This research extends the foundational concept of area convexity, introduced to address optimization issues within the challenging $\\ell_\\infty$ geometry framework. Area convexity's significance and potential for optimization, originally underscored by its application to dimension reduction for maximum matchings and the conceptualization of the fastest mixing Markov chain, set the stage for a deeper exploration of its capacities and implications. We build upon this foundation, aiming to unravel the intricate relationship between area convexity and extragradient methods' conventional analyses, reflecting on past works that transformed our understanding of optimization problem-solving. Enhanced solvers for the subproblems required by variants of the original algorithm are developed, highlighting the concept of relative smoothness as a pivotal analytical lens. Our contributions include a state-of-the-art first-order algorithm for solving box-simplex games with significantly improved computational efficiency, facilitating advancements in solving approximation problems for maximum flow, optimal transport, and min-mean-cycle among other combinatorial optimizations. Furthermore, we introduce a near-linear time algorithm addressing a matrix generalization of box-simplex games, opening new avenues in tackling problems related to semidefinite programs pivotal in robust statistics, numerical linear algebra, and graph embedment for improved conductance measures.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Thuy-Duong_Vuong1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=XPWEtXzlLy",
  "title": "Mirror Diffusion Models for Constrained and Watermarked Generation",
  "modified_abstract": "Building on the foundation laid by determinantal point processes (DPPs) in capturing complex data distributions through principled probabilistic modeling of diversity and quality, this study introduces Mirror Diffusion Models (MDM), a novel advancement in the realm of diffusion models for machine learning. The recent success of diffusion models in learning complex, high-dimensional data distributions is partly due to their capability to construct diffusion processes with analytic transition kernels and score functions, enabling a semi-definite, simulation-free framework with stable regression losses for machine learning applications. However, this success falters when applied to data confined to constrained sets rather than a standard Euclidean space, with prior attempts failing to preserve the models' desirable characteristics. MDM addresses this limitation by generating data on convex constrained sets without sacrificing tractability, utilizing a dual space constructed from a mirror map\u2014essentially a standard Euclidean space for efficient sampling. Efficient computation of mirror maps for popular constrained sets, such as simplices and $\\ell_2$-balls, is derived, showing significantly improved performance over existing methods for managing and generating diverse items in recommendation systems or content retrieval and summarization scenarios. Furthermore, this framework introduces the use of constrained sets to embed invisible, quantitative information (i.e., watermarks) in generated data for safety and privacy purposes, presenting a compelling application of MDM. This exploration brings new algorithmic possibilities for learning tractable diffusion processes on complex domains, expanding the utility and capability of diffusion models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Victor-Emmanuel_Brunel1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=4qG2RKuZaA",
  "title": "Censored Sampling of Diffusion Models Using 3 Minutes of Human Feedback",
  "modified_abstract": "Inspired by recent innovations in generating photorealistic adversarial examples through semantic manipulation of image-based visual descriptors, our work extends the exploration into the realm of diffusion models, which have demonstrated unprecedented success in high-quality image generation. The emerging challenge with diffusion models is their occasional production of undesirable outputs amidst generally high-quality images. Addressing this, we introduce a method for censored generation within pre-trained diffusion models, leveraging a reward model refined by minimal human feedback. We underscore the efficacy of censoring with an astonishingly low requirement for human intervention, demonstrating that a mere few minutes of feedback are sufficient to significantly mitigate the generation of unfavorable images through strategic perturbations, focusing on both the broader image descriptors and the finer nuances such as texture. This novel approach opens avenues for highly efficient content filtering mechanisms in advanced image generation tasks, showcasing a promising intersection of adversarial thinking and diffusion model capabilities, with an emphasis on manipulating texture descriptors for more effective content control, supporting the continuous advancement towards more reliable and user-aligned artificial intelligence systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Anand_Bhattad1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=INS3ltgjg7",
  "title": "TopoSRL: Topology preserving self-supervised Simplicial Representation Learning",
  "modified_abstract": "Inspired by groundbreaking efforts to learn disentangled representations for complex interactions within unstructured and graph-based data, our work introduces $\\texttt{TopoSRL}$, a cutting-edge self-supervised learning (SSL) methodology designed explicitly for simplicial complexes. This novel approach aims to overcome the inherent limitations faced by existing generative graph-based SSL methods, which predominantly focus on pairwise interactions, thereby ignoring the multifaceted long-range dependencies essential for capturing comprehensive topological information. $\\texttt{TopoSRL}$ utilizes an innovative simplicial augmentation technique to generate multiple views of a simplicial complex, significantly enriching the learned representations in an efficient manner. Further, we develop a unique simplicial contrastive loss function that intricately contrasts the generated simplices, ensuring the preservation of both local and global topological information within the simplicial complexes. Our extensive experimental analysis highlights the superiority of $\\texttt{TopoSRL}$ over leading graph SSL techniques and supervised models designed for simplicial data, across a variety of datasets and tasks. These findings not only underscore the effectiveness of $\\texttt{TopoSRL}$ in disentanglement and processing complex simplicial data within a self-supervised framework but also mark a significant advancement in the field of representation learning for tasks requiring an understanding of latent topological structures and enables generation of data-driven insights with potential applications in few-shot learning scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Giangiacomo_Mercatali1",
  "manipulated_ranking": 10,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=znY173SCxu",
  "title": "Time-Reversed Dissipation Induces Duality Between Minimizing Gradient Norm and Function Value",
  "modified_abstract": "Recent developments in convex optimization, including bilevel optimization that combines stochastic methods for global variance reduction, set the stage for exploring new optimization phenomena. Inspired by these advancements, this work introduces H-duality, a concept postulated on the foundation laid by first-order optimization methods and their efficiency in minimizing function values, as highlighted since Nesterov's 1983 work. Our findings reveal a one-to-one correspondence, termed H-duality, between methods that efficiently minimize function values and those that reduce gradient magnitudes. This duality, distinct from traditional Lagrange/Fenchel duality, is characterized by reversing the time dependence in the dissipation/friction term in continuous-time formulations. Through solving a wide array of learning processes, we elucidate the symmetry between Nesterov's accelerated method and OGM-G, propose a new class of algorithms for efficient reduction of gradient magnitudes in smooth convex functions, and present a composite minimization method surpassing FISTA-G in simplicity and speed. This contributes a new perspective to the field of optimization and learning. Our algorithm not only addresses theoretical risk minimization but also offers practical solutions through samples in training data, effectively bridging the gap between theory and practice.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pierre_Ablin2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=OiatK9W6tR",
  "title": "Quantum speedups for stochastic optimization",
  "modified_abstract": "In the realm of optimization, the quest for efficiency becomes paramount, especially in the context of stochastic gradient descent methods and their applications to convex and non-convex problems. Building upon the foundation laid by recent advancements such as AdaSpider, which introduced an adaptive variance-reduction technique for non-convex finite-sum minimization, our work extends these concepts into the quantum computing framework. We consider the problem of minimizing a continuous function given access to a natural quantum generalization of a stochastic gradient oracle. Our research introduces two innovative methods for the special case of minimizing a Lipschitz convex function, where each method achieves a dimension versus accuracy trade-off previously deemed unattainable with classical computing methods. Furthermore, we prove one method to be asymptotically optimal in low-dimensional settings, a result that closely matches theoretical limits. Our research also introduces quantum algorithms for identifying a critical point of a smooth non-convex function at rates not achievable by classical means, showcasing the smoothness aspect critical to the optimization process. These advancements are facilitated by building on the quantum multivariate mean estimation result of Cornelissen et al. and establishing a general quantum variance reduction technique, which is of independent interest. Our findings not only demonstrate quantum computing's potential to revolutionize stochastic optimization but also provide a bridge between classical adaptive variance reduction methods and quantum computing capabilities.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Leello_Tadesse_Dadi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=1qFnxhdbxg",
  "title": "Energy Discrepancies: A Score-Independent Loss for Energy-Based Models",
  "modified_abstract": "The development of energy-based models (EBMs) has been influenced by the need for probabilistic frameworks capable of capturing complex data distributions without the intensive computational demands typically associated with methods like normalizing flows, which strive to model distributions with complex topological structures through invertible flow processes. In this context, we propose a novel loss function for EBMs, termed Energy Discrepancy (ED), which circumvents the reliance on score computation or the computationally expensive Markov chain Monte Carlo (MCMC) sampling methods. The ED loss uniquely positions itself by approximating both explicit score matching and negative log-likelihood losses under various limits, thereby offering a versatile tool for learning in training EBMs. This dual approximation capability enables the ED approach to address the issue of nearsightedness plaguing score-based estimation methods while ensuring theoretical robustness. Our numerical experiments highlight the efficacy of ED in learning low-dimensional data distributions more swiftly and accurately than traditional methods, including those utilizing tabular data. Furthermore, for high-dimensional image datasets, we assess the challenges posed by the manifold hypothesis to our approach and illustrate how energy discrepancy can significantly enhance the training of EBMs as priors within a variational decoder framework, effectively improving the density estimation of complex data. This contribution not only underscores the practical benefits of our proposed loss function but also sets the stage for further exploration into efficient and effective training methodologies for EBMs, incorporating learning dynamics free from direct sampling efforts.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Vincent_Stimper1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=TW3ipYdDQG",
  "title": "Fair Streaming Principal Component Analysis: Statistical and Algorithmic Viewpoint",
  "modified_abstract": "Building on the momentum from previous works that introduced innovative approaches for approximating statistical distances and their application in machine learning, such as the Fast Approximation of the Sliced-Wasserstein Distance through the exploitation of the concentration of random projections, we identify a significant gap in the realm of Fair Principal Component Analysis (PCA). While these foundational studies facilitated computational and statistical advancements in handling high-dimensional data, the domain of fair PCA remains underexplored, lacking a concrete statistical foundation and facing practical challenges related to memory constraints. Our work addresses these issues by rigorously formulating fair PCA using a new notion called probably approximately fair and optimal (PAFO) learnability and introducing a fair streaming PCA setting. We propose a memory-efficient algorithm, the fair noisy power method (FNPM), and provide its statistical guarantee in terms of PAFO-learnability, marking a first in the literature of fair PCA. Verified on the CelebA dataset without any pre-processing, our algorithm transcends previous approaches by efficiently and effectively performing fair PCA in a streaming setting, overcoming the inherent memory limitations. Our methodology incorporates nonasymptotical analysis and a novel sampling technique designed to handle the challenges of high-dimensional datasets, demonstrating an exemplar use case of how fair algorithms can adapt to the streaming paradigm. Notably, our approximation technique significantly contributes to the efficient processing of vector representations in high-dimensional spaces, cutting down the distance between theoretical innovation and practical application.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kimia_Nadjahi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=TZtw5YgxTE",
  "title": "MIM4DD: Mutual Information Maximization for Dataset Distillation",
  "modified_abstract": "In the realm of machine learning, recent inquiries into multi-task and single-task learning have leveraged shared knowledge and optimization strategies, including gradient descent, to enhance model performance across diverse tasks, including training neural networks more efficiently. Drawing inspiration from these advancements, our work explores dataset distillation (DD), a technique aimed at synthesizing a small dataset that retains the generalization and test performance of its full-sized counterpart, thereby emerging as a critical tool for efficient training in settings with limited computational resources. Unlike state-of-the-art methods, which primarily rely on heuristic indicators from network comparisons and loss metrics, we introduce a principled approach utilizing mutual information (MI) to quantify the information shared between synthetic and real datasets. Through the novel MIM4DD framework, we numerically maximize MI via a contrastive learning strategy, establishing positive and negative sample pairs based on label congruency and employing an optimizable regularization objective to refine the synthetic dataset. This approach not only challenges the conventional reliance on heuristic measures but also provides a solid theoretical foundation for dataset compression and generalization. Experimentation demonstrates the efficacy of the MIM4DD as an augmentative module to existing dataset distillation methodologies, underscoring its potential to revolutionize the domain through an information-theoretic perspective.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hoang_Viet_Phan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=N5uUTWLz0E",
  "title": "Understanding and Improving Ensemble Adversarial Defense",
  "modified_abstract": "The ensemble strategy, leveraging multiple base classifiers to withstand adversarial attacks collectively, represents a significant advancement in enhancing model robustness, mirroring progress seen in other areas of machine learning such as generative adversarial networks (GANs). This paper specifically builds upon previous insights, including those from adversarial 1-Lipschitz regularization, which underlines the importance of robust training methods in improving the resilience of models against adversarial threats via incorporating gradient-based techniques and normalization processes. Despite the demonstrated empirical success of ensemble methods in adversarial contexts, a theoretical framework to elucidate the enhanced robustness provided by the ensemble over individual adversarially trained classifiers is missing. Addressing this knowledge gap, we propose a novel error theory for ensemble adversarial defense, proving a reduction in 0-1 loss for adversarially challenging sample sets. Building on this theoretical foundation, we introduce an innovative enhancement strategy, interactive global adversarial training (iGAT), which incorporates a probabilistic distribution rule for allocating globally challenging adversarial examples among base classifiers and a regularization component targeting the most significant weaknesses within the ensemble, thereby improving the ensemble's quality of defense and leveraging networks' potential to a greater extent. Tested across a spectrum of ensemble adversarial defense mechanisms, iGAT demonstrates up to a 17% performance increase on CIFAR10 and CIFAR100 datasets under both white-box and black-box attacks, marking a significant step forward in understanding and fortifying against adversarial threats through effective training methodologies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~D\u00e1vid_Terj\u00e9k1",
  "manipulated_ranking": 6,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=Eysb8t3MJ5",
  "title": "GlucoSynth: Generating Differentially-Private Synthetic Glucose Traces",
  "modified_abstract": "The challenge of generating high-quality, private synthetic time series data, particularly glucose traces, calls for innovative approaches that adequately address both privacy concerns and data utility. Inspired by recent advancements in temporal data modeling, such as the diffusion-based generative modeling that preserves the continuity of underlying functions in irregularly sampled time series, our work introduces GlucoSynth. This novel privacy-preserving Generative Adversarial Network (GAN) framework is tailored for the synthesis of glucose traces. By focusing on maintaining the intrinsic temporal dynamics and relationships amongst motifs in glucose data, integrating differential privacy mechanisms, and employing techniques for denoising and imputation, GlucoSynth aims to provide strong formal privacy guarantees without compromising the utility of synthetic data. Furthermore, its ability to assist in forecasting glucose levels demonstrates an extensive evaluation using 1.2 million glucose traces from stochastic datasets. GlucoSynth's superior capability in generating high-quality synthetic glucose traces with robust privacy assurances, thereby outperforms existing methods in this domain. Its application extends to various multivariate learning scenarios, illustrating a versatile approach to synthetic data generation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Marin_Bilo\u01611",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=p53QDxSIc5",
  "title": "DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction",
  "modified_abstract": "Inspired by recent advancements in semiparametric language models for scalable continual learning, this work introduces an innovative text-to-SQL generation technique that leverages the strengths of Large Language Models (LLMs) in decomposed in-context learning complemented by a self-correction methodology. Acknowledging the significant gap between the performance of fine-tuned models and prompting approaches using LLMs on complex tasks such as text-to-SQL conversion, as evaluated on the Spider dataset, we address this challenge by dissecting the generation task into manageable sub-tasks and incorporating streaming data. By feeding solutions of these subdivided problems back into LLMs, along with an integrated approach for storing and modeling task-specific memory recall, we demonstrate a potent method for enhancing LLM reasoning capabilities, thereby markedly improving performance. Our experimentation with three LLMs, through a process involving both in-context learning and specific training strategies, reveals that this strategy consistently bolsters their few-shot learning performance by approximately 10%, propelling LLM accuracy towards or beyond current state-of-the-art (SOTA) figures. Specifically, we achieve a new SOTA execution accuracy of 85.3% on the Spider dataset's holdout test set, surpassing the previous SOTA accuracy of 79.9%. Furthermore, when evaluated against the BIRD benchmark, our methodology, applying the learner-centric approach of breaking down complex language tasks into more simplified and digestible sub-tasks, sets a new benchmark by attaining an execution accuracy of 55.9%, thereby establishing a new SOTA on its holdout test set. This approach not only closes the performance gap between LLMs and fine-tuned models but also showcases the potential of decomposed in-context learning combined with self-correction for significantly improving the efficacy of LLMs in complex reasoning tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tao_Ge1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=l6ypbj6Nv5",
  "title": "Generative Category-level Object Pose Estimation via Diffusion Models",
  "modified_abstract": "Influenced by the recent achievements in self-supervised learning paradigms, specifically in generating realistic repositioning of humans through exploiting 3D body information from videos, our work explores object pose estimation from a novel angle. Object pose estimation is essential in embodied AI and computer vision, facilitating intelligent agents to perceive and manipulate their environment. Unlike existing methods that often struggle with the multihypothesis issue arising from partially observed point clouds in category-level pose estimation, we propose a paradigm shift by formulating the task as conditional generative modeling within a novel framework. By employing score-based diffusion models, our method generates object pose estimations through a systematic process; initially sampling pose candidates from the diffusion model followed by outlier filtering via likelihood estimation and averaging the survivors through mean pooling. To circumvent the computationally intensive integration for likelihood estimation, we introduce a more efficient technique that employs an energy-based model distilled from the original score-based diffusion models during training. This not only enhances end-to-end processing but also sets new benchmarks on the REAL275 dataset, achieving unparalleled accuracy rates exceeding 50% and 60% on stringent 5\u00b0+2cm and 5\u00b0+5cm metrics, respectively. Moreover, our approach, rooted in self-supervision and a comprehensive training strategy, exhibits exceptional generalization capabilities to unseen object categories without the need for category-specific fine-tuning and extends naturally to tasks beyond static pose estimation, including object pose tracking, rivaling top-performing baselines. Our checkpoints and demonstrations can be viewed at [Link redacted for privacy].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Soubhik_Sanyal1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=AesN5bYnJr",
  "title": "No-Regret Online Prediction with Strategic Experts",
  "modified_abstract": "Our study advances the online binary prediction with expert advice framework, incorporating insights from prior work on nonstationary dual averaging and online fair allocation of items. This prior work addresses the challenge of fair allocation with online item arrivals and suggests that algorithmic approaches, including averaging techniques, can adapt to varying conditions while maintaining fairness and efficiency in the presence of adversarial behaviors. Building on this, we generalize the learning prediction framework to allow a learner to select $m\\geq 1$ experts from a pool of $K$ and consider strategies in which experts aim to maximize their influence on the algorithm's predictions, potentially misreporting their beliefs about future events, including those with periodic characteristics. This setting finds relevance in contexts such as forecasting competitions that require not only aggregating forecasts but also ranking forecasters by performance, arriving at a mechanism that is resilient to corruption in the form of misinformation. Our objective is to devise algorithms that are both incentive-compatible, encouraging truthful reporting of beliefs, and no-regret, ensuring that the regret relative to the true beliefs of the best fixed set of $m$ experts in hindsight is sublinear. We demonstrate that the straightforward application of existing $m=1$ solutions is inadequate and introduce algorithms that leverage the unique characteristics of the utility functions to meet our dual objectives of incentivizing truthfulness and achieving sublinear regret.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Luofeng_Liao1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=v7WWesSiOu",
  "title": "End-To-End Latent Variational Diffusion Models for Inverse Problems in High Energy Physics",
  "modified_abstract": "In the domain of high-energy physics, particularly within the context of experiments conducted at the Large Hadron Collider (LHC), the complexity of dealing with detector effects presents a significant challenge in translating detector observations to theoretical quantities that underpin the collisions observed. This challenge mirrors the intricacies encountered in learning dynamical models, such as those described by Neural Dynamical Systems (NDS), where dynamical systems incorporating equations with prior knowledge have shown the potential for high accuracy in predicting future states with fewer samples. In this vein, our work investigates and compares various generative deep learning methods aiming to approximate the inverse mapping required to correct detector effects and align measurements with theoretical predictions. We introduce latent variational diffusion models, a novel architecture that synergizes the latent learning capabilities of advanced generative methods with an end-to-end variational framework, addressing the inverse problems inherent in high-energy physics data analysis. Our approach demonstrates superior performance in reconstructing global distributions of theoretical kinematic quantities and ensuring learned posterior distributions adhere strictly to known physics constraints. Compared to non-latent state-of-the-art baselines and conventional latent diffusion models, our unified method achieves a distribution-free distance to truth metrics significantly reduced, indicating its potential in enhancing the precision and reliability of particle physics analyses, even with data collected irregularly from detectors. The deep integration of variational principles and diffusion processes presents a comprehensive solution for inverse problems, contributing significant advancements in both theory and application.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Viraj_Mehta1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=8pOBo5NgTQ",
  "title": "FLSL: Feature-level Self-supervised Learning",
  "modified_abstract": "In the context of self-supervised learning (SSL), there has been a significant focus on instance-level representations, particularly through methods like SimCLR, DINO, VICReg, and MOCOv3, which often fall short in dense prediction tasks such as object detection and segmentation. This limitation points to an unaddressed gap between existing SSL approaches and the requirements of dense prediction tasks. Building on the foundational insights from previous works, such as EHSOD's exploration of hybrid-supervised object detection and the effective use of class activation maps for foreground proposal and refinement, our study introduces a novel paradigm in SSL. We leverage the Vision Transformers (ViT) for the first time to elucidate the mean-shift clustering process aligned with natural image semantics, thereby proposing the bi-level feature clustering method called Feature-Level Self-supervised Learning (FLSL). FLSL advances SSL by formalizing the problem from both the mean-shift and k-means perspectives, fostering semantic cluster representations that significantly enhance performance in dense prediction tasks through classification accuracy improvement. Empirical results demonstrate that FLSL achieves notable improvements in object detection and instance segmentation on MS-COCO, using Mask R-CNN with ViT backbones, and extends its applicability to UAV object detection and video instance segmentation across various datasets. The use of bounding-box annotations within a semi-automated cascade system for segmentation tasks is discussed, underscoring FLSL's ability to address tasks requiring complex spatial relationships through weakly-annotated data. Through extensive experimentation, visualization, and ablation studies, including the updating of benchmarks and activation process adjustments, our research substantiates the efficacy of FLSL and its superiority over traditional SSL methods in addressing the nuances of dense prediction challenges, with a special focus on the contribution of the detector component. The abstract also includes a call for further exploration into FLSL's mechanisms and potential applications, emphasizing the shift towards feature-level self-supervision as a promising direction for future SSL research.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhili_LIU1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=UlHueVjAKr",
  "title": "Textually Pretrained Speech Language Models",
  "modified_abstract": "Recent advancements in natural language understanding, exemplified by comprehensive benchmarks such as the Korean Language Understanding Evaluation (KLUE), demonstrate the potential for leveraging pretrained textual language models to enhance speech language models (SpeechLMs). In this context, our work introduces TWIST, a novel method for training SpeechLMs using a warm start from pretrained textual language models. TWIST outperforms traditional cold-start SpeechLMs in both automatic and human evaluations, highlighting the benefits of textually pretrained foundations for speech processing tasks. Our empirical analysis investigates various model design choices, including the speech tokenizer, the pretrained textual model, and dataset size, revealing the significant impact of multilingual capabilities, model, and dataset scale on SpeechLM performance. Our contributions, including speech samples, code, and models, are made publicly available as open-source to encourage community engagement and advancement in the field. Additionally, we introduce spoken versions of the StoryCloze benchmark to enhance model evaluation and stimulate further research. This effort towards making SpeechLMs more cross-lingual and identifiable in varied linguistic contexts further underlines the importance of broad understanding and ethical considerations in speech recognition tasks. We present the largest SpeechLM to date in terms of parameters and training data volume, based on our findings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jamin_Shin1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=MOAHXRzHhm",
  "title": "Enhancing Adversarial Robustness via Score-Based Optimization",
  "modified_abstract": "The vulnerability of deep neural network classifiers to adversarial attacks highlights the pressing need for developing effective defense mechanisms. This urgency is underscored by recent research into model fingerprinting, which seeks to protect model intellectual property against stealing attacks by analyzing model behavior under adversarial conditions and establishing a fingerprint or unique identifier for the models. Drawing inspiration from these concerns and the correlation between model robustness and susceptibility to intellectual property theft, our paper introduces ScoreOpt, a novel adversarial defense mechanism that leverages score-based diffusion models. Unlike traditional diffusion-based defenses that rely on computationally expensive and suboptimal sequential simulations, ScoreOpt employs a test-time optimization of adversarial samples towards their original, clean data forms, using score-based priors. Our training methodology enhances the transferability of this defense to various tasks and datasets. We evaluate the efficacy of our approach through extensive experiments on CIFAR10, CIFAR100, and ImageNet, showcasing significant enhancements in both robustness against adversarial attacks and inference speed compared to existing methods. Additionally, the introduction of a surrogate model during the optimization process further bolsters our defense mechanism's efficiency. This advancement offers a dual contribution: providing a formidable defense against adversarial threats and advancing the conversation on safeguarding neural networks from intellectual property theft through improved robustness.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jiyang_Guan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=IoizwO1NLf",
  "title": "Skill-it! A data-driven skills framework for understanding and training language models",
  "modified_abstract": "Inspired by the recognition that both the quantity and quality of data are critical for machine learning (ML) performance, as demonstrated in prior works focused on general goal-reaching policies in reinforcement learning, our study investigates the impact of training data selection on pre-trained large language models (LMs) across various tasks. With a fixed budget of tokens, we aim to determine the most effective way to select data that enhances downstream performance of LMs, embodying an exploration of data utility for optimal training efficiency. We propose a novel framework premised on the hypothesis that, akin to human skill acquisition following a deliberate sequence, language models similarly learn from training data in a natural, ordered manner. This framework not only formalizes the concept of a skill and an ordered skill set in terms of associated data but also introduces an online data sampling algorithm, Skill-It, that leverages skill mixtures for both continual pre-training and fine-tuning to efficiently learn multiple or individual skills, respectively. Utilizing synthetic and real datasets, we affirm the existence of these ordered skill sets, showing that prerequisite skills facilitate the learning of more complex skills with less data. The Skill-It controller uses a deliberate pose-achievement approach in data selection to mirror targeted learning objectives from prerequisite tasks to more complex ones, ensuring that each task selected aligns with the model's current learning trajectory. In the continual pre-training setting on the LEGO synthetic dataset, Skill-It achieves 37.5 points higher accuracy than random sampling, highlighting its aptitude for structured exploration. Furthermore, in the fine-tuning context on the Natural Instructions dataset, it yields a 13.6% reduction in validation loss for the targeted skill compared to training directly on data associated with that skill. When applied to the RedPajama dataset for continual pre-training of a 3B-parameter LM, our framework accomplishes superior accuracy on the LM Evaluation Harness using 1B tokens compared to a baseline sampling approach across data sources with 3B tokens.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alexis_Jacq1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=QlHosp050r",
  "title": "Weakly-Supervised Concealed Object Segmentation with SAM-based Pseudo Labeling and Multi-scale Feature Grouping",
  "modified_abstract": "Inspired by the recent surge in unsupervised and weakly-supervised segmentation approaches, notably in complex scenes involving multiple objects with diverse textures, as evidenced by the ClevrTex benchmark, our research advances the domain of Weakly-Supervised Concealed Object Segmentation (WSCOS). WSCOS aims to segment objects well-integrated with their surroundings using minimally annotated data. This task presents pronounced challenges, including the difficulty of distinguishing concealed objects from their backgrounds due to intrinsic similarity, and limitations in model learning imposed by sparse annotations. In response, we introduce a novel WSCOS method aimed at overcoming these obstacles. To address the complexity and similarity between objects and their environment, we devise a multi-scale feature grouping module that synergizes features at diverse granularities, promoting segmentation coherence and facilitating comprehensive segmentation outcomes for images containing single or multiple objects with intricate textures, shapes, and developed variants. For the weak supervision hurdle, we leverage the Segment Anything Model (SAM), a recently proposed vision foundation model. Utilizing sparse annotations as prompts, we generate segmentation masks that serve as training inputs to the model. To mitigate the effects of low-quality masks, our methodology includes a suite of strategies such as result ensemble through multi-augmentation, entropy-based pixel-level weighting, image-level selection criteria founded on entropy, and employing texture representations in the learning process. These strategies collectively fortify the training process, ensuring more dependable supervision. Our evaluation across numerous WSCOS tasks corroborates the superior performance of our approach, establishing new benchmarks in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Laurynas_Karazija1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=dCAk9VlegR",
  "title": "This Looks Like Those: Illuminating Prototypical Concepts Using Multiple Visualizations",
  "modified_abstract": "Drawing insight from recent advancements in post-hoc explanation methods and contrastive learning, our work introduces ProtoConcepts, a novel approach to interpretable image classification. This approach is rooted in combining the effective aspects of deep learning with case-based reasoning through prototypical parts, echoing the \"this looks like that\" reasoning in prototype-based image classification yet innovating beyond the limitations of existing methods. Existing prototype-based classifiers limit explanations to one-to-one image patch comparisons, often obscuring the broader conceptual similarities they aim to highlight. By learning and visualizing prototypical concepts through multiple image patches, ProtoConcepts clarifies the basis of its comparisons\u2014be it color, shape, or another underlying concept\u2014enabling more nuanced and interpretable visual explanations. This method proposes a significant departure from single-image comparisons, drawing upon the foundational work in enhancing model consistency, interpretability through contrastive and self-supervised learning techniques, and improving generalization. Our experimental validation across benchmark datasets demonstrates that this enriched \"this looks like those\" reasoning process, coupled with robust training protocols and optimized embeddings generation, can be seamlessly integrated into existing prototypical image classification frameworks without sacrificing accuracy, thus offering a richer, more interpretable model for visual explanations that acts on complex network representations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Soroush_Abbasi_Koohpayegani1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=iM0MWWBr4W",
  "title": "A Unified Model and Dimension for Interactive Estimation",
  "modified_abstract": "Our work is inspired by a variety of previous research efforts that have tackled complex problems within interactive learning, such as those addressing challenges in the domain of low-rank matrix completion. We study an abstract framework for interactive learning called interactive estimation, in which the goal is to estimate a target from its ``similarity'' to points queried by the learner. To approach challenges iteratively, we introduce a combinatorial measure called Dissimilarity dimension which largely captures learnability in our model. We present a simple, general, and broadly-applicable algorithm, interpreted as leveraging iterative techniques, for which we obtain both regret and PAC generalization bounds that are polynomial in the new dimension. Our methodological contributions draw on iterative techniques and leverage the structure of the problem, akin to applying squares of error terms in optimization problems for completion of low-rank matrices. We show that our framework subsumes and thereby unifies two classic learning models: statistical-query learning and structured bandits. We also delineate how the Dissimilarity dimension is related to well-known parameters for both frameworks, in some cases yielding significantly improved analyses.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Christian_K\u00fcmmerle1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=fmYmXNPmhv",
  "title": "Permutation Equivariant Neural Functionals",
  "modified_abstract": "In the context of expanding the capabilities and understanding of neural network architectures, particularly through the exploration of differentiable Neural Architecture Search (NAS) techniques via gradient-based methods, this work presents an investigation into designing neural networks capable of interacting with the weights or gradients of other neural networks, termed *neural functional networks* (NFNs). The pursuit of NFNs is imbued with the challenge of addressing the permutation symmetries inherent in the weights of deep feedforward networks, given that hidden layer neurons possess no inherent order. Our framework proposes *permutation equivariant* neural functionals as a solution, with architecture selection inductively biased by these symmetries and propelled by *NF-Layers* (neural functional layers) designed under constraints for permutation equivariance through an appropriate parameter sharing protocol. This novel exploration discovers that permutation equivariant neural functionals propose effectiveness across a breadth of tasks including predicting classifier generalization, generating \"winning ticket\" sparsity masks for initializations, and conducting classification or modifications of implicit neural representations (INRs). Furthermore, despite the broad applicability seen in prior works like the reevaluation of architecture selection in differentiable NAS, which informed enhancements in architecture optimization processes via supernet performance evaluation, our study bridges a critical methodological gap by offering a unifying principle for the design of architectures processing network weights and conducting a perturbation-based analysis on super network performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ruochen_Wang2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=X3IeHRD0zf",
  "title": "Causal Imitability Under Context-Specific Independence Relations",
  "modified_abstract": "The exploration of causal mechanisms in machine learning, especially in the context of imitation learning, has grown to acknowledge significant gaps when causal structures are ignored. This realization nests within broader research efforts, illustrated by studies such as the utilization of amortized inference for causal structure learning, which highlights the complexities and challenges of discerning causal relationships from data. Against this backdrop, our work introduces a novel perspective by investigating the role of context-specific independence (CSI) relations in causal imitation learning. CSI represents independence conditions that only apply under specific circumstances, which, hitherto, have not been fully integrated into the assessment of imitation learning's feasibility or its susceptibility to causal confounding and misspecification. Leveraging structures and graphs to model these relations, we address the gap by rigorously proving that determining the feasibility of imitation learning, given known CSI relations, is NP-hard. Furthermore, we establish a necessary graphical criterion for causal imitation under these conditions and, with an additional structural assumption, demonstrate its sufficiency. To navigate the complexities introduced by CSI in causal learning, we also propose a sound algorithmic approach that leverages both CSI relations and observational data for searching through potential imitable instances with interventional strategies, hence forwarding the discourse on how additional causal information can refine imitation learning strategies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Scott_Sussex1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=zpVCITHknd",
  "title": "Towards Personalized Federated Learning via Heterogeneous Model Reassembly",
  "modified_abstract": "Building on the insights from recent advancements in knowledge distillation techniques, specifically the transfer of knowledge in heterogeneous model ensembles for improving top-K recommendations, this paper introduces a novel framework, pFedHR, aimed at tackling the challenge of model heterogeneity in federated learning environments. Within federated learning, the disparity in client models' structures poses significant hurdles towards achieving efficient and personalized learning outcomes. By conceptualizing the issue of model heterogeneity as a model-matching optimization problem, pFedHR facilitates personalized federated learning through the reassembly of heterogeneous models, including both teacher and student components for enhanced distillation efficiency. This approach, inspired by the principles of transferring nuanced knowledge between diverse models and overcoming the training bottleneck in knowledge distillation, allows for the dynamic generation of personalized model candidates with minimal manual oversight, advancing recommender systems. The proposed framework, pFedHR, not only demonstrates superior performance over established baselines across various datasets under both IID and Non-IID settings but also actively mitigates the challenges posed by the utilization of public data, which may significantly diverge in distribution from client-specific data. Through rigorous experimentation, pFedHR showcases its capacity to not only adaptively manage the integration of heterogeneity within client models but also to refine the personalization process, emphasizing its potential to enhance efficiency and effectiveness in federated learning paradigms. Furthermore, it leverages the teacher\u2019s ranking knowledge to orchestrate a more intelligent ensemble of client models for personalized recommendations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~WONBIN_KWEON1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=jgIrJeHHlz",
  "title": "Debiasing Scores and Prompts of 2D Diffusion for View-consistent Text-to-3D Generation",
  "modified_abstract": "Inspired by recent advances in neural rendering and text-to-3D synthesis, notably the editable free-viewpoint video generation using layered neural representations, this study investigates the view inconsistency problem inherent in current score-distilling text-to-3D generation techniques. These techniques often face the Janus problem, where the canonical view of an object appears incorrectly in other views due to biases in 2D diffusion models and inadequate volume parsing. By examining existing frameworks, we pinpoint the embedded bias of 2D diffusion models as the primary cause. To address this, we propose two novel debiasing methods: score debiasing, which involves truncating the score estimated by 2D diffusion models with an incrementally increasing truncation value, and prompt debiasing, which rectifies discrepancies between user and view prompts using a language model, leveraging self-supervised learning strategies to refine the process. Our experiments demonstrate that these approaches not only enhance the realism of the generated 3D objects by reducing artifacts but also balance fidelity to the original 2D models with 3D view consistency, adding minimal overhead. Through our efforts, we contribute toward resolving significant challenges in achieving view-consistent text-to-3D generation, fostering further advancements in 3D modeling from textual descriptions. The introduction of motion-aware techniques and the utilization of multiple cameras for training further enhance the editing capabilities, ensuring that the generated 3D models adhere closely to the desired perspectives and motions, thus making significant strides in both neural representation and volume synthesis for free-viewpoint video generation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xinhang_Liu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=GqtpYUCwnu",
  "title": "$\\varepsilon$-fractional core stability in Hedonic Games.",
  "modified_abstract": "Hedonic Games (HGs) are a classical framework modeling coalition formation of strategic agents guided by their individual preferences, drawing inspiration from contemporary research avenues exploring optimization and fairness in algorithmic contexts, such as fair assortment planning in online platforms and the operational design of crowd-powered retail stores. This prior work has illuminated the challenges and opportunities inherent in designing systems that balance individual and collective well-being, a principle directly applicable to the study of HGs. In HGs, the pursuit of stability within coalition structures\u2014where agents' grouping preferences are paramount\u2014is often stymied by the rarity of core-stable partitions and the computational difficulty of identifying them. Addressing these limitations, we introduce the concept of $\\varepsilon$-fractional core-stability, a relaxation allowing for up to an $\\varepsilon$-fraction of all possible coalitions to core-block, significantly enhancing the tractability and existence of stable solutions. By developing efficient algorithms that employ oracles for evaluating the stability criteria and yield $\\varepsilon$-fractional core-stable partitions with $\\varepsilon$ diminishing exponentially with the size of the agent set for two fundamental classes of HGs (Simple Fractional and Anonymous), we demonstrate that this concept not only promotes the feasibility of achieving stability but does so within a polynomial time frame. Moreover, by examining the probabilistic nature of $\\varepsilon$-fractional core stability\u2014equating it with the probability of uniformly sampled coalitions core-blocking below $\\varepsilon$\u2014and extending this framework to accommodate more nuanced sampling distributions and PAC-learning scenarios, we offer new insights into the dynamic interplay between theoretical construct and practical application through diverse datasets. Our analysis reveals both positive and negative results regarding the ability to compute $\\varepsilon$-fractional core-stable outcomes efficiently across different distributional assumptions, marking a noteworthy advancement in the ongoing exploration of equitable, robust coalition formation. This study synthesizes dual perspectives of parameterized complexity theory and the practical requisites of scalability and flexibility in algorithmic contexts, providing a significant contribution to the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Qinyi_Chen1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
